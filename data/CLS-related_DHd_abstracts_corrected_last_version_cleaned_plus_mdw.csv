year,conference,filename,title,authors,keywords,topics,abstract,language,clean_abstract,top_words
2014,DHd2014,2014_cls_metadata_extracted.csv,A Flexible NLP Pipeline for Computational Narratology,"Thomas Bögel (Heidelberg University, Germany); Jannik Strötgen (Heidelberg University, Germany); Christoph Mayer (Heidelberg University, Germany); Michael Gertz (Heidelberg University, Germany)","NLP, Pipeline, Narratologie, Computational","NLP, Pipeline, Narratologie, Computational","1 Project Overview Temporal dependencies reveal interesting insights into the semantic discourse structure of narrative texts. The investigations of literary scientists are, as of today, mostly based on labor-intensive manual annotations. Computational Narratology, an important subtopic of the Digital Humanities, aims at facilitating annotations and supporting literary scientists with their analyses. According to Mani (2013), one aspect of Computational Narratology focuses on exploring and testing literary hypotheses through mining narrative structures from corpora. In the context of the BMBF-funded eHu¬¥ manities project heureCL EA, we address temporal phenomena in literary text, a genre whose temporal phenomena are different from others. For example, it is often not possible to anchor temporal expressions to real points in time, but literary texts tend to have their own time frame. Our project partners, as well as many other humanists, use CATMA, a comprehensive graphical tool for annotating data. Interfacing NLP with CATMA could drastically reduce the effort of manual annotation. The goal of ¬¥ heureCL EAis to provide users with a collaborative annotation environment for tagging temporal phenomena in documents, with simple annotations (e.g., temporal expressions) being added automatically, and more complex annotations (e.g., time shifts and ellipses) being suggested. Users can correct automatic annotations, and user feedback will be used to apply machine learning techniques to improve future annotation suggestions. In the following, we outline our flexible architecture for NLP in the domain of narrative texts, as well as promising first results for annotating the tense of sub-sentences to demonstrate the effectiveness of our approach. 2 Architecture and Components ¬¥ The heureCL EAcorpus currently consists of more than 20 mostly German narrative texts from various authors of the 20th century. Due to the diversity of style and text characteristics, applying NLP is challenging as most systems are optimized for factual texts characterized by stable structures. Automatically generating annotations that are related to temporal structures of texts requires information on multiple levels of the linguistic processing stack. Thus, we implemented a modular pipeline that performs annotations with increasing levels of complexity and allows for easy adaptation and exchange of different base components. We use standard off-the-shelf tools that are freely available. In order to achieve maximum flexibility and to allow for easily substitutable individual components, the pipeline is implemented as a UIMA architecture. The general pipeline architecture is shown in Fig. 1. We distinguish between general preprocessing components that are required for all subsequent narratological annotation tasks and individual machine learning modules (red background) that are tailored to one specific target annotation. We are planning to release the preprocessing stack to the research community to allow all CATMA users to perform basic NLP analyses and annotations. 2.1 Component Overview 2.1.1 CATMA Interface The texts in our corpus are annotated by literary scientists with CATMA, a web-based collaborative ¬¥ Figure 1: Overview of the NLP architecture in heureCL EA. annotation tool that offers humanists an easy way to create stand-off annotation and share their annotation with other scholars. In order to work with annotated data in CATMA, we implemented a component that interfaces CATMA with our UIMA pipeline. As CATMA is a popular tool in the humanities, we developed the interface as a stand-alone component that can easily be used by others to combine the strengths of CATMA as an annotation framework with the analytical and predictive power of UIMA pipelines. The interface is geared to literary scientists with no knowledge of programming. To achieve a simple configuration, the user only has to specify mappings between CATMA and UIMA types in a single XML file. By providing an easy-to-use interface, we want to lower the bar for other projects in the Humanities to employ simple yet effective NLP tools and thereby alleviate manual annotations. 2.1.2 Linguistic Processing Our linguistic preprocessing stack consists of state-of-the-art components for German NLP. For sentence segmentation and part-of-speech tagging, we use the TreeTagger (Schmid, 1995). While the tree tagger provides a basic analysis of tokens, it does not extract morphological information; thus, we added Morphisto (Piskorski, 2009) as a separate component for morphological analysis. Two different parsers in our pipeline provide valuable information for detecting sub-sentences and dependency relations between tokens (e.g., to extract the subject of a certain verb): the Stanford constituent parser (Rafferty and Manning, 2008) and ParZu (Sennrich et al., 2009), a dependency parser trained on the T¬® uBaD/Z. Finally, HeidelTime (Strötgen and Gertz, 2013) extracts and normalizes temporal expressions in the text which will be used in later stages of the processing pipeline. 2.1.3 Machine Learning After the data has been annotated by the above preprocessing components, it is passed to different modules that handle the annotation of specific narratological aspects of texts (e.g., the extraction of tense clusters of time shifts). Depending on the target annotation, we employ different machine learning approaches and heuristics. Verb tenses are an example for such a relevant annotation because shifts in the verb tense of a sentence can, for instance, indicate narratological order phenomena (e.g., prolepsis). To extract tense clusters, we implemented and evaluated a robust heuristic component with promising results. Detailed evaluation results of the prediction performance based on a comparison to manually annotated data will be presented in the poster. References Mani, I. (2013, October). Computational narratology. the living handbook of narratology. http://www. lhn.uni-hamburg.de/article/ computational-narratology. Piskorski, J. (2009). Morphisto-An Open Source Morphological Analyzer for German. Finitestate Methods and Natural Language Processing: Post-proceedings of the 7th International Workshop FSMNLP 191. Rafferty, A. N. and C. D. Manning (2008). Parsing three german treebanks: Lexicalized and unlexicalized baselines. In Proceedings of the Workshop on Parsing German, PaGe ""08, Stroudsburg, PA, USA, pp. 40–46. Association for Computational Linguistics. Schmid, H. (1995). Improvements in part-of-speech tagging with an application to german. In In Proceedings of the ACL SIGDAT-Workshop, pp. 47'50. Sennrich, R., G. Schneider, M. Volk, and M. Warin (2009). A new hybrid dependency parser for german. Proc. of the German Society for Computational Linguistics and Language Technology, 115–124. Strötgen, J. and M. Gertz (2013). Multilingual and cross-domain temporal tagging. Language Resources and Evaluation 47(2), 269‚Äî-298.",en,project overview temporal dependency reveal interesting insight semantic discourse structure narrative text investigation literary scientist today base labor intensive manual annotation computational narratology important subtopic digital humanity aim facilitate annotation support literary scientist analysis accord mani aspect computational narratology focus explore test literary hypothesis mining narrative structure corpora context bmbf fund manitie project heurecl ea address temporal phenomena literary text genre temporal phenomenon different example possible anchor temporal expression real point time literary text tend time frame project partner humanist use catma comprehensive graphical tool annotate datum interface nlp catma drastically reduce effort manual annotation goal heurecl eais provide user collaborative annotation environment tag temporal phenomena document simple annotation temporal expression add automatically complex annotation time shift ellipsis suggest user correct automatic annotation user feedback apply machine learn technique improve future annotation suggestion following outline flexible architecture nlp domain narrative text promise result annotate tense sub sentence demonstrate effectiveness approach architecture component heurecl eacorpus currently consist german narrative text author century diversity style text characteristic apply nlp challenge system optimize factual text characterize stable structure automatically generate annotation relate temporal structure text require information multiple level linguistic processing stack implement modular pipeline perform annotation increase level complexity allow easy adaptation exchange different base component use standard shelf tool freely available order achieve maximum flexibility allow easily substitutable individual component pipeline implement uima architecture general pipeline architecture show fig distinguish general preprocessing component require subsequent narratological annotation task individual machine learn module red background tailor specific target annotation plan release preprocessing stack research community allow catma user perform basic nlp analysis annotation component overview catma interface text corpus annotate literary scientist catma web base collaborative figure overview nlp architecture heurecl ea annotation tool offer humanist easy way create stand annotation share annotation scholar order work annotated datum catma implement component interface catma uima pipeline catma popular tool humanity develop interface stand component easily combine strength catma annotation framework analytical predictive power uima pipeline interface gear literary scientist knowledge programming achieve simple configuration user specify mapping catma uima type single xml file provide easy use interface want lower bar project humanity employ simple effective nlp tool alleviate manual annotation linguistic process linguistic preprocessing stack consist state art component german nlp sentence segmentation speech tagging use treetagger schmid tree tagger provide basic analysis token extract morphological information add morphisto piskorski separate component morphological analysis different parser pipeline provide valuable information detect sub sentence dependency relation tokens extract subject certain verb stanford constituent parser rafferty manning parzu sennrich et al dependency parser train ubad finally heideltime strötgen gertz extract normalize temporal expression text later stage processing pipeline machine learning datum annotate preprocesse component pass different module handle annotation specific narratological aspect text extraction tense cluster time shift depend target annotation employ different machine learn approach heuristic verb tense example relevant annotation shift verb tense sentence instance indicate narratological order phenomena prolepsis extract tense cluster implement evaluate robust heuristic component promising result detailed evaluation result prediction performance base comparison manually annotate datum present poster reference mani october computational narratology living handbook narratology computational narratology piskorski morphisto open source morphological analyzer german finitestate method natural language processing post proceeding international workshop fsmnlp rafferty manning parse german treebank lexicalize unlexicalized baseline proceeding workshop parse german page stroudsburg pa usa pp association computational linguistic schmid improvement speech tagging application german proceeding acl sigdat workshop pp sennrich schneider volk warin new hybrid dependency parser german proc german society computational linguistic language technology strötgen gertz multilingual cross domain temporal tagging language resource evaluation,"[('component', 0.246374779641115), ('annotation', 0.22484809154036386), ('temporal', 0.2186779848994567), ('catma', 0.2038378864827788), ('nlp', 0.15971303773381773), ('pipeline', 0.15678395068070955), ('german', 0.14787153608529469), ('tense', 0.14735635045653403), ('interface', 0.1368968894861295), ('narratology', 0.13590932097729175)]"
2014,DHd2014,2014_cls_metadata_extracted.csv,"Zwischen Ton und Textgenese: Digital gestützte Verfahren zur kritischen Edition von Operntexten in der ""Online-Edition der Libretti zu Mozarts Opern""","Iacopo Cividini (Stiftung Mozarteum Salzburg, Austria); Adriana De Feo  (Stiftung Mozarteum Salzburg, Austria); Franz Kelnreiter (Stiftung Mozarteum Salzburg, Austria)","Onlineedition, Digitale Edition","Onlineedition, Digitale Edition","Als Bestandteil einer multimedialen Gattung bezieht der Text einer Oper mehrere quellenkundliche, sprachwissenschaftliche und musikalische Ebenen mit ein, die in einem Druckmedium nur teilweise sinnvoll dargestellt werden können. Die Online-Edition der Libretti zu Mozarts Opern, die voraussichtlich im Januar 2014 im Rahmen der Digitalen Mozart-Edition erscheinen wird, bietet die Möglichkeit, all diese Dimensionen durch digital gestützte Verfahren nach wissenschaftlichen Kriterien synchron darzustellen. Am Beispiel der Texte zu Wolfgang Amadé Mozarts Le nozze di Figaro wird gezeigt, wie Fassungen und Varianten parallel ediert werden, um einen direkten Vergleich der verschiedenen Quellenstränge zu ermöglichen. Durch die diplomatische Übertragung der Quellen mit Markierung aller Abweichungen zu den edierten Textfassungen lässt sich darüber hinaus jede einzelne editorische Entscheidung zum ersten Mal direkt zurückverfolgen. Als Verbindungsglied zur geplanten digitalen Edition des Notentextes wird schließlich mit einer metrischen Analyse des vertonten Textes ein Instrumentarium zur Erforschung der komplexen Beziehung zwischen dem metrischen Duktus der Textvorlage und dessen musikalischer Umsetzung zur Verfügung gestellt. Ausstattung: Laptop mit Internet-Zugang und der Möglichkeit, Musik abzuspielen, Beamer. CURRICULUM VITAE Iacopo Cividini (Projektverantwortlicher), geboren 1975 in Bergamo (Italien), Studium der Musikwissenschaft und Geschichte an der Universit√† degli Studi di Pavia (Italien), an der Johannes Gutenberg-Universität Mainz und an der University of Oregon (USA); Promotion in Musikwissenschaft an der Ludwig-Maximilians-Universität München im Jahre 2005 mit einer Arbeit über die Solokonzerte von Anton√≠n Dvo_√°k. Wissenschaftlicher Mitarbeiter am DFG-Projekt Bayerisches Musiker-Lexikon Online an der LMU München von 2005 bis 2007 und an der Digitalen Mozart-Edition (DME) bei der Internationalen Stiftung Mozarteum Salzburg seit 2007. Hauptforschungsgebiete: Instrumentalmusik des 19. Jahrhunderts, Bayerische Musikgeschichte, Libretti-Forschung, Philosophie der Aufklärung. Wichtigste Publikationen: Die Solokonzerte von Anton√≠n Dvo_√°k. Eine Lösung der Konzertproblematik nach Beethoven, 2007; Aufsätze zur Musik des 18. und 19. Jahrhunderts (u.a. Mozart, Brahms, Dvo_√°k). Digitale Projekte: Bayerisches Musiker-Lexikon Online, Online-Katalog der Libretti zu Mozarts Opern, Online-Edition der Libretti zu Mozarts Opern (geplante Veröffentlichung: Januar 2014). Adriana De Feo, geboren 1980 in Salerno (Italien), studierte Kunst-, Musik- und Theaterwissenschaften an der Universität Bologna und schloss ihr Studium mit einer Arbeit über Mozart in Mailand ab. 2012 promovierte sie in Musikwissenschaft an der Universität Mozarteum Salzburg über das Thema Mozarts Serenate im Spiegel der Gattungsentwicklung. Zu Ihren Forschungsschwerpunkten zählen die Huldigungsoper des 17. und 18. Jahrhunderts und die Libretti-Forschung. Aufsätze zur Musikdramaturgie des Barock und der Klassik. Als Wissenschaftliche Mitarbeiterin der Stiftung Mozarteum Salzburg (seit 2009) erarbeitet sie im Rahmen der Digitalen Mozart-Edition (DME) die Online-Edition der Libretti und den Online-Katalog der Libretti zu Mozarts Opern. Franz Kelnreiter Studium der Musikwissenschaft und Romanistik (Französisch) an der Paris-Lodron-Universität sowie der Kirchenmusik an der Hochschule Mozarteum in Salzburg. Seit 1994 an der Internationalen Stiftung Mozarteum, zunächst als Leiter der Mozart Ton- und Filmsammlung, seit 2002 Mitarbeiter an der Digitalen Mozart-Edition und Leiter des IT-Bereichs. Technische Projektbetreuung.",de,Bestandteil multimedial Gattung beziehen Text Oper mehrere quellenkundlich sprachwissenschaftlich musikalisch Ebene Druckmedium teilweise sinnvoll darstellen Libretti Mozart opern voraussichtlich Januar Rahmen digital erscheinen bieten Möglichkeit all dimension Digital gestützt Verfahren wissenschaftlich kriterien Synchron darstellen Text Wolfgang Amadé mozarts le Nozze Di Figaro zeigen Fassung Variant parallel edieren direkt Vergleich verschieden quellenstränge ermöglichen diplomatisch Übertragung quellen Markierung Abweichung ediert textfassung lässt hinaus einzeln editorisch Entscheidung Mal direkt zurückverfolgen Verbindungsglied geplant digital Edition notentext schließlich metrisch Analyse vertont Text Instrumentarium Erforschung komplex Beziehung metrisch Duktus Textvorlage musikalisch Umsetzung Verfügung stellen Ausstattung Laptop Möglichkeit Musik abzuspielen Beamer Curriculum vitae Iacopo Cividini Projektverantwortlicher gebären Bergamo Italien Studium Musikwissenschaft Geschichte Degli Studi Di Pavia Italien Johannes Mainz university of Oregon USA Promotion Musikwissenschaft München Arbeit solokonzert k wissenschaftlich Mitarbeiter bayerisch Online Lmu München digital dme international Stiftung Mozarteum Salzburg hauptforschungsgebien Instrumentalmusik Jahrhundert bayerisch Musikgeschichte Philosophie Aufklärung wichtig publikationen solokonzert k Lösung Konzertproblematik beethov Aufsatz Musik Jahrhundert Mozart Brahms k digital Projekt bayerisch Online Libretti Mozart opern Libretti Mozart opern geplant Veröffentlichung Januar Adriana de feo gebären Salerno Italien studieren Theaterwissenschaft Universität bologna Schloss Studium Arbeit Mozart Mailand promovieren Musikwissenschaft Universität Mozarteum Salzburg Thema mozarts Serenat Spiegel Gattungsentwicklung Forschungsschwerpunkt zählen Huldigungsoper Jahrhundert Aufsatz Musikdramaturgie Barock Klassik wissenschaftlich Mitarbeiterin Stiftung Mozarteum Salzburg erarbeiten Rahmen digital dme Libretti Libretti Mozart opern Franz Kelnreiter Studium Musikwissenschaft Romanistik französisch Kirchenmusik Hochschule Mozarteum Salzburg international Stiftung Mozarteum Leiter Mozart Filmsammlung Mitarbeiter digital Leiter technisch Projektbetreuung,"[('mozart', 0.39656767066285104), ('mozarteum', 0.28326262190203644), ('libretti', 0.2500554262304135), ('musikwissenschaft', 0.22661009752162917), ('opern', 0.22661009752162917), ('salzburg', 0.2110701261440217), ('stiftung', 0.16995757314122187), ('bayerisch', 0.1583025946080163), ('studium', 0.1583025946080163), ('italien', 0.15003325573824813)]"
2014,DHd2014,2014_cls_metadata_extracted.csv,AAC-FACKEL. Das Beispiel einer digitalen Musteredition.,"Hanno Biber (Österreichische Akademie der Wissenschaften, Österreich)","Digitale Edition, Korpus, XML, OCR, PoS-tagging, Lemmatisierung, Webinterface","Digitale Edition, Korpus, XML, OCR, PoS-tagging, Lemmatisierung, Webinterface","Die AAC-FACKEL wurde unter Anwendung corpuslinguistischer und texttechnologischer Methodologien im Rahmen des an der Österreichischen Akademie der Wissenschaften gestarteten Forschungsprogrammes ""AAC-Austrian Academy Corpus"" als digitale Musteredition eines literaturgeschichtlich überaus bedeutenden Textes konzipiert und online gestellt. 1 Das AAC ist ein Textcorpus zur deutschen Sprache zwischen 1848 und 1989, mit dem philologische Grundlagenforschung im noch relativ jungen Paradigma der computergestützten Textwissenschaften geleistet werden kann. Im Folgenden sollen als exemplarischer Anwendungsfall die aus der Corpusforschung resultierende digitale Musteredition, ihr Zustandekommen und die dafür notwendigen Bedingungen einer sich mit der Sprache und mit Fragen des Sprachgebrauchs auf empirischer Textbasis befassenden Forschungsrichtung, beschrieben werden. Seit der Veröffentlichung der AAC-FACKEL, der digitalen Version der von Karl Kraus vom 1. April 1899 bis Februar 1936 in Wien herausgegeben satirischen Zeitschrift ""Die Fackel"", haben sich bis Jahresende 2013 mehr als 25.000 Benutzer auf der Website registriert, wo die bereitgestellten Daten sowohl von den von spezifischen Text- und Sprachinteressen geleiteten Wissenschaftlerinnen und Wissenschaftlern erforscht, als auch von allgemein an der Sprache und Literatur interessierten Leserinnen und Lesern aus aller Welt vielfältig genutzt werden. Das nach den für diese Edition entwickelten Prinzipien zur Funktionalität digitaler Textressourcen und von erforderlichen Überlegungen zum grafischen Design bestimmte Interface der AAC-FACKEL ermöglicht den Benutzern, die digital aufbereiteten Texte 1 AAC - Austrian Academy Corpus: AAC-FACKEL. Online Version: ""Die Fackel. Herausgeber: Karl Kraus, Wien 1899-1936"" AAC Digital Edition No. 1 (Hg. Hanno Biber, Evelyn Breiteneder, Heinrich Kabas, Karlheinz Mörth), http://www.aac.ac.at/fackel sowohl lesen, als auch in komplexer Weise ihre Formen untersuchen und analysieren, sowie einfach in ihnen nach sprachlichen Einheiten und deren Eigenschaften suchen zu können. Das Werk von Karl Kraus ist als bedeutender Beitrag der deutschsprachigen Literatur zur Weltliteratur zu betrachten und seine satirischen und polemischen Texte sind von thematischer Vielfalt, sprachlicher Komplexität und historischer Relevanz, weshalb ihre Überlieferung im digitalen Medium als unerlässlich erachtet werden kann. In der digitalen Edition der AAC-FACKEL wird die einzigartige sprachliche, literarische und satirische Qualität der Texte unter Nutzung texttechnologischer Instrumente durch verschiedene Suchmöglichkeiten und Register erschließbar gemacht. Neben der Volltextsuche und den Wortformen-Registern mit ungefähr 6 Millionen Wortformen bietet die AAC-FACKEL ein vollständiges, erstmals publiziertes Inhaltsverzeichnis sämtlicher Texte der Zeitschrift, das unter Nutzung informationstechnologischer Verfahren für die digitale Edition neu erstellt wurde. Dabei wurden sowohl die Angaben von Karl Kraus in den Überschriften der einzelnen Beiträge, bzw. von den Textanfängen und den Inhaltsangaben der Hefte, als auch jene Inhaltsverzeichnisse berücksichtigt, die vom Herausgeber nachträglich für die Quartalsbände erstellt wurden. Die vollständige Bild-Beigabe aller 22.586 Textseiten als Faksimiles ermöglicht den Nutzern die quelleneditorisch korrekte Zitierung der Texte in den 415 Heften bzw. 922 Nummern der 37 Jahrgänge der Zeitschrift. Es ist geplant, neue Funktionen wie etwa ein auf einer im AAC erstellten und bearbeiteten Namendatenbank der ""Fackel"" beruhendes Personennamenregister sowie ein Verzeichnis der Varianten der Hefte der Zeitschrift in einer neuen Version der AAC-FACKEL zu implementieren. Für die im AAC konzipierten digitalen Editionen wurde im Rahmen der texttechnologischen Forschungen ein spezifisches von Anne Burdick gestaltetes Navigationsmodul für Zeitschriften und andere Textformen entworfen, mit dessen Hilfe nicht nur von Seite zu Seite, von Heft zu Heft oder von Jahrgang zu Jahrgang navigiert werden kann, sondern auch zu im jeweiligen Zusammenhang relevanten Textpassagen. Die besondere grafische Umsetzung im Webinterface, im Bereich des Inhaltsverzeichnisses, kompensiert in diesen Fällen das Fehlen der ertastbaren physischen Objekteigenschaften und visuellen Informationen, die sonst nur durch die Wahrnehmung der Präsenz der gedruckten Zeitschrift gegeben sind. Die digitale Edition bietet auf diese Weise eine optisch wahrzunehmende Repräsentation der sonst in den von Papierqualität, -volumen, Druck- und Bindungstechnik bestimmten Eigenschaften eines Druckwerkes. In der AAC-FACKEL ist ein linguistisches Suchmodul eingerichtet den an den Texten in besonderer Weise interessierten Lesern ermöglicht, corpusbasierte Abfragen vorzunehmen und die Basisfunktionalität von mit linguistischen Tags versehenen Ressourcen zu nutzen und so die mit Part-of-speech- und Lemma-Informationen versehenen Text zu untersuchen. Die im AAC erstellten Prototypen geben Antwort auf die Frage, wie komplex organisierte Texte und historische literarische Zeitschriften mit einem Inventar von literarischen Formen und sprachlichen Eigenschaften in einer adäquaten und funktionellen Form im digitalen Medium so wiedergegeben werden, dass sie unter Nutzung der texttechnologischen Möglichkeiten vom wissenschaftlichen Nutzer wie auch vom interessierten Laien im neuen Medium, in das die Texte gleichsam übersetzt werden müssen, neu gelesen, interpretiert und analysiert werden können. Die zentrale Forschungsperspektive neben der Fragestellung nach den Steuerungsvorgängen und der damit verbundenen Reinterpretation und adäquaten Präsentation der Ausgangsmaterialien derartiger digitaler Editionen (scan-images, OCR-text, xml-tags, linguistic tagging, structural tagging, database-organisation etc.) liegt in der optimalen Nutzung der durch die informationstechnologische Aufbereitung der Texte gegebenen Such- und Indizierungsverfahren sowie den verschiedenen, dadurch eröffneten Zugangsmöglichkeiten zum Text sowie zu einzelnen Elementen der sprachlich, textlich und durch beschriebene Textstrukturen der Publikationsobjekte organisierten Bestände. Eine dritte zentrale corpusrelevante Forschungsperspektive liegt in der methodisch-theoretischen Reflexion über die sich durch die Kombination von editionsphilologischen Fragestellungen mit Fragen der Informationstechnologie, der Text-Technologie und der Corpus-Forschung sowie dem Interface Design sich ergebenden Konsequenzen und Forschungsansätze.",de,Anwendung corpuslinguistisch texttechnologisch Methodologie Rahmen österreichisch Akademie Wissenschaft gestartet forschungsprogrammes Academy Corpus digital Musteredition literaturgeschichtlich überaus bedeutend Text konzipieren Online stellen Aac Textcorpus deutsch Sprache philologisch Grundlagenforschung relativ jung Paradigma Computergestützten Textwissenschaft leisten folgend exemplarisch Anwendungsfall Corpusforschung resultierend digital Musteredition zustandekommen notwendig Bedingung Sprache Frage Sprachgebrauch empirisch Textbasis befassend Forschungsrichtung beschreiben Veröffentlichung digital Version Karl Kraus April Februar Wien herausgeben satirisch Zeitschrift Fackel Jahresende Benutzer Website registrieren bereitgestellt daten sowohl spezifisch Sprachinteresse geleitet wissenschaftlerinn Wissenschaftler erforschen allgemein Sprache Literatur interessieren leserinnen Leser Welt vielfältig nutzen Edition entwickelt prinzipien Funktionalität digitaler Textressourc erforderlich Überlegung grafisch Design bestimmt Interface ermöglichen benutzern Digital aufbereitet Text aac austrian Academy Corpus Online Version Fackel Herausgeber Karl Kraus Wien aac Digital Edition no hg Hanno biber Evelyn breitened heinrich Kabas Karlheinz Mörth sowohl lesen komplex Weise Form untersuchen analysieren einfach sprachlich Einheit eigenschaft suchen Werk Karl Kraus bedeutend Beitrag deutschsprachig Literatur Weltliteratur betrachten satirisch polemisch Text Thematischer Vielfalt sprachlich Komplexität historisch Relevanz weshalb Überlieferung digital Medium unerlässlich erachten digital Edition einzigartig sprachlich literarisch satirisch Qualität Text Nutzung texttechnologisch Instrument verschieden Suchmöglichkeit Register erschließbar volltextsuch ungefähr Million Wortform bieten vollständig erstmals publiziert Inhaltsverzeichnis sämtlich Text Zeitschrift Nutzung informationstechnologisch Verfahren digital Edition neu erstellen sowohl Angabe Karl Kraus überschriften einzeln Beitrag Textanfäng inhaltsangaben hefen inhaltsverzeichnisse berücksichtigen Herausgeber nachträglich Quartalsbände erstellen vollständig Textseite faksimile ermöglichen Nutzer quelleneditorisch korrekt Zitierung Text heft Nummer jahrgänge Zeitschrift planen Funktion aac erstellt Bearbeitet Namendatenbank Fackel beruhend Personennamenregister Verzeichnis Variant hefte Zeitschrift Version implementieren aac konzipiert Digital editionen Rahmen texttechnologisch Forschung spezifisch Anne Burdick gestaltet Navigationsmodul zeitschrift Textform entwerfen Hilfe Seite Seite Heft Heft Jahrgang Jahrgang navigieren jeweilig Zusammenhang relevanten Textpassage besonderer grafisch Umsetzung Webinterface Bereich Inhaltsverzeichnisse kompensieren Fall Fehlen ertastbaren physisch objekteigenschaften visuell Information Wahrnehmung Präsenz gedruckt Zeitschrift geben digital Edition bieten Weise optisch wahrzunehmend Repräsentation Papierqualität Bindungstechnik bestimmt Eigenschaft Druckwerke linguistisch Suchmodul einrichten Text besonderer Weise interessiert Leser ermöglichen corpusbasiert abfrag vornehmen Basisfunktionalität linguistisch tags versehen Ressource nutzen versehen Text untersuchen aac erstellt Prototyp geben Antwort Frage komplex organisiert Text historisch literarisch Zeitschrift inventar literarisch Form Sprachliche Eigenschaft adäquat funktionell Form digital Medium wiedergeben Nutzung texttechnologisch Möglichkeit wissenschaftlich Nutzer interessiert Laie Medium Text gleichsam übersetzen neu lesen interpretieren analysieren zentral Forschungsperspektive Fragestellung steuerungsvorgäng verbunden Reinterpretation adäquat Präsentation Ausgangsmateriali derartig Digitaler edition Linguistic Tagging Structural Tagging liegen optimal Nutzung informationstechnologisch Aufbereitung Text gegeben indizierungsverfahren verschieden eröffnet zugangsmöglichkeiten Text einzeln Element Sprachlich Textlich beschrieben Textstruktur Publikationsobjekt organisiert Beständ zentral Corpusrelevante Forschungsperspektive liegen Reflexion Kombination editionsphilologisch Fragestellung Frage Informationstechnologie Interface Design ergebend Konsequenz Forschungsansätz,"[('aac', 0.294386301034667), ('zeitschrift', 0.24738228574302457), ('texttechnologisch', 0.2107069329839), ('kraus', 0.18600552230718975), ('karl', 0.16606271681558143), ('edition', 0.16009124501255864), ('fackel', 0.158030199737925), ('digital', 0.1458061869462558), ('heft', 0.13950414173039233), ('satirisch', 0.13950414173039233)]"
2014,DHd2014,2014_cls_metadata_extracted.csv,Eine digitale Ausgabe des Welschen Gastes als Chance,Jakub Simek,"stylometrische Analyse, Visualisierungen, Netzwerk, XML/TEI","stylometrische Analyse, Visualisierungen, Netzwerk, XML/TEI","Eine digitale Ausgabe des ""Welschen Gastes"" als Chance für neue Analyse- und Visualisierungsmethoden Die seit 2011 im Rahmen des Heidelberger Sonderforschungsbereichs 933 ""Materiale Textkulturen"" entstehende digitale Neuausgabe des mittelhochdeutschen Text-Bild-Gedichts ""Welscher Gast"" Thomasins von Zerklaere kodiert Volltranskriptionen der Handschriften und editorisch hergestellte Texte im XML/TEI-Format und legt damit verknüpfte Bildannotationen in einer relationalen Datenbank ab. Das Ziel des Projekts sind eine Online-Edition des gesamten textuellen und bildlichen Materials, welche die komplexen Text-Bild-Beziehungen im Werk dokumentiert und mit mächtigen Visualisierungsmechanismen anschaulich präsentiert, sowie mehrere Buchausgaben mit jeweils abgestufter Komplexität und unterschiedlichem Zielpublikum. Die Text- und Bilddaten sollen leicht vergleichbar und durchsuchbar gemacht werden. Für die stemmatologische Analyse der transkribierten, lemmatisierten und alinierten Textdaten machen wir uns phylogenetische Software aus der Bioinformatik zunutze, die es erlaubt, mutmaßliche Verwandschaftsbeziehungen zwischen den überlieferten Handschriften in gewurzelten Baumgraphen und ungewurzelten Netzwerken darzustellen. Die so gewonnenen Erkenntnisse setzen wir in der digitalen Textausgabe um, um nicht nur herkömmliche Synopsen zu generieren, sondern auch neue Formen des kritischen Apparats zu erproben. Eine Funktion, die wir als ""Baumapparat"" bezeichnen, wird dem Benutzer, der entweder den kritisch hergestellten Text oder eine Handschriftentranskription liest, beim Überfahren eines Wortes mit der Maus ein dynamisch generiertes Baumdiagramm zeigen, das dem mutmaßlichen Handschriftenstemma für die jeweilige Textstelle entspricht und an dessen ""Östen"" die Lesarten fürs jeweils ausgewählte Wort sichtbar gemacht werden. Durch farbige Unterlegung wird den angezeigten Lesarten zudem deren semantische Relevanz zugewiesen. Auf diese Weise wird der Benutzer die angezeigten Lesarten unmittelbar und intuitiv ins Stemma einordnen können. Einen anderen Visualisierungsansatz erproben wir bei der stilometrischen Wortschatz- und Reimanalyse. Die im Text am häufigsten vorkommenden Wort- und Reimkombinationen stellen wir in einem Netzwerk dar, das einen schnellen Eindruck über die sprachlichen Eigenschaften des Werkes vermittelt. Die in den Handschriften enthaltenen Miniaturzeichnungen und -malereien bilden einen festen Bildzyklus mit einem Bestand von etwa 120 Motiven. Diese Illustrationen enthalten in der Regel allegorische Figuren, die mit Beischriften und Spruchbändern versehen sind. Die Auszeichnung der Bildzonen mit Figuren, Beischriften und Spruchbändern erfolgt in einem browserbasierten graphischen Bildeditor, der die gewonnenen Koordinaten in einer relationalen Datenbank speichert. Die TEI-konforme Transkription der Texte in den Bildern wird ebenfalls in dieser Datenbank abgelegt, soll aber später zu Archivierungszwecken als TEIDokument exportiert werden. Die verschiedenen Realisierungen einzelner Motive werden in der Datenbank diesen abstrakten Motiven zugeordnet. Ein entsprechendes Alignement erfolgt ebenfalls auf der Ebene der Bild- bzw. Motivkomponenten. Somit werden Visualisierungen möglich, die verschiedene Varianten desselben Motivs nebeneinander stellen und etwa beim Überfahren einer Bildkomponente (z.B. eines Spruchbands) mit der Maus deren jeweilige Pendants in anderen Handschriften graphisch hervorheben und die darin enthaltenen Texte anzeigen. Die geplante Text-Bild-Ausgabe soll neben verschiedenen Möglichkeiten der Textdarstellung (Synopsen, dynamische Apparate, flexibler Grad der sprachlichen Normalisierung, Verlinkung mit digitalen Wörterbüchern) und der skizzierten Präsentation von Illustrationen insbesondere das Zusammenspiel von Text und Bild in den Handschriften des ""Welschen Gastes"" analysieren, dokumentieren und visuell erlebbar machen. Zu diesem Zweck werden inhaltliche und physische Bezüge zwischen einzelnen Textpassagen und den dazugehörigen Illustrationen festgehalten. Dadurch soll es möglich sein, von der Textedition schnell zum entsprechenden Bildmaterial zu gelangen und umgekehrt. Selbstverständlich wird es auch möglich sein, Texte und Bilder im Hinblick auf ihre Platzierung auf einer Handschriftenseite zu betrachten. Ein für die zweite Projektphase (ab 2015) geplanter Text- und Bildkommentar soll mit der Online-Ausgabe dynamisch verknüpft werden. Es ist unser erklärtes Ziel, die digitale Text-Bild-Ausgabe der Öffentlichkeit im Open Access verfügbar zu machen und in Zusammenarbeit mit langfristig finanzierten öffentlichen Institutionen (z.B. UB Heidelberg) für deren dauerhafte Zugänglichkeit und Archivierung zu sorgen.",de,digital Ausgabe welsch Gast Chance visualisierungsmethoden Rahmen Heidelberger Sonderforschungsbereichs material textkulturen entstehend digital Neuausgabe mittelhochdeutsch welscher Gast Thomasin zerklaerer kodieren volltranskriptionen handschrift editorisch hergestellt Text xml legen verknüpft Bildannotation relational Datenbank Ziel Projekt gesamt Textuell bildlich Material komplex Werk dokumentieren mächtig Visualisierungsmechanism anschaulich präsentieren mehrere buchausgabe jeweils abgestuft Komplexität unterschiedlich Zielpublikum bilddaten vergleichbar durchsuchbar stemmatologisch Analyse Transkribiert lemmatisiert aliniert Textdat phylogenetisch Software Bioinformatik zunutze erlauben mutmaßlich Verwandschaftsbeziehung überlieferten Handschrift gewurzelt baumgraph ungewurzelt netzwerken darstellen gewonnen erkenntnis setzen digital Textausgabe herkömmlich synopsen generieren Form kritisch Apparat erproben Funktion Baumapparat bezeichnen Benutzer kritisch hergestellt Text Handschriftentranskription lesen überfahren Wort Maus dynamisch generiert Baumdiagramm zeigen mutmaßlich Handschriftenstemma jeweilig Textstelle entsprechen ösen Lesart für jeweils ausgewählt Wort sichtbar farbig Unterlegung angezeigten Lesarte zudem semantisch Relevanz zuweisen Weise Benutzer angezeigt lesaren unmittelbar intuitiv Stemma einordnen Visualisierungsansatz erproben stilometrisch Reimanalyse Text häufig vorkommend reimkombinationen stellen Netzwerk dar schnell Eindruck sprachlich Eigenschaft Werk vermitteln handschrift enthalten Miniaturzeichnung bilden fest Bildzyklus Bestand Motiv Illustration enthalten Regel allegorisch Figur beischrift Spruchbänder versehen Auszeichnung Bildzon Figur Beischrift spruchbändern erfolgen browserbasiert graphisch Bildeditor gewonnen Koordinate relational datenbank speichern Transkription Text bildern ebenfalls Datenbank ablegen archivierungszwecken Teidokument exportieren verschieden Realisierung einzeln Motiv datenbank abstrakt Motiven zuordnen entsprechend Alignement erfolgen ebenfalls Ebene Motivkomponent somit visualisierung verschieden Variant Motivs nebeneinander stellen überfahren bildkomponent Spruchband Maus jeweilig Pendants handschrift Graphisch hervorheben enthalten Text anzeigen geplant verschieden Möglichkeit Textdarstellung synopsen dynamisch Apparat flexibel Grad sprachlich Normalisierung Verlinkung digital wörterbüchern skizziert Präsentation Illustration insbesondere Zusammenspiel Text Bild handschriften welsch Gast analysieren dokumentieren visuell erlebbar zweck inhaltlich physisch bezüge einzeln Textpassagen dazugehörig illustrationen festhalten Textedition schnell entsprechend Bildmaterial gelangen umgekehrt selbstverständlich Text Bild Hinblick Platzierung Handschriftenseite betrachten Projektphase geplant Bildkommentar dynamisch verknüpfen erklärt Ziel digital Öffentlichkeit op access verfügbar Zusammenarbeit langfristig finanziert öffentlich Institution ub Heidelberg dauerhaft Zugänglichkeit Archivierung sorgen,"[('gast', 0.1947938807407309), ('datenbank', 0.19057109280506188), ('handschrift', 0.18382229839978512), ('überfahren', 0.1394236791272349), ('welsch', 0.1394236791272349), ('synopsen', 0.1394236791272349), ('beischrift', 0.1394236791272349), ('dynamisch', 0.1276911697377531), ('maus', 0.12307888445243009), ('hergestellt', 0.12307888445243009)]"
2014,DHd2014,2014_cls_metadata_extracted.csv,Für eine computergestützte literarische Gattungsstilistik,"Christof Schöch (Lehrstuhl für Computerphilologie);  Steffen Pielström (Universität Würzburg, Germany)","computergestützte literarische Gattungsstilistik, Principal Component Analysis, quantitative Methoden, Maschinelles Lernen, Text Mining, Support Vector Machine, computergestützte Stilistik, Clustering, Cluster Analysis, Varianzanalyse","computergestützte literarische Gattungsstilistik, Principal Component Analysis, quantitative Methoden, Maschinelles Lernen, Text Mining, Support Vector Machine, computergestützte Stilistik, Clustering, Cluster Analysis, Varianzanalyse","Einleitung Der vorliegende Beitrag plädiert für eine computergestützte literarische Gattungsstilistik, verstanden als eine Forschungsagenda für die Literaturwissenschaften, welche hermeneutische und quantitative Methoden verbindet. Diese Agenda wird im Zusammenhang mit einem in Vorbereitung befindlichen Forschungsprojekt zum gleichen Thema formuliert, das in der romanistischen Literaturwissenschaft angesiedelt ist. Aus diesem Forschungsprojekt werden zwei Zwischenergebnisse berichtet: das Erste betrifft die konzeptuelle Verknüpfung von Gattungstheorie und computergestützter Stilistik; das Zweite betrifft die methodische Erweiterung der Principal Component Analysis (PCA) für literaturwissenschaftliche Fragestellungen. 1. Die Agenda der computergesützten literarischen Gattungsstilistik Die übergeordnete Zielsetzung einer computergestützten literarischen Gattungsstilistk ist es, eine tiefgehende Konvergenz herzustellen zwischen etablierten literaturwissenschaftichen Fragestellungen einerseits und quantitativen Verfahren der Textanalyse andererseits. Eine solche Konvergenz ist Voraussetzung dafür, dass sich entsprechende Forschungsvorhaben im Kernbereich der Digital Humanities ansiedeln können, in dem computergestützte Geisteswissenschaften und Angewandte Informatik nicht nebeneinander stehen, sondern sich zu einem neuen, dritten Forschungsparadigma verbinden. Auf eine computergestützte literarische Gattungsstilistik bezogen ergeben sich daraus eine Reihe von Forschungsfragen. Einige von ihnen sind primär literaturwissenschaftlich: Wie kann die Beziehung zwischen Stil und Gattung in einer produktiven Weise konzeptualisiert werden? Wie verhalten sich Gattungsstile, Epochenstile und Autorenstile zueinander? Welche anderen Faktoren spielen für die stilistische Beschreibung literarische Texte eine Rolle? Welche automatisch identifizierbaren sprachlichen Merkmale, auf welchen Ebenen der linguistischen Beschreibung, sind Indikatoren für Gattungen? Andere Fragestellungen 1 stammen aus dem informatischen Bereich des Text Mining: Welche Verfahren der Text_Kategorisierung und des Maschinellen Lernens können eingesetzt und angepasst werden? Wie können für Verfahren wie Support Vector Machines die besten kernels definiert und geeignete features modelliert werden? Aus der Verbindung von literaturwissenschaftlicher und informatischer Fragestellungen ergeben sich aber auch ganz neue Fragen, die dem spezifischen Bereich der Digital Humanities zuzurechnen sind: Welche Besonderheiten natürlichsprachlicher und spezifisch literarischer Daten sind zu berücksichtigen, wenn es darum geht, möglichst generische Strategien zur Trennung von Autoren_ Epochen und Gattungssignal zu entwickeln? Wie können computergestützte Verfahren so weiterentwickelt werden, dass sie einerseits auch für literatursprachliche Daten statistisch signifikant, robust und verlässlich sind, dass sie andererseits aber auch aus hermeneutischer Perspektive transparent und interpretierbar, das heißt aus literaturwissenschaftlicher Sicht bedeutungsvoll sind? Und allgemeiner, wie verändert die computergestützte Herangehensweise die Weise, wie wir über literarische Interpretation und algorithmische Analyse sowie ihre wechselseitige Beziehung nachdenken? Die Bearbeitung dieser Forschungsfragen bildet den Kern der Forschungsagenda einer computergestützten literarischen Gattungsstilistik. Zu zwei dieser Teilfragen werden hier Zwischenergebnisse berichtet. 2. Die konzeptuelle Verknüpfung von Gattungstheorie und quantitativer Stilistik Das erste Zwischenergebnis bezieht sich auf die konzeptuelle Verknüpfung von Gattungstheorie und computergestützter Stilistik. Der Gattungsstilistik geht es um einen induktiven, deskriptiven Blick auf die stilistischen Merkmale literarischer Gattungen und Untergattungen sowie auf deren historische Entwicklung. In der neueren Gattungstheorie hat sich die Auffassung durchgesetzt, dass literarische Gattungen sich nicht mit einem idealistischen, deduktiven Ansatz systematisieren lassen (Schaeffer 1989). Vielmehr sind sie als historische Konventionen zu verstehen, die komplexe und sich dynamisch entwickelnde ""generic facets"" (Kessler et al. 1998) umfassen. Diese beziehen sich zu unterschiedlichen Anteilen auf Themen, Plot und diverse stilistische Merkmale (Hoffmann 2009). Die Kombination mehrerer solcher ""facets"" definiert eine Gattung oder Untergattung, und Übergangsformen oder diachrone Entwicklungen lassen sich über den Wegfall oder das Hinzutreten einzelner ""facets"" erfassen. In ähnlicher Weise wird Stil heute als ein Phänomen aufgefasst, das als ""Bündel konkurrierender Merkmale"" auf unterschiedlichen linguistischen Beschreibungsebenen (Phonologie, Morphologie, Lexik/Semantik, Syntax, Plot, etc.) verstanden werden kann 2 1 (Sandig 2006, Karlgren & Cutting 1994). Hier kann die computergestützte Stilistik ansetzen, denn sie ist in der Lage, induktiv und umfassend zahlreiche Merkmale _ in ihrer gegenseitigen Abhängigkeit, in ihrer jeweiligen Gewichtung, und unter präziser Berücksichtigung zahlreicher möglicherweise relevanter Faktoren _ zu erfassen und für die Klassifikation oder das Clustering von Texten zu nutzen. Damit wird die von Dominique Combe eingeforderte ""stylistique des genres"" (Combe 2002) computergestützt realisiert. Abb. 1 fasst das Verhältnis zwischen der Theorie literarischer Gattungen und computergestützter Stilistik / Text Mining zusammen. Abb. 1: Literarische Gattungstheorie und computergestützte Stilistik Durch die vergleichbare Konzeption von Gattungen (mit Facetten) und Stil (mit Merkmalen) können Verbindungen zwischen historisch oder theoretisch gegebenen Untergattungen einerseits und auf der Grundlage stilistischer Öhnlichkeit gruppierten Clustern von Texten andererseits entdeckt werden. Genauer gesagt: es können in ihrer Stärke statistisch charakterisierbare Korrelationen zwischen einzelnen Gattungsfacetten und stilistischen Merkmalen erhoben und eingeordnet werden. Durch die Identifikation von besonders distinktiven Merkmalen und durch Merkmalsgeneralisation können dann auch 1 Im Bereich der Corpuslinguistik geht die computergestützte Untersuchung der stilistischen Unterschiede von (literarischen) Gattungen und Untergattungen bis in die 1980er_Jahre zurück, mit Pionierarbeiten von Douglas Biber zur Modellierung des Zusammenhangs zwischen funktionalen Gattungsaspekten und stilistischen Merkmalen, die zu synthetischen Dimensionen zusammengefasst werden (Biber 1992) und der Erprobung einer breiten Auswahl von potentiellen ""style markers"" (Karlgren & Cutting 1994). Außerdem wurden bspw. die vergleichende Evaluation von token_basierten, syntaktischen und anderen Merkmalen vorgenommen (Stamatatos et al. 2000). 3 Merkmalsbündel ermittelt werden, die zugleich statistisch signifikant mit einer Facette korellieren und aus literaturwissenschaftlicher Perspektive interpretierbar sind. 3. Die methodische Erweiterung der Principal Component Analysis Die computergestützte literarische Gattungsstilistik ist Teil einer sich aktuell verstärkenden Tendenz, stilometrische Fragen über die traditionell im Vordergrund stehende 2 Autor_Attribution hinaus zu bearbeiten. Zahlreiche wohl etablierte Methoden (wie bspw. Cluster Analysis oder Principal Component Analysis), aber auch neuere informatischen Verfahren aus dem Bereich des Text Mining und Machine Learning sind für eine so konzipierte Gattungsstilistik anschlussfähig. Wir schlagen hier vor diesem Hintergrund vor, die etablierte Methode der Principal Component Analysis (PCA, siehe grundlegend Jackson 2005) auf eine Weise zu erweitern, die ihre Interpretierbarkeit erhöht. Zur verlässlichen Unterscheidung von Kategorien oder Gruppen innerhalb einer großen Menge von Texten ist die Stilometrie auf die gleichzeitige Betrachtung einer großen Zahl von Merkmalen (bspw. Worten) angewiesen. Fasst man jedes Merkmal mehrerer Texte als je eine Dimensionen auf, beruht die stilometrische Erhebung von Öhnlichkeitsrelationen zwischen Texten daher häufig auf einer Dimensionsreduktion. Anders als bspw. Burrows' Delta (Burrows 2002) erlaubt PCA die Reduktion der Dimensionen eines Datensatzes nicht auf nur eine einzige, sondern auf wenige neue und voneinander unabhängige Dimensionen, die sog. principal components (PC), die die Varianz in den Daten besonders gut beschreiben. Jede dieser PCs korreliert hierbei mit einer spezifischen Kombination bzw. Gewichtung von Merkmalsfrequenzen. Charakteristisch für die PCA ist, daß bereits die ersten 2_4 PCs oft einen großen Teil der im Datensatz enthaltenen Varianz beschreiben (Abb. 2a). Für eine graphische Exploration der Öhnlichkeit zweier oder mehrerer Gruppen kann es also ausreichen, die ersten PCs als Koordinaten zu verwenden, anstatt in einer großen Zahl von Wortfrequenzen eine Kombination zu suchen, die Unterschiede besonders deutlich macht (Abb. 2b). 2 Im Bereich der literarischen Gattungsstilistik liegen mehrere Ansätze vor, welche die diachrone Entwicklung von Gattungen allgemein (Moretti 2005, Jockers 2013) oder spezifische methodische Lösungsversuche betreffen: bspw. Cluster Analyse oder ""unmasking"" _Prozedur für die Gattungsklassifikation (Allison et al. 2011; Kestemont et al. 2012; Schöch 2013). 4 Abb. 2: Die Principal Component Analysis am Beispiel von 141 französischen Dramen (Tragödien, Komödien, Tragikomödien, Andere) aus dem siebzehnten Jahrhundert, basierend auf den relativen Häufigkeiten der 200 am meisten verwendeten Wörter. a) Scree Plot mit den Varianzanteilen für PC 1_10.; b) Scatterplot Matrix für PC 1_4 mit gattunsspezifischer Farbcodierung. Die PCA erlaubt allerdings von sich aus keine Aussagen über die Unterschiedlichkeit zweier Kategorien von Datenpunkten, oder über den Einfluß einer bestimmten Gruppierungsvariable, weshalb stilistische Unterschiede zwischen Gattungen mit dieser Methode zwar visualisiert, aber nicht analysiert werden können. Um diese Lücke zu überbrücken, haben wir ein Verfahren entwickelt, mit dem der Einfluß der Zugehörigkeit eines bestimmten Textes zu einer spezifischen Kategorie (bspw. der Gattung) auf die PCs mittels der Varianzanalyse (ANOVA) untersucht werden kann. Hierbei wird die Gattung als unabhängige faktorielle Variable betrachtet, die Werte einer bestimmten PC als abhängige Variable. Das Bestimmtheitsmaß R_ liefert hierbei eine Vergleichsgröße, anhand derer sich die Einflüße verschiedener Faktoren (bspw. Gattung, aber auch Autorschaft oder auch Publikationsdatum) auf eine bestimmte PC quantifizieren lassen (Abb 3a). 5 Abb. 3: Von der PCA zur Kategorie. a) Ergebnisse der Varianzanalysen (ANOVAs) zur Quantifizierung des Einflusses verschiedener Faktoren auf die PCs 1_4. Untersucht wurden die Faktoren Form (d.h. Vers, Prosa oder Gemischt), Gattung (Tragödie, Komödie, Tragikomödie) und Autor. Dargestellt sind die Bestimmtheitsmaße (R_) für jeden dieser Faktoren bei jeder PC. Symbole über den Balken repräsentieren das Signifikanzniveau der jeweiligen Beziehung (‚Äòn.s. ‚Äô nicht signifikant; ‚Äò*‚Äô p < 0.05; ‚Äò**‚Äô p < 0.01; ‚Äò***‚Äô p < 0.001). Den stärksten Einfluß hat die literarische Gattung in diesem Datensatz auf PC1 und PC3; b) Loading_Werte der 20 häufigsten Worte für PC 1_4. Diese Werte repräsentieren den Einfluß einzelner Wortfrequenzen auf die PCs, und erlauben so in Kombination mit den Resultaten der Varianzanalysen interpretierende Rückschlüsse. Die Kombination von PCA und ANOVA erlaubt somit die Identifikation von PCs, die durch einen bestimmten Faktor besonders stark geprägt werden, und den Abgleich mit der Gewichtung bestimmter Wortfrequenzen in dieser PC (Abb. 3b). Gemeinsam betrachtet erlaubt dies Rückschlüsse darüber, welche Merkmale und Merkmalsbündel für die Unterschiede in bestimmten Faktoren besonders relevant sind, d.h. auch welche Kombinationen charakteristisch für Gattungsunterschiede sind. 6 Fazit Mit der konzeptuellen Verbindung von Gattungstheorie und computergestützter Stilistik einerseits, und der methodischen Erweiterung der PCA zur verbesserten Interpretierbarkeit von PCs in Bezug auf relevante Kategorien andererseits, konnten wichtige Zwischenziele erreicht und Grundlagen für die eingangs beschriebene Forschungsagenda gelegt werden. Und es konnte exemplarisch gezeigt werden, so hoffen wir, wie die hier vertretene Forschungsagenda einer computergestützten literarischen Gattungsstilistik hermeneutische und quantitative Methoden zu einem eigenständigen Ansatz verbindet, der auf die Entwicklung spezifisch erweiterter informatischer Verfahren für ein erweitertes Verständnis der Natur und der Entwicklung literarischer Gattungen abzielt. Literaturangaben Allison, Sarah, Ryan Heuser, Matthew L. Jockers, Franco Moretti, and Michael Witmore (2011). Quantitative Formalism: An Experiment. Stanford: Stanford Literary Lab. Biber, Douglas (1992). Computers in the Humanities, 26.5_6, 331_347. ""The multidimensional approach to linguistic analyses of genre variation"" , in: Burrows, John (2002). Linguistic Computing 17.3, 267_287. ""Delta: A Measure of Stylistic Difference and a Guide to Likely Authorship"" . Literary and Combe, Dominique (2002). ""La stylistique des genres"" , in: Langue fran√ßaise 135, 33_49. Hoffmann, Michael (2009). ""Mikro_ und makrostilistische Einheiten im Überblick‚Äù , in: Rhetorik und Stilistik, Ein internationales Handbuch historischer und systematischer Forschung, Band 2, hg. von Ulla Fix, Andreas Gardt & Joachim Knape. Band 2, Berlin: de Gruyter, 1529_45. Jackson, Edward (2005). A User's Guide to Principal Components. New York: Wiley. Jockers, Matthew (2013). Macroanalysis. Digital Methods and Literary History. Chicago: Univ, of Illinois Press. Juola, Patrick (2006). ""Authorship Attribution. ‚Äù Foundations and Trends in Information Retrieval 1.3, 233_334. Karlgren, Jussi & Douglas Cutting (1994). analysis‚Äù . In Proceedings of COLING '94, Vol. 2, 1071_1075. ""Recognizing text genres with simple metrics using discriminant Kessler, Brett, Geoffrey Numberg, and Hinrich Schütze (1998). ""Automatic Detection of Text Genre. ‚Äù In Proceedings of ACL 1998, 32_38. doi:10.3115/976909.979622. Kestemont, Mike, Kim Luyckx, Walter Daelemans, and Thomas Crombez (2012). Verification Using Unmasking. ‚Äù English Studies 93.3, 340‚Äì356. ""Cross_Genre Authorship Moretti, Franco (2005). Graphs, Maps, Trees: Abstract Models for a Literary History. London: Verso. Sandig, Barbara (2006). Textstilistik des Deutschen. 2. Auflage. Berlin: de Gruyter. Schaeffer, Jean_Marie (1989). Qu‚Äôest_ce qu‚Äôun genre litt√©raire? Paris: Seuil, 1989. Schöch, Christof (2013). ""Fine_tuning Our Stylometric Tools: Investigating Authorship and Genre in French Classical Theater‚Äù , Digital Humanities Conference 2013, http://dh2013.unl.edu/abstracts/ab_270.html. Stamatatos, Efstathios, Nikos Fakotakis, and George Kokkinakis (2000). Terms of Genre and Author. ""Automatic Text Categorization in Computational Linguistics 26/4, 471_497.",de,Einleitung vorliegend Beitrag plädieren computergestützt literarisch Gattungsstilistik verstehen Forschungsagenda Literaturwissenschaften hermeneutisch quantitativ Methode verbinden Agenda Zusammenhang Vorbereitung befindlich Forschungsprojekt gleich Thema formulieren romanistisch Literaturwissenschaft ansiedeln Forschungsprojekt zwischenergebnisse berichten betreffen konzeptuell Verknüpfung Gattungstheorie Computergestützter Stilistik betreffen methodisch Erweiterung Principal Component Analysis pca literaturwissenschaftlich Fragestellung Agenda computergesützt literarisch Gattungsstilistik übergeordnet Zielsetzung computergestützt literarisch Gattungsstilistk tiefgehend Konvergenz herstellen etabliert literaturwissenschaftich Fragestellung einerseits quantitativ Verfahren Textanalyse andererseits Konvergenz Voraussetzung entsprechend Forschungsvorhaben Kernbereich Digital Humanitie ansiedeln computergestützt geisteswissenschaft angewandte Informatik nebeneinander stehen Forschungsparadigma verbinden computergestützt literarisch Gattungsstilistik beziehen ergeben Reihe Forschungsfrag primär literaturwissenschaftlich Beziehung Stil Gattung produktiv Weise konzeptualisieren verhalten gattungsstilen epochenstil autorenstil zueinander Faktor spielen stilistisch Beschreibung literarisch Text Rolle automatisch identifizierbaren sprachlich Merkmal Ebene linguistisch Beschreibung indikatoren gattung Fragestellung stammen informatisch Bereich Text Mining Verfahren maschinell lernen einsetzen angepasst Verfahren Support vector machines Kernel definieren geeignet Feature modellieren Verbindung literaturwissenschaftlich informatisch Fragestellung ergeben Frage spezifisch Bereich Digital Humanitie zurechnen Besonderheit natürlichsprachlicher spezifisch literarisch daten berücksichtigen möglichst generisch strategin Trennung Autor epoch gattungssignal entwickeln computergestützt Verfahren weiterentwickeln einerseits literatursprachlich daten statistisch signifikant Robust Verlässlich andererseits hermeneutisch Perspektive Transparent interpretierbar literaturwissenschaftlich Sicht bedeutungsvoll allgemein verändern computergestützt Herangehensweise Weise literarisch Interpretation algorithmisch Analyse wechselseitig Beziehung nachdenken Bearbeitung Forschungsfrag bilden Kern Forschungsagenda computergestützt literarisch Gattungsstilistik Teilfrage zwischenergebnisse berichten konzeptuell Verknüpfung Gattungstheorie quantitativ Stilistik Zwischenergebnis beziehen konzeptuell Verknüpfung Gattungstheorie Computergestützter Stilistik Gattungsstilistik induktiv deskriptiv Blick stilistisch merkmal literarisch gattung untergattung historisch Entwicklung neu Gattungstheorie Auffassung durchsetzen literarisch gattungen idealistisch deduktiv Ansatz systematisieren lassen Schaeffer vielmehr historisch Konvention verstehen komplex dynamisch entwickelnd Generic facets Kessler et umfassen beziehen unterschiedlich Anteil Thema Plot diverser stilistisch Merkmal Hoffmann Kombination mehrere facets definieren Gattung Untergattung übergangsformen Diachron Entwicklung lassen Wegfall hinzutreten einzeln Facet erfassen ähnlich Weise stil Phänomen auffassen Bündel konkurrierend Merkmal unterschiedlich linguistisch Beschreibungseben Phonologie Morphologie Lexik Semantik syntaxn plot verstehen sandig Karlgren Cutting computergestützt Stilistik ansetzen Lage induktiv umfassend zahlreich merkmal gegenseitig Abhängigkeit jeweilig Gewichtung präzis Berücksichtigung zahlreich möglicherweise relevant Faktor erfassen Klassifikation Clustering Text nutzen Dominique Combe eingefordern stylistique Genr Combe computergestützt realisieren abb fasst Verhältnis Theorie literarisch Gattung computergestützter Stilistik Text mining abb literarisch Gattungstheorie computergestützt Stilistik vergleichbar Konzeption Gattung facett Stil merkmal Verbindung historisch theoretisch gegeben Untergattung einerseits Grundlage stilistisch Öhnlichkeit gruppiert Clustern Text andererseits entdecken genau Stärke statistisch charakterisierbar Korrelation einzeln gattungsfacett stilistisch merkmalen erheben eingeordnen Identifikation distinktiv merkmal Merkmalsgeneralisation Bereich Corpuslinguistik computergestützt Untersuchung stilistisch Unterschied literarisch gattung untergattung Pionierarbeit Douglas Biber Modellierung Zusammenhang Funktional Gattungsaspekt stilistisch Merkmale synthetischen Dimension zusammengefassen biber Erprobung breit Auswahl Potentielle Style Markers Karlgren Cutting vergleichend Evaluation syntaktisch Merkmale vornehmen stamatatos et Merkmalsbündel ermitteln statistisch signifikant Facette korellieren literaturwissenschaftlich Perspektive interpretierbar methodisch Erweiterung Principal Component Analysis computergestützt literarisch Gattungsstilistik aktuell verstärkend Tendenz stilometrisch Frage traditionell vordergrund stehend hinaus bearbeiten zahlreich etabliert Methode Cluster Analysis Principal Component Analysis neu informatisch Verfahren Bereich Text mining Machine Learning konzipiert Gattungsstilistik anschlussfähig schlagen Hintergrund etabliert Methode Principal Component Analysis Pca sehen grundlegend Jackson Weise erweitern Interpretierbarkeit erhöhen verlässlich Unterscheidung Kategorie Gruppe innerhalb Menge Text Stilometrie gleichzeitig Betrachtung Zahl Merkmale Wort angewiesen fasst jeder Merkmal mehrere Text dimension beruhen stilometrisch Erhebung öhnlichkeitsrelationen Text häufig Dimensionsreduktion burrows delta burrow erlauben Pca Reduktion Dimension Datensatze einzig voneinander unabhängig dimension Principal Components pc Varianz daten beschreiben Pcs korrelieren hierbei spezifisch Kombination Gewichtung Merkmalsfrequenze charakteristisch Pca pcs Datensatz enthalten Varianz beschreiben abb graphisch Exploration Öhnlichkeit zwei mehrere Gruppe ausreichen pcs koordinaten verwenden anstatt Zahl Wortfrequenz Kombination suchen Unterschied deutlich abb Bereich literarisch Gattungsstilistik liegen mehrere Ansatz Diachrone Entwicklung Gattung Allgemein moretti jockers spezifisch methodisch Lösungsversuche betreffen Cluster Analyse Unmasking Prozedur Gattungsklassifikation Allison et Kestemont et schöch abb Principal Component Analysis französisch Dram Tragödie Komödie Tragikomödie siebzehnten Jahrhundert basierend relativ häufigkeiten meister verwendet Wörter Scree plot Varianzanteile pc b scatterplot Matrix pc gattunsspezifisch Farbcodierung Pca erlauben Aussage Unterschiedlichkeit zwei kategorien Datenpunkt Einfluß bestimmt Gruppierungsvariable weshalb stilistisch Unterschied Gattung Methode visualisiern analysieren Lücke überbrücken Verfahren entwickeln Einfluß Zugehörigkeit bestimmt Text spezifisch Kategorie Gattung pcs mittels Varianzanalyse anova untersuchen hierbei Gattung unabhängig faktoriell variabel betrachten Wert bestimmt Pc abhängig variabel Bestimmtheitsmaß r liefern hierbei Vergleichsgröße anhand der einflüß verschieden Faktor Gattung Autorschaft Publikationsdatum bestimmt Pc quantifizieren lassen abb abb Pca Kategorie Ergebnis varianzanalysen anovas Quantifizierung einflusses verschieden Faktor pcs untersuchen Faktor Form Ver Prosa mischen Gattung Tragödie Komödie Tragikomödie Autor darstellen Bestimmtheitsmaße r Faktor Pc Symbol Balken repräsentieren Signifikanzniveau jeweilig Beziehung äô signifikant p p p stark Einfluß literarisch Gattung Datensatz -- häufig Wort pc Wert repräsentieren Einfluß einzeln Wortfrequenze pcs erlauben Kombination Resultat Varianzanalys interpretierend Rückschlüsse Kombination Pca anova erlauben somit Identifikation pcs bestimmt Faktor stark prägen abgleich Gewichtung bestimmt wortfrequenzen Pc abb gemeinsam betrachten erlauben rückschlüß merkmal Merkmalsbündel Unterschied bestimmt Faktor relevant Kombination charakteristisch Gattungsunterschiede Fazit konzeptuell Verbindung Gattungstheorie Computergestützter Stilistik einerseits methodisch Erweiterung Pca verbessert Interpretierbarkeit pcs Bezug relevant kategorien andererseits wichtig Zwischenziel erreichen Grundlag eingangs beschrieben Forschungsagenda legen exemplarisch zeigen hoffen vertreten Forschungsagenda computergestützt literarisch Gattungsstilistik hermeneutisch quantitativ Methode eigenständig Ansatz verbinden Entwicklung spezifisch erweitert informatisch Verfahren erweitert Verständnis Natur Entwicklung literarisch Gattung abzielen Literaturangaben Allison Sarah Ryan heus Matthew Jockers Franco moretti And Michael Witmore quantitativ Formalism Experiment Stanford Stanford literary Lab biber Douglas Computer The humanities The Multidimensional approach to Linguistic analyses -- Genre Variation burrow John Linguistic Computing Delta measure of stylistic difference and guide to Likely Authorship Literary and Combe Dominique -- stylistique Genr langue Hoffmann Michael Mikro makrostilistisch Einheit Überblick äù Rhetorik Stilistik international Handbuch historisch systematisch Forschung Band hg Ulla fix Andreas Gardt Joachim Knape Band Berlin de Gruyter Jackson Edward guide to Principal Component New York wiley Jockers Matthew Macroanalysis Digital Methods And literary History Chicago Univ of Illinois press Juola Patrick Authorship Attribution äù foundations and Trend Information Retrieval karlgren Jussi Douglas Cutting Analysis äù Proceedings -- coling vol recognizing Text genr with Simple metrics Using Discriminant Kessler Brett Geoffrey Numberg And hinrich schützen automatic detection -- Text Genre äù proceedings -- acl Kestemont Mike Kim Luyckx Walter Daelemans And Thomas Crombez Verification Using Unmasking äù english studies Authorship moretti Franco graphs maps trees Abstract models for literary History London verso sandig Barbara Textstilistik deutsche Auflage Berlin de Gruyter Schaeffer qu qu äôun Genre Raire Paris Seuil schöch Christof our Stylometric Tools investigating Authorship And Genre French classical Theater äù Digital Humanitie Conference stamatatos Efstathio Nikos Fakotakis And George Kokkinakis Terms of Genre and Author Automatic Text Categorization Computational linguistics,"[('gattungsstilistik', 0.23204108325596362), ('pcs', 0.2214444753137173), ('gattung', 0.19975842026582136), ('stilistik', 0.19548429047617938), ('pc', 0.19548429047617938), ('pca', 0.17452520747006792), ('computergestützt', 0.16901945256693682), ('principal', 0.15776123635512604), ('gattungstheorie', 0.14661321785713455), ('literarisch', 0.14392727704899622)]"
2014,DHd2014,2014_cls_metadata_extracted.csv,Informatik und Hermeneutik. Erste Erkenntnisse,"Evelyn Gius (Universität Hamburg, Deutschland)","hermeneutisches Markup, CATMA, Annotation, computational narratology, Kategorien, Ereignis, narrative Ebene, Ereignishaftigkeit, narrative Einheiten, Digitale Narratologie","hermeneutisches Markup, CATMA, Annotation, computational narratology, Kategorien, Ereignis, narrative Ebene, Ereignishaftigkeit, narrative Einheiten, Digitale Narratologie","In unserem aktuellen BMBF-eHumanities Projekt heureCLéA arbeiten wir als Informatiker und Literaturwissenschaftlerinnen an einer digitalen Heuristik, die die Analyse von literarischen Texten unterstützen soll. Der Anwendungsfokus liegt dabei exemplarisch auf narratologischen Phänomenen der Zeit: d.h., das heuristische Modul von heureCLéA soll die Funktionalität der Textanalyse und -annotationsumgebung CATMA2 erweitern, indem es den Usern automatisch generierte Vorschläge zur Annotation narratologisch definierter Zeit-Phänomene in einem Text anbietet. Das Modul wird auf der Basis von drei Zugängen entwickelt: (1) Ausgangspunkt ist so genanntes ""hermeneutisches Markup"" (Piez 2010), das auf klassischen narratologischen Kategorien wie etwa Ordnung, Frequenz und Dauer beruht (vgl. Genette 1972, Lahn und Meister 2013) und von geschulten Annotatorinnen vergeben wird. Dieses Markup wird (2) mit regelbasierten Verfahren sowie (3) Machine-Learning-Ansätzen kombiniert.3 Aufgrund des Zusammenspiels von literaturwissenschaftlichen 'und speziell: hermeneutischen 'Verfahren und informatischen Verfahren der Information Extraction und der Statistik stehen sich non-deterministische Zugänge zu Texten und entscheidbare bzw. deterministische Verfahren gegenüber, die nicht ohne weiteres auf den jeweilig anderen Ansatz übertragen werden können. Deshalb ist die Reproduzierbarkeit von narratologischen Analysen für die Vereinbarkeit des literaturwissenschaftlichen und des informatischen Zugangs und damit für den Erfolg des heuristischen Moduls ausschlaggebend. In unserem Beitrag präsentieren wir ein methodisches Desiderat im Bereich der Narratologie, das erst durch die interdisziplinäre Zusammenarbeit zwischen Geisteswissenschaftlern und Informatikern in den Fokus gerückt ist und aus unserer Sicht exemplarisch für eine solche Zusammenarbeit ist: die Notwendigkeit, narratologische Analysekategorien eindeutiger zu konzeptionalisieren, um sie operationalisieren zu können. 1 vgl. www.heureclea.de (gesehen am 10.12.2013) 2 vgl. www.catma.de (gesehen am 10.12.2013) 3 Damit ist heureCLéA ein Beitrag zur computational narratology im Sinne von Mani, da es zu ""exploration and testing of literary hypotheses through mining of narrative structure from corpora"" (Mani, 2013, para. 1) beiträgt. 3Zu den regelbasierten Verfahren vgl. Strötgen und Gertz (2010), die Gesamtarchitektur von heureCLéA wird außerdem in einem weiteren eingereichten Beitrag vorgestellt. Die Narratologie ist eine literaturwissenschaftliche Disziplin, die eine Reihe theoretischer Konzepte und Modelle für die Analyse und Interpretation erzählender Texte zur Verfügung stellt. Diese narratologischen Kategorien dienen normalerweise der Bezeichnung und Verortung textueller Eigenschaften, die (a) als typisch für narrative Texte angesehen werden und (b) für besonders interessant und geeignet befunden werden, um die speziellen Eigenschaften eines literarischen Einzelwerkes herauszustellen. Viele der Kategorien dienen der Bezeichnung struktureller Phänomene, die hauptsächlich an der Textoberfläche zugänglich sind. Das gilt insbesondere auch für die meisten Kategorien, die der Analyse explizit markierter Zeitphänomene dienen, wie sie im Rahmen von heureCLéA untersucht werden.4 Obwohl narratologische Kategorien gemeinhin als theoretisch durchdacht und leicht operationalisierbar gelten, zeigten sich bei ihrer formalisierten Anwendung im Rahmen manueller, kollaborativer Annotation in heureCLéA einige theoretische Unzulänglichkeiten. Typischerweise wurden solche Unzulänglichkeiten dann entdeckt, wenn sich die Annotatoren hinsichtlich der korrekten narratologischen Bestimmung konkreter Textstellen nicht einig waren. In Diskussionen über die Gründe für individuelle Annotations-Entscheidungen stellte sich dann oft die uneindeutige oder unvollständige Konzeption der jeweiligen Kategorie als Ursache uneinheitlichen Markups heraus. Die festgestellten theoretischen Versäumnisse lassen sich in zwei Gruppen einteilen, die je unterschiedliche Problemlösungsstrategien erfordern: a) konzeptionelle Unvollständigkeit, die leicht durch eine Vervollständigung der Kategorie mittels funktionaler Entscheidungen behoben werden kann. Stellt sich bei der versuchten Anwendung einer Kategorie heraus, dass ihre Definition zu vage ist, um die Bestimmung einer fraglichen Textstelle vorzunehmen, müssen pragmatische Entscheidungen im Hinblick auf die Inklusion oder Exklusion bisher nicht bedachter textueller Oberflächenmerkmale getroffen werden. 5 4 Der Klarheit wegen sollte angemerkt werden, dass keine der im Feld der Narratologie interessanten Phänomene rein formale Textmerkmale sind, da die Bedeutung von Wörtern und Sätzen stets ausschlaggebend für ihre Bestimmung ist. Das bedeutet, dass solche Phänomene zwar an der Textoberfläche zugänglich sind, ihre Bestimmung jedoch trotzdem in einem weiteren Sinne des Wortes interpretativ sein kann. 5 Ein derartiger Problemfall stellte ich an folgender Textstelle in Friedrich Hebbels Erzählung Matteo im Hinblick auf die Frage, ob es sich hier um eine Prolepse - bisher konzeptionalisiert als Vorgriff in der Zeit handelt: ""Sieh, morgen feire ich meine Hochzeit; zum Zeichen, daß du mir nicht mehr böse bist, kommst du auch, meine Mutter wird dich gern sehen."" (Hebbel 1963: para. 4). Die Schwierigkeit ist hier dadurch gegeben, dass die angesprochene Figur am folgenden Tag nicht auf der Hochzeit erscheint. Um Derartige Entscheidungen haben nur für die Anwendung der jeweiligen Kategorie Konsequenzen, nicht aber für weitere Konzepte. - Die zweite Kategorie betrifft dagegen b) theoretische Unvollständigkeit, die ihrerseits auf die unzureichende Bestimmung fundamentaler narratologischer Konzepte zurückzuführen ist. Probleme dieses Typs können nicht einfach durch pragmatische Entscheidungen behoben werden, weil die für eine Problemlösung notwendigen Setzungen auf der Ebene grundlegender narratologischer Konzepte weitreichende Konsequenzen für viele erzähltheoretische Einzelkategorien nach sich zieht. Im Folgenden soll diese zweite Problemklasse anhand eines Beispiels erläutert werden. In der Erzählung Der Tod von Thomas Mann ist bei dem Vergleich der Annotationsergebnisse im Hinblick auf die Geschwindigkeit der Erzählung6 folgende Passage in den Fokus der Aufmerksamkeit gerückt: Ich habe die ganze Nacht hinausgeblickt, und mich dünkte, so müsse der Tod sein oder das Nach dem Tode: dort drüben und draußen ein unendliches, dumpf brausendes Dunkel. Wird dort ein Gedanke, eine Ahnung von mir fortleben und -weben und ewig auf das unbegreifliche Brausen horchen? Mann 2004: 76 Diese Passage wurde von einigen Annotatoren ab dem ersten Komma als zeitraffend erzählt eingeordnet, von anderen dagegen als Erzählpause. Die Diskussion über die Gründe für die individuellen Entscheidungen hat gezeigt, dass die Annotatoren unterschiedliche Auffassungen darüber vertreten, was ein Ereignis ist. Betrachtet man mentale Vorgänge als Ereignisse, so muss man die zitierte Passage als zeitraffend klassifizieren, da die Gedanken des Erzählers in der fiktiven Welt vermutlich längere Zeit anhielten als die wenigen Sekunden, die in der Erzählung für ihre Wiedergabe eingeräumt werden. Ist man jedoch der Ansicht, dass es sich bei mentalen Prozessen nicht um Ereignisse handelt, so liegt in obiger Textstelle eine Pause vor: Der Bericht von Ereignissen wird unterbrochen durch die Darstellung nicht-ereignishafter Gegebenheiten. Die Frage danach, welche Konzeption von Ereignis korrekt oder sinnvoll ist, ist Gegenstand der Debatte um Narrativität: die für erzählende Texte konstitutive Eigenschaft, von Ereignissen zu berichten. Die unterschiedlichen Intuitionen der Annotatoren in Bezug auf die Definition von ""Ereignis"" korreliert hier mit Schmids Konzeptionen von Ereignis I, das jegliche entscheiden zu können, ob hier eine Prolepse vorliegt, muss entschieden werden, ob dieses Konzept auch antizipierte Ereignisse fassen soll, die im Verlauf der Erzählung nicht eintreten. 6 Unter ""Erzählgeschwindigkeit"" versteht man in der Narratologie das Verhältnis zwischen der Menge an Ereignissen, von denen berichtet wird, und der Zeit, die für diesen Bericht notwendig ist. Form von Zustandsveränderung inkludiert, und Ereignis II, das zusätzliche Kriterien anführt, die Zustandsveränderungen aufweisen müssen, um als Ereignis zu gelten (Schmid 2003).7 Eine Entscheidung im Hinblick auf die richtige Narrativitätsdefinition, die für die Lösung von Annotationsproblemen im Bereich der Erzählgeschwindigkeit notwendig wäre, hätte nun nicht nur für die fraglichen Kategorien Konsequenzen, sondern beispielsweise auch für die Bestimmung des Gegenstandsbereich der Narratologie und potenziell für eine Reihe weiterer Kategorien.8 Angesichts insbesondere dieser zweiten Sorte von Problem stellt sich die Frage, inwieweit solche grundlegenden Fragen im Rahmen von heureCLéA geklärt werden können und sollten. Da die theoretische Arbeit an narratologischen Grundkonzepten nicht im Fokus des Projektes stehen sollte, war zunächst ein individueller Umgang der Annotatoren mit den anwendungsbezogenen Einzelproblemen vorgesehen. Es hat sich jedoch herausgestellt, dass diese Vorgehensweise weder aus narratologischer Sicht befriedigend ist, noch eine aus informationstheoretischer Perspektive verwertbare Datengrundlage liefert. Aus diesen Gründen haben wir uns dazu entschieden, den beiden geschilderten narratologischen Basisproblemen einige Aufmerksamkeit zu widmen: Für die Bestimmung von Ebenenwechsel und -zuordnung wird eine konsistente Lösung gefunden, so dass Ordnungsphänomene tatsächlich unterschiedlichen Erzählebenen zugeordnet werden können. Für die Bestimmung von ‚ÄûEreignis"" streben wir eine plausible Konzeptionalisierung an, die ein möglichst wenig interpretatives Erkennen von Ereignissen erlaubt. 7 Zu diesen Kriterien zählt neben Resultativität, Relevanz, Unvorhersehbarkeit, Effekt, Irreversibilität und Nicht-Wiederholbarkeit auch das Kriterium der Faktizität, das die Eigenschaft von Zustandsveränderungen bezeichnet, tatsächlich in der fiktiven Außenwelt stattzufinden. Wertet man Faktizität als notwendige Eigenschaft von Ereignissen, muss die oben zitierte Passage aus Der Tod als Erzählpause klassifiziert werden. 8 Eine ähnliche Verknüpfung von Annotationsproblemen und ungeklärten narratologischen Basiskonzepten findet sich bei der Annotation von Phänomenen der zeitlichen Ordnung einer Erzählung einerseits und dem grundlegenden narratologischen Konzept der Erzählebenen. Es kann nur sinnvoll das zeitliche Verhältnis von solchen Ereignissen bestimmt werden, die sich auf derselben Erzählebene befinden. Anhand welcher Faktoren ein Ebenenwechsel festzumachen ist, wird in der narratologischen Forschung noch diskutiert (vgl. Ryan 1991, Coste/Pier 2011). Die beschriebene Problematik ist ein spezifisch literaturwissenschaftliches Problem, die sie erzeugenden Rahmenbedingungen sind jedoch zugleich exemplarisch für das Zusammenspiel von Informatik und Geisteswissenschaften. Deshalb ist der entwickelte Lösungsansatz von entscheidender Bedeutung für das Gelingen des Projekts. Die Reproduzierbarkeit von Analyseergebnissen, die durch den Ansatz anvisiert wird, wird in den Geisteswissenschaften traditionell nicht thematisiert, da diese meist dem Konzept der intersubjektiven √úbereinstimmung operieren, ohne diese weiter zu bestimmen. Die Reproduzierbarkeit von Analyseergebnissen ist jedoch zugleich auch eine von mehreren, bislang wenig erforschten Gelingensbedingungen für interdisziplinäre Projekte im Bereich der Digital Humanities. Diese disziplinäre Doppelperspektive auf ein methodisches Kriterium weist insofern auf die konzeptionellen Chancen, Probleme und Bedingungen einer Kooperation zwischen Geisteswissenschaftlern und Informatikern im Kontext von DH-Projekten. Literatur Coste, D. and Pier, J. (2013). Narrative Levels. the living handbook of narratology. http://www.lhn.uni-hamburg.de/article/narrative-levels (gesehen am 06.12.2013). First published 2011. Genette, G. (1972). Discours du r√©cit. In id., Figures III. Paris: Editions Du Seuil, pp. 67-282. Lahn, S. and Meister, J. C. (2013). Einführung in die Erzähltextanalyse: 2nd, updated edition. Stuttgart: Metzler. Mani, I. (2013). Computational Narratology. the living handbook of narratology. http://www.lhn.uni-hamburg.de/article/computational-narratology (gesehen am 06.12.2013). Piez, W. (2010): Towards Hermeneutic Markup: an Architectural Outline. Digital Humanities 2010. Conference Abstracts. London: Office for Humanities Communication, Centre for Computing in the Humanities, King‚Äôs College London, pp. 202-205. Ryan, M.-L. (1991). Possible Worlds, Artificial Intelligence, and Narrative Theory. Bloomington: Indiana UP. Schmid, W. (2003): Narrativity and Eventfulness. In T. Kindt & H.-H. Müller (eds.). What Is Narratology? Questions and Answers Regarding the Status of a Theory. Berlin: de Gruyter, 17'33. Strötgen, J. and Gertz, M. (2010). HeidelTime: High Quality Rule-based Extraction and Normalization of Temporal Expressions. Proceedings of the 5th International Workshop on Semantic Evaluation (ACL 2010).",de,unser aktuell Projekt Heurecléa arbeiten informatiker literaturwissenschaftlerinnen digital Heuristik Analyse literarisch Text unterstützen Anwendungsfokus liegen exemplarisch narratologisch Phänomen heuristisch Modul Heurecléa Funktionalität Textanalyse erweitern User automatisch generiert Vorschlag Annotation narratologisch definiert Text anbieten Modul Basis zugängen entwickeln ausgangspunkt genannt hermeneutisch Markup Piez klassisch narratologisch Kategorie Ordnung Frequenz Dauer beruhen Genette Lahn Meister geschult Annotatorinn vergeben markup regelbasiert Verfahren aufgrund Zusammenspiel literaturwissenschaftlich speziell hermeneutisch Verfahren informatisch Verfahren Information Extraction Statistik stehen Zugänge texten entscheidbar deterministisch Verfahren jeweilig Ansatz übertragen Reproduzierbarkeit narratologisch Analyse Vereinbarkeit literaturwissenschaftlicher informatisch Zugang Erfolg heuristisch Modul ausschlaggebend unser Beitrag präsentieren methodisch desiderat Bereich Narratologie interdisziplinär Zusammenarbeit geisteswissenschaftl Informatikern Fokus rücken Sicht exemplarisch Zusammenarbeit Notwendigkeit narratologisch analysekategorien eindeutig konzeptionalisieren operationalisieren sehen sehen Heurecléa Beitrag Computational Narratology Sinn Mani exploration and Testing of literary Hypothese Through Mining of Narrative structure from corpora Mani Para beitragen regelbasiert Verfahren Strötg Gertz Gesamtarchitektur Heurecléa eingereicht Beitrag vorstellen Narratologie literaturwissenschaftlich Disziplin Reihe theoretisch Konzept Modell Analyse Interpretation erzählend Text Verfügung stellen narratologisch Kategorie dienen normalerweise Bezeichnung Verortung Textueller eigenschaften typisch narrativ Text ansehen -- interessant geeignet befinden spezielle Eigenschaft literarisch einzelwerkes herausstellen kategorie dienen Bezeichnung strukturell Phänomen hauptsächlich Textoberfläche zugänglich gelten insbesondere meister kategorien Analyse Explizit Markierter zeitphänomene dienen Rahmen Heurecléa untersuchen obwohl narratologisch Kategorie gemeinhin theoretisch durchdacht operationalisierbar gelten zeigen formalisiert Anwendung Rahmen manuell kollaborativ Annotation Heurecléa Theoretisch unzulänglichkeiten typischerweise Unzulänglichkeit entdecken Annotator hinsichtlich korrekt narratologisch Bestimmung konkret Textstelle einig Diskussion gründe individuell stellen uneindeutig unvollständig Konzeption jeweilig Kategorie ursach uneinheitlich Markup heraus festgestellt theoretisch Versäumnis lassen Gruppe einteilen unterschiedlich Problemlösungsstrategie erfordern konzeptionell Unvollständigkeit Vervollständigung Kategorie mittels funktional Entscheidung beheben stellen versucht Anwendung Kategorie heraus Definition vage Bestimmung fraglich Textstelle vornehmen pragmatisch Entscheidung Hinblick inklusion Exklusion Bedachter textuell Oberflächenmerkmal treffen Klarheit anmerken Feld Narratologie interessant Phänomen rein formal Textmerkmal Bedeutung wörtern satz stets ausschlaggebend Bestimmung bedeuten Phänomen Textoberfläche zugänglich Bestimmung Sinn Wort interpretativ derartig Problemfall stellen folgend Textstelle Friedrich Hebbel Erzählung matteo Hinblick Frage Prolepse konzeptionalisieren Vorgriff handeln sieh feiren Hochzeit Zeichen böse kommsen Mutter sehen hebbel Para Schwierigkeit geben angesprochen Figur folgend Hochzeit erscheinen derartig Entscheidung Anwendung jeweilig Kategorie Konsequenz Konzept Kategorie betreffen b theoretisch Unvollständigkeit ihrerseits unzureichend Bestimmung Fundamentaler narratologischer Konzept zurückführen Problem Typ einfach pragmatisch Entscheidung beheben Problemlösung notwendig Setzung Ebene Grundlegender narratologischer Konzept weitreichend Konsequenz erzähltheoretisch Einzelkategorie ziehen folgend Problemklass anhand Beispiel erläutern Erzählung Tod Thomas Mann Vergleich Annotationsergebnisse Hinblick Geschwindigkeit folgend Passage Fokus Aufmerksamkeit rücken Nacht hinausgeblicken dünken müssen Tod Tod drüben draußen unendlich Dumpf brausendes Dunkel Gedanke Ahnung fortleben ewig unbegreiflich brausen horchen Mann Passage Annotator Komma zeitraffend erzählen eingeordnen Erzählpause Diskussion gründe individuell Entscheidung zeigen Annotator unterschiedlich Auffassung vertreten Ereignis betrachten mental Vorgäng Ereignis zitiert Passage zeitraffend klassifizieren Gedanke Erzähler fiktiv Welt vermutlich lang anhielten weniger Sekunde Erzählung Wiedergabe einräumen Ansicht mental Prozesse Ereignis handeln liegen obig Textstelle Pause Bericht Ereignis unterbrechen Darstellung gegebenheiten Frage Konzeption Ereignis korrekt sinnvoll Gegenstand Debatte Narrativität erzählend Text Konstitutive Eigenschaft Ereignis berichten unterschiedlich Intuition Annotator Bezug Definition Ereignis korrelieren schmids Konzeption Ereignis i jeglicher entscheiden Prolepse vorliegen entscheiden Konzept antizipiert Ereignis fassen Verlauf Erzählung eintreten Erzählgeschwindigkeit verstehen Narratologie Verhältnis Menge Ereignis berichten Bericht notwendig Form Zustandsveränderung inkludieren Ereignis ii zusätzlich Kriterien anführen zustandsveränderung aufweisen Ereignis gelten schmid Entscheidung Hinblick richtig Narrativitätsdefinition Lösung Annotationsprobleme Bereich Erzählgeschwindigkeit notwendig fraglich Kategori Konsequenz beispielsweise Bestimmung gegenstandsbereich Narratologie Potenziell Reihe weit angesichts insbesondere Sorte Problem stellen Frage inwieweit grundlegend Frage Rahmen Heurecléa klären theoretisch Arbeit Narratologisch Grundkonzepte Fokus projektes stehen individuell Umgang Annotator Anwendungsbezogen Einzelproblemen vorsehen herausstellen vorgehensweise weder narratologisch Sicht befriedigend informationstheoretisch Perspektive verwertbar datengrundlage liefern Grund entscheiden geschilderten narratologisch Basisprobleme Aufmerksamkeit widmen Bestimmung Ebenenwechsel konsistent Lösung finden ordnungsphänomen tatsächlich unterschiedlich erzählebenen zuordnen Bestimmung Äûereignis streben plausibel Konzeptionalisierung möglichst interpretativ erkennen Ereignis erlauben kriterien zählen Resultativität Relevanz Unvorhersehbarkeit Effekt Irreversibilität Kriterium Faktizität Eigenschaft Zustandsveränderung bezeichnen tatsächlich fiktiv Außenwelt stattzufinden werten Faktizität notwendig Eigenschaft Ereignis zitiert Passage Tod Erzählpause klassifizieren ähnlich Verknüpfung Annotationsprobleme ungeklärt Narratologisch Basiskonzepte finden Annotation Phänomene zeitlich Ordnung Erzählung einerseits grundlegend narratologisch Konzept Erzähleben sinnvoll zeitlich Verhältnis Ereignis bestimmen erzähleben befinden anhand Faktor Ebenenwechsel festzumachen narratologisch Forschung diskutieren Ryan Coste Pier beschrieben Problematik spezifisch literaturwissenschaftlich Problem erzeugend Rahmenbedingung exemplarisch Zusammenspiel Informatik geisteswissenschaften entwickelt Lösungsansatz entscheidend Bedeutung gelingen Projekt Reproduzierbarkeit analyseergebnissen Ansatz anvisieren geisteswissenschaften traditionell thematisieren meist Konzept intersubjektiv operieren bestimmen Reproduzierbarkeit analyseergebnissen mehrere bislang erforscht Gelingensbedingungen interdisziplinär Projekt Bereich Digital Humanitie disziplinär Doppelperspektive methodisch Kriterium weisen insofern konzeptionell Chance Problem Bedingung Kooperation geisteswissenschaftl Informatikern Kontext Literatur Coste and Pier narrativ levels The Living handbook -- narratology sehen first published Genette Discours Cit id figures iii Paris editions seuil pp Lahn And Meister Einführung Erzähltextanalyse updated Edition Stuttgart Metzler Mani Computational narratology The Living handbook -- Narratology sehen Piez Towards Hermeneutic Markup architectural outlinen Digital Humanitie Conference abstracts London office for humanities Communication Centre for Computing The humanities king äôs College London pp Ryan possibel World artificial intelligence And narrative Theory Bloomington indiana up Schmid Narrativity and eventfulness Kindt Müller EDS what -- narratology questions and answers Regarding The status -- Theory Berlin de Gruyter strötgen and Gertz Heideltime high quality Extraction and Normalization -- Temporal expressions proceedings of the international workshop -- semantic Evaluation acl,"[('ereignis', 0.29747164770362194), ('narratologisch', 0.29292548101677696), ('heurecléa', 0.19986577925714338), ('bestimmung', 0.16504102127746637), ('narratology', 0.1375517912926988), ('kategorie', 0.13003233294827862), ('narratologie', 0.12596642394241095), ('and', 0.12407469895741931), ('entscheidung', 0.12210786647447881), ('tod', 0.10346547387457207)]"
2014,DHd2014,2014_cls_metadata_extracted.csv,New Technologies for Old Germanic Languages: Resources and Research on Parallel Gospels in Older Continental Western Germanic,"Christian Chiarcos (Goethe Universität Frankfurt, Germany); Jens Chobotsky (Goethe Universität Frankfurt, Germany); Gaye Detmold (Goethe Universität Frankfurt, Germany); Roland Mittmann (Goethe Universität Frankfurt, Germany); Maria Sukhareva (Goethe Universität Frankfurt, Germany)","Technologien, linked-lexical resources, etymological dictionaries, annotation","Technologien, linked-lexical resources, etymological dictionaries, annotation","We describe on-going e_orts at the Goethe University Frankfurt on the study of older Continental Western Germanic languages, in particular, Old High German (OHG, antecessor of German) and Old Saxon (OS, antecessor of Low German and closely related to the antecessor of Dutch) and their relation to Old English (OE), Gothic, German and other Germanic languages as well as the relation of OHG and OS religious texts to their Latin sources. This line of research is conducted in the context of two larger e_orts, the Old German Reference Corpus and the LOEWE cluster ""Digital Humanities"", in collaboration with the Applied Computational Linguistics group at the Goethe-Universit¬®at Frankfurt. The Old German Reference Corpus is a DFG-funded project that emerged from the Deutsch Diachron Digital (DDD) initiative, conducted in cooperation between HU Berlin, U Frankfurt and U Jena, and aims to provide a morphosyntactically annotatated, exhaustive reference corpus of Old High German and Old Saxon. The LOEWE cluster ""Digital Humanities""1, funded through a programme of the State of Hessen, is a collaboration between U Frankfurt, TU Darmstadt and Freies Deutsches Hochstift Frankfurt aiming to develop methodologies and infrastructures to facilitate information-technological support of research in the humanities. Here, we concentrate on biblical texts: These are available for a variety of modern and historical European languages and possess high-quality alignment (verses, segments), thus building up a valuable parallel resource for linguistic, philological and historical research questions, as well as for Natural Language Processing, whose methodologies for alignment and annotation projection can be used to support the analysis of these texts: ‚Ä¢ The Old German Reference Corpus [4] provides a lexicon and an exhaustive corpus of older continental Western Germanic, i.e., Old Saxon (OS) and Old High German (OHG), comprising 650,000 tokens automatically enriched with morphological and morphosyntactic information drawn from existing glossaries which have been digitized by the project, complemented with manual annotations[3] and metadata and published via the ANNIS database [2]. ‚Ä¢ A Historical Linguistic Database was developed in LOEWE from a collection of etymological dictionaries for all Old Germanic languages (incl. OS, OHG, Old English, Gothic, Old Norse) as a relational data base providing user-friendly means of comparing etymologically related forms between historical dialects and their daughter languages, as well as a machine-readable view on these [5]. ‚Ä¢ Major texts in the corpus are the gospel harmonies associated with the names Heliand (OS), Tatian (OHG and Latin) and Otfrid (OHG). Although not direct translations of the 1http://www.digital-humanities-hessen.de 1 Bible and hence not directly alignable with the gospel translations we have for Old English, Gothic, and later stages of English, German, Dutch and North Germanic, a section-level alignment has been manually extrapolated from the literature in a LOEWE project [5]. ‚Ä¢ This coarse-grained alignment is currently being refined to a phrase-level alignment using the linked lexical resources mentioned above as well as statistical models of systematic character correspondences like those applied by [1]. ‚Ä¢ On the basis of correspondences between historical and modern languages in parallel and quasi-parallel text, statistical annotation projection can be applied for the syntactic annotation of Older Germanic. So far, we conducted experiments on the joint projection of dependency syntax to Old English, Middle Icelandic and Early Modern High German corpora following the methodology of [6]. These indicate that projected annotations can serve as training data for mono- and cross-language parsing also for, e.g., OHG. ‚Ä¢ These annotations can be applied, for example, to compare linguistic structures in OHG gospel harmonies and their Latin sources, thereby facilitating the research of a LOEWE project that currently uses statistical word alignment and existing morphosyntactic annotations only as the basis for a qualitative, philological comparison with the TreeAligner [7]. References [1] Marcel Bollmann. POS tagging for historical texts with sparse training data. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse (LAW/ID-2013), pages 11–18, Sofia, Bulgaria, Aug 2013. [2] Christian Chiarcos, Stefanie Dipper, Michael G¬®otze, Ulf Leser, Anke L¬®udeling, Julia Ritz, and Manfred Stede. A Flexible Framework for Integrating Annotations from Di_erent Tools and Tag Sets. Traitement Automatique des Langues (TAL), 49(2), 2008. [3] Sonja Linde and Roland Mittmann. Old German Reference Corpus. Digitizing the knowledge of the 19th century. In Paul Bennett, Martin Durrell, Silke Scheible, and Richard J. Whitt, editors, New Methods in Historical Corpus Linguistics / Korpuslinguistik und interdiziplin¬®are Perspektiven auf Sprache, Korpuslinguistik und interdiziplin¬®are Perspektiven auf Sprache / Corpus linguistics and Interdisciplinary perspectives on language (CLIP): 3, T¬®ubingen, 2013. Narr. [4] Roland Mittmann. Digitalisierung historischer Glossare zur automatisierten Vorannotation von Textkorpora am Beispiel des Altdeutschen. Journal for Language Technology and Computational Linguistics (JLCL), 27(2):39–52, 2013. Special issue Alt¬®uberlieferte Sprachen als Gegenstand der Texttechnologie / Text Technological Mining of Ancient Languages. [5] Timothy Blaine Price. Multi-faceted Alignment: Toward Automatic Detection of Textual Similarity in Gospel-derived Texts. In Proceedings of Historical Corpora 2012, Frankfurt, Dec 2012. [6] Kathrin Spreyer and Jonas Kuhn. Data-Driven Dependency Parsing of New Languages Using Incomplete and Noisy Training Data. In Proceedings of CoNLL, pages 12–20, Boulder, CO, Jun 2009. [7] Martin Volk, Joakim Lundborg, and Ma¬®el Mettler. A search tool for parallel treebanks. In Proceedings of the 1st Linguistic Annotation Workshop (LAW-2007), pages 85–92, Prague, Czech Republic, Jun 2007.",en,describe go goethe university frankfurt study old continental western germanic language particular old high german ohg antecessor german old saxon os antecessor low german closely related antecessor dutch relation old english oe gothic german germanic language relation ohg os religious text latin source line research conduct context large old german reference corpus loewe cluster digital humanity collaboration apply computational linguistic group goethe frankfurt old german reference corpus dfg fund project emerge deutsch diachron digital ddd initiative conduct cooperation hu berlin u frankfurt u jena aim provide morphosyntactically annotatate exhaustive reference corpus old high german old saxon loewe cluster digital fund programme state hessen collaboration u frankfurt tu darmstadt frey deutsche hochstift frankfurt aim develop methodology infrastructure facilitate information technological support research humanity concentrate biblical text available variety modern historical european language possess high quality alignment verse segment build valuable parallel resource linguistic philological historical research question natural language processing methodology alignment annotation projection support analysis text old german reference corpus provide lexicon exhaustive corpus old continental western germanic old saxon os old high german ohg comprise token automatically enrich morphological morphosyntactic information draw exist glossary digitize project complement manual metadata publish annis database historical linguistic database develop loewe collection etymological dictionary old germanic language incl os ohg old english gothic old norse relational data base provide user friendly mean compare etymologically relate form historical dialect daughter language machine readable view major text corpus gospel harmony associate name heliand os tatian ohg latin otfrid ohg direct translation bible directly alignable gospel translation old english gothic later stage english german dutch north germanic section level alignment manually extrapolate literature loewe project coarse grain alignment currently refine phrase level alignment link lexical resource mention statistical model systematic character correspondence like apply basis correspondence historical modern language parallel quasi parallel text statistical annotation projection apply syntactic annotation old germanic far conduct experiment joint projection dependency syntax old english middle icelandic early modern high german corpora follow methodology indicate project annotation serve training datum cross language parse ohg annotation apply example compare linguistic structure ohg gospel harmony latin source facilitate research loewe project currently use statistical word alignment exist morphosyntactic annotation basis qualitative philological comparison treealigner reference marcel bollmann pos tagging historical text sparse training datum proceeding linguistic annotation workshop interoperability discourse law page sofia bulgaria aug christian chiarcos stefanie dipper michael otze ulf leser anke udeling julia ritz manfre stede flexible framework integrate annotation tool tag set traitement automatique des langues tal sonja linde roland mittmann old german reference corpus digitize knowledge century paul bennett martin durrell silke scheible richard whitt editor new method historical corpus linguistics korpuslinguistik und perspektiven auf sprache korpuslinguistik und perspektiven auf sprache corpus linguistic interdisciplinary perspective language clip ubingen narr roland mittmann digitalisierung historischer glossare zur automatisierten vorannotation von textkorpora beispiel des altdeutschen journal language technology computational linguistic jlcl special issue uberlieferte sprachen als gegenstand der texttechnologie text technological mining ancient language timothy blaine price multi faceted alignment automatic detection textual similarity gospel derive text proceeding historical corpora frankfurt dec kathrin spreyer jonas kuhn data drive dependency parsing new language incomplete noisy training datum proceeding conll page boulder co jun martin volk joakim lundborg el mettler search tool parallel treebank proceeding linguistic annotation workshop page prague czech republic jun,"[('old', 0.4667281885951799), ('ohg', 0.24136429154060346), ('historical', 0.21306888424855566), ('german', 0.20045044997340095), ('alignment', 0.18643527371748622), ('germanic', 0.1810232186554526), ('language', 0.1727742864003377), ('linguistic', 0.16192904060764843), ('loewe', 0.15085268221287718), ('frankfurt', 0.1473878490300568)]"
2014,DHd2014,2014_cls_metadata_extracted.csv,GeoBib - Georeferenzierte Online-Bibliographie früher Holocaust- und Lagerliteratur,"Frank Binder (Justus-Liebig-Universität Gießen, Germany); Annalena Schmidt (Herder-Institut für historische Ostmitteleuropaforschung); Bastian Entrup (Justus-Liebig-Universität Gießen, Germany); Markus Roth (Justus-Liebig-Universität Gießen, Germany); Henning Lobin (Justus-Liebig-Universität Gießen, Germany)","Onlinebibliographie, Georeferenzen","Onlinebibliographie, Georeferenzen","Zielsetzung Ziel des Projekts ist es, die frühen Texte der deutsch- bzw. polnischsprachigen Holocaust- und Lagerliteratur von 1933 bis 1949 bibliographisch in einer Online-Datenbank zu erfassen. So können diese frühen Texte, die in weiten Teilen aus dem kulturellen und kollektiven Gedächtnis verdrängt wurden, für die öffentliche, wissenschaftliche und didaktische Wahrnehmung erschlossen und aufbereitet werden. Ergänzt werden die bibliographischen Einträge durch inhaltliche und biographische Annotationen, Informationen zur Werkgeschichte sowie durch Georeferenzierung (Informationen zu Orten und Plätzen anhand von Kartenmaterial). Das zu entwickelnde Web-Portal soll dabei 'neben der bibliographischen Suche 'auch über geographische Karten gezielt Texte zu einer bestimmten Region zugänglich machen. Dabei sollen Abfragemöglichkeiten nach räumlichen Kriterien und Attributen beliebig kombinierbar sein. Methoden Die frühen Texte der Holocaustliteratur werden'verbunden mit einer tiefreichenden inhaltlichen Erschließung 'in einem Online-Bibliographie-Portal repräsentiert. Eine an internationalen Annotationsstandards (TEI) ausgerichtete systematische Erfassung der bis 1949 publizierten Texte, ggf. erschienener Rezensionen, der Sekundärliteratur sowie die Anreicherung durch biographische Informationen zu den Verfassern/-innen wird dabei kombiniert mit der Georeferenzierungvon Metadaten und Textinhalten (Orte, Lager, Gettos etc.). Sämtliche Daten werden in einer OnlineDatenbank erfasst, die den zukünftigen Nutzern den Zugriff auf die bibliographischen Daten und deren Auswertung durch innovative kartenbasierte Visualisierungen ermöglicht. Dies bildet eine wesentliche Grundlage für daran anschließende literatur- und geschichtswissenschaftliche Forschungsfragestellungen sowie für eine didaktische Nutzung dieser Zeugnisse in der schulischen und außerschulischen Bildungsarbeit. Genutzte Ressourcen Die aufwändige Beschaffung und inhaltliche Erschließung der frühen Holocausttexte wird als zentraler Teil der Projektarbeiten durch ein Team von Literaturwissenschaftler/innen und Historiker/innen unter intensiver Nutzung verschiedener einschlägiger Bibliotheken, Archive sowie den Kauf antiquarischer Bücher durchgeführt. Zur Erfassung bibliographischer, literaturwissenschaftlicher und historischer Daten wird ein auf TEIP5 basierendes XML-Schema erstellt und eine angepasste Autorenumgebung in Oxygen XML verwendet (s. Entrup et al. 2013b). Historisch-biographische Informationen zu Autor/innen sowie ortsbezogene Informationen werden zeitgleich zentral in einem projektinternen Redaktionswiki (Wikimedia) zusammengetragen. Somit können sie später automatisiert ausgelesen und in die Portaldatenbank übertragen werden. Über die Verlinkung von personen-, orts- und zeitbezogenen Informationen in den TEI-Dokumenten unter Nutzung der Wiki-Einträge werden die Zusammenhänge zwischen den erschlossenen HolocaustTexten technisch erfassbar. Informationen zu den Autoren/innen werden darüber hinaus mit der Gemeinsamen Normdatei (GND) der Deutschen Nationalbibliothek verknüpft. Zurückgegriffen werden kann aber auch auf die im Bibliographieportal des Herder-Instituts und anderen Abteilungen gesammelten Daten und die Forschungsbibliothek (Warmbrunn 2012), in der für die Jahre zwischen 1954 und 1998 alle landesweiten und regionalen Zeitungen Polens und weiterer Nachbarländer gesammelt wurden und wo über eine eigene Zeitungsausschnittssammlung auf zahlreiche biographische und ortsbezogene Informationen zurückgegriffen werden kann. Für die Georeferenzierung und Bereitstellung eines geographischen Suchzugriffes wird ein geographisches Informationssystem eingesetzt. Zur Aufbereitung von Karten wird für das entstehende Online-Portal ein Map-Server benötigt, der Karten und Abfragedienste sowie GISFunktionalitäten zur Verfügung stellt. In Bezug auf Kartenmaterial werden vorhandene Grundlagenkarten recherchiert aber bei Bedarf auch digitales Kartenmaterial erstellt. Entstehende Ressourcen Die frühen Texte der Holocaust- und Lagerliteratur werden in Form einer umfangreichen Bibliographie, nicht aber in einer digitalen Volltextbibliothek erschlossen.Urheberrechtsfragen spielen hier für die Projektarbeiten eine zentrale Rolle. Die Arbeitsstelle Holocaustliteratur tritt seit geraumer Zeit dagegen auf, Opfertexte als frei verfügbar anzusehen. Für den Gesamtbestand wäre eine befriedigende Rechteklärung aufgrund der jeweils notwendigen Einzelfallprüfungen nicht möglich gewesen. Jenseits der juristischen Dimension hätte eine Volltext-Digitalisierung ohne Rechteklärung aber auch einen immensen symbolischen Schaden zur Folge: Die Rechte der Opfer würden grob missachtet.Die Ermittlung und Erschließung der Texte in einer Bibliographie, die Rückholung in das kommunikative Gedächtnis ist dagegen auch den Rechten der Opfer stark verpflichtet und will helfen, dass deren frühe Zeugnisse (wieder) sichtbar werden. Die tiefreichende qualitative Erschließung der Quelldokumente in Form eigens erstellter Annotationsdokumente (inhaltliche Zusammenfassung, Autorbiographie, Werkgeschichte, Verschlagwortung u.a.) bilden die Datengrundlage für die weiteren informationsverarbeitenden Schritte sowie das entstehende Online-Portal. Das entstehende Webportal soll ausgewählte relevante Metadatenstandards unterstützen und einschlägige Schnittstellen zum Harvesting von Metadaten bedienen können. Ortsbezogene Informationen aus der Erschließung der Quelldokumente werden darüber hinaus mit Grundlagenkarten verknüpft und in ein geographisches Informationssystem eingepflegt. Parallel zum Projekt entstehen überdies Qualifikationsarbeiten in der Literatur- und Geschichtswissenschaft, in denen die frühen Textzeugnisse sowie ihre Entstehungsbedingungen auch auf Grundlage der im Projekt erhobenen Daten und gewonnenen Erkenntnisse untersucht werden. Ferner werden zwei Konferenzen mit jeweils einem literatur- und einem geschichtswissenschaftlichen Schwerpunkt durchgeführt, deren Ergebnisse publiziert werden. In Einzelfällen, die besonders aussagekräftig sind und bei denen sich die Frage des Urheberrechts zweifelsfrei klären lässt, sollen auch frühe Textzeugnisse reediert und somit als Volltext dem Diskurs zugänglich gemacht werden. Referenzen Entrup, Bastian, Maja Bärenfänger, Frank Binder and Henning Lobin(2013a): IntroducingGeoBib: An Annotatedand Geo-referenced Online Bibliographyof Early German andPolish Holocaust and Camp Literature (1933–1949). Digital Humanities 2013, University of Nebraska–Lincoln, 16-19 July 2013. http://dh2013.unl.edu/abstracts/ab-229.html Entrup, Bastian, Frank Binder and Henning Lobin(2013b): Extendingthepossibilitiesforcollaborativeworkwith TEI/XML throughtheusageof a wiki-system. In: Proceedingsofthe 1st Workshop on Collaborative Annotations in Shared Environments: metadata, vocabulariesandtechniques in the Digital Humanities, DH CASE ""13. September 10 2013, Florence, Italy. <doi:10.1145/2517978.2517988>. Warmbrunn, Jürgen (2012). ""Das Vernetzen von Menschen, Daten und Systemen 'Die Forschungsbibliothek des Herder-Instituts in Marburg."" In: Bernhard Mittermaier (Hrsg.) Vernetztes Wissen 'Daten, Menschen, Systeme. 6. Konferenz der Zentralbibliothek, Forschungszentrum Jülich 5. - 7. November 2012 (Proceedingsband).ISBN 978-3-89336-821-1 http://hdl.handle.net/2128/4699",de,Zielsetzung Ziel Projekt früh Text Polnischsprachig Lagerliteratur bibliographisch erfassen früh Text weit Teil kulturell kollektiv Gedächtnis verdrängen öffentlich wissenschaftlich didaktisch Wahrnehmung erschließen aufbereitet ergänzen Bibliographisch eintrag inhaltlich biographisch Annotation Information Werkgeschichte Georeferenzierung Information Ort Plätze anhand kartenmaterial entwickelnd Bibliographisch Suche geographisch Karte gezielt Text bestimmt Region zugänglich abfragemöglichkeiten räumlich kriterien attribut beliebig kombinierbar Methode früh Text Holocaustliteratur tiefreichend inhaltlich Erschließung repräsentieren international Annotationsstandards tei ausgerichtet systematisch Erfassung publiziert Text Erschienener rezensionen Sekundärliteratur Anreicherung biographisch Information kombinieren georeferenzierungvon metadat Textinhalt ort Lager Getto sämtlicher daten Onlinedatenbank erfassen zukünftig Nutzer Zugriff bibliographisch daten Auswertung innovativ kartenbasiert Visualisierung ermöglichen bilden wesentlich Grundlage anschließend geschichtswissenschaftlich Forschungsfragestellung didaktisch Nutzung zeugnisse schulisch außerschulisch Bildungsarbeit genutzt Ressource aufwändig Beschaffung inhaltlich Erschließung früh Holocausttext zentral Projektarbeit Team Literaturwissenschaftler innen Historiker innen intensiv Nutzung verschieden einschlägig Bibliothek Archiv Kauf antiquarisch büch durchführen Erfassung bibliographisch literaturwissenschaftlich historisch daten basierend erstellen angepas Autorenumgebung Oxyg xml verwenden Entrup et Information Autor innen ortsbezogen Information zeitgleich zentral projektintern Redaktionswiki wikimedia zusammentragen somit automatisiert ausgelesen Portaldatenbank übertragen Verlinkung zeitbezogen Information Nutzung zusammenhänge erschlossen holocausttext technisch erfassbar Information Autor innen hinaus gemeinsam Normdatei gnd deutsch Nationalbibliothek verknüpfen zurückgegriffen Bibliographieportal abteilung gesammelt daten Forschungsbibliothek Warmbrunn landesweit regional Zeitung Polen weit nachbarländ sammeln Zeitungsausschnittssammlung zahlreich biographisch ortsbezogen Information zurückgegriffen Georeferenzierung Bereitstellung geographisch Suchzugriff geographisch Informationssystem einsetzen Aufbereitung Karte entstehend benötigen Karte abfragedien gisfunktionalität Verfügung stellen Bezug kartenmaterial vorhanden Grundlagenkart recherchieren Bedarf digital kartenmaterial erstellen entstehend Ressource früh Text Lagerliteratur Form umfangreich Bibliographie digital volltextbibliothek spielen Projektarbeit zentral Rolle arbeitsstell Holocaustliteratur treten geraum opfertexte frei verfügbar ansehen Gesamtbestand befriedigend Rechteklärung aufgrund jeweils notwendig einzelfallprüfung jenseits juristisch Dimension Rechteklärung immens symbolisch Schaden Folge Opfer grob Ermittlung Erschließung Text Bibliographie Rückholung kommunikativ Gedächtnis Opfer stark verpflichten helfen Frühe zeugnisse sichtbar tiefreichend qualitativ Erschließung quelldokumente Form eigens erstellt annotationsdokumenter inhaltlich Zusammenfassung autorbiographie werkgeschichen Verschlagwortung bilden Datengrundlage informationsverarbeitend Schritt entstehend entstehend Webportal ausgewählt relevant Metadatenstandard unterstützen einschlägigen schnittstellen Harvesting metadaten bedienen ortsbezogen Information Erschließung quelldokumente hinaus Grundlagenkart verknüpfen geographisch Informationssystem eingepflegen parallel Projekt entstehen überdies qualifikationsarbeien Geschichtswissenschaft früh Textzeugnisse entstehungsbedingungen Grundlage Projekt erhoben daten gewonnen Erkenntnis untersuchen ferner Konferenz jeweils geschichtswissenschaftlich Schwerpunkt durchführen Ergebnis publizieren einzelfällen aussagekräftig Frage urheberrechts zweifelsfrei klären lässt früh Textzeugnisse reedieren somit Volltext Diskurs zugänglich referenzen Entrup Bastian Maja Bärenfänger Frank Binder And henning introducinggeobib Annotatedand Online Bibliographyof early German Andpolish Holocaust and Camp literature Digital Humanitie university of July entrup Bastian Frank Binder And Henning Extendingthepossibilitiesforcollaborativeworkwith tei xml Throughtheusageof proceedingsofthe workshop -- Collaborative annotations Shared Environments Metadata vocabulariesandtechniques The Digital Humanitie dh Case September florence italy Warmbrunn Jürgen vernetzen Mensch daten systemen Forschungsbibliothek Marburg Bernhard Mittermaier vernetzt wissen daten Mensch System Konferenz Zentralbibliothek forschungszentrum jülich November,"[('bibliographisch', 0.20003730091934901), ('früh', 0.18163449246076882), ('geographisch', 0.17924814601097902), ('erschließung', 0.16194763288039654), ('information', 0.16103051177367028), ('entstehend', 0.15544411044936937), ('entrup', 0.15228913617944148), ('ortsbezogen', 0.15228913617944148), ('kartenmaterial', 0.15228913617944148), ('biographisch', 0.13443610950823426)]"
2014,DHd2014,2014_cls_metadata_extracted.csv,LitSOM. Kartierung russischer Gegenwartsliteratur,"Gernot Howanitz (Universität Passau, Deutschland); Helmut Mayer (Universität Salzburg, Österreich)","Learning Vector Quantization, computergestützte Textanalyse, Distant Reading, literarische Self-Organizing Maps, Feature Extraction, Feature-Vektoren, Java, Unified Distance Matrix, Cross Validation","Learning Vector Quantization, computergestützte Textanalyse, Distant Reading, literarische Self-Organizing Maps, Feature Extraction, Feature-Vektoren, Java, Unified Distance Matrix, Cross Validation","Zusammenfassung Das literarische SOM (LitSOM) wendet selbstorganisierende Karten (Self-Organizing Maps, SOM) und Learning Vector Quantization (LVQ) auf russische Literatur an. Das SOM wird dafür eingesetzt, um eine Karte zeitgenössischer russischer Romane zu erstellen, die es Literaturwissenschaftlerinnen und Literaturwissenschaftlern erlaubt, Beziehungen zwischen den Romanen zu untersuchen. LitSOM ist eine ""Distant reading""-Technik. Die Qualität der Karten wird sowohl subjektiv aus der Sicht der Literaturwissenschaft, als auch objektiv, d.h. in einem ""klassischen"" Problem des Textmining, nämlich der Klassifizierung von Autorinnen und Autoren, bestimmt. Um dies zu erreichen, wird der SOM-Algorithmus durch den LVQ-Algorithmus ergänzt. 1. Einleitung 1.1 Überblick Ein Hauptziel dieses Beitrags ist die Implementierung eines Systems für computerunterstützte Textanalyse, das sogenannte Literary SOM (LitSOM). Darüber hinaus zeigen wir, wie Literaturwissenschaftlerinnen und Literaturwissenschaftler dieses System für ihre Forschungen einsetzen können. In unserer Beispielanwendung haben wir jeweils 15 Romane von acht verschiedenen Autorinnen und Autoren der russischen Gegenwartsliteratur kartiert, um die Nützlichkeit von quantitativen Methoden für die slavistische Literaturwissenschaft zu demonstrieren. Zwar gibt es eine lange russische Tradition quantitativer Zugänge zur Literatur, diese ist allerdings etwas in Vergessenheit geraten. So hat der bedeutende russische Mathematiker Andrej Markov 1913 ein Paper publiziert [1], in dem er das Konzept der Markovkette demonstriert. Markovketten erlauben es, Ereignisse zu modellieren, die nacheinander stattfinden. Heutzutage werden sie in verschiedensten Feldern angewandt, beispielsweise in den Wirtschaftswissenschaften oder in der Physik, und sie spielten auch eine Schlüsselrolle in Claude Shannons grundlegender Monographie zur Informationstheorie [2]. Trotz dieser vielfältigen Anwendungsmöglichkeiten hat Markov im Jahr 1913 den Versroman ""Eugen Onegin"" (1833) zu seinem Studienobjekt gemacht. Dieses Beispiel zeigt, wie stark die Verbindung zwischen Mathematik und Literatur im Russland des frühen 20. Jahrhunderts war. 1.2 Methodologie LitSOM basiert auf sogenannten selbstorganisierenden Karten (Self-Organizing Maps, SOM) [3]. Ein SOM ist perfekt geeignet für unstrukturierte Daten und unvollständige Information, weil es hochdimensionale Probleme vereinfachen und für den Menschen leicht verständlich darstellen kann. Deshalb eignen sich SOM sehr gut für Data Mining [4]. Ein SOM kann dazu verwendet werden, eine große Anzahl von Texten zu clustern und sie auf einer zweidimensionalen Karte anzuzeigen. Das sogenannte WEBSOM [5] clustert beispielsweise Newsgroup-Postings nach ihrem Inhalt. Das LitSOM funktioniert ähnlich, arbeitet aber mit Romanen anstelle kurzer Nachrichten. Es erstellt eine Karte, die die Abstände zwischen einzelnen Romanen darstellt 'je näher, desto ähnlicher. Dieser Text-Mining-Ansatz liefert Literaturwissenschaftlerinnen und Literaturwissenschaftlern eine automatische Visualisierung von Beziehungen zwischen unterschiedlichen Texten. Nach Franco Moretti ist LitSOM ein Werkzeug für ""distant reading"" [6], also für eine Mischung aus der klassischen literarischen Textanalyse (""close reading"") und dem Querlesen von Texten [7]. 2. Vorarbeiten 2.1 Implementierung des SOM Alle Bestandteile von LitSOM (SOM, Feature Extraction basierend auf Wortfrequenzen und eine Visualisierung mittels der Unified Distance Matrix [8]) wurden in Java implementiert. Für die Feature Extraction haben wir Sergej Sharovs Liste der 5000 häufigsten russischen Wörter verwendet [9]. Der Feature-Vektor wurde dann wie folgt zusammengestellt: Für jeden Roman wurden die Wörter aus der Sharov-Liste gezählt und jeweils durch die Gesamtanzahl der Wörter in diesem Roman dividiert. Dies gewährleistet, dass die einzelnen Feature-Vektoren untereinander vergleichbar bleiben. 2.2 Setup der Experimente Verschiedene Längen des Feature-Vektors wurden getestet: 5, 10, 25, 30, 40, 50, 75, 100, 125, 150, 175 und 200 Features. Um den Einfluss verschiedener Wortarten auf die resultierenden Karten zu untersuchen, wurde Sharovs ursprügliche Liste modifziert; Versionen rein mit Nomen und Verben sowie eine Kontrollgruppe mit allen anderen Wörtern wurde erstellt. Aufgrund eigener Testreihen haben wir uns für ein SOM aus 108 Neuronen in einem hexagonalen 9 _ 12 Gitter entschieden. Die Lernrate _(0) wurde auf 0.5 gesetzt und dann wie folgt verringert: _(t + 1) = _(t)/(1 + _(t)). Der anfängliche Nachbaschaftsradius wurde mit 2.5 festgelegt. Nach jedem Zyklus wurde dieser Radius um 0.0005 verringert. Nach einer unüberwachten SOM-Phase mit 3000 Zyklen folgte eine überwachte LVQ-Phase mit 1000 Zyklen. Insgesamt wurden 4800 Experimente durchgeführt und ebensoviele Karten erstellt. Tabelle 1 Rang Feature-Vektor Korrekt identifiziert LVQ-Genauigkeit 1NN-Genauigkeit 1 150 Verben 103 85,83% 92,50% 2 100 Nomen 102 85,00% 95,83% 3 100 Verben 101 84,16% 91,67% 4 200 Sharov 100 83,33% 90,00% 5 175 Nomen 99 82,50% 95,00% 6 100 Sharov 98 81,67% 90,00% 6 150 Sharov 98 81,67% 90,83% 6 125 Nomen 98 81,67% 93,34% 6 125 Verben 98 81,67% 91,67% 10 75 Nomen 97 80,83% 94,16% 10 40 Nomen 97 80,83% 89,16% 10 175 Verben 97 80,83% 92,50% 3. Resultate 3.1 Klassifizierung von Autorinnen und Autoren 40 verschiedene Konfigurationen und Leave One Out Cross Validation (LOOCV) resultierten in einer Gesamtanzahl von 4800 verschiedenen Experimenten. Die besten Resultate dieser 4800 SOM/LVQ-Läufe sind in Tabelle 1 angeführt. Das beste Resultat '86% richtig erkannt 'wurde mit einer Liste von den 150 häufigsten Verben als Feature-Vektor erzielt. Diese Resultate legen den Schluss nahe, dass die von LitSOM produzierten Karten die Verteilung der 120 Romane tatsächlich widerspiegelt. Mit einem 1NN-Klassifzierer, der als Kontrolle fungierte, wurde sogar eine Genauigkeit von 96% erreicht. 3.2 U-Matrix Die Qualität der Karten kann nur subjektiv bestimmt werden. Deshalb präsentieren wir hier eine Karte samt Interpretation als Beispiel. Im Allgemeinen ist anzumerken, dass zwischen einzelnen Karten durchaus Unterschiede festzustellen waren, allerdings glichen sich die Karten trotzdem meist in ihrer grundlegenden Struktur. Grafik 1: U-Matrix-Visualisierung des SOM für Viktor Pelevins ""Ananaswasser für eine feine Dame"" Grafik 1 zeigt die U-Matrix, die das LitSOM für Viktor Pelevins Roman ""Ananaswasser für eine feine Dame"" nach den SOM/LVQ-Läufen darstellt. Diese Karte wurde basierend auf einem PatternVektor mit den 100 häufigsten Nomen erstellt. Pelevins Roman diente als unbekannter Test-Text, d.h. nach dem Training mit den 119 anderen Romanen wurde dieser Text 'korrekt 'klassifiziert. Wie man sieht, weist LitSOM sehr gut auf Romane hin, die eher untypisch für die jeweilige Autorin oder den jeweiligen Autor sind. Beispiele dafür sind Vladimir Sorokins ""Die Schlange"" (gekennzeichnet durch ""ocher""). Auch die jeweiligen Relationen unterschiedlicher Autorinnen und Autoren zueinander sind nachvollziehbar, so liegen die beiden Fantasy-Autoren Sergej Lukjanenko und Viktor Pelevin nebeneinander. Unser letzter Test fand sozusagen unter realistischen Bedinungen statt: Pelevins neuester Roman ""Batman Apollo"" wurde am 8. März 2013 publiziert, nachdem der Großteil unserer Experimente bereits abgeschlossen war, damit hat er auch nicht Eingang in das ursprüngliche Textkorpus gefunden. In Grafik 1 findet man ""Batman Apollo"" (""betman"" in rot) gleich neben weiteren Pelevin-Romanen jüngeren Datums, vor allem auch ""Empire V"" (""ampir""). Blättert man diese Romane durch, erfährt man, dass ""Batman Apollo"" die Fortsetzung von ""Empire V"" ist. 4. Diskussion Die Ergebnisse der Klassifizierungsexperimente mit LVQ und 1NN sind sehr gut, vor allem in Anbetracht der Tatsache, dass literarische Texte sehr komplex sein können. Wortfrequenzen erlauben es, zwischen Romanen unterschiedlicher Autorinnen und Autoren zu differenzieren. Mit der Liste der 150 häufigsten Verben konnten 103 von 120 Romanen (86%) korrekt ihren jeweiligen Autorinnen und Autoren zugeordnet werden. Damit wurde empirisch belegt, dass LitSOM die Beziehungen zwischen Texten unterschiedlicher Autorinnen und Autoren sinnvoll darstellen kann. Die Visualisierung mittels U-Matrix, die LitSOM auch zur Verfügung stellt, erlaubt es, die Relationen zwischen unterschiedlichen Texten in einfacher Form darzustellen. Unsere subjektive Bewertung der Karten zeigte, dass die Wahl der Features großen Einfluss auf die visuelle Qualität der Karten hat. So waren die Cluster einzelner Autorinnen und Autoren bei auf der Nomen-Liste basierenden Karten am besten voneinander abgetrennt. Die Verben-Liste wiederum war für das Klassifizierungsexperiment besser geeignet, optisch waren die Karten allerdings weniger klar strukturiert. Im Allgemeinen eignen sich die Karten vor allem dazu, Texte zu finden, die für einen Autor oder eine Autorin untypisch sind bzw. die dem Stil einer anderen Autorin oder eines anderen Autors ähneln. Auch innerhalb eines Clusters lassen sich interessante Schlüsse hinsichtlich der Texte ziehen, so gibt es häufig Unterschiede zwischen noch in der Sowjet-√Ñra geschriebenen Texten und späteren, post-sowjetischen. LitSOM kann in vielerlei Hinsicht verbessert werden; bei den Feature-Vektoren sind noch viele weitere Kombinationen denkbar, die durch Feature-Selection-Algorithmen bestimmt werden könnten. Weiters ist es denkbar, die visuellen Karten automatisiert durch BildverarbeitungsAlgorithmen zu vergleichen, um eine objektivere Beurteilung zu erreichen. Auch der Einfluss der SOM-Parameter, etwa unterschiedlicher Kartengrößen, auf die visuelle Qualität der Karten ist noch nicht hinreichend untersucht. Weitere wertvolle Einblicke könnten durch Einbeziehung weiterer Texte aus unterschiedlichen literarischen Epochen gewonnen werden. Gleichzeitig würden all diese Experimente helfen, mehr Erfahrung im Umgang mit den Karten zu gewinnen. Diese Erfahrung ist sehr wichtig, denn schlussendlich kann nur ein Mensch die Karten interpretieren und als Ausgangspunkt für weitere Überlegungen nutzen 'ein Prozess, der ""distant reading"" und ""close reading"" verbindet. Quellen 1. Markov, A. 1913. Primer statisticheskogo issledovaniia nad tekstom ""Evgeniia Onegina"", illiustriruiushchii sviaz ispytanii v tsep‚Äô. Izvestiia Imperatorskoi Akademii Nauk 7.3. 2. Shannon, CE. and Weaver, W., 1949. The Mathematical Theory of Communication. University of Illinois Press, Illinois. 3. Kohonen, T. 2001. Self-Organizing Maps. Berlin. 4. Lagus, K. et al. 1999. WEBSOM for Textual Data Mining. Artificial Intelligence Review 13 (5/6). http://citeseerx.ist.psu.edu/ viewdoc/summary?doi=10.1.1.12.5452 [accessed 17 March 2013]. 5. Kohonen, T. et al. 2000. Self Organization of a Massive Document Collection. IEEE Transactions on Neural Networks 11 (3), 574‚Äì585. http://lib.tkk.fi/Diss/2000/isbn9512252600/article7. pdf [accessed 14 March 2013]. 6. Moretti, F. 2000. Conjectures on World Literature. New Left Review 1, 54-68. http://newleftreview.org/II/1/franco-moretti-conjectures-on-world-literature [accessed 30 November 2013]. 7. Kirschenbaum, M. 2007. The Remaking of Reading: Data Mining and the Digital Humanities. National Science Foundation Symposium on Next Generation of Data Mining and CyberEnabled Discovery for Innovation. http://citeseerx.ist.psu.edu/viewdoc/download? doi=10.1.1.111.959&rep=rep1&type=pdf [accessed 9 May 2013] 8. Kohonen, T. 2001. Self-Organizing Maps. Berlin, 165f. 9. Sharov, S. 2001. Chastotnyi slovar‚Äô. RosNII II Website. http: //www.artint.ru",de,Zusammenfassung literarisch Som Litsom wenden selbstorganisierend Karte maps Som Learning vector Quantization lvq russisch Literatur Som einsetzen Karte zeitgenössisch Russischer Roman erstellen literaturwissenschaftlerinnen literaturwissenschaftler erlauben Beziehung Roman untersuchen Litsom distant Qualität Karte sowohl subjektiv Sicht Literaturwissenschaft objektiv klassisch Problem textmining nämlich Klassifizierung autorinn Autor bestimmen erreichen ergänzen Einleitung Überblick Hauptziel Beitrag Implementierung System computerunterstützt Textanalyse sogenannter Literary Som Litsom hinaus zeigen literaturwissenschaftlerinn Literaturwissenschaftler System Forschung einsetzen Beispielanwendung jeweils Roman verschieden autorinn Autor russisch Gegenwartsliteratur kartieren Nützlichkeit quantitativ Methode slavistisch Literaturwissenschaft demonstrieren russisch Tradition quantitativ zugänge Literatur Vergessenheit geraten bedeutend russisch Mathematiker Andrej Markov Paper publizieren Konzept Markovkette demonstrieren Markovkette erlauben Ereignis Modelliere nacheinander stattfinden heutzutage verschieden feldern anwenden beispielsweise wirtschaftswissenschaft Physik spielen Schlüsselrolle Claude shannons grundlegend Monographie Informationstheorie trotz vielfältig anwendungsmöglichkeiten Markov Versroman eugen Onegin Studienobjekt zeigen stark Verbindung Mathematik Literatur Russland früh Jahrhundert Methodologie Litsom basieren sogenannter selbstorganisierend Karte maps som Som perfekt geeignet unstrukturiert daten unvollständig Information hochdimensional Problem vereinfachen Mensch verständlich darstellen eignen som Data Mining Som verwenden Anzahl Text clustern zweidimensional Karte anzeigen sogenannter Websom clusteren beispielsweise Inhalt Litsom funktionieren ähnlich arbeiten Roman anstelle kurz Nachricht erstellen Karte Abständ einzeln Roman darstellen nah desto ähnlich liefern literaturwissenschaftlerinnen Literaturwissenschaftler automatisch Visualisierung Beziehung unterschiedlich Text Franco moretti Litsom Werkzeug distant reading Mischung klassisch literarisch Textanalyse clos Reading Querlesen text vorarbeit Implementierung Som Bestandteil Litsom Som Feature Extraction basierend wortfrequenz Visualisierung mittels Unified Distance matrix Java implementieren Feature Extraction sergej sharovs Liste häufig russisch Wörter verwenden folgen zusammenstellen Roman Wörter zählen jeweils Gesamtanzahl Wörter Roman dividieren gewährleisten einzeln untereinander vergleichbar bleiben setup experiment verschieden Läng testen features einfluss verschieden Wortarten resultierend Karte untersuchen sharovs ursprüglich Liste modifziert versionen rein nomen verben Kontrollgruppe wörtern erstellen aufgrund testreihen Som Neuron Hexagonalen Gitter entscheiden lernrat setzen folgen verringern t t anfänglich Nachbaschaftsradius festlegen Zyklus Radius verringern unüberwacht Zykl folgen überwacht zyklen insgesamt experiment durchgeführt ebensovieler Karte erstellen Tabelle Rang korrekt identifizieren verben nomen verben Sharov nomen Sharov Sharov nomen verben nomen nomen verben resultat Klassifizierung autorinn Autor verschieden Konfiguratione Leave One out Cross Validation Loocv resultieren Gesamtanzahl verschieden experimenten Resultat som Tabelle anführen gut Resultat erkennen Liste häufig verben erzielen Resultat legen schluss nahe Litsom produziert Kart Verteilung romane tatsächlich widerspiegeln Kontrolle fungieren sogar Genauigkeit erreichen Qualität Karte subjektiv bestimmen präsentieren Karte samt Interpretation anzumerken einzeln Karte Unterschied feststellen gleichen Karte meist grundlegend Struktur Grafik Som Viktor Pelevin Ananaswasser fein dame Grafik zeigen Litsom Viktor Pelevins Roman Ananaswasser fein dame Som darstellen Karte basierend Patternvektor häufig nomen erstellen Pelevin Roman dienen unbekannt Training Roman Text korrekt klassifizieren sehen weisen Litsom Romane eher untypisch jeweilig Autorin jeweilig Autor Beispiel Vladimir Sorokin Schlange kennzeichnen och jeweilig relationen unterschiedlich autorinn Autor zueinander nachvollziehbar liegen Sergej Lukjanenko Viktor Pelevin nebeneinander letzter Test finden sozusagen realistisch Bedinung Pelevin Neuester Roman Batman apollo März publizieren Großteil experiment abschließen Eingang ursprünglich Textkorpus finden Grafik finden batman apollo betman rot jung Datum empire v Ampir blättern Roman erfahren Batman apollo Fortsetzung empire v Diskussion Ergebnis Klassifizierungsexperimente Lvq Anbetracht Tatsache literarisch Text komplex Wortfrequenz erlauben Roman unterschiedlich Autorinn Autor differenzieren Liste häufig verben Roman korrekt jeweilig autorinn Autor zuordnen empirisch belegen Litsom Beziehung Text unterschiedlich autorinn Autor sinnvoll darstellen Visualisierung mittels Litsom Verfügung stellen erlauben relationen unterschiedlich Text einfach Form darstellen subjektiv Bewertung Karte zeigen Wahl features Einfluss visuell Qualität Karte Cluster einzeln autorinn Autor basierend Karte voneinander abtrennen wiederum Klassifizierungsexperiment geeignet optisch Karte klar strukturieren eignen Karte Text finden Autor Autorin untypisch Stil Autorin Autor ähneln innerhalb Cluster lassen interessant schlüsse hinsichtlich Text ziehen häufig Unterschied geschrieben Text späteren Litsom vielerlei Hinsicht verbessern Kombinatione denkbar bestimmen können weiters denkbar visuell Karte automatisieren Bildverarbeitungsalgorithm vergleichen objektiver Beurteilung erreichen einfluss unterschiedlich kartengrößen visuell Qualität Karte hinreichend untersuchen wertvoll einblicke können Einbeziehung weit Text unterschiedlich literarisch epoch gewinnen gleichzeitig all experiment helfen Erfahrung Umgang Karte gewinnen Erfahrung wichtig schlussendlich Mensch Karte interpretieren Ausgangspunkt Überlegung nutzen Prozess distant reading clos Reading verbinden quellen Markov Primer Statisticheskogo issledovaniia Nad Tekstom evgeniia Onegina illiustriruiushchii Sviaz ispytanii v tsep äô izvestiia imperatorskoi akademii nauk Shannon Ce and Weaver The mathematical Theory -- Communication university of illinois press Illinois kohonen maps Berlin Lagus Et Websom for textual data Mining artificial intelligence Review viewdoc Summary accessed march kohonen et Self Organization of massiv Document Collection ieee Transaction -- Neural Network pdf accessed march moretti conjectures on world Literature New Left Review accessed November Kirschenbaum The remaking -- reading Data Mining and the digital Humanitie national science Foundation Symposium -- next Generation -- data Mining and cyberenabled Discovery for innovation pdf accessed may kohonen maps Berlin Sharov chastotnyi slovar Äô Rosnii ii Website Http,"[('karte', 0.44670851339031337), ('som', 0.3812533215219279), ('litsom', 0.3540209414132188), ('autorinn', 0.16677932021741346), ('roman', 0.1641270483774631), ('nomen', 0.1610850342194301), ('verben', 0.135664813242008), ('autor', 0.11871092574362199), ('sharov', 0.10892952043483654), ('pelevin', 0.10892952043483654)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Das artifizielle Manuskriptkorpus TASCFE,Armin Hoenen (Goethe Universität Frankfurt am Main),"Stemma, Stammb‚âà‚Ä†ume, R, automatische Erkennung, Distanz, Autoren Average Sign Distance","Stemma, Stammb‚âà‚Ä†ume, R, automatische Erkennung, Distanz, Autoren Average Sign Distance","1 Abstrakt In diesem Paper soll das Teheran Artificial Shahname Corpus with Frankfurt Extension (TASCFE) digitalisierter handschri_licher Texte zur Evaluation automatisch generierter Stemmata vorgestellt werden. Ein Stemma codicum oder kurz Stemma ist eine Visualisierung der genealogischen Zusammenhänge innerhalb eines Manuskriptkorpus oder einfacher gesagt ein Manuskriptstammbaum. Die Generierung solcher Stammbäume verfolgt generell zwei Hauptziele: ein genaueres Verständnis der Überlieferungsgeschichte und die Rekonstruktion eines Urtextes. Bis in die 90er Jahre hinein wurden Stemmata vornehmlich manuell erstellt, sind aber seitdem zunehmend auch automatisch generiert und analysiert worden, siehe u.a. Spencer et al. (2004), Roos and Heikkilä (2009) und Roelli and Bachmann (2010)._ Technologisch ist die bio-informatische Phylogenie Donordisziplin, wie die Nutzung phylogenetischer Programme und Algorithmen zur automatischen Manuskriptstammbaumerstellung zeigt. Dabei fehlt es in der biologischen Phylogenie an Möglichkeiten, erzeugte Stammbäume zu evaluieren, da die Aufspaltungsvorgänge der Spezies, die durch die Verzweigungen symbolisiert werden nicht beobachtet und aufgezeichnet werden konnten, lagen sie doch z.T. Millionen von Jahren in der Vergangenheit. Im Gegensatz dazu ist es in der Stemmatologie durchaus möglich, sowohl die Vorlage als auch die Kopie im Korpus vorzufinden. Mehr noch, es ist möglich neue Korpora zu erzeugen und gleichzeitig die Kopiergeschichte der Manuskripte aufzuzeichnen. Diese Daten können dann in einem klassisch informatischen Evaluationsszenario der Beurteilung von stem_Für eine detaillierte Darstellung der historischen Entwicklung der Stemmatologie siehe O""Hara (1996), Robinson and O""Hara (1996), van Reenen et al. (1996) und van Reenen et al. (2004). 1 Text Sprache Anzahl Manuskripte Anzahl Worte Publikation Parzival Englisch 21 957 Spencer et al. (2004) Notre Besoin Französisch 13 1029 Ph.V. Baret (2004) Heinrichi Altfinnisch 64 1208 Roos and Heikkilä (2009) Shahname Persisch 50 107 Hoenen (hic ipsum) Abbildung 1: Die artifiziellen Traditionen magenerierenden Methoden genutzt werden. Artifizielle Korpora wurden bisher drei Mal erzeugt, siehe Ph.V. Baret (2004), Spencer et al. (2004) und Roos and Heikkilä (2009). Nur das letztgenannte Paper evaluierte mehrere stemmagenerierende Algorithmen, darunter auch die händische Rekonstruktion, mi_els einer Distanzfunktion zwischen dem echten Stemma und den erzeugten. Diese Distanz nannten die Autoren Average Sign Distance (ASD). Sie misst die Öhnlichkeit der Topologien des korrekten und des erzeugten Stammbaums anhand der Öhnlichkeit der inneren Abstände aller Knotentripel (von vorhandenen Manuskrip_exten) im erzeugten mit deren shortest-path Abständen im echten Stammbaum. Abbildung 1 fasst Kennwerte der drei artifiziellen Korpora zusammen. Alle bisher bekannten artifiziellen Traditionen sind im lateinischen Alphabet verfasst. Hier wird das TASCFE Korpus vorgestellt, welches in persischer Sprache (Farsi) im arabischen Alphabet vorliegt. Neben der Sprache besteht seine Besonderheit _ür eine Bereicherung der Landscha_ artifizieller Korpora darin, orale Variation zu approximieren. Orale Variation ist solche Variation, die nicht aufgrund von Fehlern im Kopierprozess, sondern aufgrund der Dynamik mündlicher Überlieferung entstanden ist und die zum Teil stark von erstgenannter Variation abweicht. Die Oral Formulaic _eory (OFT) wurde in den 30er bis 60er Jahren des vorigen Jahrhunderts durch Parry and Parry (1987) und Lord (1960) im Zusammenhang mit der Homerischen Frage erarbeitet. Ergebnis dieser _eorie war u.a. die Erkenntnis, dass Texte wie die Odyssee keinen Urtext, d.h. keine Originalversion besitzen. In der Zeit vor Erfindung der Schri_ wurden Texte ausschließlich oral tradiert. Dabei war zur konkreten Textmanifestation ein Au_ührender und (mindestens ein) Zuhörer notwendig. Da die Umstände jeder Au_ührung jedoch unterschiedlich waren, war es so auch der Text selbst. Z.B. nutzte ein Barde bei derselben Geschichte viele Ausschmückungen (z.B. Adjektive), wenn er viel Zeit ha_e, erzählte sie jedoch ein anderes Mal, wo die Zeit drängte, ohne Ausschmückungen. Dazu kommen Fehler des menschlichen Erinnerungsapparates, der andersartige Variationen erzeugt, als solche, die beispiels2 weise durch Buchstabenverwechslung beim Abschreiben zu Stande kommen. Zu Beginn der Schri_ein_ührung wurden Texte via ""Pseudo-Au_ührungen"" vor einem Schreiber (Diktate) erstmals in schri_liche Form über_ührt. Da derselbe Text mehrfach in solchen Diktaten aufgezeichnet worden sein kann, da weiterhin jede Au_ührung ganz wie in der rein oralen Welt dieselbe Geschichte in unterschiedlicher Textform (mehr Ausschmückungen/weniger Ausschmückungen u.a. Arten oraler Varianz) hervorbrachte, können am Beginn mancher (meist der frühesten) Manuskrip_raditionen Varianten stehen, die sich nicht mit den Arten an Variation aus rein literarisch überlieferten (d.h. als schri_liche Texte entstandenen) Texten decken. Solch eine Variation ist _ür die Stemmagenerierung wichtig, da sie determiniert, ob ein einziges oder mehrere Stemmata und ob mehrere oder nur ein Urtext angenommen werden müssen. Das TASCFE Korpus trägt dieser Art der Variation zuminndest teilweise Rechnung, da an seinem Anfang vier verscheidene Versionen stehen. Neben der Sprache ist dies die zweite stemmatologierelevante Besonderheit des TASCFE. Der Text ist ein Auszug (Strophe) aus dem persischen Nationalepos Shahname (Buch der Könige), (l Qasim Ferdoussi, 1967, p.55). Es entstand um das Jahr 1000. Die Autorenscha_ wird generell Abu l-Qasim Ferdoussi zugerechnet, wobei orale Einflüsse im Werk bereits seit längerem diskutiert werden, siehe u.a. Yamamoto (2003) und Rubanovich (2011) . Die ca. 6.500 Token des Korpus wurden 2014 in Teheran (43 Manuskripte) und in Frankfurt (7 Manuskripte) von Freiwilligen entweder von einer gedruckten oder einer handschri_lichen Vorlage durch Abschreiben produziert. Anschließend wurde das Korpus digitalisiert und aligniert. Ein des Persischen nicht mächtiger Freiwilliger kopierte zusätzlich eines der handgeschriebenen Manuskripte, um der _ese nachzugehen, dass in historischer Zeit Analphabeten oder Schreiber anderer Schri_en Manuskripte kopiert haben könnten, was sich aber deshalb als unwahrscheinlich erwies, da es einer persischen Mu_ersprachlerin aufgrund der partiellen Unlesbarkeit nicht möglich war von dem so kopierten Manuskript eine weitere Kopie anzufertigen. Kein einziges Manuskript entsprach genau der Vorlage. Eine qualitative Analyse der Phänomene, die denen historischer Korpora ähnelten (so z.B. Zeilensprünge oder Wortsprünge, aber auch synonymische Ersetzungen) konnte zeigen, dass aufgrund des Schri_systems und der daraus teils zur lateinischen Schri_ unterschiedlichen Fehler eine andere Di_erenzierung in Fehlerklassen je nach 3 Schri_sytsem notwendig sein kann. Dies knüp_ an Andrews and Macé (2013) an, die zeigen konnten, dass Variationsklassen je nach Sprache variieren können. Das digitale Zeitalter erö_net dahingehend die Möglichkeiten einer schri_systemübergreifenden Analyse von durch Abschreibefehler verursachter Variation, die dann zur Abstraktion der dort wirkenden universalen Prinzipien beitragen wird, siehe Abbildung 2. Dies setzt die Scha_ung geeigneter Ressourcen voraus. Auf die Daten wurden im Weiteren stemmatologische Algorithmen angewandt (die dann mi_els der oben angesprochenen ASD evaluiert wurden). Hierbei konnte gezeigt werden, dass ein Ansatz zur Feststellung von Oralität durch Gruppenbildung im Stemma besteht, wobei die Levenshtein Distanz, Levenshtein (1965), bei hoher Gewichtung von Lücken im Alignment eine besonders geeignete und gleichzeitig leicht zugängliche algorithmische Basis darstellt, siehe Abbildung 3. Dabei wurde ein wortpaar-basierter Vergleich aller Manuskriptpaare durchge_ührt. Die Levenshtein Distanz aller Wortpaare des jeweiligen Manuskriptpaares wurde (auch als Baseline _ür den Vergleich mit weiteren Algorithmen) zu einer Manuskriptpaargesamtdistanz aufsummiert. Die Matrix der Manuskriptpaardistanzen wurde mi_els des Neighbor Joining Algorithmus, Saitou and Nei (1987), wiederum eine geeignete Baseline, aus dem ""ape"" Packet (Paradis et al. (2004), Paradis (2012)) der Programmiersprache R in einen Stammbaum über_ührt, der dann visualisiert und evaluiert wurde, siehe Abbildung 4._ _www.r-project.org/ 4 Abbildung 2: Die durch ungewöhnliche Buchstabenform ausgelöste Fehlkopie von (oben) nach (unten) in roten Rechtecken. Die Kennstellen, die den Abschreibefehler ausgelöst haben, sind farblich markiert. Das in ist nicht so rund wie und länger als erwartet (blau). Zudem ist der einzelne Punkt auf dem versehentlich breiter (hautfarben). Dennoch ist im oberen Rechteck eindeutig nur ein Punktmuster erkennbar, unten jedoch zwei (grün). Des Weiteren hat das zwei deutliche Hacken, wobei der mit rotem Sti_ unterlegte untere entsprechende Buchstabe nur einen aufweist. Außerdem ist das in der unteren rechten Ecke rund, was auf das vorausgehende jedoch nicht zutri_ (gelb). Obgleich der Abschreibefehler höchstwahrscheinlich durch die ungewöhnliche Form des ausgelöst wurde, passte die Ersetzung gut in den Kontext, vielleicht sogar besser als das Original. Genau diese Interaktion von kontextuellem Priming und ungewöhnlichen Buchstabenformen ist ein idealer Kandidat _ür schri_systemübergreifende Prozesse beim Abschreiben. 5 Abbildung 3: Automatische Erkennung einer durch orale Variation gekennzeichneten Gruppe. Version ASD Shahname(V1) 55, 59 Shahname(V2) 55, 13 Shahname(V3) 57, 93 Shahname(V4) 55, 83 Shahname(Durchschni_ V1-V4) 56, 12 Shahname(als eine Tradition) 38, 31 Abbildung 4: Evaluation der erzeugten Stemmata (ASD). Das Stemma der Gesamttradition unter der Annahme nur einer Wurzel evaluiert mit diesen Algorithmen deutlich schlechter als der Durchschni_ der einzelnen Versionen. 6 Literatur Andrews, T. L. and Macé, C. (2013). Beyond the tree of texts: Building an empirical model of scribal variation through graph analysis of texts and stemmas. Literary and Linguistic Computing, 28(4):504‚Äì521. l Qasim Ferdoussi, A. (1966-1967). _e Shahname - the book of kings. _e Great Islamic Encyclopaedia. Levenshtein, V. I. (1965). Binary codes capable of correcting deletions, insertions, and reversals. Doklady Akademii Nauk SSSR, 163(4):845‚Äì848. english in: Soviet Physics Doklady 10 (8) (1966) 707‚Äì710. Lord, A. B. (1960). _e Singer of Tales. Harvard University Press. O""Hara, R. J. (1996). Trees of history in systematics and philology. Memorie della Societ√† Italiana di Scienze Naturali e del Museo Civico di Storia Naturale di Milano, 27(1):81‚Äì88. Paradis, E. (2012). Analysis of Phylogenetics and Evolution with R. Springer, New York, 2nd edition. Paradis, E., Claude, J., and Strimmer, K. (2004). Ape: analyses of phylogenetics and evolution in r language. Bioinformatics, 20:289‚Äì290. Parry, M. and Parry, A. (1987). _e Making of Homeric Verse: _e Collected Papers of Milman Parry. Oxford University Press. Ph.V. Baret, C.Macé, P. (2004). Testing methods on an artificially created textual tradition. In Linguistica Computationale XXIV-XXV, volume XXIV-XXV, pages 255‚Äì281, Pisa-Roma. Instituti Editoriali e Poligrafici Internationali. Robinson, P. M. and O""Hara, R. J. (1996). Cladistic analysis of an old norse manuscript tradition. Research in Humanities Computing (4). Roelli, P. and Bachmann, D. (2010). Towards generating a stemma of complicated manuscript traditions: Petrus alfonsi""s dialogus. Revue d""histoire des textes, 5(4):307‚Äì321. 7 Roos, T. and Heikkilä, T. (2009). Evaluating methods for computer-assisted stemmatology using artificial benchmark data sets. Literary and Linguistic Computing, 24:417‚Äì433. Rubanovich, J. (2011). Medieval Oral Literature, chapter Orality in Medieval Persian Literature, pages 653‚Äì680. De Gruyter. Saitou, N. and Nei, M. (1987). _e neighbor-joining method: a new method for reconstructing phylogenetic trees. Molecular biology and evolution, 4(4):406‚Äì 425. Spencer, M., Davidson, E. A., Barbrook, A., and Howe, C. J. (2004). Phylogenetics of artificial manuscripts. Journal of _eoretical Biology, 227:503‚Äì511. van Reenen, P., den Hollander, A., and van Mulken, M. (2004). Studies in Stemmatology II. Studies in Stemmatology. John Benjamins Publishing Company. van Reenen, P., van Mulken, M., and Dyk, J. (1996). Studies in Stemmatology I. Studies in Stemmatology. John Benjamins Publishing Company. Yamamoto, K. (2003). _e Oral Background of Persian Epics. Brill, Leiden.",de,abstraken Paper Teheran artificial shahnam Corpus with Frankfurt Extension tascfe digitalisiert Text Evaluation automatisch Generierter Stemmata vorstellen Stemma Codicum stemma Visualisierung genealogisch zusammenhänge innerhalb Manuskriptkorpus einfach Manuskriptstammbaum Generierung stammbäume verfolgen generell hauptziele genauer Verständnis überlieferungsgeschichte Rekonstruktion urtextes hinein Stemmata vornehmlich manuell erstellen zunehmend automatisch neriern analysieren sehen spencer et Roos And Heikkilä Roelli And Bachmann technologisch Phylogenie Donordisziplin Nutzung phylogenetisch Programm algorithmen automatisch Manuskriptstammbaumerstellung zeigen fehlen biologisch Phylogenie Möglichkeit erzeugt Stammbäume evaluieren Aufspaltungsvorgäng Spezies Verzweigung symbolisieren beobachten aufgezeichnen liegen Million Vergangenheit Gegensatz Stemmatologie sowohl Vorlage Kopie Korpus vorzufinden Korpora erzeugen gleichzeitig Kopiergeschichte Manuskript aufzuzeichnen daten klassisch informatisch Evaluationsszenario Beurteilung detailliert Darstellung historisch Entwicklung Stemmatologie sehen o hara Robinson and o hara van reenen et van reenen et Text sprach Anzahl manuskript Anzahl wort Publikation Parzival Englisch Spencer et Notre Besoin französisch Baret Heinrichi altfinnisch Roos And Heikkilä shahnamen persisch Hoenen hic Ipsum Abbildung artifiziellen tradition magenerierende Methode nutzen artifiziell korpora mal erzeugen sehen Baret Spencer et Roos And Heikkilä letztgenannter Paper evaluieren mehrere Stemmagenerierende algorithmen händisch Rekonstruktion Distanzfunktion echt Stemma Erzeugt Distanz nennen Autor Average sign Distance asd missen Öhnlichkeit Topologie Korrekt erzeugt Stammbaum anhand Öhnlichkeit innerer abstände Knotentripel vorhanden erzeugt Abstände echt Stammbaum Abbildung fasst kennweren artifiziellen Korpora bekannt artifiziellen Tradition lateinisch Alphabet verfassen tascfe korpus vorstellen persisch Sprache Farsi arabisch Alphabet vorliegen Sprache bestehen Besonderheit ür Bereicherung Landscha artifiziell Korpora oral Variation approximieren oral Variation Variation aufgrund Fehler kopierprozess aufgrund Dynamik mündlich Überlieferung entstehen stark erstgenannt Variation abweicht Oral formulaic Eory vorig Jahrhundert Parry and Parry Lord Zusammenhang homerisch Frage erarbeiten Ergebnis Eorie Erkenntnis Text Odyssee Urtext Originalversion besitzen Erfindung Schri Text ausschließlich oral tradiern konkret Textmanifestation mindestens Zuhörer notwendig umstände unterschiedlich Text nutzen Barde Geschichte Ausschmückung adjektiv erzählen anderer Mal drängen Ausschmückung Fehler menschlich erinnerungsapparates andersartig variationen erzeugen weisen Buchstabenverwechslung abschreiben Stande Beginn Text via Schreiber Diktate erstmals Form Text mehrfach Diktat aufgezeichnen weiterhin rein oral Welt Geschichte unterschiedlich Textform ausschmückung ausschmückung Art oral Varianz hervorbracht Beginn meist Frühesten varianten stehen Art Variation rein literarisch überliefern Text entstanden Text decken solcher Variation ür Stemmagenerierung wichtig determinieren einzig mehrere Stemmata mehrere Urtext annehmen Tascfe Korpus tragen Art Variation zuminndest teilweise Rechnung Anfang verscheiden Version stehen Sprache stemmatologierelevant Besonderheit Tascfe Text Auszug Strophe persisch nationalepos shahnam Buch könig l qasim Ferdoussi entstehen Autorenscha generell abu Ferdoussi zurechnen wobei oral einflüß Werk lang diskutieren sehen Yamamoto rubanovich token Korpus Teheran manuskripen Frankfurt manuskripen Freiwillig gedruckt Vorlage abschreiben produzieren anschließend Korpus digitalisieren aligniern persisch mächtig Freiwilliger Kopiert zusätzlich Handgeschrieben manuskripte ese nachgehen historisch analphabeten schreiber anderer manuskripen kopieren können unwahrscheinlich erweisen persisch aufgrund partiell Unlesbarkeit kopiert Manuskript Kopie anfertigen einzig Manuskript entsprechen genau Vorlage qualitativ Analyse Phänomen historisch Korpora ähneln zeilensprüngen wortsprüngen synonymisch ersetzung zeigen aufgrund teils lateinisch schri unterschiedlich Fehler Fehlerklassen notwendig Knüp Andrews And Macé zeigen variationsklassen Sprache variieren digital Zeitalter dahingehend Möglichkeit Analyse abschreibefehl verursacht Variation Abstraktion wirkend universal prinzipien beitragen sehen Abbildung setzen geeignet Ressource voraus daten Stemmatologische algorithm anwenden angesprochen asd evaluiern hierbei zeigen Ansatz Feststellung Oralität Gruppenbildung Stemma bestehen wobei Levenshtein Distanz Levenshtein hoch Gewichtung Lücke Alignment geeignet gleichzeitig zugänglich algorithmisch Basis darstellen sehen Abbildung Vergleich Manuskriptpaar Levenshtein Distanz Wortpaare jeweilig Manuskriptpaares baselin ür Vergleich Algorithm Manuskriptpaargesamtdistanz aufsummieren Matrix Manuskriptpaardistanze neighbor Joining Algorithmus saitou And nei wiederum geeignet baselin ape Packet Paradis et Paradis Programmiersprache r Stammbaum visualisiern evaluieren sehen Abbildung Abbildung ungewöhnlich Buchstabenform ausgelöst Fehlkopie unten rot Rechtecke Kennstelle Abschreibefehler auslösen farblich markieren lang erwarten blau zudem einzeln Punkt versehentlich Breiter hautfarben dennoch oberer Rechteck eindeutig Punktmuster erkennbar unten grün Deutliche hacken wobei rotem Sti unterlegt unt entsprechend Buchstabe Aufweist unterer Ecke Vorausgehende Zutri gelb obgleich Abschreibefehler höchstwahrscheinlich ungewöhnlich Form auslösen passen Ersetzung Kontext sogar original genau Interaktion kontextuellem Priming ungewöhnlich buchstabenformen ideal Kandidat ür prozesse abschreiben Abbildung automatisch Erkennung oral Variation gekennzeichnet Gruppe Version asd Shahname shahnam shahnam shahname shahname durchschni shahnamen Tradition Abbildung Evaluation erzeugt Stemmata asd Stemma Gesamttradition Annahme Wurzel evaluieren algorithm deutlich schlecht Durchschni einzelner versionen Literatur andrews and Macé Beyond The Tree of texts Building empirical Model -- scribal Variation Through Graph Analysis of texts and Stemmas Literary and Linguistic Computing l qasim Ferdoussi e shahnam The Book of kings e great Islamic Encyclopaedia Levenshtein Binary Codes capable -- correcting deletions Insertion And reversals Doklady akademii nauk Sssr english Soviet Physics Doklady Lord e Singer -- Tales Harvard University press o hara trees of History systematics and Philology Memorie Della italiana di Scienze Naturali e del Museo Civico Di Storia natural Di Milano Paradis Analysis of phylogenetics and Evolution with springer New York edition Paradis Claude And Strimmer ape analyses of phylogenetics and Evolution r Language Bioinformatics Parry and Parry e making of homeric verse e Collected Papers -- milman Parry Oxford University press Baret Testing Methods -- Artificially Created textual Tradition Linguistica computationale volume pag Instituti editoriali e Poligrafici Internationali Robinson and o hara Cladistic Analysis of old Norse Manuscript Tradition research Humanities Computing Roelli And Bachmann Towards Generating stemma of Complicated manuscript Tradition Petrus alfonsi -- dialogus revue d Histoire Text Roos And heikkilä Evaluating Methods for Stemmatology Using artificial Benchmark Data Set Literary and Linguistic Computing rubanovich Medieval oral literature Chapter orality Medieval Persian literature pag de Gruyter saitou And nei e Method New Method for reconstructing Phylogenetic Trees Molecular Biology And Evolution Spencer Davidson Barbrook And howe phylogenetics of artificial Manuscripts Journal -- eoretical Biology van reenen Hollander and van mulken studies Stemmatology ii studies Stemmatology John Benjamin Publishing Company van reenen van Mulken And dyk studies Stemmatology studies Stemmatology John Benjamin Publishing Company yamamoto e oral Background -- persian epics Brill leiden,"[('and', 0.32423541707078746), ('oral', 0.2530522603313446), ('variation', 0.17909949711644882), ('stemma', 0.1377480668317752), ('parry', 0.13584157653988993), ('shahnam', 0.13584157653988993), ('stemmatology', 0.13584157653988993), ('persisch', 0.13584157653988993), ('of', 0.13519428580179352), ('van', 0.12154298763690086)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Digitale Netzwerkanalyse dramatischer Texte,Peer Trilcke (Georg-August-Universität Göttingen); Frank Fischer (Göttingen Centre for Digital Humanitie); Dario Kampkaspar (Herzog August Bibliothek Wolfenbüttel),"Netzwerkanalyse, Dramen","Netzwerkanalyse, Dramen","1 Einleitung Das Projekt ""Digitale Netzwerkanalyse dramatischer Texte"" steht in der Tradition strukturanalytischer Ansätze in der Literaturwissenschaft (allgemein Titzmann 1977), die es einerseits im Sinne eines konsequent netzwerkanalytischen Relationismus (mit Rekurs auf die Social Network Analysis, siehe u. a. Wasserman/Faust 1998), andererseits unterst¬® utzt durch Verfahren der automatisierten Datenerhebung und -auswertung weiterentwickelt, um sie auf größere Textkorpora anzuwenden und so umfassende relationale Daten ¬® uber Prozesse des literaturgeschichtlichen Strukturwandels gewinnen zu können. Als theoretisches Fundament dient dabei eine netzwerkanalytische Konzeptualisierung dramatischer Interaktion (erste Ideen dazu prominent bei Moretti 2011; Kritik und literaturtheoretisch begr¬® undete Rekonzeptualisierung bei Trilcke 2013 'dort auch ein ausf¬® uhrlicher Forschungs¬® uberblick), die 'in Fortf¬® uhrung von Konzepten der dramatischen Konfiguration (Marcus 1973, Pfister 1977; problematisch hingegen, weil mit di_user Konzeptualisierung: Pohlheim 1997) 'zunächst bei einer rudimentären Operationalisierung ansetzt, nach der eine ""Interaktion"" dann vorliegt, wenn zwei Figuren innerhalb einer durch die ¬® uberlieferte Struktur des Textes vorgegebenen Subsegmentierungseinheit (in der Regel ""Szene"" oder ""Auftritt"") als Sprecher aufgef¬® uhrt werden. ""Interaktion"" wird in diesem Sinne 'und zu Zwecken einer ersten Automatisierung 'verstanden als ""szenische Kopräsenz zweier Sprecher"". Auf Grundlage der so definierten Relation werden im Rahmen des Projekts automatisiert netzwerkanalytische Daten erhoben, die sowohl global die ""Interaktions""-Netzwerke der Dramen (Density, Average Degree, Connectedness u. dergl.) als auch fokussiert einzelne Akteure charakterisieren (Degree sowie diverse weitere Centrality-Indices). Der erstellte Workflow ermöglicht auch die Datenerhebung auf Mesoebene (u. a. Identifizierung von Clustern) und beinhaltet dar¬® uber hinaus Visualisierungen der Netzwerkdaten, die wiederum zur Analyse des literaturgeschichtlichen Strukturwandels beitragen. 2 Wahl des Dramenkorpus F¬® ur die automatisierte Analyse von Dramen war ein verlässliches und gen¬® ugend großes Dramenkorpus vonnöten. Infrage kamen hier: ‚Ä¢ Deutsches Textarchiv (DTA): 49 Dramen1 ‚Ä¢ Wikisource: 50 Dramen2 ‚Ä¢ Projekt Gutenberg-DE: 641 Dramen3 ‚Ä¢ Textgrid Repository: 690 Dramen4 1http://www.deutschestextarchiv.de 2http://de.wikisource.org/wiki/Kategorie:Drama 3http://projekt.gutenberg.de 4http://www.textgridrep.de 1 Das DTA hat zwar das qualifizierteste (TEI-)Markup, besteht aber bisher nur aus vergleichsweise wenigen Texten. Letzteres gilt auch f¬® ur die Dramen im deutschsprachigen Zweig von Wikisource. Das Projekt Gutenberg-DE wiederum, das seit 2002 bei Spiegel Online gehostet wird, hat das Problem, dass es nicht mit brauchbarem Markup versehen ist, nur rudimentärem XHTML. Deshalb kam eigentlich nur das TextGrid Repository infrage, das sich aus den alten Zeno.org-Volltexten speist und basale TEI-Auszeichnungen aufweist. Aus dem TextGrid-Gesamtkorpus wurden zunächst die in den Metadaten mit dem Genre ""drama"" versehenen Texte extrahiert, insgesamt 690. Dazu gehören vor allem deutschsprachige Dra¬® men von ca. 1500 bis 1930 sowie ferner u. a. Ubersetzungen von einem Dutzend griechischer Tragödien und einiger Shakespeare-Dramen. Aus der Gesamtmenge lassen sich prinzipiell auch recht einfach zeitlich gesta_elte Teilkorpora erstellen, denn im TEI-Header stehen innerhalb von <creation></creation> rudimentäre Entstehungsdaten (Beispiele: <date when=""1802""/>, aber auch weitläufige Eingrenzungen wie <date notBefore=""1738"" notAfter=""1758""/>). 3 Erhebung der Netzwerkdaten Als Zwischenschritt wurde f¬® ur jede der 690 TEI-Dateien eine Relationsliste (CSV-Datei) erzeugt, die den gängigen Formaten der Speicherung netzwerkanalytischer Daten entspricht. Zur Extraktion der Sprecherdaten sind in der Regel zwei getrennte Schritte nötig: Das Erkennen der einzelnen Teile eines Theaterst¬® uckes und danach das Erkennen der einzelnen Sprecher. Zur Erleichterung der nachstehenden Arbeiten teilt das Skript die vorliegenden Dateien auf: F¬® ur jede erkannte Ebene (die Datei selbst ist dabei auch eine) wird ein Unterverzeichnis angelegt, in dem wiederum TEI-Dateien mit den einzelnen Teilen stehen und in das auch die jeweiligen Registerdateien geschrieben werden. Anhand der aufgeteilten Dateien werden verschiedene Arten von Ausgaben erstellt. Zum einen ist dies ein kleinteiliges Register aller <speaker>-Tags, aber auch aller Auszeichnungen <rs> und <person>. Um eindeutige Referenzziele zu erhalten, werden ggf. ID-Nummern vergeben (dies erleichtert insbesondere auch spätere Eingri_e, wenn problematische Namen manuell korrigiert werden m¬® ussen). Zum andern werden die Kookkurrenzlisten erstellt. Im untersten Verzeichnis werden die Vorkommen aller Sprecherpaare in allen Dateien gezählt. In den dar¬® uberliegenden werden die Werte aller Unterverzeichnisse addiert. Neben dem Erkennen der Struktur ist die korrekte Zuordnung von Namen die größte Herausforderung. Im Idealfall sind alle <speaker> mit einem Attribut @who versehen, ¬® uber das eine normierte Form des Namens erreicht werden kann. Ist dies nicht der Fall (oder sind stattdessen die Tags <rs> oder <person> verwendet worden), muss das Skript den Textinhalt des Tags auswerten, wobei neben möglichen Verschreibungen (bei der Transkription oder in der Vorlage) auch syntaktische änderungen auftreten können. So findet sich bei Lessing, Nathan der Weise V/1, neben Saladin ""Ein Mameluck"", der nach seinem ersten Auftreten als ""Der Mameluck"" gef¬® uhrt wird. Es folgt ein weiterer: ""Ein zweiter Mameluck"", danach ""zweiter Mameluck"". Ist es hier noch möglich, durch ein Ber¬® ucksichtigen der Artikel mit einfachen Mitteln gute Ergebnisse zu erzielen, stellt sich dies bei Emilia Galotti etwas schwieriger dar, da teils nur ""Odoardo"", teils aber ""Odoardo Galotti"" erscheint. Auch Fälle mit mehreren Sprechern (z. B. ""Alle"") sind nicht ganz trivial zu bearbeiten. Neben dem Versuch, diese Fälle automatisch zu klären, besteht in diesen Zweifelsfällen aber immer noch die Möglichkeit des manuellen Eingri_s, wozu die erstellten Indexdateien mit eindeutiger ¬® ID beitragen können. In einer weiteren Uberarbeitung des Skriptes ist es vorgesehen, eine einfache graphische Oberfläche anzubieten, ¬® uber die solche Zweifelsfälle bearbeitet werden können. 4 Datenauswertung und Visualisierung Die Datenauswertung erfolgt ¬® uber Python (3.4.x) mit dem igraph-Paket, das sowohl zum Visualisieren der Graphen als auch zum Berechnen der netzwerkanalytischen Daten genutzt wird. F¬® ur eine erste Visualisierung des Datenbestands wurden die Graphdaten an eine spring-embeddingMethode ¬® ubergeben (Fruchterman-Reingold), die versucht, a_ne Knoten näher beieinander anzuordnen und dadurch deutlich sichtbar zu clustern. Einen Eindruck des gesamten Korpus vermittelt 2 Abbildung 1, die 671 Dramen aus 2500 Jahren Dramengeschichte enthält, chronologisch links oben mit den Griechen beginnend und bis rechts unten ins zweite Viertel des 20. Jahrhunderts reichend: Abbildung 1: Netzwerkgraphen von 671 Dramen aus dem TextGrid Repository Die visualisierten Graphen haben auch deutlich gemacht, dass die meisten berechneten CSVDateien wegen des teils nicht eindeutigen Markups zumindest kleine Fehler aufwiesen. Diese Erkenntnisse konnten zur Fehlerbehandlung an den vorhergehenden Schritt (Erhebung der Netzwerkdaten) zur¬® uckgegeben werden. Erste strukturanalytische Berechnungen erfolgten auf Basis der 12 (vollendeten) Lessing-Dramen. Entsprechende Diagramme sind in Abbildung 2 zu finden. 5 Ausblick Die erhobenen und bereinigten Netzwerkdaten sollen als Grundlage f¬® ur alle statistischen Berechnungen dienen und auch ö_entlich zur Verf¬® ugung gestellt werden. Im Mittelpunkt der Forschung steht nun die Implementierung zusätzlicher netzwerkanalytischer Berechnungstools (etwa zur Bestimmung der Betweenness Centrality, mit der die Wichtigkeit einzelner Figuren f¬® ur das Netzwerk bestimmt werden kann). Dar¬® uber hinaus wird an der Qualifizierung der Netzwerkdaten gearbeitet (außer dem reinen Fakt, dass Figuren miteinander sprechen: Redeanteile quantifizieren, B¬® uhnenpr äsenz nicht sprechender Personen mit einbeziehen usw.) sowie an der Erstellung multiplexer Netzwerke, die nicht nur die oben definierten ""Interaktions""-Relationen erfassen, sondern auch u. a. Verwandschafts- oder instrumentelle Beziehungen ber¬® ucksichtigen. 3 Abbildung 2: Beispielberechnungen anhand der Lessing-Dramen (x-Achse): Damon, 1747 'Der junge Gelehrte, 1747 'Der Misogyn, 1748 'Die alte Jungfer, 1748 'Der Freigeist, 1749 'Die Juden, 1749 'Der Schatz, 1750 'Miß Sara Sampson, 1755 'Philotas, 1759 'Minna von Barnhelm, 1767 'Emilia Galotti, 1772 'Nathan der Weise, 1779 Literatur Marcus, Solomon. Mathematische Poetik. Frankfurt/M. 1973. Moretti, Franco. Network Theory, Plot Analysis. Stanford Literary Lab Pamphlets 2 (1. 5. 2011). http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf. Pfister, Manfred. Das Drama. Theorie und Analyse. M¬® unchen 1977 u. ö. Pohlheim, Karl Konrad (Hg.). Die dramatische Konfiguration. Paderborn u. a. 1997. Titzmann, Michael. Strukturale Textanalyse. Theorie und Praxis der Interpretation. M¬® unchen 1977 u. ö. Trilcke, Peer. Social Network Analysis (SNA) als Methode einer textempirischen Literaturwissenschaft. In: Philip Ajouri, Katja Mellmann u. Christoph Rauen (Hg.): Empirie in der Literaturwissenschaft. M¬® unster 2013. S. 201‚Äì247. Wasserman, Stanley; Katherine Faust. Social Network Analysis. Methods and Applications. Cambridge u. a. 1998.",de,Einleitung Projekt digital Netzwerkanalyse dramatisch Text stehen Tradition strukturanalytisch Ansatz Literaturwissenschaft allgemein Titzmann einerseits Sinn konsequent netzwerkanalytisch Relationismus Rekurs Social Network Analysis sehen Wasserman Faust andererseits utzt Verfahren automatisiert Datenerhebung weiterentwickeln groß Textkorpora anwenden umfassend relational daten uber prozeß literaturgeschichtlich Strukturwandel gewinnen theoretisch Fundament dienen netzwerkanalytisch konzeptualisierung dramatisch Interaktion Idee prominent Moretti Kritik literaturtheoretisch undet Rekonzeptualisierung Trilcke uhrlich Uberblick Uhrung Konzept dramatisch Konfiguration Marcus pfister problematisch hingegen Konzeptualisierung Pohlheim rudimentär Operationalisierung ansetzen Interaktion vorliegen Figur innerhalb uberliefert Struktur Text vorgegebenen Subsegmentierungseinheit Regel Szene auftreten sprech uhren Interaktion Sinn zwecken Automatisierung verstehen szenisch Kopräsenz zwei Sprecher Grundlage definiert Relation Rahmen Projekt automatisieren netzwerkanalytisch daten erheben sowohl global dramen density Average Degree Connectedness Dergl fokussieren einzeln Akteur Charakterisieren Degree diverser erstellt Workflow ermöglichen Datenerhebung mesoeben Identifizierung clustern beinhalten uber hinaus visualisierung netzwerkdaten wiederum Analyse literaturgeschichtlich Strukturwandel beitragen Wahl Dramenkorpus ur automatisiert Analyse dramen verlässlich ugend dramenkorpus vonnöten Infrage kommen deutsch Textarchiv dta Wikisource projekt Textgrid repository dta qualifiziertest bestehen vergleichsweise weniger Text letzterer gelten ur Dram deutschsprachig Zweig Wikisource Projekt wiederum Spiegel Online hosten Problem brauchbar Markup versehen Rudimentärem xhtml eigentlich Textgrid Repository Infrage alt speisen basal aufweist metadaten Genre Drama versehen Text extrahiern insgesamt gehören deutschsprachig Men ferner Ubersetzung Dutzend griechisch Tragödie Gesamtmenge lassen prinzipiell einfach zeitlich Teilkorpora erstellen stehen innerhalb rudimentär Entstehungsdat Beispiel dat weitläufig eingrenzungen dat Erhebung netzwerkdaten Zwischenschritt ur relationsliste erzeugen gängig formaten Speicherung netzwerkanalytisch daten entsprechen Extraktion Sprecherdate Regel getrennt Schritt nötig erkennen einzeln Teil uck erkennen einzeln Sprecher Erleichterung nachstehend Arbeit teilen Skript vorliegend dateien ur erkennen Ebene Datei Unterverzeichnis anlegen wiederum einzeln Teil stehen jeweilig Registerdateie schreiben anhand aufgeteilt dateien verschieden Art Ausgabe erstellen kleinteilig Register auszeichnung rs Person eindeutig Referenzziele erhalten vergeben erleichtern insbesondere spät problematisch Name manuell korrigieren ussen kookkurrenzlisten erstellen unter Verzeichnis Vorkommen Sprecherpaar Datei zählen uberliegenden Wert Unterverzeichnisse addieren Erkenn Struktur korrekt Zuordnung Name groß Herausforderung Idealfall speaker Attribut versehen uber normiert Form Namen erreichen Fall stattdessen tags rs Person verwenden Skript Textinhalt tags auswerten wobei möglich verschreibungen Transkription Vorlage syntaktisch Änderung auftreten finden Lessing Nathan Weise v Saladin Mameluck auftreten Mameluck uhren folgen weit Mameluck Mameluck ucksichtigen Artikel einfach Mittel Ergebnis erzielen stellen Emilia Galotti schwierig dar teils odoardo teils odoardo Galotti erscheinen Fall mehrere Sprecher trivial bearbeiten Versuch Fall automatisch klären bestehen zweifelsfällen Möglichkeit manuell wozu erstellt indexdateien eindeutig id beitragen Uberarbeitung Skript vorsehen einfach graphisch oberfläch anbieten uber Zweifelsfälle bearbeiten Datenauswertung Visualisierung Datenauswertung erfolgen uber python sowohl visualisieren graphen berechnen netzwerkanalytisch daten nutzen ur Visualisierung Datenbestand Graphdat Ubergeb versuchen Knoten nah beieinand anordnen deutlich sichtbar clustern Eindruck gesamt Korpus vermitteln Abbildung Dram dramengeschichen enthalten chronologisch links griechen beginnend rechts unten Viertel Jahrhundert reichend Abbildung netzwerkgraphen Dram Textgrid Repository Visualisiert graphen deutlich meister berechnet Csvdateie teils eindeutigen Markup zumindest Fehler aufweisen Erkenntnis Fehlerbehandlung vorhergehend Schritt Erhebung netzwerkdaten uckgegeben strukturanalytisch berechnung Erfolgt Basis vollendet entsprechend diagramme Abbildung finden Ausblick erhoben bereinigt netzwerkdaten Grundlage ur statistisch berechnung dienen Ugung stellen Mittelpunkt Forschung stehen Implementierung zusätzlich netzwerkanalytisch Berechnungstools Bestimmung Betweenness Centrality Wichtigkeit einzeln figuren ur Netzwerk bestimmen uber hinaus Qualifizierung netzwerkdaten arbeiten rein Fakt Figur miteinander sprechen Redeanteil quantifizieren uhnenpr äsenz sprechend Person einbezieh Erstellung Multiplexer netzwerken Definiert erfassen instrumentell beziehung ucksichtigen Abbildung beispielberechnungen anhand damon jung gelehrt Misogyn alt Jungfer Freigeist Jude Schatz miß Sara Sampson Philotas minna barnhelm Emilia Galotti Nathan Weise Literatur Marcus Solomon mathematisch Poetik Frankfurt moretti Franco Network Theory Plot Analysis Stanford Literary Lab Pamphlet Pfister Manfred Drama Theorie Analyse unchen Pohlheim Karl Konrad hg dramatisch Konfiguration Paderborn Titzmann Michael struktural Textanalyse Theorie Praxis Interpretation unchen trilcken peer Social Network Analysis sna Methode textempirisch Literaturwissenschaft Philip Ajouri Katja Mellmann Christoph Rauen hg Empirie Literaturwissenschaft unst Wasserman Stanley katherine Faust Social Network Analysis Methods And Applications Cambridge,"[('ur', 0.27974863101475334), ('uber', 0.2574386019630514), ('netzwerkanalytisch', 0.1832134063366698), ('netzwerkdaten', 0.17467060341053942), ('mameluck', 0.17162573464203426), ('network', 0.11126610108775177), ('dramatisch', 0.10202239303968572), ('galotti', 0.09853948420438445), ('repository', 0.09597533149969222), ('skript', 0.0916067031683349)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Wann findet die deutsche Literatur statt? Zur Untersuchung von Zeitausdr¬® ucken in großen Korpora,Frank Fischer; Jannik Strötgen,"Korpus, Zeitangaben","Korpus, Zeitangaben","1 Einleitung Exakte Datumsangaben sind ein Merkmal vieler Prosatextsorten. In der Literatur finden sich dagegen bevorzugt ungefähre Datumsangaben, die Interpretationsräume ö_nen. So sind etwa alle 19 Monatsnennungen in Theodor Storms Schimmelreiter ungefährer Natur (""an einem OctoberNachmittag"", ""Ende März"" usw.). Ausnahmen von dieser Regel bilden Tagebuch-, Brief-, Abenteuerund historische Romane: In Goethes Werther oder Jules Vernes Tour du monde en quatre-vingts jour wimmelt es genretypisch von exakten Datumsangaben, im letztgenannten Roman sind sie in Form eines Countdowns gar unentbehrlicher Teil der Handlung. Ansonsten lässt sich als Hypothese formulieren: Kommt in der erzählenden Literatur ein exaktes Datum vor, ist das eine narrative Setzung, die der näheren Analyse lohnt. Davon ausgehend lassen sich dezidiert literaturwissenschaftliche Fragen ins Blickfeld nehmen: ‚Ä¢ Ist die Vermeidung exakter Datumsnennungen tatsächlich ein durchgehendes Merkmal bestimmter literarischer Genres? In welchem Verhältnis stehen exakte zu ungefähren Datumsangaben? ‚Ä¢ Kann die Frequenzanalyse von Datumsangaben zu Genreuntersuchungen genutzt werden? ‚Ä¢ Welche andere Bedeutung haben Datumsangaben neben der zeitlichen Verortung der Handlung? (""Semiotisierung"" eines Datums) ‚Ä¢ Gibt es zeitliche Konjunkturen f¬® ur bestimmte Datumsnennungen? Wenn ja, warum? ‚Ä¢ Welche Rolle spielen fiktive Daten? (vgl. etwa Erich Kästners 35. Mai und Shakespeares 80. April in The Winter""s Tale, Autolycus"" Ballade im 4. Akt) In unserem Vortrag widmen wir uns den Datumsangaben als einem isolierbaren feature literarischer Korpora im Sinne Matthew Jockers: ""Indeed, the very object of analysis shifts from looking at the individual occurrences of a feature in context to looking at the trends and patterns of that feature aggregated over an entire corpus"" (Jockers 2013, S. 24). Anhand dieses Features (der expliziten Datumsnennung) als einzeln betrachtbare Einheit soll die Praxis der literaturwissenschaftlichen Makroanalyse methodisch bereichert werden. Die Extraktion der Zeitangaben erfolgt automatisiert mithilfe eines Temporal Taggers. Indem es dabei um die Untersuchung der Repräsentanz eines außerliterarischen Phänomens (Zeit, Datumsangaben) in literarischen Texten geht, wird auch ein Beitrag zur Erzählforschung geleistet. Die mit den Methoden der Digital Humanities erzielten Erkenntnisse werden dadurch in die Fachdisziplin (in diesem Fall die Literaturwissenschaften) zur ¬® uckgespielt. 1 2 Vorgehen Unser Vorgehen bestand aus vier meist parallel ablaufenden Schritten: 1. Zusammenstellung geeigneter (deutschsprachiger) Korpora. 2. Erhebung der Daten durch Einsatz des Temporal Taggers HeidelTime (Strötgen & Gertz 2012) zur automatischen Extraktion zeitlicher Ausdr¬® ucke im Sinne der Temporal Markup Language TimeML (Pustejovsky et al. 2003). 3. Analyse der Daten (von Heatmaps zu Einzelfällen). 4. Entwicklung einer Android-App zur explorativen Analyse des ""literarischen Jahres"". Zunächst haben wir mit dem TextGrid Repository1 und Gutenberg-DE2 zwei große literarische Korpora zusammengebracht, mit HeidelTime auf Datumsstrukturen untersucht und anhand der expliziten (und damit sehr sicher richtigen) Ausdr¬® ucke eine kalendarische Heatmap erstellt (""1"" bedeutet 0–9 Vorkommen, ""2"" bedeutet 10–19 Vorkommen usw., ""+"" bedeutet 90 oder mehr Vorkommen). Dabei zeigten sich erwartete und unerwartete Konjunkturen: JAN: +566554435455576554574445555455 FEB: 65655546454635554446554666762 MAR: 84553334565364646+4644435444465 APR: +55555344664466554465555646447 MAY: +777765557566565674836454565466 JUN: 957486574646656657586656444576 JUL: 9479486554468975676654555565465 AUG: +6+565555+5665+6974755775555556 SEP: 9745735555446575+7695554546457 OCT: +375564536445665+76734555645555 NOV: +68557546656549665554645545456 DEC: 77455755464455547644554+5533457 Wie man sieht, kommen Monatserste und fixe Feiertage (Neujahr, Weihnachten) besonders häufig vor. Ansonsten fiel die Konjunktur weiterer Tage auf, etwa des ""18. März"". Die Vermutung lag nahe, dass die Nennung dieses Datums in Werken nach 1848 und damit nach der Märzrevolution ansteigt, da dieser Tag wegen der blutigen Ereignisse in Berlin eine eigene Semantik annahm. Die Erhebung und Analyse solcher Datumskonjunkturen soll systematisch ausgebaut werden. 3 Untersuchung des DTA-Korpus Dieser ersten quantitativen Analyse lagen die beiden erwähnten Korpora zugrunde, in denen aber äuch nichtliterarische Werke und (v. a. bei Gutenberg-DE) auch Ubersetzungen fremdsprachiger Literatur vorkommen. Um mit dieser Methode belastbare Ergebnisse zu gewinnen, bedurfte es eines qualifizierteren Textkorpus. Das Deutsche Textarchiv (DTA) ist zwar weit weniger umfangreich, aber es beinhaltet (nomen est omen) nur original deutschsprachige Texte und ist sowohl grob zeitlich 'nach Jahrhunderten 'als auch thematisch 'nach Genres 'sortiert, sodass dort Hinweise auf Konjunkturen in literarischen Texten zu erwarten waren. In Tabelle 1 sind Informationen zu den DTA-Teilkorpora dargestellt. Die Anzahl der enthaltenen Texte stellt zwar letztlich keine kritische Masse f¬® ur large-scale Untersuchungen wie die unsere dar, wir versprachen uns aber weitere Hinweise auf Chancen und Grenzen der Methode. Eine Analyse des einzeln abrufbaren Belletristik-Korpus ergab eine weit weniger stabile Heatmap als oben und als häufigste Daten (zwischen ca. 20 und 70 Nennungen) die folgenden: 1. 1., 1. 4., 20. 4., 1. 5., 10. 8., 3. 11. Die DTA-Stichprobe hebt wieder die Bedeutung des 1. Mai heraus, unter den unerwarteten Daten sei der ""10. August"" herausgegri_en, der neben der zeitlichen Verortung fiktionaler Handlungen wiederum auch auf ein historisches Datum zur¬® uckzuf¬® uhren ist, den Tuileriensturm am 10. August 1792 (vgl. in B¬® uchners Dantons Tod : ""Erster B¬® urger: Danton war unter uns am 10. August, 1http://www.textgridrep.de/ 2http://gutenberg.spiegel.de/ 2 TimeML explizite Dokumente Sätze Tokens Zeitausdr¬® ucke Tagausdr¬® ucke 1600–1699 124 957,950 15,779,536 34,425 635 1700–1799 341 1,866,106 30,077,557 97,219 1,531 1800–1899 559 3,565,758 60,750,795 289,697 18,644 Belletristik 401 1,774,209 29,303,975 106,988 3,366 Gebrauchsliteratur 103 624,829 11,041,708 35,079 787 Wissenschaft 532 4,103,679 68,268,553 298,195 17,170 Tabelle 1: Informationen zu den Teilkorpora des DTA-Korpus. Danton war unter uns im September.""). Das DTA-Korpus ist aber, wie gesagt, relativ klein (nur 401 belletristische Werke, wobei Belletristik hier neben Fiktionalem auch Reiseberichte und Lebenserinnerungen einschließt) und nicht sehr belastbar f¬® ur Korpusanalysen. 4 Bedeutung f¬® ur die Literaturwissenschaft Es wäre analog zu Hans Ulrich Gumbrechts Untersuchung 1926 denkbar und w¬® unschenswert, dass man die Literaturgeschichte einzelner Tage schriebe. Dass jedes Datum seine eigene Literaturgeschichte hat, dies also in Ansätzen schon untersucht wird (allerdings noch ohne Korpusanalysen ö. A.), zeigt das Beispiel Paul Celan und der ""20. Jänner"". In Celans Prosagedicht Gespräch im Gebirg, 1960 in der Neuen Rundschau (Heft 2) erschienen, wird auf Georg B¬® uchners Erzählung Lenz angespielt, in der ebenfalls ein Gang durchs Gebirge geschildert wird. B¬® uchners Text beginnt mit dem Satz: ""Den 20. [Jänner] ging Lenz durchs Gebirg."" Dass Lenz"" Wanderung durchs Gebirge am 20. Jänner stattfindet, darauf weist auch Celan in seiner B¬® uchner-Preis-Rede Der Meridian hin. Er erweitert den Anspielungsraum noch, indem er auf einen weiteren 20. Januar verweist, nämlich den des Jahres 1942, als in Berlin die Wannseekonferenz stattfand. Und er folgert: ""Vielleicht darf man sagen, daß jedem Gedicht sein ""20. Jänner"" eingeschrieben bleibt?"" (vgl. Sieber 2007, S. 114_). Die computergest¬® utzte Erhebung von Zeitangaben aus großen Korpora macht solche Gleichzeitigkeiten sichtbarer und ermöglicht so auch deren systematische Erforschung. 5 Android-App zur Exploration expliziter Zeitausdr¬® ucke in der Weltliteratur Um eine Idee f¬® ur die Jahreszeitlichkeiten der Literatur zu entwickeln, haben wir parallel zum Projekt eine Android-App entwickelt, die als Kalender funktioniert und f¬® ur jeden Tag des Jahres Passagen aus der Weltliteratur anzeigt, die an diesem Tag spielen. Beispiele sind in Abbildung 1 dargestellt. Die von uns entwickelte und mit HeidelTime belieferte App soll die einfache Exploration expliziter Zeitausdr¬® ucke in der Weltliteratur ermöglichen. Dass James Joyce"" Ulysses etwa am 16. Juni 1904 stattfindet (""Bloomsday""), ist allgemein bekannt, allerdings im Text nicht sofort ersichtlich. In der App wird die einzige Stelle des ¬® uberlangen Romans, an dem das Datum erwähnt wird, zitiert (die Sekretärin Miss Dunne tippt es auf ihrer Schreibmaschine ein). Weitere Beispiele f¬® ur Passagen sind der 12. Juni in der Blechtrommel (Oskar Matzeraths erklärter Sohn Kurt wird geboren), der 29. Februar in Thomas Manns Zauberberg (der als spezielle Variante der Walpurgisnacht eine zentrale Rolle spielt, siehe Abbildung 1) oder der 27. Juli in Stefan Zweigs Schachnovelle (der Protagonist Dr. B. eignet sich an diesem Tag das Schachbuch an). Die App stellt somit ein erweiterbares Korpus mit Datumsnennungen in der Weltliteratur dar, das der weiteren Forschung zur Verf¬® ugung steht. Um allerdings das gesamte ""literarische Jahr"" abzubilden, m¬® ussen auch die ungefähren zeitlichen Verortungen in den Blick genommen werden, was im nächsten Abschnitt versucht werden soll. 3 Fontane Storm JAN 30 5 FEB 13 3 MAR 18 7 APR 13 7 MAY 28 11 JUN 17 9 JUL 16 6 AUG 17 10 SEP 36 10 OCT 41 11 NOV 27 13 DEC 25 1 Abbildung 1: Screenshots unserer Android-App Literjahrtur. Tabelle 2: Fontane vs. Storm. 6 Die Jahreszeiten der Literatur Von dem f¬® ur literarische Texte sehr spezifischen Verhältnis zwischen exakten und ungefähren Datumsangaben war schon die Rede. Die Analyse des DTA-Korpus nur mit (auch nicht-literarischen) Texten aus dem 19. Jahrhundert hat eine besondere Konjunktur f¬® ur die Monate März bis Juli erbracht: JAN: +575544555454453554443444444667 FEB: 53553443533424444343445645352 MAR: +6667676789669+49+8+6877888699+ APR: +78878876+76597+75699+89668869 MAY: ++6+968758+899+8886+9+98987+9++ JUN: +8489768+++++978697+976++9988+ JUL: +789987+758978+7765887565767755 AUG: 6556544458455555554454455445345 SEP: 644333434433345345334322333334 OCT: 7222222222233332231242323233324 NOV: 845553454434436445534344443434 DEC: 2423222216222222112222242141123 Bei der Suche nach Abweichungen im Werk einzelner Autoren des 19. Jahrhunderts stießen wir etwa auf Theodor Fontane und Theodor Storm. Eine Erhebung nur der Monatsnennungen in ausgew ählten fiktionalen Texten beider Autoren ergab das in Tabelle 2 verzeichnete Bild. Analog zur Popularität des 1. Mai ist auch der Gesamtmonat bei beiden stark repräsentiert. Doch f¬® ur die Sommermonate gilt das nicht. Fontanes Romane und Erzählungen scheinen vor allem von September–Januar stattzufinden, Storms Texte von August–November. Auch unter der Maßgabe, dass der Monatsname als sprachlich-klangliches Zeichen einen stilistischen E_ekt hat, scheinen beide Autoren herbstlich-winterliche Settings und Stimmungen zu bevorzugen. Mit den vorgestellten Methoden zur Ermittlung von Datumskonjunkturen, zur Beschreibung des Verhältnisses zwischen ungefähren und exakten Datumsangaben, zum Aufbau eines Korpus mit exakten Datumsnennungen und zur Jahreszeitlichkeit der Literatur und bestimmter Autoren kann die im Titel gestellte Frage ""Wann findet die deutsche Literatur statt?"" tatsächlich makroanalytisch beantwortet werden. Damit die getro_enen Aussagen tatsächlich literaturhistorisch belastbar sind, soll f¬® ur die weitere Forschung ein größeres und besser (v. a. mit Entstehungs-/Verö_entlichungsdaten) ausgezeichnetes Korpus zusammengestellt werden. 4 Literatur Matthew Jockers. Macroanalysis. Digital Methods and Literary History. Chicago: University of Illinois Press, 2013. James Pustejovsky, Jos¬¥e M. CastaÀúno, Robert Ingria, Roser Sauri, Robert J. Gaizauskas, Andrea Setzer, Graham Katz and Dragomir R. Radev. TimeML: Robust Specification of Event and Temporal Expressions in Text. In New Directions in Question Answering, S. 28–34, 2003. Mirjam Sieber. Paul Celans ""Gespräch im Gebirg"". Erinnerung an eine versäumte Begegnung. T¬® ubingen: Niemeyer, 2007. Jannik Strötgen, Michael Gertz. Temporal Tagging on Di_erent Domains: Challenges, Strategies, and Gold Standards. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012), S. 3746–3753, 2012.",de,Einleitung Exakte datumsangaben Merkmal vieler prosatextsorten Literatur finden bevorzugt ungefähr datumsangaben interpretationsräume Monatsnennunge Theodor Storms Schimmelreiter ungefähr Natur octobernachmittag März Ausnahme Regel bilden abenteuerund historisch Roman goeth werth Jules vern Tour monde Jour wimmeln genretypisch exakter datumsangaben letztgenannt Roman Form Countdown unentbehrlich Handlung ansonsten lässen Hypothese formulieren erzählend Literatur exakt Datum narrativ Setzung nah Analyse lohnen ausgehend lassen dezidiert literaturwissenschaftlich Frage Blickfeld nehmen Vermeidung exakt datumsnennungen tatsächlich durchgehend Merkmal bestimmt literarisch Genre Verhältnis stehen Exakte ungefähren datumsangaben Frequenzanalyse datumsangaben genreuntersuchungen nutzen Bedeutung datumsangaben zeitlich Verortung Handlung Semiotisierung Datum zeitlich konjunktur ur bestimmen datumsnennungen Rolle spielen fiktiv daten erich Kästner Mai shakespear April The Winter -- talen autolycus ballade Akt unser Vortrag widmen datumsangaben isolierbar Feature literarisch Korpora Sinn matthew jockers indeed the very Object of Analysis Shifts from looking at -- individual occurrences -- Feature context to looking at The Trends and patterns of that featur Aggregated over entir Corpus Jockers anhand Feature explizit Datumsnennung einzeln betrachtbar Einheit Praxis literaturwissenschaftlich Makroanalyse methodisch bereichern Extraktion zeitangaben erfolgen automatisiert Mithilfe Temporal Tagger Untersuchung Repräsentanz außerliterarisch Phänomen datumsangaben literarisch Text Beitrag Erzählforschung leisten Methode Digital Humanitie erzielt erkenntnis Fachdisziplin Fall Literaturwissenschaften uckgespielt vorgehen vorgeh bestehen meist parallel ablaufenden Schritt Zusammenstellung Geeigneter deutschsprachig Korpora Erhebung daten Einsatz Temporal Tagger Heideltime strötgen Gertz automatisch Extraktion zeitlich ucke Sinn Temporal Markup Language Timeml pustejovsky et Analyse daten Heatmaps einzelfällen Entwicklung explorativ Analyse literarisch Jahr Textgrid literarisch Korpora zusammenbringen Heideltime Datumsstruktur untersuchen anhand explizit sicher richtig ucke kalendarisch Heatmap erstellen bedeuten vorkommen bedeuten vorkommen bedeuten vorkommen zeigen erwartet unerwartet konjunkturen Jan feb Mar Apr may jun jul aug sep oct nov dec sehen monatserste fixe Feiertage Neujahr Weihnachten häufig ansonsten fallen Konjunktur weit März Vermutung liegen nahe Nennung Datum Werk Märzrevolution ansteigt blutig Ereignis Berlin semantik annehmen Erhebung Analyse datumskonjunkturen systematisch ausbauen Untersuchung quantitativ Analyse liegen erwähnt Korpora zugrunde äuch nichtliterarisch Werk ubersetzung fremdsprachig Literatur vorkommen Methode belastbar Ergebnis gewinnen bedürfen qualifizierteren Textkorpus deutsch Textarchiv dta umfangreich beinhalten nomen est Om original deutschsprachig Text sowohl grob zeitlich jahrhundert thematisch genr sortieren sodass Hinweis Konjunktur literarisch Text erwarten Tabelle Information darstellen Anzahl enthalten Text stellen letztlich kritisch Masse ur Untersuchung dar versprechen Hinweis Chance Grenze Methode Analyse Einzel abrufbar ergeben stabil Heatmap häufigst daten nennung folgend heben Bedeutung Mai heraus unerwartet daten August zeitlich Verortung Fiktionaler handlungen wiederum historisch Datum uhren Tuileriensturm August Uchner Danton Tod urg Danton August Timeml explizit dokument Sätz token ucke ucke Belletristik Gebrauchsliteratur Wissenschaft Tabelle Information Teilkorpora Danton September relativ klein belletristisch Werk wobei Belletristik fiktional Reisebericht Lebenserinnerung einschließen belastbar ur korpusanalysen Bedeutung ur Literaturwissenschaft analog Hans Ulrich Gumbrecht Untersuchung denkbar unschenswert Literaturgeschichte einzeln schrieben jeder Datum Literaturgeschichte Ansatz untersuchen Korpusanalyse zeigen Paul Celan Jänner Celan Prosagedicht Gespräch Gebirg Rundschau Heft erscheinen georg Uchner Erzählung Lenz angespielen ebenfalls Gang durch Gebirge schildern uchner Text beginnen Satz Jänner Lenz durchs Gebirg Lenz Wanderung durch Gebirge Jänner stattfinden weisen Celan Meridian erweitern anspielungsraum Januar verweisen nämlich Jahr Berlin Wannseekonferenz stattfinden folgeren sagen dichen Jänner einschreiben bleiben Sieber utzt Erhebung Zeitangab Korpora gleichzeitigkeit sichtbarer ermöglichen systematisch Erforschung Exploration explizit ucke Weltliteratur Idee ur Jahreszeitlichkeit Literatur entwickeln parallel Projekt entwickeln Kalend funktionieren ur Jahr passage Weltliteratur anzeigt spielen Beispiel Abbildung darstellen entwickelt Heideltime belieferen app einfach Exploration explizit ucke Weltliteratur ermöglichen James Joyce ulysses Juni stattfinden bloomsday allgemein Text sofort ersichtlich app einzig Stelle uberlang Roman Datum erwähnen zitieren Sekretärin Miss Dunne tippen Schreibmaschine beispiel ur passagen Juni Blechtrommel Oskar Matzerath Erklärter Sohn Kurt gebären Februar Thomas Mann Zauberberg speziell Variante Walpurgisnacht zentral Rolle spielen sehen Abbildung Juli Stefan Zweigs Schachnovelle Protagonist dr eignen Schachbuch app stellen somit erweiterbares Korpus Datumsnennung Weltliteratur dar Forschung Ugung stehen gesamt literarisch abzubilden ussen ungefähren zeitlich Verortung Blick nehmen nächster Abschnitt versuchen Fontane Storm Jan feb Mar apr may jun jul aug sep oct nov dec Abbildung Screenshot Literjahrtur Tabelle Fontan Storm Jahreszeite Literatur ur literarisch Text spezifisch Verhältnis exakter ungefähren datumsangaben Rede Analyse texen Jahrhundert besonderer Konjunktur ur Monat März Juli erbringen Jan feb Mar apr may jun jul aug sep oct nov dec Suche Abweichung Werk einzeln Autor Jahrhundert stoßen Theodor Fontane Theodor Storm Erhebung monatsnennunge Ausgew ählten fiktionalen Text beide Autor ergeben Tabelle verzeichnet Bild analog Popularität Mai Gesamtmonat stark repräsentieren ur Sommermonate gelten fontan Roman Erzählung scheinen stattzufind Storms Text Maßgabe Monatsname Zeichen Stilistisch scheinen Autor Setting Stimmung bevorzugen vorgestellt Methode Ermittlung datumskonjunkturen Beschreibung verhältnisses ungefähren exakter datumsangaben Aufbau korpus exakter datumsnennungen Jahreszeitlichkeit Literatur bestimmt Autor Titel gestellt Frage finden deutsch Literatur tatsächlich makroanalytisch beantworten aussagen tatsächlich literaturhistorisch belastbar ur Forschung größeres ausgezeichnet korpus zusammenstellen Literatur matthew Jocker macroanalysis Digital Methods And literary History Chicago university of illinois press James Pustejovsky castaàúno Robert Ingria ros Sauri Robert Gaizauskas andrea Setzer Graham Katz And dragomir Radev Timeml Robust Specification of Event and Temporal Expression Text New Direction Question Answering Mirjam Sieber Paul Celans Gespräch Gebirg Erinnerung versäumt Begegnung ubing niemeyer Jannik Strötge Michael Gertz Temporal tagging -- domains Challenge Strategie and Gold Standard proceedings of the international conference on language resources and Evaluation Lrec,"[('ur', 0.3185877618472565), ('datumsangaben', 0.27449592671016165), ('ucke', 0.18656928283500512), ('jänner', 0.12437952189000341), ('ungefähren', 0.12437952189000341), ('exakter', 0.11585009521718417), ('temporal', 0.11315392156141686), ('konjunktur', 0.10979837068406467), ('weltliteratur', 0.10510428845507225), ('zeitlich', 0.10218857647561537)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,An End-To-End Integration of Automatic Annotations into CATMA,"Thomas Bögel (Institute of Computer Science, Heidelberg University); Marco Petris (Institute for German Studies, University of Hamburg); Michael Gertz (Institute of Computer Science, Heidelberg University); Jannik Strötgen (Institute of Computer Science, Heidelberg University)",Annotation,Annotation,"1 Introduction Natural Language Processing offers solutions for predicting linguistic annotations at different levels of complexity. Thus, it seems obvious and 'in general 'a good idea to apply these methods to the Humanities in order to automate laborious manual annotations and to facilitate a deeper text analysis understanding. Apart from the purely technical aspect of developing suitable models, however, additional challenges for NLP in the Humanities arise: in order to be used as part of an analysis tool, humanists often desire justifications and explanations of automatic annotations. Just implementing a black-box approach, evaluating it intrinsically and returning the presumably best results to the user is not sufficient. In this paper, we suggest a transparent way of presenting the results of a NLP pipeline in a collaborative setting. This gives the user the possibility to judge the results directly within an already existing annotation interface and potentially use them for individual analysis tasks. We will first present individual components that are combined with each other, namely the collaborative annotation tool CATMA and UIMA as a processing pipeline for Natural Language Processing. We will then show our end-to-end integration of UIMA into CATMA and its advantages. 2 CATMA integration CATMA1 is a flexible, collaborative annotation tool for literary scholars. So far, it integrates three functional and interactive modules, namely the tagger, the analyzer, and the visualizer. While the tagger module is a graphical interface to allow the easy creation of manual annotations in texts using flexible tag sets (including feature structures, overlapping annotations, etc.), the analyzer component offers a wide range of possibilities to query a document collection or single documents, e.g., for frequently occurring patterns. Finally, the visualizer module can be used to explore a document collection, e.g., by generating distribution charts of the analysis results. In this paper, we present an extension to CATMA, which was developed in the con¬¥ text of the heureCL EA project2 - the integration of a UIMA-based text processing pipeline for the automatic creation of tag annotations created by natural language processing tools. UIMA (Unstructured Information Management Architecture)3 is a wide-spread framwork for developing and using natural language processing pipelines. One of its key characteristics is that it allows the easy combination of tools that have initially not been built to be used together. All UIMA components rely on the same data structure - the Common Analysis Structure (CAS) - there are three types of components: collection readers, analysis engines, and CAS consumers. The collection readers task is to access the source of the documents that are to be processed and to initialize a CAS object for each document. Then, the analysis engines perform linguistic processing of the data and stand-off add annotations to the CAS object. The subsequently called analysis engines can access the annotation results of the earlier components, i.e., they can perform more complex tasks. Finally, a CAS consumer performs the final processing of the CAS object. 1Website: http://www.catma.de/clea 2http://heureclea.de/ 3Website: http://uima.apache.org/ Figure 1: End-To-End architecture of combining the collaborative annotation platform CATMA with the automatic text processing pipeline UIMA. In our case, the pipeline architecture is set up as depicted in Figure 1. The Collection Reader accesses the documents directly from CATMA and returns annotation information back to CATMA. However, the actual key feature of our development is that the user can directly access the automatic processing feature within the CATMA interface. That is, the user can select the types of annotations that shall be added to her document or document collection automatically. This significantly decreases the boundary for users not familiar with applying NLP tools for automatic processing of textual data, i.e., for typical CATMA users who are often literary scholars or students of the Humanities. Nevertheless, our implementation is not a black box solution that only adds annotations that the user has to accept. In contrast, we are currently working on integrating a user feedback interface that will allow the initialization of user parameters based on the users feedback in the form of accepted or rejected annotations. 3 Research Workflow within CATMA The advantage of a direct integration of UIMA into CATMA is best illustrated with an example: in order to analyse the temporal structure of documents (such as order phenomena), many linguistic aspects need to be taken into account. Temporal signals, e.g., calendrical, deictic or relational temporal expressions (Lahn and Meister, 2008), offer a hint for temporal phenomena of order. As manual annotation for these basic linguistic phenomena is laborious, we are currently developing a machine learning system for predicting temporal signals. Figure 2 shows the possibility to create and directly inspect automatic annotations directly within the CATMA interface. With one click, the prediction of our NLP pipeline for temporal signals 'or other annotations such as date and time expressions (Strötgen and Gertz, 2013) 'can be shown. Note that the system output can easily be compared to any manual annotation as the type systems are completely independent. This flexibility allows scholars to focus on complex phenomena of the text with the possibility of automating simpler annotations. All automatic annotations are, however, non-obtrusive and completely changeable and reversible to give the choice of the level of automation to the user. References Lahn, S. and J. C. Meister (2008). Einf¬® uhrung in die Erzähltextanalyse. Stuttgart: JB Metzler. Strötgen, J. and M. Gertz (2013). Multilingual and cross-domain temporal tagging. Language Resources and Evaluation 47(2), 269–298. Figure 2: Screenshot showing automatic annotations within CATMA.",en,introduction natural language processing offer solution predict linguistic annotation different level complexity obvious general good idea apply method humanity order automate laborious manual annotation facilitate deep text analysis understanding apart purely technical aspect develop suitable model additional challenge nlp humanity arise order analysis tool humanist desire justification explanation automatic annotation implement black box approach evaluate intrinsically return presumably good result user sufficient paper suggest transparent way present result nlp pipeline collaborative setting give user possibility judge result directly exist annotation interface potentially use individual analysis task present individual component combine collaborative annotation tool catma uima processing pipeline natural language processing end end integration uima catma advantage catma integration flexible collaborative annotation tool literary scholar far integrate functional interactive module tagger analyzer visualizer tagger module graphical interface allow easy creation manual annotation text flexible tag set include feature structure overlap annotation etc analyzer component offer wide range possibility query document collection single document frequently occur pattern finally visualizer module explore document collection generate distribution chart analysis result paper present extension catma develop text heurecl ea integration uima base text processing pipeline automatic creation tag annotation create natural language processing tool uima unstructured information management wide spread framwork develop natural language processing pipeline key characteristic allow easy combination tool initially build uima component rely data structure common analysis structure cas type component collection reader analysis engine cas consumer collection reader task access source document process initialize cas object document analysis engine perform linguistic processing datum stand add annotation cas object subsequently call analysis engine access annotation result early component perform complex task finally cas consumer perform final processing cas object figure end end architecture combine collaborative annotation platform catma automatic text processing pipeline uima case pipeline architecture set depict figure collection reader access document directly catma return annotation information catma actual key feature development user directly access automatic processing feature catma interface user select type annotation shall add document document collection automatically significantly decrease boundary user familiar apply nlp tool automatic processing textual datum typical catma user literary scholar student humanity implementation black box solution add annotation user accept contrast currently work integrate user feedback interface allow initialization user parameter base user feedback form accept reject annotation research workflow catma advantage direct integration uima catma well illustrate example order analyse temporal structure document order phenomenon linguistic aspect need take account temporal signal calendrical deictic relational temporal expression lahn meister offer hint temporal phenomenon order manual annotation basic linguistic phenomenon laborious currently develop machine learning system predict temporal signal figure show possibility create directly inspect automatic annotation directly catma interface click prediction nlp pipeline temporal signal annotation date time expression strötgen gertz show note system output easily compare manual annotation type system completely independent flexibility allow scholar focus complex phenomenon text possibility automate simple annotation automatic annotation non obtrusive completely changeable reversible choice level automation user reference lahn meister uhrung die erzähltextanalyse stuttgart jb metzler strötgen gertz multilingual cross domain temporal tagging language resource evaluation figure screenshot show automatic annotation catma,"[('user', 0.2724476717629011), ('catma', 0.2636640310057507), ('annotation', 0.24503062880519672), ('processing', 0.21721021707097032), ('cas', 0.19930845286659885), ('document', 0.19708275950211282), ('uima', 0.18932148654550648), ('automatic', 0.1816163505168157), ('temporal', 0.16923228614752586), ('pipeline', 0.15599986812587857)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Ausgezeichnete Mären analysieren ‚- ein Werkstattbericht,"Friedrich Michael Dimpel (FAU Erlangen, Deutschland)","Korpora, Annotation, XML, TEI, ODD","Korpora, Annotation, XML, TEI, ODD","1. Digitale Korpora können dank der Fortschritte im Bereich der Digital Humanities mit zahlreichen quantitativen Analyseverfahren untersucht werden: Im Rahmen der Stilometrie beschäftigen sich Computerphilologen etwa mit Fragen der Autorschaftsattribution oder der Werkchronologie sowie mit Fragen nach Spezifika von Gattungen oder Epochen.1 Auch wenn digitale Texte mit zahlreichen quantitativen Analyseverfahren untersucht werden können: In der Regel sind vollständige Texte oder zusammenhängende Textabschnitte wie Buchkapitel der Gegenstand etwa von stilometrischen Studien. Die Möglichkeiten für automatische Analysen enden jedoch dort, wo solche spezifische Textebenen in den Blick zu nehmen wären, die repräsentieren, welche Eigenschaft auf welche Figur bezogen wird, und welche Figuren oder Erzähler diese Zuschreibung vornehmen: Wer spricht? Zudem: Über wen wird gesprochen 'welche Terme werden auf welche Figuren bezogen? Gibt es dabei bspw. genderspezifische Distributionen? Auf welche Weise wird gesprochen? Es ist ein erheblicher Unterschied, ob eine Figur über ihre Liebe spricht und damit ihre Emotion in der erzählten Welt öffentlich macht, oder ob nur eine Gedankenrede einer Figur erzählt wird. Wenn man auf solche Informationen zugreifen möchte, dann ist es nötig, die Texte mit einer entsprechenden Annotation zu versehen. Quantitative Analysen von Textsamples, die spezifische Textdaten enthalten wie Figurenrede bestimmter Aktanten, fokalisierte Textpassagen oder Textpassagen, die sich auf bestimmte Aktanten beziehen, sind derzeit nicht möglich ohne eine aufwendige manuelle Textaufbereitung für eine konkrete Fragestellung. Wie wichtig jedoch eine narratologische Textauszeichnung in einem diachron angelegten Korpus wäre, unterstreichen Fotis Jannidis, Gerhard Lauer und Andrea Rapp: ""Wer hat, wann und wo zum ersten Mal die Form der erlebten Rede eingesetzt, wer die episodische Reihung zu psychologischer Figurenzeichnung verdichtet? Wo verknüpfen sich populäre Novellenstoffe mit hochkulturellen Erzähltechniken [‚Ä¶]? Denn nur mit Hilfe des Computers lässt sich ein hinreichend großes Korpus über einen langen Zeitraum hinweg untersuchen und damit etablierte literaturhistorische Methoden um serielle, computerbasierte Verfahren so ergänzen, dass eine Geschichte des Erzählens überhaupt erst geschrieben werden kann.""2 1 Vgl. exemplarisch JOHN F. BURROWS: ""Delta"". A Measure of Stylistic Difference and a Guide to Likely Authorship. In: Literary and Linguistic Computing 17, 2002, S. 267–287, FRIEDRICH MICHAEL DIMPEL: Computergestützte textstatistische Untersuchungen an mittelhochdeutschen Texten. Tübingen 2004, FOTIS JANNIDIS: Methoden der computergestützten Textanalyse. In: Vera Nünning / Ansgar Nünning (Hrsg.), Methoden der literatur- und kulturwissenschaftlichen Textanalyse. Ansätze 'Grundlagen 'Modellanalysen. Stuttgart 2010, S. 109–132, CHRISTOF SCHÖCH: Corneille, Moli√®re et les autres. Stilometrische Analysen zu Autorschaft und Gattungszugehörigkeit im französischen Theater der Klassik. In: Christof Schöch / Lars Schneider (Hrsg.), Literaturwissenschaft im digitalen Medienwandel. 2014, S. 130–157. 2 FOTIS JANNIDIS / GERHARD LAUER / ANDREA RAPP: Hohe Romane und blaue Bibliotheken. Zum Forschungsprogramm einer computergestützten Buch- und Narratologiegeschichte des 2. Bei narratologisch annotierten Korpora zur deutschen Literatur handelt es sich um ein Desiderat.3 Daher soll ein Korpus von 100 Kurzerzählungen narratologisch annotiert werden. Das Korpus soll historisch relativ ausgewogen angelegt werden: 30 mittelhochdeutsche und frühneuhochdeutsche Mären, 10 Boccaccio-Novellen, 10 Chaucer-Tales, 30 neuhochdeutsche Novellen und 20 neuhochdeutsche Kurzgeschichten werden aufgenommen. Als Ebenen der Auszeichnung sind vorgesehen: 1. Welche Figuren befinden sich an dem Ort, von dem erzählt wird? 2. Welche Figurenbewegungen im Raum finden statt? 3. Welche temporalen Abweichungen ereignen sich (Prolepsen etc.)? 4. Welche Figur ist wie fokalisiert? 5. Um welche Art von Redewiedergabe (direkte Rede, Bewusstseinsdarstellung etc.) handelt es sich 'und welche Figur denkt oder spricht? 6. Um welche Art von Erzählerrede (Descriptio, Bericht Figurenaktivität, Erzählerreflexion etc.) handelt es sich? 7. 8. Auf welche Figur bezieht sich eine Figuren- oder Erzählerrede? Auf welche Figur bezieht sich eine wertende Öußerung einer anderen Figur oder des Erzählers? 9. Steht eine Öußerung in Negation? 10. Liegt eine uneigentliche Rede vor (metaphorisch, ironisch etc.)? 11. Inwieweit besteht Unsicherheit hinsichtlich der Eindeutigkeit der Textdaten? Die nötigen XML-Elemente sind noch nicht im TEI-Standard enthalten, daher muss ein geeignetes Tagset entwickelt werden.4 TEI-Kompatibilität wird jedoch angestrebt: Die Elemente können über das Roma-Tool in eine ODD-Datei integriert werden.5 Romans in Deutschland (1500-1900). In: Lucas Marco Gisi / Jan Loop / Michael Stolz (Hrsg.), Literatur und Literaturwissenschaft auf dem Weg zu den neuen Medien. germanistik.ch 2006. (= online unter http://www.germanistik.ch/publikation.php?id=Hohe_Romane_und_blaue_ Bibliotheken). 3 Allerdings kann in vielfältiger Weise auf die wichtige Grundlagenstudie von ANNELEN BRUNNER: Automatische Erkennung von Redewiedergabe in literarischen Texten. Diss. masch. Würzburg 2012, aufgebaut werden. Brunner hat in ihrer Dissertation ein Annotationsverfahren für Redewiedergabe entwickelt und ein Korpus, das aus Texten von 1787 bis 1913 besteht, manuell annotiert. Auch wenn die automatische Erkennung der Redewiedergabe (regelbasiert und via Maschinelles Lernen) noch nicht eine Fehlerrate erreichen kann, die narratologische Auswertungen erlaubt, kann das vorliegende Projekt in konzeptioneller Hinsicht von Brunners Studie erheblich profitieren. 4 Zu narratologischen Desideraten von TEI-P5 vgl. FOTIS JANNIDIS: TEI in a Crystal Ball. In: Literary and Linguistic Computing 24, 2009, S. 253–265, hier S. 261f. 5 Vgl. http://www.tei-c.org/Guidelines/Customization/odds.xml. Als XML-Elemente werden vorgestellt: 1. <FigurFokusort> (@Bezeichnung, @Figurengruppe) 2. <BewegungLokal> (@Typ) 3. <Chronologie> (@Typ) 4. <Fokalisiert> (@Bezeichnung, @Typ) 5. <Redewiedergabe> (@Typ, @Bezeichnung, @non-fact, @level) 6. <Erzählerrede> (@Typ, @Bezeichnung) 7. <Figurenbezug> (@Unmittelbar, @Mittelbar) 8. <Wertung> (@BezeichnungWertende, @BezeichnungGewertete, @Typ) 9. <Negation> (@Typ) 10. <UneigentlicheRede> 11. <certainty> sowie @cert als Attribut zu allen Elementen (> TEI P5) 3. Zentral für den Workflow ist die Entwicklung von Annotationsrichtlinien. Es wird angestrebt, dass verschiedene Versuchspersonen beim gleichen Text zu homogenen Ergebnissen kommen. In ähnlicher Weise, wie man sich bei der Herstellung einer Edition über Kollationierungsregeln verständigen muss, sind hier Annotationsregeln zu erarbeiten. Solche Regeln sind nötig, weil selbst scheinbar eindeutig definierte narratologische Phänomene sich oft nicht eindeutig in Texten wiederfinden lassen. Zudem ist Ambiguität ein charakteristisches Merkmal von literarischen Texten. Mit Blick auf das Homogenitätsziel werden Annotationsregel und Bearbeitungsdatum direkt im XML-Code dokumentiert, damit bei einer Weiterentwicklung der Annotationsrichtlinien einerseits rasch auf eine einschlägige Fallsammlung zugegriffen werden kann; anderseits können bei einer Regelrevision Entscheidungen gezielt aufgesucht und revidiert werden. Dabei hilft ein eigener Projekteditor, der in Perl/TK implementiert wurde, der neben dem Annotationsfenster in einem zweiten Fenster in Kurzform Informationen zu bereits ausgezeichneten Elementen einblendet. 4. Annotiert wurden bislang sechs Texte. Vorgestellt werden einige Probleme, die sich bei der Auszeichnung von ""Sperber"" und das ""Häslein""6 etwa durch Segmentierung oder durch Ambiguitäten ergeben haben. Anhand von Auswertungsdaten soll exemplarisch aufgezeigt werden, in welch vielfältiger Weise ein entsprechend annotiertes Korpus Analysen möglich macht; etwa in Bezug auf a) multiple Methoden: i. Das Korpus kann wie andere Korpora auch mit einer Vielzahl an statistischen Methoden analysiert werden 'etwa in Hinblick auf Heterogenität oder Homogenität. ii. Eine besondere Stellung nimmt das Korpus jedoch dadurch ein, dass auf Basis der Textauszeichnung eine Sample-Erstellung für spezifische Fragestellung möglich ist, die 6 Ausgabe: KLAUS GRUBMÜLLER: Novellistik des Mittelalters. Märendichtung. Frankfurt/Main 1996 (=Bibliothek des Mittelalters 23) nicht nur chronologisch-lineare Zugriffe auf Korpussegmente erlaubt, sondern systematische Zugriffe auf gleichartig annotierte Korpussegmente. So lässt sich ein Korpussegment bspw. mit Bewusstseinsdarstellung von weiblichen Figuren mit einem Korpussegment vergleichen, das aus Erzählerrede besteht; die Figurenrede von Antagonisten lässt sich mit Figurenrede von Protagonisten vergleichen, u.v.m. b) multiple Fragestellungen, beispielsweise: i. Wie steht es um die diachrone Entwicklung von Fokalisierung, um die Eigenschaften von Erzähler- und Figurenrede, um temporale Alternationen, wie verteilt sich der Redebezug auf verschiedene Figurentypen wie Protagonist oder Antagonist, wie steht es um quantitative Parameter bei uneigentlicher Rede? ii. Korrelation kulturwissenschaftlich relevanter Terme und aktantieller Rolle. Hier werden bspw. zahlreiche gender-bezogene Auswertungen möglich, indem eine SampleAnalyse mit Figuren- oder Erzählerrede möglich wird, die jeweils auf weibliche oder männliche Figuren bezogen ist oder durch eine Sample-Analyse mit Figurenrede, die jeweils von weiblichen oder männlichen Figuren stammt. iii. Studien zur Wertungstheorie: Wie sind evaluative Öußerungen auf Erzählerrede und Figurenrede verteilt? Welche Aktanten bewerten bevorzugt, welche werden bevorzugt bewertet? iv. Lassen sich für diese oder für andere Fragestellungen epochenspezifische Verteilungen ausmachen? Es werden Studien ermöglicht, die einen Beitrag zur Gattungsgeschichte leisten. So wären beispielsweise Theoriebildungen zu überprüfen, inwieweit und inwiefern das Verhältnis vom Märe zur Novelle in Anschluss an die Unterscheidungskriterien von Hans-Jörg Neuschäfer unter dem Gesichtspunkt eines ""Noch-Nicht"" beschrieben werden kann.7 7 Vgl. HANS-JÖRG NEUSCHÖFER: Boccaccio und der Beginn der Novelle. Strukturen der Kurzerzählung auf der Schwelle zwischen Mittelalter und Neuzeit. München 1969 (=Theorie und Geschichte der Literatur und der schönen Künste 8). Kritisch dazu FRIEDRICH MICHAEL DIMPEL: Sprech- und Beißwerkzeuge, Kunsthandwerk und Kunst in Kaufringers ""Rache des Ehemanns"". In: Daphnis 42, 2013, S. 1–27 (im Erscheinen).",de,digital Korpora Fortschritte Bereich Digital Humanitie zahlreich quantitativ Analyseverfahre untersuchen Rahmen Stilometrie beschäftigen computerphilolog Frage Autorschaftsattribution Werkchronologie Frage Spezifika gattung digital Text zahlreich quantitativ Analyseverfahr untersuchen Regel vollständig Text zusammenhängend Textabschnitt Buchkapitel Gegenstand stilometrisch Studie Möglichkeit automatisch Analyse enden spezifisch textebenen Blick nehmen sein repräsentieren eigenschaft Figur beziehen Figur Erzähler Zuschreibung vornehmen sprechen zudem sprechen Term Figur beziehen genderspezifisch distributionen Weise sprechen erheblich Unterschied Figur Liebe sprechen Emotion erzählt Welt öffentlich Gedankenrede Figur erzählen Information zugreifen nötig Text entsprechend Annotation versehen quantitativ Analyse Textsamples spezifisch Textdat enthalten Figurenred bestimmt aktanen Fokalisiert Textpassagen Textpassage bestimmt Aktant beziehen derzeit aufwendig Manuelle Textaufbereitung konkret Fragestellung wichtig narratologisch Textauszeichnung diachron angelegt Korpus unterstreich Foti Jannidis Gerhard Lauer andrea rapp Mal Form erlebt Rede einsetzen episodisch Reihung psychologisch Figurenzeichnung verdichten verknüpfen populär Novellenstoff hochkulturell Erzähltechniken Hilfe Computer lässt hinreichend Korpus lang Zeitraum Hinweg untersuchen etabliert literaturhistorisch Methode Serielle Computerbasierte Verfahren ergänzen Geschichte erzählens schreiben exemplarisch John Burrow delta measure of stylistic difference and guide to Likely Authorship literary and Linguistic Computing friedrich Michael dimpel computergestützt textstatistisch Untersuchung mittelhochdeutsch Text Tübinge Foti Jannidis Methode computergestützt Textanalyse vera nünning Ansgar nünning Methode kulturwissenschaftlich Textanalyse Ansatz grundlag Modellanalysen Stuttgart Christof schöch Corneille re et les autres stilometrisch Analyse Autorschaft Gattungszugehörigkeit französisch Theater Klassik Christof schöch Lars Schneider Literaturwissenschaft digital Medienwandel Foti Jannidis Gerhard Lauer andrea rapp hoch Roman blau bibliotheken Forschungsprogramm computergestützt narratologiegeschichen narratologisch annotiert Korpora deutsch Literatur handeln Korpus Kurzerzählung narratologisch annotiert korpus historisch relativ ausgewogen anlegen mittelhochdeutsch Frühneuhochdeutsch mären neuhochdeutsch Novell neuhochdeutsch Kurzgeschichte aufnehmen Ebene Auszeichnung vorsehen Figur befinden Ort erzählen Figurenbewegung Raum finden Temporal Abweichung ereignen proleps Figur fokalisieren Art Redewiedergabe direkt Rede Bewusstseinsdarstellung handeln Figur denken sprechen Art Erzählerrede descriptio Bericht Figurenaktivität Erzählerreflexion handeln Figur beziehen Erzählerrede Figur beziehen wertend Öußerung Figur Erzähler stehen Öußerung Negation liegen uneigentlich Rede metaphorisch ironisch inwieweit bestehen Unsicherheit hinsichtlich Eindeutigkeit textdaten nötig enthalten geeignet Tagset entwickeln anstreben element integrieren Roman Deutschland Lucas Marco Gisi Jan Loop Michael Stolz Literatur Literaturwissenschaft Weg Medium online bibliotheken vielfältig Weise wichtig Grundlagenstudie annel Brunner automatisch Erkennung Redewiedergabe literarisch Text diss Masch Würzburg aufbauen Brunner Dissertation annotationsverfahren Redewiedergabe entwickeln Korpus Text bestehen manuell annotiert automatisch Erkennung Redewiedergabe regelbasieren via maschinelles lernen Fehlerrate erreichen narratologisch Auswertung erlauben vorliegend Projekt konzeptionell Hinsicht Brunner Studie erheblich profitieren narratologisch Desiderat Foti Jannidis tei Crystal Ball literary and Linguistic Computing vorstellen Figurfokusort Bewegunglokal Chronologie fokalisieren Redewiedergabe erzählerrede Figurenbezug Wertung Negation uneigentlichered Certainty Attribut Element tei zentral Workflow Entwicklung Annotationsrichtlini anstreben verschieden Versuchsperson gleich Text homogen Ergebnis ähnlich Weise Herstellung Edition kollationierungsregeln verständigen annotationsregeln erarbeiten Regel nötig scheinbar eindeutig definiert narratologisch Phänomen eindeutig Text wiederfinden lassen zudem Ambiguität charakteristisch Merkmal literarisch Text Blick Homogenitätsziel Annotationsregel Bearbeitungsdatum direkt dokumentieren Weiterentwicklung Annotationsrichtlinie einerseits rasch einschlägig Fallsammlung zugegriffen anderseits Regelrevision Entscheidung gezielt Aufgesucht revidieren helfen Projekteditor Perl tk implementieren Annotationsfenster Fenster Kurzform Information Ausgezeichnet Element einblenden annotiert bislang Text vorstellen Problem Auszeichnung Sperber Segmentierung Ambiguität ergeben anhand auswertungsdaten exemplarisch aufzeigen welcher vielfältig Weise entsprechend annotiert Korpus analysen Bezug Multiple Methode korpus Korpora Vielzahl statistisch Methode analysieren Hinblick Heterogenität Homogenität ii besonderer Stellung nehmen korpus Basis Textauszeichnung spezifisch Fragestellung Ausgabe Klaus Grubmüller Novellistik Mittelalter Märendichtung Frankfurt Main Bibliothek Mittelalter Zugriff Korpussegment erlauben systematisch zugriffe gleichartig annotiert Korpussegment lässen Korpussegment Bewusstseinsdarstellung weiblich Figur Korpussegment vergleichen Erzählerrede bestehen Figurenrede Antagonist lässt Figurenrede Protagonist vergleichen b Multiple fragestellungen beispielsweise stehen Diachrone Entwicklung Fokalisierung Eigenschaft Figurenrede temporal alternation verteilen Redebezug verschieden Figurentype Protagonist antagonisen stehen quantitativ parameter uneigentlich Rede ii Korrelation kulturwissenschaftlich relevanter Term aktantiell Rolle zahlreich Auswertung sampleanalyse Erzählerrede jeweils weiblich männlich Figur beziehen Figurenrede jeweils weiblich männlich Figur stammen iii Studie Wertungstheorie evaluativ öußerunge Erzählerrede Figurenrede verteilen Aktant bewerten bevorzugen bevorzugt bewerten iv lassen Fragestellung epochenspezifisch verteilung ausmachen Studie ermöglichen Beitrag Gattungsgeschicht leisten sein beispielsweise theoriebildungen überprüfen inwieweit inwiefern Verhältnis märe Novelle Anschluss Unterscheidungskriterien Neuschäfer Gesichtspunkt beschreiben Neuschöfer Boccaccio Beginn Novelle Struktur Kurzerzählung Schwelle Mittelalter Neuzeit München Theorie Geschichte Literatur schön kün kritisch friedrich Michael dimpel Beißwerkzeuge Kunsthandwerk Kunst Kaufringer Rache ehemanns daphnis erscheinen,"[('erzählerrede', 0.23751695274414564), ('figur', 0.2357959233532924), ('korpussegment', 0.17000270891853544), ('redewiedergabe', 0.16267936434591987), ('narratologisch', 0.15790917663263976), ('foti', 0.15007310019763567), ('figurenrede', 0.14256652972654413), ('beziehen', 0.12369831195684572), ('mittelalter', 0.11875847637207282), ('korpus', 0.10801763902397658)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Modellierung eines maschinell lesbaren Lexikons für das Korpus der altäthiopischen Literatur,Alessandro Bausi (Universität Hamburg); Andreas Ellwardt (Universität Hamburg); Cristina Vertan (Universität Hamburg),"Lexikonmodell, Transkription, Korpora, XML, TEI","Lexikonmodell, Transkription, Korpora, XML, TEI","1. Einführung Die Entwicklung und ständige Erweiterung des Unicode-Kodierungssytems Unicode1 sowie der Mark-upSprachen XML2, TEI3 haben in den letzten Jahren u.a. die digitale textuelle Repräsentation von historischen Dokumenten, die mit unterschiedlichen Alphabeten geschrieben wurden, ermöglicht. Diese textuelle Repräsentation eröffnet wiederum, im Kontrast zur reinen Speicherung von BildDigitalisaten, die Möglichkeit, computergestützte linguistische sowie philologische Untersuchungen auf großen Textmengen durchzuführen. Durch solche Methoden lässt sich beispielsweise eine diachrone Analyse der Sprache gleichzeitig auf mehreren Ebenen (morphologisch, syntaktisch, semantisch) realisieren, vorausgesetzt, die elektronischen Ressourcen wie Lexika oder annotierte Korpora sowie die sprachtechnologischen Prozesse (morphologische Analysierer, Wortart-Tagger, Parser) sind vorhanden. Während die sprachtechnologischen Ressourcen und Werkzeuge für moderne Sprachen sehr weit entwickelt sind, gelten viele historische Sprachen als stark ""under-ressourced"". Laut Krauwer (2003) gibt es ein minimales Set von Ressourcen, die für eine computergestützte Sprachanalyse unabdingbar sind. Dessen Weiterentwicklung stellt die Wissenschaft vor neue Forschungsprobleme, da sich häufig Modelle, die für moderne Sprachen entwickelt wurden, nicht 1:1 auf historische Sprachen übertragen lassen (VertanEtAl.2014) In diesem Beitrag werden wir die Modellierung und Entwicklung von sprachtechnologischen Ressourcen für das Altäthiopische (Ge_ez) erläutern. Die Besonderheiten des Ge_ez (s. Sektion 2), bedingen die Entwicklung von neuen Modellen, z.B. im Bereich der Lexika. In Sektion 3 werden wir exemplarisch die Entwicklung eines Lexikon-Modells für Ge_ez darstellen, während wir in Sektion 4 die Einbindung des Lexikons in einer Architektur für die diachrone Analyse des Ge_ez diskutieren werden. 2. Kurze Darstellung des Altäthiopischen (Ge_ez) Das südsemitische Ge_ez ist die Sprache des Königreichs Aksum in der heutigen nordäthiopischen Provinz Tigray, von wo aus die im 4. Jahrhundert beginnende Christianisierung Öthiopiens ihren Anfang nahm. Die in der Folge entstehende reiche Literatur ist in großem Umfang geprägt von Übersetzungen aus dem Griechischen und später, ab dem 13. Jahrhundert, aus dem Arabischen, was durch grammatische Interferenzphänomene reflektiert wird. Während seine Verdrängung als gesprochene Sprache bereits im 9./10. Jahrhundert beginnt, bleibt es als Schriftsprache sehr viel länger erhalten und ist bis in die Gegenwart hinein Liturgiesprache des äthiopischen und eriträischen Klerus. Das Altäthiopische hat aus einer südsemitischen Schrift ein eigenes Silbenalphabet entwickelt, das bis heute in mehreren modernen Sprachen Öthiopiens und Eritreas Verwendung findet. Innerhalb der semitischen Sprachen fällt es durch die verwendete Rechtsläufigkeit auf, außerdem werden die Vokale vollständig geschrieben. Beides unterscheidet das Ge_ez von den ihm nächst verwandten Sprachen Altsüdarabisch, Arabisch, Hebräisch und Syro-Aramäisch. Des weiteren sind Grapheme, die ursprünglich distinkten Phonemen zugeordnet waren, schon früh in identischer phonetischer Realisierung zusammengefallen, was sich konkret bereits in den ältesten überlieferten Handschriftzeugnissen (aber noch nicht in den aksumitischen Inschriften) niederschlägt, wo eine beliebige Austauschbarkeit der Laryngale und Sibilanten jeweils untereinander zu konstatieren ist. Mit den genannten eng verwandten semitischen Sprachen teilt das Altäthiopische die nichtkonkatenative Morphologie. Hierbei muss das einzelne Lexem als Kombination von zwei Elementen beschrieben werden, nämlich der Wurzel und dem Schema: Die konsonantische Wurzel gibt veränderliche Positionen zwischen 1 http://www.unicode.org/ 2 http://www.w3.org/XML/ 3 http://www.tei-c.org/index.xml ihren, zumeist drei, Wurzelkonsonanten vor, die durch die Vokale des Schemas aufgefüllt werden, häufig, jedoch nicht zwingend, ergänzt um (vokalische oder konsonantische) Affixe. 3. Arbeitsschritte zu einer computergestützten Analyse des Altäthiopischen Wie bereits in Sektion 2 erwähnt, sind Ge_ez-Dokumente für die gesamte Geschichte des christlichen Orients extrem wertvoll. Manche Überlieferungen von alten griechischen Texten sind in der Originalsprache verloren und nur im Altäthiopischen erhalten. In der Zeit digitaler Bibliotheken erscheint also die Entwicklung von computergestützten Tools für die Ge_ez-Sprache umso dringender. Das primäre Ziel des Projekts TraCES4 ist die Entwicklung eines digitalen Korpus der Ge_ez-Sprache, zusammen mit Annotationen auf morphologischer, syntaktischer und semantischer Ebene. Dieses annotierte Korpus soll einerseits eine diachrone Analyse des Altäthiopischen ermöglichen, anderseits soll es selbst als Ressource für weitere computergestützte Prozesse dienen. Langfristig soll eine vergleichende digitale Analyse von altäthiopischen und griechischen (z.B. die in der digitalen PERSEUS Sammlung5 verfügbaren) oder arabischen sowie anderen christlich-orientalischen Dokumenten möglich sein. Mit Ausnahme von einigen wenigen Texten gibt es zur Zeit keine verfügbare elektronische Ressource für das Altäthiopische. Daher haben wir uns als erstes der Entwicklung eines maschinell lesbaren Lexikons des Ge_ez gewidmet. Dessen Modellierung wird in der nächsten Sektion erklärt. 4. Ein Lexikon-Modell für Ge_ez Die in Sektion 2 erwähnte Austauschbarkeit der Laryngalen und Sibilanten untereinander stellt uns vor eine erste Modellierungsanforderung. Für einen Lexikon-Eintrag muss nicht nur die Grundform, sondern es müssen auch alle möglichen graphischen Varianten gespeichert werden, wobei wohlgemerkt diese graphische Variationen auch in einigen Fällen als selbständige Lexikon-Einträge mit ganz anderer Bedeutung existieren können. Das Lexikonmodell muss daher eine starke Modularisierung und Verlinkung zwischen den einzelnen Modulen unterstützen. Wir haben uns für das Lemon-Modell (McCraeEtAl.2012) entschieden. Unserer Kenntnis nach, ist dies der erste Versuch, eine semitische Sprache mit dem Lemon-Modell zu beschreiben. Die Grundkomponenten eines Lemon-Lexikon-Modells für Ge_ez wurden wie folgt angepasst. Die Zitierform eines Wortes in klassischen Lexika semitischer Sprachen ist in der Regel eine verbale Repräsentation der Wurzel in der 3. Person Perfekt Singular maskulin. Diese Form wird in unserem LemonModell als ""Lexical Entry"" gespeichert. Ein ""Lexical Entry"" ist mit den folgenden weiteren Modulen verknüpft: -Das Lexical Form-Modul beinhaltet alle möglichen graphischen Varianten des Lemmas. Jede graphische Variante wird zusammen mit ihrer Transkription gespeichert. -Das Morphologie-Modul beinhaltet eine Subkomponente für den lexikalischen Eintrag, die das Paradigma, Ausnahmen der morphologischen Realisierung (z.B. Sonderformen im Imperfekt oder Plural) sowie die jeweiligen anderen morphologischen Kategorien für das Lemma umfasst. Das Semantik-Modul setzt sich aus einer Übersetzungs-, einer Korpusevidenz- und einer semantischeMerkmale-Komponente zusammen. Unter Korpusevidenz verstehen wir Beispiele aus Korpora für dieses Lemma oder eine seiner morphologischen Realisierungen. Die Übersetzungen sind unterteilt in eine Übersetzung ins Englische und semantische Öquivalente in anderen Sprachen wie (falls vorhanden) Arabisch, Hebräisch, Syrisch, Koptisch, Griechisch oder sogar Sanskrit. -Das Syntax-Modul beinhaltet syntaktische Funktion des Lemmas, zusammen mit Beispielen von syntaktischen Bäumen. Dieses Modul wird in einer späteren Projektphase entwickelt. 4 European Union Seventh Framework Programme IDEAS (FP7/2007-2013), European Research Council, grant agreement no. 338756, project ""TraCES 'From Translation to Creation: Changes in Ethiopic Style and Lexicon from Late Antiquity to the Middle Ages"", http://www1.uni-hamburg.de/ethiostudies/traces.html 5 http://www.perseus.tufts.edu/hopper/ 4.1. Wurzel-Modellierung Da die Wurzel eine zentrale Stellung in der semitischen Morphologie hat, haben wir als ersten Schritt ein Wurzel-Sublexikon erstellt. Dieses entspricht dem Wurzel-Submodul im morphologischen Modul. Die Erstellung des Wurzel-Lexikons wurde vollständig automatisiert. Aus einer digitalen Version des trotz seiner Abfassung im Jahre 1865 unverändert als Standardwerk geltenden ""Lexicon linguae aethiopicae"" von August Dillmann (Dillmann1865) (im Unicode-Format) wurden zirka 4000 Wurzel-Einträge mit Hilfe von String-basierten Regeln extrahiert. Für jede Wurzel wurden: - die vollständige Transkription - die auf das konsonantische Gerüst zurückgeführte Transkription - das konsonantischen Wortbildungsschema - alle graphischen Varianten zusammen mit deren Transkriptionen durch regel-basierte Verfahren extrahiert. Die Automatisierung ermöglicht zum ersten Mal die Sammlung aller graphischen Varianten für alle 4000 Wurzeln (wobei hervorgehoben werden muss, dass manche Wurzeln bis zu 50 graphische Varianten haben). Jede Wurzel wird automatisch mit ihren Homophonen (Einträge mit identischer graphischer Form, aber unterschiedlicher Bedeutung) verknüpft. Erfasst werden durch automatische Prozesse auch alle Lexikoneinträge von graphischen Varianten (falls vorhanden). Das Wurzel-Lexikon wird im XML-Format gespeichert. Dafür wurde ein eigenes XML-Schema entworfen. Eine Java-basierte graphische Oberfläche wurde implementiert. Diese Oberfläche ermöglicht nicht nur die Visualisierung von den Einzeleinträgen und die Navigation durch das Wurzel-Lexikon, sondern auch manuelle Korrekturen, das Löschen oder das Einfügen von neuen Einträgen. Nach Korrekturen wird das Wurzel-Lexikon: - als eine ""Authority List"" für das Ge_ez-Lexikon und - als Generierungsquelle für Lexikoneinträge benutzt. 5. Zusammenfassung und weitere Arbeit In diesem Beitrag haben wir die Modelle für ein Wurzel- und ein Lemma-Lexikon für die Ge_ez-Sprache erklärt. Die Wurzel und Lemma-Akquisition werden weitgehend durch computergestützte Prozesse realisiert. Die erstellte Software wird bei der Präsentation des Beitrags vorgeführt. Das Projekt TraCES wurde im März 2014 begonnen und hat eine Laufzeit von fünf Jahren. Die Erstellung des Lexikons der Ge_ez Sprache ist zurzeit die zentrale Arbeit im Projekt, wobei derzeit die Erstellung von Generierungsparadigmen im Vordergrund steht. Mit deren Hilfe werden durch Computerverfahren Lexikoneinträge generiert. Ein erster Test hat mehr als 13 000 Einträge generiert. Dies zeigt, dass die Automatisierung eine erhebliche Zeitersparnis für die Lexikon-Akquisition ermöglicht. Literatur (Dillmann1865) Dillmann, August, Lexicon lingu√¶ ßthiopic√¶ cum indice Latino, Lipsiae 1865. (Krauwer2003) Krauwer, Steven, ""The Basic Language Resource Kit (BLARK) as the First Milestone for the Language Resources"", http://www.elsnet.org/dox/krauwer-specom2003.pdf (09.11.2014) (McCraeEtAl2012). McCrae, John und Aguado-de-Cea, Guadalupe und Buitelaar, Paul und, Cimiano, Philipp und Declerck, Thierry und G√≥mez P√©rez, Asunci√≥n und Gracia, Jorge und Hollink, Laura und Montiel-Ponsoda, Elena und Spohr, Dennis und Wunner, Tobias, The Lemon Cookbook, http://lemon-model.net/lemon-cookbook.pdf (09.11.2014) (VertanET.AL.2014) Vertan, Cristina und Zervanou, Kalliopi und van den Bosch, Antal und Sporeleder, Caroline (Hrsg.), Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH), Association for Computational Linguistics, Götheborg, Sweden, 2014, http://www.aclweb.org/anthology/W14-06 (09.11.2014)",de,Einführung Entwicklung ständig Erweiterung letzter digital textuell Repräsentation historisch dokumenten unterschiedlich alphabet schreiben ermöglichen textuell Repräsentation eröffnen wiederum Kontrast rein Speicherung bilddigitalisaten Möglichkeit computergestützt linguistisch philologisch Untersuchung Textmeng durchführen Methode lässt beispielsweise Diachrone Analyse Sprache gleichzeitig mehrere eben morphologisch syntaktisch semantisch realisieren voraussetzen elektronisch Ressource Lexika annotiert Korpora sprachtechnologisch prozeß morphologisch Analysierer Parser vorhanden sprachtechnologisch Ressource Werkzeug modern Sprache entwickeln gelten historisch Sprache stark laut Krauwer minimal set Ressource computergestützt Sprachanalyse unabdingbar Weiterentwicklung stellen Wissenschaft Forschungsproblem häufig modellen modern Sprache entwickeln historisch Sprache übertragen lassen Beitrag Modellierung Entwicklung sprachtechnologisch Ressource altäthiopisch erläutern Besonderheite Sektion bedingen Entwicklung Modell Bereich Lexika Sektion exemplarisch Entwicklung darstellen Sektion Einbindung Lexikon Architektur Diachrone Analyse diskutieren kurz Darstellung altäthiopisch südsemitisch Sprache Königreich Aksum heutig nordäthiopisch Provinz Tigray Jahrhundert beginnend Christianisierung öthiopiens Anfang Folge entstehend reich Literatur groß Umfang prägen übersetzungen griechisch Jahrhundert arabisch grammatisch Interferenzphänomene reflektieren Verdrängung gesprochen Sprache Jahrhundert beginnen bleiben Schriftsprache lang erhalten Gegenwart hinein liturgiesprachen äthiopisch eriträisch Klerus altäthiopisch südsemitisch Schrift Silbenalphabet entwickeln mehrere modern Sprache öthiopiens eritrea verwendung finden innerhalb semitisch Sprache fallen verwendet Rechtsläufigkeit vokal vollständig schreiben beide unterscheiden nächst verwandt Sprache altsüdarabisch arabisch hebräisch Graphem ursprünglich distinkt Phoneme zuordnen früh Identischer phonetisch Realisierung zusammenfallen konkret alt überlieferten Handschriftzeugnisse aksumitisch inschrifen niederschlagen beliebig austauschbarkeit laryngale Sibilant jeweils untereinander konstatieren Genannt eng verwandt semitisch Sprache teilen altäthiopisch nichtkonkatenativ Morphologie hierbei einzeln Lexem Kombination Element beschreiben nämlich Wurzel Schema konsonantisch Wurzel veränderlich Position zumeist wurzelkonsonann vokal Schemas auffüllen häufig zwingend ergänzen vokalisch konsonantisch Affixe Arbeitsschritte computergestützt Analyse altäthiopisch Sektion erwähnen gesamt Geschichte christlich Orient extrem wertvoll überlieferungen alt griechisch Text Originalsprach verlieren altäthiopischen erhalten Digitaler bibliotheken erscheinen Entwicklung computergestützt Tools umso dringend primär Ziel Projekt Entwicklung digital Korpus annotatio Morphologischer syntaktisch semantisch Ebene annotiert korpus einerseits Diachrone Analyse altäthiopische ermöglichen anderseits Ressource computergestützt prozesse dienen langfristig vergleichend digital Analyse altäthiopisch griechisch digital Perseus verfügbaren arabisch Dokument Ausnahme weniger Text verfügbar elektronisch Ressource altäthiopisch Entwicklung Maschinell lesbar Lexikon widmen Modellierung nächster Sektion erklären Sektion erwähnt Austauschbarkeit laryngalen Sibilant untereinander stellen Modellierungsanforderung Grundform möglich Graphische Variant speichern wobei wohlgemerkt graphisch Variation Fall selbständig anderer Bedeutung existieren lexikonmodell stark Modularisierung Verlinkung einzeln modul unterstützen entscheiden Kenntnis Versuch semitisch Sprache beschreiben Grundkomponente folgen angepasst Zitierform Wort klassisch Lexika semitisch sprechen Regel verbal Repräsentation Wurzel Person Perfekt Singular Maskulin Form unser Lemonmodell lexical entry speichern Lexical entry folgend Module verknüpfen Lexical beinhalten möglich Graphisch Variant Lemmas graphisch Variante Transkription speichern beinhalten subkomponent lexikalisch Eintrag Paradigma Ausnahme morphologisch Realisierung Sonderformen Imperfekt Plural jeweilig morphologisch Kategorie Lemma umfassen setzen Korpusevidenz verstehen Beispiel Korpora Lemma morphologisch Realisierung übersetzungen unterteilt Übersetzung englisch semantisch öquivalente Sprache falls vorhanden arabisch hebräisch syrisch koptisch griechisch sogar sanskrit beinhalten syntaktisch Funktion Lemmas Beispiel syntaktisch Bäume Modul spät Projektphase entwickeln European Union Seventh Framework programme Ideas european research council grant agreement no Project traces from Translation to Creation Change Ethiopic Style and Lexicon from later antiquity to -- middle ages wurzel zentral Stellung semitisch Morphologie Schritt erstellen entsprechen morphologisch Modul Erstellung vollständig automatisieren digital Version trotz Abfassung unverändert Standardwerk geltend Lexicon linguae Aethiopicae August Dillmann zirka Hilfe Regel extrahiern wurzel vollständig Transkription konsonantisch Gerüst zurückgeführt Transkription konsonantisch Wortbildungsschema graphisch Variant Transkription Verfahren extrahiern Automatisierung ermöglichen Mal Sammlung graphisch Variant Wurzel wobei hervorheben wurzeln graphisch Variant Wurzel automatisch homophonen einträge Identischer graphisch Form unterschiedlich Bedeutung verknüpfen erfasst automatisch prozesse lexikoneinträge graphisch Variant falls vorhanden speichern entwerfen graphisch Oberfläche implementieren Oberfläche ermöglichen Visualisierung Einzeleinträg Navigation Manuell korrekturen löschen Einfügen Einträg korrekturen Authority List generierungsquell lexikoneinträge benutzen Zusammenfassung Arbeit Beitrag Modell erklären Wurzel weitgehend computergestützt prozeß realisieren erstellt Software Präsentation Beitrag vorführen Projekt traces März beginnen Laufzeit Erstellung Lexikon sprache Zurzeit zentral Arbeit Projekt wobei derzeit Erstellung Generierungsparadigme Vordergrund stehen Hilfe computerverfahr lexikoneinträge neriern Test eintrag neriern zeigen Automatisierung erheblich Zeitersparnis ermöglichen Literatur Dillmann August Lexicon cum indice Latino Lipsiae Krauwer Steven -- basic Language Resource kit Blark As The first Milestone for -- language resources mccrae John guadalupe buitelaar Paul Cimiano Philipp Declerck Thierry rez Gracia Jorge Hollink Laura Elena spohr Dennis Wunner Tobias The Lemon Cookbook vertan Cristina Zervanou kalliopi van Bosch antal Sporeleder Caroline Proceedings of the workshop -- language Technology for cultural heritage social sciences and humanities Latech Association for computational linguistics Götheborg sweden,"[('wurzel', 0.2688179332667014), ('altäthiopisch', 0.252533372617679), ('sprache', 0.20639047748616185), ('morphologisch', 0.20161344995002606), ('graphisch', 0.20105899923258794), ('semitisch', 0.1803809804411993), ('sektion', 0.1762379317567856), ('konsonantisch', 0.14430478435295943), ('variant', 0.12399243749225886), ('arabisch', 0.12194171073831282)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,"Gleiche Textdaten, unterschiedliche Erkenntnisziele Zum Potential vermeintlich widersprüchlicher Zug‚âà‚Ä†nge zu Textanalyse",Thomas Bögel (Universität  Heidelberg) ;  Michael Gertz (Universität  Heidelberg) ;  Evelyn Gius (Universität  Hamburg) ;  Janina Jacke (Universität  Hamburg) ;  Jan Christoph Meister  (Universität  Hamburg) ;  Marco Petris  (Universität  Hamburg) ;  Jannik Strötgen  (Universität  Heidelberg),"Digitale Heuristik, Machine Learning, NLP-Methoden, Annotation","Digitale Heuristik, Machine Learning, NLP-Methoden, Annotation","1. Einleitung Dieser Beitrag beleuchtet disziplinäre Errungenschaften, die durch die genaue Betrachtung unterschiedlicher disziplinäre Auffassungen von Daten und Erkenntnissen bzw. Erkenntnisinteressen im Projekt heureCLéA ermöglicht wurden und die das große Potential interdisziplinärer Zusammenarbeit im Digital Humanities-_Bereich herausstellen. heureCLéA ist ein Digital Humanities-_Kooperationsprojekt zwischen Literaturwissenschaft und Informatik, in dem eine ""digitale Heuristik"" zur narratologischen Analyse literarischer 1 Texte entwickelt wird. Mit dieser Heuristik sollen (1) bislang nur manuell durchführbare Annotationsaufgaben bis zu einem bestimmten Komplexitätsniveau automatisiert durchgeführt und (2) statistisch auffällige Textphänomene als Kandidaten für eine anschließende Detailanalyse durch den menschlichen Nutzer identifiziert werden können. Dazu wird ein Korpus literarischer Erzählungen kollaborativ manuell annotiert. Anschließend wird mit regelbasierten NLP-_Methoden sowie Machine Learning-_Verfahren an der Entwicklung der Heuristik gearbeitet, die als zusätzliches Modul in die Textanalyseplattform CATMA  implementiert werden wird. 2 1 Das Projekt heureCLéA ist ein vom BMBF gefördertes eHumanities-_Projekt, das von 02/2013-_01/2016 an den Universitäten Hamburg und Heidelberg als Verbundprojekt durchgeführt wird (vgl. dazu auch www.heureclea.de). Zum aktuellen Projektstand vgl. Bögel et al. (im Erscheinen). 2 vgl. www.catma.de 1 Die gemeinsame Frage, wie diese Heuristik erstellt werden soll, und die gemeinsame Betrachtung der literarischen Texte, die als Basis dienen, hat schnell gezeigt, dass es in den beteiligten Disziplinen unterschiedliche Auffassungen über die Qualität von und den Zugang zu Textanalysedaten gibt. So wird etwa der in der Literaturwissenschaft als notwendig geltende Interpretationspluralismus in der NLP als widersprüchlicher Noise betrachtet. Die in der NLP gängige Praxis, Verfahren weniger nach ihrer Nachvollziehbarkeit, sondern vielmehr nach der Qualität ihrer Ergebnisse zu beurteilen, wird wiederum in der Literaturwissenschaft abgelehnt, da dort die Qualität von Verfahren über einen inhaltlichen Austausch über die angewendeten Verfahren ausgehandelt wird. Unser Beitrag will auf die möglichen methodischen und methodologischen Konsequenzen solcher disziplinär unterschiedlicher Zugänge zum Forschungsgegenstand 'in unserem Fall: zu Texten und zu Textanalyse 'in der Zusammenarbeit im Digital Humanities-_Bereich hinweisen. Im Fokus stehen dabei zwei exemplarische Konsequenzen in den beteiligten Disziplinen: (1) der narratologische Workflow, für den eine Erweiterung des traditionellen hermeneutischen Zugangs zur Textanalyse entwickelt wurde, sowie (2) der für das Machine Learning gewählte Zugang der NLP, der sich durch eine besonders hohe Prozesstransparenz von klassischen Machine Learning-_Ansätzen unterscheidet. Beide Beispiele sind aus unserer Sicht exemplarisch für Interferenzen, die von Digital Humanities-_Projekten erzeugt werden können. Diese Interferenzen bedeuten vorerst Störungen des geplanten Forschungsprozesses und erzeugen teilweise erheblichen Mehraufwand. Gelingt die Lösung der damit verbundenen Probleme, generieren sie aber einen Mehrwert sowohl für den Projekterfolg als auch für den von der Projektzusammenarbeit unabhängigen Fortschritt der beteiligten Disziplinen. 2. Die Erweiterung des traditionellen Zugangs zu literaturwissenschaftlicher Textanalyse Die für die Entwicklung der Heuristik in heureCLéA eingesetzten NLP-_Verfahren werden auf ein Korpus 21 deutschsprachiger Erzählungen um etwa 1900 angewendet, das mit dem Textanalysetool CATMA annotiert wird. Das oben erwähnte Noise-_Problem wird dadurch abgemildert, dass die Texte von mehreren Annotatorinnen mit Markup versehen werden. 3 Dieser Zugang verändert den traditionellen Prozess der Textanalyse in der Literaturwissenschaft zweifach. 3 Für eine ausführlichere Beschreibung der durch das Spannungsfeld von Informatik und Hermeneutik bedingten Problematik und ihre Auswirkung auf die Anforderungen an die manuelle Annotation vgl. Gius & Jacke (in Vorbereitung). Dort werden auch die methodologischen Konsequenzen für die narratologische Theorie dargelegt. 2 Erweiterte Analysegrundlage durch Annotation Eine offensichtliche Veränderung zum traditionellen Zugang ist die Erweiterung der betrachteten Datenbasis bzw. der Annahmen über diese. Zu den Vorannahmen des Textinterpreten, den Annahmen über Textteile und den Annahmen über das Textganze, die sich immer wieder gegenseitig beeinflussen und dadurch die Annahmen bestätigen oder modifizieren, kommen die in den Annotationen festgehaltenen Annahmen weiterer Interpretinnen. Der traditionelle hermeneutische Zugang zu Texten wird hier also nicht nur durch das Annotieren selbst 'wie weiter unten ausgeführt 'intensiviert, sondern auch um 4 Annahmen anderer ergänzt. Dadurch wird gewissermaßen die Grundlage für die weitere Analyse erweitert. close(r) reading durch kollaborative, computergestützte Analysen Der computergestützte Zugang an sich forciert bereits durch sein Sichtbarmachen der Analysen in den Annotationen ein intensiveres close reading als Textanalyseverfahren, in denen Interpretationen ohne eine ausführliche Dokumentation zugrundeliegender Analysen generiert werden. Durch die kollaborative Annotation derselben Texte durch mindestens zwei Annotatoren wird außerdem offensichtlich, an welchen Stellen es keine intersubjektive Übereinstimmung zwischen den Annotatorinnen gibt. Dies führte u.a. dazu, dass schnell deutlich wurde, dass die aus Gius (2013) übernommenen Beschreibungen der narratologischen Analysekategorien in der vorliegenden Form nicht als Arbeitsgrundlage für heureCLéA ausreichen. Deshalb wurden zusätzlich Annotationsguidelines erarbeitet, die die Beschreibung der narratologischen Kategorien weiter systematisieren: Neben der Beschreibung und Operationalisierung des Phänomens enthalten die Guidelines Angaben zum typischen Umfang der getaggten Textmenge (etwa Wort/Wortgruppe, Satz, Absatz etc.), zu unmarkierten Fällen, die nicht annotiert werden, zu Indikatoren auf der Textoberfläche, zur Taggingroutine sowie Textbeispiele zur betreffenden Tagkategorien (vgl. 5 Abbildung 1). Die Taggingroutine zielt dabei insbesondere darauf ab, die Analyse so zu organisieren, dass die damit verbunden Aktivitäten in einer von einfachen zu komplexeren Aktivitäten geordneten Reihenfolge ausgeführt werden können. 6 4 vgl. zu den für das hermeneutische Verfahren relevanten Aspekten z.B. Bühler (2003). 5 vgl. Gius & Jacke (2014). 6 Dasselbe Verfahren wird in heureCLéA auch auf die Reihenfolge der annotierten Phänomen angewendet. Dieses an der Komplexität der Analysekatogerien orientierte Vorgehen wurde bereits in Gius (2013) auf Ebene der narratologischen Phänomenbereiche entwickelt und erfolgreich angewendet. 3  ¬†Abbildung 1: Annotation von Ordnung, Zusammenfassung aus den Guidelines (vgl. Gius & Jacke 2014) Das close reading wird außerdem durch Diskussionen intensiviert, die zwischen den Annotatoren stattfinden, wenn sie nach ihrem ersten Annotationsdurchgang die gesetzten Annotationen mit denen der anderen vergleichen. Die der hermeneutischen Textanalyse eigene fortdauernde Bewegung zwischen Text und Analyse/Interpretation des Textes, deren Erkenntnisse wiederum in die erneute Betrachtung des Textes mit einfließen, wird durch die beiden durch die interdisziplinäre Zusammenarbeit notwendigen Erweiterungen des Zugangs sowohl in Bezug auf die Analyse bzw. Interpretation als auch in Bezug auf das zur Verfügung stehende Interpretationsmaterial wesentlich verstärkt (vgl. Abbildung 2). 4  ¬†Abbildung 2: Der erweiterte hermeneutische Zirkel 3. NLP vor dem Hintergrund besonderer Textdomänen und notwendiger Transparenz von automatischen Entscheidungsprozessen Bei der Verarbeitung deutscher literarischer Texte im Kontext einer Zusammenarbeit mit Narratologen stellen sich im Bereich der NLP zwei Hauptaspekte heraus: Zum einen bedingt der Fokus auf eine spezielle Textdomäne die Anpassung und den flexiblen Einsatz von NLP-_Komponenten, die zumeist für Zeitungstexte optimiert sind. Auf der anderen Seite ergeben sich im Bereich der Modellbildung insbesondere im Bereich des maschinellen Lernens spezifische Herausforderungen, um die Akzeptanz von automatischen Annotationen sicherzustellen. Beide Aspekte sollen im Folgenden erläutert werden. Der NLP-_Workflow Zur Erfassung und automatischen Vorhersage linguistischer Oberflächenphänomene 7 entwickelten wir eine flexible und modulare NLP-_Pipelinearchitektur auf Basis von UIMA , die Annotationen mit steigendem Komplexitätsgrad vornimmt und die Ergebnisse in einem Schichtenmodell speichert. Die modular aufgebaute Pipeline ermöglicht einen flexiblen Austausch von Komponenten. Diese Flexibilität ist im Kontext unserer Textdomäne, also literarischer Texte, besonders 7 http://uima.apache.org/ 5 hilfreich und unabdingbare Voraussetzung, wie sich im Verlauf des Projekts gezeigt hat. Da NLP-_Komponenten auf der Domäne von Zeitungstexten entwickelt werden, funktionieren viele Systeme nur auf einem Teil der Daten ähnlich qualitativ gut wie auf der Ursprungsdomäne. Details zur Architektur und den verwendeten NLP-_Komponenten sind in Bögel et al. (2014) beschrieben. Sichtbarmachung von Entscheidungsprozessen im maschinellen Lernen Neben Features, die die Grundvoraussetzung für die Modellierung maschineller Lernverfahren darstellen und aus der oben dargestellten Pipeline gewonnen werden, ergeben sich auch bei der Wahl des konkreten Lern-_Algorithmus interessante Aspekte durch das Gesamtprojekt. In der Theorie des maschinellen Lernens werden Modelle und Gesamtsysteme danach bewertet, welchen empirischen Fehler sie auf ungesehenen Testdaten produzieren (Vapnik, 1998). Ein ideales System würde auf ungesehenen Daten perfekte Ergebnisse liefern und keine Fehler bei der Vorhersage machen. Vor dem Hintergrund unseres Kollaborationsprojektes zeigt sich jedoch, dass die Minimierung des Fehlers von Annotationen nur ein Qualitätsaspekt von Algorithmen ist. Um höhere Akzeptanz von Ergebnissen solcher Systeme zu erreichen, müssen sie einerseits verlässliche Vorhersagen produzieren, aber andererseits auch transparenten, nachvollziehbaren Entscheidungsprozessen zugrundeliegen. Mit zunehmendem Komplexitätsgrad maschineller Lernverfahren sinkt jedoch die direkte Nachvollziehbarkeit. So ist bei einer Support Vector Machine (Hearst et al., 1998), einem Standardverfahren des maschinellen Lernens, nicht ohne Weiteres nachvollziehbar, weshalb eine konkrete Entscheidung getroffen wurde und welche Einzelentscheidungen und Featurekonstellationen konkret zum Endergebnis gefüht haben. Derartige Black-_Box-_Ansätze erschweren jedoch die Akzeptanz automatischer Annotationen. Ein Beispiel für nachvollziehbare Algorithmen stellen Entscheidungsbäume (decision trees) dar, wie sie in Quinlan (1986) erstmalig beschrieben sind. Durch eine Visualisierung des Modells ist es möglich (vgl. Abbildung 3), jede Teilentscheidung, die zur Klassifikation beigetragen hat, nachzuvollziehen und den Einfluss von individuellen Kriterien (Features) zu verfolgen. Abgesehen von der Nachvollziehbarkeit und Transparenz verhindern Black-_Box-_Ansätze auch direkte Eingriffsmöglichkeiten in den Vorhersageprozess. Für die Vorhersage bestimmter Phänomene (beispielsweise der Erzählgeschwindigkeit in Erzähltexten), die 6 ambigen Konzepten zugrunde liegen, können verschiedene Features als relevant erachtet werden. Bezogen auf Abbildung 3 wäre es so beispielsweise möglich, ein Feature zu entfernen und die Auswirkungen auf das neue Modell direkt zu beobachten. Abbildung 3: Schematische Visualisierung eines Decision Trees. Dieses dargestellte Transparenz-_ und damit auch Akzeptanz-_Problem stellt sich für maschinelle Lernprozesse grundsätzlich, wenn sie abseits eines reinen Selbstzwecks in einen konkreten Anwendungskontext eingebettet werden, anstatt für synthetische Benchmarks Ergebnisse zu produzieren. 4. Gemeinsame Erkenntnisse aus der interdisziplinären Arbeit Die hier beschriebenen, durch die Zusammenarbeit veränderten Bedingungen der Textanalyse sind aus unserer Sicht typisch für Ansätze im Bereich der Digital Humanities und werden von den dort häufig genutzten kollaborativen Verfahren verstärkt. Damit wird offensichtlich, dass der Einsatz neuer Methoden nicht nur die Bearbeitung neuer Fragestellungen ermöglicht, sondern auch traditionelle Methoden wie etwa die für die Literaturwissenschaft zentrale Methode der hermeneutischen Textanalyse oder den ergebnisorientierten Zugang der NLP ergänzt bzw. modifiziert 'und dadurch so weiter entwickelt, dass sowohl die interdisziplinäre als auch die disziplinäre Forschungsarbeit von der Entwicklung profitiert. In beiden Fällen hat die Erhöhung der Transparenz der genutzten Prozesse gemäß den methodischen Bedarfen der anderen Disziplin maßgeblich zum Erfolg der Weiterentwicklung 7 beigetragen. Entsprechend wäre es interessant zu prüfen, ob dies generell eine produktive Strategie zur methodischen und methodologischen Verbesserung von in interdisziplinären Digital Humanities-_Projekten genutzten Forschungsstrategien ist. Bibliographie Bögel, T. & Gertz, M., Gius, E. & Jacke, J. & Meister, J.C & Petris, M. & Strötgen, J. (im Erscheinen). Collaborative Text Annotation Meets Machine Learning. heureCLéA, a Digital Heuristics of Narrative. DHCommons Journal. Bögel, T. & Strötgen, J. & Gertz, M. (2014). Computational Narratology: Extracting Tense Clusters from Narrative Texts. Proceedings of the 9th Edition of the Language Resources and Evaluation Conference (LREC'14). Reykjavik, Iceland, S. 950-_955. Bühler, A. (2003). Grundprobleme der Hermeneutik. Hermeneutik. Basistexte zur Einführung in die wissenschaftstheoretischen Grundlagen von Verstehen und Interpretation. Hg. von Axel Bühler. Heidelberg: Synchron, S. 3-_19. Gius, E. (2013). Erzählen über Konflikte. Eine computergestützte narratologische Untersuchung von narrativen Interviews zu Arbeitskonflikten. Dissertation, Universität Hamburg. Gius, E. & Jacke, J. (in Vorbereitung). Informatik und Hermeneutik. Zum Mehrwert interdisziplinärer Textanalyse. Zeitschrift für digital Humanities. Gius, E. & Jacke, J. (2014). Zur Annotation narratologischer Kategorien der Zeit. Guidelines zur Nutzung des CATMA-_Tagsets. Hamburg. http://heureclea.de/publications/guidelines.pdf/ Hearst, M.A. & Dumais, S.T. & Osman, E. & Platt, J. & Scholkopf, B. (1998). Support Vector Machines. Intelligent Systems and their Applications 13 (4), IEEE, S. 18-_28. Quinlan, J.R. (1986). Induction of Decision Trees. Machine learning 1 (1), S. 81-_106. Vapnik, V. N. (1998). Statistical Learning Theory. Vol. 2. New York: Wiley.",de,Einleitung Beitrag beleuchten disziplinär Errungenschaft genau Betrachtung unterschiedlich disziplinär Auffassung daten Erkenntnis Erkenntnisinteressen Projekt Heurecléa ermöglichen Potential interdisziplinär Zusammenarbeit Digital herausstellen Heurecléa Digital Literaturwissenschaft Informatik digital Heuristik narratologisch Analyse Literarischer Text entwickeln Heuristik bislang manuell durchführbar Annotationsaufgabe bestimmt Komplexitätsniveau automatisieren durchführen statistisch auffällig textphänomene Kandidat anschließend detailanalyse menschlich Nutzer identifizieren korpus literarisch erzählung kollaborativ manuell annotieren anschließend regelbasierten machin Entwicklung Heuristik arbeiten zusätzlich Modul Textanalyseplattform Catma implementieren Projekt Heurecléa Bmbf gefördertes Universität Hamburg Heidelberg verbundprojekt durchführen aktuell Projektstand Bögel et erscheinen gemeinsam Frage Heuristik erstellen gemeinsam Betrachtung literarisch Text Basis dienen schnell zeigen beteiligt disziplinen unterschiedlich Auffassung Qualität Zugang textanalysedaten Literaturwissenschaft notwendig geltend Interpretationspluralismus Nlp widersprüchlich Noise betrachten nlp gängig Praxis Verfahren Nachvollziehbarkeit vielmehr Qualität Ergebnis beurteilen wiederum Literaturwissenschaft ablehnen Qualität Verfahren inhaltlich Austausch angewendeten Verfahren aushandeln Beitrag möglich methodisch methodologisch Konsequenz disziplinär unterschiedlich zugänge Forschungsgegenstand unser Fall texten Textanalyse Zusammenarbeit Digital hinweisen Fokus stehen exemplarisch Konsequenz beteiligt disziplinen narratologisch Workflow Erweiterung traditionell hermeneutisch Zugang Textanalyse entwickeln machin Learning gewählt Zugang Nlp hoch Prozesstransparenz Klassisch Machine unterscheiden Beispiel Sicht exemplarisch interferenzen Digital erzeugen Interferenz bedeuten vorerst Störung geplant Forschungsprozesse erzeugen teilweise erheblich Mehraufwand gelingen Lösung verbunden Problem generieren mehrweren sowohl Projekterfolg Projektzusammenarbeit unabhängig Fortschritt beteiligter disziplinen Erweiterung traditionell Zugang literaturwissenschaftlich Textanalyse Entwicklung Heuristik Heurecléa eingesetzt Korpus deutschsprachig erzählungen anwenden Textanalysetool Catma annotiert erwähnt abmildern Text mehrere Annotatorinn Markup versehen Zugang verändern traditionell Prozess Textanalyse Literaturwissenschaft zweifach ausführlich Beschreibung Spannungsfeld Informatik Hermeneutik bedingt Problematik Auswirkung Anforderung Manuelle Annotation Gius Jacke Vorbereitung methodologisch Konsequenz narratologisch Theorie darlegen erweitert Analysegrundlage Annotation offensichtlich Veränderung traditionell Zugang Erweiterung betrachtet datenbasis annahmen Vorannahm Textinterpret annahmen Textteil annahmen Textganze gegenseitig beeinflussen annahmen bestätigen modifizieren annotation Festgehalten Annahm weit Interpretinn traditionell hermeneutisch Zugang texten annotieren unten ausführen intensivieren Annahme anderer ergänzen gewissermaßen Grundlage Analyse erweitern Close r Reading Kollaborative computergestützt Analyse computergestützt Zugang forcieren sichtbarmache Analyse annotatio intensiver clos Reading textanalyseverfahrer Interpretation ausführlich dokumentation zugrundeliegend analyse neriern kollaborativ Annotation Text mindestens Annotator offensichtlich stellen intersubjektiv übereinstimmung Annotatorinn führen schnell deutlich Gius übernommen Beschreibung narratologisch Analysekategorie vorliegend Form Arbeitsgrundlage Heurecléa ausreichen zusätzlich annotationsguidelin erarbeiten Beschreibung narratologisch kategorien systematisieren Beschreibung Operationalisierung Phänomen enthalten guidelin Angabe typisch Umfang getaggter Textmenge Wort Wortgruppe Satz Absatz unmarkierten fällen annotiert indikatoren Textoberfläche Taggingroutine Textbeispiele betreffend Tagkategorien Abbildung Taggingroutine zielen insbesondere Analyse organisieren verbinden Aktivität einfach komplex Aktivität geordnet Reihenfolge ausführen hermeneutisch Verfahren relevann Aspekt Bühler Gius Jacke Verfahren Heurecléa Reihenfolge annotierter Phänom anwenden Komplexität Analysekatogerie orientiert vorgehen Gius Ebene narratologisch Phänomenbereiche entwickeln erfolgreich anwenden Annotation Ordnung Zusammenfassung guidelin Gius jack clos Reading Diskussion intensivieren Annotator stattfinden Annotationsdurchgang gesetzt Annotation vergleichen hermeneutisch Textanalyse fortdauernd Bewegung Text Analyse interpretation Text erkenntnis wiederum erneut Betrachtung Text einfließen interdisziplinär Zusammenarbeit notwendig Erweiterunge Zugang sowohl Bezug Analyse Interpretation Bezug Verfügung stehend interpretationsmaterial wesentlich verstärken Abbildung erweitert hermeneutisch Zirkel Nlp Hintergrund besonderer Textdomän notwendig Transparenz automatisch entscheidungsprozessen Verarbeitung deutsch literarisch Text Kontext Zusammenarbeit narratolog Stelle Bereich Nlp Hauptaspekt heraus bedingt Fokus speziell Textdomäne Anpassung Flexibl Einsatz zumeist zeitungstext optimieren Seite ergeben Bereich Modellbildung insbesondere Bereich maschinell Lernen spezifisch Herausforderung Akzeptanz automatisch Annotation sicherstellen Aspekt folgend erläutern Erfassung automatisch Vorhersage linguistisch oberflächenphänomen entwickeln flexibel modular Basis Uima Annotation steigend Komplexitätsgrad vornehmen Ergebnis schichtenmodell speichern Modular aufgebaut Pipeline ermöglichen flexiblen austausch Komponente Flexibilität Kontext Textdomäne literarisch Text hilfreich unabdingbar Voraussetzung Verlauf Projekt zeigen Domäne zeitungstexten entwickeln funktionieren System daten ähnlich qualitativ ursprungsdomäne Detail Architektur verwendet Bögel Et beschreiben Sichtbarmachung Entscheidungsprozesse maschinell Lernen Features Grundvoraussetzung Modellierung Maschineller lernverfahren darstellen dargestellt Pipeline gewinnen ergeben Wahl konkret interessant Aspekt Gesamtprojekt Theorie maschinell lernen Modell Gesamtsystem bewerten empirisch Fehler ungesehen Testdate produzieren Vapnik ideal System ungesehen daten perfekt ergebnisse liefern Fehler Vorhersage Hintergrund unser kollaborationsprojektes zeigen Minimierung Fehler annotatio qualitätsaspekt Algorithm hoch Akzeptanz Ergebnis System erreichen einerseits verlässlich Vorhersag produzieren andererseits Transparent Nachvollziehbaren entscheidungsprozessen zugrundeliegen zunehmend Komplexitätsgrad Maschineller lernverfahren sinken direkt Nachvollziehbarkeit Support vector Machine hearst Et Standardverfahren maschinell Lernen nachvollziehbar weshalb konkret Entscheidung treffen einzelentscheidungen Featurekonstellatione konkret Endergebnis fühen derartig erschweren Akzeptanz automatisch Annotation nachvollziehbar algorithm stellen entscheidungsbaum Decision Trees dar quinlan erstmalig beschreiben Visualisierung Modell Abbildung Teilentscheidung Klassifikation beitragen nachvollziehen einfluss individuell kriterien Feature verfolgen absehen Nachvollziehbarkeit Transparenz verhindern direkt eingriffsmöglichkeien Vorhersageprozess Vorhersage bestimmt Phänomen beispielsweise Erzählgeschwindigkeit Erzähltext ambig Konzept zugrunde liegen verschieden Feature relevant erachten beziehen Abbildung beispielsweise Feature entfernen Auswirkung Modell direkt beobachten Abbildung schematisch Visualisierung Decision trees dargestellt stellen maschinell Lernprozesse grundsätzlich abseits rein Selbstzwecks konkret Anwendungskontext einbetten anstatt synthetisch Benchmark Ergebnis produzieren gemeinsam erkenntnis interdisziplinär Arbeit beschriebenen Zusammenarbeit verändert Bedingung Textanalyse Sicht typisch Ansatz Bereich Digital Humanitie häufig genutzt kollaborativ Verfahren verstärken offensichtlich Einsatz neu Methode Bearbeitung neu Fragestellung ermöglichen traditionell Methode Literaturwissenschaft zentral Methode hermeneutisch Textanalyse ergebnisorientiert Zugang Nlp ergänzen modifizieren entwickeln sowohl Interdisziplinär disziplinär Forschungsarbeit Entwicklung profitieren Fall Erhöhung Transparenz genutzt prozesse gemäß methodisch bedarfen Disziplin maßgeblich Erfolg Weiterentwicklung beitragen entsprechend interessant prüfen generell produktiv Strategie methodisch methodologisch Verbesserung interdisziplinären Digital genutzt Forschungsstrategie Bibliographie Bögel Gertz Gius Jacke Meister Petris Strötg erscheinen Collaborative Text Annotation Meet Machine Learning Heurecléa Digital heuristics of Narrative Dhcommons Journal Bögel Strötg Gertz Computational narratology extracting tense Cluster from Narrative texts proceedings of the edition -- -- language resources and Evaluation Conference Reykjavik Iceland Bühler Grundproblem Hermeneutik Hermeneutik Basistext Einführung wissenschaftstheoretisch Grundlag verstehen Interpretation hg Axel Bühler Heidelberg Synchron Gius erzählen Konflikt computergestützt narratologisch Untersuchung narrativ Interview arbeitskonflikten Dissertation Universität Hamburg Gius Jacke Vorbereitung Informatik Hermeneutik Mehrwert interdisziplinärer Textanalyse Zeitschrift Digital Humanitie Gius Jacke Annotation narratologischer kategorien guidelines Nutzung Hamburg hearst dumais Osman platt Scholkopf support vector machines intelligent systems and their Applications Ieee quinlan induction of Decision trees Machine Learning Vapnik statistical learning Theory vol New York wiley,"[('heurecléa', 0.20740434386894907), ('zugang', 0.18089651705753584), ('gius', 0.1728434678657554), ('narratologisch', 0.15198704205039523), ('nlp', 0.1437771911627976), ('disziplinär', 0.130633859481637), ('textanalyse', 0.1297327691372119), ('annotation', 0.12369695551945939), ('jacke', 0.12218765253140186), ('hermeneutik', 0.11851676792511376)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Comedia - Comédie: Topic Modeling als Perspektive auf das spanische und französische Theater des 17. Jhdts.,Christof Schöch (Universität Würzburg); Nanette Rißler-Pipka (Universität Siegen),"Topic Modeling, Dramen","Topic Modeling, Dramen","1. Hintergrund Im Europa des 17. Jahrhunderts entwickelten sich zeitgleich verschiedene Formen des Theaters. Trotz unterschiedlicher sozialer und poetologischer Kontexte weisen das spanische und französische Theater viele (stoffliche / stilistische) Verbindungen auf, die auf eine gemeinsame europäische Theatergeschichte hindeuten (Couderc 2013). Die Frage, ob man dazu nicht Methoden der Digital Humanities nutzen sollte, stellte sich bisher in der Romanistik nicht (anders als in weiteren Philologien, vgl. Rybicki 2012, Jockers 2013, Eder 2014). Allgemein gilt, dass sprachübergreifende, quantitative Textanalysen eine Herausforderung bleiben (vgl. Steinberger 2009, Eder/Rybicki 2011). In romanistischer Tradition über Sprachgrenzen hinweg quantitative Verfahren anzuwenden, scheint mit Topic Modelling möglich zu sein: Die Topics mehrerer einheitlich strukturierter Textsammlungen können zunächst unabhängig voneinander modelliert werden, um dann auf der Grundlage von Topic-Labels und strukturellen Merkmalen Öhnlichkeiten und Unterschiede zu ermitteln. 2. Fragestellungen Anstelle von Einzelhypothesen und konkreter Passagenvergleiche sind hier zwei spanische und französische Textsammlungen durch Topic Modeling verglichen worden. Welche Arten von Topics liegen vor, und wie verhalten sie sich zueinander? Welche Relation besteht zwischen den Topics und Kategorien wie Untergattungen (Komödie / Tragödie)? Wie gestaltet sich dies im Vergleich des spanischen und französischen Theaters? Über diese Fragen hinaus soll die Eignung der Methode für Theaterstücke geprüft werden. Wie verhalten sich die ""Topics"" zu theaterwissenschaftlich relevanten ""Themen""? Welche Perspektivenverschiebung ergibt sich durch ein quantitatives Verfahren wie Topic Modeling? 3. Textsammlungen Die spanische Textsammlung enthält 145 Theaterstücke von sechs Autoren. Die Stücke sind zwischen 1585 und 1688 erschienen. Die Untergattungen sind ""drama"", ""comedia"" und ""auto sacramental"". Die Texte stammen von www . comedias . org, Wikisource und der Biblioteca Cervantes. Die französische Textsammlung enthält 143 Theaterstücke von neun Autoren. Die Stücke sind zwischen 1630 und 1708 erschienen, und stammen von www.theatre-classique.fr. Die Untergattungen sind ""comédie"", ""tragédie"" ""tragi-comédie"" und ""pastorale"". 1 4. Methode: Topic Modeling Topic Modeling ist ein quantitativer Ansatz, um in größeren Textsammlungen thematische Muster zu entdecken (Blei 2003; Anwendungen in den DH: Blevins 2010, Rhody 2012, Jockers 2013). Mathematisch gesehen sind ""Topics"" Verteilungen von Auftretenswahrscheinlichkeiten von Wörtern. Die Wörter eines Topic mit der höchsten Auftretenswahrscheinlichkeit sind sich semantisch (oder anderweitig) ähnlich (vgl. Blei 2011 und Steyvers & Griffiths 2007). Durch Verknüpfung mit Metadaten können thematische Trends über einen Zeitverlauf oder thematische Differenzen zwischen Textgattungen entdeckt werden. Wichtige Parameter sind das Präprozessieren der Texte (bspw. Lemmatisierung), die Auswahl der zu berücksichtigenden Wörter (nach Wortarten, Wortfrequenzen oder Stoplist), die Textsegmentierung sowie die Anzahl der Topics, die gefunden werden sollen. Für diese Studie wurden die Texte mit TreeTagger (Schmidt 1994) lemmatisiert und nach Wortarten annotiert. Es wurden verschiedene Textfassungen generiert, die bspw. nur Substantive und Verben enthalten, die Texte in Segmente von 40 Lemmata zerlegt und Topic Modeling mit MALLET (McCallum 2009) durchgeführt. Die Anzahl der Topics wurde auf 50 bzw. 200 festgelegt. 5. Ergebnisse und Diskussion 5.1 Die ermittelten Topics Die ermittelten 50 Topics lassen sich meist mit einem Begriff zusammenfassen, der die inhaltliche Gemeinsamkeit der wichtigsten Worte im Topic fasst. Es gibt allgemeinere und spezifische Topics mit unterschiedlichem Gewicht in der Textsammlung, was hier am Beispiel der französischen Topics gezeigt wird (Abb. 1). Abb. 1: Auswahl von Topics aus der französischen Textsammlung. Einige Topics betreffen allgemein gefasste, erwartbare Themen, wie bspw. Liebe / Intrigen (5 der 6 wichtigsten Topics gehören in diesen Themenbereich). Das Liebestopic enthält oft ein Element des Schmerzes und Hasses, das auf die Tragödie hindeutet (Topic 14). Dagegen lässt sich ein inhaltlich typisches Komödientopic nur durch selbstreferentiellen Begriffe erkennen (Topic 34). Faktisch am distinktivsten für die Komödie sind dagegen ein relativ unbestimmtes Topic (33, ""Suchen-Finden"") sowie Topic 01 (""Vergnügen""; vgl. 5.3). Andere Topics sind spezifischer (bspw. Topic 11, ""Gefahr"" oder Topic 24 ""Geheimnis"") und könnten vermuten lassen, dass sie mit bestimmten Untergattungen des Theaters verknüpft sind (bspw. Topics 30 und 38, ""Verbrechen""). Allerdings zeigt ein Vergleich von Topics und Textklassen (vgl. 5.3.), dass Topic 38 zwar der Tragödie zugeordnet werden kann, es in Topic 30 aber offenbar um ein ""Verbrechen"" geht, das sich in Komödien abspielt. 5.2 Die Topics im Vergleich Vergleicht man die Topics der französischen und der spanischen Textsammlung miteinander, stellt man einige Übereinstimmungen und Unterschiede fest (Abb. 2). 2 Abb. 2: Topics im Sprachvergleich In beiden Textsammlungen präsent sind allgemeine Topics, wie diejenigen um das Thema ""Liebe"" (bspw. Topic 41fr vs. Topic 35sp). Zwar kann man, abgesehen von einer leichten Tendenz in Richtung Lust (""celo, ver"") im spanischen und Leid (""souffrir"") im französischen Topic, kaum von einer semantischen Differenz sprechen. Dennoch kann Topic 35sp in der Topicverteilung nach Gattungen (vgl. 5.3) der (spanischen) ""Comedia"" zugeordnet werden, während Topic 41fr der (französischen) Tragödie zugeordnet wird. Die ähnlich große Wichtigkeit beider Topics in den jeweiligen Korpora belegt die stoffgeschichtliche Verwandtschaft des Theaters beider Länder. Auch bei noch spezifischeren Topics gibt es zahlreiche Übereinstimmung, bspw. Topic 4fr und Topic 2sp, die beide mit dem Titel ""Gnade-Gottes"" versehen werden könnten, oder sehr konkrete Topics wie ""Krieg"" (Topic 46fr und 13sp) oder ""Arzt"" (Topic 7fr und 17sp), die mit fast identischen Wörtern vorkommen. Topics in der spanischen Sammlung ohne Übereinstimmung in der französischen Sammlung sind bspw. Topic 45 (""Schuld-Unschuld"") oder 16 (""Gericht-König""). Umgekehrt sind Topics in der französischen Sammlung ohne Übereinstimmung in der spanischen Sammlung bspw. Topic 18 ""Gehorsam"" oder 38 ""Verbrechen""). Diese Ergebnisse bieten Ausgangspunkte für einen Abgleich mit Erkenntnissen der Literaturgeschichte. 5.3 Topics und Textklassen Mit unterschiedlicher Ausprägung zeigt sich in beiden Textsammlungen, dass die Untergattungen jeweils mindestens einen charakteristischen Topics besitzen (Abb. 3 und 4). 3 Abb. 3: Heatmap für Topic-Scores in Genres (Spanisch) (20 Topics mit größter Varianz, gemessen als Standardabweichung) Der wesentliche Kontrast bei den spanischen Stücken (Abb. 3) liegt zwischen ""Auto sacramentales"" einerseits, ""Comedias"" und ""Dramas"", andererseits. Letztere haben schwächer kontrastive Topics, bspw. Topic 35 (""Liebe-Hoffnung"") oder, auf niedrigerem Niveau, Topic 49 (unklar). Bei den französischen Stücken hat jede Untergattung zumindest einen charakteristischen Topic (Abb. 4): Topic 33 (""Suchen-Finden"") für die Komödie, Topic 41 (""Liebe-Hoffnung"") für die Tragödie, Topic 08 (""Liebe-Schönheit"") für die Pastorale. Ausnahme ist die Tragikomödie. Abb. 4: Heatmap für Topic-Scores in Genres (Französisch) (20 Topics mit größter Varianz, gemessen als Standardabweichung) Insgesamt scheint die gattungsbezogene Trennschärfe in den französischen Texten deutlicher als in den spanischen Texten. Dieser Befund entspricht den unterschiedlichen französischen und spanischen Poetiken der Zeit. 5.4 Gruppierung auf Grundlage von ""topic scores"" Es ist nicht auszuschließen, dass die verwendeten Gattungsbezeichnungen tatsächlich vorhandene Differenzierungen verdecken. Ohne vorgängige Kategorien, nur auf Grundlage der Öhnlichkeit von Stücken nach der Verteilungen von 200 Topics sollten daher mit Principal Component Analysis Strukturen in den Textsammlungen gefunden werden. Die räumliche Verteilung der Stücke zeigt für die spanischen Texte kaum Struktur und bildet eine recht einheitliche Wolke (Abb. 6). Die französischen Texten (Abb. 5) zeigen mehr Struktur: ein kompakterer, leicht separierter Bereich rechts oben sowie ein weiterer, besonders dichter Bereich links oben. Die in den ersten beiden Komponenten enthaltene Varianz der Daten ist mit zusammen 17,3% (französisch) und 9,2% (spanisch) verhältnismäßig gering. 4 Abb. 5: PCA-Plot auf Grundlage von 200 topic scores (französische Sammlung, Genre-Labels) Die Verteilung der Gattungssymbole zeigt, dass die französischen Texte nach Gattungen gruppiert sind: rechts oben die Komödien, links oben die Tragödien; die stärker verteilten Tragikomödien überlappen vor allem mit den Tragödien. Bei den spanischen Texten gibt es ebenfalls Gruppen: die ""Auto Sacramentales"" im linken unteren Quadranten, die ""Comedias"" eher in der rechten Hälfte, die Dramen breit gestreut in der Mitte. 5 Abb. 6: PCA-Plot auf Grundlage von 200 topic scores (spanische Sammlung, Genre-Labels) Einfache Korrelationstests bestätigen den Gesamteindruck (Abb. 7). In den französischen Stücken korreliert Genre sehr deutlich nur mit PC1, Autorschaft dagegen vor allem mit PC2. Bei den spanischen Stücken ist nur die Korrelation zwischen Autorschaft und PC1 stark. Abb. 7: Korrelationtests zwischen Principal Components und Autorschaft bzw. Gattungszugehörigkeit 6 Die thematische Differenzierung der Stücke ist also in der französischen Textsammlung stärker ausgeprägt und korreliert auch stärker mit den vorhandenen Gattungs-Kategorien als in der spanischen Textsammlung. Bilanz und nächste Schritte Zahlreiche Einzelergebnissen zum Verhältnis der inhaltlichen Bestimmung einzelner Topics und ihrer eventuellen Zuordnung zu Untergattungen des Theaters zeigen, dass sich spanisches und französisches Theater auf Grundlage der Topic-Verteilungen auf eine Weise unterscheiden, die gattungspoetischen Positionen der Zeit entspricht und an vorhandene literaturwissenschaftliche Erkenntnisse anschlussfähig ist. Außerdem zeigen die Ergebnisse den Unterschied zwischen ""Topics"" und ""Themen"" im literaturwissenschaftlichen Sinn. Der semantische Gehalt des Topics, der in einem Begriff wie ""Liebe-Leidenschaft"" (Topic 14) gebündelt werden kann, beschreibt nicht unbedingt das zentrale Thema der sich dahinter verbergenden Theaterstücke (vgl. die Diskussion der Tragödien-, Komödien und Pastoralentopics). Diese vermeintliche Kluft zwischen Topics und literaturwissenschaftlichen Themen ist aber eher eine Chance als ein Dilemma: so lassen sich vorschnelle Interpretationsansätze überprüfen und neue Erkenntnisse gewinnen. Methodisch wird deutlich, dass Topic Modeling selbst nur ein Schritt in der Analyse- und Interpretationskette sein kann, der durch linguistische Annotation und Metadaten vorbereitet werden muss, und dessen Ergebnisse durch weitere Verarbeitung und Kontextualisierung erst bedeutungsvoll werden. Als nächste Schritte könnte die Textsammlung erweitert und um weitere Metadaten ergänzt werden, um die Vergleichbarkeit der Textsammlungen zu erhöhen. Es könnte mit ""Multilingual Topic Modeling"" (Boyd-Graber & Blei 2009) operiert werden, das unmittelbar thematische Bezüge zwischen Dokumenten in unterschiedlichen Sprachen ermittelt. Alternativ wäre ein algorithmisches Verfahren zur Öhnlichkeitsbestimmung verschiedensprachiger Topics zu entwickeln (vgl. Pouliquen 2006). 7 Bibliographie Blei, David M. 2011. ""Introduction to Probabilistic Topic Models."" Communication of the ACM. Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. ""Latent Dirichlet Allocation."" Journal of Machine Learning Research 3,March: 993‚Äì1022. Blevins, Cameron. 2010. ""Topic Modeling Martha Ballard‚Äôs Diary."" Historying. http :// historying . org /2010/04/01/ topic - modeling - martha - ballards - diary /. Boyd-Graber, Jordan, and David M. Blei. 2009. ""Multilingual Topic Models for Unaligned Text."" In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, 75‚Äì82. UAI ‚Äô09. Arlington, Virginia, United States: AUAI Press. http :// dl . acm . org / citation . cfm ? id =1795114.1795124. Couderc, Christophe 2013: La tragédie Espagnole et son contexte Européen : XVIe - XVIIe si√®cles, Paris : Presses Sorbonne Nouvelle. Eder, M. 2014. Stylometry, network analysis and Latin literature. In: Digital Humanities 2014: Book of Abstracts, EPFL-UNIL, Lausanne, pp. 457-58. http :// dharchive . org / paper / DH 2014/ Poster 324. xml Jockers, Matthew L. 2013. Macroanalysis: Digital Methods and Literary History. Topics in the Digital Humanities. University of Illinois Press. McCallum, Andrew K. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu. Pouliquen, Bruno, Ralf Steinberger, and Camelia Ignat. 2006. ""Automatic Annotation of Multilingual Text Collections with a Conceptual Thesaurus."" arXiv:cs/0609059, September. http :// arxiv . org / abs / cs /0609059. Rhody, Lisa M. 2012. ""Topic Modeling and Figurative Language."" Journal of Digital Humanities 2,1. http :// journalofdigitalhumanities . org /2-1/ topic - modeling - and - figurative - language - by - lisa - m - rhody /. Rybicki, Jan. 2012. The great mystery of the (almost) invisible translator: stylometry in translation. In M. Oakley and M. Ji (eds.), Quantitative Methods in Corpus-Based Translation Studies. Amsterdam: John Benjamins, pp. 231-248. Rybicki, Jan, and Maciej Eder. 2011. Deeper Delta across genres and languages : do we really need the most frequent words ?Literary and Linguistic Computing 26(3), 315-21. Schmid, Helmut. 1994. ""Probabilistic Part-of-Speech Tagging Using Decision Trees."" In Proceedings of International Conference on New Methods in Language Processing. Manchester. Steyvers, Mark, and Tom Griffiths. 2006. ""Probabilistic Topic Models."" In Latent Semantic Analysis: A Road to Meaning, edited by T. Landauer, D. McNamara, S. Dennis, and W. Kintsch. Laurence Erlbaum.",de,Hintergrund Europa Jahrhundert entwickeln zeitgleich verschieden Form Theater trotz unterschiedlich sozial poetologisch Kontexte weisen spanisch französisch Theater stofflich Stilistische Verbindung gemeinsam europäisch Theatergeschichte hindeut Couderc Frage Methode Digital Humanitie nutzen stellen Romanistik Philologien Rybicki Jockers ed allgemein gelten sprachübergreifend quantitativ textanalyse Herausforderung bleiben steinberger Eder Rybicki romanistisch Tradition sprachgrenz Hinweg quantitativ Verfahren anwenden scheinen Topic Modelling Topics mehrere einheitlich Strukturierter Textsammlung unabhängig voneinander modellieren Grundlage strukturell Merkmale öhnlichkeiten Unterschiede ermitteln Fragestellung Anstelle einzelhypothesen konkret passagenvergleich spanisch französisch Textsammlung Topic Modeling vergleichen Art Topics liegen verhalten zueinand Relation bestehen Topics Kategorie untergattung Komödie Tragödie gestalten Vergleich spanisch französisch Theater Frage hinaus Eignung Methode Theaterstücke prüfen verhalten topics theaterwissenschaftlich relevant Thema Perspektivenverschiebung ergeben quantitativ Verfahren Topic Modeling Textsammlunge spanisch Textsammlung enthalten Theaterstücke Autor Stück erscheinen untergattungen Drama comedia Auto Sacramental Text stammen www Comedias org Wikisource biblioteca cervantes französisch Textsammlung enthalten Theaterstücke Autor Stück erscheinen stammen untergattungen Comédie Tragédie Pastoral Methode Topic Modeling Topic Modeling quantitativ Ansatz groß textsammlungen thematisch Muster entdecken blei Anwendung dh blevins Rhody jockers mathematisch sehen topics verteilungen auftretenswahrscheinlichkeiten wörtern Wörter Topic hoch Auftretenswahrscheinlichkeit semantisch anderweitig ähnlich blei steyver griffiths Verknüpfung Metadat thematisch Trend Zeitverlauf thematisch Differenz Textgattunge entdecken wichtig Parameter Präprozessier Text Lemmatisierung Auswahl berücksichtigend Wörter Wortart wortfrequenz stoplisen Textsegmentierung Anzahl Topics finden Studie Text Treetagger Schmidt lemmatisieren Wortarten annotiert verschieden Textfassung neriern substantiv verben enthalten Text Segmente Lemmata zerlegt Topic Modeling Mallet Mccallum durchführen Anzahl Topics festlegen Ergebnis Diskussion ermittelt Topics ermittelt Topic lassen meist Begriff zusammenfassen inhaltlich Gemeinsamkeit wichtig Wort Topic fasst allgemeiner spezifisch Topics unterschiedlich Gewicht Textsammlung französisch Topics zeigen abb abb Auswahl Topics französisch Textsammlung Topics betreffen allgemein gefasst erwartbar Thema lieben intrigen wichtig Topics gehören themenbereich Liebestopic enthalten Element schmerzes Hasses Tragödie hindeuten topic lässen inhaltlich typisch Komödientopic Selbstreferentielle begriffe erkennen topic faktisch distinktiv Komödie relativ unbestimmt Topic Topic vergnüg Topics spezifisch Topic Gefahr Topic Geheimnis können vermuten lassen bestimmt untergattungen Theater verknüpfen topics verbrechen zeigen Vergleich Topics Textklassen Topic Tragödie zuordnen Topic offenbar verbrechen Komödie abspielen Topics Vergleich vergleichen Topics französisch spanisch Textsammlung miteinander stellen übereinstimmung Unterschied fest abb abb Topics sprachvergleich textsammlungen präsent allgemein Topics Thema liebe Topic Topic absehen leicht Tendenz Richtung Lust Celo v spanisch Leid Souffrir französisch Topic semantisch Differenz sprechen dennoch Topic Topicverteilung gattung spanisch comedia zuordnen Topic französisch Tragödie zuordnen ähnlich Wichtigkeit beide Topics jeweilig Korpora belegen stoffgeschichtlich Verwandtschaft Theater beide Land spezifischeren topics zahlreich übereinstimmung Topic Topic Titel versehen können konkret Topics Krieg Topic Arzt Topic fast identisch wörtern vorkommen topics spanisch Sammlung Übereinstimmung französisch Sammlung topic umgekehrt Topic französisch Sammlung Übereinstimmung spanisch Sammlung Topic gehorsam verbrechen Ergebnis bieten ausgangspunkte abgleich Erkenntnis Literaturgeschichte topics textklassen unterschiedlich Ausprägung zeigen Textsammlung untergattungen jeweils mindestens charakteristisch Topics besitzen abb abb heatmap genre Spanisch Topics groß Varianz messen Standardabweichung wesentlich Kontrast spanisch Stück abb liegen Auto sacramental einerseits Comedias dramas andererseits letzterer schwach Kontrastive Topics Topic niedrig Niveau topic unklar französisch stücken Untergattung zumindest charakteristisch Topic abb Topic Komödie topic Tragödie Topic Pastorale Ausnahme tragikomödie abb heatmap genre französisch Topics groß Varianz messen Standardabweichung insgesamt scheinen gattungsbezogen Trennschärfe französisch Text deutlich spanisch Text Befund entsprechen unterschiedlich französisch spanisch Poetik Gruppierung Grundlage Topic Scores ausschließen verwendet Gattungsbezeichnunge tatsächlich vorhanden Differenzierung verdecken vorgängig kategorien Grundlage Öhnlichkeit stücken verteilung Topic Principal Component Analysis Struktur Textsammlunge finden räumlich Verteilung Stück zeigen spanisch Text Struktur bilden einheitlich Wolke abb französisch Text abb zeigen Struktur Kompakterer separiert Bereich rechts weit dicht Bereich links Komponent enthalten Varianz daten französisch spanisch verhältnismäßig gering abb Grundlage Topic Scores französisch Sammlung Verteilung Gattungssymbole zeigen französisch Text Gattunge gruppieren rechts Komödie links Tragödie stark verteilt Tragikomödien überlappen Tragödie spanisch Text ebenfalls Gruppe Auto sacramentales linker unterer quadranten Comedia eher Hälfte Dram breit streuen Mitte abb Grundlage Topic Scores spanisch Sammlung einfach korrelationstests bestätigen Gesamteindruck abb französisch Stück korrelieren Genre deutlich Autorschaft spanisch stücken Korrelation Autorschaft stark abb Korrelationtest Principal Components Autorschaft Gattungszugehörigkeit thematisch Differenzierung Stück französisch Textsammlung stark ausprägen korrelieren stark vorhanden spanisch Textsammlung Bilanz nächster Schritt zahlreich Einzelergebniss Verhältnis inhaltlich Bestimmung einzeln Topics eventuell Zuordnung untergattung Theater zeigen spanisch französisch Theater Grundlage Weise unterscheiden gattungspoetisch Position entsprechen vorhanden literaturwissenschaftlich erkenntnis anschlussfähig zeigen Ergebnis Unterschied topics Thema literaturwissenschaftlich Sinn semantisch Gehalt Topic Begriff topic bündeln beschreiben unbedingt zentral Thema verbergend Theaterstücke Diskussion Komödie Pastoralentopic vermeintlich Kluft Topic literaturwissenschaftlich Thema eher Chance Dilemma lassen vorschnell interpretationsansätze überprüfen erkenntnis gewinnen methodisch deutlich Topic Modeling Schritt interpretationskette linguistisch Annotation metadaten vorbereiten Ergebnis Verarbeitung Kontextualisierung bedeutungsvoll nächster Schritte Textsammlung erweitern Metadat ergänzen Vergleichbarkeit Textsammlung erhöhen Multilingual Topic Modeling Blei operieren unmittelbar thematisch bezug dokumenten unterschiedlich Sprache ermitteln alternativ algorithmisch Verfahren Öhnlichkeitsbestimmung verschiedensprachig Topics entwickeln Pouliqu Bibliographie Blei David introduction to probabilistic Topic Model communication -- -- acm Blei David Andrew ng And Michael Jordan latent dirichlet Allocation Journal -- Machine Learning research Blevins Cameron Topic Modeling Martha Ballard äôs diary historyehen Http historying org Topic Modeling Martha ballards diary Jordan And David Blei Multilingual Topic models for Unaligned Text proceedings -- -- conference -- uncertainty Artificial intelligence uai arlington Virginia United states auai press Http dl Acm org Citation Cfm id Couderc Christophe -- Tragédie Espagnole et son contexte européen xvie xviie Cles Paris Presses Sorbonne nouvellen eder stylometry Network Analysis and latin literature Digital Humanitie Book of Abstracts Lausanne pp Http Dharchive org Paper dh Poster xml Jockers Matthew Macroanalysis Digital Methods And literary History topics The Digital Humanitie university of Illinois press Mccallum Andrew mallet Machine Learning for Language Toolkit pouliquen Bruno Ralf Steinberger And Camelia Ignat Automatic annotation -- Multilingual Text Collection with conceptual Thesaurus arxiv cs September Http Arxiv org abs cs Rhody Lisa Topic Modeling and figurative Language Journal of digital Humanitie Http journalofdigitalhumanities org Topic Modeling and figurative Language by lisa m Rhody Rybicki Jan The great mystery -- -- Almost invisible translator Stylometry Translation Oakley and Ji eds quantitativ Methods Translation studies Amsterdam john benjamins pp Rybicki Jan And maciej eedner Deeper Delta Across genr and languages do we Really need The Most Frequent words literary and Linguistic Computing Schmid Helmut probabilistic Tagging Using Decision trees proceedings -- international conference -- New Methods Language Processing Manchester Steyver Mark and Tom Griffiths probabilistic Topic Model latent Semantic Analysis Road to meaning Edited by Landauer Mcnamara Dennis And Kintsch Laurence erlbaum,"[('topic', 0.5310386877999285), ('topics', 0.4558565585234605), ('spanisch', 0.2705638340842408), ('französisch', 0.24755958896851504), ('textsammlung', 0.17367123030963005), ('modeling', 0.12377979448425752), ('abb', 0.12367025296003756), ('and', 0.11582396666285269), ('org', 0.09841323100809017), ('theater', 0.09669684574740071)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Die explorative Visualisierung von Texten. Von den Herausforderungen der Darstellung geisteswissenschaftlicher Primär- und Annotationsdaten,Evelyn Gius (Universität Hamburg); Marco Petris (Universität Hamburg),"Annotation, Analyse, Visualisierung","Annotation, Analyse, Visualisierung","1. Zur Komplexität von Textdaten Die Visualisierung von Textdaten ist innerhalb des Bereichs der Datenvisualisierung eine besondere Herausforderung, da es sich bei ihnen um unstrukturierte Daten handelt: Bevor man Textdaten visualisieren kann, muss aus ihnen eine Struktur abgeleitet werden. Hinzu kommt, dass Textdaten eine Vielzahl an Betrachtungsmöglichkeiten eröffnen, die durch die zahlreichen Bedeutungsdimensionen von Texten bedingt werden. Die einzelnen Dimensionen von Texten können durch Annotationen herausgearbeitet werden, wobei jede Annotationsschicht eine oder mehrere Dimensionen des Textes offenlegen kann. In diesem Sinne sind Textdaten also multidimensional. Insbesondere im Bereich der geisteswissenschaftlichen Textanalyse ist aufgrund des hermeneutischen Zugangs zu Texten auch in spezifischen Analysen nicht von vornherein klar, auf welche Weise Analyse und Interpretation zusammenhängen. Entsprechend muss eine sinnvolle Visualisierung von Textdaten im geisteswissenschaftlichen Kontext als exploratives Werkzeug zum Herausarbeiten möglicher Zusammenhänge fungieren können. 1 Ein Blick in die einschlägige Literatur zur Datenvisualisierung zeigt, dass der Besonderheit von Textdaten häufig nicht Rechnung getragen wird. Zumindest scheint im traditionell mit Datenvisualisierung befassten informationswissenschaftlichen Bereich die Komplexität von annotierten Textdaten nicht immer im vollen Umfang wahrgenommen zu werden. So verweisen etwa Ward et al. (2010) in ihrer umfassenden Einführung zu Datenvisualisierung im Kapitel zu Textdaten auf drei mit Texten zusammenhängende Sucharten, die für die Anforderungen an die Visualisierung von Texten ausschlaggebend sind. 2 Die auf die Sucharten folgende Darstellung von Möglichkeiten der Textvisualisierung beschränkt sich allerdings auf die Darstellung von Texten und Korpora mit Metadaten (Erscheinungsjahr, Publikation o.ä.). Das Problem wird in der Zusammenfassung des Textvisualisierungskapitels offensichtlich: Die diskutierten Ansätze beträfen das ""transforming unstructured text into structured data suitable for visualization and analysis"" (Ward et al. 2010: 311). Die Option, dass der Text bereits mit Analysedaten in Form von 1 Vorgehen, die darauf basieren, die Komplexität der Daten automatisiert zu reduzieren, erscheinen uns deshalb auch nicht geeignet für das beschriebene Problem (vgl. zu solchen Ansätzen z.B. Yang et al. 2003; Tatu et al. 2011). 2 Typischerweise würden Zeichenketten in Form von Wörtern, Phrasen oder Themen gesucht, im Falle von partiell strukturierten Daten könnte außerdem nach Beziehungen zwischen Wörtern, Phrasen, Themen oder Dokumenten gesucht werden und schließlich ginge es in strukturierten Texten oder Textkorpora meistens um das Identifizieren von Mustern oder Auffälligkeiten innerhalb von Texten bzw. Dokumenten (vgl. Ward et al. 2010:291). Annotierte Textdaten fallen also potentiell unter die letzten beiden Fälle. 1 Annotationen angereichert sein könnte, wird nicht in Betracht gezogen. Das, obwohl in der Einleitung auf die drei Ebenen von Texten verwiesen 'die lexikalische, die syntaktische und die semantische 'und im Fall der syntaktischen Ebenen sogar explizit die Möglichkeit von Annotationen im Rahmen von named entity recognition (NER)_Prozessen erwähnt wurde (Ward et al. 2010:294). 2. Geisteswissenschaftliche Textdaten In der stark geisteswissenschaftlich orientierten Position von Drucker (2014) werden hingegen die vielfältigen Interpretationsmöglichkeiten in den Fokus gerückt. Sie schreibt über die Visualisierung geisteswissenschaftlicher Interpretation: ""The challenge is enormous, but essential, if the humanistic worldview, grounded in the recognition of the interpretive nature of knowledge, is to be part of the graphical expressions that come into play in the digital environment"" (Drucker 2014: 136). Drucker geht es v.a. darum, die mit geisteswissenschaftlichen Analysen einhergehende Unsicherheit in der Darstellung des Wissens zu verdeutlichen, wobei sie sich nicht nur auf Texte beschränkt. Was bedeutet das im Falle von Texten? Betrachten wir die Problematik an mit CATMA 3 annotierten Texten, die durch die flexiblen Annotationsmöglichkeiten des Werkzeugs exemplarisch für die große Bandbreite und gleichzeitig eingeschränkte Vorhersagbarkeit 4 geisteswissenschaftlicher Analysen sind. Für die Visualisierung von in CATMA erzeugten Text_ und Annotationsdaten ist die von Drucker angesprochene Unsicherheit geringer, da es um die Analyse von Texten geht: Sie beschränkt sich auf (Text_)Interpretationen und liegt zudem nur in Form von Annotationen vor, die diese Unsicherheit konzeptionell durch entsprechende Tags fassen. Die Tags selbst beinhalten aber keine Unsicherheit, die für 5 die weitere Analyse berücksichtigt werden muss. Trotzdem ist Druckers Beobachtung zur Besonderheit geisteswissenschaftlicher Aussagen auch für unseren Zweck gültig und muss für die Visualisierung der Text_ und Annotationsdaten berücksichtigt werden: ""[‚Ä¶] we need to conceive of every metric ""as a factor of X"" , where X is a point of view, agenda, assumption, presumption, or simply a convention. By qualifying any metric as a factor of some condition, the character of the ""information"" shifts from self_evident ""fact"" to constructed interpretation motivated by a human agenda. "" (Drucker 2014:131). Aufgrund des freien Annotationsschemas, das CATMA zur Verfügung stellt, ist die Art der ""Information"" , die die Annotationen enthalten, nämlich nicht über die vorliegenden Daten zugänglich: Man kann in CATMA genauso gut strukturelle Textmerkmale wie inhaltliche Aspekte annotieren und dafür eine eigene Annotationshierarchie entwickeln, deren Struktur zwar von der Anlage her hierarchisch ist, die aber prinzipiell überlappendes und widersprüchliches Markup zulässt. 3 CATMA = Computer Aided Text Markup and Analysis, vgl. www.catma.de (gesehen am 10.11.2014). 4 In CATMA können Texte anhand von frei gewählten Tags annotiert werden, die zu so genannten Tagsets zusammengefasst werden. Die so entstehende Taxonomie oder Systematik kann wiederverwendet werden. Die Texte und die Annotationen können außerdem mit einer umfangreichen Suchfunktionalität durchsucht und analysiert 'und ggf. weiter annotiert werden. Zum damit außerdem verbundenen Konzept des hermeneutischen Markups vgl. Bögel et al. (im Erscheinen). 5 vgl. dazu Jacke & Meister (2014). 2 3. Anforderungen an Visualisierung als Exploration Aufgrund der nicht a priori eingrenzbaren Zwecke der Annotation und der Analyse muss die Visualisierung von Textdaten so generisch wie möglich gestalten werden. Nur so kann sie ohne ein tieferes Verständnis über die jeweils vorliegenden Text_ und Annotationsdaten eingesetzt werden und einen Mehrwert bei der Analyse der Daten 6 erzeugen. Grundsätzlich konzipieren wir Visualisierungen deshalb ausgehend von der Frage, wie viele und welche Dimensionen der Daten dargestellt werden sollen. 7 Für die Auswahl der Dimensionen stellt CATMA über die Struktur der Ergebnismenge der Abfragen folgende Kategorien zur Verfügung: _ Metadaten der Dokumente (z.B. Titel, Autor, etc.), _ Tag bzw. Typ der Annotation, _ Properties der Annotation und die für den annotierten Text vergebenen Werte, _ annotierter Text, _ Position im Text (via Zeichen_Offset), _ Textkontext des annotierten Textes (variable Anzahl von Token), _ Vorkommenshäufigkeit des annotierten Textes, _ Vorkommenshäufigkeit der Annotation, _ weitere berechnete Kategorien, wie der z_Faktor oder der TF_IDF Neben dem generischen Zugang über die Dimensionen der Daten muss auch ein Mechanismus zur Verfügung gestellt werden, mit dem der Zweck einer spezifischen Analyse in der Visualisierung der Daten herausgearbeitet werden kann 'und der die erzeugten Visualisierungen als explorative Heuristik nutzbar macht. Für die damit zusammenhängenden spezifischen Erkenntnisinteressen werden deshalb zusätzliche Anpassungsmöglichkeiten in Form von wählbaren Parametern eingeführt. Diese sollen typische Varianten abfangen, wie etwa die Frage, ob die Häufigkeit einer Annotation oder aber der annotierte Textumfang dargestellt werden soll, wie mit überlappenden Annotationen verfahren werden soll (soll etwa eine zweifach annotierte Stelle zweimal oder nur einmal gezählt bzw. dargestellt werden?) oder ob die Struktur der Tagsets so ist, dass sich Tags auf derselben Hierarchieebene gegenseitig ausschließen oder ob sie sich ergänzen können. 8 4. Beispiele Die oben angestellten √úberlegungen sollen an folgenden Beispielen demonstriert werden. Datengrundlage ist das in Gius (2013) beschriebene und analysierte umfangreich 6 Dies gilt nicht für die Visualisierung zu Demonstrations_ bzw. Kommunikationszwecken 'also von Daten, die bereits analysiert und interpretiert wurden. 7 Mit ""Dimensionen"" sind also nicht räumliche Dimensionen gemeint. Dies wird allerdings von einigen gängigen Ansätzen zur Visualisierung mehrdimensionaler Daten angenommen, die Textdaten nur als einen 'eindimensionalen 'Datentyp betrachten und allgemeine Modelle entwickeln (vgl. etwa Shneiderman 1996). 8 Auch hier unterscheidet sich der vorgestellte Ansatz durch seinen Fokus auf die Spezifik von Texten wieder deutlich von Ward et al. (2010) oder Shneiderman (1996), die so genannte tasks als Basis für zusätzliche explorative Funktionalitäten betrachten. 3 annotierte Korpus. 9 Die hier nur kurz beschriebenen Visualisierungen werden ebenso wie eine Reihe weiterer Visualisierungen in CATMA zur Verfügung gestellt. Ihre Funktionen und der damit verbundene explorative Gewinn werden im Rahmen des Vortrags näher vorgestellt werden. Interaktive TreeMap Abbildung 1: Interaktive TreeMap 10 9 Das Korpus besteht aus 24 Texten mit insgesamt 86.246 Wörter, die auf etwa 150 als Tags eingeführte narratologische Konzepte untersucht und mit insgesamt 24.347 Annotationen versehen wurden. 10 Erstellt auf Basis von Google Charts: https://developers.google.com/chart/interactive/docs/gallery/treemap?hl=de (gesehen am 10.11.2014). 4 Das erste Beispiel ist eine interaktive TreeMap. Jede einzelne Sicht zeigt zwei Dimensionen: (1) Die Vorkommenshäufigkeit der Abfrageergebnisse (Tags, Wörter, o.ä.) als Größe des zugehörigen Rechtecks und (2) die durchschnittliche annotierte Textmenge als Farbintensität auf einer Skala von rot (weniger) bis grün (mehr). Die interaktive Komponente ermöglicht das Ergründen einer dritten Dimension: Durch Klicken der einzelnen Rechtecke kann man durch (3) die Hierarchie des Tagsets navigieren. Abbildung 1 zeigt zwei Ebenen: Rechts die höchste Ebene mit den beiden Top_Level Tags ""narratological _ tagset"" und ""Konfliktanalyse"" und links die Ebene 1 den Zweig entlang dem Tag ""Konfliktanalyse"" mit den Tags der darunter liegenden Ebene. Gezeigt wird also die Verteilung von Vorkommenshäufigkeit und annotierter Textmenge für die Hierarchieebene. Für die dargestellte Datenbasis ist das insofern interessant, als hier die Konflikthaftigkeit von Erzählungen bzw. die als konflikthaft oder konfliktlos annotierten Passagen dargestellt werden. Für die Analyse des Korpus ist sowohl die Frage nach der Häufigkeit, in der konflikthafte Passagen auftauchen (sie unterbrechen nämlich von den Erzählerinnen eigentlich als konfliktlos deklarierte Erzählabschnitte und deshalb ist ihre Anzahl relevant), als auch die reine Textmenge, die sie umfassen (wird ausgiebiger über konfliktlose oder über konflikthafte Situationen erzählt?), interessant. Die Visualisierung als dreidimensionale TreeMap ermöglicht es, die beiden Betrachtungsweisen 'Anzahl vs. Textmenge 'überblickshaft in Beziehung zu setzen und dabei durch die hierarchisch angeordneten Tags zu navigieren, also zusammengefasste und detailliertere Perspektiven zu wählen. Small Multiples Das zweite Beispiel (vgl. Abbildung 2) zeigt die Vorkommenshäufigkeit von zwei Annotationen (Wiedergabe von mentalen Prozessen und Wiedergabe von Rede) im Textverlauf bei neun Texten des Korpus. Die Vorkommenshäufigkeit wird auf der y_Achse und der Textverlauf in 10%_Schritten auf der x_Achse dargestellt. Für jeden ausgewählten Text wird jeweils ein Koordinatensystem als dritte Dimension erstellt, in dem die Annotationen als farbige Linien abgebildet werden. 11 Diese Darstellung ermöglicht eine explorative Betrachtung der Verteilung der beiden annotierten Phänomene in den Einzeltexten und einen ersten √úberblick über mögliche Muster im gesamten Korpus. Für eine weitere Analyse können auffällige Stellen 'wie etwa besondere Häufigkeiten in einem Textabschnitt oder der Wechsel von dominierender Redewiedergabe zu dominierender Wiedergabe von mentalen Prozessen 'genauer betrachtet werden: Das Anklicken der entsprechenden Punkte im Graphen erzeugt eine KWIC(=KeyWord In Context)_Anzeige der Annotationen im betreffenden Textabschnitt, von denen aus wiederum durch Klicken in den Volltext gesprungen werden kann. 11 Die Darstellung als Linie wurde aus Gründen der √úbersichtlichkeit gewählt, mathematisch gesehen handelt es sich natürlich um diskrete Werte. 5 Abbildung 2: Small Multiples: Distributionsgraphen 12 12 Erstellt auf Basis von Highcharts: http://www.highcharts.com/ (gesehen am 10.11.2014). 6 5. Ausblick Für das dargestellte Korpus sind oben beschriebene Visualisierungen von großem Gewinn. Sie ermöglichen einen √úberblick über die Daten, für die nicht bereits bei der Annotation festgelegt wurde, welche Dimensionen genauer betrachtet und in Zusammenhang gebracht werden müssen. Dadurch sind die für die Analyse der Daten von großem Nutzen. Inwiefern sich nach in diesem Beitrag vorgestellten √úberlegungen entwickelte Visualisierungen auch systematisch als heuristisches Werkzeug eignen und ob sie sich als Alternative gegen generelle Datenvisualisierungen durchsetzen können, ist zum momentanen Zeitpunkt allerdings noch nicht abschätzbar. Neben der Vieldimensionalität der Daten ist dafür insbesondere die Frage der explorativen Funktion der Visualisierungen zentral: Reichen (1) die dargelegte Aufschlüsselung der Analysen nach ihrer Dimensionalität und (2) die Explorationsmöglichkeit durch einstellbare Parameter aus, um Visualisierungen zu erzeugen, die systematische Rückschlüsse auf die dargestellten Daten und Strukturen zulassen 'und nicht nur assoziative Denkanstöße zu liefern? Dies wird in breit angelegten Nutzerstudien zu ergründen sein, ebenso wie untersucht werden muss, ob und auf welche Weise die in den Visualisierungen zum Einsatz kommenden visuellen Metaphern den Verstehensprozess beeinflussen. Referenzen Bögel, Thomas, Michael Gertz, Evelyn Gius, Janina Jacke, Jan Christoph Meister, Marco Petris, and Jannik Strötgen. ""Collaborative Text Annotation Meets Machine Learning: heureCL√âA, a Digital Heuristics of Narrative. "" DHCommons Journal, im Erscheinen. Drucker, Johanna. Graphesis: Visual Forms of Knowledge Production. MetaLABprojects. Cambridge, Massachusetts: Harvard University Press, 2014. Gius, Evelyn. ""Erzählen √úber Konflikte. Eine Computergestützte Narratologische Untersuchung von Narrativen Interviews Zu Arbeitskonflikten. "" Dissertation, Universität Hamburg, 2013. Jacke, Janina, und Jan Christoph Meister. ‚ÄûPushing Back the Boundary of Interpretation: Concept, Practice and Relevance of a Digital Heuristic"" . In Digital Humanities 2014 'Book of Abstracts, 264‚Äì66. Lausanne, 2014. Shneiderman, Ben. ""The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations. "" In In IEEE Symposium on Visual Languages, 336‚Äì43, 1996. Tatu, Andrada, Georgia Albuquerque, Martin Eisemann, Peter Bak, Holger Theisel, Marcus Magnor, and Daniel Keim. ""Automated Analytical Methods to Support Visual Exploration of High_Dimensional Data. "" IEEE Transactions on Visualization and Computer Graphics 17, no. 5 (May 2011): 584‚Äì97. Ward, Matthew, Georges G. Grinstein, and Daniel Keim. Interactive Data Visualization: Foundations, Techniques, and Applications. Natick, Mass: A K Peters, 2010. Yang, J., M. O. Ward, E. A. Rundensteiner, und S. Huang. ‚ÄûVisual Hierarchical Dimension Reduction for Exploration of High Dimensional Datasets"" . In Proceedings of the 7 Symposium on Data Visualisation 2003, 19‚Äì28. VISSYM ""03. Aire_la_Ville, Switzerland, Switzerland: Eurographics Association, 2003.",de,Komplexität Textdat Visualisierung Textdat innerhalb Bereich datenvisualisierung besonderer Herausforderung unstrukturiert daten handeln bevor textdaten visualisieren Struktur ableiten hinzu textdan Vielzahl betrachtungsmöglichkeite eröffnen zahlreich Bedeutungsdimension Text bedingen einzeln Dimension Text annotation herausarbeiten wobei Annotationsschicht mehrere dimension Text offenlegen Sinn textdan multidimensional insbesondere Bereich geisteswissenschaftlich Textanalyse aufgrund hermeneutisch Zugang texten spezifisch Analyse vornherein klar Weise Analyse Interpretation zusammenhängen entsprechend sinnvoll Visualisierung Textdat geisteswissenschaftlich Kontext explorativ Werkzeug herausarbeit möglich Zusammenhäng fungieren Blick einschlägig Literatur Datenvisualisierung zeigen Besonderheit Textdat häufig Rechnung tragen zumindest scheinen Traditionell Datenvisualisierung befasst informationswissenschaftlich Bereich Komplexität Annotiert textdan voll Umfang wahrnehmen verweisen werden et umfassend Einführung datenvisualisierung Kapitel Textdat Text zusammenhängend Suchart Anforderung Visualisierung Text ausschlaggebend Suchart folgend Darstellung Möglichkeit Textvisualisierung beschränken Darstellung Text Korpora Metadat erscheinungsjahr Publikation Problem Zusammenfassung Textvisualisierungskapitel offensichtlich diskutiert Ansatz betreffen Transforming unstructured Text Into Structured Data suitabel for visualization and Analysis werden et Option Text Analysedat Form vorgehen basieren Komplexität daten automatisieren reduzieren erscheinen geeignet beschrieben Problem Ansätz Yang et tatu et Typischerweise Zeichenkett Form wörtern Phrase Thema suchen Fall partiell strukturiert daten Beziehung wörtern phrasen Them dokument suchen schließlich gehen strukturiert Text Textkorpora meistens Identifizieren Muster Auffälligkeit innerhalb Text dokumenten werden et annotiert textdaten fallen potentiell letzter Fall Annotation anreichern betracht ziehen obwohl Einleitung Ebene Text verweisen lexikalisch syntaktisch semantisch Fall syntaktisch Ebene sogar Explizit Möglichkeit annotatio Rahmen Named entity Recognition erwähnen Ward et geisteswissenschaftlich textdan stark geisteswissenschaftlich orientiert Position Drucker hingegen vielfältig Interpretationsmöglichkeit Fokus rücken schreiben Visualisierung geisteswissenschaftlich Interpretation The challenge -- Enormou But essential if The Humanistic Worldview grounded The Recognition of -- interpretive nature -- knowledge -- to be Part -- -- Graphical Expression That Come Into play The Digital Environment drucker Drucker geisteswissenschaftlich Analyse einhergehend Unsicherheit Darstellung Wissen verdeutlichen wobei Text beschränken bedeuten Fall Text betrachten Problematik catma Annotiert Text flexibl Annotationsmöglichkeit werkzeugs exemplarisch bandbreit gleichzeitig eingeschränkt Vorhersagbarkeit geisteswissenschaftlicher Analyse Visualisierung Catma erzeugt Text Annotationsdat Drucker angesprochen Unsicherheit gering Analyse Text beschränken liegen zudem Form annotatio Unsicherheit konzeptionell entsprechend tags fassen tags beinhalen Unsicherheit Analyse berücksichtigen Druckers Beobachtung Besonderheit geisteswissenschaftlich Aussage unser Zweck gültig Visualisierung Text Annotationsdat berücksichtigen we need to conceiv -- Every Metric as factor -- x wh x -- point of view Agenda Assumption Presumption or simply Convention by Qualifying any metric as Factor -- Some Condition The character -- -- Information shifts from fact to Constructed interpretation Motivated by human Agenda Drucker aufgrund frei Annotationsschemas Catma Verfügung stellen Art Information annotatio enthalten nämlich vorliegend daten zugänglich Catma genauso strukturell Textmerkmal inhaltlich Aspekt annotieren Annotationshierarchie entwickeln Struktur Anlage hierarchisch prinzipiell überlappend widersprüchlich markup zulässt Catma Computer Aided Text Markup And Analysis sehen Catma Text anhand frei gewählt tags annotiert genannt Tagset zusammengefassen entstehend Taxonomie Systematik wiederverwendet Text annotation umfangreich Suchfunktionalität Durchsucht analysieren annotiert verbunden Konzept hermeneutisch Markup Bögel et erscheinen jack Meister anforderungen Visualisierung Exploration aufgrund Priori eingrenzbar Zweck Annotation Analyse Visualisierung Textdat generisch gestalten tief Verständnis jeweils vorliegend Text Annotationsdat einsetzen mehrweren Analyse daten erzeugen grundsätzlich konzipieren visualisierungen ausgehend Frage Dimension daten darstellen Auswahl dimension stellen Catma Struktur Ergebnismenge abfrag folgend kategorien Verfügung Metadat dokument Titel Autor typ Annotation properties Annotation annotierter Text vergeben wert annotiert Text Position Text via Textkontext annotierter Text variabel Anzahl token Vorkommenshäufigkeit annotierter Text Vorkommenshäufigkeit Annotation berechnet Kategorie generisch Zugang Dimension daten Mechanismus Verfügung stellen Zweck spezifisch Analyse Visualisierung daten herausgearbeiteen erzeugt visualisierung explorativ Heuristik nutzbar zusammenhängend spezifisch Erkenntnisinteresse zusätzlich Anpassungsmöglichkeit Form Wählbare Parameter einführen typisch Variant abfangen Frage Häufigkeit Annotation annotiert Textumfang darstellen überlappend annotation Verfahren zweifach annotiert Stelle zweimal zählen darstellen Struktur Tagset tags hierarchieeben gegenseitig ausschließen ergänzen Beispiel angestellt folgend Beispiel demonstrieren datengrundlage Gius beschrieben analysieren umfangreich gelten Visualisierung demonstrationr Kommunikationszweck daten analysieren interpretieren dimension räumlich Dimension meinen gängig ansätzen Visualisierung mehrdimensional daten annehmen textdan eindimensional datentyp betrachten allgemein Modell entwickeln shneiderman unterscheiden vorgestellt Ansatz Fokus Spezifik Text deutlich Ward et Shneiderman genannt Tasks Basis zusätzlich explorativ funktionalitäten betrachten annotiert korpus beschrieben visualisierungen Reihe weit visualisierung Catma Verfügung stellen Funktion verbunden explorativ Gewinn Rahmen Vortrag nah vorstellen interaktiv Treemap Abbildung interaktiv Treemap korpus bestehen Text insgesamt Wörter tags eingeführt narratologisch Konzept untersuchen insgesamt annotation versehen erstellen Basis Google Charts sehen interaktiv treemap einzeln Sicht zeigen dimensionen Vorkommenshäufigkeit abfrageergebnisse tags Wörter Größe zugehörig Rechteck durchschnittlich annotiert Textmenge Farbintensität Skala rot grün interaktiv Komponente ermöglichen ergründer Dimension Klicken einzelner Rechtecke Hierarchie Tagset navigieren Abbildung zeigen Ebene rechts hoch Ebene tags narratological Tagset Konfliktanalyse links Ebene Zweig entlang Konfliktanalyse tags liegend Ebene zeigen Verteilung Vorkommenshäufigkeit annotiert Textmenge hierarchieebene dargestellt datenbasis insofern interessant konflikthaftigkeit Erzählung Konflikthaft konfliktlos annotieret Passag darstellen Analyse Korpus sowohl Frage Häufigkeit konflikthafter Passage auftauchen unterbrechen nämlich Erzählerinn eigentlich konfliktlos deklariert Erzählabschnitte Anzahl relevant rein Textmenge umfassen ausgiebiger konfliktlos konflikthaft Situation erzählen interessant Visualisierung Dreidimensionale Treemap ermöglichen betrachtungsweis Anzahl Textmeng Überblickshaft Beziehung setzen hierarchisch angeordnet tags navigieren zusammengefas detailliertere Perspektive wählen small multiples Abbildung zeigen Vorkommenshäufigkeit Annotation Wiedergabe mental Prozessen Wiedergabe Rede Textverlauf Text korpus Vorkommenshäufigkeit Textverlauf darstellen ausgewählt Text jeweils Koordinatensystem Dimension erstellen annotatio farbig Linie abbilden Darstellung ermöglichen explorativ Betrachtung Verteilung Annotiert Phänomen einzeltexten möglich Muster gesamt korpus Analyse auffällig stellen besonderer häufigkeiten Textabschnitt Wechsel dominierend Redewiedergabe dominierend Wiedergabe mental prozeß genau betrachten anklicken entsprechend Punkt Graphen erzeugen annotatio betreffend textabschneten wiederum Klicken Volltext sprungen Darstellung Linie Grund wählen mathematisch sehen handeln diskret Wert Abbildung small multiples distributionsgraph erstellen Basis Highcharts sehen Ausblick dargestellt Korpus beschrieben visualisierungen groß Gewinn ermöglichen daten Annotation festlegen Dimension genau betrachten Zusammenhang bringen Analyse daten groß nutzen inwiefern Beitrag vorgestellt entwickelt Visualisierung systematisch heuristisch Werkzeug eignen Alternative generell Datenvisualisierung durchsetzen momentan Zeitpunkt abschätzbar Vieldimensionalität daten insbesondere Frage explorativ Funktion visualisierung Zentral reich dargelegt Aufschlüsselung Analyse Dimensionalität Explorationsmöglichkeit Einstellbare parameter visualisierung erzeugen systematisch Rückschlüsse dargestellt daten Struktur zulassen assoziativ Denkanstöße liefern breit angelegt nutzerstudien ergründen untersuchen Weise Visualisierung Einsatz kommend visuell Metapher verstehensprozess beeinflussen referenz Bögel Thomas Michael Gertz Evelyn Gius Janina Jacke Jan Christoph Meister Marco Petris And Jannik strötgen collaborative Text Annotation Meets Machine Learning Digital heuristics of narrativen Dhcommons Journal erscheinen Drucker Johanna Graphesis visual forms -- knowledge Production Metalabprojects Cambridge Massachusetts Harvard University press Gius Evelyn erzählen konfliken computergestützt narratologisch Untersuchung narrativ Interview arbeitskonflikten dissertation Universität Hamburg Jacke Janina Jan Christoph Meister äûpushangen back The boundary -- Interpretation Concept Practice and Relevance of Digital heuristic Digital Humanitie Book of abstracts Lausanne shneiderman ben The Eyes Have -- Task By Data Type Taxonomy for information visualizations Ieee symposium -- visual languag Tatu Andrada Georgia albuquerque Martin eisemann Peter Bak Holger Theisel Marcus Magnor And Daniel Keim Automated Analytical Methods to Support Visual Exploration -- data ieee transactions on visualization and Computer Graphics no may werden Matthew Georges Grinstein and Daniel Keim Interactive Data Visualization Foundation Techniques And Applications Natick missen k Peters Yang Ward Rundensteiner Huang äûvisual hierarchical Dimension Reduction for exploration -- high dimensional datasets proceedings -- -- Symposium -- Data Visualisation vissym switzerlehen switzerlehen Eurographic Association,"[('visualisierung', 0.22530311853708837), ('text', 0.1836512239327274), ('tags', 0.18245349025973218), ('dimension', 0.1623706347193781), ('drucker', 0.15711492381544576), ('vorkommenshäufigkeit', 0.15711492381544576), ('annotiert', 0.14545923647283845), ('catma', 0.14489127391398052), ('textdat', 0.1295332136881782), ('daten', 0.12597795816926827)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Sprachwissenschaftliche Untersuchungen zum Klagspiegel Conrad Heydens (1436) und zum Laienspiegel Ulrich Tenglers (1511),"Barbara Aehnlich (Universität Jena, Deutschland); Elisabeth Witzenhause (Universität Jena, Deutschland)","Normalisierung, automatische Annotation, Transkription, WebAnno, GATE, XML, Digitalisierung","Normalisierung, automatische Annotation, Transkription, WebAnno, GATE, XML, Digitalisierung","Das Forschungsvorhaben ist interdisziplinär angelegt und beruht auf einem Korpus von verschiedenen Textzeugen zweier frühneuhochdeutscher Rechtsbücher des 15. und 16. Jahrhunderts, Klagspiegel und Laienspiegel. Der Klagspiegel ist das mit Abstand älteste populärwissenschaftliche Rechtsbuch der Rezeptionszeit und bildet mit dem Laienspiegel zusammen die wichtigste Grundlage an rechtswissenschaftlichen populären Texten des 15. und 16. Jahrhunderts. Ziel ist die Untersuchung der sprachlichen Besonderheiten der Texte und ihrer Auswirkungen auf die Rezeptionsgeschichte des römischen Rechts in Deutschland. Neben der korpusbasierten linguistischen Analyse der Bücher, die eine völlig neue Textsorte begründen, bietet das Projekt auch aus der Perspektive der historischen Rechtssprachenforschung einen innovativen Ansatz. Das Erkenntnisinteresse liegt hierbei auf der Geschichte von Kulturtransferprozessen innerhalb der Jurisprudenz. Durch semantische und linguistische Annotationen wird eine umfassende Forschungsgrundlage geschaffen, die für die Schließung rechts- und sprachhistorischer Forschungslücken einen zentralen Beitrag leistet. Ein weiterer Schritt soll die Digitalisierung mehrerer Ausgaben des Klagspiegels sein, um Prozesse des Schreibsprachwandels im 15. und frühen 16. Jahrhundert nachzuvollziehen. Bisher gibt es kein Korpus frühneuhochdeutscher Rechtstexte. In einem ersten Schritt zur Vorbereitung des Projektes wurden verschiedene Annotationstools getestet und geeignete Formate für die Speicherung evaluiert. Aktuell werden mit der Jenaer Computerlinguistik Möglichkeiten der Normalisierung und automatischen Annotation erprobt. Ziel ist die Beantragung eines größeren Forschungsprojektes, das bestehende Werkzeuge nutzt und die Technologie auf die Besonderheiten des Rechtskorpus anpasst. Das Poster soll die bisherigen methodologischen Überlegungen und Probleme darstellen und bietet somit gleichzeitig einen Überblick und eine Evaluation der aktuell zur Verfügung stehenden Open Source Software zu Annotationszwecken. Die Untersuchungen beziehen sich zum einen auf die sprachliche Herkunft des Klagspiegels und des Laienspiegels. Es soll festgestellt werden, welche Textsorte mit welchen spezifischen 1 sprachlichen Eigenheiten vorliegt. Zudem muss geklärt werden, ob diese Rechtsbücher aufgrund ihrer Herkunft nur im südwestdeutschen Raum oder aber im gesamten hochdeutschen Sprachgebiet verständlich waren. Dabei wird nach möglichen Ausgleichstendenzen gesucht, die vom Oberdeutschen abweichen. Auf der Ebene der Syntax ist zu fragen, welche Strukturen die sprachliche Einfachheit und leichte Verständlichkeit ausmachen, die den Texten in der gesamten (bisher ausschließlich juristischen) Forschung zugeschrieben wird. Im Bereich des Wortschatzes sind besonders die Bezeichnungen juristischer Fachbegriffe oder Tatbestände für die Forschung interessant, denn für diese gab es zuvor im Deutschen keine entsprechenden Termini. Zum anderen soll untersucht werden, inwieweit Klagspiegel und Laienspiegel frühneuhochdeutschen Sprachstandard aufweisen und ob die beiden Bücher durch ihre Verbreitung eine wesentliche Rolle für die Entwicklung des neuhochdeutschen Sprachstandards im Rahmen rechtswissenschaftlicher Prozesse gespielt haben können. Ein Vergleich mehrerer Textzeugen der Rechtsbücher liefert Erkenntnisse des frühneuhochdeutschen Schreibsprachwandels. Der Einfluss der beiden Texte auf die deutsche Standardsprache sowie auf die deutsche Rechtssprache wurde bisher noch nicht analysiert; das Vorhaben soll hierfür eine nutzbare Ausgangsbasis liefern. Eine zentrale Frage ist dabei auch, inwieweit römisches Recht und deutsches Recht sprachlich unterschiedlich vermittelt wurden und ob textintern Varianz zwischen den einzelnen Passagen, die zum Teil auch literarisiert sind, festzustellen ist. Zwei Textzeugen, jeweils eine Ausgabe des Laien- und des Klagpiegels, liegen bereits in digitalisierten Abbildungen vor und wurden transkribiert. Im nächsten Schritt werden sie in ein XML-Format übertragen und sollen semantisch sowie linguistisch annotiert werden, um eine valide Datenbasis für die Untersuchung zu schaffen und das Korpus in einem standardisierten Format in einer Infrastruktur der Digital Humanities zur Verfügung stellen zu können. Im Sinne eines vielseitig nutzbaren Korpus soll die Transkription diplomatisch, mit allen Sonderzeichen und typografischen Besonderheiten, abgebildet werden. Problematisch ist die heterogene Gestalt der Texte, die Mehrfachannotationen notwendig macht. Alle Annotationen werden deshalb in einem XML-Stand-off Format vorgenommen, um eine leichte Übertragung in andere Formate und einen annotationsfreien Primärtext zu ermöglichen. Das TCF-Format bietet hierfür eine gute Möglichkeit und ist mit vielen anderen Formaten kompatibel. 1 Werkzeuge wie WebAnno2 oder GATE3 bieten geeignete Arbeitsoberflächen, deren Vor- und Nachteile es zu diskutieren gilt. 1 http://weblicht.sfs.uni-tuebingen.de/weblichtwiki/index.php/The_TCF_Format.(06.10.2014, 13.30 Uhr). 2 https://code.google.com/p/webanno/. (06.10.2014, 13.46 Uhr). 2 Die Präsentation stellt somit zum einen den Mehrwert der bisher geleisteten Forschungsarbeit im Rahmen der Digital Humanities für sprachwissenschaftliche Untersuchungen historischer Texte heraus, zum anderen werden Grenzen in der Annotation heterogener und nicht standardisierter Sprachdaten deutlich, die weiterer Forschungsarbeit bedürfen. Die interdisziplinär angelegte Forschungsfrage und die unterschiedlichen Zielgruppen des zu erstellenden Korpus sind Faktoren, die es bei der Aufarbeitung der Daten besonders zu beachten gilt. 3 https://gate.ac.uk/sale/tao/split.html. .(06.10.2014, 12.51 Uhr).",de,forschungsvorhaben interdisziplinär anlegen beruhen Korpus verschieden textzeugen zwei frühneuhochdeutsch Rechtsbücher Jahrhundert Klagspiegel Laienspiegel Klagspiegel Abstand alt populärwissenschaftlich Rechtsbuch Rezeptionszeit bilden Laienspiegel wichtig Grundlage rechtswissenschaftlich populär Text Jahrhundert Ziel Untersuchung sprachlich Besonderheit Text Auswirkung Rezeptionsgeschicht römisch Recht Deutschland korpusbasiert linguistisch Analyse Bücher völlig Textsort begründen bieten Projekt Perspektive historisch Rechtssprachenforschung innovativ Ansatz Erkenntnisinteresse liegen hierbei Geschichte Kulturtransferprozess innerhalb Jurisprudenz semantisch linguistisch annotationen umfassend Forschungsgrundlage schaffen Schließung sprachhistorisch forschungslücken zentral Beitrag leisten weit Schritt Digitalisierung mehrere Ausgabe Klagspiegel Prozesse Schreibsprachwandel früh Jahrhundert nachvollziehen Korpus frühneuhochdeutsch rechtstexte Schritt Vorbereitung Projekte verschieden Annotationstool testen geeignet Formate Speicherung evaluieren aktuell jenaer Computerlinguistik Möglichkeit Normalisierung automatisch Annotation erproben Ziel Beantragung groß forschungsprojektes bestehend Werkzeuge nutzen Technologie Besonderheit Rechtskorpus anpassen Poster bisherig methodologischen überlegungen Problem darstellen bieten somit gleichzeitig Überblick evaluation aktuell Verfügung stehend open source software annotationszwecken Untersuchung beziehen sprachlich Herkunft Klagspiegel Laienspiegel feststellen textsoren spezifisch sprachlich eigenheiter vorliegen zudem klären Rechtsbücher aufgrund Herkunft südwestdeutsch Raum gesamt hochdeutsch Sprachgebiet verständlich möglich Ausgleichstendenz suchen oberdeutschen abweichen Ebene Syntax fragen Struktur sprachlich Einfachheit leicht Verständlichkeit ausmachen Text gesamt ausschließlich juristisch Forschung zuschreiben Bereich Wortschatz Bezeichnung juristisch fachbegriffe tatbestände Forschung interessant zuvor deutsch entsprechend Termini untersuchen inwieweit Klagspiegel Laienspiegel frühneuhochdeutschen Sprachstandard aufweisen büch Verbreitung wesentlich Rolle Entwicklung neuhochdeutsch Sprachstandard Rahmen rechtswissenschaftlich Prozesse spielen Vergleich mehrere Textzeug Rechtsbücher liefern erkenntnis frühneuhochdeutsch Schreibsprachwandel einfluss Text deutsch Standardsprache deutsch Rechtssprache analysieren Vorhaben hierfür nutzbar Ausgangsbasis liefern zentral Frage inwieweit römisch deutsch sprachlich unterschiedlich vermitteln Textinter varianz einzeln Passag literarisieren feststellen Textzeuge jeweils Ausgabe Klagpiegel liegen digitalisiert Abbildung transkribieren nächster Schritt übertragen semantisch linguistisch annotiert valid datenbasis Untersuchung schaffen korpus standardisiert Format Infrastruktur Digital Humanitie Verfügung stellen Sinn vielseitig nutzbaren Korpus Transkription diplomatisch sonderzeich typografisch Besonderheit abbilden problematisch heterogen Gestalt Text Mehrfachannotation notwendig Annotation Format vornehmen leicht Übertragung Formate annotationsfrei Primärtext ermöglichen bieten hierfür Möglichkeit Format Kompatibel werkzeuge bieten geeignet arbeitsoberflächen Nachteil diskutieren gelten Präsentation stellen somit Mehrwert geleistet Forschungsarbeit Rahmen Digital Humanitie sprachwissenschaftlich Untersuchung historisch Text heraus Grenze Annotation heterogen standardisierter sprachdaten deutlich weit Forschungsarbeit bedürfen interdisziplinär angelegt Forschungsfrag unterschiedlich zielgruppen erstellend Korpus faktoren Aufarbeitung daten beachten gelten,"[('klagspiegel', 0.3221875858432547), ('laienspiegel', 0.25775006867460376), ('rechtsbücher', 0.19331255150595278), ('frühneuhochdeutsch', 0.17065030372848594), ('sprachlich', 0.15375301202954048), ('rechtswissenschaftlich', 0.12887503433730188), ('sprachstandard', 0.12887503433730188), ('schreibsprachwandel', 0.12887503433730188), ('römisch', 0.12003732424938887), ('juristisch', 0.11376686915232397)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,eComparatio. Editionsvergleich,"Oliver Bräckel (Universität Erfurt, Max-Weber-Kolleg, Deutschland); Hannes Kahl (Universität Erfurt, Max-Weber-Kolleg, Deutschland); Friedrich Wilhelm Meins (Universität Erfurt, Max-Weber-Kolleg, Deutschland); Charlotte Schubert (Universität Leipzig\Historisches Seminar, Deutschland)",Digitale Edition,Digitale Edition,"Das von der Deutschen Forschungsgemeinschaft (DFG) geförderte Projekt eComparatio wird seit 2014 als Kooperationsprojekt des Lehrstuhls für Alte Geschichte der Universität Leipzig und des ICE (Interdisciplinary Center of E-Humanities in History and Social Sciences/ Forschungsstelle am Max-Weber-Kolleg für kultur- und sozialwissenschaftliche Studien an der Universität Erfurt) entwickelt. Das Ziel des Projektes ist es, eine modular aufgebaute Anwendung zu entwickeln, die es ermöglicht, verschiedene Versionen eines Textes (aus Handschriften, gedruckten oder digitalen Texteditionen) miteinander zu vergleichen. Das Kernstück der Anwendung ist ein Modul zum Vergleich von Textausgaben, das auch die Erstellung eines Variantenapparates für digitale Editionen antiker Autoren ermöglicht. Die Zahl der Vergleichstexte ist beliebig, ebenso das Eingabeformat (TXT, HTML, XML, JSON, PDF). Die Anwendung wird frei skalierbar sein, so dass der Umfang der zu vergleichenden Texte nicht beschränkt ist, das Ergebnis (Kollationierung) soll in Form von Listen als kritischer Apparat (positiver oder negativer Apparat) oder auch in beliebiger anderer Form ausgegeben werden können. In einem weiteren Modul soll für Autorenreferenzen bei der Abfrage von online-Datenbanken die Anbindung an das Referenzsystem CTS (Canonical Text Services) und die Referenz auf Images von Handschriften (über das Image Citation Tool der CITE Collection Services) ermöglicht werden. Die Ansprechbarkeit für weitere Adressschemata wird ebenfalls implementiert (z.B. für JSON und den im Aufbau befindlichen PID-Service von CLARIN-D). Im bisherigen Verlauf des Projektes ist es gelungen, die Grundfunktionen des Tools zu implementieren und es in die Lage zu versetzen eine beliebig große Anzahl an Texten miteinander zu vergleichen. Dabei sind drei unterschiedliche Ansichten entstanden, die es dem Benutzer ermöglichen das Ergebnis aus verschiedenen Perspektiven zu betrachten. Die Detailansicht zeigt einen Text und markiert entsprechende Unterschiede zu anderen Texten. Die Parallelansicht (siehe auch Abbildung) zeigt alle Texte nebeneinander und markiert die Unterschiede farbig. Die Buchansicht schließlich zeigt wieder nur einen Text an und visualisiert die Varianten im Stile traditioneller Printeditionen unter dem betreffenden Abschnitt. Zu betonen ist dabei, dass der Ausgangstext für den Vergleich bei jeder Ansicht frei wählbar ist und sich somit nicht auf einen zu bevorzugenden Haupttext festgelegt bzw. eine Gewichtung der Textzeugen vorgenommen wird. Die Visualisierung und Ergebnissicherung ermöglicht zum einen, einen schnellen Überblick über die Text- und Editionsgeschichte verschiedener in digitalisierter Form vorliegender Werke zu erlangen. Darüber hinaus eignet sich das Tool als Hilfsmittel zum Kollationieren bei der Erstellung beliebiger kritischer, historischer bzw. genetischer Editionen. Weitere Funktionen, die das Spektrum von eComparatio noch einmal entscheidend erweitern werden, sind in Entwicklung. So ist die Einbindung von hochauflösenden Images der Handschriften der betreffenden Editionen geplant, um auch diesen Abschnitt der Textgeschichte dem Nutzer zugänglich zu machen. Weiterhin ist ein weiteres Modul in Entwicklung, das für die Abfrage von online-Datenbanken die Anbindung an das Notationssystem CTS (Canocical Text Services) ermöglicht. Beide Erweiterungen des Tools werden in absehbarer Zeit implementiert werden. Nach seiner Fertigstellung soll das Tool als freier Webservice für Forschung und Lehre zur Verfügung gestellt werden. Davon können Handschriften-Digitalisierungsprojekte, Editionsprojekte sowie Projekte profitieren, die sich Spezialfragen einzelner Textpassagen widmen; es ist auch für Seminararbeiten, d.h. den Einsatz in der Lehre geeignet, da es sowohl von Nicht-Editionsphilologen als auch von Editionsphilologen eingesetzt werden kann. Es ist natürlich auch nicht an den Fachbereich der Alten Geschichte gebunden, sondern kann in verschiedenen Bereichen der Textwissenschaften, unabhängig von der Sprache, eingesetzt werden. In der Fachcommunity der E-Humanities im Speziellen kann das Tool darüber hinaus in einem Bereich angewandt werden, der in jüngerer Zeit vermehrt ins Zentrum der Aufmerksamkeit gerückt ist, nämlich bei der Qualitätssicherung der digitalen Datengrundlage an sich. Gerade im Falle der Altertumswissenschaften, in denen bereits früh umfangreiche, abgeschlossene Korpora (TLG, BTL u.a.) vorlagen, ist ein nächster Schritt ein Ausbau dieser Datengrundlagen in die Tiefe, d.h. hinsichtlich der zahlreichen verschiedenen Editionen und Textausgaben. Solche Varianten spielen in der herkömmlichen altertumswissenschaftlichen Diskussion oftmals eine zentrale Rolle bei der Erörterung fachwissenschaftlicher Fragestellungen; die Möglichkeit, solche Varianten im Falle auch großer Textmengen schnell zu überblicken, kann als eine wesentliche Grundlage dafür gesehen werden, auch auf ""klassischem"" Textmining basierende Untersuchungen mit einer besseren Datengrundlage zu versehen. Da es sich bei dem Tool in erster Linie um ein Mittel zur Visualisierung handelt, ist es in hohem Maße für die Präsentation in Form eines Posters geeignet. Geplant ist die Darstellung des gesamten Workflows anhand eines Beispiels, von der Eingabe unstrukturierter Textdokumente bis hin zu den drei oben genannten Visualisierungsformen. Abb. der Parallelansicht von eComparatio am Beispiel des Fragments B1 des Anaximander.",de,deutsch Forschungsgemeinschaft dfg gefördert Projekt ecomparatio kooperationsprojekt Lehrstuhl alt Geschichte Universität Leipzig ice interdisciplinary center of History And social sciences forschungsstelle sozialwissenschaftlich Studie Universität Erfurt entwickeln Ziel Projekte Modular aufgebaut Anwendung entwickeln ermöglichen verschieden Version Text handschrift gedruckt digital Texteditione miteinander vergleichen Kernstück Anwendung Modul Vergleich Textausgaben Erstellung Variantenapparat digital edition antiker Autor ermöglichen Zahl Vergleichstexte beliebig eingabeformat txt html xml Json Pdf Anwendung frei skalierbar Umfang vergleichend Text beschränken Ergebnis Kollationierung Form List Kritischer Apparat positiv negativ Apparat Beliebiger anderer Form ausgeben Modul Autorenreferenze Abfrage Anbindung Referenzsystem Cts canonical Text services Referenz imag handschrift Image Citation Tool Cite Collection services ermöglichen Ansprechbarkeit Adressschemata ebenfalls implementieren Json Aufbau befindlich bisherig Verlauf Projekte gelingen Grundfunktion Tool implementieren Lage versetzen beliebig Anzahl Text miteinander vergleichen unterschiedlich Ansicht entstehen Benutzer ermöglichen Ergebnis verschieden Perspektive betrachten Detailansicht zeigen Text markieren entsprechend Unterschied Text Parallelansicht sehen Abbildung zeigen Text nebeneinander markieren Unterschied farbig buchansichen schließlich zeigen Text visualisieren Variant Stil traditionell Printedition betreffend abschneten betonen Ausgangstext Vergleich Ansicht frei wählbar somit bevorzugend Haupttext festlegen Gewichtung Textzeug vornehmen Visualisierung Ergebnissicherung ermöglichen schnell Überblick editionsgeschicht Verschiedener digitalisiert Form vorliegend Werk erlangen hinaus eignen Tool Hilfsmittel Kollationieren Erstellung Beliebiger Kritischer historisch genetisch editionen Funktion Spektrum ecomparatio entscheidend erweitern Entwicklung Einbindung hochauflösend imag Handschrift betreffend editionen planen Abschnitt textgeschichte Nutzer zugänglich weiterhin Modul Entwicklung Abfrage Anbindung notationssyst Cts canocical Text services ermöglichen Erweiterunge Tool absehbar implementieren Fertigstellung Tool frei Webservice Forschung Lehre Verfügung stellen editionsprojeken Projekt profitieren spezialfragen einzeln Textpassage widmen seminararbeit Einsatz Lehre geeignet sowohl Editionsphilolog einsetzen Fachbereich alt Geschichte binden verschieden Bereich Textwissenschaft unabhängig Sprache einsetzen Fachcommunity speziell Tool hinaus Bereich anwenden jung vermehrt Zentrum Aufmerksamkeit rücken nämlich Qualitätssicherung digital Datengrundlage Fall Altertumswissenschaft früh umfangreich abgeschlossen Korpora Tlg btl vorlagen nächster Schritt Ausbau Datengrundlagen tief hinsichtlich zahlreich verschieden editionen Textausgaben Variant spielen herkömmlich altertumswissenschaftlich Diskussion oftmals zentral Rolle Erörterung fachwissenschaftlich Fragestellung Möglichkeit Variant Fall Textmeng schnell überblicken wesentlich Grundlage sehen klassisch textmining basierend Untersuchung gut Datengrundlage versehen Tool Linie Visualisierung handeln hoch Maß Präsentation Form Poster geeignet planen Darstellung gesamt Workflow anhand Beispiel Eingabe unstrukturiert Textdokument genannt visualisierungsformen abb Parallelansicht ecomparatio Fragment anaximander,"[('tool', 0.23709366402956408), ('ecomparatio', 0.19509840970850928), ('services', 0.17222680376590274), ('modul', 0.1419921852951784), ('editionen', 0.13088969419017085), ('parallelansicht', 0.13006560647233953), ('cts', 0.13006560647233953), ('imag', 0.13006560647233953), ('handschrift', 0.12861315348748242), ('ermöglichen', 0.12396313992940104)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Automatische Erkennung von Figuren in deutschsprachigen Romanen,Fotis Jannidis (Universität Würzburg); Markus Krug (Universität Würzburg); Frank Puppe (Universität Würzburg); Isabella Reger (Universität Würzburg); Martin Töpfer (Universität Würzburg); Lukas Weimer (Universität Würzburg),"Netzwerkanalyse, Named Entity Recognition, Conditional Random Fields, maschinelles Lernen","Netzwerkanalyse, Named Entity Recognition, Conditional Random Fields, maschinelles Lernen","Eine wichtige Grundlage für die quantitative Analyse von Erzähltexten, etwa eine Netzwerkanalyse der Figurenkonstellation, ist die automatische Erkennung von Referenzen auf Figuren in Erzähltexten, ein Sonderfall des generischen NLP-Problems der Named Entity Recognition [Sharnagat 2014]. Mit dem Stanford Parser [Finkel 2005] unter Verwendung eines Modells für deutsche Sprache [Faruqui and Pado 2010] liegen inzwischen auch freie Werkzeuge für Texte in deutscher Sprache vor. Allerdings ist die Erkennungsrate des Modells, das an einem Korpus von Zeitungstexten trainiert wurde, für literarische Texte nur eingeschränkt brauchbar (Abb. 1). Eine Auswertung anhand unseres Testkorpus (265 000 Tokens) hat einen F1-Score von nur 31% ergeben, was vor allem am sehr niedrigen Recall lag. Dieser Befund deckt sich mit vergleichbaren Erfahrungen aus der Computerlinguistik: Viele NLP-Werkzeuge müssen erst für einen neuen Anwendungsbereich angepasst werden, um brauchbare Resultate zu erbringen. Im Fall des Romankorpus führt die Einbeziehung von Appellativen in die Named Entity-Definition und deren häufige Verwendung in Romantexten zu dem schlechten Ergebnis. Da die Figurenreferenzen allerdings für fast alle nachfolgenden Verarbeitungsschritte eine hohe Relevanz haben, sind wir nicht den Weg einer automatischen Domänenadaption [Qi Li 2012] gegangen, sondern haben ein umfangreiches Trainingskorpus aufgebaut, um auf diese Weise möglichst hohe Erkennungsraten zu erhalten. Im Folgenden berichten wir über unser Vorgehen, diese Aufgabe möglichst effizient zu gestalten. Zusammenfassend können wir feststellen, dass wir die Erstellung des notwendigen Trainingskorpus durch ein Werkzeug erheblich beschleunigen konnten, das den Annotatoren bereits gute Vorschläge machte. Außerdem konnten die Resultate des verwendeten Lernverfahrens dadurch deutlich verbessert werden, dass über die üblichen Standardfeatures hinaus word2vec-Informationen (s.u.) als Feature verwendet wurden. 2 Abb. 1: Ergebnisse des Stanford-Parsers mit deutschem Modell (Faruqui and Pado 2010) angewandt auf ein Zeitungskorpus (CoNLL 2003) und ein Korpus deutschsprachiger Romane. Material und Methoden Als annotierte Trainings- und Testdaten dienten das Zeitungskorpus der CoNLL 2003 [Sang 2003] (ca. 220 000 Tokens) und ein von uns aufbereitetes Romankorpus mit je 130 zusammenhängenden Sätzen aus 50 Romanen mit 140 000 Tokens für das erste Experiment, und 85 Romanen mit 265 000 Tokens für das zweite Experiment. Die Annotation geschah mittels einem eigens für diesen Zweck entwickelten Werkzeug, das über eine komfortable grafische Benutzeroberfläche dem Annotator die mit einfachen Regeln ermittelten Vorschläge zur Bearbeitung anbietet, wodurch sich die Annotation erheblich beschleunigen ließ (die vorher direkt in XML-Dateien und dann in einem Annotationswerkzeug durchgeführt wurde, das nicht spezifisch für die Aufgabe angepasst wurde). Notiert wurden folgende Eigenschaften: a) Handelt es sich um einen wirklichen Namen, z.B. ""Effi Briest"", oder um einen Appellativ, z.B. der ""Lehrer"". b) Handelt es sich um eine einzelne Person oder um eine Personengruppe bzw. um mehr als eine Person, z.B. die ""Gäste"". c) Koreferenz per Identität (ID), d.h. alle Referenzen auf die gleiche Figur erhalten die gleiche grafisch angezeigte ID. Für die Anwendung unüberwachter Lernverfahren verwendeten wir Texte aus der FAZ (ca. 15 Millionen Tokens) und unser Erweiterungskorpus deutschsprachiger Romane (ca. 60 Millionen Tokens), beide Textsammlungen nicht annotiert. In der ersten Serie von Experimenten wurde die Frage untersucht, mit welchen Features das maschinelle Lernverfahren Conditional Random Fields (CRF), das auch im Stanford Parser eingesetzt wird, die besten Ergebnisse erbringt. Folgende sechs Features, die vom StanfordTagger [Finkel 2005] verwendet werden, wurden als Basis betrachtet: 3 1) 2) 3) 4) 5) 6) Current Word: das Wort an Position i Previous Word: das Wort an Position i-1 Next Word: das Wort an Position i+1 Word Shape: für Groß/Kleinschreibung oder Zahlen Part-Of-Speech Tags (POS-Tags) an den Positionen i, i-1 und i+1, die mit Hilfe des TreeTaggers [Schmid 1995] bestimmt wurden. Präfix bzw. Suffix, das aus den ersten oder letzten 2 Zeichen besteht. Außerdem getestete Features: 7) Gazeteers: Listen bestehend aus rd. 5200 männlichen, 3400 weiblichen Vornamen, 160 Adelstiteln, Anreden und 8700 Berufen. 8) Semantische Felder, je nach Wortart 15-23, auf der Grundlage von GermaNet 9) Satzsubjekt ermittelt mit dem Mate-Dependecy Parsers [Bohnet 2010]. 10) Compound-Words: alle von SFST [Fitschen 2004] erkannten Teilworte des Eingabewortes inkl. Prä- und Suffixe. 11) Head-Lemma: Grundform des zum Subjekt gehörenden Verbes. 12) LDA-Cluster: Es wird die Zugehörigkeit aller Nicht-Stop-Wörter zu dem wahrscheinlichsten von 250 Clustern mit der Latent-Dirichlet-Allocation (LDA) [Blei 2003] in Anlehnung an [Chrupala 2011] auf der Basis der oben erwähnten nicht annotierten Korpora mit 15 Millionen bzw. 60 Millionen Token ermittelt. Das LDA wurde mit dem Framework MALLET [MALLET 2002] implementiert. 13) Word2Vec-Cluster: Es wird ebenfalls die Zugehörigkeit aller Nicht-Stop-Wörter zu einem semantischen Cluster ermittelt. Dabei wurde eine effiziente Implementierung des ""Continuous Bag-of-Words"" Modells nach [Mikolov 2013] genutzt und die resultierenden Vektoren mit einem k-means Verfahren geclustert. Ergebnisse Zum Testen der gelernten CRFs wurde eine 10-fache Kreuzvalidierung auf der Trainingsmenge des Romankorpus (120.000 Tokens) durchgeführt. Die Baseline mit den Features 1-6 erbrachte einen F1-Score von 86,66%. Die Kombination der besten Features (letzte Zeile) erzielte einen F1-Score von 89,98, d.h. eine Steigerung um 3,32 Prozentpunkte. Der mit Abstand größte Anteil an dieser Steigerung ging auf das semantische Feature ""Word2Vec-Cluster"" zurück. Dagegen erbrachte das semantische Clustering mit LDAs einen eher negativen Effekt. In [Tkachenko 2012] wird der gleiche Effekt berichtet und die Vermutung geäußert, dass die LDA-Cluster redundant zu den POS-Tagging-Features sind. Beim Trainingskorpus mit den Zeitungsartikeln war die Baseline mit 87,9% etwas besser, aber die Steigerung durch Hinzunahme des Word2Vec-Cluster mit 1,6 Prozentpunkten (auf 89,5%) etwas schlechter. 4 Verfahren Precision in % Recall in % F1-Score in % Baseline (Features 195.12 79.60 86.66 Unterschied zur Baseline (F1-Score) in % +0 6) Baseline + (Feature 95.73 79.28 86.70 +0.04 7) Baseline +(8) 94.53 81.74 87.65 Baseline + (9) 94.96 79.74 86.67 Baseline + (10) 95.07 81.00 87.45 Baseline + (11) 95.03 79.63 86.63 Baseline + (12) 96.47 77.83 86.13 Baseline + (13) 94.97 85.28 89.84 Baseline + (7),(8),(10),(13) 94.86 85.60 89.98 +0.99 +0.01 +0.79 -0.03 -0.53 +3.18 +3.32 Tab. 1. Einfluss verschiedener Features auf die NER mit CRFs; Trainingsset ca. 120 000 Tokens. Wir haben beim Feature 13 ""Word2Vec-Cluster"" untersucht, welchen Einfluss die Anzahl der vorgegeben Cluster im k-means Verfahren zwischen 100 und 1000 auf die Qualität der NER hat. Dabei stellte sich heraus, dass bei einer Clusteranzahl ab 250 (relativ konstant bis 1000) das beste Ergebnis erzielt wird, so dass in weiteren Experimenten die Clusteranzahl von 250 gewählt wurde. In unserem zweiten Experiment beschäftigten wir uns mit den Fragen, wie groß unser annotiertes Korpus für das Training eines praktisch nutzbaren NER-Modells sein muss, bzw. ab welcher Größe eine Erweiterung des Trainingsmaterials keine nennenswerte Verbesserung der Erkennungsleistung mehr bringt. Als zweiten Aspekt gilt es das für unseren Task beste Lernverfahren zu ermitteln. Für diesen Zweck haben wir die Erkennungsgenauigkeit mit immer größeren Mengen von Trainingsdaten gemessen: Für beide Domänen wurde zunächst nur eine Trainingsmenge von 30 000 Tokens genutzt, die dann in Schritten von 10 000 Tokens auf die Maximalzahl von 230 000 Tokens bei den Romanen bzw. 170 000 Tokens bei den Zeitungsartikeln gesteigert wurde. Als Features haben wir die jeweils beste FeatureMenge für das CRF verwendet. Neben dem CRF-Klassifikator wurden auch MaximumEntropy, Naive Bayes und Decision-Trees mit der gleichen Menge an Features getestet. Abb. 2 zeigt, dass die beiden besten, von uns getesteten Klassifikationsverfahren MaxEnt, sowie CRFs sind. Auf dem Zeitungskorpus sind CRFs ca. 3-5% besser als MaxEnt, die Evaluation auf dem Romankorpus zeigt genau entgegengesetzte Ergebnisse. Eine Ausnutzung der Zustandsübergangsinformation, die CRFs zusätzlich zu MaxEnt nutzen, scheint im Fall der Romane keine nützlichen Informationen zu liefern, sondern das Ergebnis zu verschlechtern. Dies könnte in einer deutlich höheren durchschnittlichen Satzlänge (24,2 Tokens vs. 16,3 Tokens) in unserer Domäne begründet liegen. Ab einer Trainingsmenge von etwa 150 000 5 Tokens zeigt sich keine signifikante Verbesserung der Ergebnisse mehr. Wenn statt dieser 10Fold Cross-Validation eine Leave-One-Out-Evaluation verwendet wird, bei der der zu testende Roman nicht in der Trainingsmenge enthalten ist, verringert sich der durchschnittliche F1-Score um ca. fünf Prozentpunkte von 88% auf 83.4%. Entgegen unserer Erwartung führte die Hinzunahme von 35 Romanen in dem Trainingskorpus zu keiner Verbesserung der Erkennungsrate, sondern sogar zu einer Verschlechterung um ca. 2%. Eine genauere Analyse zeigte, dass unter diesen zufällig ausgewählten Romanen auch solche mit Dialekten und anderen Besonderheiten waren, was die Verschlechterung erklären könnte.1 Abb. 2. Einfluss verschiedener Größen von Trainingsdaten von 30 000 bis 230 000 bzw. 170 000 Tokens auf den F1-Wert der NER mit CRFs in zwei verschiedenen Domänen (Romane und Zeitungsartikel) und verschiedenen maschinellen Lernverfahren Ausblick Es gibt eine Reihe von weiteren Optimierungsverfahren, die im Anschluss an die berichteten Experimente exploriert werden sollen. Wir haben bisher nur Lernverfahren für die NER in Romanen auf der Basis annotierter Textkorpora untersucht. Wir versprechen uns sowohl beim Erstellen eines Goldstandards, als auch bei dem erzielbaren F1-Wert der NER Verbesserungen durch die Integration von komplexeren regelbasierten Verfahren [Klügl et al. 2014] zur Information Extraction. Außerdem soll der Vermutung nachgegangen werden, dass die Erkennungsleistung durch Verwendung von Strategien der Domänenanpassung noch verbessert werden kann, wenn diese auf das vorhandene umfangreiche Korpus mit nichtannotierten Daten angewandt werden [Qi Li 2012]. Außerdem sollen Alternativen zum 1 Unsere Implementierung des MaxEnt-Modells ist unter https://github.com/MarkusKrug/NERDetection/ zu finden. Sie ist so aufbereitet, dass sie mit dem DkPro-Framework kompatibel ist. Die Eingliederung dort soll demnächst folgen. 6 word2vec-Feature erprobt werden, die in NLP-Tasks gleichwertige Ergebnisse erbracht haben [Pennington 2014]. Literatur Blei, D., Ng, A. and Jordan, M. (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research. 3, 993‚Äì1022. Bohnet, B. (2010). Very High Accuracy and Fast Dependency Parsing is not a Contradiction. The 23rd Int. Conference on Computational Linguistics (COLING 2010), Beijing, China. Chrupala, G. (2011). Efficient induction of probabilistic word classes with LDA. Proceedings of 5th International Joint Conference on Natural Language Processing, 363-372. Faruqui, M. and Pado. S. (2010) Training and Evaluating a German Named Entity Recognizer with Semantic Generalization. Proceedings of Konvens 2010, Saarbrücken, Germany. Finkel, F., Grenager, T. and Manning, C. (2005). Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370. http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf Fitschen, A., Schmid, H. and Heid, U. (2004) SMOR: A German computational morphology covering derivation, composition, and inflection. Proceedings of the IVth International Conference on Language Resources and Evaluation (LREC 2004), 1263‚Äì1266. Klügl, P., Toepfer, M., Beck, P.D., Fette, G., Puppe, F. (2014) UIMA Ruta: Rapid development of rule-based information extraction applications. Natural Language Engineering First View, 1‚Äì40 (2014). DOI 10.1017/S1351324914000114. McCallum, A. MALLET: A Machine Learning for Language Toolkit. 2002. Mikolov, T., Chen, K., Corrado, G. and Dean, J. 2013. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013. Nadeau, D. and Sekine, S. (2007) A survey of named entity recognition and classification. Lingvisticae Investigationes 30 (1), 3-26 Pennington, J, Socher, R. and Manning, C. (2014) Glove: Global Vectors for Word Representation. Conference on Empirical Methods in Natural Language Processing (EMNLP 2014). Qi Li (2012): Literature Survey. Domain Adaption Algorithms for Natural Language Processing. nlp.cs.rpi.edu/paper/qisurvey.pdf Sang E. and Meulder, F. (2003) Introduction to the CoNLL-2003 shared task: languageindependent named entity recognition. Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 (4), 142-147. Schmid, H. (1995) Improvements in Part-of-Speech Tagging with an Application to German. Proceedings of the ACL SIGDAT-Workshop. Dublin, Ireland. Sharnagat, R. (2014) Named Entity Recognition: A Literature Survey. Surveys of the Center for Indian Language Technology. http://www.cfilt.iitb.ac.in/resources/surveys/rahul-ner-survey.pdf Tkachenko, M. and Simanovsky, A. (2012) Named entity recognition: Exploring features. Proceedings of KONVENS 2012, 118-127.",de,wichtig Grundlage quantitativ Analyse Erzähltext Netzwerkanalyse Figurenkonstellation automatisch Erkennung Referenz Figur Erzähltext Sonderfall generisch named Entity Recognition sharnagatn stanford pars Finkel Verwendung Modell deutsch Sprache faruqui and Pado liegen inzwischen frei Werkzeug Text deutsch Sprache Erkennungsrat Modell Korpus zeitungstexten trainieren literarisch Text eingeschränkt brauchbar abb Auswertung anhand unser Testkorpus Token ergeben niedrig Recall liegen Befund decken vergleichbar Erfahrung Computerlinguistik anwendungsbereich angepasst brauchbar Resultat erbringen Fall Romankorpus führen Einbeziehung Appellativ named häufig Verwendung Romantext schlecht Ergebnis Figurenreferenze fast nachfolgend Verarbeitungsschritt hoch Relevanz Weg automatisch Domänenadaption qi li gehen umfangreich Trainingskorpus aufbauen Weise möglichst hoch erkennungsrater erhalten folgend berichten vorgehen Aufgabe möglichst effizient gestalten zusammenfassend feststellen Erstellung notwendig Trainingskorpus Werkzeug erheblich beschleunigen annotatoren Vorschlag Resultat verwendet lernverfahrens deutlich verbessern üblich Standardfeature hinaus Feature verwenden abb Ergebnis deutsch Modell faruqui and Pado anwenden Zeitungskorpus conll Korpus deutschsprachig Roman Material Methode annotiert Testdat dienen Zeitungskorpus conll singen Token aufbereitet Romankorpus zusammenhängend Satz Roman Token Experiment romanen Token Experiment Annotation geschehen mittels eigens Zweck entwickelt Werkzeug komfortabel grafisch benutzeroberfläche Annotator einfach Regel ermittelt vorschlag Bearbeitung anbieten wodurch Annotation erheblich beschleunigen lassen vorher direkt Annotationswerkzeug durchführen spezifisch Aufgabe angepasst notieren folgend eigenschaften handeln wirklich Name effi Briest Appellativ Lehrer b handeln einzeln Person Personengruppe Person gäste c Koreferenz per Identität id Referenz gleich Figur erhalten gleich grafisch angezeigen id Anwendung unüberwacht lernverfahren verwenden Text Faz Million Token Erweiterungskorpus deutschsprachig Roman Million Token Textsammlunge annotiert Serie experimenten Frage untersuchen Feature maschinell Lernverfahren Conditional Random Field crf Stanford Parser einsetzen Ergebnis erbringen folgend Feature Stanfordtagger Finkel verwenden Basis betrachten Current Word Wort Position i Previous Word Wort Position Next Word Wort Position Word Shape Kleinschreibung zahlen tags Position i Hilfe Treetagger Schmid bestimmen präfix suffix letzter Zeichen bestehen getestet Features gazeteers Liste bestehend rd männlich Weibliche vornamen adelstiteln anred berufen semantisch feld Wortart Grundlage rmanen satzsubjekt ermitteln Parsers bohnen Sfst fitsch erkannt teilworen eingabewortes Suffixe Grundform subjekt gehörend verbes Zugehörigkeit wahrscheinlich clustern lda blei Anlehnung chrupala Basis erwähnt annotiert Korpora Million Million token ermitteln lda Framework mallet mallet implementieren ebenfalls Zugehörigkeit semantisch Cluster ermitteln effizient Implementierung continuous modells mikolov nutzen resultierenden vektoren Verfahren clustern Ergebnis test gelernt Crf Kreuzvalidierung Trainingsmenge Romankorpus tokens durchführen Baselin Feature erbringen Kombination features letzter zeile erzielen Steigerung Prozentpunkt Abstand groß Anteil Steigerung semantisch Feature erbringen semantisch Clustering Ldas eher negativ Effekt tkachenko gleich Effekt berichten Vermutung äußern redundant Trainingskorpus Zeitungsartikel baselin Steigerung Hinzunahme Prozentpunkt schlecht Verfahren Precision recall baselin features Unterschied baselin baselinen Feature baselinen Baselin baselin Baselin baselin baseline baselin tab einfluss verschieden Feature ner Crf trainingsset Token Feature untersuchen einfluss Anzahl vorgegeb Cluster Verfahren Qualität ner stellen heraus Clusteranzahl relativ konstant gut Ergebnis erzielen Experiment Clusteranzahl wählen unser Experiment beschäftigen Frage annotiert Korpus Training praktisch nutzbaren Größe Erweiterung Trainingsmaterial nennenswert Verbesserung Erkennungsleistung bringen Aspekt gelten unser Task gut lernverfahren ermitteln Zweck Erkennungsgenauigkeit groß Menge Trainingsdat messen Domäne Trainingsmeng Token nutzen Schritt Token Maximalzahl Token romanen Token zeitungsartikel steigern Features jeweils gut Featuremeng Crf verwenden Maximumentropy naiv Bayes gleich Menge Features testen abb zeigen getesteter klassifikationsverfahr Maxent crf Zeitungskorpus crfs Maxent Evaluation Romankorpus zeigen genau entgegengesetzt Ergebnis Ausnutzung Zustandsübergangsinformation Crf zusätzlich Maxent nutzen scheinen Fall Roman nützlich Information liefern Ergebnis verschlechtern deutlich hoch durchschnittlich Satzlänge Token Token Domäne begründen liegen Trainingsmenge Token zeigen signifikant Verbesserung Ergebnis verwenden testend Roman Trainingsmenge enthalten verringern durchschnittlich Prozentpunkt entgegen Erwartung führen Hinzunahme Romane Trainingskorpus Verbesserung erkennungsrat sogar Verschlechterung genau Analyse zeigen zufällig ausgewählt Roman Dialekt Besonderheit Verschlechterung erklären abb einfluss verschieden Größ Trainingsdat Token ner Crf verschieden Domän Roman zeitungsartikel verschieden Maschinelle lernverfahren Ausblick Reihe optimierungsverfahren Anschluss berichten experiment explorieren lernverfahren ner romanen Basis Annotierter textkorpora untersuchen versprechen sowohl Erstell Goldstandard erzielbarer ner Verbesserung Integration komplex regelbasiert Verfahren Klügl et Information Extraction Vermutung nachgegangen Erkennungsleistung Verwendung Strategie Domänenanpassung verbessern vorhanden umfangreich korpus nichtannotiert daten anwenden qi li Alternative Implementierung finden aufbereiten kompatibel Eingliederung demnächst folgen erproben gleichwertig Ergebnis erbringen Pennington Literatur Blei ng and Jordan latent Dirichlet Allocation Journal of Machine Learning research bohnen very high accuracy and fast dependency parsing -- not Contradiction The int conference on computational linguistics coling beijing China Chrupala efficient Induction of probabilistic Word Classes with lda proceedings of international joinen Conference -- natural language Processing Faruqui And Pado Training and evaluating German named entity recognizer with Semantic Generalization Proceedings of konvens saarbrücken Germany Finkel grenag and Manning incorporating Information into Information Extraction systems by gibbs sampling proceedings of the annual meeting -- -- association for computational linguistics acl pp fitschen Schmid and heid Smor German Computational Morphology Covering Derivation Composition And inflection proceedings of the ivth international conference on language resources and Evaluation Lrec Klügl Toepfer Beck fetten Puppe uima Ruta Rapid development -- Information Extraction Applications natural Language engineering First View doi Mccallum Mallet Machine Learning for Language Toolkit mikolov Chen Corrado And Dean Efficient Estimation -- Word Representation vector Space Proceedings -- workshop at Iclr Nadeau And sekinen survey -- named entity Recognition and Classification Lingvisticae investigation Pennington j soch and Manning glove global vectors for Word Representation conference on empirical Methods Natural language Processing emnlp qi li literatuer Survey Domain Adaption Algorithms for natural language Processing singen And meulder introduction to -- shared Task Languageindependent named entity Recognition Proceedings of -- seventh conference on natural language Learning at Schmid Improvement Tagging with Application to German proceedings of -- acl Dublin Ireland sharnagatn Named entity Recognition Literature Survey surveys of the center for indian language Technology tkachenko And Simanovsky Named entity Recognition Exploring Features Proceedings of konvens,"[('token', 0.2661619719261018), ('and', 0.2230530641234395), ('baselin', 0.20380849089952308), ('crf', 0.17965255951184805), ('proceedings', 0.16017158016975733), ('of', 0.1511330484970293), ('erbringen', 0.14144295857670278), ('named', 0.13165038208766391), ('feature', 0.13034041570445282), ('features', 0.12225504494229135)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Computerlinguistische Verfahren zur Aufdeckung struktureller Ähnlichkeiten in Narrativen,"Nils Reiter (Universität Stuttgart, Deutschland); Anette Frank (Universität Heidelberg, Deutschland)","Sequence Alignment, Domain Adaptation, connectivity score, Graph-_based predicate alignment  ‚àöä","Sequence Alignment, Domain Adaptation, connectivity score, Graph-_based predicate alignment  ‚àöä","Einführung In diesem Beitrag stellen wir eine Methode zur automatischen  Erkennung von strukturellen Öhnlichkeiten narrativer Texte auf der Handlungsebene vor. Dafür operationalisieren  wir  strukturelle Öhnlichkeiten als (intertextuelle)  Verbindungen (Alignments) zwischen Ereignissen. Die  verwendeten  Alignierungsalgorithmen bauen auf automatisch erzeugten  linguistischen Analysen der Texte auf und verwenden  als Kriterien Eigenschaften verschiedener  linguistischer  Ebenen. Ziel unseres Ansatzes ist es, materiell in Texten vorliegende Öhnlichkeiten  auffindbar zu machen und hervorzuheben, so dass sie  von  Wissenschaftlerinnen und Wissenschaftlern zielgerichtet  analysiert und interpretiert werden können. Anwendungsszenarien Die Untersuchung  struktureller  Öhnlichkeiten zwischen Narrativen spielt  in  vielen geisteswissenschaftlichen Disziplinen eine Rolle. Als Beispielszenarien  verwenden wir  die Märchen-__  und Ritualforschung. Öhnlichkeiten zwischen Märchen  sind auf verschiedenen  Granularitätsebenen untersucht worden. Propp (1958)  veröffentlichte eine Analyse, in der in russischen Märchen  prototypische Handlungen und Charaktere identifiziert  werden. Regelmäßigkeiten im  Auftreten  von Handlungen  und Charakteren werden in einer sog. ""Morphology of the  Folktale"" erfasst. Damit sollen typische Handlungsmuster (Ereignis X folgt auf Ereignis Y) beschrieben werden. Am  anderen Ende der Granularitätsskala existieren Sammlungen  wie der ATU-__Index (Uther, 2014), in dem Märchen mit gleichen  Handlungselementen (Aussetzen von Kindern) oder Charakteren  (Lebkuchenhaus) in Klassen zusammengefasst werden. Im Bereich der Ritualforschung  werden Rituale aus  diversen religiösen, kulturellen oder politischen Kontexten  untersucht. Unter dem Stichwort ""Ritualgrammatik"" (vgl. Hellwig und Michaels, 2013) wird  diskutiert, dass in verschiedenen Ritualen ähnliche Handlungen vorkommen und Teilnehmer ähnliche  Rollen übernehmen. Verschiedene Forscher vertreten die Auffassung, dass die  Zusammensetzung wiederkehrender Ereignisse zu Ritualen  Regeln folgt. Existierende Überlegungen zur Ritualgrammatik  sind nicht formalisiert  und daher für eine automatische Analyse nur begrenzt  nutzbar. Um unsere Methode entwickeln und testen zu können, haben wir für diese beiden Szenarien ein  englischsprachiges Korpus zusammengestellt, das  mehrere Beschreibungen des gleichen Typs enthält (ATU-__Märchenklasse bzw. Ritualtyp). Computerlinguistische Verarbeitung Wir wenden die gleichen computerlinguistischen Komponenten  auf beide Korpora an. Damit werden  linguistische  Repräsentationen  für Wortarten, (syntaktische) Dependenzrelationen, semantische Rollen, Wortbedeutungen und  Koreferenzketten erstellt. Verknüpft  ergeben  diese Annotationen eine Diskursrepräsentation,  die als Basis für die Alignierungsverfahren verwendet wird.  Da Ritualbeschreibungen untypische linguistische  Phänomene enthalten, wurden  sämtliche Komponenten auf die Domäne angepasst (Domain  Adaptation). Dadurch konnten deutliche Qualitätssteigerungen der computerlinguistischen Analyse erreicht  werden. Alignierungsexperimente Drei Alignierungsalgorithmen mit unterschiedlicher  Mächtigkeit wurden verglichen: Sequence  alignment  (Needleman-__Wunsch, 1970) ist der einfachste Algorithmus, der ausschließlich paarweise und nicht-__kreuzende  Alignierungen erzeugen kann. Graph-__ based predicate alignment  (GPA; Roth, 2014, Roth & Frank, 2012) kann paarweise und kreuzende Alignierungen erzeugen.  Bayesian model merging  (BMM; Stolcke & Omohundro,  1993) ist der mächtigste Algorithmus, der Alignierungen  beliebiger Länge mit Überkreuzungen  erzeugen kann.  Diese drei Algorithmen wurden in zwei Experimenten evaluiert: In einer intrinsischen Evaluation wurden die Ergebnisse mit einem von zwei Ritualwissenschaftlern parallel erzeugten Goldstandard verglichen (_=0.61). Dabei erzielte BMM die besten Ergebnisse insgesamt und GPA die besten Ergebnisse auf einem Einzeldokumentpaar. Im zweiten Experiment wurde aus den automatisch erzeugten  Alignierungen ein Maß für Dokumentenähnlichkeit berechnet  und in einem Clustering-__Verfahren eingesetzt. Das Ergebnis  des Clusterings ' eine Einteilung der Dokumente auf Basis der errechneten strukturellen Öhnlichkeit ' konnte  dann mit der Gruppierung verglichen werden, die  ""natürlicherweise""  in den Korpora vorkommt (Ritualtypen bzw.  ATU-__Klassen). Dabei zeigten sich wieder GPA und BMM als die leistungsstärksten Algorithmen. Visualisierung und Nutzung Um es Wissenschaftlerinnen und Wissenschaftlern aus der  Ritual-__  bzw. Märchenforschung zu ermöglichen die Analysen zu  nutzen, haben wir Visualisierungen entwickelt, die eine systematische Untersuchung der gefundenen Öhnlichkeiten ermöglichen.  Auf  einer Vogelperspektive stellen wir die Dokumentenähnlichkeit in  einer Heatmap dar.  Auf interessante, dicht verknüpfte Stellen  können wir hinweisen, indem  für jedes  Ereignis ein  connectivity score  in einem Diagramm anzeigt  wird.  Eine detaillierte Darstellung der Einzelereignisse (mit  Teilnehmern und Kontext-__Ereignissen)  ist ebenfalls  möglich.  Direkt aus der Diskursrepräsentation können wir außerdem  eine Visualisierung des sozialen Netzwerks erzeugen, in der wichtige  Entitäten (Charaktere, Gegenstände  undD Materialien) in einem Netzwerk angezeigt und gemeinsam auftretende Figuren verknüpft werden. Konklusion Der Posterbeitrag präsentiert eine Methode  zur Erkennung struktureller Öhnlichkeiten  zwischen narrativen Texten. Die  Öhnlichkeiten  werden basierend auf computerlinguistischen  Analysen  vollautomatisch identifiziert  und können zielgerichtet  auf unterschiedlichen Granularitätsebenen  dargestellt und manuell inspiziert werden. Damit eignet sich die Methode  auch zur Analyse von größeren Datenmengen, ohne bestimmte  Interpretationen vorwegzunehmen.  Eine ausführliche Darstellung des Verfahrens sowie des  geisteswissenschaftlichen Anwendungskontexts  findet sich in Reiter  (2014) und Reiter et al. (2014). Auf einer methodischen Ebene zeigt sich in diesem Projekt,  dass komplexe linguistische Analysen auch für nicht-__kanonische Textsorten erstellt werden können und eine  vielversprechende Ausgangsbasis für Analysen darstellen. Die Besonderheiten natürlicher Sprache (z.B. Ambiguität,  Vielseitigkeit) stellen für automatische Verarbeitung eine große Herausforderung  dar, werden aber in der Computerlinguistik bereits untersucht. Auf (computer-__)linguistische  Analysen aufzubauen erlaubt die Untersuchung komplexer  semantischer Phänomene, die vergleichsweise eng mit den  Zielkategorien vieler Geisteswissenschaften verwandt sind. Bibliographie Oliver Hellwig and Axel Michaels. Ritualgrammatik. In Christiane Brosius, Axel Michaels, and Paula Schrode, Hrsg., Ritual und Ritualdynamik, S.  144–150. Vandenhoeck & Ruprecht, Göttingen, Germany, 2013. Saul B. Needleman and Christian D. Wunsch. A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of Molecular Biology, 48(3):443–453, March 1970. Vladimir Yakovlevich Propp. Morphology of the Folktale. University of Texas Press, Austin, TX, 2nd edition, 1958.  Translated by Laurence Scott (Original work published 1928). Nils Reiter. Discovering Structural Similarities in Narrative Texts using Event Alignment Algorithms. PhD thesis, Heidelberg University, June 2014. Nils Reiter, Anette Frank, and Oliver Hellwig. An NLP-__based cross-__document approach to narrative structure discovery. Literary and Linguistic Computing, 29(4):583–605, 2014. Michael Roth. Inducing Implicit Arguments via Cross-__document Alignment ' A Framework and its Applications. PhD thesis, Heidelberg University, 2014. Michael Roth and Anette Frank. Aligning predicates across monolingual comparable texts using graph-__based clustering. In Jun""ichi Tsujii, James Henderson, and Marius Pa_ca, editors, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 171–182, Jeju Island, Korea, July 2012. Andreas Stolcke and Stephen Omohundro. Hidden markov model induction by bayesian model merging. In Steve J. Hanson, J. D. Jack D. Cowan, and C. Lee Giles, Hrsg., Advances in Neural Information Processing Systems, volume 5, pages 11–18. Morgan Kaufmann, San Mateo, California, 1993. Hans-__Jörg Uther. The Types of International Folktales: A Classification and Bibliography. Based on the system of Antti Aarne and Stith Thompson. Number 284'286 in FF Communications. Suomalainen Tiedeakatemia, Helsinki, 2004.",de,Einführung Beitrag stellen Methode automatisch Erkennung strukturell öhnlichkeiten narrativ Text Handlungsebene operationalisieren strukturell öhnlichkeiten intertextuell verbindung Alignment Ereignis verwendet alignierungsalgorithmen bauen automatisch erzeugt linguistisch Analyse Text verwenden kriterien eigenschaften verschieden linguistisch ebenen Ziel unser Ansatze materiell text vorliegend öhnlichkeiten auffindbar hervorzuheben wissenschaftlerinn Wissenschaftler zielgerichten analysieren interpretieren Anwendungsszenarie Untersuchung strukturell öhnlichkeiten narrativ spielen geisteswissenschaftlich disziplinen Rolle Beispielszenari verwenden Ritualforschung öhnlichkeiten märcher verschieden Granularitätseben untersuchen Propp veröffentlichen Analyse russisch märcher prototypisch handlungen Charaktere identifizieren regelmäßigkeiten auftret handlungen Charakteren morphology -- the folktalen erfasst typisch Handlungsmuster Ereignis x folgen Ereignis Y beschreiben Granularitätsskala existieren Sammlung uth märcher gleich handlungselementen Aussetz Kind Charakteren Lebkuchenhaus klasse zusammengefassen Bereich Ritualforschung ritual diverser religiösen kulturell politisch Kontext untersuchen Stichwort Ritualgrammatik Hellwig Michaels diskutieren verschieden ritual ähnlich Handlung vorkommen Teilnehmer ähnlich rollen übernehmen verschieden Forscher vertreten Auffassung Zusammensetzung wiederkehrend Ereignis ritualen Regel folgen existierend Überlegung Ritualgrammatik formalisieren automatisch Analyse begrenzen nutzbar Methode entwickeln testen szenarien englischsprachig Korpus zusammenstellen mehrere Beschreibung gleich Typ enthalten Ritualtyp computerlinguistisch Verarbeitung wenden gleich computerlinguistisch Komponente Korpora linguistisch Repräsentation Wortarten syntaktisch dependenzrelationen semantisch Rolle wortbedeutungen Koreferenzkett erstellen verknüpfen ergeben annotationen Diskursrepräsentation Basis Alignierungsverfahr verwenden ritualbeschreibungen untypisch linguistisch phänomen enthalten sämtlich Komponente Domäne angepasst domain Adaptation deutlich Qualitätssteigerungen computerlinguistisch Analyse erreichen alignierungsexperiment Alignierungsalgorithme unterschiedlich mächtigkeit vergleichen Sequence Alignment einfach Algorithmus ausschließlich paarweise Alignierung erzeugen based predicate alignment gpa Roth Roth Frank paarweise kreuzend Alignierunge erzeugen Bayesian Model Merging bmm stolcke omohundro mächtig Algorithmus Alignierunge Beliebiger Länge überkreuzung erzeugen Algorithm experimenten evaluieren intrinsisch Evaluation Ergebnis Ritualwissenschaftler parallel erzeugt Goldstandard vergleichen erzielen Bmm Ergebnis insgesamt gpa Ergebnis Einzeldokumentpaar Experiment automatisch erzeugt Alignierunge Maß Dokumentenähnlichkeit berechnen einsetzen Ergebnis Clustering Einteilung Dokument Basis errechnet strukturell Öhnlichkeit Gruppierung vergleichen natürlicherweise Korpora vorkommen Ritualtype zeigen gpa bmm leistungsstärkster algorithmen Visualisierung Nutzung wissenschaftlerinn Wissenschaftler Märchenforschung ermöglichen Analyse nutzen visualisierungen entwickeln systematisch Untersuchung gefunden öhnlichkeiten ermöglichen vogelperspektiv stellen Dokumentenähnlichkeit Heatmap dar interessant dicht verknüpft Stelle hinweisen jeder Ereignis Connectivity score Diagramm anzeigt detailliert Darstellung Einzelereignis Teilnehmer ebenfalls direkt Diskursrepräsentation Visualisierung sozial Netzwerk erzeugen wichtig entität charaktere Gegenstände Undd Materialien Netzwerk anzeigen gemeinsam auftretend Figur verknüpfen Konklusion Posterbeitrag präsentieren Methode Erkennung strukturell öhnlichkeiten narrativ Text öhnlichkeien basierend computerlinguistisch Analyse vollautomatisch identifizieren zielgerichten unterschiedlich Granularitätseben darstellen manuell inspizieren eignen Methode Analyse groß datenmenger bestimmt Interpretation vorwegnehmen ausführlich Darstellung Verfahren geisteswissenschaftlich Anwendungskontexts finden Reiter Reiter et methodisch Ebene zeigen Projekt komplex linguistisch analysen Textsort erstellen vielversprechend Ausgangsbasis Analyse darstellen Besonderheit natürlich Sprache Ambiguität Vielseitigkeit Stelle automatisch Verarbeitung Herausforderung dar Computerlinguistik untersuchen analyse aufbauen erlauben Untersuchung komplex semantisch phänomene vergleichsweise eng zielkategorien vieler geisteswissenschaften verwandt Bibliographie Oliver Hellwig and Axel Michaels Ritualgrammatik Christiane Brosius Axel Michaels And Paula Schrode ritual Ritualdynamik Vandenhoeck Ruprecht Göttingen Germany saul needleman and Christian Wunsch General method applicabel to -- search for similarities The Amino acid Sequence of two Proteins Journal of Molecular Biology march Vladimir Yakovlevich Propp Morphology -- -- folktale university of Texas press Austin tx Edition translated by Laurence Scott original work published nils Reiter Discovering Structural similariteisen Narrative texts Using Event alignment Algorithms phd Thesis Heidelberg University june nils Reiter aneten Frank And oliver Hellwig approach to Narrative structuer Discovery Literary and Linguistic Computing Michael Roth inducing implicit arguments via alignment Framework and its Applications phd Thesis Heidelberg University Michael Roth and anette Frank Aligning predicat Across monolingual comparabel texts using clustering jun Ichi Tsujii James Henderson And marius editor Proceedings of -- joinen Conference -- empirical Methods Natural Language Processing and computational natural language Learning pag jeju Island Korea July Andreas Stolcke And stephen omohundro hidd Markov Model Induction by Bayesian Model Merging Steve Hanson Jack Cowan and lee giles advancesn Neural Information processing systems volume pag Morgan Kaufmann San Mateo California uth The Types of international folktal Classification and bibliography based -- -- system of antti aarn and Stith Thompson Number ff communications Suomalainen Tiedeakatemia Helsinki,"[('and', 0.24608029992632824), ('öhnlichkeiten', 0.21009603366256718), ('alignment', 0.18202293661632815), ('roth', 0.1536443530615965), ('ritual', 0.1237172775240776), ('ritualgrammatik', 0.1237172775240776), ('alignierunge', 0.1237172775240776), ('bmm', 0.1237172775240776), ('gpa', 0.1237172775240776), ('hellwig', 0.1237172775240776)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Theorie und Praxis der erklärenden Annotation im Kontext der Digital Humanities,"Angelika Zirker (Eberhard Karls Universität Tübingen, Deutschland); Fabian Schwabe (Eberhard Karls Universität Tübingen, Deutschland); Matthias Bauer (Eberhard Karls Universität Tübingen, Deutschland)",Annotation,Annotation,"Einführung In diesem Beitrag werden zwei Referenzkorpora der deutschen Sprache verwendet, um daraus frequenznormierte Verlaufskurven für Wörter und Wortverbindungen zu berechnen: das DWDS_Kernkorpus des 20. Jhs. sowie das Referenzkorpus des Deutschen Textarchivs (1600–1900). Da beide Korpora bezüglich ihrer Metadaten vereinheitlicht und auch mit denselben linguistischen Informationen annotiert wurden, können korpusübergreifende Abfragen gestellt werden. Beispiele hierfür sind schreibweisentolerante Lemmasuchen oder textsortenspezifische Suchen. Die auf dieser Grundlage generierten Verlaufskurven stehen auf der Website des Deutschen Textarchivs für die Abfrage zur Verfügung. 1 Korpusgrundlage und Annotation Die Grundlage für die zeitlichen Verlaufskurven (Histogramme) bilden die 100 Millionen Textwörter des DWDS_Kernkorpus des 20. Jhs. sowie weitere 140 Millionen Textwörter des Deutschen Textarchivs, welches Werke des 17. bis 19. Jh. als Erstausgaben umfasst. Beide Korpora sind hinsichtlich der repräsentierten Textsorten aus Belletristik, Gebrauchsliteratur, Wissenschaft und (im DWDS:) Journalistischer Prosa sowie hinsichtlich der enthaltenen Disziplinen ausgewogen (Geyken 2007, 2013; Geyken et al. 2011). Sie wurden beide gemäß den TEI/P5_Richtlinien annotiert (Geyken et al. 2012; Haaf et al., forthcoming) und sind bezüglich der Metadaten untereinander interoperabel, insbesondere bezüglich der in den Histogrammen verwendeten Angaben zum Datum und zu den Textsorten. Beide Korpora wurden für die Auswertung linguistisch annotiert, insbesondere wurden alle Texte tokenisiert, lemmatisiert, nach lexikalischen Kategorien analysiert (PoS_Tagging) und mit GermaNet_Kategorien versehen. Von besonderer Bedeutung für die Generierung der Histogramme ist die CAB_Analyse der historischen Texte: Mit CAB werden historische Varianten einer Wortform auf die wahrscheinlichste Normalform reduziert und ihrem neuhochdeutschen Lemma zugeordnet (Jurish 2013). Damit ist nicht nur die Suche in den Korpora, sondern auch die Darstellung der Wortverläufe schreibweisenübergreifend möglich. Dies geschieht, indem das Suchwort extensional zu all denjenigen Wortformen expandiert wird, die eine (möglicherweise flektierte) Variante des Suchwortes darstellen. In den folgenden Abschnitten wird gezeigt, dass diese Vorgenerierung der möglichen Formen für den Benutzer eine effektive Hilfe darstellt (s. Abschnitt 3: Beispiele). Beide Korpora, das DWDS_Kernkorpus und das DTA_Korpus, sind mit der Suchmaschine DDC (Dialing DWDS Concordancer) indiziert (Jurish et al. 2014). DDC verfügt über reichhaltige Metadatenfilter sowie die Möglichkeit, mehrere Annotationen auf einer Wortposition zu indizieren und abfragbar zu machen. Darüber hinaus können reguläre Ausdrücke, Boolesche Verknüpfungen und Abstandsoperatoren genutzt werden. Insbesondere ist es möglich, Metadatenfilter und mehrfache linguistische Annotationen miteinander zu verbinden. Die Indizes von DDC wurden so optimiert, dass Abfragen über die zeitliche Verteilung (Histogramme) ausreichend schnell für eine dynamische Berechnung zur Laufzeit sind. Dadurch ist es auch möglich, zeitliche Verläufe für die gesamte Mächtigkeit der DDC_Abfragesprache dynamisch zu berechnen. 2 Visualisierung Grundlage der Visualisierung sind die nach Messpunkten (Jahreszahlen bzw. Datumsintervallen) normierten relativen Häufigkeiten pro Million Textwörter. Da die beiden Korpora weder gleich verteilt noch gleich groß sind und zudem die Wortanzahl je Zeitintervall variiert, ist die Normierung notwendig, um die Histogramme beider Korpora einheitlich zu präsentieren. Wie im vorigen Abschnitt erwähnt, können normierte relative Frequenzen nicht nur für einzelne Wortformen oder Lemmata, sondern auch für Kombinationen aus linguistischen Annotationen und Phrasensuchen ausgegeben werden. Darüber hinaus können Histogramme textsortenspezifisch (z. B. nur für Belletristik oder nur für Wissenschaft) oder textsortenübergreifend gebildet werden. Da die Häufigkeiten von Wortformen (Types) in Textkorpora zipfsch verteilt sind, ist aufgrund der Größe der beiden Referenzkorpora bereits bei Wortformen einer mittleren Häufigkeit damit zu rechnen, dass die Histogramme bei einzelnen Messpunkten Nullstellen aufweisen können. Umgekehrt kann auch die Unausgewogenheit der Textkorpora an einem Messpunkt zu ""Ausschlägen"" mit zu hoher Frequenz führen. Aus diesem Grund wurden in die Visualisierungskomponente verschiedene Parameter zur Glättung implementiert. Aus Platzgründen soll hier nur auf die beiden wichtigsten eingegangen werden: die Parameter ""window"" und ""pruning"" . Der Parameter ""window"" gibt die Fensterbreite (als natürliche Zahl) für die Glättung nach dem gleitenden Mittelwert an. Der Parameter ""pruning"" implementiert ein zweistufiges Verfahren. Im ersten Schritt wird eine Fehlerverteilung für die normierten Datenpunkte berechnet. Die beobachteten ""Fehler"" werden unter Annahme einer Normalverteilung in p_Werte überführt, und alle Datenpunkte mit p_Werten außerhalb des angegebenen Konfidenzbereichs (p=0.05) werden als Ausreißer behandelt. Datenpunkte, die Ausreissern entsprechen, werden durch eine lineare Interpolation der nächstliegenden Datenpunkte, die innerhalb des Konfidenzbereichs liegen, ersetzt. Die Visualisierung selbst erfolgt mittels der Javascript_Bibliothek Highcharts 1 Website des DTA abfragbar. 2 und ist auf der 3 Beispiele, Ergebnisse und Diskussion Im folgenden, abschließenden Abschnitt sollen einige Vorzüge der Visualisierung auf der Grundlage der oben beschriebenen Referenzkopora und ihrer linguistischen Annotationen anhand konkreter Beispiele illustriert werden. Die Abfragemöglichkeiten und Ergebnisse werden mit den entsprechenden Ergebnissen im Google Ngram Viewer 3 in Beziehung gesetzt. Beispiel 1: Tatsache Als erstes Beispiel dient das Lemma ""Tatsache"" , ein Begriff, der laut dem Etymologischen Wörterbuch (Pfeifer) erst mit einer Publikation aus dem Jahr 1756 Eingang in die deutsche Sprache fand. 4 Die Wortverlaufskurve in Abb. _ 1 bestätigt den Beginn dieser ""Wortkarriere"" und zeichnet ihn von der Mitte des 18. Jhs. bis in das 20. Jh. nach. 5 Die Verlaufskurve im Google Ngram Viewer, Abb. _ 2, zeigt eine vergleichbare Tendenz. Der Vergleich beider Histogramme lässt dennoch Probleme bei der Arbeit mit dem Ngram Viewer sichtbar werden: Historische Schreibweisen wie ""Thatsache"" 'sowie ggf. etliche weitere möglich Varianten 6 und idealerweise auch alle möglichen Flexionsformen bzw. Expansionen 'müssen bei der Eingabe der Anfrage explizit ergänzt werden, um die historische Entwicklung des Begriffs zu visualisieren. Auf den Punkt Flexionsformen/Expansionen wird im dritten Beispiel (""billig"") noch näher eingegangen; hier sind zunächst nicht erklärbare Ausschläge der Kurve vor 1756 von Interesse, als Spalding das Wort in die deutsche Sprache brachte. Besonders deutlich zeigt sich darin ein Problem des Google_Books_Korpus. Eine Recherche in den Dokumenten daselbst zeigt, dass es sich bei sämtlichen früheren Treffern um falsch datierte Dokumente handelt, darunter so prominente Beispiele aus dem 19. Jh. wie Goethes Schriften zur Morphologie (II. Teil, datiert auf 1659) und die Fliegenden Blätter (in Google Books datiert auf 1692). Die Wortverlaufskurven werden hier also durch Metadatenfehler verfälscht. 1 http://www.deutschestextarchiv.de/search/plot. 2 http://www.highcharts.com/products/highcharts. 3 https://books.google.com/ngrams. 4 Vgl. z. B. das Etymologische Wörterbuch des Deutschen (nach W. Pfeifer), digitale Version via DWDS: ""[...] nachgebildet (1756) von dem Theologen Spalding für engl. matter of fact [...]. "" Siehe Bestätigung der natürlichen und geoffenbarten Religion, Leipzig, 1756, sowie Johann Joachim Spaldings Übersetzung von Joseph Butlers The analogy of religion, natural and revealed, to the constitution and course of nature, 1736. Die Texterfassung dieses Werks ist für das DTA in Arbeit; derzeit stammt der früheste Beleg im DTA_Korpus aus Münter 1772. 5 Aus Platzgründen kann hier auf den Abfall der Kurve ab Mitte des 20. Jh. nicht eingegangen werden. 6 Im DTA_Korpus sind für den in dieser Hinsicht einfach erscheinenden Begriff ""Tatsache"" immerhin 15 Expansionen belegt, vgl. http://kaskade.dwds.de/dstar/dta/lizard.perl?q=Tatsache. Beispiel 2: merkwürdig Das zweite Beispiel illustriert die Möglichkeiten, die eine Textsortendifferenzierung für die Interpretation der Histogramme bietet. Der Begriff ""merkwürdig"" wurde Pfeifers Etymologischem Wörterbuch zufolge ab dem 19. Jh. vorrangig in der Bedeutung ""seltsam, verwunderlich"" verwendet; bis dahin dominierte die seit dem 17. Jh. gebräuchliche Verwendung im Sinne von ""bemerkenswert, bedeutsam"" . Abb. _ 3 zeigt das nach Textsorten differenzierte Histogramm. Belegt ist die relativ hochfrequente Verwendung von ""merkwürdig"" in den Textsorten Wissenschaft und Gebrauchsliteratur bis über die Mitte des 19. Jhs. hinaus. Folgt man der These (wie die im DTA verfügbaren Belege bestätigen), dass die Verwendung in der Wissenschaft und der Gebrauchsliteratur vorrangig im Sinne von ""bemerkenswert, bedeutsam"" geschah, legt dies einen etwas länger andauernden Gebrauch des Wortes in dieser Bedeutung nahe, als bei Pfeifer angegeben. Auch hier zeigt sich der Vorteil gegenüber der Verlaufskurve des Google Ngram Viewers (Abb. _ 4), wo die Textsortendifferenzierung fehlt. 7 Beispiel 3: billig Das abschließende Beispiel (""billig"") illustriert zwei weitere Vorzüge der auf den Referenzkorpora beruhenden Histogramme gegenüber dem Google Ngram Viewer: die bessere Abdeckung der DTA_Korpora bezüglich des 17. Jhs. und die bessere Handhabung der großen graphematischen Varianz v. a. in diesen historischen Texten. Abb. _ 5 zeigt das Histogramm für ""billig"" aus DTA und DWDS; die Kurve veranschaulicht die breite Verwendung des Begriffs im 17. Jh., die dazugehörigen Belege zeigen das Bedeutungsspektrum zwischen ‚Äòangemessen, gerechtfertigt‚Äô und ‚Äòmäßig, wohlfeil, günstig‚Äô . Die Kurve aus dem Google Ngram Viewer, Abb. _ 6, zeigt dagegen keinen kontinuierlichen Verlauf im 17. Jh., was angesichts der u. a. im DTA_Korpus belegten Verbreitung des Begriffs die verhältnismäßig geringe Substanz des Google_Books_Korpus für diesen Zeitraum belegt. Daher erscheint auf dessen Grundlage quellenbasierte Forschung zumindest in diesem Zeitraum kaum möglich. Ein weiteres, bereits angesprochenes Problem, kommt erschwerend hinzu: Insbesondere bei Abfragen für Zeiträume vor 1700 liefert das Google_Books_Korpus aufgrund der sehr heterogenen Graphie schlichtweg keine befriedigende Anzahl von Belegen; hier liefert die CAB_Analyse der DTA_Korpora klare Vorteile. Allein für das in dieser Hinsicht relativ unproblematisch erscheinende Lemma ""billig"" sind im DTA_Korpus die in Abb. _ 7 gezeigten immerhin 67 Flexions_ und Expansionsformen belegt. Bei einer Abfrage via DTA/DWDS werden alle diese Formen berücksichtigt, während der Nutzer des Google Ngram Viewers sie manuell eingeben (und zu diesem Zweck selbstverständlich überhaupt erst einmal präsent haben) müsste. 4 Ausblick Wie im vorigen Abschnitt gezeigt werden konnte, liefert die Zeitverlaufskurve auf der Grundlage der beiden Referenzkorpora interessante Ergebnisse, die mit dem wesentlich 7 NB: Überraschend ist zudem, dass die Graphien ""merkwirdig"" und ""merckwirdig"" in dem der Kurve zugrundeliegenden Korpus German (das nicht identisch mit dem abfragbaren aktuellen GB_Korpus ist) nicht belegt sind. größeren Google_Books_Korpus entweder nicht oder nur mit großem Rechercheaufwand ermittelbar gewesen wären. Grund dafür sind die verlässlichen Metadaten, die Zuordnung nach Textsorten sowie die aufgrund der genauen Texterfassung möglichen Erschließungsmethoden (CAB_Software). Es ist damit zu rechnen, dass sich die Lage der auf Referenzkorpora basierten zeitlichen Verlaufskurven künftig weiter verbessern wird. Zum einen liegt dies daran, dass immer mehr historische Volltexte in hoher Qualität entstehen. Diese Bemühungen werden dadurch verstärkt, dass die DFG erst unlängst eine spezifische OCR_Förderlinie aufgelegt hat. Zum anderen hat das DTA für die sich dynamisch verändernden Korpusgrundlagen bereits die geeigneten technischen Lösungen: Das Korpus des DTA wird automatisch im Wochenrhythmus indiziert. Abbildungen 8 http://www.deutschestextarchiv.de/search/ddc/lemmata/?lemma=Tatsache&mode=extended;norm=date%2Bclass&smooth=spli ne&single=0&grand=1&slice=10&prune=1&window=3&wbase=0&logavg=0&logscale=0&xrange=1740%3A2000&totals=0 Abb. _ 1: DTA_DWDS_Histogramm ""Tatsache"" https://books.google.com/ngrams/graph?content=Thatsache%2CTatsache&year _ start=1600&year _ end=2000&corpus=20&smo othing=3&share=&direct url=t1%3B%2CThatsache%3B%2Cc0%3B.t1%3B%2CTatsache%3B%2Cc0 _ Abb. _ 2: Google_Ngram_Histogramm ""Tatsache,Thatsache"" 8 Zu den gewählten Parametern der Kurvengenerierung siehe die jeweils angegebene URL. http://www.deutschestextarchiv.de/search/ddc/lemmata/?lemma=merkw%C3%BCrdig&mode=extended;norm=date%2Bclass&s mooth=spline&single=0&grand=1&slice=10&prune=1&window=3&wbase=0&logavg=0&logscale=0&xrange=1600%3A2000&tot als=0 Abb. _ 3: DTA_DWDS_Histogramm ""merkwürdig"" https://books.google.com/ngrams/graph?content=merkw%C3%BCrdig%2Cmerckw%C3%BCrdig%2Cmerckwirdig%2Cmerkwirdi g&year _ start=1600&year _ end=2000&corpus=20&smoothing=3&share=&direct _ url=t1%3B%2Cmerkw%C3%BCrdig%3B%2Cc0 %3B.t1%3B%2Cmerckw%C3%BCrdig%3B%2Cc0 Abb. _ 4: Google_Ngram_Histogramm ""merkwürdig,merckwürdig,merckwirdig,merkwirdig"" http://www.deutschestextarchiv.de/search/ddc/lemmata/?lemma=billig&mode=extended;norm=date%2Bclass&smooth=spline&s ingle=0&grand=1&slice=10&prune=1&window=3&wbase=0&logavg=0&logscale=0&xrange=1600%3A2000&totals=0 Abb. _ 5: DTA_DWDS_Histogramm ""billig"" https://books.google.com/ngrams/graph?content=billich%2Cbillig&year _ start=1600&year _ end=2000&corpus=20&smoothing=3& share=&direct _ url=t1%3B%2Cbillich%3B%2Cc0%3B.t1%3B%2Cbillig%3B%2Cc0 Abb. _ 6: Google_Ngram_Histogramm ""billig"" http://kaskade.dwds.de/dstar/dta/dstar.perl?fmt=expand_html&q=billig&x=Token Abb. _ 7: im DTA belegte Expansions_ und Flexionsformen des Lemma ""billig"" Bibliographie Geyken 2007: Alexander Geyken: The DWDS corpus 'A reference corpus for the German language of the 20th century. In: Fellbaum, Christiane (Hg.): Idioms and Collocations: Corpus_based Linguistic, Lexicographic Studies. London: Continuum Press, 2007, S. 23–40. Geyken et al. 2011: Alexander Geyken, Susanne Haaf, Bryan Jurish, Matthias Schulz, Jakob Steinmann, Christian Thomas, Frank Wiegand: Das Deutsche Textarchiv: Vom historischen Korpus zum aktiven Archiv. In: Digitale Wissenschaft. Stand und Entwicklung digital vernetzter Forschung in Deutschland, 20./21. September 2010. Beiträge der Tagung. Hrsg. von Silke Schomburg, Claus Leggewie, Henning Lobin und Cornelius Puschmann. 2., ergänzte Fassung. hbz, 2011, S. 157–161. http://www.hbz_nrw.de/dokumentencenter/veroeffentlichungen/Tagung_ Digitale Wisse _ nschaft.pdf#page=159 Geyken et al. 2012: Alexander Geyken, Susanne Haaf, Frank Wiegand: The DTA ‚Äòbase format‚Äô: A TEI_Subset for the Compilation of Interoperable Corpora. In: 11th Conference on Natural Language Processing (KONVENS) 'Empirical Methods in Natural Language Processing, Proceedings of the Conference. Hrsg. von Jeremy Jancsary. Wien, 2012 (= Schriftenreihe der √ñsterreichischen Gesellschaft für Artificial Intelligence 5). http://www.oegai.at/konvens2012/proceedings/57 _geyken12w/57 _geyken12w.pdf Geyken 2013: Alexander Geyken: Wege zu einem historischen Referenzkorpus des Deutschen: das Projekt Deutsches Textarchiv. In: Perspektiven einer corpusbasierten historischen Linguistik und Philologie. Internationale Tagung des Akademienvorhabens ""Altägyptisches Wörterbuch"" an der Berlin_Brandenburgischen Akademie der Wissenschaften, 12. –13. Dezember 2011. Hrsg. von Ingelore Hafemann, Berlin 2013, S. 221–234. urn:nbn:de:kobv:b4_opus_24424 Haaf et al., forthcoming: Susanne Haaf, Alexander Geyken, Frank Wiegand: The DTA ‚ÄòBase Format‚Äô: A TEI Subset for the Compilation of a Large Reference Corpus of Printed Text from Multiple Sources. To appear in: Journal of the Text Encoding Initiative (jTEI), Issue 8. [Abstract des korrespondierenden Vortrags im Rahmen der Veranstaltung: TEI Conference and Members Meeting, 2. –5. Oktober 2013, Sapienza, Universit√† di Roma (IT): http://digilab2.let.uniroma1.it/teiconf2013/program/papers/abstracts_paper#C137] Jurish 2013: Bryan Jurish: Canonicalizing the Deutsches Textarchiv. In: Perspektiven einer corpusbasierten historischen Linguistik und Philologie. Internationale Tagung des Akademienvorhabens ""Altägyptisches Wörterbuch"" an der Berlin_Brandenburgischen Akademie der Wissenschaften, 12. –13. Dezember 2011. Hrsg. von Ingelore Hafemann, Berlin 2013, S. 235–244. urn:nbn:de:kobv:b4_opus_24433 Jurish et al. 2014: Bryan Jurish, Christian Thomas, Frank Wiegand: Querying the Deutsches Textarchiv. In: Proceedings of the Workshop MindTheGap 2014: Beyond Single_Shot Text Queries: Bridging the Gap(s) between Research Communities (co_located with iConference 2014, Berlin, Germany, 4th March, 2014). Hrsg. von Udo Kruschwitz, Frank Hopfgartner, Cathal Gurrin, Berlin 2014, S. 25–30. http://ceur_ws.org/Vol_1131/mindthegap14 _ 7.pdf Münter 1772: Balthasar Münter: Bekehrungsgeschichte des vormaligen Grafen [...] Johann Friederich Struensee. Kopenhagen, 1772. In: Deutsches Textarchiv, www.deutschestextarchiv.de/muenter bekehren _ _ 1772, abgerufen am 07.11.2014.",de,Einführung Beitrag referenzkorpora deutsch Sprache verwenden frequenznormiert verlaufskurven Wörter Wortverbindunge berechnen Jhs Referenzkorpus deutsch Textarchivs Korpora bezüglich metadaten vereinheitlichen linguistisch Information annotiert korpusübergreifend Abfrag stellen Beispiel hierfür schreibweisentolerant lemmasuch textsortenspezifisch suchen Grundlage generiert Verlaufskurv stehen Website deutsch Textarchivs Abfrage Verfügung Korpusgrundlage Annotation Grundlage zeitlich Verlaufskurv histogramm bilden Million Textwörter Jhs Million Textwörter deutsch Textarchivs Werk jh erstausgaben umfassen Korpora hinsichtlich repräsentiert Textsort Belletristik Gebrauchsliteratur Wissenschaft Dwds journalistisch Prosa hinsichtlich enthalten disziplin Ausgewog geyken geyken et gemäß tei annotiert geyken et Haaf et forthcoming bezüglich metadaten untereinander interoperabel insbesondere bezüglich Histogramme verwendet Angabe Datum Textsort Korpora Auswertung linguistisch annotiert insbesondere Text tokenisieren lemmatisiern lexikalisch Kategori analysieren versehen besonderer Bedeutung Generierung histogramm historisch Text cab historisch Variant Wortform wahrscheinlichster Normalform reduzieren neuhochdeutsch Lemma zuordnen Jurish Suche Korpora Darstellung Wortverläuf schreibweisenübergreifend geschehen Suchwort extensional all derjenige Wortforme expandieren möglicherweise flektiert Variante Suchwort darstellen folgend abschnitten zeigen Vorgenerierung möglich Form Benutzer effektiv Hilfe darstellen abschneten Beispiel Korpora Suchmaschin ddc dialing Dwds Concordancer indizieren Jurish et ddc verfügen reichhaltig Metadatenfilter Möglichkeit mehrere annotatio Wortposition indizieren abfragbar hinaus regulär ausdrücke boolesch Verknüpfung abstandsoperatoren nutzen insbesondere metadatenfilt mehrfach linguistisch annotationen miteinander verbinden Indizes ddc optimiert Abfrag zeitlich Verteilung histogramme ausreichend schnell dynamisch Berechnung Laufzeit zeitlich Verläuf gesamt mächtigkeit dynamisch berechnen Visualisierung Grundlage Visualisierung messpunkten Jahreszahl Datumsintervall normiert Relative Häufigkeite pro Million Textwörter Korpora weder verteilen zudem Wortanzahl zeitintervall variieren Normierung notwendig histogramme beide Korpora einheitlich präsentieren vorig Abschnitt erwähnen normiert relativ Frequenz einzeln Wortforme Lemmata Kombination linguistisch annotation Phrasensuch ausgeben hinaus histogramm textsortenspezifisch Belletristik Wissenschaft textsortenübergreifend bilden Häufigkeit Wortform Types Textkorpora zipfsch verteilen aufgrund Größe Referenzkorpora Wortforme mittlerer Häufigkeit rechnen histogramme einzeln Messpunkt nullstell aufweisen umgekehrt unausgewogenheit Textkorpora Messpunkt Ausschläg hoch Frequenz führen Grund visualisierungskomponent verschieden Parameter Glättung implementieren Platzgründe wichtig eingehen parameter Window Pruning parameter Window Fensterbreite natürlich Zahl Glättung gleitend Mittelwert parameter pruning implementieren zweistufig Verfahren Schritt Fehlerverteilung normiert datenpunkte berechnen beobachtet Fehler Annahme Normalverteilung überführen Datenpunkt außerhalb angegeben Konfidenzbereich Ausreißer behandeln datenpunkte Ausreisser entsprechen linear interpolation nächstliegend datenpunken innerhalb Konfidenzbereich liegen ersetzen Visualisierung erfolgen mittels Highcharts Website dta abfragbar Beispiel Ergebnis Diskussion folgend abschließend abschnitt vorzüge Visualisierung Grundlage beschrieben Referenzkopora linguistisch Annotation anhand konkret Beispiel illustrieren abfragemöglichkeit Ergebnis entsprechend Ergebnis Google ngram viewer Beziehung setzen tatsachen dienen Lemma Tatsache Begriff laut etymologisch Wörterbuch Pfeifer Publikation Eingang deutsch Sprache finden Wortverlaufskurve abb bestätigen Beginn wortkarriere zeichnen Mitte Jhs jh Verlaufskurve Google Ngram viewer abb zeigen vergleichbar Tendenz Vergleich beide histogramme lässt dennoch Problem Arbeit Ngram Viewer sichtbar historisch Schreibweise thatsach etlicher variann idealerweise möglich Flexionsform expansionen Eingabe Anfrage explizit ergänzen historisch Entwicklung Begriff visualisieren Punkt flexionsformen expansionen billig nah eingehen erklärbar Ausschlag Kurve Interesse Spalding Wort deutsch Sprache bringen deutlich zeigen Problem Recherche dokumenten zeigen sämtlich früh Treffern falsch datiert Dokument handeln prominent Beispiel jh goeth Schrift Morphologie ii datiern fliegend blätter Google Books datiern wortverlaufskurv Metadatenfehler verfälschen etymologisch Wörterbuch deutsch Pfeifer digital Version via Dwds nachgebilden theolog Spalding Matter of fact sehen Bestätigung natürlich geoffenbart Religion Leipzig johann Joachim Spalding Übersetzung Joseph Butlers the Analogy -- Religion natural -- Revealed to -- constitution and course -- nature Texterfassung Werks Dta Arbeit derzeit stammen Früheste Beleg münt Platzgründe Abfall Kurve Mitte jh eingehen Hinsicht einfach erscheinend Begriff Tatsache immerhin Expansione belegen merkwürdig illustrieren Möglichkeit Textsortendifferenzierung Interpretation histogramme bieten Begriff merkwürdig pfeifers etymologisch Wörterbuch zufolge jh vorrangig Bedeutung seltsam verwunderlich verwenden dominieren jh gebräuchlich Verwendung Sinn bemerkenswert bedeutsam abb zeigen textsort differenzierte Histogramm belegen relativ hochfrequent Verwendung merkwürdig textsort Wissenschaft Gebrauchsliteratur Mitte Jhs hinaus folgen These Dta verfügbar Beleg bestätigen Verwendung Wissenschaft Gebrauchsliteratur vorrangig Sinn bemerkenswert bedeutsam geschehen legen lang andauernd Gebrauch Wort Bedeutung nahe Pfeifer angeben zeigen Vorteil Verlaufskurve Google ngram Viewer abb Textsortendifferenzierung fehlen billig abschließend billig illustrieren Vorzüg Referenzkorpora beruhend histogrammen Google ngram viewer gut Abdeckung bezüglich Jhs gut Handhabung graphematisch Varianz historisch Text abb zeigen Histogramm billig Dta Dwds Kurve veranschaulichen breit Verwendung Begriff jh dazugehörig Beleg zeigen Bedeutungsspektrum äòangemessen rechtfertigen äô äòmäßig wohlfeil günstig äô Kurve Google Ngram viewer abb zeigen kontinuierlich Verlauf jh angesichts belegt Verbreitung Begriff verhältnismäßig gering Substanz Zeitraum belegen erscheinen Grundlage quellenbasiert Forschung zumindest Zeitraum angesprochen Problem erschwerend hinzu insbesondere Abfrage Zeiträume liefern aufgrund heterogen Graphie schlichtweg befriedigend Anzahl belegen liefern klar Vorteil Hinsicht relativ unproblematisch erscheinend Lemma billig abb Gezeigt immerhin Flexion Expansionsforme belegen Abfrage via dta Dwds Form berücksichtigen Nutzer Google ngram Viewer manuell eingeben Zweck selbstverständlich präsent müsste Ausblick vorig Abschnitt zeigen liefern zeitverlaufskurve Grundlage referenzkorpora interessant Ergebnis wesentlich nb überraschend zudem Graphie merkwirdig merckwirdig Kurve zugrundeliegend Korpus German identisch abfragbar aktuell belegen groß groß Rechercheaufwand ermittelbar sein Grund verlässlich Metadat Zuordnung Textsort aufgrund genau Texterfassung möglich Erschließungsmethode rechnen Lage referenzkorpora basiert zeitlich Verlaufskurve künftig verbessern liegen historisch Volltexte hoch Qualität entstehen Bemühung verstärken Dfg unlängst spezifisch auflegen Dta dynamisch verändernd korpusgrundlagen geeignet technisch Lösung korpus dta automatisch Wochenrhythmus indizieren abbildungen abb Tatsache url abb Tatsache thatsach gewählt Parameter Kurvengenerierung sehen jeweils angegeben url Mooth abb merkwürdig url abb merkwürdig merckwürdig merckwirdig merkwirdig abb billig url abb billig abb Dta belegen Expansion Flexionsforme Lemma billig bibliographie geyken Alexander geyken The Dwds Corpus Reference Corpus for The German Language -- the Century fellbaum Christiane hg idioms and collocations Linguistic Lexicographic studies London Continuum press geyken et Alexander geyken susanne Haaf Bryan Jurish Matthias Schulz Jakob Steinmann Christian Thomas Frank Wiegand deutsch Textarchiv historisch Korpus aktiv Archiv digital Wissenschaft Stand Entwicklung digital vernetzt Forschung Deutschland September beiträge Tagung Silke Schomburg Claus Leggewie Henning Lobin Cornelius Puschmann ergänzen Fassung hbz digital wisse geyken et Alexander geyken susanne Haaf Frank Wiegand The dta äòbase Format äô for the compilation -- interoperabel Corpora conference -- natural language Processing konvens Empirical Methods Natural Language Processing Proceedings -- -- conference Jeremy Jancsary Wien Schriftenreihe Gesellschaft artificial intelligence geyken Alexander geyken Weg historisch Referenzkorpus deutsche Projekt deutsch Textarchiv Perspektive corpusbasierten historisch Linguistik Philologie international Tagung Akademienvorhaben altägyptisch Wörterbuch Akademie Wissenschaft Dezember Ingelore Hafemann Berlin urn nbn de kobv Haaf et forthcoming Susanne Haaf Alexander geyken Frank Wiegand The dta äòbase Format äô Tei subset for the compilation -- large Reference Corpus of Printed Text from Multiple Source to Appear journal -- -- Text Encoding Initiative Jtei issue abstract korrespondierend Vortrag Rahmen Veranstaltung tei conference and Member Meeting Oktober sapienza di roma -- Jurish Bryan Jurish canonicalizing -- deutsch Textarchiv Perspektive corpusbasierten historisch Linguistik Philologie international Tagung Akademienvorhaben altägyptisch Wörterbuch Akademie Wissenschaft Dezember Ingelore Hafemann Berlin urn nbn de Kobv Jurish et Bryan Jurish Christian Thomas Frank Wiegand Querying The deutsch Textarchiv proceedings -- -- workshop mindthegap Beyond Text queries bridging The gap -- between research communities with iconference Berlin Germany march udo Kruschwitz Frank Hopfgartner Cathal Gurrin Berlin münt Balthasar münt Bekehrungsgeschicht vormalig graf johann friederich struensee Kopenhagen deutsch Textarchiv bekehren abgerufen,"[('geyken', 0.24910749814076452), ('billig', 0.18998339955513202), ('jh', 0.18005913321924472), ('dta', 0.1680775903574435), ('ngram', 0.15755174156683913), ('abb', 0.15161755705257607), ('viewer', 0.14531270724877932), ('histogramme', 0.14248754966634902), ('jurish', 0.14065964235409353), ('dwds', 0.13504434991443354)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Zeitliche Verlaufskurven in den DTA- und DWDS-Korpora: Wörter und Wortverbindungen über 400 Jahre (1600‚-2000),Alexander Geyken (Berlin-Brandenburgische Akademie der Wissenschaften); Matthias Boenig (Berlin-Brandenburgische Akademie der Wissenschaften); Susanne Haaf (Berlin-Brandenburgische Akademie der Wissenschaften); Bryan Jurish (Berlin-Brandenburgische Akademie der Wissenschaften); Christian Thomas (Berlin-Brandenburgische Akademie der Wissenschaften); Kay-Michael Würzner (Berlin-Brandenburgische Akademie der Wissenschaften); Frank Wiegand (Berlin-Brandenburgische Akademie der Wissenschaften),"Korpora, Referenzkorpora, GermaNet","Korpora, Referenzkorpora, GermaNet","Einführung In diesem Beitrag werden zwei Referenzkorpora der deutschen Sprache verwendet, um daraus frequenznormierte Verlaufskurven für Wörter und Wortverbindungen zu berechnen: das DWDS_Kernkorpus des 20. Jhs. sowie das Referenzkorpus des Deutschen Textarchivs (1600–1900). Da beide Korpora bezüglich ihrer Metadaten vereinheitlicht und auch mit denselben linguistischen Informationen annotiert wurden, können korpusübergreifende Abfragen gestellt werden. Beispiele hierfür sind schreibweisentolerante Lemmasuchen oder textsortenspezifische Suchen. Die auf dieser Grundlage generierten Verlaufskurven stehen auf der Website des Deutschen Textarchivs für die Abfrage zur Verfügung. 1 Korpusgrundlage und Annotation Die Grundlage für die zeitlichen Verlaufskurven (Histogramme) bilden die 100 Millionen Textwörter des DWDS_Kernkorpus des 20. Jhs. sowie weitere 140 Millionen Textwörter des Deutschen Textarchivs, welches Werke des 17. bis 19. Jh. als Erstausgaben umfasst. Beide Korpora sind hinsichtlich der repräsentierten Textsorten aus Belletristik, Gebrauchsliteratur, Wissenschaft und (im DWDS:) Journalistischer Prosa sowie hinsichtlich der enthaltenen Disziplinen ausgewogen (Geyken 2007, 2013; Geyken et al. 2011). Sie wurden beide gemäß den TEI/P5_Richtlinien annotiert (Geyken et al. 2012; Haaf et al., forthcoming) und sind bezüglich der Metadaten untereinander interoperabel, insbesondere bezüglich der in den Histogrammen verwendeten Angaben zum Datum und zu den Textsorten. Beide Korpora wurden für die Auswertung linguistisch annotiert, insbesondere wurden alle Texte tokenisiert, lemmatisiert, nach lexikalischen Kategorien analysiert (PoS_Tagging) und mit GermaNet_Kategorien versehen. Von besonderer Bedeutung für die Generierung der Histogramme ist die CAB_Analyse der historischen Texte: Mit CAB werden historische Varianten einer Wortform auf die wahrscheinlichste Normalform reduziert und ihrem neuhochdeutschen Lemma zugeordnet (Jurish 2013). Damit ist nicht nur die Suche in den Korpora, sondern auch die Darstellung der Wortverläufe schreibweisenübergreifend möglich. Dies geschieht, indem das Suchwort extensional zu all denjenigen Wortformen expandiert wird, die eine (möglicherweise flektierte) Variante des Suchwortes darstellen. In den folgenden Abschnitten wird gezeigt, dass diese Vorgenerierung der möglichen Formen für den Benutzer eine effektive Hilfe darstellt (s. Abschnitt 3: Beispiele). Beide Korpora, das DWDS_Kernkorpus und das DTA_Korpus, sind mit der Suchmaschine DDC (Dialing DWDS Concordancer) indiziert (Jurish et al. 2014). DDC verfügt über reichhaltige Metadatenfilter sowie die Möglichkeit, mehrere Annotationen auf einer Wortposition zu indizieren und abfragbar zu machen. Darüber hinaus können reguläre Ausdrücke, Boolesche Verknüpfungen und Abstandsoperatoren genutzt werden. Insbesondere ist es möglich, Metadatenfilter und mehrfache linguistische Annotationen miteinander zu verbinden. Die Indizes von DDC wurden so optimiert, dass Abfragen über die zeitliche Verteilung (Histogramme) ausreichend schnell für eine dynamische Berechnung zur Laufzeit sind. Dadurch ist es auch möglich, zeitliche Verläufe für die gesamte Mächtigkeit der DDC_Abfragesprache dynamisch zu berechnen. 2 Visualisierung Grundlage der Visualisierung sind die nach Messpunkten (Jahreszahlen bzw. Datumsintervallen) normierten relativen Häufigkeiten pro Million Textwörter. Da die beiden Korpora weder gleich verteilt noch gleich groß sind und zudem die Wortanzahl je Zeitintervall variiert, ist die Normierung notwendig, um die Histogramme beider Korpora einheitlich zu präsentieren. Wie im vorigen Abschnitt erwähnt, können normierte relative Frequenzen nicht nur für einzelne Wortformen oder Lemmata, sondern auch für Kombinationen aus linguistischen Annotationen und Phrasensuchen ausgegeben werden. Darüber hinaus können Histogramme textsortenspezifisch (z. B. nur für Belletristik oder nur für Wissenschaft) oder textsortenübergreifend gebildet werden. Da die Häufigkeiten von Wortformen (Types) in Textkorpora zipfsch verteilt sind, ist aufgrund der Größe der beiden Referenzkorpora bereits bei Wortformen einer mittleren Häufigkeit damit zu rechnen, dass die Histogramme bei einzelnen Messpunkten Nullstellen aufweisen können. Umgekehrt kann auch die Unausgewogenheit der Textkorpora an einem Messpunkt zu ""Ausschlägen"" mit zu hoher Frequenz führen. Aus diesem Grund wurden in die Visualisierungskomponente verschiedene Parameter zur Glättung implementiert. Aus Platzgründen soll hier nur auf die beiden wichtigsten eingegangen werden: die Parameter ""window"" und ""pruning"" . Der Parameter ""window"" gibt die Fensterbreite (als natürliche Zahl) für die Glättung nach dem gleitenden Mittelwert an. Der Parameter ""pruning"" implementiert ein zweistufiges Verfahren. Im ersten Schritt wird eine Fehlerverteilung für die normierten Datenpunkte berechnet. Die beobachteten ""Fehler"" werden unter Annahme einer Normalverteilung in p_Werte überführt, und alle Datenpunkte mit p_Werten außerhalb des angegebenen Konfidenzbereichs (p=0.05) werden als Ausreißer behandelt. Datenpunkte, die Ausreissern entsprechen, werden durch eine lineare Interpolation der nächstliegenden Datenpunkte, die innerhalb des Konfidenzbereichs liegen, ersetzt. Die Visualisierung selbst erfolgt mittels der Javascript_Bibliothek Highcharts 1 Website des DTA abfragbar. 2 und ist auf der 3 Beispiele, Ergebnisse und Diskussion Im folgenden, abschließenden Abschnitt sollen einige Vorzüge der Visualisierung auf der Grundlage der oben beschriebenen Referenzkopora und ihrer linguistischen Annotationen anhand konkreter Beispiele illustriert werden. Die Abfragemöglichkeiten und Ergebnisse werden mit den entsprechenden Ergebnissen im Google Ngram Viewer 3 in Beziehung gesetzt. Beispiel 1: Tatsache Als erstes Beispiel dient das Lemma ""Tatsache"" , ein Begriff, der laut dem Etymologischen Wörterbuch (Pfeifer) erst mit einer Publikation aus dem Jahr 1756 Eingang in die deutsche Sprache fand. 4 Die Wortverlaufskurve in Abb. _ 1 bestätigt den Beginn dieser ""Wortkarriere"" und zeichnet ihn von der Mitte des 18. Jhs. bis in das 20. Jh. nach. 5 Die Verlaufskurve im Google Ngram Viewer, Abb. _ 2, zeigt eine vergleichbare Tendenz. Der Vergleich beider Histogramme lässt dennoch Probleme bei der Arbeit mit dem Ngram Viewer sichtbar werden: Historische Schreibweisen wie ""Thatsache"" 'sowie ggf. etliche weitere möglich Varianten 6 und idealerweise auch alle möglichen Flexionsformen bzw. Expansionen 'müssen bei der Eingabe der Anfrage explizit ergänzt werden, um die historische Entwicklung des Begriffs zu visualisieren. Auf den Punkt Flexionsformen/Expansionen wird im dritten Beispiel (""billig"") noch näher eingegangen; hier sind zunächst nicht erklärbare Ausschläge der Kurve vor 1756 von Interesse, als Spalding das Wort in die deutsche Sprache brachte. Besonders deutlich zeigt sich darin ein Problem des Google_Books_Korpus. Eine Recherche in den Dokumenten daselbst zeigt, dass es sich bei sämtlichen früheren Treffern um falsch datierte Dokumente handelt, darunter so prominente Beispiele aus dem 19. Jh. wie Goethes Schriften zur Morphologie (II. Teil, datiert auf 1659) und die Fliegenden Blätter (in Google Books datiert auf 1692). Die Wortverlaufskurven werden hier also durch Metadatenfehler verfälscht. 1 http://www.deutschestextarchiv.de/search/plot. 2 http://www.highcharts.com/products/highcharts. 3 https://books.google.com/ngrams. 4 Vgl. z. B. das Etymologische Wörterbuch des Deutschen (nach W. Pfeifer), digitale Version via DWDS: ""[...] nachgebildet (1756) von dem Theologen Spalding für engl. matter of fact [...]. "" Siehe Bestätigung der natürlichen und geoffenbarten Religion, Leipzig, 1756, sowie Johann Joachim Spaldings Übersetzung von Joseph Butlers The analogy of religion, natural and revealed, to the constitution and course of nature, 1736. Die Texterfassung dieses Werks ist für das DTA in Arbeit; derzeit stammt der früheste Beleg im DTA_Korpus aus Münter 1772. 5 Aus Platzgründen kann hier auf den Abfall der Kurve ab Mitte des 20. Jh. nicht eingegangen werden. 6 Im DTA_Korpus sind für den in dieser Hinsicht einfach erscheinenden Begriff ""Tatsache"" immerhin 15 Expansionen belegt, vgl. http://kaskade.dwds.de/dstar/dta/lizard.perl?q=Tatsache. Beispiel 2: merkwürdig Das zweite Beispiel illustriert die Möglichkeiten, die eine Textsortendifferenzierung für die Interpretation der Histogramme bietet. Der Begriff ""merkwürdig"" wurde Pfeifers Etymologischem Wörterbuch zufolge ab dem 19. Jh. vorrangig in der Bedeutung ""seltsam, verwunderlich"" verwendet; bis dahin dominierte die seit dem 17. Jh. gebräuchliche Verwendung im Sinne von ""bemerkenswert, bedeutsam"" . Abb. _ 3 zeigt das nach Textsorten differenzierte Histogramm. Belegt ist die relativ hochfrequente Verwendung von ""merkwürdig"" in den Textsorten Wissenschaft und Gebrauchsliteratur bis über die Mitte des 19. Jhs. hinaus. Folgt man der These (wie die im DTA verfügbaren Belege bestätigen), dass die Verwendung in der Wissenschaft und der Gebrauchsliteratur vorrangig im Sinne von ""bemerkenswert, bedeutsam"" geschah, legt dies einen etwas länger andauernden Gebrauch des Wortes in dieser Bedeutung nahe, als bei Pfeifer angegeben. Auch hier zeigt sich der Vorteil gegenüber der Verlaufskurve des Google Ngram Viewers (Abb. _ 4), wo die Textsortendifferenzierung fehlt. 7 Beispiel 3: billig Das abschließende Beispiel (""billig"") illustriert zwei weitere Vorzüge der auf den Referenzkorpora beruhenden Histogramme gegenüber dem Google Ngram Viewer: die bessere Abdeckung der DTA_Korpora bezüglich des 17. Jhs. und die bessere Handhabung der großen graphematischen Varianz v. a. in diesen historischen Texten. Abb. _ 5 zeigt das Histogramm für ""billig"" aus DTA und DWDS; die Kurve veranschaulicht die breite Verwendung des Begriffs im 17. Jh., die dazugehörigen Belege zeigen das Bedeutungsspektrum zwischen ‚Äòangemessen, gerechtfertigt‚Äô und ‚Äòmäßig, wohlfeil, günstig‚Äô . Die Kurve aus dem Google Ngram Viewer, Abb. _ 6, zeigt dagegen keinen kontinuierlichen Verlauf im 17. Jh., was angesichts der u. a. im DTA_Korpus belegten Verbreitung des Begriffs die verhältnismäßig geringe Substanz des Google_Books_Korpus für diesen Zeitraum belegt. Daher erscheint auf dessen Grundlage quellenbasierte Forschung zumindest in diesem Zeitraum kaum möglich. Ein weiteres, bereits angesprochenes Problem, kommt erschwerend hinzu: Insbesondere bei Abfragen für Zeiträume vor 1700 liefert das Google_Books_Korpus aufgrund der sehr heterogenen Graphie schlichtweg keine befriedigende Anzahl von Belegen; hier liefert die CAB_Analyse der DTA_Korpora klare Vorteile. Allein für das in dieser Hinsicht relativ unproblematisch erscheinende Lemma ""billig"" sind im DTA_Korpus die in Abb. _ 7 gezeigten immerhin 67 Flexions_ und Expansionsformen belegt. Bei einer Abfrage via DTA/DWDS werden alle diese Formen berücksichtigt, während der Nutzer des Google Ngram Viewers sie manuell eingeben (und zu diesem Zweck selbstverständlich überhaupt erst einmal präsent haben) müsste. 4 Ausblick Wie im vorigen Abschnitt gezeigt werden konnte, liefert die Zeitverlaufskurve auf der Grundlage der beiden Referenzkorpora interessante Ergebnisse, die mit dem wesentlich 7 NB: Überraschend ist zudem, dass die Graphien ""merkwirdig"" und ""merckwirdig"" in dem der Kurve zugrundeliegenden Korpus German (das nicht identisch mit dem abfragbaren aktuellen GB_Korpus ist) nicht belegt sind. größeren Google_Books_Korpus entweder nicht oder nur mit großem Rechercheaufwand ermittelbar gewesen wären. Grund dafür sind die verlässlichen Metadaten, die Zuordnung nach Textsorten sowie die aufgrund der genauen Texterfassung möglichen Erschließungsmethoden (CAB_Software). Es ist damit zu rechnen, dass sich die Lage der auf Referenzkorpora basierten zeitlichen Verlaufskurven künftig weiter verbessern wird. Zum einen liegt dies daran, dass immer mehr historische Volltexte in hoher Qualität entstehen. Diese Bemühungen werden dadurch verstärkt, dass die DFG erst unlängst eine spezifische OCR_Förderlinie aufgelegt hat. Zum anderen hat das DTA für die sich dynamisch verändernden Korpusgrundlagen bereits die geeigneten technischen Lösungen: Das Korpus des DTA wird automatisch im Wochenrhythmus indiziert. Abbildungen 8 DHd2025",de,Einführung Beitrag referenzkorpora deutsch Sprache verwenden frequenznormiert verlaufskurven Wörter Wortverbindunge berechnen Jhs Referenzkorpus deutsch Textarchivs Korpora bezüglich metadaten vereinheitlichen linguistisch Information annotiert korpusübergreifend Abfrag stellen Beispiel hierfür schreibweisentolerant lemmasuch textsortenspezifisch suchen Grundlage generiert Verlaufskurv stehen Website deutsch Textarchivs Abfrage Verfügung Korpusgrundlage Annotation Grundlage zeitlich Verlaufskurv histogramm bilden Million Textwörter Jhs Million Textwörter deutsch Textarchivs Werk jh erstausgaben umfassen Korpora hinsichtlich repräsentiert Textsort Belletristik Gebrauchsliteratur Wissenschaft Dwds journalistisch Prosa hinsichtlich enthalten disziplin Ausgewog geyken geyken et gemäß tei annotiert geyken et Haaf et forthcoming bezüglich metadaten untereinander interoperabel insbesondere bezüglich Histogramme verwendet Angabe Datum Textsort Korpora Auswertung linguistisch annotiert insbesondere Text tokenisieren lemmatisiern lexikalisch Kategori analysieren versehen besonderer Bedeutung Generierung histogramm historisch Text cab historisch Variant Wortform wahrscheinlichster Normalform reduzieren neuhochdeutsch Lemma zuordnen Jurish Suche Korpora Darstellung Wortverläuf schreibweisenübergreifend geschehen Suchwort extensional all derjenige Wortforme expandieren möglicherweise flektiert Variante Suchwort darstellen folgend abschnitten zeigen Vorgenerierung möglich Form Benutzer effektiv Hilfe darstellen abschneten Beispiel Korpora Suchmaschin ddc dialing Dwds Concordancer indizieren Jurish et ddc verfügen reichhaltig Metadatenfilter Möglichkeit mehrere annotatio Wortposition indizieren abfragbar hinaus regulär ausdrücke boolesch Verknüpfung abstandsoperatoren nutzen insbesondere metadatenfilt mehrfach linguistisch annotationen miteinander verbinden Indizes ddc optimiert Abfrag zeitlich Verteilung histogramme ausreichend schnell dynamisch Berechnung Laufzeit zeitlich Verläuf gesamt mächtigkeit dynamisch berechnen Visualisierung Grundlage Visualisierung messpunkten Jahreszahl Datumsintervall normiert Relative Häufigkeite pro Million Textwörter Korpora weder verteilen zudem Wortanzahl zeitintervall variieren Normierung notwendig histogramme beide Korpora einheitlich präsentieren vorig Abschnitt erwähnen normiert relativ Frequenz einzeln Wortforme Lemmata Kombination linguistisch annotation Phrasensuch ausgeben hinaus histogramm textsortenspezifisch Belletristik Wissenschaft textsortenübergreifend bilden Häufigkeit Wortform Types Textkorpora zipfsch verteilen aufgrund Größe Referenzkorpora Wortforme mittlerer Häufigkeit rechnen histogramme einzeln Messpunkt nullstell aufweisen umgekehrt unausgewogenheit Textkorpora Messpunkt Ausschläg hoch Frequenz führen Grund visualisierungskomponent verschieden Parameter Glättung implementieren Platzgründe wichtig eingehen parameter Window Pruning parameter Window Fensterbreite natürlich Zahl Glättung gleitend Mittelwert parameter pruning implementieren zweistufig Verfahren Schritt Fehlerverteilung normiert datenpunkte berechnen beobachtet Fehler Annahme Normalverteilung überführen Datenpunkt außerhalb angegeben Konfidenzbereich Ausreißer behandeln datenpunkte Ausreisser entsprechen linear interpolation nächstliegend datenpunken innerhalb Konfidenzbereich liegen ersetzen Visualisierung erfolgen mittels Highcharts Website dta abfragbar Beispiel Ergebnis Diskussion folgend abschließend abschnitt vorzüge Visualisierung Grundlage beschrieben Referenzkopora linguistisch Annotation anhand konkret Beispiel illustrieren abfragemöglichkeit Ergebnis entsprechend Ergebnis Google ngram viewer Beziehung setzen tatsachen dienen Lemma Tatsache Begriff laut etymologisch Wörterbuch Pfeifer Publikation Eingang deutsch Sprache finden Wortverlaufskurve abb bestätigen Beginn wortkarriere zeichnen Mitte Jhs jh Verlaufskurve Google Ngram viewer abb zeigen vergleichbar Tendenz Vergleich beide histogramme lässt dennoch Problem Arbeit Ngram Viewer sichtbar historisch Schreibweise thatsach etlicher variann idealerweise möglich Flexionsform expansionen Eingabe Anfrage explizit ergänzen historisch Entwicklung Begriff visualisieren Punkt flexionsformen expansionen billig nah eingehen erklärbar Ausschlag Kurve Interesse Spalding Wort deutsch Sprache bringen deutlich zeigen Problem Recherche dokumenten zeigen sämtlich früh Treffern falsch datiert Dokument handeln prominent Beispiel jh goeth Schrift Morphologie ii datiern fliegend blätter Google Books datiern wortverlaufskurv Metadatenfehler verfälschen etymologisch Wörterbuch deutsch Pfeifer digital Version via Dwds nachgebilden theolog Spalding Matter of fact sehen Bestätigung natürlich geoffenbart Religion Leipzig johann Joachim Spalding Übersetzung Joseph Butlers the Analogy -- Religion natural -- Revealed to -- constitution and course -- nature Texterfassung Werks Dta Arbeit derzeit stammen Früheste Beleg münt Platzgründe Abfall Kurve Mitte jh eingehen Hinsicht einfach erscheinend Begriff Tatsache immerhin Expansione belegen merkwürdig illustrieren Möglichkeit Textsortendifferenzierung Interpretation histogramme bieten Begriff merkwürdig pfeifers etymologisch Wörterbuch zufolge jh vorrangig Bedeutung seltsam verwunderlich verwenden dominieren jh gebräuchlich Verwendung Sinn bemerkenswert bedeutsam abb zeigen textsort differenzierte Histogramm belegen relativ hochfrequent Verwendung merkwürdig textsort Wissenschaft Gebrauchsliteratur Mitte Jhs hinaus folgen These Dta verfügbar Beleg bestätigen Verwendung Wissenschaft Gebrauchsliteratur vorrangig Sinn bemerkenswert bedeutsam geschehen legen lang andauernd Gebrauch Wort Bedeutung nahe Pfeifer angeben zeigen Vorteil Verlaufskurve Google ngram Viewer abb Textsortendifferenzierung fehlen billig abschließend billig illustrieren Vorzüg Referenzkorpora beruhend histogrammen Google ngram viewer gut Abdeckung bezüglich Jhs gut Handhabung graphematisch Varianz historisch Text abb zeigen Histogramm billig Dta Dwds Kurve veranschaulichen breit Verwendung Begriff jh dazugehörig Beleg zeigen Bedeutungsspektrum äòangemessen rechtfertigen äô äòmäßig wohlfeil günstig äô Kurve Google Ngram viewer abb zeigen kontinuierlich Verlauf jh angesichts belegt Verbreitung Begriff verhältnismäßig gering Substanz Zeitraum belegen erscheinen Grundlage quellenbasiert Forschung zumindest Zeitraum angesprochen Problem erschwerend hinzu insbesondere Abfrage Zeiträume liefern aufgrund heterogen Graphie schlichtweg befriedigend Anzahl belegen liefern klar Vorteil Hinsicht relativ unproblematisch erscheinend Lemma billig abb Gezeigt immerhin Flexion Expansionsforme belegen Abfrage via dta Dwds Form berücksichtigen Nutzer Google ngram Viewer manuell eingeben Zweck selbstverständlich präsent müsste Ausblick vorig Abschnitt zeigen liefern zeitverlaufskurve Grundlage referenzkorpora interessant Ergebnis wesentlich nb überraschend zudem Graphie merkwirdig merckwirdig Kurve zugrundeliegend Korpus German identisch abfragbar aktuell belegen groß groß Rechercheaufwand ermittelbar sein Grund verlässlich Metadat Zuordnung Textsort aufgrund genau Texterfassung möglich Erschließungsmethode rechnen Lage referenzkorpora basiert zeitlich Verlaufskurve künftig verbessern liegen historisch Volltexte hoch Qualität entstehen Bemühung verstärken Dfg unlängst spezifisch auflegen Dta dynamisch verändernd korpusgrundlagen geeignet technisch Lösung korpus dta automatisch Wochenrhythmus indizieren abbildungen,"[('jh', 0.234103580354953), ('ngram', 0.20484063281058387), ('viewer', 0.18892807284920607), ('histogramme', 0.185254948952133), ('histogramm', 0.15437912412677746), ('billig', 0.15437912412677746), ('jhs', 0.15437912412677746), ('dta', 0.15296805823529103), ('google', 0.15296805823529103), ('dwds', 0.1463147377218456)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,WebLicht: Bombardieren bevor die Services explodieren,"Danièl de Kok (Universität Tübingen, Deutschland); Wie Qiu (Universität Tübingen, Deutschland); Marie Hinrichs (Universität Tübingen, Deutschland)","Annotation, Web-Analyse Software","Annotation, Web-Analyse Software","Einleitung Web-Analyse Software, die Informationen über das Nutzerverhalten sammelt und auswertet, kann eingesetzt werden, um Probleme zu verhindern, bevor diese entstehen. In diesem Abstrakt zeigen wir, wie wir eine derartige Software auf ein System mit verteilten Webservices, WebLicht [1], angewandt haben und wie wir die entstandene Analyse benutzt haben, um das System zu verbessern. WebLicht ist eine Webanwendung zur automatischen Annotation von Texten und multimodalen Daten. WebLicht nutzt eine serviceorientierte Architektur. Dies ermöglicht den CLARIN-Zentren Annotationswerkzeuge hinzuzufügen durch diese als Webservice zu implementieren und die CMDI-Metadaten [2] über diesen Service an ihr Repositorium zuzufügen. WebLicht aggregiert diese Metadaten und bietet dem Nutzer die Annotationswerkzeuge an. Wenn ein Benutzer mehrere Annotationswerkzeuge ausführen will oder ein Annotationswerkzeug vom Output eines anderen Werkzeugs abhängig ist, erlaubt WebLichts Verkettungsmechanismus es den Benutzern, Annotationswerkzeuge auf eine kompatible Art und Weise zu kombinieren. WebLicht ist eine voll entwickelte Anwendung, die mehr als hundert Annotationswerkzeuge anbietet. Dadurch weitet sich die Nutzung von WebLicht in zwei Dimensionen aus: (1) Die Anzahl der WebLicht-Benutzer steigt; (2) durchschnittlich lassen die Benutzer größere Datasets annotieren. Um diese Wachstumsstrukturen zu verstehen, sammeln wir Nutzungsstatistiken. Um WebLicht und die Annotationswerkzeuge für unsere Nutzerbasis zu optimieren, müssen wir zuerst die Nutzungsmuster der derzeitigen Benutzer kennen. Zweitens müssen wir zukünftige Kapazitätsengpässe vorhersagen können, damit wir uns mit ihnen auseinandersetzen können, bevor sie die User Experience beeinträchtigen. In den folgenden Abschnitten werden wir zuerst beschreiben, wie wir Benutzeraktivität und Nutzungsmuster messen. Dann werden wir die Simulation verschiedener Nutzungsszenarien behandeln. Schließlich werden wir einen kurzen Überblick über die Veränderungen geben, die wir im vergangenen Jahr aufgrund der Messungen vorgenommen haben. Die Messung von Benutzeraktivität Das Sammeln von Nutzerstatistiken für Webseiten ist ein wohlverstandenes Feld, in dem es mehrere konkurrierende und voll entwickelte Produkte, wie etwa Google Analytics, gibt. Es sind auch viele gute Open Source Webanalyse-Tools, wie zum 1 2 3 Beispiel Piwik , Webalizer and AWStats verfügbar. Diese Tools funktionieren gewöhnlich auf eine von zwei Arten: (1) Sie analysieren die vom Webserver protokollierten Logdateien; oder (2) sie benötigen einen Maintainer, der auf jede Seite einen Javascript-Snippet einfügt, wodurch der Browser des Benutzers mit der Analysesoftware Kontakt aufnimmt. Bereits existierende Lösungen sind nicht direkt auf WebLicht anwendbar, da ihre Verwendung nur zeigen würde, wie oft, aus welchem Land, etc. WebLicht besucht wurde. Die nützlichste Information würden fehlen: welche Annotationswerkzeuge verwenden die Benutzer, und wie oft? Piwik bietet eine Programmierschnittstelle (Application Programming Interface) an, durch die jedes Annotationswerkzeug seine eigene Nutzung registrieren kann. Jedoch würde dies erfordern, dass knapp hundert Dienste upgedatet werden müssten, um die Programmierschnittstelle aufrufen zu können. Ein weiterer Nachteil dieser Lösung ist, dass interessante Informationen, wie etwa das Land des Benutzers, nicht verfügbar sind, da die Annotationswerkzeuge von WebLicht aufgerufen werden und nicht direkt vom Browser des Benutzers. Wir haben die oben genannten Probleme durch das Melden von Statistiken im WebLicht-Verkettungsmechanismus gelöst. Da der Verkettungsmechanismus jedes Annotationswerkzeug aufruft, kann er gleichzeitig die Verwendung des Werkzeugs über die Piwik-Programmierschnittstelle melden. Dies macht die Statistiken sofort erhältlich für alle Annotationswerkzeuge, die in WebLicht verfügbar sind, ohne auch nur eine von ihnen zu verändern. Da der Verkettungsmechanismus in der WebLicht-Anwendung ausgeführt wird, hat es den Zusatznutzen, dass wir einige Metadaten bereitstellen können, die es Piwik erlauben das Land des Besuchers, den Webbrowser, etc. zu ermitteln. 1 http://piwik.org/ 2 http://www.webalizer.org/ 3 http://www.awstats.org/ Abb. 1: Piwik Nutzerstatistiken für die Stuttgarter Werkzeuge Die Simulation von Nutzungsmustern Eines der Probleme, auf das wir mit der gestiegenen Nutzung von WebLicht gestoßen sind, ist, dass manche Annotationswerkzeuge nicht zur Bewältigung vieler Simultanbenutzer oder großen Inputs entwickelt wurden. Leider wurden diese Probleme oft erst entdeckt, wenn ein Benutzer eines der Annotationswerkzeuge nicht ausführen konnte. Durch das Simulieren von WebLicht-Nutzungsmustern konnten wir solche Probleme aufdecken, bevor die Benutzer ihnen begegnen. Zu diesem Zweck haben wir ein Simulationswerkzeug namens Bombard entwickelt. Bombard ermöglicht es den Entwicklern von WebLicht-Annotationswerkzeugen, Testfälle zu spezifizieren. Jeder Testfall besteht aus: der Kette von Annotationswerkzeugen, die getestet werden soll; dem Input; einem Intervall, das anzeigt, wie oft der Testfall ausgeführt werden soll; und die maximal erlaubte Bearbeitungsdauer. Eine Bombard-Konfigurierung kann aus vielen solcher Testfälle bestehen. Wenn Bombard gestartet wird, beginnt es jeden Testfall zu einem willkürlich gewählten Zeitpunkt und wiederholt den Testfall ab da in dem festgelegten Intervall. Während des ""Bombardements"" behält Bombard den Überblick über Fehlschläge und inakzeptable Bearbeitungsdauern. Danach können die Entwickler einen Bericht mit Statistiken für jedes Annotationswerkzeug abrufen. Im CLARIN-Zentrum in Tübingen wird Bombard zum Testen neuer Dienste genutzt, indem es das folgende Szenario simuliert: Zwei Gruppen von jeweils 40 Studierenden reichen innerhalb von zwei Minuten einen Text ein. In Gruppe A bearbeitet jeder Studierende zwei Textabschnitte aus Wikipedia, in Gruppe B bearbeitet jeder Studierende den Roman Alice im Wunderland (oder eine Übersetzung davon). Mithilfe dieses Testszenarios konnten wir Kapazitätsengpässe in früher entwickelten Annotationsdiensten bestimmen und sicherstellen, dass die neuen Dienste in der Lage sind, solche Szenarien zu verarbeiten. Da die Konfigurierung der Testfälle in Bombard anpassungsfähig ist, kann sie einfach auf andere Szenarien oder Annotationswerkzeuge, für die andere Erwartungen gelten, angewendet werden. Önderungen an WebLicht Wir haben unser Klassenzimmer-Szenario an Annotationsketten getestet, die unseren Feststellungen nach häufig genutzt werden, nämlich: Part-of-speech Tagging, Lemmatisierung, Konstituenz-Parsen, Dependenz-Parsen und Named-entity Recognition. Während der Simulation entdeckten wir die folgenden Probleme: (1) Manche Dienste versagten, wenn viele Instanzen des längeren Textes gesandt wurden; (2) manche versagten an dem längeren Text, da er relativ verrauscht ist; (3) bei manchen Diensten, insbesondere Parsern, kamen die Anfragen schneller herein als der Dienst sie verarbeiten konnte. Das erste Problem war am leichtesten zu lösen 'diese Dienste hatten eine ältere Version der TCF interchange format library genutzt, die sich nicht linear an die Größe des Inputs anpasste. Das zweite Problem musste von Fall zu Fall einzeln gelöst werden. Diese Dienste hatten einige Programmierfehler, die sie bei unerwartetem Input versagen ließen. Das dritte Problem jedoch war schwerer zu lösen und wird im Rest dieses Abschnitts besprochen werden. Parser für natürliche Sprachen sind oft langsam im Vergleich zu anderen Annotationsdiensten. Zum Beispiel können gebräuchliche Konstituenz-Parser normalerweise höchstens ein paar Sätze pro Sekunde parsen. Um die von uns vorhergesehenen Gebrauchsfälle bewältigen zu können, mussten wir die Fähigkeiten moderner Server und Computercluster, Prozesse parallel durchführen zu können, ausnutzen. Dafür haben wir ein Framework entwickelt [3], das auf einer verteilten Task-Warteschlange (Jesque) basiert und das folgende bietet: Parallele Prozessverarbeitung innerhalb der Anfragen, gleichzeitige Verarbeitung von Anfragen, Garantien, was die Verwendung von Ressourcen und Fairness betrifft (z.B.: Eine große Anfrage sollte keine sichtbaren Auswirkungen auf kleine Anfragen haben.) Wir benutzen dieses Framework in den upgedateten Diensten für die Malt, Stanford- und Berkeley-Parser. Dadurch können wir solche Szenarien leicht bewältigen. Was vielleicht noch wichtiger ist: Unser Framework erlaubt es uns, Webservices an noch größere simultane Benutzerzahlen oder größere Inputs anzupassen, indem wir weitere Prozessorkerne oder Geräte hinzufügen. Fazit Wir haben diesem Abstract zwei neue Vorgehensweisen zur Messung der Nutzung und Kapazität von WebLicht dargestellt. Zuerst haben wir Nutzungsmeldungen zum Verkettungswerkzeug hinzugefügt, sodass wir die Nutzungsstatistiken der Annotationswerkzeuge bekommen ohne die CLARIN-Partner darum bitten zu müssen, ihre Werkzeuge anzupassen. Zweitens haben wir das Dienstprogramm Bombard vorgestellt, das es uns ermöglicht die Auswirkung der Hochrechnung des momentanen Wachstums zu messen. Solche Messungen machen uns die Nutzung zur Bestimmung von Kapazitätsengpässen in der WebLicht-Infrastruktur und ihre frühe Behandlung möglich. Bibliografie [1] Hinrichs, E., Hinrichs, M., and Zastrow, T. (2010). WebLicht: Web-based LRT services for German. In Proceedings of the ACL 2010 System Demonstrations, pages 25'29. Association for Computational Linguistics. [2] ISO 24622-1:2014. 2014. Language resource management -- Component Metadata Infrastructure (CMDI) -- Part 1: The Component Metadata Model. Technical Report, ISO. [3] De Kok, D., De Kok, D., and Hinrichs, M. (2014). Build your own treebank. In: Proceedings of the CLARIN Annual Conference 2014. Soesterberg, Netherlands.",de,Einleitung Software Information Nutzerverhalt sammeln auswerten einsetzen Problem verhindern bevor entstehen Abstrakt zeigen derartig Software System verteilt webservices weblichen anwenden entstanden Analyse benutzen System verbessern weblicht Webanwendung automatisch Annotation Text multimodal daten Weblicht nutzen serviceorientiert Architektur ermöglichen Annotationswerkzeuge hinzufügen Webservice implementieren Service Repositorium zuzufügen Weblicht aggregieren metadaten bieten Nutzer Annotationswerkzeuge Benutzer mehrere Annotationswerkzeuge ausführen Annotationswerkzeug Output werkzeugs abhängig erlauben Weblicht Verkettungsmechanismus benutzern Annotationswerkzeuge kompatibel Art Weise kombinieren weblicht voll entwickelt Anwendung hundert Annotationswerkzeuge anbieten weiten Nutzung Weblicht Dimension Anzahl steigen durchschnittlich lassen benutz groß datasets annotieren Wachstumsstrukture verstehen sammeln nutzungsstatistiken Weblicht Annotationswerkzeug Nutzerbasis optimieren Nutzungsmuster derzeitig Benutzer kennen zweitens zukünftig Kapazitätsengpässe vorhersagen auseinandersetzen bevor us experience beeinträchtigen folgend abschnitten beschreiben Benutzeraktivität nutzungsmuster messen Simulation verschieden nutzungsszenarien behandeln schließlich kurz Überblick Veränderung geben aufgrund Messunge vornehmen Messung Benutzeraktivität Sammeln nutzerstatistiken Webseit wohlverstanden Feld mehrere konkurrierend voll entwickelt Produkt Google analytics open Source Piwik Webalizer And awstats verfügbar Tools funktionieren gewöhnlich Art analysieren webserv protokolliert Logdateien benötigen Maintainer Seite einfügt wodurch Browser Benutzer analysesoftwar Kontakt aufnimmen existierend Lösung direkt Weblicht anwendbar Verwendung zeigen Land weblicht besuchen nützlich Information fehlen Annotationswerkzeug verwenden Benutzer Piwik bieten programmierschnittstelle Application Programming interface jeder Annotationswerkzeug Nutzung registrieren erfordern knapp hundert dien upgedaten Müsst programmierschnittstelle aufrufen weit Nachteil Lösung interessant Information Land Benutzer verfügbar Annotationswerkzeuge Weblicht aufrufen direkt Browser Benutzer genannt Problem Melden Statistik lösen Verkettungsmechanismus jeder Annotationswerkzeug aufrufen gleichzeitig Verwendung Werkzeug melden Statistik sofort erhältlich Annotationswerkzeug Weblicht verfügbar verändern Verkettungsmechanismus ausführen Zusatznutzen metadaten bereitstellen Piwik erlauben Land Besucher Webbrowser ermitteln abb piwik Nutzerstatistike Stuttgarter Werkzeug Simulation Nutzungsmuster Problem gestiegen Nutzung Weblicht stoßen Annotationswerkzeuge Bewältigung vieler simultanbenutz Input entwickeln Problem entdecken Benutzer Annotationswerkzeug ausführen Simulieren Problem aufdecken bevor Benutzer begegnen zweck Simulationswerkzeug namens Bombard entwickeln Bombard ermöglichen entwicklern Testfälle spezifizieren Testfall bestehen Kette Annotationswerkzeug testen Input intervall anzeigt Testfall ausführen maximal erlaubt Bearbeitungsdauer Testfälle bestehen Bombard starten beginnen Testfall willkürlich gewählt Zeitpunkt wiederholen Testfall festgelegt intervall Bombardements behalten Bombard Überblick fehlschläge inakzeptabel bearbeitungsdauern Entwickler Bericht Statistik jeder Annotationswerkzeug abrufen Tübingen Bombard test neu Dienst nutzen folgend Szenario simulieren Gruppe jeweils Studierend reichen innerhalb Minute Text Gruppe bearbeiten Studierende Textabschnitt wikipedia Gruppe b bearbeiten Studierende Roman Alice Wunderland Übersetzung Mithilfe Testszenarios kapazitätsengpässen entwickelt annotationsdienst Bestimme sicherstellen Dienst Lage Szenarium verarbeiten Konfigurierung Testfälle Bombard anpassungsfähig einfach Szenarie Annotationswerkzeug Erwartung gelten anwenden önderungen weblichen annotationsketten testen unser Feststellung häufig nutzen nämlich Tagging Lemmatisierung Recognition Simulation entdecken folgend Problem dienste versagn Instanz lang Text gesandt versagn lang Text relativ verrauschen Dienst insbesondere parsern kommen Anfrag schnell Herein Dienst verarbeiten Problem leichtesten lösen Dienst alt Version Tcf Interchange Format Library nutzen Linear Größe Input anpassen Problem Fall Fall einzeln lösen Dienst Programmierfehler unerwartet Input versagen lassen Problem Schwerer lösen Rest Abschnitt besprechen Parser natürlich Sprache langsam Vergleich annotationsdiensen gebräuchlich normalerweise höchstens paar Sätz pro Sekunde parse vorhergesehen Gebrauchsfäll Bewältig fähigkeit modern Server Computercluster prozesse parallel durchführen ausnutzen Framework entwickeln verteilt Jesque basieren folgend bieten parallele Prozessverarbeitung innerhalb anfrag gleichzeitig Verarbeitung anfragen Garantie Verwendung Ressource Fairness betreffen Anfrage sichtbar Auswirkung Anfrage benutzen Framework upgedatet Dienst Malt Szenarium bewältigen wichtig Framework erlauben webservices groß Simultane Benutzerzahle groß Inputs anpassen prozessorkern gerät hinzufügen Fazit Abstract vorgehensweisen Messung Nutzung Kapazität Weblicht darstellen nutzungsmeldungen Verkettungswerkzeug hinzufügen sodass nutzungsstatistiken Annotationswerkzeuge bekommen bitten werkzeuge anpassen zweitens Dienstprogramm Bombard vorstellen ermöglichen Auswirkung Hochrechnung momentan Wachstum messen messungen Nutzung Bestimmung Kapazitätsengpässe früh Behandlung Bibliografie Hinrichs Hinrichs And zastrow weblicht Lrt Services for German proceedings -- -- acl System Demonstration pag association for computational linguistics ISO language resource Management Component Metadata infrastructure Cmdi Part The component Metadata Model Technical Report iso -- Kok -- Kok And Hinrichs Build Your own Treebank Proceedings -- -- clarin annual Conference Soesterberg netherlands,"[('weblicht', 0.44149603415246175), ('annotationswerkzeug', 0.27566311373550084), ('annotationswerkzeuge', 0.252283448087121), ('bombard', 0.23700052011727), ('dienst', 0.19296417961485063), ('benutzer', 0.18877753388968038), ('piwik', 0.13542886863844), ('testfall', 0.1261417240435605), ('problem', 0.12610457075618836), ('testfälle', 0.10157165147882999)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Annotationen für die automatisierte Verarbeitung von Märchen,Thierry Declerck (Universität des Saarlandes),Annotation,Annotation,"In diesem Poster- und Demobeitrag fassen wir ältere und aktuelle Arbeiten zur Entwicklung eines Annotationsschemas für Märchen zusammen, das auch die Einbettung von Märchentexten in automatisierten Verarbeitungszenarien erlaubt. Eine Entwicklung unserer Arbeit in diesem Bereich führte zur automatischen Erkennung von Charakteren in Märchen, deren Rolle in Dialogen und deren Emotionen, die als Grundlage eines TextToSpeech Szenarios dient, das Märchentexte ""vorliest"". Dieses Ergebnis basiert auf einer Zusammenarbeit mit Studenten der Computerlinguistik an der Universität des Saarlandes, die in den letzten Jahren in Form von Bachelor- oder Masterarbeiten, oder auch in Form eines Softwareprojekts erfolgten. Angefangen hat es mit der Masterarbeit von Antonia Scheidel zur Annotation von Märchen mit Proppschen1 Funktionen. Antonia Scheidel entwickelte ein neues Annotationsschemas, nach dem Märchen nach Texteigenschaften, temporalen Strukturen, Charakteren, Dialogen, und Proppschen Funktionen abfragen kann (s. [1]). Ein Annotationsschema ist insofern wichtig, als dadurch automatisierte Systeme ein Ziel haben, in das sie ihre Ergebnisse abbilden können. Wenn dazu auch Märchen mit dem Annotationsschema manuell annotiert werden, können die Ergebnisse der automatischen Verarbeitungen mit den menschlichen Annotationen verglichen werden. Darauf aufbauend hat Nikolina Koleva an einem automatisierten System gearbeitet, das in Märchentext (sie hat mit 2 Beispielen gearbeitet; ""The Magic Swan Geese"", eine englische Version eines russischen Märchens, und ""Väterchen Frost"", eine deutsche Version eines russischen Märchens). Sie hat ein Programm geschrieben, dass der Text nach linguistischen Kriterien analysiert, mit dem Ziel, die darin vorkommenden Charaktere zu erkennen, und in eine Datenbank zu speichern. Diese Datenbank ist von der Sorte ""Ontologie"": darin können logische Operationen durchgeführt werden. Als Hintergrund fungiert eine formale Beschreibung dessen, was in den genannten Märchen vorkommen kann, inklusive eine Ontologie über Familienverhältnissen. So kann das System erkennen, dass im Text ""die Tochter"" die gleiche Person wie die ""Schwester"" ist, wenn der Kontext dies suggeriert. Erkannte Charaktere im Märchen werden somit mit allgemeineren Kategorien 1 Auszug aus Wikipedia: ""Propp gilt als Begründer der morphologischen oder strukturalistischen Folkloristik. Zwischen 1914 und 1918 studierte er russische und deutsche Philologie. Danach unterrichtete er die deutsche Sprache an verschiedenen Hochschulen in Leningrad. Von 1938 bis 1969 war er Professor für Germanistik, russische Literatur und Folklore an der Staatlichen Universität Leningrad. 1928 erschien sein bahnbrechendes Werk Morphologie des Märchens. Das Buch wurde 1958 in den USA in englischer Sprache veröffentlicht, was Propp weltweite Anerkennung verschaffte. 1946 erschien das Buch Die historischen Wurzeln des Zaubermärchens."" (http://www.wikiwand.com/de/Wladimir_Jakowlewitsch_Propp. Zugriff am 2014.11.10) semantisch annotiert. Und wir wissen dann in welchen Kontexten (oder Situationen) die Tochter (zum Beispiel) involviert ist (s. hierzu [2]). Schließlich eine Gruppe von Studenten (Christian Eisenreich, Jana Ott, Tonio Süßdorf und Christian Wilms) im Rahmen eines Softwareprojekts an Erweiterungen der oben genannten Arbeiten gearbeitet. Sie haben zum einem das Annotationsschema erweitert, mit detaillierteren Dialogbeschreibungen, und mit der Kodierung von Emotionen. Die Ontologie wurde auch erweitert, und sie inkludiert jetzt auch eine Beschreibung von Dialogen (Fragen, Antworten, Monologe, etc.), inklusive der Kodierungen der Teilnehmern und der Dialogwechseln. Auch 6 Basisemotionen (Angst, Trauer, Freude, etc) sind in der Ontologie kodiert. Eine Haupterweiterung der vergangenen Arbeiten besteht darin, dass auch synthetische Stimmen eine Rolle spielen. Ist einmal ein Charakter erkannt worden, zum Beispiel die Prinzessin (im Märchen ""Froschkönig""), werden zusätzliche Merkmale kodiert (zum Bsp. Alter, usw.). Dann wird automatisch eine vorher definierte synthetische Stimme zum Charakter addiert. Wenn dann der Text von dem System analysiert wird, kann die Geschichte von den Stimmen ""erzählt"" werden. Wenn kein Charakter in einer Dialogsituation vorkommt, dann wird angenommen, dass der Erzähler/die Erzählerin ""daran"" ist. Eine Demo kann hier gehört werden: https://bytebucket.org/ceisen/apftml2repo/raw/763c5eb533f09997e757ec61652310c74223838 4/example%20output/audio_output.mp3 Im Anhang sind 2 Screenshots, die (für den ersten Teil der Audiodatei) zeigen wie das System den Text bearbeitet und kodiert, so dass die Sprachausgabe (s. Link oben) erzeugt werden kann. Unser Poster/Demo zeigt die Korrelation zwischen die Annotationen, die zum größten Teil automatisch generiert worden sind, und den verschiedenen Stufen der Verarbeitung bis hin zur Sprachausgabe. Referenzen [1] Thierry Declerck, Antonia Scheidel, Piroska Lendvai. Proppian Content Descriptors in an Integrated Annotation Schema for Fairy Tales. Language Technology for Cultural Heritage. Selected Papers from the LaTeCH Workshop Series,Theory and Applications of Natural Language Processing, Pages 155-169, Springer, Heidelberg, 2011 [2] Nikolina Koleva, Thierry Declerck, Hans-Ulrich Krieger. An Ontology-Based Iterative Text Processing Strategy for Detecting and Recognizing Characters in Folktales in: Jan Christoph Meister (ed.): Digital Humanities 2012 Conference Abstracts, Pages 467-470, Hamburg. [3] Christian Eisenreich, Jana Ott, Tonio Süßdorf, Christian Willms, Thierry Declerck. From Tale to Speech: Ontology-based Emotion and Dialogue Annotation of Fairy Tales with a TTS Output Proceedings of ISWC 2014, Riva del Garda, Italy, Springer. Anhang Abbildung 1: Wie der Text analysiert wird, Charaktere erkannt werden, sowie Dialogstrukturen und Emotionen. Die Basis für die Generierung der Sprachausgabe Abbildung 2 Wie der Text analysiert wird, Charaktere erkannt werden, sowie Dialogstrukturen und Emotionen. Die Basis für die Generierung der Sprachausgabe (Fortsetzung von Abbildung 1)",de,Demobeitrag fassen alt aktuell Arbeit Entwicklung Annotationsschemas märcher Einbettung Märchentext automatisiert Verarbeitungszenari erlauben Entwicklung Arbeit Bereich führen automatisch Erkennung Charaktere märch Rolle Dialogen Emotion Grundlage texttospeech Szenario dienen Märchentexte Vorliest Ergebnis basieren Zusammenarbeit Student Computerlinguistik Universität Saarlandes letzter Form masterarbeit Form Softwareprojekt erfolgten anfangen Masterarbeit Antonia Scheidel Annotation märcher funktionen Antonia Scheidel entwickeln neu Annotationsschemas märcher Texteigenschaft temporal Struktur Charakteren dialogen proppsch Funktion abfragen Annotationsschema insofern wichtig automatisiert System Ziel Ergebnis abbilden märchen Annotationsschema manuell annotieren Ergebnis automatisch Verarbeitung menschlich annotation vergleichen aufbauend Nikolina Koleva automatisiert System arbeiten Märchentext Beispiel arbeiten The Magic Swan Geese englisch Version russisch Märchen väterch Frost deutsch Version russisch Märchen Programm schreiben Text linguistisch kriterien analysieren Ziel vorkommend charaktere erkennen Datenbank speichern datenbank Sorte Ontologie logisch operationen durchführen Hintergrund fungieren formal Beschreibung genannt märcher vorkommen inklusive ontologie Familienverhältnisse System erkennen Text Tochter gleich Person Schwester Kontext suggerieren erkennen Charaktere märcher somit allgemein Kategorie Auszug wikipedia Propp gelten Begründer morphologisch strukturalistisch Folkloristik studieren russisch deutsch Philologie unterrichten deutsch Sprache verschieden Hochschule Leningrad Professor Germanistik russisch Literatur Folklore staatlich Universität Leningrad erscheinen bahnbrechend Werk Morphologie Märchen Buch USA englisch Sprache veröffentlichen propp weltweit Anerkennung verschaffen erscheinen Buch historisch wurzeln Zaubermärchens Zugriff semantisch annotiert wissen Kontext Situation Tochter involvieren hierzu schließlich Gruppe Student Christian Eisenreich Jana Ott Tonio Süßdorf Christian Wilms Rahmen Softwareprojekt Erweiterunge genannt Arbeit arbeiten Annotationsschema erweitern detaillierteren Dialogbeschreibung Kodierung Emotion Ontologie erweitern inkludieren Beschreibung Dialog Frage Antwort Monologe inklusive Kodierungen teilnehmern dialogwechseln Basisemotione Angst Trauer Freude etc Ontologie kodieren Haupterweiterung Arbeit bestehen synthetisch Stimme Rolle spielen Charakter erkennen Prinzessin märch Froschkönig zusätzlich merkmale kodieren Bsp alt automatisch vorher definiert synthetisch Stimme Charakter addieren Text System analysieren Geschichte Stimme erzählen Charakter Dialogsituation vorkommen annehmen Erzähler Erzählerin Demo hören Anhang Screenshot Audiodatei zeigen System Text bearbeiten kodieren Sprachausgabe link erzeugen poster Demo zeigen Korrelation Annotation groß automatisch neriern verschieden Stufe Verarbeitung Sprachausgabe referenz Thierry declerck Antonia Scheidel Piroska Lendvai Proppian Content Descriptors Integrated Annotation Schema for Fairy tales Language Technology for cultural Heritage Selected Papers from -- Latech workshop series Theory and applications -- natural language Processing pag spring Heidelberg Nikolina Koleva Thierry Declerck Krieger iterativ Text Processing strategy for detecting and recognizing Character Folktales Jan Christoph Meister ed Digital Humanitie Conference abstracts Pages Hamburg Christian Eisenreich Jana Ott Tonio Süßdorf Christian Willms Thierry declerck from tale to speech Emotion and dialogue annotation of Fairy tales with tts Output Proceedings of iswc riva Del Garda Italy spring Anhang Abbildung Text analysieren charaktere erkennen dialogstrukturen Emotion Basis Generierung Sprachausgabe Abbildung Text analysieren charaktere erkennen dialogstrukturen Emotion Basis Generierung Sprachausgabe Fortsetzung Abbildung,"[('märcher', 0.2164581091386741), ('sprachausgabe', 0.1961628826085294), ('charaktere', 0.18282803688654775), ('märchen', 0.15971445841008874), ('christian', 0.15460023437784756), ('emotion', 0.15184032201432726), ('antonia', 0.14712216195639705), ('russisch', 0.14276690628591931), ('scheidel', 0.13703314028075916), ('thierry', 0.13703314028075916)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Over-tagging with XML in Digital Scholarly Editions,Elise Hanrahan (BBAW),"Digitale Edition, XML","Digitale Edition, XML","This talk looks at the phenomenon of over_tagging (term created here) in XML, which consists of exaggerated and unfocused tagging that concentrates on diplomatic characteristics. These tags are used for the display of the digital edition text on the computer screen__an especially questionable result when digital facsimiles exist. To what degree computer technology affects practices and theories in scholarly editing is an open question. But whether or not we are in the midst of a revolution or simply experiencing a change of tools, there are certain influences that can already be observed. One is the availability of digital facsimiles combined with the use of XML. How could digital facsimiles change online scholarly editions? Digital facsimiles challenge one of the claimed purposes of a scholarly edition‚Äîthe recreation of the original manuscript. 1 This aim, often strived for by means of a diplomatic transcription, is particularly prevalent in recent editions. Elements of diplomatic transcribing can for instance be found in most German critical editions from the last twenty_five years. 2 Yet the question ""How could digital facsimiles change online scholarly editions?"" was posed because astonishingly the diplomatic method continues to be dominant. It is thus the argument of this talk that online editions do not reflect this significant alteration in the relationship between researcher and original source. Instead the same editorial method is being used, and the primary new development is the use of XML instead of Microsoft Word. Indeed not only do digital editions continue to be diplomatic, but the tendency has even increased. 3 1 It should be noted that this very strong focus on an ""authentic"" recreation of the original handwriting is fairly new in Editionswissenschaft, starting in the 1970s and becoming very dominant in the 1990s with editors like Hans Zeller. 2 For a very concise summary of the take_over of the material_paradigma, see: Rasch, Wolfgang, Wolfgang Lukas, and Jörg Ritter. ""Gutzkows Korrespondenz 'Probleme Und Profile Eines Editionsprojekts. "" Brief_Edition Im Digitalen Zeitalter (Beihefte Zu Editio) 34 (2013): 99. 3 Elena Pierazzo addresses the popularity of digital documentary editions in: Pierazzo, Elena. ""Digital Documentary Editions and the Others. "" Scholarly Editing: The Annual of the Association for Documentary Editing 35 (2014). Accessed November 10, 2014. http://www.scholarlyediting.org/2014/essays/essay.pierazzo.html. 1 Structure of the Talk I. Arguments for why diplomatic transcriptions are no longer necessary when digital facsimiles are available II. An examination of why diplomatic aspects in digital editions have surprisingly increased instead of decreased III. Arguments and counter_arguments for continuing to transcribe diplomatically in digital editions IV. Suggestions for alternative editorial priorities I. Digital facsimiles are a game-changer There are two main arguments for recreating the original manuscript in the edited text (thus using a diplomatic transcription), both of which online facsimiles challenge. The first is to bridge the gap between the original manuscript and the researcher. Before digital facsimiles it was quite possible that the researcher never set eyes on the original manuscript, which was carefully stored away in a library or archive. The edition thus strove to offer the researcher an objective depiction of the handwriting. Now, however, the researcher can look at the image of the handwriting online, thus the edition must no longer bridge this gap. The second argument for a diplomatic transcription is to preserve the manuscript. If anything happened to the original source, the text of the edition could be used as a replacement. Additionally the edition protects the manuscript from being over_handled, because it functions as an authoritative substitute. There is however no longer a need to create a substitute for the manuscript, because a high_quality image exists. Despite these arguments, in praxis one finds digital editions still ruled by the diplomatic trend. Why is this? II. The diplomatic-tradition and XML There are two reasons for the prevalence of diplomatically_influenced digital transcriptions. The first is that digital editions emerged at the same time diplomatic editing was the dominant method for scholarly editions. 4 It is therefore not surprising that the current method for print editions was transferred to the newly emerging digital editions. The second reason is due to the very nature of XML. In XML, unlike in Microsoft Word, specifications for the visual presentation of the edited text are completely separated from the documentation of the original source. Hence in XML the editor is no longer limited by the 4 That is, the end of the 1990s/start of the 2000s 2 space on the page for recording textual phenomena and can enter as many XML tags as desired, allowing a theoretically endless documentation of the characteristics of the manuscript. Combine this opportunity with an already diplomatic trend, and the result is a lot of diplomatic XML_tagging, sometimes to an incredibly minute degree. This very real phenomenon shall be called 'over_tagging' . Over_tagging refers to an exaggerated amount of XML tags that do not pursue a specific research question, but are in praxis only used for the display of the edited text on the screen. Over_tagging could be for example using a character in the line below an indentation to mark the length of an indentation, tagging the exact location and angle of marginalia, tagging orthographical elements like the long s in German, or tagging line breaks that are not semantically meaningful. While there is nothing inherently wrong with tagging these kinds of textual phenomena, it is important to ask, what is the purpose of these tags? Such tagging is especially problematic when it comprises a large part of the XML schema. And one must add, no matter how detailed a transcription is, it can't recreate the image of the handwriting and the vast amount of data that the image carries. And not only is the usefulness of the results debatable, over_tagging takes a great deal of time and energy. Other fundamental editorial tasks fall by the wayside__tasks such as editorial commentary, to name only one of many. There are many other gaps to be bridged between the reader and original source besides the material gap. An essential benefit to be gained from more reflective tagging practices is the time to focus on these other editorial tasks. III. Counter-arguments: machine searchable and a reader-aid One argument for over_tagging is machine readability. This means that tagged textual information can be found in automated searches. Yet does anyone truly search for aspects such as indentation size? An editor might argue: ""Perhaps not now, but someone could in the future. And what""s more, someone could discover that this seemingly insignificant textual characteristic actually carries a semantic meaning"" . Such an answer reveals the editorial tradition that still strongly underlies digital editions today and is inseparable from diplomatic transcribing. This is the philosophy that literally everything on the page could be significant. 5 There are two responses to this. First of all search masks are currently not being made to search for diplomatic characteristics. In praxis these tags are used for the display of the text only. It is true that, if desired, search masks could be made for this purpose. It is also true that 5 This of course leads back again to the authenticity/materiality movement from Hurlebusch, Zeller and others. For an example of such a perspective that does not directly lead to the solution of a diplomatic transcription (but instead to digital facsimiles), see: Richter, Elke. ""Goethes Briefhandschriften digital 'Chancen und Probleme elektronischer Faksimilierung. "" Brief_Edition Im Digitalen Zeitalter (Beihefte Zu Editio) 34 (2013): 53_75. 3 it is impossible to prove that a certain aspect of a page is not meaningful. However, the ""everything could be significant"" position is not a feasible editorial method. It is definitely not the basis on which to build a well_functioning XML schema. In reality, at the end of the project there exists a very large amount of information that is solely used for the display of the text on the computer screen. Another argument for over_tagging is that the resulting text is a kind of facsimile_reading_aid. 6 This is however a problematic stance. Firstly, critical editions are not made primarily to be reading_aids for facsimiles, although they can be helpful for this purpose. Critical editions are editorial arguments and offer a readable edited text according to that argument. A facsimile_reading tool is potentially very useful, but is something different than a critical edition. Secondly, a diplomatic transcription was not conceived for this aim and is probably not the best method. There are certainly much better ways to help guide a researcher through a facsimile than simply mirroring the facsimile in the edited text, especially in consideration of technological possibilities. IV. Different priorities for digital editions Relinquishing over_tagging means more time for editors to concentrate on other aspects of editing, such as commentary and semantic tagging (including not just person or place names, but also more abstract themes, such as concepts found in the texts). There would also be more time to tag the creative process of the author (such as capturing the layers of the text's development by tagging crossed out/added words). There would be more time to enter meta data, to link through standard IDs like VIAFs for persons and geonames for places, and to simply to think about how to use the digital space to the researcher's best advantage. In many ways, instead of reflecting on what doors technology opens for critical editions and thus shaping technology to this end, editors have let technology define them, losing sight of priorities in today""s digital world. For instance, an essential current challenge for digital editions is to avoid the 'island' problem‚Äîsingle editions floating in the internet without a real connection to one another. Minute diplomatic tagging does not address this problem (standard IDs and meta data to some degree does). Yet it isn""t a question of what is important and what is not__all editorial tasks are important__it is a questioning of appreciating what an edition has to offer and carefully considering the energy invested and the benefits gained. ""Over_tagging"" is perhaps a very small piece of the debate on digital editions, but it could point to a general direction and is therefore worthwhile to consider in this context. 6 This idea is touched on in Pierazzo(2014), 4.",en,talk look phenomenon term create xml consist exaggerated unfocused tagging concentrate diplomatic characteristic tag display digital edition text computer especially questionable result digital facsimile exist degree computer technology affect practice theory scholarly editing open question midst revolution simply experience change tool certain influence observe availability digital facsimile combine use xml digital facsimile change online scholarly edition digital facsimile challenge claim purpose scholarly recreation original manuscript aim strive mean diplomatic transcription particularly prevalent recent edition element diplomatic transcribing instance find german critical edition year question digital facsimile change online scholarly edition pose astonishingly diplomatic method continue dominant argument talk online edition reflect significant alteration relationship researcher original source instead editorial method primary new development use xml instead microsoft word digital edition continue diplomatic tendency increase note strong focus authentic recreation original handwriting fairly new editionswissenschaft start dominant editor like han zeller concise summary rasch wolfgang wolfgang lukas jörg ritter gutzkow korrespondenz probleme und profile eine editionsprojekts m digitalen zeitalter beihefte zu editio elena pierazzo address popularity digital documentary edition pierazzo elena digital documentary edition scholarly editing annual association documentary edit access november structure talk argument diplomatic transcription long necessary digital facsimile available ii examination diplomatic aspect digital edition surprisingly increase instead decrease iii argument continue transcribe diplomatically digital edition iv suggestion alternative editorial priority digital facsimile game changer main argument recreate original manuscript edited text diplomatic transcription online facsimile challenge bridge gap original manuscript researcher digital facsimile possible researcher set eye original manuscript carefully store away library archive edition strove offer researcher objective depiction handwriting researcher look image handwriting online edition long bridge gap second argument diplomatic transcription preserve manuscript happen original source text edition replacement additionally edition protect manuscript function authoritative substitute long need create substitute manuscript image exist despite argument praxis find digital edition rule diplomatic trend ii diplomatic tradition xml reason prevalence digital transcription digital edition emerge time diplomatic editing dominant method scholarly edition surprising current method print edition transfer newly emerge digital edition second reason nature xml xml unlike microsoft word specification visual presentation edited text completely separate documentation original source xml editor long limit end start space page record textual phenomenon enter xml tag desire allow theoretically endless documentation characteristic manuscript combine opportunity diplomatic trend result lot diplomatic incredibly minute degree real phenomenon shall call refer exaggerated xml tag pursue specific research question praxis display edited text screen example character line indentation mark length indentation tag exact location angle marginalia tag orthographical element like long s german tagging line break semantically meaningful inherently wrong tag kind textual phenomenon important ask purpose tag tagging especially problematic comprise large xml schema add matter detailed transcription recreate image handwriting vast datum image carry usefulness result debatable take great deal time energy fundamental editorial task fall editorial commentary gap bridge reader original source material gap essential benefit gain reflective tagging practice time focus editorial task iii counter argument machine searchable reader aid argument machine readability mean tag textual information find automate search truly search aspect indentation size editor argue future discover seemingly insignificant textual characteristic actually carry semantic meaning answer reveal editorial tradition strongly underlie digital edition today inseparable diplomatic transcribing philosophy literally page significant response search mask currently search diplomatic characteristic praxis tag display text true desire search mask purpose true course lead authenticity materiality movement hurlebusch zeller example perspective directly lead solution diplomatic transcription instead digital facsimile richter elke goethe briefhandschriften digital chancen und probleme elektronischer faksimilierung m digitalen zeitalter beihefte zu editio impossible prove certain aspect page meaningful significant position feasible editorial method definitely basis build xml schema reality end project exist large information solely display text computer screen argument result text kind problematic stance firstly critical edition primarily facsimile helpful purpose critical edition editorial argument offer readable edited text accord argument tool potentially useful different critical edition secondly diplomatic transcription conceive aim probably good method certainly well way help guide researcher facsimile simply mirror facsimile edited text especially consideration technological possibility iv different priority digital edition relinquish mean time editor concentrate aspect editing commentary semantic tagging include person place name abstract theme concept find text time tag creative process author capture layer text development tag cross add word time enter meta datum link standard id like viafs person geoname place simply think use digital space researcher good advantage way instead reflect door technology open critical edition shape technology end editor let technology define lose sight priority digital world instance essential current challenge digital edition avoid island edition float internet real connection minute diplomatic tagging address problem standard ids meta datum degree question important editorial task questioning appreciate edition offer carefully consider energy invest benefit gain small piece debate digital edition point general direction worthwhile consider context idea touch,"[('diplomatic', 0.4132383653742005), ('edition', 0.3304956792873281), ('facsimile', 0.2827420394665583), ('argument', 0.20216746829003712), ('tag', 0.1831497504284807), ('editorial', 0.16540974678275763), ('transcription', 0.1620632459123816), ('manuscript', 0.1620632459123816), ('digital', 0.1422934191717595), ('xml', 0.13245930635577258)]"
2015,DHd2015,2015_cls_metadata_extracted.csv,Digitale Analyse Graphischer Literatur,"Alexander Dunst (Institut für Anglistik und Amerikanistik, Universität Paderborn); Rita Hartel (Institut für Informatik, Universität Paderborn); Sven Hohenstein (Department Psychologie, Universität Potsdam); Jochen Laubrock (Department Psychologie, Universität Potsdam)","Graphic Novels, Graphic‚àöäNarrative Markup Language, Eyetracking, Annotation","Graphic Novels, Graphic‚àöäNarrative Markup Language, Eyetracking, Annotation","Zusammenfassung Der  hier  vorgeschlagene  Vortrag  stellt  erste  Ergebnisse  der  vom  deutschen  Bundesministerium  für Bildung  und  Forschung  (BMBF)  finanzierten  Nachwuchsgruppe  ""Hybride  Narrativität:  Digitale  und Kognitive Methoden zur  Erforschung  Graphischer Literatur"" vor. Der erste Teil des Vortrages wird das Projekt  der  Nachwuchsgruppe  und  zentrale  Forschungsfragen  vorstellen.  Im  zweiten  Teil  wird  eine kurze Einführung in die wichtigsten Merkmale der Beschreibungssprache  ""Graphic Narrative Markup Language""  (GNML)  gegeben  sowie  die  wesentlichen  Funktionen  des  Editors  zur  Erfassung  und Analyse graphischer Erzählungen demonstriert.  Außerdem präsentieren wir erste Ergebnisse, die mit Hilfe von Netzwerkgraphen und Beziehungsmatrizen zu Paul Austers City of Glass  (1985) und dessen graphischer  Adaptation  durch  David  Mazzuchelli  und  Paul  Karasik  (1994)  strukturelle  Öhnlichkeiten und  Differenzen  zwischen  den  narrativen  Systemen  des  literarischen  und  graphischen  Romans darstellen.  Der  abschließende  dritte  Teil  stellt,  wiederum  am  Beispiel  von  City  of  Glass,  eines  der interdisziplinären  Anwendungsgebiete  der  digitalen  Annotation  graphischer  Literatur  vor:  anhand von  Blickbewegungsmaßen  können  Rückschlüsse  auf  das  Leseverständnis  graphischer  Literatur gewonnen werden. 1. Forschungsfragen Die digitale  Annotation  und Analyse  literarischer  Text-__Korpora kann  in den vergangenen Jahren  auf enorme  Fortschritte  verweisen  und  hat  neue  Erkenntnisprozesse  etabliert,  die  mittlerweile  Eingang in die Forschungsbestrebungen einer breiteren Literaturwissenschaft und Literaturgeschichte finden (1).  Im  Gegensatz  dazu  steckt  die  Analyse  visueller  Kultur  erst  in  den  Anfängen  und  stellt  in  den Digitalen  Geisteswissenschaften  aus  mehreren  Gründen  oft  eine  Randerscheinung  dar:  zu  der institutionellen Verortung in traditionell text-__fokussierten Disziplinen und den Forschungsinteressen der  Computerphilologie  gesellen  sich  urheberrechtliche  Fragen,  sowie  der  vergleichsweise  hohe technische Aufwand und niedrigere Entwicklungsstand von Methoden der Bildanalyse. Ziel  dieses  interdisziplinären  Projektes  ist  die  empirische  Erforschung  graphischer  Literatur, insbesondere  des  Genres  des  graphischen  Romans  (""graphic  novel"").  Durch  die  Entwicklung  von empirischen  Methoden  für  graphische Literatur  sollen  Ansätze  aus  dem  ""Distant  Reading""  für multimediale  Kulturformen  erschlossen  werden.  Die  durch  die  Nachwuchsgruppe  entwickelten Annotations-__Werkzeuge, insbesondere die XML-__Sprache GNML  und der GNML-__Web-__Editor  (siehe 2.), sind in weiterer  Folge nicht nur für die Beschäftigung mit graphischer Literatur sondern auch für die Analyse  von  Handschriften,  Film  und  Fernsehen  von  Interesse.  In  erster  Linie  zielt  die Nachwuchsgruppe  jedoch  darauf  ab,  grundlegende  Fragen  zur  spezifischen  Narrativität  und  dem formalen  Aufbau  des  graphischen  Romans  zu  beantworten,  die  im  Rahmen  qualitativer  Methoden nicht  empirisch  überprüft  werden  können  oder  vollständig  außerhalb  des  Forschungsradius hermeneutischer  Fragestellungen  in  den  Geisteswissenschaften  liegen.  Folgende  zentrale Forschungsfragen sind hier beispielshaft zu erwähnen: Beschreibt der Terminus graphischer Roman,  1 ursprünglich  ein  Begriff  aus  der  Verlagswerbung,  tatsächlich  strukturelle  Öhnlichkeiten  mit  dem literarischen  Roman?  Welche  Charakteristika  unterscheiden  den  graphischen  Roman  vom Comicbuch?  Lassen  sich  strukturelle  Innovationen  isolieren  und  historisch  verfolgen,  die  das  Genre erfolgreich haben werden lassen? Sind diese narratologischer oder thematischer Natur, oder handelt es  sich  um  eine  Kombination  beider?  Aus  welchen  Sub-__Genres  besteht  der  graphische  Roman,  und wie interagieren diese im System des Genres? Welche gesellschaftlich relevanten Fragen werden im graphischen Roman kulturell verarbeitet und tragen so zu seiner Popularität bei? 2. Editor als Erfassungs-__  und Analysewerkzeug Die  auf der ""Comic Book Markup Language  (CBML)""  (2)  und damit auf der ""Text Encoding Initiative"" (TEI)  (3)  basierende  und  im  Rahmen  dieses  Projektes  entwickelte  XML-__Sprache  GNML  erlaubt  dem Bearbeiter nicht nur das Erfassen textueller  sondern insbesondere auch visueller  Aspekte graphischer Erzählungen.  Mit  Hilfe  von  GNML können  unter  anderem  Seiten,  Panel-__Anordnungen,  Texte, Sprechblasen, Charaktere und andere Objekte erfasst werden, sowie Interpretationen,  z.B.  zu  Panel-__ Übergänge und Texttypen,  abgelegt werden.  GNML bietet somit eine abstrakte Sicht auf visuelle  und textuelle  Aspekte,  die  so  effizient  analysiert  werden  können.  Basierend  auf  GNML  können  z.B. Eyetracking-__Experimente  ausgewertet  werden,  und  Fragestellungen  wie  ""Wie  oft  wechselt  die Aufmerksamkeit  des  Lesers  vom  Text  zum  Bild""  oder  ""Was  ist  der  relative  (visuelle)  Anteil  eines Charakters an der gesamten Erzählung"" effizient beantwortet werden. Eine  zentrale  Rolle  dieses  Projektes  nimmt  der  GNML-__Editor  ein.  Er  erlaubt  ein  effizientes, benutzerfreundliches  Erfassen  der  visuellen  und  textuellen  Aspekte,  ohne  dass  der  Bearbeiter  XML oder  GNML  beherrschen  muss.  Neben  der  Erfassung  visueller  Aspekte,  bei  der  Objekte  durch  den Bearbeiter  nachgezeichnet  und annotiert  werden können, bietet der Editor auch die automatisierte Erkennung  verschiedener  Aspekte.  Derzeit  bietet  der  Editor  z.B.  eine  automatische  Erkennung  der Panels.  Hierbei  werden  nicht  nur  regelmäßige  Formen  (Rechtecke),  sondern  nahezu  beliebige Umrandungsformen  automatisch  erkannt.  Eine  weiterführende  automatisierte  Erfassung,  etwa  mit Hilfe  von  Texterkennungssystemen  und  Handschriftenerfassung  oder  das  automatische  Erkennen von  Charakter-__Objekten,  befinden  sich  derzeit  in  Entwicklung.  Verfahren  aus  dem  Bereich  des maschinellen Lernens sollen dafür sorgen, dass die Erkennung neuer Charaktere mit zunehmendem Training zuverlässiger funktioniert. Ein zusätzliches Analysetool ermöglicht  die  Analyse bereits erfasster GNML Dokumente.  So kann der Benutzer z.B. sich die Beziehungen der Charaktere untereinander in Form von Netzwerkgraphen  und Beziehungsmatrizen  anzeigen  lassen.  Auch  erweiterte  Statistiken  über  die  Objekte  und  Charaktere der Erzählung können berechnet und dem Benutzer in Form von Diagrammen und Tabellen angezeigt werden, basierend z.B. auf den folgenden Fragestellungen: ‚Ä¢ Wie oft erscheint ein Charakter? ‚Ä¢ Was ist der relative visuelle Anteil eines Charakters? ‚Ä¢ Mit wem zusammen erscheint der Charakter auf derselben  Seite oder in demselben Panel? 3. Empirische Ergebnisse: Eyetracking-__Maße für die Aufmerksamkeitszuwendung des Lesers Die  Methode  der  Blickbewegungsmessung  (Eyetracking)  liefert  Einblicke  in  die  größtenteils unbewusste  und  in  gewissem  Maße  kulturspezifische  Verteilung  der  Aufmerksamkeit  bei  der visuellen  Rezeption  von  Informationen.  Für  die  Rezeption  von  Texten  hat  die  psycholinguistische Forschung  hier  bereits  viele  grundlegende  Erkenntnisse  gewonnen,  und  die  Methode  wird  auch  2 erfolgreich  bei  der  Erforschung  der  kognitiven  Verarbeitung  von  Bildern  oder  visuellen  Szenen angewandt.  Sequenzielle  Kunst,  Comics  und  graphische  Literatur  sind  jedoch  bisher  fast  völlig vernachlässigt, obwohl sie idealtypisch textuelle und graphische Elemente kombinieren. Die Nutzung per  GNML  annotierten  Materials  eröffnet  neue  Möglichkeiten,  die  psychologische  Wirkung graphischer Literatur zu untersuchen. Wie  gelingt  es  einem  Zeichner,  die  Aufmerksamkeit  des  Lesers  von  einem  Panel  zum  nächsten  zu lenken? Wie interagieren textuelle und graphische Elemente bei der Rezeption eines Comics? Scott McCloud (4) hat dazu erste theoretische Überlegungen angestellt und visualisiert; Neil Cohn (5) hat die  theoretische  Analyse  weiterentwickelt  zu  einer  formalen  ""visuellen  Sprache"",  die  vergleichbar einer  generativen  Grammatik  die  narrativen  Elemente  einer  graphischen  Geschichte  kategorisiert. Haben  diese  Ordnungsschemata  eine  psychologische  Realität?  Erste  Eyetracking-__Studien  aus unserem  Labor  zeigen,  dass  das  von  McCloud  postulierte  Ausmaß  an  ""Closure"",  das  zwischen unterschiedlichen  Arten  von  Panel-__Übergängen  variiert,  sich  in  unterschiedlich  langen Betrachtungszeiten niederschlägt. Wir  zeigen  außerdem  erste  Ergebnisse  aus  einem  empirischen  Eyetracking-__Corpus  graphischer Literatur.  Im  Kontext  des  Projektes  wird  ein  R-__Paket  zum  Import  von  GNML-__Daten  und  zur statistischen  Analyse  und  Visualisierung  von  Blickbewegungen  und  Corpusdaten  entwickelt,  das  in diesen  Analysen zur Anwendung kommt. Literaturverzeichnis 1. Siehe etwa: Moretti, Franco. Distant Reading. London: Verso, 2013. 2.  Walsh,  John.  Comic  Book  Markup  Language:  An  Introduction  and  Rationale.  Digital  Humanities Quarterly.  2012, Volume 6, Number 1. 3.  Text  Encoding  Initiative  -__  P5:  Guidelines  for  Electronic  Text  Encoding  and  Interchange.  [Online] 2.7.0, 09 16, 2014. http://www.tei-__c.org/release/doc/tei-__p5-__doc/en/html/. 4.  McCloud,  Scott.  Understanding  Comics:  The  Invisible  Art.  Northampton,  MA:  Kitchen  Sink  Press, 1993. 5. Cohn, Neil. The Visual Language of Comics. London: Bloomsbury, 2013.",de,Zusammenfassung vorgeschlagen Vortrag stellen Ergebnis deutsch Bundesministerium Bildung Forschung bmbf finanzieren Nachwuchsgruppe hybride Narrativität digital kognitiv Methode Erforschung graphisch Literatur Vortrag Projekt Nachwuchsgruppe zentral forschungsfrag vorstellen kurz Einführung wichtig merkmal beschreibungssprach Graphic Narrative markup Language gnml geben wesentlich Funktion Editor Erfassung Analyse graphisch erzählung demonstrieren präsentieren Ergebnis Hilfe netzwerkgraph beziehungsmatrizen Paul Austers City -- Glass graphisch Adaptation David Mazzuchelli Paul Karasik strukturell öhnlichkeiten differenzen narrativ Systeme literarisch graphisch Roman darstellen abschließend stellen wiederum City -- Glass interdisziplinären anwendungsgebien digital Annotation graphisch Literatur anhand blickbewegungsmaßen Rückschlüss Leseverständnis graphisch Literatur gewinnen forschungsfragen digital Annotation Analyse literarisch enorm Fortschritt verweisen erkenntnisprozesse etablieren mittlerweile Eingang Forschungsbestrebunge breit Literaturwissenschaft literaturgeschichen finden Gegensatz stecken Analyse visuell Kultur anfängen stellen digital geisteswissenschaften mehrere Grund Randerscheinung dar institutionell Verortung traditionell disziplinen Forschungsinteresse Computerphilologie gesellen urheberrechtlich Frage vergleichsweise hoch technisch Aufwand niedrig Entwicklungsstand Methode Bildanalyse Ziel interdisziplinär projekt empirisch Erforschung graphisch Literatur insbesondere genr graphisch Roman graphic novel Entwicklung empirisch Methode graphisch Literatur Ansatz distant reading multimedial kulturformen erschließen Nachwuchsgruppe entwickelt insbesondere gnml sehen weit Folge Beschäftigung graphisch Literatur Analyse handschrift Film Fernsehen Interesse Linie zielen Nachwuchsgruppe grundlegend Frage spezifisch Narrativität formal Aufbau graphisch roman beantworten Rahmen qualitativ Methode empirisch überprüfen vollständig außerhalb Forschungsradius hermeneutisch Fragestellung geisteswissenschaft liegen folgend zentral forschungsfragen Beispielshaft erwähnen beschreiben Terminus graphisch Roman ursprünglich Begriff Verlagswerbung tatsächlich strukturell öhnlichkeien literarisch Roman Charakteristika unterscheiden graphisch Roman Comicbuch lassen strukturell Innovation isolieren historisch verfolgen Genre erfolgreich lassen narratologischer thematisch Natur handeln Kombination beide bestehen graphisch Roman interagier System genr gesellschaftlich relevant Frage graphisch Roman kulturell verarbeiten tragen Popularität Editor Analysewerkzeug Comic Book Markup Language cbml Text Encoding Initiative tei basierenden Rahmen projekt entwickelt gnml erlauben Bearbeiter erfassen Textueller insbesondere visuell Aspekt graphisch Erzählung Hilfe gnml Seite Text Sprechblase charakter Objekt erfassen Interpretation übergänge Texttype ablegen gnml bieten somit abstrakt Sicht visuell textuell aspeken effizient analysieren basierend gnml auswerten Fragestellung wechseln Aufmerksamkeit Leser Text Bild relativ visuell Anteil Charakter gesamt Erzählung effizient beantworten zentral Rolle projekt nehmen erlauben effizient benutzerfreundlich erfassen visuell textuell Aspekt Bearbeiter xml Gnml beherrschen Erfassung visuell aspeken Objekt Bearbeiter nachgezeichnen annotiert bieten Editor automatisiert Erkennung verschieden aspeken derzeit bieten Editor automatisch Erkennung Panel hierbei regelmäßig Form Rechtecke nahezu beliebigen Umrandungsformen automatisch erkennen weiterführend automatisiert Erfassung Hilfe Texterkennungssystem Handschriftenerfassung automatisch erkennen befinden derzeit Entwicklung Verfahren Bereich maschinell Lernen sorgen Erkennung neu Charaktere zunehmend Training zuverlässig funktionieren zusätzlich Analysetool ermöglichen Analyse erfasst gnml dokumenen Benutzer Beziehung charaktere untereinander Form netzwerkgraph beziehungsmatrizen Anzeig lassen erweitert Statistik Objekt charaktere Erzählung berechnen Benutzer Form Diagramm Tabell anzeigen basierend folgend Fragestellung erscheinen Charakter relativ visuell Anteil Charakter erscheinen Charakter Seite Panel empirisch Ergebnis Aufmerksamkeitszuwendung Leser Methode Blickbewegungsmessung eyetracking liefern einblicken größtenteils unbewusst Gewissem maß kulturspezifisch Verteilung Aufmerksamkeit visuell Rezeption Information Rezeption Text psycholinguistisch Forschung grundlegend erkenntnis gewinnen Methode erfolgreich Erforschung kognitiv Verarbeitung bildern visuell Szen anwenden sequenziellen Kunst Comic graphisch Literatur fast völlig vernachlässigen obwohl idealtypisch textuell graphisch element kombinieren Nutzung per gnml annotierten Material eröffnen Möglichkeit psychologisch Wirkung graphisch Literatur untersuchen gelingen zeichn Aufmerksamkeit Leser Panel nächster lenken interagier textuellen graphisch Element Rezeption Comic Scott Mccloud theoretisch Überlegung anstellen visualisiern neil Cohn theoretisch Analyse weiterentwickeln formal visuell Sprache vergleichbar generativ Grammatik narrativ element graphisch Geschichte kategorisieren Ordnungsschemata psychologisch Realität unser Labor zeigen Mccloud postuliert Ausmaß closure unterschiedlich Art variieren unterschiedlich lang Betrachtungszeit niederschlagen zeigen Ergebnis empirisch graphisch Literatur Kontext projekt Import statistisch Analyse Visualisierung blickbewegung corpusdaten entwickeln Analyse Anwendung literaturverzeichnis sehen moretti Franco distant reading London verso Walsh John Comic Book Markup Language Introduction And rationalen Digital Humanitie quarterly volumen numb Text Encoding Initiative guidelin for electronic Text Encoding And interchangen Online Mccloud Scott Understanding Comics The invisible Art Northampton ma kitch Sink press Cohn neil The visual language -- Comics London Bloomsbury,"[('graphisch', 0.5164666096250095), ('gnml', 0.3011957377408871), ('visuell', 0.20133327979306598), ('charakter', 0.13490029058099604), ('nachwuchsgruppe', 0.13386477232928315), ('roman', 0.13056170418382396), ('literatur', 0.12905055329570267), ('comic', 0.1195122999745517), ('editor', 0.11608761885859756), ('aspeken', 0.10593221823753955)]"
2016,DHd2016,vortraege-062.xml,Emosaic 'Visualisierung von Emotionen in Texten durch Farbumwandlung zur Analyse und Exploration,"Martin von Lupin (FH Potsdam, Deutschland); Philipp Geuder (FH Potsdam, Deutschland); Marie-Claire Leidinger (FH Potsdam, Deutschland); Tobias Schröder (FH Potsdam, Deutschland); Marian Dörk (FH Potsdam, Deutschland)","Datenvisualisierung, Emotionen, Textumwandlung, Textexploration, Interaktiv","Umwandlung, Entdeckung, Programmierung, Inhaltsanalyse, Identifizierung, Webentwicklung, Visualisierung, Daten, Literatur, Methoden, Forschung, Forschungsergebnis, Software, Text, Werkzeuge, Visualisierung","   Während beide Tools als Datengrundlage Emotionen aus Webeinträgen nutzen, ist es hingegen unser Ziel die benutzerdefinierte Eingabe eines Textes zu ermöglichen. Zusätzlich sollen sowohl differenzierte als auch nachvollziehbare farbliche Repräsentationen für Emotionen verwendet werden. Die Farbkodierung von Emotionen zur besseren Orientierung innerhalb visueller Darstellungen scheint eine etablierte Methode zu sein, jedoch lassen die bisher vorliegenden Kodierungssysteme keine fundierte Emotionsbeschreibung zu.  Während die Sättigung und die Helligkeit Minima und Maxima analog zu Dominanz und Erregung beschreiben, stellt der Farbwert einen kontinuierlichen Farbverlauf dar. Um eine Farbwertannäherung an den Rändern zu vermeiden, haben wir bei der Farbzuweisung einen Farbwertbereich bewusst ausgespart, sodass eine Grenze zwischen Minima und Maxima deutlich hervortritt. Blau entspricht dem Minimum, rot dem Maximum und grün einem mittleren neutralen Valenzwert. Für die Beurteilung der Zuordnung orientierten wir uns an den sechs Basisemotionen (Liebe, Überraschung, Freude, Wut, Trauer und Angst), wobei wir die Zuweisung der Emotionsdimensionen auf die Farbdimensionen so wählten, dass Liebe einem Rot- / Pinkton entspricht, um der tradierten Farb-Emotions-Zuweisung in der westlichen Kultur zu entsprechen (Abbildung 1a). Eine informelle Studie zeigte, dass diese Form der Zuordnung als intuitiv bewertet wurde. Daneben zeigte sich in Übereinstimmung mit unserer Farbzuweisung, dass negative Gefühle eher dunkel sind und kühlen Farbtönen wie blau oder grün zugeordnet werden (Abbildung 1b), dagegen positive Gefühle eher hell sind und mit warmen Farbtönen wie gelb oder orange in Verbindung gebracht werden (Abbildung 1c). Grundlegend für die Funktionsweise des Tools ist die Eingabe eines Textes. Der Nutzer kann aus vorgegebenen Texten verschiedenster Länge wählen oder einen eigenen Text innerhalb eines Textfeldes platzieren. Nach der serverseitigen Textanalyse sind verschiedene statische und interaktive Darstellungen verfügbar. Die Darstellung der Emotionsanalyse teilt sich in drei Bereiche auf: Makroansicht, Textansicht und Mikroansicht (Abbildung 2). Die drei Bereiche sind miteinander verlinkt. Grundlegend für die dynamische Önderung einer Ansicht  ist die Auswahl von einzelnen Emotionen bzw. Wörtern oder einem Bereich innerhalb einer Emotionsdimension. Die Die Die ",de,Tools Datengrundlage Emotion webeinträg nutzen hingegen Ziel benutzerdefiniert Eingabe Text ermöglichen zusätzlich sowohl differenziert nachvollziehbar farblich Repräsentatione Emotion verwenden Farbkodierung Emotion gut Orientierung innerhalb visuell Darstellung scheinen etabliert Methode lassen vorliegend Kodierungssystem fundiert Emotionsbeschreibung Sättigung Helligkeit minima Maxima analog Dominanz Erregung beschreiben stellen farbweren Kontinuierliche farbverlauf dar Farbwertannäherung rändern vermeiden Farbzuweisung farbwertbereich bewusst ausgesparen sodass Grenze minima Maxima deutlich hervortritt blau entsprechen Minimum rot maximum grün mittlerer neutralen valenzweren Beurteilung Zuordnung orientieren basisemotioner Liebe überraschung Freude Wut Trauer Angst wobei Zuweisung Emotionsdimension Farbdimension wählen lieben Pinkton entsprechen tradiert westlich Kultur entsprechen Abbildung informell Studie zeigen Form Zuordnung intuitiv bewerten zeigen Übereinstimmung Farbzuweisung negativ Gefühl eher dunkel kühl farbtönen blau grün zuordnen Abbildung positiv Gefühl eher hell warm farbtönen gelb Orange Verbindung bringen Abbildung grundlegend Funktionsweise Tools Eingabe Text nutzer vorgegeben Text verschiedenst länge wählen Text innerhalb textfeld platzieren serverseitig Textanalyse verschieden statisch interaktiv Darstellung verfügbar Darstellung Emotionsanalyse teilen Bereich makroansichen textansichen mikroansichen Abbildung Bereich miteinander verlinken grundlegend dynamisch Önderung Ansicht Auswahl einzeln Emotion wörtern Bereich innerhalb Emotionsdimension,"[('emotion', 0.23293213516822167), ('farbtönen', 0.18807849621478726), ('farbzuweisung', 0.18807849621478726), ('emotionsdimension', 0.18807849621478726), ('minima', 0.17518086067300706), ('maxima', 0.16602984262437817), ('grün', 0.15893176157918634), ('abbildung', 0.14643412712128745), ('blau', 0.14398118903396906), ('gefühl', 0.14398118903396906)]"
2016,DHd2016,vortraege-040.xml,Attribuierung direkter Reden in deutschen Romanen des 18.-20. Jahrhunderts. Methoden zur Bestimmung des Sprechers und des Angesprochenen,"Markus Krug (Universität Würzburg, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland); Isabella Reger (Universität Würzburg, Deutschland); Luisa Macharowsky (Universität Würzburg, Deutschland); Lukas Weimer (Universität Würzburg, Deutschland); Frank Puppe (Universität Würzburg, Deutschland)","Sprechererkennung, NLP, Quantitative Textanalyse","Programmierung, Inhaltsanalyse, Strukturanalyse, Modellierung, Annotieren, Literatur, Personen"," Eine der ersten Arbeiten auf diesem Gebiet ist das System ESPER (Zhang et al.  2003), das direkte Reden innerhalb von Kindergeschichten erkennen soll. Das  System extrahiert zunächst die direkten Reden im Text und klassifiziert diese  mit einem Entscheidungsbaum in zwei Kategorien, Sprecherwechsel bzw. kein  Sprecherwechsel. Evaluiert werden die Ergebnisse mit zwei manuell annotierten,  sehr unterschiedlichen Geschichten. Sie berichten eine Genauigkeit, gemeint ist  hier die Anzahl der korrekt bestimmten Sprecher für alle direkten Reden, von  47.6% und 86.7%. Glass und Bangay (2006), ebenfalls regelbasiert, bestimmen  zunächst für eine direkte Rede das Kommunikationsverb und anschließend eine  Menge von Akteuren, woraus letztendlich der Sprecher bestimmt wird. Sie  evaluieren ihre Techniken auf 13 englischsprachigen fiktionalen Werken und  berichten eine Genauigkeit von 79.4% (Glass / Bangay 2007). Iosif und Mishra  (2014) ¬†folgen im Prinzip dem Schema von Glass und Bangay (2007), ergänzen es  aber durch eine aufwendigere Vorverarbeitung einschließlich  Koreferenzresolution. Sie erreichen eine Genauigkeit von ca 84.5% und zählen  damit zu den besten bisher veröffentlichten Ergebnissen. Ruppenhofer und andere  (Ruppenhofer et al. 2010) berichten einen F-Score von 79% in der Zuordnung von  Politikern zu ihren Aussagen in deutschsprachigen Kabinettsprotokollen aus den  Jahren 1949-1960. Neben diesen regelbasierten Ansätzen werden auch maschinelle Lernverfahren eingesetzt. Zu den ersten erfolgreichen Systemen zählt das von Elson und McKeown (2010). Ihre Daten für die Sprecherzuordnung ließen sie über Amazons Die Zuordnung einer angesprochenen Figur wurde unserer Wissens noch in keiner anderen Arbeit untersucht. Für diese Arbeit verwenden wir Abschnitte des frei zugänglichen Korpus DROC. DROC besteht aus 89 Romanausschnitten, jeweils 130 Sätze lang, in denen alle Figurenreferenzen (mit und ohne Namen) und Koreferenzen annotiert sind. Aus dem Korpus wurden 77 Ausschnitte ausgewählt und mit einem eigens entwickelten Tool alle direkten Reden sowie die zugehörigen Sprecher und angesprochenen Figuren eingetragen. Jeder Text wurde von einem Annotator bearbeitet; eine zweite Annotation ist vorgesehen. Insgesamt wurden so 2264 direkte Reden mit Sprecher und Angesprochenen annotiert. Für die in Abschnitt 5 diskutierten Experimente wurde das Korpus in drei zufällige Mengen aufgeteilt: Wir verwenden regelbasierte Verfahren und maschinelle Lernverfahren, aber anders als in (He et al. 2013) oder (O""Keefe et al. 2012) dienen erstere nicht nur als Baseline-Verfahren, sondern wurden soweit wie möglich optimiert. Wir verwenden die Techniken 2-Way Klassifikation und N-Way Klassifikation wie in (O""Keefe et al. 2012) vorgeschlagen. Zusätzlich evaluieren wir MaxEnt2WayToMatch, bei dem Kandidaten nur bis zum ersten tatsächlichen Sprecherkandidaten erzeugt werden. Für die Sprecherzuordnung und Zuordnung eines Angesprochenen sind die in dieser Arbeit verwendeten Features in Tabelle A1 im Anhang zusammengefasst. Für diese Aufgabe haben sich regelbasierte Verfahren als konkurrenzfähig mit den   aktuellen ML-Verfahren erwiesen. Sie besitzen außerden den Vorteil, dass sie   nicht so viele Trainingsbeispiele benötigen. Die Grundstruktur des Algorithmus   ist der Idee des regelbasierten Koreferenzsystems von Stanford (Lee et al. 2011)   angelehnt. Es werden eine Reihe von Regelpässen nacheinander ausgeführt. Die   Regelpässe sind gemäß ihrer Mit Hilfe der Trainingsdaten konnte eine optimale Reihenfolge der Ausführung der Regeln empirisch ermittelt werden, bei der einige Regeln auch mehrfach angewendet werden. (1)‚Üí(2)‚Üí(3)‚Üí(4)‚Üí(5)‚Üí(6)‚Üí(7)‚Üí(5)‚Üí(6)‚Üí(8) ‚Üí(9)‚Üí(5)‚Üí(6)‚Üí(7)‚Üí(10). Die Parameter für die ML-Verfahren wurden auf dem Development-Anteil der Daten optimiert und anschließend gegen die Testmenge evaluiert. Für die regelbasierten Verfahren gibt es keine Unterscheidung zwischen Trainings- und Development-Korpus. Ein Sprecher gilt als korrekt bestimmt, wenn sich der vom System bestimmte Kandidat in der selben Koreferenzkette befindet, wie die Entität, die von unserem Annotator als korrekt markiert wurde. Tabelle 2 beschreibt die Ergebnisse bei der Anwendung der Verfahren auf das Testkorpus. Unsere Experimente bestätigen die Aussagen von O""Keefe (O""Keefe et al. 2012),  dass 2Way ML-Verfahren bessere Ergebnisse in der Sprechererkennung liefern, als  korrespondierende NWay Verfahren. Analoges gilt für die Evaluation der CRFs, die  sogar beinahe den selben Wert für die Sprechererkennung liefern wie in (O""Keefe  et al. 2012). Sowohl auf dem Developmentkorpus, als auch auf dem Testkorpus  zeigen regelbasierte Ansätze deutliche Vorteile gegenüber den in dieser Arbeit  verwendeten ML-Verfahren. Es ist weiterhin ersichtlich, dass die Bestimmung des  Sprechers einfacher ist, als die Bestimmung des Angesprochenen. Wahrscheinlich  liegt das daran, dass im Fall der Sprecherzuschreibung mehr Information  vorliegt, nämlich die direkte Rede und der Kontext, während bei der Ermittlung  des Angesprochenen die direkte Rede selbst nur hilfreich ist, wenn ein  Angesprochener direkt darin vermerkt ist. Ein direkter Vergleich mit dem besten in der Literatur zu findenden Verfahren  (Almeida et al. 2014) kann direkt nicht durchgeführt werden. Berücksichtigt man  den Unterschied, der Verfahren von O""Keefe auf den Texten des WSJ und den  literarischen Texten, könnte eine Qualität von 90% Genauigkeit erreicht werden  und damit ein mit der state of the art vergleichbares, sogar möglicherweise  besseres Ergebnis. Im Gegensatz zu ihrem Verfahren ermitteln wir zudem auch noch  eine angesprochene Entität. Die Ergebnisse zeigen, dass das regelbasierte Verfahren für diese Aufgabe  deutlich bessere Ergebnisse erzielen kann als alle ML-Verfahren, die in dieser  Arbeit getestet wurden. Es ist geplant, die hier erstellte Zuordnung in die  regelbasierte Koreferenzauflösung von (Krug et al. 2015) einzuarbeiten, um diese  damit zu verbessern. Weil unsere Hauptmotivation die Verbesserung der  Koreferenzresolution ist, diese aber im Ansatz von Almeida nicht wirksam  verbessert werden konnte, haben wir darauf verzichtet, deren komplexes  Lernverfahren nachzuvollziehen. Gerade die Ergebnisse, die in Tabelle 2 zu sehen  sind, zeigen, dass mögliche Dialogsequenzen genauer untersucht werden müssen, um  diese zuverlässig erkennen und auflösen zu können. Eine genaue Dialoganalyse  vereinfacht wiederum die Korefenzauflösung, so dass eine Extraktion von  Beziehungen zwischen Personen und Attributen zu Entitäten innerhalb der Romane  möglicher erscheint.  (1)‚Üí(2)‚Üí(3)‚Üí(4)‚Üí(5)‚Üí(6)‚Üí(7)‚Üí(5)‚Üí(6)‚Üí(8) ‚Üí(9)‚Üí(5)‚Üí(6)‚Üí(7)‚Üí(10). ",de,arbeiten Gebiet System Esper zhang Et direkt reden innerhalb Kindergeschicht erkennen System extrahiern direkt Rede Text klassifizieren entscheidungsbaum kategorien Sprecherwechsel Sprecherwechsel evaluieren Ergebnis Manuell annotierten unterschiedlich Geschicht berichten Genauigkeit meinen Anzahl korrekt bestimmt Sprecher direkt reden Glass Bangay ebenfalls regelbasieren bestimmen direkt Rede Kommunikationsverb anschließend Menge Akteur woraus letztendlich Sprecher bestimmen evaluieren Technik Englischsprachig fiktionalen Werk berichten Genauigkeit Glass Bangay Iosif Mishra Prinzip Schema Glass Bangay ergänzen aufwendiger Vorverarbeitung einschließlich Koreferenzresolution erreichen Genauigkeit ca zählen veröffentlicht Ergebnis Ruppenhofer Ruppenhofer et berichten Zuordnung Politiker Aussage Deutschsprachig Kabinettsprotokoll Regelbasiert Ansatz maschinell lernverfahren einsetzen erfolgreich Systeme zählen Elson Mckeown daten Sprecherzuordnung lassen Amazons Zuordnung angesprochen Figur wissens Arbeit untersuchen Arbeit verwenden abschnitten frei zugänglich Korpus droc Droc bestehen romanausschniten jeweils Sätz Figurenreferenze Name Koreferenz annotiert Korpus Ausschnitt auswählen eigens entwickelt Tool direkt reden zugehörig sprech angesprochen Figur eintragen Text Annotator bearbeiten Annotation vorsehen insgesamt direkt Rede Sprecher Angesprochen annotiert abschneten Diskutiert experimente korpus zufällig Menge aufteilen verwend regelbasiert Verfahren maschinell lernverfahren he et o keefe et dienen erstern soweit optimieren verwenden Technik Klassifikation Klassifikation o keefe et vorschlagen zusätzlich evaluieren Kandidat tatsächlich Sprecherkandidat erzeugen Sprecherzuordnung Zuordnung Angesprochen Arbeit verwendet Feature tabell Anhang zusammengefassen Aufgabe regelbasiert Verfahren konkurrenzfähig aktuell erweisen besitzen außerden Vorteil trainingsbeispiel benötigen Grundstruktur Algorithmus Idee regelbasierten Koreferenzsystem Stanford lee et anlehnen Reihe Regelpässe nacheinander ausführen Regelpäss gemäß Hilfe Trainingsdat optimal Reihenfolge Ausführung Regel empirisch ermitteln Regel mehrfach anwenden Parameter daten optimieren anschließend Testmeng evaluieren regelbasiert Verfahren Unterscheidung Sprecher gelten korrekt bestimmen System bestimmt Kandidat selber Koreferenzkette befinden Entität unser Annotator korrekt markieren Tabell beschreiben Ergebnis Anwendung Verfahren Testkorpus experiment Bestätigen Aussage o keefe o keefe et gut Ergebnis Sprechererkennung liefern korrespondierend Nway verfahren Analoges gelten Evaluation Crf sogar beinahe selber Wert Sprechererkennung liefern o keefe et sowohl Developmentkorpus Testkorpus zeigen regelbasiert Ansatz deutlich Vorteil Arbeit verwendet weiterhin ersichtlich Bestimmung Sprecher einfach Bestimmung Angesprochen wahrscheinlich liegen Fall Sprecherzuschreibung Information vorliegen nämlich direkt Rede Kontext Ermittlung Angesprochen direkt Rede hilfreich Angesprochener direkt vermerken direkt Vergleich Literatur findend Verfahren Almeida et direkt durchführen berücksichtigen Unterschied Verfahren o keefe Text wsj literarisch Text Qualität Genauigkeit erreichen state -- -- art vergleichbares sogar möglicherweise besseres Ergebnis Gegensatz Verfahren ermitteln zudem angesprochen Entität Ergebnis zeigen regelbasiert Verfahren Aufgabe deutlich gut Ergebnis erzielen Arbeit testen planen erstellt Zuordnung regelbasiert Koreferenzauflösung krug et einzuarbeiten verbessern Hauptmotivation Verbesserung Koreferenzresolution Ansatz Almeida wirksam verbessern verzichten Komplexes lernverfahren nachvollziehen Ergebnis Tabelle sehen zeigen möglich Dialogsequenze genau untersuchen zuverlässig erkennen auflösen genau Dialoganalyse vereinfachen wiederum Korefenzauflösung Extraktion Beziehung Person attribut entität innerhalb Roman Möglicher erscheinen,"[('keefe', 0.30975126881029336), ('angesprochen', 0.26300899236295044), ('regelbasiert', 0.25186565256950566), ('direkt', 0.2320573936094363), ('sprecher', 0.17990403754964693), ('verfahren', 0.16376547464387944), ('bangay', 0.15487563440514668), ('rede', 0.15344224530690564), ('glass', 0.13671938963863073), ('et', 0.1364159450596105)]"
2016,DHd2016,vortraege-019.xml,Weibliches Erzählen im Expressionismus? Eine Stilometrie von Mela Hartwigs Prosa,"Melanie Mihm (Justus-Liebig-Universität Gießen, Deutschland)","Stilometrie, Log-Likelihood-Test, Bar Chart, Mela Hartwig","Inhaltsanalyse, Stilistische Analyse, Visualisierung, Literatur","""Die Stilanalyse ist eine Schlüsselqualifikation literarturwissenschaftlicher  Arbeit"" (Meyer 2007: 70). So die Einführung, die man im zweiten Band Diese vorliegende Stilometrie setzt sich zum Ziel, mithilfe statistischer  Analysetechniken sowohl eine quantitative als auch qualitative Stiluntersuchung  von Prosa der österreichischen Autorin Mela Hartwig zu realisieren. Bislang  liegen keine computergestützten Studien für diese Autorin der  Zwischenkriegsjahre vor. Dies soll zum Anlass genommen werden, exemplarisch die  Verwendung einer Herzmetaphorik als spezifischen Stil der Autorin zu  untersuchen. Es sollen mithilfe der Stilometrie intuitive Annahmen überprüft und  in Zusammenhang mit ausgewählten Prosatexten des literarischen Expressionismus  gesetzt werden. Hartwig verwendet das Herz mit einer tieferen Bedeutung, sie  meint nicht nur das Organ, das den Körper mit Blut versorgt. Bei den  expressionistischen Texten von Hartwig kann das Herz bei der Frau beispielsweise  für das Pendant des männlichen Gehirns stehen. Hierbei wird der Frau scheinbar  das Vermögen des Denkens abgesprochen und an Stelle des Gehirns, also dem  rationalen Vermögen des Mannes, arbeitet das Herz der Frau im Sinne eines  affektiven, emotionalen, spontanen Teils des weiblichen Körpers. Es scheint  auffällig, dass Hartwig ein binäres System von Herz 'Verstand / Gehirn und  damit einhergehend eine Gegenüberstellung von Frau 'Mann herausarbeitet. Diese  Dichotomie und das überwiegend weibliche konfliktgeladene Figurenrepertoire  sowie die Erzähltextperspektive aus Sicht von weiblichen Figuren könnte als  typisches ,weibliches Erzählen"" gefasst werden. Die erste Hypothese lautet, dass  in Mela Hartwigs gesamter Prosa die Herzmetaphorik Verwendung findet. Die zweite  Hypothese lautet, dass die Herzmetapher bei Mela Hartwig wesentlich mit  Weiblichkeit verbunden ist. Ziele des Vortrages sind das zielgruppenorientierte Präsentieren des methodischen  Vorgehens und der Zwischenergebnisse, die Erläuterung der interdisziplinären  Fragestellungen, Arbeitshypothesen und das in Zusammenhangsetzen der  Zwischenergebnisse der Stilometrie für Prosatexte von Mela Hartwig mit dem von  Moretti (2013) etablierten Es wurden zwei literarische Textkorpora manuell erstellt und für das dritte  Vergleichskorpus eine Kombination von zwei Korpora aus COSMAS II In dem Korpus von Mela Hartwig (KMH) kommt die Herzmetapher in neun von elf Texten vor. Die Auswertung des Korpus mit den expressionistischen Texten ergab, dass von 122 Texten 37 eine Herzmetapher aufweisen. Die Ergebnisse der Berechnung zeigen, dass Hartwig in 81,82 % ihrer publizierten Texte die Herzmetapher verwendet. Im Falle der expressionistischen Texte ist berechnet worden, dass 30,33 % der Prosatexte eine ähnliche Herzmetapher nachweisen. Da der G¬≤-Wert 5,78 beträgt und der zuvor festgelegte kritische Wert bei 3,84 oder größer liegt (bei einem Signifikanzniveau von 5 %), ist die Herzmetapher bei Hartwigs Texten gegenüber den expressionistischen Texten im Korpus signifikant. Auch gegenüber dem größeren, epochenübergreifenden COSMAS II-Korpus tritt die Herzmetapher signifikant häufig auf: Die Berechnung ergab einen G¬≤-Wert von 8,42 und übersteigt den kritischen Punkt. Dieser G¬≤-Wert bestätigt meine zu Beginn dieser Arbeit geäußerte Vermutung, dass es sich bei der Herzmetapher, wie sie Mela Hartwig verwendet, um eine Besonderheit ihres Schreibstils handelt. Betrachtet man den niedrigen G¬≤-Wert von 0,97, der bei der Berechnung der expressionistischen Texte (KEX) und COSMAS II herauskommt, fällt auf, dass hier eine ähnliche prozentuale Verteilung der Herzmetapher (KEX = 30,33 % und COSMAS II = 24,71 %) vorliegt. Der Wert unterschreitet den festgelegten kritischen Wert von 3,84. Dennoch sei darauf aufmerksam gemacht, dass in den Texten des Expressionismus ca. 6 % häufiger die Herzmetapher vorkommt. Für die zweite Hypothese, dass Mela Hartwig die Herzmetapher nur in Texten verwendet, bei der sie eine weibliche Erzähltextperspektive und einen weiblichen autodiegetischen Erzähler einsetzt, wurde ein Konkordanz-Plot im Barcode-Format für das Personalpronomen 'ich' erstellt. Je dunkler der Barcode-Streifen des Konkordanz-Plots ausfällt, desto häufiger tritt an der schwarzen Stelle das angesteuerte Wort der Suchanfrage auf. Das Ergebnis der Visualisierung zeigt, dass das Personalpronomen in den Texten, in denen die Herzmetapher vorkommt, sehr viel häufiger verwendet wird. In Texten, in denen die Herzmetapher nicht vorkommt, findet sich das Pronomen, wenn überhaupt, nur in der wörtlichen Rede. Diese Stilometrie brachte das Zwischenergebnis zu Tage, dass man durch die Kombination von Es werden weitere korpusbasierte Analysen angestrebt. Diese erfordern aber eine Voraussetzung: Im Zuge der ""digitale[n] Wende"" (Schöch 2014: 130) ist es weiterhin wünschenswert und erforderlich, dass immer mehr literarische Texte digital zur Verfügung stehen oder diese durch leichte und praktikable Verfahren der Texterkennung (OCR,",de,Stilanalyse Schlüsselqualifikation literarturwissenschaftlich Arbeit Meyer Einführung Band vorliegend Stilometrie setzen Ziel Mithilfe statistisch analysetechniken sowohl quantitativ qualitativ Stiluntersuchung prosa österreichisch Autorin Mela Hartwig realisieren bislang liegen computergestützt Studie Autorin Zwischenkriegsjahre Anlass nehmen exemplarisch Verwendung Herzmetaphorik spezifisch Stil Autorin untersuchen Mithilfe Stilometrie intuitiv annahmen überprüfen Zusammenhang ausgewählt Prosatext literarisch Expressionismus setzen Hartwig verwenden Herz tief Bedeutung meinen Organ Körper Blut versorgen expressionistisch Text Hartwig Herz Frau beispielsweise pendant männlich Gehirn stehen hierbei Frau scheinbar Vermögen Denken absprechen Stelle Gehirn rational Vermögen Mann arbeiten Herz Frau Sinn affektiv emotional spontan teils weiblich Körper scheinen auffällig Hartwig binär System Herz verstand Gehirn einhergehend Gegenüberstellung Frau Mann herausarbeiten Dichotomie überwiegend weiblich konfliktgeladen Figurenrepertoire erzähltextperspektiv Sicht weiblich Figur typisch weiblich erzählen fassen Hypothese lauten Mela Hartwigs gesamt Prosa Herzmetaphorik Verwendung finden Hypothese lauten Herzmetapher Mela Hartwig wesentlich Weiblichkeit verbinden Ziel Vortrag zielgruppenorientiert Präsentiere methodisch vorgehen Zwischenergebnisse Erläuterung interdisziplinären Fragestellung arbeitshypothesen zusammenhangsetzen zwischenergebnisse Stilometrie Prosatext Mela Hartwig moretti etablieren Literarische Textkorpora manuell erstellen Vergleichskorpus Kombination Korpora Cosmas ii Korpus Mela Hartwig kmh Herzmetapher Text Auswertung korpus expressionistisch Text ergeben text Herzmetapher aufweisen Ergebnis Berechnung zeigen Hartwig publiziert Text Herzmetapher verwenden Fall expressionistisch Text berechnen Prosatext ähnlich Herzmetapher nachweisen betragen zuvor festgelegt kritisch Wert groß liegen Signifikanzniveau Herzmetapher Hartwigs Text expressionistisch Text Korpus signifikant groß Epochenübergreifenden Cosmas treten Herzmetapher Signifikant häufig Berechnung ergeben übersteigen kritisch Punkt bestätigen Beginn Arbeit geäußert Vermutung Herzmetapher Mela Hartwig verwenden Besonderheit Schreibstil handeln betrachten niedrig Berechnung expressionistisch Text kex Cosmas ii herauskommen fallen ähnlich prozentual Verteilung Herzmetapher kex Cosmas ii vorliegen Wert unterschreiten festgelegt kritisch Wert dennoch aufmerksam Text Expressionismus häufig Herzmetapher vorkommen Hypothese Mela Hartwig Herzmetapher Text verwenden weiblich erzähltextperspektiv weiblich autodiegetisch Erzähler einsetzen Personalpronomen erstellen Dunkler ausfallen desto häufig treten schwarz Stelle angesteuert Wort Suchanfrage Ergebnis Visualisierung zeigen Personalpronomen Text Herzmetapher vorkommen häufig verwenden Text Herzmetapher vorkommen finden Pronomen wörtlich Rede Stilometrie bringen Zwischenergebnis Kombination Korpusbasiert Analyse anstreben Erforder Voraussetzung Zug digital n Wende schöch weiterhin wünschenswert erforderlich literarisch Text Digital Verfügung stehen leicht praktikabel Verfahren Texterkennung ocr,"[('herzmetapher', 0.5411001104776709), ('hartwig', 0.416230854213593), ('mela', 0.29136159794951505), ('expressionistisch', 0.19384374276506283), ('cosmas', 0.1664923416854372), ('weiblich', 0.15464852741720653), ('herz', 0.14697425726239927), ('gehirn', 0.12486925626407788), ('frau', 0.12413956231562547), ('text', 0.11637917197385975)]"
2016,DHd2016,posters-058.xml,Romantik im Wandel der Zeit 'eine quantitative Untersuchung,"Johannes Hellrich (Jena University Language, Information Engineering Lab, Friedrich-Schiller-Universität Jena, Deutschland); Udo Hahn (Jena University Language, Information Engineering Lab, Friedrich-Schiller-Universität Jena, Deutschland)","Sprachwandel, Diachronie, Romantik, Google Books, Deutsch","Entdeckung, Inhaltsanalyse, Modellierung, Daten, Sprache, Literatur, Text","""Romantik"" und ""romantisch"" bezeichnen heute Profanes, wie ein privates Abendessen in   edlem Ambiente bei Kerzenschein oder ein idyllisch gelegenes Hotel mit   Butzenglasscheiben, meinte aber ursprünglich Östhetisches, insbesondere   Literarisches (DWB). Dieser Bedeutungswandel kann durch die automatische Analyse   eines großen Korpus' quantifiziert werden; etwa unter Verwendung des Google Books   N-Gram Korpus, das 4% aller je gedruckten Bücher enthält (Michel et al. 2011).   Methodische Grundlage für die Analyse sind Verfahren aus der Computerlinguistik   (distributionelle Semantik), mittels derer die Bedeutung von Wörtern über den für   sie typischen Kotext (also ihren direkten textuellen Kontext) approximiert wird. Ein   aktuelles und mächtiges Verfahren ist word2vec (Mikolov et al. 2013), das auf   Forschungsarbeiten zu künstlichen neuronalen Netzen basiert (LeCun et al. 2015).   Damit gewonnene Repräsentationen können sowohl synchron als auch diachron auf ihre   Öhnlichkeit hin verglichen werden, wodurch Bedeutungswandel quantifiziert werden   kann. Dies wurde bereits am Beispiel des Englischen durch Kim et al. (2014)   demonstriert, die für einen Zeitraum von 100 Jahren Wortrepräsentationen erzeugten   und verglichen. Ein Ergebnis war die Quantifizierung des Bedeutungswandels von ""gay""   hin zu einer Bezeichnung für (männliche) Homosexualität. Zwischen den   word2vec-Repräsentationen einzelner Wörter sind zudem semantisch sinnvolle   arithmetische Operationen möglich (Mikolov et al. 2013). Beim Vergleich der modernen   und der historischen Bedeutung von ""romantisch"" und ""Romantik"" könnte somit   ermittelt werden, ob sie einander ähnlicher sind, wenn beispielsweise sexuelle   Aspekte ignoriert werden. Alternative Verfahren zur Quantifizierung von Bedeutungswandel nutzen die Kookkurrenz  von Wörtern in Bi-Grammen (Gulordava / Baroni 2011), oder einen auf Nachbarwörtern  trainierten Klassifikator (Mihalcea / Nastase 2012). Dabei kann der erste Ansatz  lediglich lokale Zusammenhänge erfassen, während der zweite vordefinierte Zeiträume  erfordert, zwischen denen ein Bedeutungsunterschied gesucht werden soll. Riedl,  Steuer und Biemann (2014) entwickelten einen distributionellen Thesaurus, der für  vordefinierte Zeiträume ähnliche Wörter gruppiert. Nachteil dieser Methoden ist  wiederum die Notwendigkeit, den untersuchten Zeitraum in Abschnitte zu unterteilen.  Vorteilhaft gegenüber word2vec ist die Möglichkeit, die einzelnen Bedeutungen  polysemer Wörter getrennt zu erfassen 'statt einer veränderlichen Gesamtbedeutung,  liegen Teilbedeutungen mit unterschiedlicher Frequenz vor. Eine semantisch  schwächere Form des quantitativen Zugangs, aber nützlich für spätere qualitative  Interpretationen, ist die Visualisierung von Kollokationen im Zeitverlauf (Jurish  2015). Die bei unseren Untersuchungen erwarteten Ergebnisse umfassen nicht nur die  zunehmende Trivialisierung der Wörter ""romantisch"" und ""Romantik"" während der  letzten 200 Jahre, sondern auch eine Reflexion des deutschen Nationalismus im 19.  und 20. Jahrhundert, von dem die Epoche der Romantik instrumentalisiert wurde  (Kremer 2007: 50-58). Dem entspricht etwa die erhöhte Frequenz von ""Romantik"" in  deutschen Texten der Zwischenkriegszeit und direkten Nachkriegszeit, die mit einer  Verdrängung des Bi-Gramms ""romantische Liebe"" einhergeht, das dafür Ende des 20.  Jahrhunderts seine maximale Popularität erreicht. Wörter mit hoher Öhnlichkeit zu ""romantisch"" im Zeitverlauf (hoher Cosinus entspricht hoher Öhnlichkeit). Geplante Folgearbeiten beinhalten, neben der geisteswissenschaftlichen Einordnung der  quantitativen Ergebnisse, den Vergleich über mehrere europäische Sprachen hinweg und  die Einbeziehung von Sentiment Analysis-Technologien, um die emotionale Ladung der  Wörter im Verlauf der Zeit abzubilden (Acerbi et al. 2013). Die beschriebenen Arbeiten sind Teil eines Promotionsvorhabens am von der Deutschen Forschungsgemeinschaft finanzierten Graduiertenkolleg ""Modell 'Romantik'"" der Friedrich-Schiller-Universität Jena.",de,Romantik romantisch bezeichnen profan privat Abendessen edl Ambiente Kerzenschein idyllisch gelegen Hotel butzenglasscheiben meinen ursprünglich östhetisch insbesondere literarisch dwb Bedeutungswandel automatisch Analyse Korpus quantifizieren Verwendung google Books korpus gedruckt bücher enthalten Michel et methodisch Grundlage Analyse Verfahren Computerlinguistik distributionellen Semantik mittels der Bedeutung wörtern typisch Kotext direkt textuell Kontext approximiert aktuell mächtig Verfahren mikolov et Forschungsarbeit künstlich neuronal Netz basieren Lecun et gewonnen Repräsentation sowohl Synchron diachron Öhnlichkeit vergleichen wodurch Bedeutungswandel quantifizieren englisch Kim et demonstrieren Zeitraum wortrepräsentation Erzeugt vergleichen Ergebnis Quantifizierung Bedeutungswandel Gay Bezeichnung männlich Homosexualität einzeln Wörter zudem semantisch sinnvoll arithmetisch operationen mikolov et Vergleich modern historisch Bedeutung romantisch Romantik somit ermitteln ähnlich beispielsweise sexuell Aspekt ignorieren alternativ Verfahren Quantifizierung Bedeutungswandel nutzen Kookkurrenz wörtern Gulordava baroni nachbarwörtern trainiert Klassifikator mihalcea Nastase Ansatz lediglich lokal zusammenhänge erfassen vordefiniert Zeiträume erfordern Bedeutungsunterschied suchen Riedl Steuer Biemann entwickeln distributionell Thesaurus vordefiniert Zeiträum ähnlich Wörter gruppieren Nachteil Methode wiederum Notwendigkeit untersucht Zeitraum Abschnitte unterteilen Vorteilhaft Möglichkeit einzeln Bedeutunge Polysemer Wörter trennen erfassen veränderlich Gesamtbedeutung liegen teilbedeutungen unterschiedlich Frequenz semantisch schwach Form quantitativ Zugang nützlich spät qualitativ Interpretation Visualisierung Kollokation zeitverlauf Jurish unser Untersuchung erwartet Ergebnis umfassen zunehmend Trivialisierung Wörter romantisch Romantik letzter Reflexion deutsch Nationalismus Jahrhundert Epoche Romantik instrumentalisieren Kremer entsprechen erhöht Frequenz Romantik deutsch Text Zwischenkriegszeit direkt Nachkriegszeit Verdrängung romantisch Liebe einhergehen Jahrhundert maximal Popularität erreichen Wörter hoch Öhnlichkeit romantisch zeitverlauf hoch Cosinus entsprechen hoch Öhnlichkeit geplant folgearbeiten beinhalen geisteswissenschaftlich Einordnung quantitativ Ergebnis Vergleich mehrere europäisch Sprache hinweg Einbeziehung Sentiment emotional Ladung Wörter Verlauf abzubilden Acerbi et beschrieben arbeiten Promotionsvorhaben deutsch Forschungsgemeinschaft finanziert graduiertenkolleg Modell Romantik Jena,"[('romantik', 0.34950450936988936), ('romantisch', 0.29125375780824114), ('bedeutungswandel', 0.2861766042772849), ('wörter', 0.19413347568867054), ('öhnlichkeit', 0.1439949008587497), ('quantifizierung', 0.12091374813151723), ('zeitverlauf', 0.12091374813151723), ('vordefiniert', 0.12091374813151723), ('mikolov', 0.11650150312329646), ('et', 0.11343019471313562)]"
2016,DHd2016,vortraege-035.xml,Operationalisierung von Forschungsfragen in CLARIN-D - Der Anwendungsfall Ernst Jünger,"Dirk Goldhahn (Universität Leipzig, Deutschland); Thomas Eckart (Universität Leipzig, Deutschland); Gerhard Heyer (Universität Leipzig, Deutschland)","Differenzanalyse, Webapplikation, Forschungsinfrastruktur, Operationalisierung, Forschungsfrage","Inhaltsanalyse, Stilistische Analyse, Visualisierung, Sprache, Forschungsprozess, Werkzeuge","Als konkretes Beispiel für die Nutzung einer solchen Forschungsinfrastruktur wird  im Folgenden ein Usecase vorgestellt, der zur Beantwortung einer realen  Forschungsfrage der Germanistik verschiedene Bestandteile der Infrastruktur  CLARIN nutzt (Goldhahn 2015). Dabei werden verteilte Daten und Werkzeuge  genutzt, um Ressourcen zu finden, zweckmäßig aufzubereiten, zu analysieren und  die Ergebnisse zu visualisieren. Ernst Jüngers politische Publizistik der Jahre 1919 bis 1933 liegt in einer  philologisch aufbereiteten und annotierten Edition (Berggötz 2001) vor. Die  Relevanz dieser Texte liegt in der Vielzahl behandelter Themen begründet, die  relevant für die Entwicklung Deutschlands in den zwanziger und frühen dreißiger  Jahren sind. Dies umfasst unter anderem Fronterfahrungen, Konsequenzen des  verlorenen Krieges sowie das Thema der nationalen Neuorientierung. Dabei ändern  Jüngers Texte in den 15 Jahren ihrer Erstellung deutlich thematische Prioritäten  und linguistische Form (Gloning 2016). Schlüsselfragen, die aus linguistischer und diskurshistorischer Perspektive bezüglich dieses Korpus bestehen, umfassen eine mögliche Korrelation der Sprachverwendung auf Wortebene mit den konkreten Themen, die in den Texten behandelt werden. Dabei sollte das lexikalische Profil Jüngers über die Dimension Zeit charakterisiert und mit den lexikalischen Profilen zeitgenössischen Materials (wie zum Beispiel Zeitungstexte der 1920er oder Werke anderer Autoren der gleichen Zeit) abgeglichen werden. Um diese Forschungsfragen systematisch zu beantworten, müssen sie zuerst operationalisiert werden. Wichtige Aspekte dieses Prozesses sind: Fokus der Operationalisierung wird auf der Nutzung der CLARIN Infrastruktur liegen, um relevante Daten und Algorithmen zu suchen und die Analyse durchzuführen. Dabei werden zuerst Texte gesucht, die für die Forschungsfrage von Relevanz sind. Das Korpus von Ernst Jüngers politischer Publizistik der Jahre 1919 bis 1933, das unter anderem auch die Veröffentlichungsdaten aller Texte enthält, dient dabei als Startpunkt. Für den eigentlichen Vergleich wird eine konkrete Analysemethode benötigt. Eine  Möglichkeit ist hier die Nutzung einer sogenannten Differenzanalyse (Heyer et  al. 2008). Dabei können Unterschiede zwischen Jüngers Texten unterschiedlicher  Jahre oder zwischen Jüngers Texten und Referenzkorpora untersucht werden. Dies erlaubt uns die: Eine Voraussetzung für die Durchführung einer Differenzanalyse ist die Verfügbarkeit von Referenzmaterial. Für die Suche nach entsprechenden Textdaten bietet sich das bereits erwähnte CLARIN Virtual Language Observatory an. Durch die Einschränkung der vorhandenen Ressourcen des VLO über facettierte und Volltextsuche auf Korpora in deutscher Sprache des 20. Jahrhunderts stellt sich das DWDS Kernkorpus als relevante Ressource heraus (Abbildung 1). Das DWDS Korpus (Geyken 2006) wurde an der Berlin-Brandenburgischen Akademie der Wissenschaften zwischen 2000 und 2003 erstellt. Der Hauptzweck des DWDS Kernkorpus ist der Einsatz als empirische Basis eines großen monolingualen Wörterbuches des 20. Jahrhunderts. Das Kernkorpus besteht aus ungefähr 100 Millionen laufenden Wörtern und ist weitgehend über Zeit und vier Genres balanciert. Über die DWDS Webservices wurden Texte aller Genres extrahiert. Voraussetzung für die Durchführung einer Differenzanalyse ist die Aufbereitung des Rohmaterials. Dabei müssen insbesondere die Wortfrequenzen der zugrunde liegenden Texte extrahiert werden. Damit sind vor allem Satzsegmentierung und Tokenisierung wichtige Vorverarbeitungsschritte. Darüber hinaus ist die Nutzung eines POS-Taggers zur Generierung von Wortartinformationen für erweiterte Analysen hilfreich. Für derartige Verarbeitungen ist die bereits erwähnte verteilte Umgebung WebLicht (Hinrichs et al. 2010) ein wichtiges Hilfsmittel. Abbildung 2 stellt einen Überblick über eine WebLicht-basierte Prozesskette dar. Sie importiert die Plaintext-Dateien, konvertiert diese in ein internes Format (das Text Corpus Format TCF), extrahiert Sätze und Wörter, annotiert Wortarten und zählt die Häufigkeit aller vorkommenden Wörter. Diese Verarbeitung wurde auf der Basis der Ernst Jünger Texte für die Jahre 1919 bis 1933 durchgeführt. Als Resultat stehen die Worthäufigkeiten für jedes einzelne Jahr dieser Zeitspanne zur Verfügung. Darüber hinaus wurden die Referenztexte des DWDS in 15 Jahresscheiben zerlegt und jeweils für jedes Genre ein Teilkorpus erstellt. Diese 60 Einzelressourcen wurden anschließend mittels der bereits erläuterten Prozesskette aufbereitet. Die eigentliche Analyse wurde im Anschluss mithilfe der Webanwendung Corpus Diff Die Nutzung von Worthäufigkeitslisten hat verschiedenen Vorteile: Wortlisten sind verdichtete Repräsentationen des Inhalts eines Korpus, die aufgrund ihrer geringen Größe einfach zu verarbeiten sind. Darüber hinaus unterliegen diese Informationen keinen Einschränkungen durch das Urheberrecht, da kein Zugriff auf die eigentlichen Volltexte benötigt wird. Dies bedeutet, dass in den meisten Fällen selbst für Ressourcen mit sehr restriktiven Lizenzbedingungen ein Austausch dieser Daten unbedenklich ist. Über die Weboberfläche kann ein Nutzer alle relevanten Einstellungen vornehmen:   Auswählen einer Korpusmenge, des zu nutzenden Öhnlichkeitsmaßes und wie viele   der häufigsten Wörter für die Analyse genutzt werden sollen (s. Abbildung 3).   Als Resultat wird dem Benutzer eine Matrixdarstellung der paarweisen   Korpusähnlichkeit mit verschiedenen Farbschemata präsentiert. Diese Farbschemata   werden zur Betonung ähnlicher und somit zusammengeclusteter Korpora genutzt. Ein   Dendogram stellt darüber hinaus eine Visualisierung der Korpusähnlichkeiten auf   der Basis eines Single-Linkage-Clusterings für alle genutzten Wortlisten dar.   Beide Visualisierungen, Matrix und Dendogram, sind Mittel zur Identifikation   interessanter Korpuspaare mit ungewöhnlich hoher oder niedriger   Vokabularähnlichkeit. Die beschriebene Analyse kann genutzt werden, um eine   diachrone Analyse der Önderungen über die Zeit durchzuführen, aber auch um   Korpora unterschiedlichen Genres oder unterschiedlicher Herkunft miteinander zu   vergleichen. Durch die Auswahl zweier Korpora können detailliertere Informationen über die Unterschiede ihrer Vokabulare angezeigt werden. Dies beinhaltet vor allem auch Listen von Wörtern, die in einem der Korpora signifikant häufiger oder sogar exklusiv auftreten. Beides sind wertvolle Hilfsmittel um Wörter zu identifizieren, die spezifisch für die jeweilige Ressource sind. Darüber hinaus sind diese Ergebnisse Ausgangspunkt für tiefere hermeneutische Analysen durch die jeweiligen Fachwissenschaftler. Ist der Nutzer an einem konkreten Wort interessiert, kann die Entwicklung seiner Häufigkeit über den Untersuchungszeitraum durch ein Liniendiagramm angezeigt werden. Dies ist üblicherweise relevant für wichtige Schlüsselterme der jeweiligen Texte oder Wörter, die in den vorherigen Analyseschritten als relevant herausgearbeitet wurden. Dabei kann die diachrone Entwicklung der Nutzungshäufigkeit des Wortes über verschiedene Genres hinweg einfach dargestellt werden. Abbildung 4 (links) stellt die Öhnlichkeitsmatrix und das Dendogram für Ernst  Jüngers Texte der Jahre 1919 bis 1933 dar. Unter anderen ist hier auch das  Korpuspaar der Texte von 1920 und 1927 interessant, da hier eine besonders  geringe Öhnlichkeit vorliegt. Bei der Analyse hervorstechenden Vokabulars fällt  hier unter anderem die deutlich prominentere Nutzung des Wortes ""Feuer"" in den  Texten von 1920 auf (Abbildung 4, rechts). Das Beispiel ""Feuer"" (hier vor allem in seiner militärischen Bedeutung) zeigt die  Nützlichkeit dieser Visualisierung. Sowohl in der Verwendung durch Ernst Jünger  über 15 Jahre hinweg als auch im Vergleich mit Zeitungstexten der gleichen  Periode, können Unterschiede in dessen Verwendung identifiziert werden (s.  Abbildung 5) und sind damit ein idealer Einstiegspunkt für die tiefere Analyse  durch Fachwissenschaftler. Ein zweites Beispiel für diese Form der Analyse ist das Wort ""Krieg"", das ebenfalls eine interessante Häufigkeitsverteilung aufweist. Die Verwendung dieses Wortes reflektiert das Nachwirken und die Allgegenwärtigkeit der Kriegserfahrungen in Texten dieser Zeit. Dabei ist die relative Häufigkeit in der Publizistik Ernst Jüngers deutlich höher als in Zeitungstexten. Anhand eines konkreten Anwendungsfalls der Germanistik wurde dargestellt wie sich die Infrastrukturbestandteile zu einem umfangreichen Workflow kombinieren lassen. Dabei wurden auf der Basis verteilter Ressourcen mit Hilfe einer Metadatensuchmaschine relevante Daten und Werkzeuge identifiziert und anschließend über eine föderierte Prozesskette aufbereitet. Die Analyse dieser Daten erfolgte über eine benutzerfreundliche Weboberfläche, die auch erweiterte Visualisierungsmöglichkeiten anbietet.",de,konkret Nutzung Forschungsinfrastruktur folgend Usecase vorstellen Beantwortung real Forschungsfrage Germanistik verschieden Bestandteil Infrastruktur Clarin nutzen Goldhahn verteilt daten Werkzeug nutzen Ressource finden zweckmäßig aufzubereiten analysieren Ergebnis visualisieren Ernst Jünger politisch Publizistik liegen philologisch aufbereiteten annotiert Edition Berggötz Relevanz Text liegen Vielzahl behandelt Thema begründen relevant Entwicklung Deutschland Zwanziger früh dreißig umfassen fronterfahrung Konsequenz verloren Krieg Thema national Neuorientierung ändern jüngers Text Erstellung deutlich thematisch Priorität linguistisch Form Gloning Schlüsselfrage linguistisch diskurshistorisch Perspektive bezüglich Korpus bestehen umfassen möglich Korrelation Sprachverwendung Wortebene konkret Thema Text behandeln lexikalisch Profil Jünger Dimension charakterisieren lexikalisch Profil zeitgenössisch Material Zeitungstexte werk anderer Autor gleich abgeglichen forschungsfrag systematisch beantworten operationalisieren wichtig Aspekt prozesses Fokus Operationalisierung Nutzung Clarin infrastruktur liegen relevant daten algorithmen suchen Analyse durchführen Text suchen Forschungsfrage Relevanz Korpus Ernst Jünger politisch Publizistik veröffentlichungsdaten Text enthalten dienen Startpunkt eigentlich Vergleich konkret Analysemethode benötigen Möglichkeit Nutzung sogenannter Differenzanalyse Heyer et Unterschied Jünger text unterschiedlich jüngerr Text Referenzkorpora untersuchen erlauben Voraussetzung Durchführung Differenzanalyse Verfügbarkeit Referenzmaterial Suche entsprechend Textdate bieten erwähnt Clarin virtual language observatory Einschränkung vorhanden Ressource vlo Facettiert volltextsuch Korpora deutsch Sprache Jahrhundert stellen Dwds Kernkorpus relevant Ressource heraus Abbildung Dwds Korpus geyken Akademie Wissenschaft erstellen Hauptzweck Dwds Kernkorpus Einsatz empirisch Basis monolingual Wörterbuch Jahrhundert Kernkorpus bestehen ungefähr Million laufend wörtern weitgehend Genre balancieren Dwds webservices Text Genre extrahieren Voraussetzung Durchführung Differenzanalyse Aufbereitung rohmaterials insbesondere Wortfrequenz zugrunde liegend Text extrahieren Satzsegmentierung Tokenisierung wichtig Vorverarbeitungsschritt hinaus Nutzung Generierung Wortartinformation erweitert Analyse hilfreich derartig verarbeitunge erwähnt verteilt Umgebung Weblicht Hinrichs et wichtig Hilfsmittel Abbildung stellen Überblick Prozesskette dar importieren konvertieren intern Format Text Corpus Format Tcf extrahiern Sätz wört annotiert Wortarten zählen Häufigkeit vorkommend Wörter Verarbeitung Basis Ernst jüng Text durchführen Resultat stehen Worthäufigkeite jeder einzeln Zeitspanne Verfügung hinaus Referenztexte Dwds jahresscheibe zerlegt jeweils jeder Genre Teilkorpus erstellen einzelressourcen anschließend mittels erläutert Prozesskette aufbereiten eigentlich Analyse Anschluss Mithilfe Webanwendung Corpus Diff Nutzung Worthäufigkeitslist verschieden Vorteil wortli verdichtet Repräsentation inhalts korpus aufgrund gering Größe einfach verarbeiten hinaus unterliegen Information Einschränkung urheberrechen Zugriff eigentlich Volltexte benötigen bedeuten meister Fall Ressource restriktiv Lizenzbedingunge Austausch daten unbedenklich Weboberfläche Nutzer Relevante Einstellung vornehmen Auswähl korpusmenge nutzend öhnlichkeitsmaß häufig Wörter Analyse nutzen Abbildung Resultat Benutzer Matrixdarstellung paarweise Korpusähnlichkeit verschieden Farbschemata präsentieren Farbschemata Betonung ähnlich somit zusammengeclustet Korpora nutzen Dendogram stellen hinaus Visualisierung korpusähnlichkeien Basis genutzt wortlisn dar visualisierung matrix Dendogram Identifikation interessant Korpuspaar ungewöhnlich hoch niedrig Vokabularähnlichkeit beschrieben Analyse nutzen Diachrone Analyse önderungen durchführen Korpora unterschiedlich Genr unterschiedlich Herkunft miteinander vergleichen Auswahl zwei Korpora detailliertere Information Unterschiede vokabulare anzeigen beinhalten Liste wörtern Korpora Signifikant häufig sogar exklusiv auftreten beide wertvoll Hilfsmittel Wörter identifizieren spezifisch jeweilig Ressource hinaus Ergebnis Ausgangspunkt tief hermeneutisch Analyse jeweilig Fachwissenschaftler Nutzer konkret Wort interessieren Entwicklung Häufigkeit Untersuchungszeitraum Liniendiagramm anzeigen üblicherweise relevant wichtig Schlüsselterm jeweilig Text Wörter vorheriger analyseschritten Relevant herausarbeiten Diachrone Entwicklung Nutzungshäufigkeit Wort verschieden Genres hinweg einfach darstellen Abbildung links stellen Öhnlichkeitsmatrix Dendogram Ernst jüngers Text dar korpuspaar Text interessant gering Öhnlichkeit vorliegen Analyse hervorstechend Vokabular fallen deutlich prominenter Nutzung Wort Feuer Text Abbildung rechts Feuer militärisch Bedeutung zeigen Nützlichkeit Visualisierung sowohl Verwendung Ernst jung hinweg Vergleich zeitungstexten gleich Periode Unterschied Verwendung identifizieren Abbildung Idealer einstiegspunkt tief Analyse Fachwissenschaftler Form Analyse Wort Krieg ebenfalls interessant Häufigkeitsverteilung aufweisen Verwendung Wort reflektieren nachwirken Allgegenwärtigkeit Kriegserfahrunge Text relativ Häufigkeit Publizistik Ernst Jünger deutlich hoch zeitungstexten anhand konkret Anwendungsfall Germanistik darstellen Infrastrukturbestandteile umfangreich Workflow kombinieren lassen Basis verteilt Ressource Hilfe Metadatensuchmaschine relevant daten Werkzeug identifizieren anschließend föderiert Prozesskette aufbereiten Analyse daten erfolgen benutzerfreundlich Weboberfläche erweitert visualisierungsmöglichkeit anbieten,"[('ernst', 0.23366512394935662), ('jünger', 0.23043110186390942), ('dwds', 0.20341740469115333), ('text', 0.14726652253790903), ('ressource', 0.14006184405836414), ('dendogram', 0.13825866111834564), ('differenzanalyse', 0.13825866111834564), ('publizistik', 0.13825866111834564), ('prozesskette', 0.13825866111834564), ('analyse', 0.13676889603267142)]"
2016,DHd2016,vortraege-007.xml,Play(s): Crowdbasierte Anreicherung eines literarischen Volltext-Korpus,"Mathias Göbel (Universität Göttingen, Seminar für Deutsche Philologie); Hanna-Lena Meiners (Universität Göttingen, Seminar für Deutsche Philologie)","Crowdsourcing, Gamification, TextGrid Repository, Korpora","Strukturanalyse, Annotieren, Bereinigung, Bearbeitung, Crowdsourcing, Kollaboration, Kommentierung, Webentwicklung, Literatur","Crowdsourcing, Social Editing und viele verwandte Begriffe sind Konzepte, die   innerhalb der Digital-Humanities-Community in den vergangenen Jahren einen kleinen   Hype erfahren haben. An Umsetzungen mangelt es, während man sich noch über die   Definitionen streitet. Dabei sind die Lösungsansätze sehr vielversprechend 'allen   voran die von Methodisch betrachtet bieten Konzepte, die auf der zunehmenden Beteiligung der  sogenannten Während Play(s) sich methodisch eher in der zweiten genannten Kategorie wiederfinden  soll, ist wichtig zu betonen, dass der Erfolg solcher Projekte eng mit der  Entwicklung und dem Design selbst zusammenhängt. Die Oberfläche sollte  beispielsweise ansprechend gestaltet bzw. angemessen bezogen auf die Zielgruppe und  einfach zu bedienen sein. Als ¬†Spielelemente können Levels, das Sammeln von Punkten  in Kombination mit einfachen Spielanweisungen dienen. Neben der tatsächlichen  Entwicklung einer neuen Anwendung hängt ein großer Teil des Erfolgs von der zu  tätigenden Handlung der Teilnehmer_innen ab. Die Aufgabe, die im Rahmen eines  Crowdsourcing oder Citizien-Science-Projektes von den Teilnehmer_innen bearbeitet  werden soll, ist im besten Fall einfach zu verstehen und in simple Teilbereiche  unterteilt. Gleichzeitig unterliegt die Einbindung von freiwilligen, fachfremden  Teilnehmer_innen gewissen eher impliziten und wenig ausgesprochenen Regeln. So  sollten die Teilnehmenden generell als Partner oder Mitarbeiter_innen betrachtet  werden und nicht als günstige Arbeitskräfte. Zudem sollten sie nicht zur Bewältigung  von Aufgaben angehalten werden, die eigentlich einfacher und besser von einem  Computer ausgeführt werden könnten. Diese Grundethik sollte bei jeder Umsetzung  einer neuen Idee zumindest mitbedacht werden, um künftige Teilnehmer_innen nicht zu  verärgern oder zu verschrecken. Die wenigen bisher gesammelten und verfügbaren Erfahrungen aus Projekten für die Geisteswissenschaft sollen nun ausgewertet werden und in die Umsetzung einer neuen Projektidee eingebracht werden. ""Play(s)"" ist der Name der Anwendung, die sich damit befassen soll, ein literaturwissenschaftliches Volltext-Korpus anzureichern. Das TextGrid-Repositorium bietet dafür optimale Voraussetzungen: alle Texte sind im TEI-Format erfasst und diese Quelle ist frei zugänglich. In diesem Projekt knüpfen wir an die von einer Projektgruppe (vgl. Trilcke et al. 2015) bereits herausgefilterten Dramen des Repositoriums an. Dabei wurden bereits in einem manuellen Durchgang allen Sprecherinstanzen im Auswahlkorpus eindeutige Namen (IDs) zugewiesen, um eine Ausgangsbasis für Netzwerkanalysen zu schaffen. In diesen Vorarbeiten wurden die genuin deutschen Texte ausgewählt und dabei aus den insgesamt 666 Dramen auf 465 Werke zurückgegriffen. Um diese Analysen mit einer quantitativ und qualitativ erweiterten Quellenbasis zu vertiefen, bedarf es einer noch genaueren Referenzierung. So sollten zum Beispiel die als Sprecher auftretenden Personengruppen aufgelöst werden und zu diesen die beteiligten Akteure genannt werden. Auch eine Klassifizierung des Geschlechtes, der sozialen Stellung und weitere Features sind denkbar, um differenzierte Analysen tätigen zu können. Hier wird deutlich, dass jeder Text einer bestimmten Aufbereitung bedarf, die aber in vielen kleinen Einzelschritten erfolgen kann, da die Informationen und einzelnen semantischen Anreicherungen in ihren Kategorien unabhängig voneinander sind. Innerhalb des TextGrid-Korpus beschränken wir uns auf die Betrachtung der Dramen und  innerhalb derer sind es die Strukturinformationen, die auf Grundlage des XML-Codes  Netzwerkanalysen auf Basis des gemeinsamen Auftretens in einer Szene ermöglichen.  Gemeinsames Auftreten heißt in diesem Fall, dass innerhalb einer Szene alle  Sprecher_innen in Verbindung gebracht werden. Dazu gilt es die einzelnen Akteure  ausfindig zu machen, da das Korpus selbst keine Information, wie man sie im  TEI-Attribut Die Ursache kann drucktechnisch bedingt in den Buchausgaben liegen, in denen Sprechernamen abgekürzt werden, um Platz und Papier zu sparen, es können auch schlicht Fehler im Satz auftauchen und eine weitere Fehlerquelle kann der Digitalisierungsprozess sein. In all diesen Fällen ist die Korrekturaufgabe denkbar simpel: man muss jene Sprecher zusammenführen, bei denen es sich offensichtlich um die gleiche Person handelt. Getreu der Buchausgaben handelt es sich dabei nicht um Fehler, das Encoding muss hier schlicht um semantische Information erweitert werden, wie es das Attribute who in den TEI Guidelines vorsieht. Dazu zählen auch Fälle, in denen das Markup innerhalb des TextGrid-Korpus fehlerhaft ist. Das betrifft leere speaker-Elemente, solche, in denen noch Teile der Bühnenanweisung mit einfließen und auch jene, die noch ein leeres Element stellvertretend für zum Beispiel einen Seitenumbruch beinhalten und dadurch als Auswertung des Inhaltes von tei:speaker ein Leerzeichen voran steht. Man findet außerdem bei gemeinsam sprechenden Personengruppen unterschiedliche Nennungen. In einem Drama Friedrich Kaisers ist eine solche Aggregation mit ""HELFER UND ROBERT""  benannt, später folgt aber ""ROBERT UND HELFER"". Eine bestimmte vom Autor intendierte Hierarchie soll das Datenmodell nicht abdecken und somit gilt es die verschiedenen Zeichenketten als eine Entität zu betrachten. Zudem soll die Tiefenauszeichnung dieser Elemente weiter gehen und jeder Gruppe die einzelnen, sofern bestimmbaren, Akteure zugewiesen werden. Diesen Beobachtungen folgt die Spielstruktur. In einem ersten Level gilt es die unterschiedlich benannten aber in der fiktiven Welt gleichen Sprecher zu identifizieren. Dazu werden alle unterschiedlichen Zeichenketten innerhalb der tei:speaker-Elemente eines Dramas zunächst in der Reihenfolge ihres ersten Auftretens gelistet. Mutmaßlich gleiche Namen sind nacheinander auswähl- und abspeicherbar. Ist dies für ein Drama vollständig geschehen, kann dieses Drama als ""gelöst"" markiert werden. Weiterhin gilt es Aggregationen ausfindig zu machen (Level 2). Diese Aggregationen sollen schließlich aufgelöst werden (Level 3). Dazu sind nicht nur die an einer Gruppe beteiligten Akteure zu nennen, sondern auch deren Vollständigkeit zu deklarieren. Es kann zum Beispiel das Volk sprechen und weiterhin einzelne Personen aus dem Volk auftreten. Diese sind Teil des Volkes, die Gruppe selbst ist aber eine weitaus größere und daher unvollständig durch die einzelnen Akteure belegt. Sprechen zwei auch näher bestimmte Einzelpersonen gemeinsam, so kann diese Gruppe vollständig aufgelöst werden. Die Geschlechter der Akteure sind in Level 4 zu bestimmen. Dabei ist zu wählen aus male, female, both, und unknown. Die letzte Gruppe umfasst dann schließlich auch metaphysische Konstrukte, die personifiziert auftreten. In Level 5 sollen diese dann genauer spezifiziert werden. Dabei stehen die Kategorien Tier, metaphysisches Wesen (z. B. Gottheit, Hexen und Magier) und Eigenschaft / Gefühl / Moral zur Auswahl. Schließlich lässt sich noch der soziale Status bestimmen, sofern Berufsbezeichnungen, Adelstitel oder andere Indikatoren ausfindig zu machen sind. Zwischen den einzelnen Levels gilt es die Eingaben anderer Spieler zu verifizieren oder auch zu falsifizieren. Diese Eingabe wirkt sich auf die eigenen Punkte immer positiv aus, die jeweils anonym bleibende begutachtete Spielerin wird bei Fehleingaben aber Punktabzüge bekommen. Da die Dramen immer zufällig gewählt werden und auch mehrfach erfasst werden, stehen damit verschiedene Qualitätskontrollen zur Auswahl, die auch kontinuierliche nicht sinnvolle Eingaben erkennen lassen. Die betreffenden Spielerinnen können weiterspielen, finden aber nur noch eine persönliche Highscoreliste vor, während sie aus den Highscorelisten anderer getilgt werden und ihre Eingaben auch nicht in den weiteren Forschungsprozess Einzug halten. Zudem stehen die Daten für 465 Dramen im Kritisch betrachtet stammen aus der Welt der Computerspiele die Levelstruktur und einzelne Elemente, wie Avatar und Highscoreliste. Tatsächlich ist das Angebot eines, das Social Editing auf einfachste Fragestellungen hin anwendet und jeder Spielerin die Möglichkeit bietet, aktiv an der Tiefenerschließung von literarischen Texten mitzuwirken. Außerdem gibt es einen didaktischen Aspekt, da komplexe Probleme im Hinblick auf Korpuserstellung implizit aufgezeigt werden.",de,Crowdsourcing social Editing verwandter begriffe Konzept innerhalb Hype erfahren Umsetzungen mangeln Definition streiten lösungsansätz vielversprechend voran methodisch betrachten bieten Konzept zunehmend Beteiligung sogenannter Play s methodisch eher genannt Kategorie wiederfinden wichtig betonen Erfolg Projekt eng Entwicklung Design zusammenhängen Oberfläche beispielsweise ansprechend gestalten angemessen beziehen zielgruppe einfach bedienen Level sammeln Punkt Kombination einfach Spielanweisung dienen tatsächlich Entwicklung Anwendung hängen Erfolg tätigend Handlung Aufgabe Rahmen Crowdsourcing bearbeiten Fall einfach verstehen Simple teilbereich unterteilen gleichzeitig unterliegen Einbindung Freiwillig fachfremden gewiß eher implizit ausgesprochenen regeln Teilnehmende generell Partner betrachten günstig arbeitskräfte zudem Bewältigung Aufgabe anhalten eigentlich einfach Computer ausführen können Grundethik Umsetzung Idee zumindest mitbedachen künftig verärgern verschrecken weniger gesammelt verfügbar Erfahrung Projekt Geisteswissenschaft auswerten Umsetzung Projektidee einbringen play s Name Anwendung befassen literaturwissenschaftlich anreichern bieten optimal Voraussetzung Text erfasst Quelle frei zugänglich Projekt knüpfen Projektgruppe trilcken et herausgefiltert dramen Repositorium manuell durchgang Sprecherinstanze Auswahlkorpus eindeutig Name ids zuweisen Ausgangsbasis netzwerkanalysen schaffen vorarbeiter genuin deutsch Text auswählen insgesamt Dram Werk rückgegriffen Analyse quantitativ qualitativ erweitert quellenbasis vertiefen bedürfen genau Referenzierung Sprecher auftretend Personengruppe auflösen beteiligt Akteur nennen Klassifizierung Geschlecht sozial Stellung Feature denkbar differenziert Analyse Tätig deutlich Text bestimmt Aufbereitung bedürfen einzelschritter erfolgen Information einzeln semantisch Anreicherung Kategorie unabhängig voneinander innerhalb beschränken Betrachtung Dram innerhalb der Strukturinformation Grundlage netzwerkanalysen Basis gemeinsam Auftreten Szene ermöglichen gemeinsam auftreten Fall innerhalb Szene Verbindung bringen gelten einzeln Akteur ausfindig Korpus Information Ursache drucktechnisch bedingt buchausgab liegen Sprechername abkürzen Platz Papier sparen schlicht fehl Satz auftauchen Fehlerquelle digitalisierungsprozess all Fall Korrekturaufgabe denkbar Simpel Sprecher zusammenführ offensichtlich gleich Person handeln getreu Buchausgab handeln Fehler Encoding schlicht semantisch Information erweitern attribut Who Tei guidelines vorsehen zählen Fall Markup innerhalb fehlerhaft betreffen leer Teil Bühnenanweisung einfließen leer Element stellvertretend Seitenumbruch beinhalten Auswertung inhalt tei speaker Leerzeichen voran stehen finden gemeinsam sprechend Personengrupp unterschiedlich Nennung Drama Friedrich Kaiser Aggregation helfer Robert benennen folgen Robert Helfer bestimmte Autor intendiert Hierarchie datenmodell abdecken somit gelten verschieden Zeichenkett Entität betrachten zudem Tiefenauszeichnung Elemente Gruppe einzelner sofern bestimmbaren Akteur zuweisen Beobachtung folgen Spielstruktur Level gelten unterschiedlich benannen fiktiv Welt gleich Sprecher identifizieren unterschiedlich Zeichenkett innerhalb tei Drama Reihenfolge Auftreten listen mutmaßlich gleich Name nacheinander abspeicherbar Drama vollständig geschehen Drama lösen markieren weiterhin gelten aggregation ausfindig Level Aggregation schließlich auflösen Level Gruppe beteiligt Akteur nennen Vollständigkeit deklarieren Volk sprechen weiterhin einzeln Person Volk auftreten Volk Gruppe weitaus groß unvollständig einzeln Akteur belegen sprechen nah bestimmt Einzelperson gemeinsam Gruppe vollständig auflösen geschlechter Akteur Level bestimmen wählen Mal female Both Unknown letzter Gruppe umfassen schließlich metaphysisch Konstrukt personifizieren auftreten Level genau spezifizieren stehen kategorie Tier metaphysisch Wesen Gottheit hex Magier eigenschaft Gefühl Moral Auswahl schließlich lässen sozial Status bestimmen sofern berufsbezeichnungen Adelstitel indikatoren ausfindig einzeln Levels gelten Eingabe anderer Spieler verifizieren falsifizieren Eingabe wirken Punkt positiv jeweils anonym bleibend begutachtet Spielerin Fehleingabe punktabzüge bekommen Dram zufällig wählen mehrfach erfasst stehen verschieden qualitätskontrollen Auswahl kontinuierlich sinnvoll Eingabe erkennen lassen betreffend Spielerinn weiterspielen finden persönlich Highscoreliste Highscorelist anderer tilgen Eingabe Forschungsprozess einzug halten zudem stehen daten Dram Kritisch betrachten Stammen Welt Computerspiele Levelstruktur einzeln elemenen Avatar highscoreliste tatsächlich Angebot Social editehen einfach Fragestellung anwenden Spielerin Möglichkeit bieten aktiv Tiefenerschließung literarisch Text mitwirken didaktisch Aspekt komplex Problem Hinblick Korpuserstellung implizit aufzeigen,"[('akteur', 0.22060014429929012), ('level', 0.196505875040717), ('eingabe', 0.1355674861759317), ('gruppe', 0.13414364437006157), ('auftreten', 0.13282907893674156), ('aggregation', 0.12339355628182959), ('volk', 0.12339355628182959), ('ausfindig', 0.11944236553027032), ('innerhalb', 0.11529753119072313), ('auflösen', 0.11300067324665754)]"
2016,DHd2016,vortraege-051.xml,Der falsche Quijote? Autorschaftsattribution für spanische Prosa der frühen Neuzeit.,"Nanette Rißler-Pipka (Universität Siegen, Deutschland)","Stilometrie, Autorschaftsattribution, Spanien, Cervantes, Don Quijote","Kontextsetzung, Bewertung, Stilistische Analyse, Daten, Forschung, Text","Das bis heute bekannteste Werk der spanischen Literatur ist weltweit als ""Don Quijote"" von Miguel de Cervantes geläufig. Dass Cervantes aber, wie kurz zuvor Mateo Alem√°n, mit einem Fälscher zu kämpfen hatte, der den ersten Band des ""Quijote"" ungefragt weiter dichtete und ein Jahr vor der eigenen Fortsetzung durch Cervantes einen zweiten Band aus eigener Feder unter dem Namen Alonso Fern√°ndez de Avellaneda heraus brachte, wissen die wenigsten Leser des ""Quijote"". Es ist im strengen Sinne des Wortes auch keine Fälschung oder ein Plagiat, sondern die freie Fortsetzung eines erfolgreichen Romans unter eigenem Namen bzw. in diesem Fall unter Pseudonym. Die Identität Avellanedas ist bis heute unbekannt. Angesichts der historischen Publikationsbedingungen in Spanien der frühen Neuzeit  (Chartier 2006), stellt sich die Frage, ob eine Autorschaftsattribution mithilfe  stilometrischer Methoden auf der Grundlage aktuell zugänglicher digitalisierter  Buchausgaben überhaupt möglich ist. Doch auch Patrick Juola und Christopher  Coufal nahmen 2010 die Ausgabe des ""Don Quijote"", die im Project Gutenberg  zugänglich ist als Grundlage ihrer Analyse, die als Ergebnis hatte, dass  Cervantes nicht der Autor der letzten 69 (von 74) Kapitel des 2. Bandes des ""Don  Quijote"" sei (Coufal / Juola 2010). Aus dieser provokanten These entwickelte  sich aber keine wissenschaftliche Debatte innerhalb der internationalen  Hispanistik und auch die DH-Experten Juola und Coufal erweiterten ihre  Fragestellung nicht auf die sich unmittelbar anschließende Frage, wer der Autor  des ""falschen"" Quijotes von Avellaneda sei. Auch eine weitere statistische  Untersuchung der beiden Teile des ""Quijote"" von Cervantes berücksichtigt nicht  Avellaneda (López Quintero 2011). Die Tatsache, dass sich der Stil Cervantes'  innerhalb des zweiten Teils des ""Quijote"" ändert, ist in der Hispanistik  anerkannt und wird im Allgemeinen mit dem Erscheinen der apokryphen Fortsetzung  durch Avellaneda in kausalen Zusammenhang gebracht (Strosetzki 1991: 93;  Ehrlicher 2008: 42ff.; Blasco 2007: XVII; Gómez Canseco 2008). Bislang wurde  jedoch der Stil Cervantes' gerade im Vergleich zu seinem Nachahmer Avellaneda  zumeist als in jeder Hinsicht überragend dargestellt (vgl. zu einer kompakten  Darstellung dieser Missachtung Avellanedas: Alvarez Roblin 2014). Erst durch die  jüngste Reihe von neuen Ausgaben der Avellaneda-Fortsetzung wird dessen  literarische Leistung in der Fachwelt anerkannt (Gómez Canseco 2014; Alvarez  Roblin 2009; Su√°rez Figaredo 2014; López-V√°zquez 2011). Mit dem wachsenden  Interesse an Avellaneda nimmt auch die Suche nach dessen Identität mithilfe  digitalisierter Korpora zu. Zum größten Teil stützen sich diese  Autorschaftsattributionen auf schlichte Recherchemöglichkeiten von CORDE (Corpus  diacrónica del espa√±ol), CREA (Corpus de Referencia del Espa√±ol Actual) und  GoogleBooks (Su√°rez Figaredo 2011; Madrigal 2009; López-V√°zquez 2011; Blasco  2005; Jiménez 2007) oder auf philologisch-historische Recherchen (Cruz Casado  2008; S√°nchez Portero 2006). Insgesamt werden im Laufe der Recherche nach der  wahren Identität Avellanedas 39 Namen ins Spiel gebracht. Von 18 dieser  Kandidaten liegen digitalisierte Texte frei zugänglich vor. Dennoch nutzt keiner  der Autorschaftsdetektive aktuelle Methoden der DH zur Autorschaftsattribution  (wie z. B. JGAAP oder Stilometrie mit R; vgl. Juola 2012; Eder 2015). Neben der  Autorschaftsattribution bzgl. Avellanedas apokrypher Fortsetzung des ""Quijote"",  stellt die Hauptfrage dieses Papers, die stilistische und stilometrische  Unterscheidung zwischen Cervantes und Avellaneda dar. Es kann gezeigt werden,  dass nur mithilfe stilometrischer Methoden, die festgefahrene Fachdiskussion  neue Perspektiven und unerwartete Ergebnisse erhält. Mithilfe des stylo-Pakets für R (Eder / Rybicki 2011), das für spanischsprachige Texte noch vergleichsweise wenig getestet wurde, sollen zum einen die vorliegenden Autorschaftsattributionen falsifiziert und die Fragen nach stilistischer Nähe zwischen Cervantes, Avellaneda und anderen zeitgenössischen Autoren spanischer Prosa geklärt werden. Somit wird auch die Methode hinlänglich ihrer Komptabilität mit spanischsprachigem Korpus überprüft. Dazu konnte die neueste Version des stylo-package (0.6.0) und die von Jannidis et al. (2015) vorgestellte Cosine Distance genutzt werden. Zu diesem Zweck wurde zunächst ein passendes Korpus erstellt, das repräsentativ  für die Zeit von 1585-1630 spanische Prosawerke enthält und sich auch aus den  Kandidaten für die Autorschaft des apokryphen ""Quijote"" zusammensetzt. Die Texte  stammen aus digitalen Editionen von cervantesvirtual, Wikisource und Project  Gutenberg. Sie wurden einheitlich in plain-text-Format abgespeichert und von  Textteilen, die nicht von selben Autor stammen (wie z. B. einleitende  Bemerkungen des Herausgebers) befreit. Um zunächst ein sicheres Set zu haben, das sowohl die Autorschaft als auch Genre und Epoche betreffend vergleichbar ist, wurden zunächst nur 4 Autoren mit unterschiedlich vielen Texten ausgewählt (insgesamt 32 Texte). Eine Vergleichbarkeit die Textlänge betreffend hätte bedeutet entweder nur die Novellen oder nur die Romane miteinander zu vergleichen. Da es auch mit unterschiedlicher Textlänge zu guten Ergebnissen kam, wurde auf diese Angleichung verzichtet (vgl. Eder 2010). Das Korpus wurde mit zwei verschiedenen und anerkannten Distanzmaßen (Eder""s Delta und Cosine) und 100-5000 MFW als Cluster-Analyse ausgewertet. Schrittweise wurden dann weitere Kandidaten hinzugefügt und die Ergebnisse bewertet. Die Problematik, die sich dabei ergab, war, dass es wenig Sinn macht, Autoren mit ins Korpus zu nehmen, von denen nur ein Textbeispiel vorliegt, da diese keinem ""Partner"" im Dendrogramm zugeordnet werden können und somit fälschlicherweise eigentlich weiter voneinander entfernte Texte zusammen geclustert dargestellt werden. Um dieses Problem der Cluster-Analyse zu umgehen, wurde im Vergleich eine Principle Component Analysis (PCA) durchgeführt, die eine bessere Darstellung der Distanzen zwischen den einzelnen Texten im Raum zeigt. Im ersten Korpus mit 32 Texten funktioniert die Zuordnung sehr gut. Die Cervantes-Texte sind trotz ihrer starken Größenunterschiede (Novellen mit ca. 7000 und Romane mit ca. 200.000 Wörtern) klar zusammen geclustert. Ab 900 MFW und darunter ist die Autorschaftszuordnung mit Eder""s Delta einwandfrei, bei darüber liegenden Zahlen schlich sich beständig der Nachahmer-""Lazarillo"" von Cortés de Tolosa und auch der Block der Werke von Castillo Slórzano zwischen die Cervantes-Werke. Zum Vergleich wurde der gleiche Versuch mit Cosine Distance durchgeführt und brachte keine größere Veränderung in der Darstellung. Weiterhin blieb der Roman von Cortés de Tolosa hartnäckig bis zum 300 MFW unter den Cervantes-Werken, jedoch konnte Castillo Solórzano früher heraus genommen werden. Interessant ist hier, dass die PCA mit denselben Variablen zeigt, wie weit entfernt der ""Lazarillo"" von Cortés de Tolosa doch von den übrigen Werken Cervantes"" entfernt ist (und zwar bei allen 100-5000 MFW): Deutlich wird vor allem, dass der ""Lazarillo"", den Cortés Tolosa in Nachahmung des Originals (anonym 1554) geschrieben hat, sehr von seinem übrigen Werk abweicht, aber ebenso noch deutlich von den Cervantes-Werken entfernt ist. Das erklärt sich allerdings leicht durch die sprachliche Unterscheidung der vorliegenden Edition des Werkes, die eine ältere Form des Kastilischen verwendet und daher in der Vergleichbarkeit eingeschränkt bleibt. Spannender werden die Ergebnisse, wenn man Avellaneda und weitere Kandidaten hinzunimmt: Auch mit der Korpuserweiterung bleiben die Zuordnungen relativ stabil (außer Cortés de Tolosa, wie zuvor). Keiner der möglichen Kandidaten wird jedoch Avellaneda zugeordnet, sondern der apokryphe ""Quijote"" wird mit den anderen beiden Teilen von Cervantes zusammen geclustert 'dies bleibt über 100-5000 MFW konstant. Auch mit dem einer PCA bestätigt sich dieses Ergebnis (vgl. Fig. 8: der blaue Punkt bei den schwarzen zeigt Avellaneda im Umkreis der Cervantes-Werke): In einer nächsten Korpuserweiterung nehmen wir weitere Kandidaten mit Prosabeispielen hinzu, von denen nur ein Text zur Verfügung steht. Es wird schnell deutlich, dass die Einzelgänger unter den Texten, das zuvor stabile Gefüge auseinanderbringen und offensichtlich nicht für die statistische Untersuchung zu gebrauchen sind. Erstaunlich bleibt jedoch, dass selbst in diesem sehr vagen Clustering (je nach MFW schieben sich die einzelnen Texte mal dort mal dort hin), die drei ""Quijote""-Texte konsistent bei 100-5000 MFW ein Cluster bilden. Kein anderer Kandidat schafft es in die Nähe von Avellaneda. Auch die PCA bringt keine entscheidenden Vorteile gegenüber der Cluster Analysis. Espinel und √öbeda gruppieren sich zwar in die Cervantes-Gruppe, aber nicht direkt zu Avellaneda. Wenn überhaupt ein Autorname anstelle desjenigen Avellanedas genannt werden  sollte, müsste es nach den vorliegenden Ergebnissen Cervantes selbst sein, der  in einem gekonnten Spaß ganz im Sinne seines Quijote die Leser mit der eigenen  falschen Fortsetzung an der Nase herum führt. Oder aber beide Autoren haben  ihren Stil gegenseitig aufeinander abgestimmt, dass sie sich derart im  Wortgebrauch ähneln. Es würde sich als folgende Untersuchung ein rolling delta  anbieten, um eine kollaborative Autorschaft ausmachen zu können und um den  vorgeblichen Stilwechsel im zweiten Teil von Cervantes' ""Quijote"" genauer  definieren und mit Avellaneda in Zusammenhang bringen zu können.",de,bekannt Werk spanisch Literatur weltweit don Quijote Miguel de cervantes geläufig cervantes zuvor mateo n Fälscher kämpfen Band quijot ungefragen dichtet Fortsetzung cervantes Band Feder Name alonso Ndez -- avellaneda heraus bringen wissen wenigster Leser quijot streng Sinn Wort Fälschung Plagiat frei Fortsetzung erfolgreich Roman eigen Name Fall Pseudonym Identität Avellanedas unbekannt angesichts historisch Publikationsbedingung Spanien früh Neuzeit Chartier stellen Frage Autorschaftsattribution Mithilfe stilometrisch Methode Grundlage aktuell zugänglich digitalisiert Buchausgab Patrick Juola Christopher Coufal nehmen Ausgabe don Quijote Project gutenberg zugänglich Grundlage Analyse Ergebnis cervantes Autor letzter Kapitel band Don Quijote coufal Juola provokant These entwickeln wissenschaftlich Debatte innerhalb international Hispanistik Juola Coufal erweitern Fragestellung unmittelbar anschließend Frage Autor falsch quijotes Avellaneda statistisch Untersuchung Teil Quijote cervantes berücksichtigen Avellaneda lópez Quintero Tatsache Stil cervantes innerhalb Teil Quijote ändern Hispanistik anerkennen erscheinen apokryph Fortsetzung Avellaneda kausal Zusammenhang bringen Strosetzki ehrlich Blasco xvii Gómez Canseco bislang Stil cervantes Vergleich Nachahmer Avellaneda zumeist Hinsicht überragend darstellen kompakt Darstellung Missachtung Avellanedas Alvarez Roblin jung Reihe Ausgabe literarisch Leistung Fachwelt anerkennen Gómez Canseco Alvarez Roblin Rez figaredo zquez wachsend Interesse Avellaneda nehmen Suche Identität Mithilfe Digitalisierter Korpora groß stützen Autorschaftsattribution schlicht Recherchemöglichkeite Corde Corpus Diacrónica del Crea Corpus de referencia del actual googlebooks Rez Figaredo madrigal zquez Blasco Jiménez recherch Cruz Casado Nchez Portero insgesamt Lauf Recherche wahr Identität avellaneda Name Spiel bringen Kandidat liegen digitalisiert Text frei zugänglich dennoch nutzen autorschaftsdetektiv aktuell Methode dh Autorschaftsattribution Jgaap Stilometrie r Juola eder Autorschaftsattribution Avellanedas apokryph Fortsetzung quijot stellen Hauptfrage Paper stilistisch stilometrisch Unterscheidung cervantes Avellaneda dar zeigen Mithilfe Stilometrischer Methode festgefahren Fachdiskussion Perspektive unerwartet Ergebnis erhalten Mithilfe r Eder Rybicki spanischsprachig Text vergleichsweise testen vorliegend Autorschaftsattribution falsifizieren Frage stilistisch Nähe cervantes Avellaneda zeitgenössisch Autor spanisch Prosa klären somit Methode hinlänglich Komptabilität spanischsprachig Korpus überprüfen neu Version Jannidis et vorgestellt Cosine Distance nutzen zweck passend Korpus erstellen repräsentativ spanisch Prosawerke enthalten Kandidat Autorschaft apokryph Quijote zusammensetzen Text stammen digital Edition cervantesvirtual Wikisource Project Gutenberg einheitlich abgespeicheren Textteile selber Autor stammen einleitend Bemerkung herausgebers befreien sicher set sowohl Autorschaft Genre epoch betreffend vergleichbar Autor unterschiedlich Text auswählen insgesamt Text Vergleichbarkeit Textläng betreffend bedeuten Novell Roman miteinander vergleichen unterschiedlich Textlänge gut Ergebnis Angleichung verzichten ed korpus verschieden anerkannt Distanzmaß ed -- Delta Cosine mfw auswerten schrittweise Kandidat hinzufügen Ergebnis bewerten Problematik ergeben Sinn Autor Korpus nehmen Textbeispiel vorliegen Partner Dendrogramm zuordnen somit fälschlicherweise eigentlich voneinander entfernt Text geclustert darstellen Problem umgehen Vergleich Principle Component Analysis pca durchführen gut Darstellung Distanz einzeln Text Raum zeigen Korpus Text funktionieren Zuordnung trotz stark Größenunterschiede Novell Roman wörtern klar clustern Mfw Autorschaftszuordnung ed -- delta Einwandfrei liegend Zahl schlich beständig Cortés de tolosa Block Werk Castillo Slórzano Vergleich gleich Versuch Cosine Distance durchführen bringen groß Veränderung Darstellung weiterhin bleiben Roman Cortés de tolosa hartnäckig Mfw Castillo Solórzano heraus nehmen interessant Pca variable zeigen entfernt Lazarillo Cortés de tolosa übrig Werk cervantes entfernt mfw deutlich Lazarillo Cortés tolosa Nachahmung originals anonym schreiben übrig Werk abweichen deutlich entfernen erklären sprachlich Unterscheidung vorliegend Edition Werk alt Form Kastilische verwenden Vergleichbarkeit einschränken bleiben spannend Ergebnis avellaneda Kandidat hinzunimmen Korpuserweiterung bleiben Zuordnung relativ stabil Cortés de tolosa zuvor möglich Kandidat Avellaneda zuordnen Apokryphe quijot Teil cervantes clustern bleiben mfw konstant Pca bestätigen Ergebnis fig blau Punkt schwarze zeigen Avellaneda Umkreis nächster Korpuserweiterung nehmen Kandidat Prosabeispiel hinzu Text Verfügung stehen schnell deutlich Einzelgänger Text zuvor stabil gefüge auseinanderbringen offensichtlich statistisch Untersuchung gebrauchen erstaunlich bleiben vage Clustering Mfw schieben einzeln Text mal mal Konsistent mfw Cluster bilden anderer Kandidat schaffen Nähe Avellaneda Pca bringen entscheidend Vorteil Cluster Analysis Espinel gruppieren direkt avellaneda Autorname anstelle Desjenigen Avellanedas nennen müsste vorliegend Ergebnis cervantes gekonnt Spaß Sinn Quijote Leser falsch Fortsetzung Nase herum führen Autor Stil gegenseitig aufeinander abstimmen derart Wortgebrauch ähneln folgend Untersuchung Rolling Delta anbieten kollaborativ Autorschaft ausmachen vorgeblich Stilwechsel cervantes quijot genau definieren Avellaneda Zusammenhang bringen,"[('avellaneda', 0.46261958963633515), ('cervantes', 0.3539346849694313), ('quijote', 0.21588914183028973), ('mfw', 0.18243256038012215), ('kandidat', 0.1571236335333724), ('cortés', 0.1542065298787784), ('tolosa', 0.1542065298787784), ('quijot', 0.1542065298787784), ('fortsetzung', 0.1361287249882428), ('autorschaftsattribution', 0.12555388703192596)]"
2016,DHd2016,posters-040.xml,Entwicklung einer digitalen Brief-Edition und eines Forschungsportals zu Theodor Fontane,"Sabine Seifert (Theodor-Fontane-Archiv, Universität Potsdam, Deutschland)","digitale Edition, Forschungsportal, Datenmodellierung, Handschriften, Briefe","Sammlung, Transkription, Inhaltsanalyse, Annotieren, Archivierung, Veröffentlichung, Kommentierung, Daten, Literatur, Manuskript, Metadaten, virtuelle Forschungsumgebungen","Das Theodor-Fontane-Archiv konzipiert eine digitale, kritische Edition aller Briefe von und an Theodor Fontane. Hierbei handelt es sich um etwa 10.000 Briefe, die bislang unvollständig und nach heutigen editionswissenschaftlichen Standards unzureichend veröffentlicht sind. Ein Großteil des Briefnachlasses befindet sich im Archiv, doch sind weitere Bestände aus anderen Institutionen und Privatbesitz zu berücksichtigen. Somit wird der gesamte, stark verstreute Briefnachlass virtuell zusammengeführt und erstmalig als ein Korpus recherchierbar, wodurch die Voraussetzung für die systematische Erforschung dieses zentralen Werkbestandes geschaffen wird. Verschiedene editorische Herausforderungen stellen sich bezüglich der Briefe.  Neben den Originalhandschriften sind mitunter mehrfache Abschriften sowie  bisherige Drucke einzubeziehen. Somit werden mehrere Überlieferungsträger eines  Briefes verzeichnet und in der Edition präsentiert, wodurch sich die Frage einer  sinnvollen Darstellung von Überlieferungsvarianten stellt. Daneben muss eine  Lösung für die digitale Umsetzung materialspezifischer Besonderheiten gefunden  werden, z. B. die häufig beschriebenen Briefränder. Diese Textbestandteile sind  oft seitenübergreifend und stehen in den unterschiedlichsten Winkeln zur  regulären Beschreibrichtung. In der Online-Präsentation werden neben den  Digitalisaten und Transkriptionen die Metadaten, die Kommentierung sowie die XML  / TEI P5-Auszeichnung verfügbar gemacht. Für die individuelle Benutzbarkeit soll  die Anzeige der genannten Daten flexibel anzupassen sein. Aufgrund der  besonderen Schriftbildlichkeit müssen die Digitalisate drehbar und im Falle von  mehreren Textzeugen soll deren parallele Anzeige möglich sein. Auf die  Verwendung von Standards (z. B. XML / TEI P5) und Normdaten (z. B. GND für  Personen) und die Erstellung von projektinternen Indizes (zu Personen,  Institutionen, Orten, Werken, Periodika) wird besonderer Wert gelegt. So können  etwa personelle und institutionelle Netzwerke, an denen Fontane teilhatte,  rekonstruiert und intertextuelle Verbindungen nachvollzogen werden. Die Edition der Briefe steht im Zusammenhang mit dem ebenfalls in der  konzeptionellen Entwicklung befindlichen Fontane-Forschungsportal. Dessen Ziel  ist die Präsentation der Digitalen Sammlungen des Archivs und optional anderer  Bestandshalter. Die aufbewahrten Handschriften liegen digitalisiert vor, die  Verknüpfung der Digitalisate mit dem technischen und bibliographischen  Metadatensatz wird über den METS / MODS- bzw. METS / EAD-Standard erfolgen. Die  Handschriften- und Bibliothekskataloge des Archivs, bisher intern als allegro-C-  und allegro-HANS-Datenbanken geführt, werden ebenfalls über das Portal als OPACs  zugänglich gemacht. Somit stellt das Portal die Verknüpfung von archivalischen  Quellen- und Erschließungsdaten und von Forschungsprimärdaten her. Neben der virtuellen Zusammenführung des zerstreuten Nachlasses ist das zweite Ziel des Portals, alle Forschungsressourcen zu Fontane schnell zugänglich bereitzustellen. Die Fontane-Aktivitäten außerhalb des Archivs sollen hier gebündelt werden und das Portal als Kommunikationsplattform für die verschiedenen Akteure dienen. Dafür werden technische Schnittstellen für Datenaustausch sowie Kooperationen mit anderen Projekten geschaffen. Doch richtet sich das Portal nicht nur an die Wissenschaft, sondern auch an die breite Öffentlichkeit, der hier ein umfassender Zugang zu Fontane, seinem Leben und Werk ermöglicht werden soll. Die technische Umsetzung der digitalen Edition und des Forschungsportals wird im Rahmen der virtuellen Forschungsumgebung FuD (""Forschungsnetzwerk und Datenbanksystem"") erfolgen, die am Kompetenzzentrum für elektronische Erschließungs- und Publikationsverfahren in den Geisteswissenschaften an der Universität Trier entwickelt wurde. Die in FuD bereitgestellten Tools werden an die projektspezifischen Bedürfnisse angepasst und weiterentwickelt, etwa für die Verzeichnung unterschiedlicher Textzeugen. Diese Weiterentwicklungen sollen von anderen Institutionen nachgenutzt werden können. Im Hintergrund stehen eine Neustrukturierung der gesamten Datenstruktur des Theodor-Fontane-Archivs und die Überführung in ein Content Management System, das ebenfalls in Zusammenarbeit mit dem Kompetenzzentrum Trier erarbeitet wird. Die Datenmodellierung des CMS, auf das Edition und Portal gleichermaßen zugreifen, muss gewährleisten, dass unterschiedliche Arten von Daten in FuD zusammengeführt, angereichert und weiterverarbeitet werden können. Hierzu gehören bibliographische Metadaten von Handschriften und Bibliotheksbeständen des Archivs, bibliographische Metadaten anderer Institutionen, Digitalisate plus deren Metadaten, Normdaten, die Transkriptionen der Edition, die TEI-Textauszeichnung, editorischen Kommentare und die erstellten Indizes. Auch werden Schnittstellen zu anderen Projekten und zu Archivdatenbanken hergestellt und der Zugang per Open Access gewährleistet. Die digitale Edition und das Forschungsportal werden auf Grundlage der gemeinsamen Datenbasis so konzipiert und miteinander verbunden, dass zwischen ihnen Datenaustausch möglich wird. So wird der jeweilige Rechercherahmen erweitert, eine umfassendere Kontextualisierung der Informationen erreicht und eine digitale Arbeits- und Forschungsumgebung geschaffen, die den Erfordernissen eines Archivs in seinen Aufgaben des Sammelns, Erschließens und Langzeitarchivierens sowie den Erfordernissen einer Forschungseinrichtung gleichermaßen gerecht wird. Dies und die Vernetzung mit verschiedenen Forschungsvorhaben ermöglicht nicht nur neue Erkenntnisse für die Fontane-Forschung selbst, sondern auch zur Literatur- und Geistesgeschichte des 19. Jahrhunderts. Für weitere, interdisziplinäre und neue Fragestellungen wird die Struktur offen und flexibel angelegt. Zudem soll die entwickelte Arbeitsumgebung in ihrer Datenstruktur auch Modellcharakter für kleinere Archive, Museen und Sammlungen haben. Im Poster werden der derzeitige Konzeptionsstand von Edition und Portal vorgestellt, Probleme und offene Fragen aufgezeigt sowie Lösungsansätze zur Diskussion gestellt.",de,konzipieren digital kritisch Edition Brief Theodor Fontane hierbei handeln Brief bislang unvollständig heutig editionswissenschaftlich Standard unzureichend veröffentlichen Großteil Briefnachlass befinden Archiv Beständ Institution Privatbesitz berücksichtigen somit gesamt stark verstreut Briefnachlass Virtuell zusammenführen erstmalig Korpus recherchierbar wodurch Voraussetzung systematisch Erforschung zentral werkbestandes schaffen verschieden editorisch Herausforderung stellen bezüglich Brief Originalhandschrift mitunter mehrfach Abschrift bisherig Druck einbeziehen somit mehrere überlieferungsträg Brief verzeichnen Edition präsentieren wodurch Frage sinnvoll Darstellung überlieferungsvarianten stellen Lösung digital Umsetzung materialspezifisch Besonderheit finden häufig beschrieben Briefränder textbestandteile seitenübergreifend stehen Unterschiedlichst winkeln regulär Beschreibrichtung Digitalisat Transkription metadaten Kommentierung xml tei verfügbar individuell Benutzbarkeit Anzeige genannt daten flexibel anpassen aufgrund besonderer Schriftbildlichkeit Digitalisat drehbar Fall mehrere textzeugen parallel Anzeige Verwendung Standard xml tei normdat Gnd Person Erstellung projektintern Indizes Person Institution Ort Werk Periodika besonderer wert legen personell institutionell netzwerk fontan Teilhatt rekonstruieren intertextuell verbindungen nachvollziehen Edition Brief stehen Zusammenhang ebenfalls konzeptionell Entwicklung befindlich Ziel Präsentation digital Sammlung Archivs optional anderer bestandshalt aufbewahrten Handschrift liegen digitalisieren Verknüpfung Digitalisat technisch Bibliographisch Metadatensatz Met Met erfolgen Bibliothekskataloge Archivs intern führen ebenfalls Portal Opacs zugänglich somit stellen Portal Verknüpfung archivalisch erschließungsdaten Forschungsprimärdat virtuell Zusammenführung zerstreuter nachlasses Ziel Portal Forschungsressource Fontan schnell zugänglich bereitstellen außerhalb Archiv bündeln Portal Kommunikationsplattform verschieden Akteur dienen technisch Schnittstelle datenaustausch kooperation Projekt schaffen richten Portal Wissenschaft breit Öffentlichkeit umfassend Zugang fontane Leben Werk ermöglichen technisch Umsetzung digital Edition Forschungsportal Rahmen virtuell Forschungsumgebung Fud Forschungsnetzwerk datenbanksyst erfolgen Kompetenzzentrum elektronisch Publikationsverfahre geisteswissenschaften Universität Trier entwickeln fud bereitgestellt Tools projektspezifisch bedürfnis angepasst weiterentwickeln Verzeichnung unterschiedlich Textzeuge weiterentwicklung Institution nachnutzen Hintergrund stehen Neustrukturierung gesamt Datenstruktur Überführung Content Management System ebenfalls Zusammenarbeit Kompetenzzentrum Trier erarbeiten Datenmodellierung cms Edition Portal gleichermaßen zugreifen gewährleisten unterschiedlich Art daten Fud zusammenführen angereicheren weiterverarbeitet hierzu gehören bibliographisch Metadat handschrift bibliotheksbestände Archivs bibliographisch Metadat anderer Institution Digitalisat plus Metadat Normdate Transkription Edition editorisch Kommentar erstellt Indizes Schnittstelle Projekt Archivdatenbanke herstellen Zugang per op Access gewährleisten digital Edition Forschungsportal Grundlage gemeinsam datenbasis konzipieren miteinander verbinden datenaustausch jeweilig Rechercherahme erweitern umfassend Kontextualisierung Information erreichen digital Forschungsumgebung schaffen Erfordernis Archiv Aufgabe Sammeln Erschließen Langzeitarchivierens Erfordernis Forschungseinrichtung gleichermaßen gerecht Vernetzung verschieden Forschungsvorhaben ermöglichen erkenntnis Geistesgeschicht Jahrhundert interdisziplinär Fragestellung Struktur flexibel anlegen zudem entwickelt Arbeitsumgebung Datenstruktur Modellcharakter klein Archiv Museum Sammlung Poster derzeitig Konzeptionsstand Edition Portal vorstellen Problem offen Frage aufzeigen Lösungsansätz Diskussion stellen,"[('portal', 0.31745082757260285), ('edition', 0.2257036122114638), ('brief', 0.20765241093697384), ('digitalisat', 0.17559183999718028), ('fud', 0.1670983955894219), ('institution', 0.15856035065388413), ('archivs', 0.1556394874776336), ('archiv', 0.15226549776330045), ('virtuell', 0.13605035467397264), ('bibliographisch', 0.1316938799978852)]"
2016,DHd2016,vortraege-055.xml,"Topic, Genre, Text","Christof Schöch (Universität Würzburg, Deutschland); Ulrike Henny (Universität Würzburg, Deutschland); José Calvo (Universität Würzburg, Deutschland); Daniel Schlör (Universität Würzburg, Deutschland); Stefanie Popp (Universität Würzburg, Deutschland)","Topic Modeling, Spanische Literatur, Lateinamerikanische Literatur, Textverlauf","Programmierung, Inhaltsanalyse, Annotieren, Visualisierung, Literatur, Text","Der Beitrag möchte zeigen, wie die Berücksichtigung detaillierter,  gattungsbezogener Metadaten auf produktive Weise mit dem Verfahren des Topic  Modeling verbunden werden kann, um bisher nicht bekannte thematische Strukturen  im Textverlauf in einer Sammlung spanischer und hispanoamerikanischer Romane zu  entdecken. Ausgangshypothese ist, dass die Wichtigkeit bestimmter Topics nicht  nur im Textverlauf variiert, sondern dies auch in verschiedenen Untergattungen  auf unterschiedliche Weise tut. Eine Pilotstudie wurde im März 2015 beim  Workshop zu Computational Narratology bei der DHd-Tagung in Graz vorgestellt. Im  Rahmen der interdisziplinären Würzburger eHumanities-Nachwuchsgruppe  ""Computergestützte literarische Gattungsstilistik ( Die Frage nach dem Text- oder Handlungsverlauf in narrativen literarischen Texten hat jüngst zunehmende Aufmerksamkeit in der digitalen Literaturwissenschaft erhalten. Matthew Jockers kam durch Sentiment Analysis im Verlauf zahlreicher Romane zu dem (kontrovers diskutierten) Ergebnis, es gäbe sechs oder sieben grundlegende Plotstrukturen (Jockers 2015). Ben Schmidt hat unter anderem den Verlauf von Topic-Wahrscheinlichkeiten in der ""screen time"" amerikanischer Fernsehserien verfolgt (Schmidt 2014). Der vorliegende Beitrag verbindet die Frage nach dem Textverlauf mit der nach den Untergattungen, seine zentrale Fragestellung lautet: Können wir nach Untergattung unterschiedliche Verlaufsmuster für bestimmte Topics über den Textverlauf hinweg feststellen? Die Textsammlung enthält 150 spanische und hispanoamerikanische Romantexte aus  der Zeit von 1880 bis 1930 (für den spanischen Roman: Altisent 2008; de Nora  1963, für den hispanoamerikanischen Roman: Gallo 1981; Williams 2009). Die Texte  sind in TEI aufbereitet und mit detaillierten Metadaten versehen worden. Es  wurden vier weit gefasste Untergattungen gewählt, um die Romane miteinander  vergleichen zu können: Topic Modeling ist eine unüberwachte, nicht-deterministische Methode aus dem  Bereich des Hier wurde Topic Modeling als Teil eines umfassenden, weitgehend automatischen Arbeitsablaufes als Serie von Python-Skripten implementiert: Präprozessieren der Texte (Segmentierung, Binning, Lemmatisierung, POS-Tagging), das eigentliche Topic Modeling (mit Mallet, siehe McCallum 2002), Aufbereitung des Mallet-Outputs, zahlreiche Visualisierungen als Perspektiven auf die Ergebnisse. Die wichtigsten Parameter: Berücksichtigung ausschließlich der Substantive, Weglassung der 70 häufigsten Substantive, Romansegmente von ca. 600 Wörtern (unter Berücksichtigung von Absatzgrenzen), Anzahl von 70 Topics. Die Python-Skripte sind frei verfügbar und ausführlich dokumentiert, Begleitmaterialien (Skripte, Parameterdatei, Metadaten, Abbildungen) sind unter Es werden zunächst die Topics selbst dargestellt, dann Unterschiede in den Topic-Verteilungen nach Untergattungen, über den Textverlauf hinweg und schließlich über den Textverlauf in Abhängigkeit der Untergattung. Die Mehrheit der erhobenen Topics beinhaltet konkrete typische Themen und Motive des spanischsprachigen Romans der Epoche. Man erkennt eine klare semantische Beziehung der Wörter: ein konkreter Bereich menschlicher Tätigkeiten, wie in Topic 19 (maestro-colegi o-escuela, dt. ""Lehrer-Schule-Schule"") oder Topic 23 (sangre-golpe-arma, dt. ""Blut-Schlag-Waffe""); oder abstrakte Begriffe und Gefühle, wie bei Topic 69 (conciencia-honor-crimen, dt. ""Gewissen-Ehre-Verbrechen""). Weniger kohärent ist Topic 45 (marido-rato-chico, dt. ""Ehegatte-Weile-Junge""). Die folgenden Wordclouds (Abbildung 2) veranschaulichen die erwähnten Topics. Die folgende Heatmap (Abbildung 3) zeigt die Verteilung der durchschnittlichen Topic-Wahrscheinlichkeiten in den vier Untergattungen für diejenigen 20 Topics, deren Werte zwischen den Untergattungen besonders stark schwanken (nach Standardabweichung). Besonders distinktive Topics existieren für die Die Ausprägung der Topics variiert nicht nur hinsichtlich der Untergattungen,   sondern auch über den Textverlauf hinweg. So gibt es einige Topics, deren   Vorkommen am Anfang der Romane besonders wahrscheinlich ist (Abbildung 4a).   Dazu zählen Topic 10 (vino-plato-pan, dt. ""Wein-Teller-Brot""), Topic 17   (sombrero-ropa-bota, dt. ""Hut-Kleidung-Stiefel"") und Topic 19, welche auf   die Beschreibung von Ambiente, Situation und Personen hindeuten. Gegen Ende   der Romane sind andere Topics wahrscheinlicher (Abbildung 4b), z. B. Topic 2   (pecado-caridad-conciencia, dt. ""Sünde-Wohltätigkeit-Gewissen""), Topic 23   und Topic 69, also abstraktere Themen oder solche, die sich auf   Wertvorstellungen beziehen. Dies deutet darauf hin, dass in den Romanen am   Ende Bilanz gezogen wird, die Handlung einen drastischen Ausgang nimmt oder   das im Textverlauf Behandelte in gesellschaftliche oder religiöse Diskurse   eingebunden wird. Für einige der genannten Topics, die in bestimmten Bereichen des Textverlaufs  wahrscheinlicher sind, kann die Tendenz über alle Untergattungen hinweg  bestätigt werden (bspw. bei Topic 10 und 17, siehe oben). Es gibt aber auch  Themen, bei denen sich durch die Betrachtung des Verlaufs in den einzelnen  Untergattungen ein differenzierteres Bild ergibt. Die Wahrscheinlichkeit von  Topic 23 beispielsweise nimmt nur für die Das kann so interpretiert werden, dass die Allgemein gilt, dass die Untergattungen sich in ihrer Topicverteilung im Textverlauf auch dann deutlich unterscheiden können, wenn dies für alle Untergattungen zusammengenommen nicht der Fall ist und so leicht übersehen werden könnte. Für die Berechnung wurden die Romansegmente von 600 Wörtern bezüglich des Textverlaufs auf 15 Romanabschnitte (Bins) verteilt, um die unterschiedliche Romanlänge zu berücksichtigen. Diese Bins wurden hinsichtlich der Untergattung gruppiert und jeweils das arithmetische Mittel bestimmt. Die in den Plots eingezeichneten Kurven entsprechen der linearen Interpolation dieser gemittelten Werte. Zusätzlich wurde der Standardfehler vertikal um den jeweiligen Kurvenpunkt eingezeichnet, der deutlich macht, wie sehr die jeweiligen dem Mittelwert zugrunde liegenden Werte streuen, also wie gut der Mittelwert die Gesamtheit der Segmentwerte repräsentiert. Insgesamt zeigen sich verschiedene Zusammenhänge: Zwischen bestimmten Topics und einzelnen Roman-Untergattungen, zwischen Topics und dem Textverlauf, und dies zum Teil dann auch wieder in Abhängigkeit von den Untergattungen. Aus literaturgeschichtlicher Perspektive betrachtet erweisen sich die in die Untersuchung einbezogenen Metadaten für eine Einordnung der Topic-Resultate als nützlich. Topics sind für die Romangattungen im vorliegenden Korpus ein wichtiger Faktor, ähnlich wie dies für Gattungen wie die klassische Komödie und Tragödie bereits gezeigt werden konnte (Schöch 2015). Ein detaillierterer Blick zeigt beispielsweise Folgendes: Topic 11, welches typisch für die Die Nutzung von Topic Modeling als Methode kann für die digitale Literaturwissenschaft verbessert werden, wenn spezifisch literaturwissenschaftliche Metadaten in die Betrachtungen einbezogen werden und die Textstruktur - hier als Sequenz von Textverlaufseinheiten - berücksichtigt wird. Verschiedene Visualisierungsstrategien erweisen sich als entscheidende ""Interfaces"" zu den Daten (im Sinne von Doueihi 2012), die Muster sichtbar machen und den Blick lenken. Die Ergebnisse des Topic Modelings können differenzierter und aus verschiedenen Perspektiven betrachtet und mit literaturhistorischem Wissen in Verbindung gebracht werden. Die Ergebnisse ergänzen und erweitern etablierte hermeneutische Lektürestrategien, insofern sie einen synthetisierenden Blick auf sehr umfangreiche Textsammlungen erlauben. Nächste Schritte betreffen insbesondere die weitere Auseinandersetzung mit der  Signifikanz von Unterschieden in den Topic-Wahrscheinlichkeiten im Textverlauf,  deren Berechnung u. a. durch die mangelnde Normalverteilung der Werte nicht  trivial ist. Zusätzlich zu den Untergattungen sollen auch Kategorien wie das  Setting modelliert werden. Zudem sollen die Textverlaufs-Daten für die  automatische Klassifikation von Romanen nach Untergattungen genutzt werden.  Schließlich wird bereits an der Erweiterung der Textsammlung gearbeitet,  insbesondere mit Blick auf den Umfang und ein ausgeglicheneres Verhältnis der  Untergattungen.",de,Beitrag zeigen Berücksichtigung Detaillierter gattungsbezogen Metadat produktiv Weise Verfahren Topic Modeling verbinden bekannt thematisch Struktur Textverlauf Sammlung spanisch hispanoamerikanisch Roman entdecken Ausgangshypothese Wichtigkeit bestimmt Topics Textverlauf variieren verschieden Untergattung unterschiedlich Weise tun Pilotstudie März Workshop Computational Narratology graz vorstellen Rahmen interdisziplinären Würzburger computergestützt literarisch Gattungsstilistik Frage handlungsverlauf narrativ literarisch Text jüngst zunehmend Aufmerksamkeit digital Literaturwissenschaft erhalten Matthew Jockers Sentiment Analysis Verlauf zahlreich Roman kontrovers diskutiert Ergebnis geben Grundlegend plotstrukturen Jockers ben Schmidt Verlauf screen time amerikanisch Fernsehseri verfolgen Schmidt vorliegend Beitrag verbinden Frage Textverlauf untergattung zentral Fragestellung lauten Untergattung unterschiedlich Verlaufsmuster bestimmt Topics Textverlauf hinweg feststellen Textsammlung enthalten spanisch hispanoamerikanisch Romantext spanisch Roman Altisent de Nora hispanoamerikanisch Roman Gallo williams Text tei aufbereiten Detailliert metadaten versehen gefas Untergattung wählen Roman miteinander vergleichen Topic Modeling unüberwacht Methode Bereich Topic Modeling umfassend weitgehend automatisch Arbeitsablaufes Serie implementieren Präprozessier Text Segmentierung Binning Lemmatisierung eigentlich Topic Modeling Mallet sehen mccallum Aufbereitung zahlreich Visualisierung Perspektive Ergebnis wichtig parameter Berücksichtigung ausschließlich substantiv Weglassung häufig Substantiv romansegmenen wörtern Berücksichtigung absatzgrenzen Anzahl Topics frei verfügbar ausführlich dokumentieren Begleitmaterialien Skript Parameterdatei Metadat abbildungen Topics darstellen Unterschied untergattung Textverlauf Hinweg schließlich Textverlauf Abhängigkeit Untergattung Mehrheit erhoben Topics beinhalten konkret typisch Them Motiv spanischsprachig Roman Epoche erkennen klar semantisch Beziehung Wörter konkret Bereich menschlich Tätigkeit Topic dt Topic dt abstrakt begriffe Gefühl Topic dt Kohärent Topic dt folgend wordclouds Abbildung veranschaulichen erwähnt Topics folgend Heatmap Abbildung zeigen Verteilung durchschnittlich Untergattung Topic Wert untergattung stark schwanken Standardabweichung Distinktive Topics existieren Ausprägung Topics variieren hinsichtlich untergattung Textverlauf hinweg Topics vorkommen Anfang Roman wahrscheinlich Abbildung zählen Topic dt topic dt topic Beschreibung Ambiente Situation Person hindeuen Romane Topics wahrscheinlich Abbildung Topic dt topic Topic abstrakter Thema wertvorstellungen beziehen deuten Roman Bilanz ziehen Handlung drastisch Ausgang nehmen Textverlauf behandeln gesellschaftlich religiös Diskurse einbinden genannt Topics bestimmt Bereich textverlauf wahrscheinlich Tendenz untergattung Hinweg bestätigen Topic sehen Thema Betrachtung Verlauf einzelner Untergattung differenzierter Bild ergeben Wahrscheinlichkeit Topic beispielsweise nehmen interpretieren allgemein gelten untergattung Topicverteilung Textverlauf deutlich unterscheiden untergattung Zusammengenommen Fall übersehen Berechnung Romansegmente wörtern bezüglich Textverlauf Romanabschnitte Bins verteilen unterschiedlich Romanlänge berücksichtigen Bins hinsichtlich Untergattung gruppieren jeweils arithmetisch bestimmen plots eingezeichneten Kurv entsprechen linear Interpolation gemittelt Wert zusätzlich Standardfehler Vertikal jeweilig Kurvenpunkt eingezeichnen deutlich jeweilig mittelwert zugrunde liegend Wert streuen mittelwern Gesamtheit Segmentwerte repräsentieren insgesamt zeigen verschieden zusammenhänge bestimmt Topics einzeln Topics Textverlauf Abhängigkeit untergattungen literaturgeschichtlich Perspektive betrachten erweisen Untersuchung einbezogen Metadat Einordnung nützlich Topics Romangattunge vorliegend Korpus wichtig Faktor ähnlich gattung klassisch Komödie Tragödie zeigen schöch detaillierter Blick zeigen beispielsweise folgend Topic typisch Nutzung Topic Modeling Methode digital Literaturwissenschaft verbessern spezifisch literaturwissenschaftlich Metadat betrachtung einbeziehen Textstruktur Sequenz Textverlaufseinheit berücksichtigen verschieden Visualisierungsstrategi erweisen entscheidend Interfaces daten Sinn doueihi Muster sichtbar Blick lenken Ergebnis Topic modelings differenziert verschieden Perspektive betrachten literaturhistorisch Wissen Verbindung bringen Ergebnis ergänzen erweitern etabliert Hermeneutische lektürestrategien insofern synthetisierend Blick umfangreich textsammlungen erlauben nächster Schritt betreffen insbesondere Auseinandersetzung Signifikanz unterschiede Textverlauf Berechnung mangelnd Normalverteilung Wert trivial zusätzlich untergattung Kategorie Setting modellieren zudem automatisch Klassifikation Roman untergattungen nutzen schließlich Erweiterung Textsammlung arbeiten insbesondere Blick Umfang ausgeglichener Verhältnis untergattungen,"[('untergattung', 0.41720830004445203), ('topic', 0.389807655062725), ('textverlauf', 0.3722474523763295), ('topics', 0.36199842864458637), ('dt', 0.2543273749624933), ('roman', 0.1407680522410998), ('modeling', 0.10531529740968716), ('hispanoamerikanisch', 0.10152285805122238), ('untergattungen', 0.09621955321069378), ('wahrscheinlich', 0.08590325824069142)]"
2016,DHd2016,posters-017.xml,: aichinger,"Mathias Mueller (ÖAW, Österreich); Andreas Dittrich (ÖAW, Österreich); Gilbert Waltl (ÖAW, Österreich); Marlene Csillag (ÖAW, Österreich); Katharina Godler (ÖAW, Österreich); Christine Ivanovic (ÖAW, Österreich)","Raumbezüge, Textkorpus, Ortsannotationen, Wientopographie","Datenerkennung, Bilderfassung, Gestaltung, Programmierung, Räumliche Analyse, Modellierung, Annotieren, Visualisierung, Karte, Visualisierung","Der Fokus der Untersuchung liegt auf der literarischen Repräsentation von Raum.  Bisherige Untersuchungen ihres Werks haben erwiesen, dass Aichingers Bezugnahmen  auf Orte und Ereignisse in Wien zentrale Bedeutung zukommt (Fässler 2013). Dabei  fällt auf, dass Aichinger Raumbezüge in verschiedenen Phasen ihres Werks auf  ganz unterschiedliche Weise elaboriert: Der Wienbezug ihres ersten Romans, Textgrundlage ist die achtbändige Ausgabe der Werke Ilse Aichingers (S.  Fischer Verlag 1991) sowie die danach erschienenen Einzelbände. Diese Bände  wurden gescannt und mittels OCR (Optical Character Recognition) erfasst und  dadurch maschinenlesbar gemacht. Als Vergleichskorpora sollen zusätzlich die  davon abweichenden Textfassungen der Erstausgabe des Romans sowie der  zwischen 2000 und 2004 in Tageszeitungen publizierten Texte erfasst  werden. Im zweiten Arbeitsschritt wird eine TEI-konforme Datei erstellt, in der die Texte mithilfe von Standards wie RDF (Resource Description Framework), XML (Extensible Markup Language) und PoS (Part-of-Speech-Tagging) codiert und dadurch der maschinellen Abfrage durch Abfragesprachen wie SPARQL (SPARQL Protocol And RDF Query Language) zugänglich gemacht werden. Im Hinblick auf den primären Fokus der Untersuchung, die Erfassung und Analyse der literarischen Topographien Aichingers, werden vorrangig Personennamen sowie Orts- und Zeitangaben kodiert. Außerdem ist eine Analyse anhand semantischer Felder geplant, wofür eine Vernetzung mit unterschiedlichen Datenbanken (z. B. Dornseiff) vorgesehen ist. Von vornherein soll so gearbeitet werden, dass die Möglichkeit weitere bzw. speziellere Codierungen zu ergänzen offen bleibt. Ergänzend zur digitalen Erfassung und Erschließung der Texte werden weitere Metadaten eingebracht. Dies können textgenetisch relevante Daten sein wie Entstehungs- und Publikationsdaten, oder Sacherläuterungen, wie sie in Apparaten wissenschaftlicher Editionen oder Kommentaren üblich sind, sowie Hinweise auf Varianten, Querverweise, Illustrationen etc. Ein Teil dieser Daten ist durch Recherchen in Wiener Archiven oder am Aichinger-Vorlass im Deutschen Literaturarchiv (DLA) in Marbach zu erheben. Die Auszeichnung durch RDF ermöglicht aber auch die Verlinkung mit online Datenbanken und damit den Anschluss an das semantic web (Hitzler et al. 2008; Ivanovic / Frank 2015). Das Textkorpus :aichinger soll die Basis bilden für die Durchführung von Abfragen und Analysen, die eine präzise, systematische und vollständige Evaluierung der Raumbezugnahmen im Gesamtwerk der Autorin ermöglichen soll. Erfassbar werden dadurch beispielsweise Personenkonstellationen in Verbindung mit Orten sowie Frequenzen der Nennung bestimmter Orte resp. Wege in Korrelation zur beschriebenen Zeit wie zur Zeit der Textabfassung. Diese Ergebnisse verlangen unterschiedliche Darstellungsformen. So sind Diagramme möglich, oder Wordclouds, die wiederum Häufungen oder Übereinstimmungen bzw. Korrelationen darstellen können. Auf der Basis der RDF Codierung lassen sich z. B. maschinell Karten generieren, in denen die erwähnten Orte oder Wege der in den Texten genannten Figuren u. a. aufscheinen. Die kartographische Darstellung ermöglicht es darüber hinaus Leerstellen ihres Werkes (nie genannte Orte oder Zonen) oder verdeckte Strukturen (die Wientopographie, die der für Aichinger maßgebliche Film Der Dritte Mann darstellt) sichtbar zu machen. Insbesondere anhand solcher Karten wird das Poster das Konzept und die Analysemöglichkeiten unseres Projektes darstellen. Das Projekt dient der Sichtbarmachung und besseren Analyse der raumrelevanten Strukturen im Werk Aichingers und deren Relevanz für die erinnerungskulturell motivierte Schreibweise, der die Autorin verpflichtet ist. Das Projekt hat insofern paradigmatischen Charakter, als die an diesem Beispiel entwickelten Methoden den Status eines Prototyps haben und auch bei der Analyse anderer Textkorpora Anwendung finden sollen.",de,Fokus Untersuchung liegen literarisch Repräsentation Raum bisherig Untersuchung werks erweisen aichinger bezugnahmen Ort Ereignis Wien zentral Bedeutung zukommen Fässler fallen Aichinger raumbezüge verschieden Phase Werks unterschiedlich Weise elaborieren Wienbezug Roman Textgrundlage achtbändig Ausgabe werk Ilse Aichinger fisch Verlag erschienen Einzelbänd Bänd gescannen mittels ocr Optical character Recognition erfassen maschinenlesbar Vergleichskorpora zusätzlich abweichenden textfassungen Erstausgabe Roman tageszeitung publiziert Text erfassen Arbeitsschritt Datei erstellen Text Mithilfe Standard rdf resource description Framework xml extensibel markup Language pos codieren maschinell Abfrage abfragesprach sparql Sparql Protocol -- rdf query Language zugänglich Hinblick primär Fokus Untersuchung Erfassung Analyse literarisch Topographie aichinger vorrangig personennamen Zeitangabe kodieren Analyse anhand semantisch felder planen wofür Vernetzung unterschiedlich datenbanke Dornseiff vorsehen vornherein arbeiten Möglichkeit spezieller codierungen ergänzen bleiben ergänzend digital Erfassung Erschließung Text Metadat einbringen textgenetisch relevant daten Publikationsdat sacherläuterungen apparat wissenschaftlich edition Kommentar üblich Hinweis Variant querverweise illustrationen daten Recherche Wiener archiven deutsch Literaturarchiv dla Marbach erheben Auszeichnung Rdf ermöglichen Verlinkung Online datenbanken Anschluss Semantic Web Hitzler et ivanovic Frank Textkorpus Aichinger Basis bilden Durchführung abfrag Analyse präzis systematisch vollständig Evaluierung Raumbezugnahm Gesamtwerk Autorin ermöglichen erfassbar beispielsweise Personenkonstellation Verbindung Ort Frequenz Nennung bestimmt Ort Resp Weg Korrelation beschrieben Textabfassung Ergebnis verlangen unterschiedlich darstellungsformen diagramme Wordclouds wiederum häufungen übereinstimmungen Korrelation darstellen Basis Rdf Codierung lassen maschinell kart Generier erwähnt Ort Weg text genannt Figur aufscheinen kartographisch Darstellung ermöglichen hinaus leerstellen Werk genannt Ort zon verdeckt Struktur Wientopographie Aichinger maßgeblich Film Mann darstellen sichtbar insbesondere anhand Kart Poster Konzept Analysemöglichkeit unser projektes darstellen Projekt dienen Sichtbarmachung gut Analyse raumrelevanten Struktur Werk aichinger Relevanz erinnerungskulturell motiviert Schreibweise Autorin verpflichten Projekt insofern paradigmatisch Charakter entwickelt Methode Status Prototyp Analyse anderer Textkorpora Anwendung finden,"[('aichinger', 0.46999834318425804), ('rdf', 0.209856043201708), ('ort', 0.18595280327170788), ('kart', 0.12727050962859104), ('werks', 0.1136250490699034), ('autorin', 0.104928021600854), ('sparql', 0.104928021600854), ('analyse', 0.0972398641346607), ('korrelation', 0.08802658186737079), ('untersuchung', 0.08468466352921877)]"
2016,DHd2016,posters-014.xml,Distant-Reading-Showcase: 200 Jahre deutscher Dramengeschichte auf einen Blick,Frank Fischer (Niedersächsische Staats- und Universitätsbibliothek Göttingen); Andreas Vogel (Universität Leipzig); Mathias Göbel (Niedersächsische Staats- und Universitätsbibliothek Göttingen); Peer Trilcke (Georg-August-Universität Göttingen); Dario Kampkaspar (Herzog August Bibliothek Wolfenbüttel); Christopher Kittel (Karl-Franzens-Universität Graz),"Distant Reading, Literaturgeschichte, Dramatik, Netzwerkanalyse","Datenerkennung, Gestaltung, Netzwerkanalyse, Visualisierung, Literatur, Visualisierung","Der Terminus 'Distant Reading' wurde im Jahr 2000 von Franco Moretti geprägt. Ebenfalls auf Moretti zurück geht das Konzept der ""Maps, Graphs, Trees"" (2005), die Nutzbarmachung verschiedener Visualisierungsmethoden für literaturhistorische Daten. Die Beispiele für die Zusammenführung von Distant-Reading- und Visualisierungs-Ansätzen haben als Hintergrund jedoch fast ausschließlich englischsprachige Korpora. Im Mittelpunkt unseres Distant-Reading-Showcase-Posters steht nun mit der ""Digital Bibliothek"" das größte deutschsprachige Korpus literarischer Texte. Auf einem einzigen A0-Poster werden die Figurennetzwerke von 465 deutschsprachigen Dramen aus den Jahren 1730 bis 1930 gezeigt. Dabei werden verschiedene semantischen Dimensionen vereint. Zum einen folgt die Positionierung der Dramen chronologisch. Neuralgische Punkte der deutschen Literaturgeschichte werden sofort sichtbar, etwa die Explosion des Figurennetzwerks in Goethes ""Götz von Berlichingen"" von 1773. Das bekannte Faktum der damals einsetzenden verstärkten Shakespeare-Lektüre wird so auf einen Blick erkennbar und erscheint im Kontext des zeitlichen Davor und Danach. Eine weitere semantische Ebene ist die Abbildung der Figurennetzwerke mit dem clusternden Visualisierungsverfahren Fruchterman–Reingold. Dadurch werden jenseits der Chronologie (teils bisher unbekannte) Gemeinsamkeiten bei der Konstruktion von Dramen über zwei Jahrhunderte hinweg sichtbar. Eine zusätzliche semantische Ebene bilden die Namen tausender Figuren, wie sie die Bühnen und Dramenanthologien zweiter Jahrhunderte bevölkert haben. Dabei fällt nicht nur auf, dass darunter etwa 24 Faust-Figuren sind (inklusive einer weiblichen ""Faustine""). Dieses Wimmelbild der deutschen Dramenliteratur ist zugleich ein möglicher Wiedereinsteig ins Close Reading, der zeigt, dass sich Close und Distant Reading nicht ausschließen, sondern ergänzen. Die Vorarbeiten zu diesem Poster wurden bereits in Graz präsentiert (Trilcke et al.   2015). Der derzeitige Stand des Projekts erlaubt es uns, auf einem Schaubild die   aktuellen Ergebnisse zu verschränken. Dabei möchten wir mit dem Genre 'Poster'   (unter Beachtung der Data-Ink Ratio nach Edward Tufte) versuchen, digital betriebene   Forschung so ins Zielformat zu übersetzen, dass das Ergebnis als wissenschaftlicher   Showcase für das Feld des Distant Reading dienen kann.",de,Terminus distant Reading Franco moretti prägen ebenfalls moretti Konzept maps graphs Trees Nutzbarmachung verschieden visualisierungsmethoden literaturhistorisch daten Beispiel Zusammenführung Hintergrund fast ausschließlich englischsprachig Korpora Mittelpunkt unser stehen Digital Bibliothek groß deutschsprachig Korpus literarisch Text einzig Figurennetzwerk Deutschsprachig dramen zeigen verschieden Semantische Dimension vereinen folgen Positionierung dramen chronologisch neuralgisch Punkt deutsche Literaturgeschichte sofort sichtbar Explosion Figurennetzwerk goeth Götz Berliching bekannt Faktum einsetzend verstärkt Blick erkennbar erscheinen Kontext zeitlichen semantisch Ebene Abbildung Figurennetzwerk Clusternden visualisierungsverfahren jenseits Chronologie teils unbekannt Gemeinsamkeit Konstruktion Dram jahrhunderte hinweg sichtbar zusätzlich semantisch Ebene bilden Name tausend figuren Bühne dramenanthologi Jahrhunderte bevölkeren fallen inklusive weiblich Faustine Wimmelbild deutsch Dramenliteratur möglich Wiedereinsteig Close Reading zeigen clos distant reading ausschließen ergänzen vorarbeien Poster graz präsentieren trilcke et derzeitig Stand Projekt erlauben Schaubild aktuell Ergebnis verschränken möchten Genre poster Beachtung ratio Edward Tufte versuchen digital betrieben Forschung Zielformat übersetzen Ergebnis wissenschaftlich Showcase Feld distant Reading dienen,"[('reading', 0.25101093244116085), ('figurennetzwerk', 0.25074307718070427), ('distant', 0.20255200572190507), ('jahrhunderte', 0.1716280847527714), ('dramen', 0.1388295808102534), ('moretti', 0.12550546622058043), ('poster', 0.11459729334215199), ('sichtbar', 0.11254720175442795), ('dramenliteratur', 0.11209642143221114), ('neuralgisch', 0.11209642143221114)]"
2016,DHd2016,vortraege-011.xml,Aufbau und Annotation des Kafka/Referenzkorpus,"Berenike Herrmann (Universität Göttingen, Deutschland); Gerhard Lauer (Universität Göttingen, Deutschland)","Korpus, Kafka, Stil, POS, Literarische Moderne","Teilen, Entdeckung, Sammlung, Programmierung, Modellierung, Annotieren, Theoretisierung, Bereinigung, Archivierung, Community-Bildung, Veröffentlichung, Stilistische Analyse, Identifizierung, Crowdsourcing, Kollaboration, Lehre, Artefakte, Computer, Daten, Sprache, Literatur, Metadaten, Personen, Forschung, Forschungsprozess, Standards, Text","Der vorgeschlagene Beitrag dokumentiert das Ineinandergreifen philologischer und informatischer Fragestellungen und Entscheidungen bei Aufbau und Aufbereitung eines digitalen Korpus für vergleichende quantitative Stilanalysen von Franz Kafkas Prosa. In den letzten Jahren haben digitale Ressourcen wie TextGrid, das Deutsche Textarchiv [DTA], und Gutenberg-DE reichhaltige digitale Korpora von historischen Texten (literarischer und nichtliterarischer Art) zur Verfügung gestellt. Kafkas Werk selbst ist zudem fast vollständig digitalisiert. Dennoch liegen derzeit weder ein vollständiges Kafka-Kernkorpus noch ein ""Kafka-Referenzkorpus"" vor, das eine sinnvolle quantitative Analyse seines Sprachgebrauchs durch den Vergleich mit ausreichend großen Stichproben anderer Texte zulässt. Unser Projekt möchte diese Lücke füllen und ein Kafka/Referenzkorpus vorstellen, das sowohl philologisch als auch informatisch solide aufbereitetet ist, und eine hypothesengetriebene aber auch explorative quantitative Stilistik ermöglicht. Bei der Konzeption des Kafka/Referenzkorpus verfolgen wir einen autororientierten Ansatz der digitalen Stilistik. Ausgehend von der Hypothese, dass der Stil eines Autors durch von ihm rezipierte Texte beeinflussbar ist, und dass Stil quantitativ beschreibbar ist (vgl. Herrmann / van Dalen-Oskam / Schöch 2015), gehen wir zunächst vom faktischen textuellen Input Kafkas aus und ergänzen diesen durch Stichproben kanonischer und populärer zeitgenössischer Texte. Der Aufbau des Kafka/Referenzkorpus wird von drei Kriterien geleitet: (a) Vollständigkeit von Kafkas Schriften in der Originalfassung (=Kafka-Kernkorpus); (b) Abbildung von Texten, die Kafkas Schreibprozess beeinflusst haben könnten / Abbildung von Texten, die eine näherungsweise Repräsentativität der Epoche der klassischen Moderne herstellen (=Kafka-Referenzkorpus); (c) eine hohe Akkuratheit bzw. Konsistenz bei informatischer Vorverarbeitung wie Normalisierung, linguistischer Annotation ( Das Kafka-Kernkorpus (je nach Zählart ca. 120 Texte) wurde dabei intern in die Dimensionen Kafka_Publikation (zu Lebzeiten/posthum) und Kafka_Genre (Literarisch, Brief, Tagebuch, Amtliche Schriften) unterteilt. Das Referenzkorpus (ca. 8.000 Texte) wurde hauptsächlich aus TextGrid, DTA, Gutenberg-DE und Gutenberg.org extrahiert, und beinhaltet Metadaten zu Autor (Name, Gender), Publikationsdatum und -Ort, sowie Gattung. Es umfasst literarische Texte der Kategorien ""kanonisch"" und ""populär"" ebenso wie Gebrauchstexte. Neben Kinder- und Jugendliteratur die Kafka rezipierte sind hier auch Sach- und Fachliteratur von Interesse, nicht zuletzt weil Kafkas Stil durch Elemente der Fachsprache, aber auch ein hochsprachliches ""Prager Deutsch"" ohne sozio- oder dialektale Einflüsse geprägt sein soll (vgl. Nekula 2003). Zur Korpuszusammensetzung wurden Aufzeichnungen zu Kafkas Lesegewohnheiten untersucht, wobei Zeugnisse über seine Bibliothek, biographische Berichte, aber auch Dokumente zur zeitgenössischen Rezeption sowie Autor- und Werk-Indices in Literaturgeschichten konsultiert wurden (Blank 2001; Born 1990; Born / Koch 1983; Born / Mühlfeit 1979). Das Ergebnis dieses Forschungsschrittes ist eine Liste von 765 Titeln, die das Metadatum ""in Kafkas Bibliothek"" tragen, und einen Schwerpunkt zu Kafkas Lebzeiten setzen, aber eben auch Werke von älteren Autoren wie Goethe und Kleist, sowie Flaubert und Dostojewski (in Übersetzung) beinhalten. Dass die von uns einbezogenen Online-Repositorien hinsichtlich der editionsphilologischen Textqualität variieren ist ein hinzunehmendes Übel, dem wir zum einen pragmatisch (Wahl der bestmöglichen verfügbaren Ausgabe; Ziel, die Fehlermarge unter 2% zu halten), zum anderen unter Hinweis auf die flexible Struktur des Korpus (Austausch durch eine qualitativ hochwertigere Version ist möglich) begegnen. Durch die nahtlose Dokumentation des Korpus wird zudem die nötige Transparenz gewährleistet um auch Nachnutzern flexible Kontrolle der Daten zu ermöglichen. Die Hauptaufgabe der informatischen Dimension des Projektes   besteht neben der Einbettung in einen praktikablen und anschlussfähigen Workflow   (eXist Datenbank) und der Homogenisierung und informatischen Aufbereitung der   Ausgangsdaten (Tokenisierung, Normalisierung, Lemmatisierung) in einer reliablen   linguistischen Annotation auf POS (STTS Tagset). Wortarten gelten als verlässliche   Indikatoren für Register und Genrevariation (vgl. z. B. Biber / Conrad 2009), und   sind im Vergleich mit anderen Variationsmarkern durch eine relativ akkurate   automatische Annotation besonders praktikabel. Obwohl bei der POS-Annotation gute   Accuracy für das gegenwärtige Standarddeutsch mithilfe von Unser Projekt dokumentiert in seinem gegenwärtigen Status Entscheidungen auf  verschiedenen konzeptionellen, analytischen und prozeduralen Ebenen. Es zeigt, dass  der Aufbau eines digitalen Autor-Korpus, das den quantitativen Vergleich mit  synchronen und diachronen Daten erlauben soll, bei Weitem keine triviale Aufgabe  darstellt. So wird zum Beispiel deutlich, wie Forschungsfragen beziehungsweise  Hypothesen zur Konstitution von Schreibweisen und Autorschaft die Korpuskompilation  steuern 'und deshalb auf einer möglichst präzisen Modellierung der  zugrundeliegenden textwissenschaftlichen Theorien fußen sollten. Gleichzeitig sind  Metadaten (u. a. Autor, Titel, Publikationsdatum, Publikationsort, Genre) und  linguistische Parameter (wie POS) gerade die Ansatzpunkte, an denen philologische  Fragestellungen in präzise und praktikable Kategorien umgewandelt werden können.  Nicht zuletzt deshalb sollten literarische Daten in flexiblen Architekturen  gespeichert werden, die zusätzliche Annotationsebenen zulassen 'denn hermeneutische  Erkenntnisprozesse stellen eine erwachsene Stärke der Geisteswissenschaften dar, die  auch im digitalen Zeitalter einen explizit modellierten Platz einnehmen muss.",de,vorgeschlagen Beitrag dokumentieren Ineinandergreifen philologisch informatisch Fragestellung Entscheidung Aufbau Aufbereitung digital Korpus vergleichend quantitativ stilanalysen Franz Kafka Prosa letzter digital Ressource Textgrid deutsch Textarchiv dta reichhaltig digital Korpora historisch Text literarisch nichtliterarisch Art Verfügung stellen Kafka Werk zudem fast vollständig digitalisieren dennoch liegen derzeit weder vollständig sinnvoll quantitativ Analyse Sprachgebrauch Vergleich ausreichend Stichprobe anderer Text zulässt Projekt Lücke füllen Kafka Referenzkorpus vorstellen sowohl philologisch informatisch solide aufbereitetet hypothesengetriebene explorativ quantitativ Stilistik ermöglichen Konzeption Kafka Referenzkorpus verfolgen autororientiert Ansatz digital Stilistik ausgehend Hypothese Stil Autor rezipiert Text beeinflussbar Stil quantitativ beschreibbar Herrmann van schöch faktisch textuell Input Kafka ergänzen Stichprobe kanonisch Populärer zeitgenössisch Text Aufbau Kafka Referenzkorpus kriterien leiten Vollständigkeit Kafka Schrift Originalfassung b Abbildung Text Kafka schreibprozess beeinflussen können Abbildung Text näherungsweise Repräsentativität Epoche klassisch modern Herstelle c hoch Akkuratheit Konsistenz informatisch Vorverarbeitung Normalisierung linguistisch Annotation Zählart Text intern dimension lebzeiten posthum literarisch brief Tagebuch amtlich Schrift unterteilen Referenzkorpus Text hauptsächlich Textgrid Dta extrahiern beinhalten metadaten Autor Name Gender Publikationsdatum Gattung umfassen literarisch Text kategorie kanonisch populär gebrauchstexte jugendliteratur Kafka rezipieren Fachliteratur Interesse zuletzt Kafka Stil Elemente fachsprach hochsprachlich Prager deutsch dialektal einflüsse prägen nekula Korpuszusammensetzung Aufzeichnung Kafka lesegewohnheiten untersuchen wobei zeugnisse Bibliothek biographisch Bericht dokument zeitgenössisch Rezeption literaturgeschichten konsultieren Blank born Born koch born mühlfeit Ergebnis forschungsschritt Liste Titel Metadatum Kafka Bibliothek tragen schwerpunken Kafka Lebzeiten setzen werk alt Autor Goethe Kleist flauberen Dostojewski Übersetzung beinhalen einbezogen hinsichtlich editionsphilologisch Textqualität variieren hinzunehmend Übel pragmatisch Wahl bestmöglich verfügbar Ausgabe Ziel Fehlermarge halten Hinweis flexibel Struktur Korpus austausch qualitativ hochwertig Version begegnen nahtlos Dokumentation korpus zudem nötig Transparenz gewährleisten nachnutzern flexibel Kontrolle daten ermöglichen Hauptaufgabe informatisch Dimension projektes bestehen Einbettung praktikabl anschlussfähig workflow exisen Datenbank Homogenisierung informatisch Aufbereitung Ausgangsdat Tokenisierung Normalisierung Lemmatisierung reliabl linguistisch Annotation pos stts tagset wortarten gelten verlässlich indikatoren Register Genrevariation biber Conrad Vergleich variationsmarkern relativ akkurat automatisch Annotation praktikabel obwohl accuracy gegenwärtig standarddeutsch Mithilfe Projekt dokumentieren gegenwärtig Status Entscheidung verschieden konzeptionell analytischen Prozedurale Ebene zeigen Aufbau digital quantitativ Vergleich Synchrone diachron daten erlauben weit trivial Aufgabe darstellen deutlich forschungsfrag beziehungsweise Hypothese Konstitution Schreibweise Autorschaft Korpuskompilation Steuer möglichst präzies Modellierung zugrundeliegend textwissenschaftlich Theorie fußen gleichzeitig Metadat Autor Titel Publikationsdatum Publikationsort Genre linguistisch parameter pos ansatzpunken philologisch Fragestellung präzise praktikabel Kategorie umwandeln zuletzt literarisch daten Flexibl architekturen speichern zusätzlich annotationseben zulassen hermeneutisch erkenntnisprozesse stellen erwachsen Stärke geisteswissenschaften dar digital Zeitalter explizit modelliert Platz einnehmen,"[('kafka', 0.5317032995958494), ('informatisch', 0.19519273089243058), ('referenzkorpus', 0.16797190910574225), ('born', 0.14527079798370288), ('quantitativ', 0.11069335323205565), ('stil', 0.10668706567491051), ('philologisch', 0.10499974424802574), ('text', 0.09859741327436093), ('lebzeiten', 0.09684719865580192), ('praktikabel', 0.09684719865580192)]"
2016,DHd2016,posters-063.xml,CRETA (Centrum für reflektierte Textanalyse) 'Fachübergreifende Methodenentwicklung in den Digital Humanities,"Jonas Kuhn (Universität Stuttgart, Deutschland); Artemis Alexiadou (Universität Stuttgart, Deutschland); Manuel Braun (Universität Stuttgart, Deutschland); Thomas Ertl (Universität Stuttgart, Deutschland); Sabine Holtz (Universität Stuttgart, Deutschland); Cathleen Kantner (Universität Stuttgart, Deutschland); Catrin Misselhorn (Universität Stuttgart, Deutschland); Sebastian Pado (Universität Stuttgart, Deutschland); Sandra Richter (Universität Stuttgart, Deutschland); Achim Stein (Universität Stuttgart, Deutschland); Claus Zittel (Universität Stuttgart, Deutschland)","Methodenentwicklung, Textanalyse, Modulariserung","Inhaltsanalyse, Beziehungsanalyse, Modellierung, Annotieren, Kontextsetzung, Theoretisierung, Bearbeitung, Visualisierung, Infrastruktur, Interaktion, Sprache, Literatur, Methoden, Personen, Projekte, Forschung, Forschungsprozess, Text, Werkzeuge, Visualisierung","Dieser Beitrag soll das Konzept des neu eingerichteten Stuttgarter DH-Zentrums CRETA Das methodische Konzept hinter CRETA geht einerseits aus von der strukturellen Gleichartigkeit vieler Teilfragestellungen über ganz unterschiedliche Teilgebiete der Digital Humanities hinweg (eingebettet in sehr unterschiedliche Gesamtzusammenhänge und methodische Rahmenbedingungen). Beispielsweise findet sich das Teilziel einer systematischen Kategorisierung von Relationen, die in einer Textquelle zwischen zwei realen oder fiktionalen Personen ausgedrückt ist, in geschichtswissenschaftlichen Fragestellungen ebenso wie in sprach-, literatur- oder sozialwissenschaftlichen Gesamtuntersuchungen. Abbildung 1 skizziert weitere Typen von wiederkehrenden Fragestellungen, die disziplinübergreifend bei der Auseinandersetzung mit Texten (und allgemeiner mit kulturellen Werken) auftauchen und bei deren Modellierung daher Synergien zu erwarten sind. Eine komputationelle Modellierung des Teilfrage-Typs kann so für ganz unterschiedliche Rahmenuntersuchung die Erschließung größerer Korpora per Aggregation über Aspekte des Textinhalts bzw. der –form erschließen.  Zugleich anerkennt das methodische Konzept die Unterschiedlichkeit sowohl der jeweiligen inneren Ausprägung der Fragestellung (so gehen im genannten Beispiel Texteigenschaften und Relationstypen weit auseinander) als auch der interpretatorischen Anforderungen, die sich aus dem jeweiligen Modellierungs- und Fragekontext ergeben. Das technische Ziel, eine einzige optimale Werkzeuglösung für jede der fachübergreifend identifizierten Teilfragen zu entwickeln bzw. aus dem Der zentrale CRETA-Gedanke zur Erschließung von disziplinübergreifenden Synergien ist folgender: Für eine praktisch umsetzbare und dennoch methodisch adäquate Integration in die jeweilige Gesamtfragestellung kann es vorteilhaft sein, Modellinstanzen anfänglich auch über Kontexte hinweg zu übertragen, deren Randbedingungen nicht in vollem Maße übereinstimmen, die eingebetteten Teilmodelle aber sehr bewusst als vorläufig anzusehen 'als Gegenstand eines Folgerichtig wird bei aufgedeckten Unzulänglichkeiten die Die notwendigen Anpassungen der vorläufigen Modellinstanzen lassen sich mit Techniken   aus der Informatik (insbes. maschinellen Lernverfahren) prinzipiell ohne weiteres   umsetzen 'dabei muss jedoch die Zielrichtung der Optimierung vorgegeben sein (beim   maschinellen Lernen in der Regel unterschiedliche Eingabe- /   Ausgabe-""Trainingsdatensätze"", anhand derer die Parameter für eine gegebene   Modellklasse eingestellt werden). Und hier beginnt die eigentliche Herausforderung   für eine echte fachübergreifende Methodenintegration: selbst wenn man 'rein   hypothetisch 'für eine geisteswissenschaftliche Naheliegender Weise wird man vielmehr versuchen, Modelle für relativ eng umrissene Teilfragestellungen empirisch zu optimieren, die dann in ein Geflecht von Analyseschritten einfließen. Der Annotationsaufwand für die Erzeugung von Referenzdaten hält sich damit in vertretbaren Grenzen und Erkenntnisse zu studienübergreifend gleichartigen Teilaspekten lassen sich so systematisch übertragen. Der Identifikation von sinnvollen Teilfragestellungen, die über unterschiedliche Projekt- und Fachkontexte hinweg tragen 'einer ""Modularisierung"" 'kommt also auch aus praktischen Erwägungen heraus eine zentrale Bedeutung zu. Was aus informatischer Sicht wie eine Binsenweisheit klingt, ist jedoch in der Modellierungspraxis extrem anspruchsvoll, ist bei vielen übergeordneten Fragen eine Untergliederung in effektive Teilschritte doch alles andere als klar. Eine Vorstrukturierung auf dem Reißbrett ist nur in Einzelfällen möglich (wie im Fall der Sprachwissenschaft mit ihrer etablierten Ebenenstruktur der Sprachbetrachtung möglich ist, die auch die computerlinguistische Modulstruktur prägt, selbst wenn bewusst klassische Teilschritte kombiniert werden). Für alle offenen Fragen der Modularisierung bietet die komputationelle Modellierung und die Verwendung von digitalen Arbeitsumgebungen Potenziale, die noch lange nicht ausgeschöpft sind: alternative Modularisierungen können exploriert und gegeneinander abgewogen werden. Der CRETA-Ansatz legt diese Exploration in die interdisziplinären Verantwortung: statt auf dem Reißbrett die plausibelste Untergliederung einer Projekt-Problematik festzuhalten, Softwarelösungen anhand dieser Spezifikation umzusetzen und nach zwei Jahren Entwicklung auf die inhaltliche Fragestellung anzuwenden, findet ein Dialog zwischen komputationellen Modellierungsexpertinnen und –experten und Fachwissenschaftlerinnen und –wissenschaftlern unterschiedlicher Disziplinen statt. Überlegungen aus der fachspezifischen Kultur der Fragestellung müssen herangezogen werden, um eine geeignete Einbindung eines technisch übertragbaren Teilmodells in den Erkenntnisprozess und seine methodenkritische Reflexion zu gewährleisten. Gleichzeitig fleißen aus den informatischen Disziplinen Überlegungen zur formalen Adäquatheit möglicher Modellklassen, Erfahrungswerte aus der zu erwartenden Qualität, sowie Möglichkeiten einer Visualisierung und explorativen Ergebnispräsentation ein, um die wechselseitige Optimierung von Modellierungskomponenten zu unterstützen. Konkret stellt sich das Vorgehen bei der Modellierung folgendermaßen dar: Im multidisziplinären Dialog im Rahmen von Werkstattklausuren werden für geistes- und sozialwissenschaftliche Fragestellungen mit Bezug zu ausgewählten digitalen Ressourcensammlungen Die angesprochenen methodischen Desiderate eines transparenten Zugangs zu den  Analyse-Teilergebnissen und der Adaptierbarkeit von analytischen Teilmodellen haben  wir exemplarisch anhand mehrerer Erweiterungsszenarien des  Relationsextraktionsmodells aus Blessing und Kuhn (2014) umgesetzt: Über die  ursprüngliche Zielrelation (Emigrationsbewegungen, die in Kurzbiographien textuell  beschrieben werden) können andere Relationen interaktiv trainiert werden. Eine  Erweiterung des Korpusbestands um Texte aus weiteren Quellen wurde vorgenommen,  einschließlich eines Wechsels der Sprache (Übertragung des Teilmodells als  Erweiterung einer deutschen Analysekette auf eine französische). Eine analoge  Adaptionsplattform wurde für Zeitungstexte erstellt, die in  politikwissenschaftlichen Studien zum öffentlichen Diskurs analysiert werden. Fallstudien zum Einsatz der resultierenden Analysekette zeigen, dass eine kritische  Betrachtung der übertragenen Teilmodelle vor allem durch den Wechsel des  Blickwinkels auf aggregierte Daten mit einer Verlinkung von Einzelinstanzen  unterstützt werden: Textuelle Einzelinstanzen eines Relationstyps (z. B. Emigration  einer Person X aus dem Land A in ein Land B) werden aggregiert und das  Aggregationsergebnis kann beispielsweise geographisch visualisiert werden. Das interaktive Springen zwischen unterschiedlichen Dimensionen der Aggregation bzw. zwischen aggregierter Sicht und Einzelinstanzen erlaubt es, Datenpunkte gezielt unter die Lupe zu nehmen, die von allgemeinen Tendenzen in bestimmter Weise abweichen. Für solche Beobachtungen ist zu klären, ob es sich (a) um einen aus bekannten Zusammenhängen erklärbaren, (b) einen neuartigen, validen Effekt oder (c) um einen technisch erklärbaren Scheineffekt handelt, der durch eine methodische Verbesserung eliminiert werden könnte. Ein Beispiel ist die fehlerhafte Klassifikation von UN-Resolution 1261 und 1973 in Zeitungsartikeln als Datumsangaben. Bei der Visualisierung der Extraktionsergebnisse auf einem Zeitstrahl fällt ein unerwartetes Muster beim Jahr 1261 auf (während Scheineffekte zum Jahr 1973 möglicherweise zunächst unerkannt bleiben). Die Fallstudien unterstützen die These, dass interaktive Nachforschungen und ein adaptierbares Instrumentarium gerade bei nicht perfekten Analysekomponenten die kritische Distanz zum Modellinventar unterstützen.",de,Beitrag Konzept neu eingerichtet Stuttgarter Creta methodisch Konzept Creta einerseits strukturell Gleichartigkeit vieler Teilfragestellung unterschiedlich teilgebiete Digital Humanitie Hinweg einbetten unterschiedlich Gesamtzusammenhäng methodisch Rahmenbedingung beispielsweise finden Teilziel systematisch Kategorisierung relationen Textquelle real fiktional Person ausdrücken geschichtswissenschaftlich Fragestellung sozialwissenschaftlich Gesamtuntersuchunge Abbildung Skizziert Typ wiederkehrend Fragestellung disziplinübergreifend Auseinandersetzung Text allgemein kulturell Werk auftauchen Modellierung Synergi erwarten komputationell Modellierung unterschiedlich Rahmenuntersuchung Erschließung groß Korpora per Aggregation Aspekt Textinhalt Form erschließen anerkennen methodisch Konzept Unterschiedlichkeit sowohl jeweilig innerer Ausprägung Fragestellung genannt texteigenschafen relationstypen auseinander interpretatorische anforderungen jeweilig Fragekontext ergeben technisch Ziel einzig optimal Werkzeuglösung fachübergreifend identifiziert Teilfrage entwickeln zentral Erschließung disziplinübergreifend Synergie folgend praktisch umsetzbar dennoch methodisch adäquat Integration jeweilig Gesamtfragestellung vorteilhaft modellinstanzen anfänglich Kontexte Hinweg übertragen Randbedingung voll Maß übereinstimmen eingebettet Teilmodell bewussen vorläufig ansehen Gegenstand folgerichtig aufgedeckt Unzulänglichkeit notwendig Anpassung vorläufig Modellinstanz lassen Technik Informatik insbes Maschinelle lernverfahren prinzipiell umsetzen Zielrichtung Optimierung vorgeben Maschinellen lernen Regel unterschiedlich anhand Derer parameter gegeben Modellklasse einstellen beginnen eigentlich Herausforderung echt fachübergreifend Methodenintegration rein hypothetisch geisteswissenschaftlich Naheliegender Weise vielmehr versuchen Modell relativ eng umrissen Teilfragestellung empirisch optimieren Geflecht analyseschritte einfließen Annotationsaufwand Erzeugung Referenzdat halten vertretbar Grenze erkenntnis studienübergreifend gleichartig Teilaspekt lassen systematisch übertragen Identifikation Sinnvoll Teilfragestellung unterschiedlich Fachkontexte Hinweg tragen Modularisierung praktisch Erwägung heraus zentral Bedeutung informatisch Sicht Binsenweisheit klingen Modellierungspraxis extrem anspruchsvoll übergeordnet Frage Untergliederung effektiv Teilschritte klar Vorstrukturierung Reißbrett einzelfällen Fall Sprachwissenschaft etabliert Ebenenstruktur Sprachbetrachtung computerlinguistisch Modulstruktur prägen bewussen klassisch Teilschritte kombinieren offen Frage Modularisierung bieten komputationell Modellierung Verwendung digital Arbeitsumgebunge Potenziale ausschöpfen alternativ Modularisierunge explorieren gegeneinander Abgewog legen Exploration interdisziplinär Verantwortung Reißbrett plausibel Untergliederung festhalten Softwarelösung anhand Spezifikation umsetzen Entwicklung inhaltlich Fragestellung anwenden finden Dialog komputationell Modellierungsexpertinne Experte fachwissenschaftlerinnen Wissenschaftler unterschiedlich disziplinen Überlegung fachspezifisch Kultur Fragestellung heranziehen geeignet Einbindung technisch übertragbar Teilmodell Erkenntnisprozess methodenkritisch Reflexion gewährleisten gleichzeitig Fleiße informatisch disziplin Überlegung formal adäquatheit möglich modellklassen erfahrungsweren erwartend Qualität Möglichkeit Visualisierung Explorative Ergebnispräsentation wechselseitig Optimierung modellierungskomponenten unterstützen konkret stellen vorgehen Modellierung folgendermaßen dar multidisziplinär Dialog Rahmen Werkstattklausur sozialwissenschaftlich Fragestellung Bezug ausgewählt digital ressourcensammlunger angesprochen methodisch Desiderate transparent Zugang Adaptierbarkeit analytisch Teilmodelle exemplarisch anhand mehrere Erweiterungsszenarie Relationsextraktionsmodell Blessing Kuhn umsetzen ursprünglich Zielrelation emigrationsbewegungen Kurzbiographien Textuell beschreiben Relation interaktiv trainieren Erweiterung Korpusbestand Text quellen vornehmen einschließlich Wechsel Sprache übertragung Teilmodell Erweiterung deutsch Analysekette französisch Analoge Adaptionsplattform Zeitungstexte erstellen politikwissenschaftlich Studie öffentlich Diskurs analysieren Fallstudie Einsatz resultierend Analysekette zeigen kritisch Betrachtung übertragen Teilmodelle Wechsel Blickwinkels aggregiert daten Verlinkung Einzelinstanze unterstützen textuell einzelinstanzen Relationstyp Emigration Person x Land Land b aggregieren Aggregationsergebnis beispielsweise geographisch visualisieren Interaktive Springen unterschiedlich Dimension Aggregation aggregiert Sicht Einzelinstanze erlauben datenpunken gezielt Lupe nehmen Tendenz bestimmt Weise abweichen beobachtung klären bekannt Zusammenhäng erklärbaren b neuartig validen Effekt c technisch erklärbar Scheineffekt handeln methodisch Verbesserung eliminieren fehlerhaft Klassifikation zeitungsartikeln datumsangaben Visualisierung extraktionsergebnisse zeitstrahl fallen unerwartet Muster Scheineffekt möglicherweise unerkannt bleiben Fallstudie unterstützen These interaktiv Nachforschung adaptierbar Instrumentarium Perfekt Analysekomponent kritisch Distanz Modellinventar unterstützen,"[('teilfragestellung', 0.16654212173829996), ('teilmodell', 0.16654212173829996), ('methodisch', 0.16024816381457405), ('komputationell', 0.1470182015436913), ('fragestellung', 0.13784504153051308), ('unterstützen', 0.11350511839018235), ('analysekette', 0.11102808115886664), ('einzelinstanze', 0.11102808115886664), ('untergliederung', 0.11102808115886664), ('reißbrett', 0.11102808115886664)]"
2016,DHd2016,vortraege-032.xml,Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts,"Matthias Boenig (Berlin-Brandenburgische Akademie der Wissenschaften - Berlin, Deutschland); Kay-Michael Würzner (Berlin-Brandenburgische Akademie der Wissenschaften - Berlin, Deutschland); Arne Binder (Berlin-Brandenburgische Akademie der Wissenschaften - Berlin, Deutschland); Uwe Springmann (Centrum für Informations- und Sprachverarbeitung - Ludwig-Maximilians-Universität München, Deutschland)","OCR, 17. Jahrhundert, Texterfassung, Ocropus, Tesseract","Umwandlung, Datenerkennung, Transkription, Programmierung, Modellierung, Annotieren, Bearbeitung, Computer, Datei, Text","Dieser Beitrag stellt eine neuartige Methode zur optischen Zeichenerkennung ( OCR bezeichnet die Gesamtheit von Verfahren, die in der Lage sind, aus  Rastergrafiken Schriftzeichen zu erkennen. Der Begriff wird sowohl für die  eigentliche Mustererkennung als auch für den gesamten Prozess der  Bildverarbeitung verwendet. Letzterer gliedert sich normalerweise in drei  Schritte: Grundsätzlich lassen sich bei OCR zwei unterschiedliche Erkennungsansätze unterscheiden: zeichenorientierte Verfahren wie Tesseract vergleichen das Bild eines Zeichens Pixel für Pixel mit einer Datenbasis (dem sog. Modell) und geben das ähnlichste Zeichen zurück. Sequenzorientierte (segmentierungsfreie) Verfahren wie OCRopus legen ein Raster fester Größe über eine Zeile und bestimmen anhand der Folgen der einzelnen Spalten, repräsentiert als Bitvektoren (0 entspricht weiß, 1 schwarz) die wahrscheinlichste Zeichensequenz. Unsere Studie beschäftigt sich mit OCR am Beispiel von Gelegenheitsgedichten  des 17. Jahrhunderts, denen durch die von Segebrecht (1977) initiierte  literaturwissenschaftliche Neubewertung eine zunehmende kulturgeschichtliche  Bedeutung zukommt (vgl. Klöker 2010: 39). Der Zugriff auf diese Drucke wurde  durch das 111 Funeralschriften Simon Dachs wurden im Verlauf des DFG-Pilotprojektes zum  Abbildung 1 gibt einen Überblick über den Arbeitsablauf der hier vorgestellten Methode. Im Unterschied zu existierenden Workflows unterteilt unser Vorschlag die Bildoptimierung in zwei Phasen: 1. Unser Vorgehen bei der OCR orientiert sich an der manuellen Texterfassung per Die Güte der hier vorgestellten Methode wird anhand der Volltexterfassung von Funeralschriften Simon Dachs (vgl. 1.2) evaluiert. Dabei konzentriert sich die Evaluation auf drei Punkte: Ein typisches Beispiel für die Untersuchungsgrundlage sowie die entsprechenden OCR-Ausgaben gibt Abbildung 2.  Voraussetzung für die Evaluation und das Modelltraining ist fehlerfreier  Volltext ( Für das Training der spezifischen OCR-Modelle wurden 30 Seiten Ground-Truth zufällig ausgewählt. Für die Evaluation der Modelle wurden 25 andere zufällig ausgewählte Seiten verwendet. Zur Vereinigung beider OCR-Versionen wurde ein Referenzlexikon gültiger historischer Schreibungen des 17. Jahrhunderts herangezogen. Dazu wurden Wortformen ( Für Beschneidung und Begradigung wurde das Programm Die einzelnen Textzonen (Abschnitte und Kustoden) wurden mit Hilfe von Die Zeichenerkennung erfolgte sowohl mit OCRopus als auch mit Tesseract. Die erste Versuchsreihe basierte auf mitgelieferten Modellen. Für die zweite Versuchsreihe wurden die OCR-Programme mit Ground-Truth-Daten trainiert. Für das Training der OCRopus-Modelle wurde OCRopus eingesetzt. Dabei wurde für das Training aus Gründen der Modellvergleichbarkeit eine feste Anzahl von Iterationsschritten ( Die Textvereinigung wurde in Die Bestimmung der Textqualität erfolgte durch Messung des Anteils falsch erkannter Zeichen (Fehlerrate in Prozent) im Vergleich zum fehlerfreien Volltext. Tabelle 1 gibt einen Überblick über die Ergebnisse der Evaluation bzgl. der  Fehlerrate auf Zeichenebene unter Berücksichtigung der Vorverarbeitung des  Trainings- und Testmaterials, der Modellklasse (standard vs. spezifisch) und der  eingesetzten OCR-Software (OCRopus, Tesseract). Das beste (  Die geringste erreichte Fehlerrate (3,89 %) liegt etwa im Bereich der Textgenauigkeit der 111 Gedichte aus der Pilotstudie von Federbusch (Federbusch / Polzin 2013). Die Fehlerrate von Tesseract ist jeweils höher als die von OCRopus. Der sequenzorientierte Ansatz hat klare Vorteile bei der Erkennung von Schriftzeichen, die die typischen Charakteristika früher Drucke aufweisen. Desweiteren zeigt sich, dass die Vorverarbeitung mit nlbin für Tesseract sowohl auf Trainings- als auch auf Testebene jeweils schlechtere Ergebnisse bringt. Für OCRopus sind die Ergebnisse bzgl. der Vorverarbeitung differenzierter: Die beste Kombination liefert eine Vorverarbeitung des Trainingsmaterials mit nlbin bei einer nachfolgenden Vorverarbeitung des Testmaterials mit Scantailor. Unterschiede im Ergebnis der Vorverarbeitung beider Programme illustriert Abbildung 3. Abb. 3: Bild einer Textzeile nach der Vorverarbeitung mit nlbin (oben) und Scantailor (unten). Die von Scantailor durchgeführte Bildvorverarbeitung ist deutlich normativer und für einen zeichenorientierten Ansatz wie Tesseract besser geeignet. Das Training sequenzorientierter Ansätze leidet unter dieser Vergröberung. Es zeigt sich erneut, dass spezifisch trainierte Modelle eine massive Textgenauigkeitsverbesserung mit sich bringen können (vgl. auch Springmann et al. 2015). Betrachtet man die Beispielausgaben in Abbildung 2, so wird der Qualitätsunterschied zwischen beiden OCR-Programmen ersichtlich. An einzelnen Stellen jedoch (z. B. Großbuchstaben am Anfang der Zeile im letzten Abschnitt) hat Tesseract Erkennungsvorteile. Ausgehend von diesem Befund wurde der jeweils genaueste Text von OCRopus und Tesseract miteinander vereinigt. Es hat sich gezeigt, dass die Konfidenzen, die die Programme für jedes Zeichen zurückliefern, kein verlässliches Kriterium sind, um Konflikte aufzulösen. Die Fehlerrate nimmt zu. Die Strategie, Wörter bzw. Sequenzen zu bevorzugen, die sich im Referenzlexikon befinden, hat dagegen eine messbare Verbesserung mit sich gebracht. Die Anzahl der falsch erkannten Zeichen konnte um 14 % reduziert werden (Fehlerrate 3,34 %). Es ist zu vermuten, dass der Effekt größer wäre, wenn zwei OCR-Ergebnisse mit vergleichbarer Qualität vorlägen. Dies bleibt jedoch zum jetzigen Zeitpunkt für Drucke des 17. Jahrhunderts ein Desiderat.",de,Beitrag stellen neuartig Methode optisch Zeichenerkennung ocr bezeichnen Gesamtheit Verfahren Lage Rastergrafike schriftzeichen erkennen Begriff sowohl eigentlich Mustererkennung gesamt Prozess Bildverarbeitung verwenden Letzterer gliederen normalerweise Schritt grundsätzlich lassen ocr unterschiedlich Erkennungsansätz unterscheiden zeichenorientiert Verfahren tesseract vergleichen Bild zeichens Pixel Pixel datenbasis Modell geben ähnlichste Zeichen Sequenzorientiert segmentierungsfrei Verfahren Ocropus legen rast fest Größe Zeile Bestimme anhand Folge einzeln Spalte repräsentieren bitvektoren entsprechen wissen schwarz wahrscheinlichster Zeichensequenz Studie beschäftigen ocr gelegenheitsgedichten Jahrhundert Segebrecht initiiert literaturwissenschaftlich Neubewertung zunehmend Kulturgeschichtliche Bedeutung zukommen Klöker Zugriff Druck funeralschrift Simon Dachs Verlauf Abbildung Überblick Arbeitsablauf vorgestellt Methode Unterschied existierend Workflow unterteilen Vorschlag Bildoptimierung Phase vorgehen ocr orientieren manuell Texterfassung per Güte vorgestellt Methode anhand Volltexterfassung Funeralschrift Simon Dachs evaluieren konzentrieren Evaluation Punkt typisch untersuchungsgrundlag entsprechend Abbildung Voraussetzung Evaluation Modelltraining fehlerfrei Volltext Training spezifisch Seite zufällig auswählen Evaluation Modell zufällig ausgewählt Seite verwenden Vereinigung beide Referenzlexikon gültig Historischer Schreibunge Jahrhundert heranziehen Wortforme Beschneidung Begradigung Programm einzeln Textzon abschnitt kustod Hilfe Zeichenerkennung erfolgen sowohl Ocropus tesseract Versuchsreihe basieren Mitgeliefert modellen Versuchsreihe trainieren Training Ocropus einsetzen Training Grund Modellvergleichbarkeit fest Anzahl iterationsschritten Textvereinigung Bestimmung Textqualität erfolgen Messung Anteil falsch erkannt Zeichen Fehlerrat Prozent Vergleich Fehlerfreien Volltext Tabelle Überblick Ergebnis Evaluation Fehlerrate zeichenebene Berücksichtigung Vorverarbeitung testmaterial Modellklasse Standard spezifisch eingesetzt Ocropus tesseract gut gering erreicht fehlerrat liegen Bereich Textgenauigkeit Gedicht Pilotstudie federbusch federbusch Polzin Fehlerrat Tesseract jeweils hoch Ocropus sequenzorientiert Ansatz klar Vorteil Erkennung schriftzeichen typisch Charakteristika Druck aufweisen desweiteren zeigen Vorverarbeitung Nlbin Tesseract sowohl Testebene jeweils schlecht Ergebnis bringen Ocropus Ergebnis Vorverarbeitung differenziert gut Kombination liefern Vorverarbeitung Trainingsmaterial Nlbin nachfolgend Vorverarbeitung testmaterials Scantailor Unterschied Ergebnis Vorverarbeitung beide Programm illustrieren Abbildung abb Bild Textzeil Vorverarbeitung Nlbin scantailor unten Scantailor durchgeführt Bildvorverarbeitung deutlich normativ zeichenorientiert Ansatz Tesseract geeignet Training sequenzorientiert Ansatz leiden Vergröberung zeigen erneut spezifisch trainiert modell massiv Textgenauigkeitsverbesserung bringen Springmann et betrachten Beispielausgabe Abbildung qualitätsunterschied ersichtlich einzeln Stelle großbuchstaben Anfang Zeile letzter Abschnitt Tesseract Erkennungsvorteile ausgehend Befund jeweils genauest Text Ocropus Tesseract miteinander vereinigen zeigen Konfidenze Programm jeder Zeichen zurückliefern verlässlich Kriterium Konflikt auflösen Fehlerrat nehmen Strategie Wörter Sequenzen bevorzugen Referenzlexikon befinden messbar Verbesserung bringen Anzahl falsch erkannt Zeichen reduzieren fehlerrat vermuten Effekt groß vergleichbar Qualität vorlägen bleiben jetzig Zeitpunkt drucken Jahrhundert desiderat,"[('tesseract', 0.3983326876683069), ('ocropus', 0.3485411017097685), ('vorverarbeitung', 0.24291988222844343), ('fehlerrat', 0.21977231174991588), ('scantailor', 0.14937475787561505), ('sequenzorientiert', 0.14937475787561505), ('nlbin', 0.14937475787561505), ('ocr', 0.14495281863401516), ('zeichen', 0.13611514210518758), ('training', 0.1199608393692936)]"
2016,DHd2016,vortraege-005.xml,Die Corpusanalyse multimodaler Erzählungen am Beispiel graphischer Romane,"Alexander Dunst (Universität Paderborn, Deutschland); Rita Hartel (Universität Paderborn, Deutschland)","Corpusanalyse, multimodale Erzählungen, Comics, Visualisierung","Datenerkennung, Aufzeichnung, Programmierung, Inhaltsanalyse, Räumliche Analyse, Annotieren, Theoretisierung, Netzwerkanalyse, Stilistische Analyse, Visualisierung, Bilder, Literatur, Methoden, Multimodale Kommunikation","Dieser Vortrag präsentiert Analysen und Visualisierungen eines derzeit im Aufbau befindlichen Corpus an graphischen Romanen (oder ""Graphic Novels"", einer Unterform des Medium Comics) und stellt den für die Annotation dieser multimodalen Erzählform entwickelten Editor vor, der zeitgerecht zur DHd-Jahrestagung in Leipzig für den Download zur Verfügung stehen wird. Während sich die Analyse literarischer Text-Corpora bereits seit mehreren Jahren im Fokus der Digitalen Geisteswissenschaften befindet, stehen Bestrebungen zur Erforschung visueller Erzählformen wie Theater, Comics, Film, Fernsehen, sowie Computerspiele, oft eine Randerscheinung in den DH dar und vor einer Reihe ungelöster Herausforderungen. Diese bestehen sowohl in technischer - etwa in Bezug auf die automatisierte Erkennung visueller Objekte und die Annotation komplexer Bild-Text- Kombinationen 'als auch in methodischer Hinsicht. Angesichts der Dominanz visueller Erzählformen seit dem frühen 20. Jahrhundert, sowie noch verstärkt in der Gegenwartskultur, stellt dies eine außerordentliche Forschungslücke dar. In einer kurzen Einleitung wird der Vortrag den derzeit im Aufbau befindlichen Corpus sowie die Zielsetzungen der vom deutschen Bundesministerium für Bildung und Forschung (BMBF) geförderten Nachwuchsgruppe ""Hybride Narrativität: Digitale und Kognitive Methoden zur Erforschung Graphischer Literatur"" erläutern. Darauf folgt die Vorstellung der für die Annotation entwickelten XML-Beschreibungssprache und des graphischen Editors. Im zweiten Teil des Vortrages stellen wir einige Methodenkombinationen vor, die es ermöglichen sollen, die Bild-¬≠Text-¬≠Verbindungen multimodaler Kulturformen, sowie deren Beitrag zur spezifischen Narrativität graphischer Romane, zu verstehen. Während die Analyse von Textcorpora oft bereits automatisiert möglich ist, bleibt eine automatische Analyse multimodaler Narrative derzeit eine Zukunftsvision. Im Fall von Comics und gezeichneter, sowie aus anderen Gründen nicht perspektivischer, Bilder gelingt die Objekt-Identifikation (etwa die Wiedererfassung eines vorab bekannten Charakters) nur mit viel Trainingsaufwand und recht hohen Fehlerzahlen. Auch bei der automatischen Erkennung Handschriften-ähnlicher Fonts versagen übliche Standard-OCRs. Daher führt der Weg über eine Annotation des Bild-¬≠Materials mit anschließender Analyse der Annotationen und Bild-Daten. Hierzu wird im Rahmen unseres Forschungsprojektes die XML-Sprache ""Graphic Narrative Markup Language"" (GNML) entwickelt, welche die visuellen und textuellen Aspekte Graphischer Literatur beschreibt. GNML baut auf der ""Text Encoding Initiative"" (TEI), und damit auf etablierten Standards, auf. Basierend auf den GNML-Annotationen können die in der graphischen Literatur enthaltenen Texte analysiert werden, Auswertungen der Bildinhalte vorgenommen, oder deren Kombination analysiert werden. Um die Fehleranfälligkeit bei der Annotation gering zu halten wird ein graphischer GNML-Editor entwickelt. Dieser unterstützt Fachwissenschaftler bei der effizienten Annotation mit Mechanismen wie Autovervollständigung von Charakter-Namen oder integrierten Rechtschreibprüfungen. Durch eine halb-automatische Erfassung wird die Annotation beschleunigt und so erst der Aufbau eines größeren Corpus ermöglicht. Teil des Editors ist eine Erkennung der Panel-Strukturen, sowie Werkzeuge, welche die Eckpunkte einer Sprechblase oder eines Textkästchens automatisch ermitteln. Ergänzt werden diese Werkzeuge um eine effiziente Charakter-Erfassung. Da sich die Konzepte des Editors (visuelle Objekte mit graphischen und textuellen Eigenschaften) nicht nur auf Comics beschränken sondern auch auf andere Bild-Text- Kombinationen anwendbar sind, lässt sich der Editor auf eine Obermenge solcher Formate erweitern. Diese Generalisierung erlaubt es, eine XML-basierte Annotationssprache zu hinterlegen und automatisch einen entsprechenden Editor zu generieren, sowie Daten in der hinterlegten Annotationssprache zu erfassen. Damit kann der Editor auch in der Annotation anderer multimodaler Medien Anwendung finden. Der zweite Teil des Vortrags stellt Ansätze vor, die exemplarisch für einige strukturelle Bestandteile von Comics (und insbesondere des graphischen Romans) Methoden der Bild- und Textanalyse miteinander verbinden. Für die digitale Literaturwissenschaft entwickelte Zugänge wie das Topic Modelling sind für solche Kulturformen aufgrund ihrer Bildlastigkeit nur von beschränkter Relevanz. Ansätzen zur computergestützten Bilderkennung und der Analyse großer Bildmengen fehlt hingegen bisher oft das narrative Erkenntnisinteresse. Erschwerend kommt noch hinzu, dass die Konzepte der Narratologie meist für literarische Texte entwickelt wurden und den Spezifika multimodalen Erzählens häufig nicht gerecht werden. In einer ersten Analyse des derzeit noch in Erstellung befindenden Gesamtkorpus  von rund 300 graphischen Romanen vergleichen wir die historische Entwicklung der  visuellen und Textebenen ihrer Buchcovers. Dazu gehören sowohl grammatische und  semantische Auswertungen der Romantitel mit Hilfe des Stanford Parser (vgl. De  Marneffe et al. 2006), als auch der farblichen und stilistischen Gestaltung. In  einem weiteren Schritt widmen wir uns detaillierteren Analysen eines ersten  Sub-Corpus, der aus den zehn meist zitierten Titeln des Gesamtcorpus besteht.  Zwar lassen sich aufgrund der geringen Zahl hier keine Genre-Vergleiche  anstellen, oder stichhaltig historische Entwicklungen nachverfolgen.  Beispielhaft können allerdings narrative Entwicklungen dargestellt werden: so  kombinieren wir Netzwerkanalysen der Figuren mit deren visueller Prominenz und  zugeordnetem Textanteil, sowie mit stilistischen Analysen dieser Figurentexte.  Weiters stellen wir, im Anschluss an Arbeiten von Lev Manovich (vgl. u. a.  Manovich 2012), explorative Visualisierungen aller Einzelseiten im Gesamtverlauf  der Erzählung vor. Abschließend wendet sich der Vortrag der Frage zu, ob die Text-Bild-Verbindungen  multimodaler Narrative mit solchen Methodenkombinationen aus der digitalen  Literatur- und Bildwissenschaft zu erfassen sind, oder sich durch die  Operationalisierung alternativer Ansätze aus der intermedialen Narratologie,  etwa Rick Altmans Konzept des ""Following"" (vgl. Altman 2008), eigenständige  Analysemethoden entwickeln lassen.",de,Vortrag präsentieren analyse visualisierung derzeit Aufbau befindlich Corpus graphisch Roman Graphic Novels Unterform Medium Comics stellen Annotation multimodal Erzählform entwickelt Editor Zeitgerecht Leipzig Download Verfügung stehen Analyse literarisch mehrere Fokus digital geisteswissenschaften befinden stehen Bestrebung Erforschung visuell Erzählforme Theater Comics Film fernsehen Computerspiele Randerscheinung dh dar Reihe ungelöst Herausforderung bestehen sowohl technisch Bezug automatisiert Erkennung visuell Objekt Annotation komplex kombination methodisch Hinsicht angesichts Dominanz visuell Erzählforme früh Jahrhundert verstärkt Gegenwartskultur stellen außerordentlich Forschungslücke dar kurz Einleitung Vortrag derzeit Aufbau befindlich Corpus Zielsetzung deutsch Bundesministerium Bildung Forschung Bmbf gefördert Nachwuchsgruppe hybrid Narrativität digital kognitiv Methode Erforschung graphisch Literatur erläutern folgen Vorstellung Annotation entwickelt graphisch Editor vortrag stellen Methodenkombination ermöglichen multimodaler kulturformen Beitrag spezifisch Narrativität graphisch Roman verstehen Analyse Textcorpora automatisieren bleiben automatisch Analyse Multimodaler Narrative derzeit Zukunftsvision Fall Comic Gezeichneter Grund perspektivisch Bild gelingen Wiedererfassung vorab bekannt Charakter Trainingsaufwand hoch fehlerzahlen automatisch Erkennung Font versagen üblich führen Weg Annotation anschließend Analyse annotatio hierzu Rahmen unser forschungsprojektes Graphic Narrative markup Language Gnml entwickeln visuell textuell Aspekt graphisch Literatur beschreiben Gnml bauen Text Encoding Initiative tei etabliert Standard basierend graphisch Literatur enthalten Text analysieren auswertungen bildinhalte vornehmen Kombination analysieren Fehleranfälligkeit Annotation gering halten graphisch entwickeln unterstützen fachwissenschaftler effizient Annotation mechanismen Autovervollständigung integriert Rechtschreibprüfung Erfassung Annotation beschleunigen Aufbau groß Corpus ermöglichen editors Erkennung Werkzeug Eckpunkt Sprechblase textkästchens automatisch ermitteln ergänzen Werkzeug effizient Konzept Editor visuell Objekt graphisch Textuell eigenschaften Comic Beschränk Kombination anwendbar lässen Editor Obermenge Formate erweitern Generalisierung erlauben Annotationssprach hinterlegen automatisch entsprechend Editor generieren daten hinterlegt Annotationssprach erfassen Editor Annotation anderer multimodaler Medium Anwendung finden Vortrag stellen Ansatz exemplarisch strukturell Bestandteil Comics insbesondere graphisch Roman Methode Textanalyse miteinander verbinden digital Literaturwissenschaft entwickelt zugänge Topic Modelling Kulturforme aufgrund Bildlastigkeit Beschränkter Relevanz Ansatz computergestützt Bilderkennung Analyse Bildmeng fehlen hingegen Narrative Erkenntnisinteresse erschwerend hinzu Konzept Narratologie meist literarisch Text entwickeln Spezifika multimodal Erzählen häufig gerecht Analyse derzeit Erstellung befindenden Gesamtkorpus graphisch Roman vergleichen historisch Entwicklung visuell textebenen Buchcover gehören sowohl grammatisch semantisch Auswertung Romantitel Hilfe Stanford Parser de marneff Et farblich stilistisch Gestaltung Schritt widmen detaillierteren analyse meist zitiert Titel Gesamtcorpus bestehen lassen aufgrund gering Zahl anstellen stichhaltig historisch Entwicklung nachverfolgen beispielhaft narrativ Entwicklung darstellen kombinieren netzwerkanalysen Figur visuell Prominenz zugeordnet Textanteil stilistisch Analyse Figurentexte Weiter stellen Anschluss arbeiten lev Manovich manovich explorative visualisierung einzelseiten Gesamtverlauf Erzählung abschließend wenden Vortrag Frage Multimodaler Narrative Methodenkombination digital Bildwissenschaft erfassen Operationalisierung alternativ Ansatz intermedial Narratologie rick altmans Konzept following altman eigenständig analysemethoden entwickeln lassen,"[('graphisch', 0.351693322711784), ('editor', 0.26086854564457723), ('visuell', 0.21113399392695612), ('multimodaler', 0.19197089924532404), ('vortrag', 0.1572231945228762), ('annotation', 0.1457175673847345), ('comics', 0.14397817443399302), ('analyse', 0.1379017978112523), ('narrative', 0.13333658199795123), ('annotationssprach', 0.11358836549820688)]"
2016,DHd2016,panels-006.xml,Nachhaltigkeit technischer Lösungen für digitale Editionen. Eine kritische Evaluation bestehender Frameworks und Workflows von und für Praktiker_innen,"Peter Andorfer (ACDH, Österreich); Matej Durco (ACDH, Österreich); Thomas Stäcker (HAB, Deutschland); Christian Thomas (BBAW, Deutschland); Vera Hildenbrandt (TCDH, Deutschland); Hubert Stigler (ZIM Uni Graz, Österreich); Sibylle Söring (SUB Göttingen, Deutschland); Lukas Rosenthaler (DH Lab, Schweiz)","Digitale Editionen, Infrastruktur",Veröffentlichung,"Digitale Editionen machen in den Digital Humanities das ""Brot- und  Buttergeschäft"" aus. Doch während sich der methodisch-theoretische Hintergrund  digitaler Editionen zusehends konsolidiert und sich diese neue Form der  Publikation von Forschungsergebnissen im (fach)wissenschaftlichen Diskurs  bereits etabliert hat, fehlt es nach wie vor an umfassend dokumentierten und  selbstkritisch reflektierten Best-Practice-Beispielen von Frameworks und  Workflows zur Erstellung und / oder Publikation von digitalen Editionen, welche  als Blaupausen für künftige digitale Editionsprojekte herangezogen werden  können. Das Resultat ist so bekannt wie unerfreulich und kann 'nur geringfügig  überspitzt 'auf folgende Formel gebracht werden: So gut wie jedes Projekt  erfindet das Rad 'das technische Grundgerüst der Edition 'wieder neu. Die wichtigsten Gründe für diese Entwicklung lassen sich rasch benennen: Im Rahmen des Panels sollen einige der aktivsten Institutionen aus dem Bereich der digitalen Editionen an einen Tisch gebracht werden. Diese erhalten im Vorfeld der Tagung einen Fragebogen zur Vorbereitung einer kurzen (pro Teilnehmer ca. fünf Minuten) Vorstellung ihrer Systeme, wobei darin der Fokus auf dem Thema Reusability der in den Projekten verwendeten Technologien und Workflows liegen sollte. Konkret sollen die Teilnehmer_innen des Panels auf folgende Punkte eingehen: Ein Ziel dieser Vorstellungsrunde soll es sein, potenziell interessierten Nutzer_innen im Auditorium einen kompakten Überblick über bestehende Angebote zur Erstellung und / oder Veröffentlichung von digitalen Editionen zu vermitteln. Im Anschluss an diese Kurzvorstellung erfolgt eine moderierte Podiumsdiskussion, worin folgende Punkte weiter thematisiert werden: Es sollen gemeinsame Problemfelder identifiziert und reflektiert werden. Auf dieser Basis kann dann über mögliche (gemeinsame) Lösungen diskutiert werden. Im letzten Drittel des Panels wird die Diskussion zum Publikum hin geöffnet werden. Dabei sollen vor allem potentielle Nutzer_innen die Möglichkeit bekommen, gezielt konkrete und ggf. eigene Projekte betreffend Fragen zu stellen und direkt mit möglicherweise zukünftigen Projektpartnern ins Gespräch zu kommen. Bei der Auswahl der Teilnehmer wurde einerseits darauf geachtet, vornehmlich etablierte Institutionen anzusprechen, die sich als Dienstleister im Bereich digitaler Editionen profiliert haben, deshalb an möglichst generischen Lösungen zur Erstellung und Publikationen von digitalen Editionen interessiert sind und dafür selbst Frameworks und Workflows entwickelt haben. Außerdem wurde versucht, bei der Auswahl der Teilnehmer möglichst den gesamten deutschsprachigen Raum abzudecken. Matej Durco und Peter Andorfer Das Das ACDH übernimmt auch die Organisation und Moderation des Panels. Thomas Stäcker Die Herzog August Bibliothek (HAB) hat für ihre digitalen Editionsprojekte  die Sibylle Söring Christian Thomas Das DFG-geförderte Projekt Thomas Burch, Vera Hildenbrandt Das Hubert Stigler Das ZIM hat im Rahmen einer Vielzahl von Editionsprojekten   forschungsgetrieben ein Lukas Rosenthaler Während die anderen Teilnehmer vor allem an Lösungen für XML / TEI-basierte  digitale Editionen arbeiten, legt",de,digital editionen Digital Humanitie Buttergeschäft Hintergrund Digitaler editionen zusehends konsolidieren Form Publikation Forschungsergebnisse Fach wissenschaftlich Diskurs etablieren fehlen umfassend dokumentiert selbstkritisch reflektiert Framework workflows Erstellung Publikation digital Edition Blaupause künftig digital editionsprojekte heranziehen Resultat unerfreulich geringfügig überspitzen folgend Formel bringen jeder Projekt erfinden Rad technisch grundgerüst Edition neu wichtig gründe Entwicklung lassen rasch benennen Rahmen Panel aktivst Institution Bereich digital editionen Tisch bringen erhalten Vorfeld Tagung Fragebogen Vorbereitung kurz pro Teilnehmer Minute Vorstellung System wobei Fokus Thema Reusability Projekt verwendet Technologie workflows liegen konkret Panel folgend Punkt eingehen Ziel vorstellungsrund potenziell interessiert Auditorium kompakt Überblick bestehend Angebot Erstellung Veröffentlichung digital editionen vermitteln Anschluss Kurzvorstellung erfolgen moderiert Podiumsdiskussion worin folgend Punkt thematisieren gemeinsam Problemfelder identifizieren reflektieren Basis möglich gemeinsam Lösung diskutieren letzter Drittel Panel Diskussion Publikum öffnen potentiell Möglichkeit bekommen gezielt konkret Projekt betreffend Frage stellen direkt möglicherweise zukünftig Projektpartner Gespräch Auswahl Teilnehmer einerseits achten vornehmlich etabliert Institution anzusprechen Dienstleister Bereich Digitaler editionen profilieren möglichst generisch Lösung Erstellung Publikation digital editionen interessiert framework workflows entwickeln versuchen Auswahl Teilnehmer möglichst gesamt deutschsprachig Raum abdecken matej Durco Peter Andorfer Acdh übernehmen Organisation Moderation Panel Thomas Stäcker Herzog August Bibliothek Hab digital Editionsprojekt sibyll Söring Christian Thomas Projekt Thomas Burch vera hildenbrandt Hubert Stigler Zim Rahmen Vielzahl editionsprojekten Forschungsgetriebe Lukas Rosenthaler Teilnehmer Lösung xml digital editionen arbeiten legen,"[('editionen', 0.3781913908162424), ('teilnehmer', 0.22014678929568451), ('panel', 0.19947208823402035), ('workflows', 0.19670279350200306), ('digital', 0.1823764749682297), ('publikation', 0.1566259038348038), ('thomas', 0.15414876903056288), ('lösung', 0.1455146349990045), ('erstellung', 0.12704642773474853), ('interessiert', 0.12009026924365007)]"
2016,DHd2016,vortraege-031.xml,"ePoetics 'Korpuserschließung und Visualisierung deutschsprachiger Poetiken (1770-1960) für den ,Algorithmic Criticism""","Stefan Alscher (Universität Stuttgart, Deutschland); Michael Bender (Technische Universität Darmstadt); Markus John (Universität Stuttgart, Deutschland); Andreas Müller (Universität Stuttgart, Deutschland); Sandra Richter (Universität Stuttgart, Deutschland); Andrea Rapp (Technische Universität Darmstadt); Thomas Ertl (Universität Stuttgart, Deutschland); Steffen Koch (Universität Stuttgart, Deutschland); Jonas Kuhn (Universität Stuttgart, Deutschland)","Projektbeschreibung, manuelle Annotation, algorithmische Verfahren, interaktive Visualisierung, Algorithmic Criticism","Inhaltsanalyse, Modellierung, Visualisierung, Literatur, Methoden, Projekte, Forschungsprozess, Text, Werkzeuge, Visualisierung","ePoetics ist ein Forschungskooperationsprojekt der Universität Stuttgart und der   Technischen Universität Darmstadt. Gefördert vom Bundesministerium für Bildung und   Forschung zielt es gleichermaßen auf einen Erkenntnisgewinn für die Informatik sowie   die Sprach- und Literaturwissenschaft dank einer wechselseitigen Anregung und   Ergänzung im Sinne des ""Algorithmic Criticism"" nach Stephen Ramsay (Ramsay 2007).   Dieser Ansatz ist explizit nicht darauf ausgerichtet, lediglich hermeneutische   Hypothesen mit algorithmischen Verfahren zu überprüfen. Vielmehr zielt er darauf,   durch den iterativen Einsatz analoger und digitaler Methoden verschiedene   Perspektiven auf Texte einnehmen und abgleichen zu können. Darüber hinaus ist ein   zentraler Aspekt dieses Forschungsparadigmas, Erschließungsentscheidungen und   -verfahren sowie Analyseschritte transparent bzw. nachvollziehbar und nachnutzbar zu   machen. Das Projekt ePoetics ist der Digitalisierung, Annotation, Analyse und   Visualisierung eines für die Geisteswissenschaften zentralen Textkorpus"" gewidmet:   Poetiken und Östhetiken von 1770 bis 1960. Diese Texte dokumentieren das Denken und   Schreiben über Literatur und andere Künste in der zentralen Periode nach der Abkehr   von der Normen- und Regelpoetik (vor 1770) und vor dem Übergang zur Literaturtheorie   und damit dem Ende der Poetik als literaturwissenschaftlicher Textgattung (nach   1960). Sie enthalten dabei grundlegendes Wissen über Sprache und Literatur   (-wissenschaft), etwa die Erläuterungen zentraler Begriffe und deren Zusammenhänge.   ePoetics betreibt die Entwicklung und Untersuchung eines Testkorpus"" von zwanzig   Poetiken, ausgewählt aus einem Gesamtkorpus von 1240 Texten (inkl. aller Auflagen),   die Sandra Richter in ihrer Studie ""A history of Poetics"" (Richter 2010) als zur   Gattung ""Deutschsprachiger Poetik"" zählbarer Werke bibliographiert hat. Die Auswahl   des Testkorpus"" enthält 'historisch und systematisch betrachtet 'die   repräsentativsten Texte des Gesamtkorpus"", d. h. die, die am häufigsten zitiert und   in den meisten Auflagen herausgegeben wurden, und stellt dennoch auf den ersten   Blick ein sehr heterogenes Korpus dar. Aus sprach- und literaturwissenschaftlicher   Sich zeigen wir auf, wie sich diese Heterogenität im Einzelnen darstellt, aber auch,   welche tiefergehenden Gemeinsamkeiten und Abhängigkeiten die Texte auf den zweiten   Blick aufweisen und auf welche Ursprünge sich diese zurückführen lassen. Für   ausführlichere Informationen zum ausgewählten Textkorpus und zum Projekt insgesamt   besuchen Sie unsere Homepage (vgl. Im Zentrum unseres Interesses steht aktuell beispielsweise der Begriff der Metapher als ein zentrales sprach- und literaturwissenschaftliches Konzept, das in unserem Textkorpus verhandelt wird. Die mit diesem zusammenhängenden Fragen lauten: Wie wird der Begriff in einzelnen Poetiken verstanden und erklärt? Wie ändert sich dieses Verständnis innerhalb unseres Testkorpus""? Welche literarischen oder theoretischen Werke werden im Zusammenhang damit genannt oder zitiert? Wie verändert sich der ""Kanon"" dieser Werke? Verändern sich die Zusammenhänge, in denen die Werke zitiert werden? Und schließlich: Wie verändert sich insgesamt der Umgang mit Zitaten und deren Nachweisen? Problemstellungen für die digitale Annotation mit dem Ziel der computergestützten  Auswertbarkeit liegen bei solchen Texten und Anforderungen auf mehreren Ebenen vor:  Das jeweilige Metaphernverständnis muss differenziert erschlossen und die  Komponenten der Begriffsbestimmung müssen trennscharf kategorisiert werden können.  Beispiele aus der Primärliteratur müssen eindeutig erkannt und den jeweiligen  theoretischen Aspekten, für die sie stehen, zugeordnet werden. Und schließlich  müssen die Textebenen und Referenzstrukturen der Poetik explizit gemacht werden ' also wo der Autor selbst theoretisiert, wo zitiert oder paraphrasiert wird,  inwiefern dies kenntlich gemacht wird oder nicht und sogar, wo bei Zitaten vom  ursprünglichen Text abgewichen wird. Dies wird durch die Annotation nach einem  komplexen Schema umgesetzt. Die Annotationen werden einerseits in TEI-konformen  XML-Dateien publiziert, andererseits aber auch als Grundlage von computergestützten  Analysen und Visualisierungen genutzt. Abbildung 1 veranschaulicht das Vorgehen im  Projekt ePoetics im Sinne eines ""Algorithmic Criticism"" nach Stephen Ramsay  (2007). Die Texte des Testkorpus"" stehen als Image-Digitalisate und als nach dem ""Double Keying""-Verfahren transkribierte und aufbereitete digitale Volltexte zur Verfügung. Die strukturellen (und auch die semantischen) Annotationen des Korpus"" erfolgen nach den Konventionen der Text Encoding Initiative (TEI). Das Korpus wird in virtuelle Forschungs-Infrastrukturen wie TextGrid und das Deutsche Textarchiv (DTA) integriert und dort mit den vorhandenen Referenztexten verlinkt. Nach der Identifikation relevanter und interessanter Begriffe und Konzepte wurden zu einzelnen ausgesuchten Begriffen wie der Metapher mithilfe des UAM CorpusTool Annotationsschemata für manuelle Annotationen erstellt. Diese wurden unter ausführlicher Dokumentation von Annotationsguidelines durch mehrere Annotatoren getestet, kontinuierlich verbessert, ausgebaut und schließlich in den Poetiken durchgeführt. Abbildung 2 zeigt eine vereinfache Version des daraus hervorgegangenen Annotationsschemas, das sich in zwei Teilbereiche gliedern lässt, die teils direkt und teils mit leichten Veränderungen auch auf andere Begriffe übertragen werden können. Das Schema resultiert aus den oben genannten sprach- und literaturwissenschaftlichen Fragen, die sich in die Aspekte der Repräsentation und des Verständnisses bzw. der Anwendung des Metaphernbegriffs in den Poetiken aufteilen lassen. Das Annotationsschema stellt eine Systematisierung des Begriffs, d. h. seines Vorkommens und Verständnisses in den Poetiken dar. Die für den Begriff relevanten Textstellen werden zunächst dahingehend klassifiziert, ob es sich um Poetikentext handelt (also Text vom Autor der Poetik selbst), oder ob andere theoretische oder literarische Texte zitiert, paraphrasiert oder genannt werden. Neben den Verweisungsformen annotieren wir hierbei auch die Quellenangaben 'beides im Übrigen nicht nur, wenn es explizit angegeben ist. So berücksichtigen wir auch die Möglichkeit von ""versteckten"" Zitaten oder solchen, bei denen die Quelle nicht oder unvollständig benannt ist. Das Auffinden bestimmter Muster sowie zum Beispiel Titel und Personennamen oder Zitate wird dabei unterstützt durch computerlinguistische Methoden und Verfahren der interaktiven Visualisierung. Darüber hinaus systematisieren wir das vorliegende Begriffsverständnis, d. h. ob die Metapher z. B. als Übertragung erklärt wird, und grenzen sie von anderen Begriffen ab, z. B. im Unterschied zum Vergleich. Zusätzlich lassen sich auch Beobachtungen zu konkreten hermeneutischen Hypothesen annotieren, z. B. ob anhand des Metapherngebrauchs zwischen poetischer und prosaischer Sprache unterschieden wird. Schon durch die Annotation von implizitem Wissen entsteht somit bereits bei den manuellen Annotationen eine Metaebene an Informationen, mit der der digitalisierte Poetikentext angereichert wird. Die Systematisierung erfordert eine andere Herangehensweise an den Gegenstand, als es bei einer rein hermeneutischen Analyse der Fall wäre. Ebenso führt diese zwangsläufig zur Problematisierung der Systematisierungs(un)möglichkeit eines per se komplexen, weil heterogenen Untersuchungsgegenstandes. Das Ziel der algorithmischen Weiterverarbeitung wird zum Paradigma für die systematisch-kategorisierende Ausdifferenzierung von theoretischen Begriffen, wobei diesbzgl. neue Erkenntnisse, aber auch Grenzen aufgezeigt werden können. Die Operationalisierung der Daten führt so bereits zu Erkenntnissen, bevor computertechnologische Auswertungen durchgeführt werden, womit sie sich über den Status bloßer Vorverarbeitung erheben und einen Eigenwert besitzen. Mit algorithmischen Verfahren können aus kleinen Mengen annotierter Daten (aus der manuellen und damit zeitaufwendigen Annotation) große Mengen gemacht werden, indem die annotierten Arten von Informationen automatisch auf größere Datenmengen übertragen werden. Im Folgenden wird anhand eines Beispiels in Anlehnung an den rechten Teil von  Abbildung 1 beschrieben, wie die manuelle Annotation, das Training von  Klassifikationsmodellen und die Analyse der Klassifikationsergebnisse ineinander  greifen. Zur Klassifizierung von Text zwischen Anführungszeichen als eine der drei  Klassen ""Hervorhebung"" (Wörter deren Bedeutung hervorgehoben wird), ""Titel""  (Werktitel) und ""Zitat"" (Zitate aus anderen Werken) wurde manuell ein Korpus  annotiert, in dem jeder Text zwischen Anführungszeichen einer dieser drei Klassen  zugewiesen wurde (Manuelles Annotieren von Konzepten). Auf der Basis dieses Korpus  wurden Klassifikationsmodelle zur automatischen Erkennung dieser drei Klassen  trainiert (Modell trainieren / entwickeln). Die automatischen Modelle wiederum  wurden benutzt, um in anderen Poetiken Text in Anführungszeichen automatisch in  diese drei Klassen einzuteilen. Es wurde durch Stichproben und formale Evaluation  auf einem für diesen Zweck annotierten separaten Korpus erkannt, dass die  Klassifikation gut funktioniert (Evaluierung). Da so unter anderem direkte Zitate  und Werktitel automatisch erkannt werden, ermöglicht dieser Schritt wiederum die  automatische Verlinkung von Werktiteln und Zitaten mit ihren Einträgen (sofern  vorhanden) im TextGridRepository-Korpus (Evaluierung). Durch diese Information kann  vom Analysten manuell die Verteilung von Werken und Zitaten in den Poetiken  untersucht und bedeutende Werke / Zitate erkannt werden. Diese Erkenntnisse können  dann wiederum als Metadaten im Dokument annotiert werden (Manuelles Annotieren von  Konzepten und Metadaten). Interaktive Visualisierung spielt eine wesentliche Rolle in der Vorgehensweise von  ePoetics, siehe Abbildung 1, da sie eine zusätzliche Interaktion zwischen Forschern  und den Untersuchungsgegenständen ermöglicht. Zum einen können interaktive Systeme  die hermeneutischen Vorgehensweise unterstützen, indem sie den  Geisteswissenschaftlern die Möglichkeiten bieten, Annotationsschemata und  -guidelines zu entwerfen, Konzepte und Metadaten in Texten manuell zu annotieren  sowie diese Ergebnisse zu analysieren und darzustellen. Zum anderen kann die  computerlinguistische Vorgehensweise unterstützt werden, so dass Forscher Einfluss  auf komplexe Prozesse nehmen können wie beispielsweise dem Trainieren maschineller  Lernmethoden durch visuelle Veränderungsparameter. Durch diese Art der Interaktion  kann unterstützt werden, dass Modelle mit Hilfe des Experten entwickelt, angepasst,  trainiert sowie die Ergebnisse evaluiert werden können. Um diese Herausforderungen  umzusetzen, wurden zwei interaktive visuelle Analysewerkzeuge konzipiert und  entwickelt. Der VarifocalReader (Ertl et al. 2014), der auf einem hierarchischen  Navigationskonzept basiert (Wörner / Ertl 2013), ermöglicht den Anwendern einen  direkten Zugang zu Details und Dokumentquellen, während sie auf unterschiedlichen  Abstraktionsebenen mit Zusammenfassungen vorhandener Annotationen interagieren  können. Des Weiteren bietet das System die Möglichkeit, computerlinguistische  Modelle anzupassen bzw. zu trainieren sowie Metadaten zu analysieren, zu annotieren  und zu korrigieren. Eine beispielhafte Analyse ist in Abbildung 3 dargestellt, in  der der Forscher einen schnellen Überblick und Zugang zur ausgewählten Annotation  ""Wallenstein"" (in der 3. Word Cloud sichtbar) erhält. Der zweite Ansatz (Heimerl et al. 2014) wurde konzipiert, um eine textvergleichende Analyse zu ermöglichen (siehe Abbildung 4). Die Visualisierung bietet einen Vergleich von mehreren Dokumenten auf einer abstrakten Ebene in Bezug auf die Verteilung der Annotationen, während die Textfelder eine flexible Navigation durch die einzelnen Texte ermöglichen. Zusätzlich unterstützt dieser focus+context Ansatz einen reibungslosen Übergang zwischen close und distant reading. Ergebnis des Projekts ePoetics ist ein digitalisiertes und annotiertes Korpus poetologischer Texte (TEI-konform und nachnutzbar), in denen zentrale Konzepte der Sprach- und Literaturtheorie durch XML-Auszeichnung explizit gemacht und systematisiert werden. Durch Korpus-übergreifende Analysen dieser Auszeichnungen können Gemeinsamkeiten und Unterschiede sowie diachrone Entwicklungen gezeigt werden. Darüber hinaus werden die Referenz- und Diskursstrukturen erschlossen (auch implizite, ""versteckte"" Verweisungen), die auf verschiedenen Ebenen der Texte bestehen 'einerseits Verweisungen auf andere Poetiken sowie die Identifikation bestimmter Denkschulen bzw. Theorielinien, die bis auf Ansätze aus der Antike zurückgehen (z. B. Aristoteles, Quintilian), andererseits die Diskussion von literarischen Beispielen, die Rückschlüsse auf die Entwicklungen des Literaturkanons erlauben. Die manuellen Annotationen werden iterativ gestützt durch automatisierte Methoden und Verfahren der interaktiven Visualisierung. Die dabei entwickelten computerlinguistischen Anwendungen und Visualisierungssysteme (siehe Abbildungen 3 und 4) stellen ebenfalls Ergebnisse des Projekts dar.",de,epoetics Forschungskooperationsprojekt Universität Stuttgart technisch Universität Darmstadt fördern Bundesministerium Bildung Forschung zielen gleichermaßen Erkenntnisgewinn Informatik Literaturwissenschaft wechselseitig Anregung Ergänzung Sinn Algorithmic Criticism steph Ramsay Ramsay Ansatz explizit ausrichten lediglich hermeneutisch Hypothesen algorithmisch Verfahren überprüfen vielmehr zielen iterativ Einsatz Analoger Digitaler Methode verschieden Perspektive Text einnehmen abgleichen hinaus zentral Aspekt Forschungsparadigmas erschließungsentscheidungen Analyseschritte Transparent nachvollziehbar nachnutzbar Projekt epoetics Digitalisierung Annotation Analyse Visualisierung geisteswissenschaft zentral Textkorpus widmen poetiken östhetiken Text dokumentieren denken schreiben Literatur künste zentral Periode Abkehr Regelpoetik Übergang Literaturtheorie Poetik literaturwissenschaftlich Textgattung enthalten grundlegend wissen Sprache Literatur erläuterung zentral begriffe zusammenhängen epoetics betreiben Entwicklung Untersuchung Testkorpus poetiken ausgewählen Gesamtkorpus text auflage Sandra Richter Studie history -- poetics Richter Gattung deutschsprachig Poetik zählbar Werk bibliographiern Auswahl Testkorpus enthalten historisch systematisch betrachten repräsentativst Text Gesamtkorpus häufig zitieren meister auflage herausgeben stellen dennoch Blick heterogen Korpus dar literaturwissenschaftlich zeigen Heterogenität einzelner darstellen tiefergehend Gemeinsamkeit Abhängigkeit Text Blick aufweisen ursprüngen zurückführen lassen ausführlich Information ausgewählt Textkorpus Projekt insgesamt besuchen Homepag Zentrum unser Interesse stehen aktuell beispielsweise Begriff Metapher zentral literaturwissenschaftlich Konzept unser Textkorpus verhandeln zusammenhängend Frage lauten Begriff einzeln Poetik verstehen erklären ändern Verständnis innerhalb unser Testkorpus literarisch theoretisch Werk Zusammenhang nennen zitieren verändern Kanon Werk verändern zusammenhang Werk zitieren schließlich verändern insgesamt Umgang zitat nachweisen Problemstellung digital Annotation Ziel computergestützt Auswertbarkeit liegen Text Anforderung mehrere ebenen jeweilig Metaphernverständnis differenziert erschließen Komponent Begriffsbestimmung trennscharf kategorisieren Beispiel Primärliteratur eindeutig erkennen jeweilig theoretisch Aspekt stehen zuordnen schließlich textebenen Referenzstruktur Poetik explizit Autor theoretisieren zitieren paraphrasieren inwiefern kenntlich sogar zitat ursprünglich Text abgewichen Annotation komplex Schema umsetzen Annotation einerseits publizieren andererseits Grundlage computergestützten analyse visualisierung nutzen Abbildung veranschaulichen vorgehen Projekt epoetics Sinn Algorithmic Criticism steph Ramsay Text Testkorpus stehen double transkribieren aufbereitet digital Volltexte Verfügung strukturell semantisch annotation Korpus erfolgen Konvention Text encoding Initiative tei korpus virtuell Textgrid deutsch Textarchiv dta integrieren vorhanden Referenztext verlinkt Identifikation relevant interessant Begriff Konzept einzeln ausgesuchter begreifen Metapher Mithilfe uam corpustool Annotationsschemata Manuelle Annotation erstellen ausführlich dokumentation annotationsguidelin mehrere Annotator testen kontinuierlich verbessern ausbauen schließlich Poetik durchführen Abbildung zeigen vereinfach Version hervorgegangen Annotationsschemas teilbereich Glieder lässen teils direkt teils leicht Veränderung Begriff übertragen Schema resultieren genannt literaturwissenschaftlich Frage Aspekt Repräsentation Verständniss Anwendung Metaphernbegriff poetiken aufteilen lassen Annotationsschema stellen Systematisierung Begriff vorkommens Verständnisses Poetik dar Begriff relevant Textstelle dahingehend klassifizieren Poetikentext handeln Text Autor Poetik theoretisch literarisch Text zitieren paraphrasieren nennen verweisungsform annotieren hierbei quellenangaben beider übrig explizit angeben berücksichtigen Möglichkeit versteckt zitat Quelle unvollständig benennen auffind bestimmt Muster Titel Personenname Zitat unterstützen computerlinguistisch Methode Verfahren interaktiv Visualisierung hinaus systematisieren vorliegend Begriffsverständnis Metapher Übertragung erklären grenzen begreifen Unterschied Vergleich zusätzlich lassen Beobachtung konkret hermeneutisch Hypothese annotieren anhand Metapherngebrauch poetisch prosaisch Sprache unterscheiden Annotation implizit Wissen entstehen somit manuell Annotation Metaebene Information digitalisierter Poetikentext angereichert Systematisierung erfordern Herangehensweise Gegenstand rein hermeneutisch Analyse Fall führen zwangsläufig Problematisierung Systematisierung un Möglichkeit per se komplexen heterogen untersuchungsgegenstandes Ziel algorithmisch Weiterverarbeitung Paradigma Ausdifferenzierung Theoretisch begreifen wobei Diesbzgl erkenntnisse Grenze aufzeigen Operationalisierung daten führen erkenntnissen bevor computertechnologisch Auswertung durchführen womit Status bloß Vorverarbeitung erheben eigenwert besitzen algorithmisch Verfahren menge annotiert daten Manuelle zeitaufwendig Annotation menge annotiert Art Information automatisch groß datenmenger übertragen folgend anhand Beispiel Anlehnung Abbildung beschreiben Manuelle Annotation Training klassifikationsmodell Analyse klassifikationsergebnisse ineinander greifen Klassifizierung Text anführungszeich klass hervorhebung Wörter Bedeutung hervorheben Titel Werktitel Zitat Zitat Werk manuell Korpus annotiert Text anführungszeich Klasse zuweisen manuelles annotieren konzepten Basis korpus Klassifikationsmodell automatisch Erkennung Klasse trainieren Modell trainieren entwickeln automatisch Modell wiederum benutzen poetik Text anführungszeich automatisch Klasse einzuteilen Stichprobe formal evaluation zweck Annotiert Separat Korpus erkennen Klassifikation funktionieren Evaluierung direkt Zitat Werktitel automatisch erkennen ermöglichen Schritt wiederum automatisch Verlinkung werktiteln zitaen Einträg sofern vorhanden Evaluierung Information analyst Manuell Verteilung Werk zitaen Poetik untersuchen bedeutend werk Zitat erkennen Erkenntnis wiederum Metadat Dokument annotiert manuelles annotieren konzept Metadat interaktiv Visualisierung spielen wesentlich Rolle vorgehensweise epoetics sehen Abbildung zusätzlich Interaktion forschern untersuchungsgegenständ ermöglichen interaktiv System hermeneutisch vorgehensweise unterstützen geisteswissenschaftlern Möglichkeit bieten Annotationsschemata entwerfen Konzept metadaten Text Manuell annotieren Ergebnis analysieren darstellen computerlinguistisch vorgehensweise unterstützen Forscher einfluss komplex prozesse nehmen beispielsweise trainier Maschineller Lernmethod visuell Veränderungsparameter Art Interaktion unterstützen Modell Hilfe Experte entwickeln angepasst trainieren Ergebnis evaluieren Herausforderung umsetzen interaktiv visuell Analysewerkzeuge konzipieren entwickeln varifocalreader Ertl et hierarchisch Navigationskonzept basieren Wörner Ertl ermöglichen Anwender direkt Zugang Detail dokumentquellen unterschiedlich Abstraktionseben zusammenfassunger vorhanden annotation Interagier bieten System Möglichkeit Computerlinguistische modell anpassen trainieren metadaten analysieren annotier korrigieren beispielhaft Analyse Abbildung darstellen Forscher schnell Überblick Zugang ausgewählt Annotation Wallenstein Word Cloud sichtbar erhalten Ansatz heimerl et konzipieren textvergleichend Analyse ermöglichen sehen Abbildung Visualisierung bieten Vergleich mehrere dokumenten abstrakt Ebene Bezug Verteilung annotatio textfeld flexibel Navigation einzeln Text ermöglichen zusätzlich unterstützen Ansatz Reibungslose Übergang Close distant reading Ergebnis Projekt epoetics digitalisiert annotiert Korpus poetologisch Text nachnutzbar zentral Konzept Literaturtheorie Explizit systematisieren Analyse Auszeichnung Gemeinsamkeit Unterschied diachron Entwicklung zeigen hinaus Diskursstruktur erschließen implizit versteckt verweisung verschieden Ebene Text bestehen einerseits Verweisung poetiken Identifikation bestimmt denkschul Theorielinie Ansatz Antike zurückgehen aristotele Quintilian andererseits Diskussion literarisch Beispiel rückschluß Entwicklung Literaturkanon erlauben manuell Annotation iterativ stützen automatisiert Methode Verfahren interaktiv Visualisierung entwickelt computerlinguistisch Anwendung visualisierungssystem sehen abbildungen stellen ebenfalls Ergebnis Projekt dar,"[('poetik', 0.26297113935637545), ('zitat', 0.2337521238723337), ('epoetics', 0.2224454443060338), ('zitieren', 0.1566440211150303), ('annotation', 0.15457301877300833), ('poetiken', 0.14829696287068922), ('text', 0.13327777304841953), ('begriff', 0.12315903378821759), ('testkorpus', 0.11352692346000075), ('ramsay', 0.1112227221530169)]"
2016,DHd2016,posters-019.xml,Dissertation: Der Berner Chorherr Heinrich Wölfli (1470-1532) und die Beschreibung seiner Heiligland-Wallfahrt von 1520/21 - Erschliessung und Darstellung durch klassisch-literaturwissenschaftliche und digital-moderne Methoden,"Stephanie Habicht (Universität Zürich, Schweiz)","Reisebeschreibung, Visualisierung, lebendige Geschichte, Mapping","Umwandlung, Entdeckung, Bilderfassung, Transkription, Gestaltung, Programmierung, Inhaltsanalyse, Strukturanalyse, Übersetzung, Beziehungsanalyse, Räumliche Analyse, Modellierung, Annotieren, Kontextsetzung, Bereinigung, Bearbeitung, Netzwerkanalyse, Bewertung, Schreiben, Veröffentlichung, Stilistische Analyse, Identifizierung, Einführung, Kommentierung, Webentwicklung, Organisation, Konservierung, Visualisierung, Artefakte, Bibliographie, Computer, Curricula, Daten, Datei, Bilder, Interaktion, Link, Literatur, Manuskript, Karte, Metadaten, Methoden, Multimedia, benannte Entitäten (named entities), Personen, Projekte, Forschung, Forschungsprozess, Software, Text, Werkzeuge, Visualisierung","Gegenstand meiner Dissertation ist die Reise nach Jerusalem von 1520/21 von Heinrich Wölfli, erhalten in der deutschen Übersetzung von Johannes Haller. Einerseits soll - ganz klassisch im literaturwissenschaftlichen Sinne - eine narratologische Analyse der Beschreibung sowie ein Handschriftenvergleich, inkl. Stemma und Jahresschätzung der 6 bekannten Handschriften, anhand von inhaltlichen Abweichungen und Wasserzeichen vorgenommen werden. Zudem wird ein kodikologischer Befund, die Biographien des Autors sowie des Übersetzers, ein Stellenkommentar und ein kleines Lexikon der für diesen Text speziellen und ansonsten eher unbekannten Wörter erstellt. Andererseits wird mithilfe von Zusätzlich - und das ist der für das Poster relevanteste Punkt - ist angedacht, die   teilweise sehr detailliert beschriebenen und auf das Datum und die kleinste   Ortschaft genau festgehaltenen Reisestationen Wölflis in XML zu erfassen, und die   Reise im Das vorgestellte Teilprojekt dient sicher der Visualisierung geisteswissenschaftlicher Daten und vielleicht auch der Vernetzung in einem ganz speziellen Sinne. Zudem kann es, weiter angereichert, auch für andere Fächer nutzbar gemacht werden, so zum Beispiel für die Fächer Geschichte, Geographie und Religionswissenschaften. Dieses Teilprojekt der Dissertation ist derzeit in Arbeit. Wieviel davon im März 2016 zur DHd-Tagung bereits abgeschlossen ist, ist derzeit nicht abschätzbar. Sicher ist aber, dass bis dahin ein Teil der Daten schon aufbereitet ist und auf dem Poster vorgestellt werden kann, sowie, dass die generelle technische Umsetzung und die theoretische Überlegung dargestellt und erörtert werden kann. Dasselbe gilt für den Handschriftenvergleich, welcher sowohl mit klassisch-literaturwissenschaftlichen als auch mit digital-modernen Methoden angegangen werden soll, wobei der Fokus für den Beitrag doch primär auf der Visualisierung der Reiseroute liegt. Für die Zukunft muss (urheberrechtspolitisch) noch eruiert werden, inwiefern die  digitalisierte Version der vermuteten Originalhandschrift, welche mit aufwändigen  Bildern versehen ist, im Netz (wenigstens eingeschränkt) zugänglich gemacht werden  kann. Falls dem nichts entgegensteht, ist als Projekt nach der Dissertation eine  digitale Edition von Wölflis Reise nach Jerusalem angedacht, im Stile des",de,Gegenstand Dissertation Reise Jerusalem Heinrich Wölfli erhalten deutsch Übersetzung Johannes Haller einerseits klassisch literaturwissenschaftlich Sinn narratologisch Analyse Beschreibung handschriftenvergleich Stemma jahresschätzung bekannt handschrift anhand inhaltlich Abweichung wasserzeich vornehmen zudem kodikologisch Befund Biographie Autor übersetzers Stellenkommentar Lexikon Text speziell ansonsten eher unbekannt Wörter erstellen andererseits Mithilfe zusätzlich Poster relevantesen Punkt angedacht teilweise detailliert beschrieben Datum klein Ortschaft genau Festgehalten reisestationen Wölfli xml erfassen Reise vorgestellt Teilprojekt dienen sicher Visualisierung geisteswissenschaftlich daten Vernetzung speziell Sinn zudem angereichert fäch nutzbar fäch Geschichte Geographie religionswissenschafen Teilprojekt Dissertation derzeit Arbeit wieviel März abschließen derzeit abschätzbar sicher daten aufbereiten Poster vorstellen generell technisch Umsetzung theoretisch Überlegung darstellen erörtern gelten handschriftenvergleich sowohl Methode angehen wobei Fokus Beitrag primär Visualisierung Reiseroute liegen Zukunft urheberrechtspolitisch eruiern inwiefern digitalisiert Version vermutet Originalhandschrift aufwändig bildern versehen Netz einschränken zugänglich falls entgegenstehen Projekt Dissertation digital Edition Wölflis Reise Jerusalem angedacht Stil,"[('reise', 0.27038990750797587), ('dissertation', 0.23858046026533222), ('handschriftenvergleich', 0.21331807936711125), ('jerusalem', 0.21331807936711125), ('wölfli', 0.21331807936711125), ('fäch', 0.19868961892361886), ('angedacht', 0.18025993833865056), ('teilprojekt', 0.18025993833865056), ('sicher', 0.1633030427606346), ('speziell', 0.12520253925842129)]"
2016,DHd2016,vortraege-014.xml,Moving around the City of Glass,"Jochen Laubrock (Universität Potsdam, Deutschland); Sven Hohenstein (Universität Potsdam, Deutschland); Alexander Thoß (Universität Potsdam, Deutschland)","Eye-Tracking, graphische Romane, Bildanalyse","Datenerkennung, Bilderfassung, Strukturanalyse, Beziehungsanalyse, Annotieren, Daten, Bilder, Literatur, Multimedia, Text","Graphische Romane und Comics vereinen als hybride Gattung Aspekte von Literatur und bildender Kunst und werden deshalb auch als sequenzielle Kunst bezeichnet. Man kann erwarten, dass sich die psychologische Wirkung graphischer Romane von der rein wortbasierter Romane unterscheidet. Einerseits sagt ein Bild mehr als tausend Worte, was die deutlich geringeren Zahl von Wörtern bei graphischen Adaptionen klassischer Romane erklärt, andererseits hat der Leser weniger Freiheitsgrade bei der visuellen Ausgestaltung der Szene und wird durch die erforderliche Integration von Bild und Text möglicherweise vor besondere Aufgaben gestellt. Wie interagieren Bild und Text beim Lesen graphischer Literatur und ermöglichen das Verstehen des Gesamtwerkes? Welche besonderen Herausforderungen stellt das multimediale Format an den Leser? Wie unterscheidet sich die Narrativität graphischer von der herkömmlicher Romane? Im Kontext der interdisziplinär ausgerichteten Nachwuchsgruppe ""Hybride Narrativität:   Digitale und Kognitive Methoden zum Leseverständnis Graphischer Literatur"" wurde die Im vorliegenden Beitrag illustrieren wir den potenziellen Nutzen einer solchen  Kombination anhand einer Analyse der Eye-tracking-Daten von (a) einer Sammlung  kürzerer Passagen aus mehreren kanonischen graphischen Romanen 'einem  repräsentativen Korpus 'und (b) Passagen der Graphic-Novel-Adaptation von Paul  Austers Roman ""City of Glass"" (Auster 1985; Karasik / Mazzucchelli / Auster 1994).  Für (a) berichten wir eine Analyse des relativen Anteils von Fixationen auf Text vs.  Bild. Effekte der Wortlänge sowie statistischen Worthäufigkeit in der geschriebenen  Sprache auf die Fixationsdauern zeigen, dass der Text auch tatsächlich gelesen und  rezipiert wird. Analysen der Fixationsmuster zeigen zudem, dass der Text meist vor  dem Bild angeschaut wird und das Bild oft entweder gar nicht oder rein im peripheren  Sehfeld analysiert wird. Interessante Objekte wie Personen oder Gesichter werden mit  höherer Aufmerksamkeit bedacht als Objekte des Hintergrundes. Ob das Bild überhaupt  betrachtet wird, ist unter anderem vom Informationsgehalt des Bildes abhängig, der  sich wiederum je nach Art des Überganges zwischen zwei Panels unterscheidet (McCloud  1993). Wenn sich die bestehende Handlung auf dem nächsten Panel fortsetzt, wird  dieses mit höherer Wahrscheinlichkeit übersprungen als ein Panel, das sich  deutlicher von seinem Vorgänger unterscheidet und damit einen entscheidenderen  Anteil an der Handlung hat, beispielsweise bei einem Szenenwechsel. Bei (b)  fokussieren wir insbesondere auf die Frage der Text-Bild-Beziehung. Unterscheidet  sich beispielsweise das Blickverhalten, wenn Bild und Text auf gemeinsame vs.  unterschiedliche Handlungsstränge fokussieren? Zudem berichten wir über deutliche  Zusammenhänge von Leser-Expertise mit graphischer Literatur und explizit gemessener  Verständnistiefe bei diesem speziellen Werk sowie implizit gemessenen Blickdauern.  Anders als beim Lesen von reinem Text drückt sich Expertise beim Lesen graphischer  Literatur nicht in geringeren, sondern in höheren Betrachtungszeiten aus, die sich  speziell auf den Bildanteil konzentrieren. Perspektivisch soll das Material anhand von aus dem computationalem Sehen abgeleiteten Deskriptoren beschrieben und klassifiziert werden. Beispielsweise sollen dazu Farb-Histogramme, lokales Fourier-Spektrum, der SURF-Algorithmus etc. genutzt und Klassifikationsverfahren aus dem Bereich des maschinellen Lernens angewandt werden. Diese werden sicherlich die stilistische Beschreibung anreichern und können als potenzielles Nebenprodukt auch die Suche in Bilddatenbanken ohne explizites verbales Tagging vorbereiten. Im Rahmen unseres Projektes erhoffen wir uns von einer derartigen Anreicherung der Daten jedoch eine Antwort auf die Frage, wie sich die Wechselwirkung solcher Bottom-up-Merkmale mit Top-down-Einflüssen von einfacher Worthäufigkeit bis hin zu narratologischen Elementen auf das Blickbewegungsverhalten und die Rezeption der Literatur auswirkt. Letztlich ist es eine empirische Frage, wie viel des Verhaltens und Verstehens sich durch simple Deskriptoren erklären lässt und welche Anteile sich durch Hinzunahme weiterer, beispielsweise konfigurationaler oder strategisch-aufgabenorientierter Merkmale aufklären lassen. Zusammenfassend berichten wir über eine von kognitiven und psychologischen Fragen geleitete Analyse graphischer Literatur und darauf erhobenen Blickbewegungsdaten. Zum einen werden dabei allgemeine Prinzipien anhand einer Sammlung verschiedener kanonischer Werke des Genres illustriert. Zum anderen beschreiben wir eine tiefere Analyse eines spezifischen Exemplars dieser Gattung.",de,graphisch Roman Comics verein hybrid Gattung Aspekt Literatur Bildender Kunst sequenziell Kunst bezeichnen erwarten psychologisch Wirkung graphisch Roman rein Wortbasierter Roman unterscheiden einerseits Bild tausend Wort deutlich gering Zahl wörtern graphisch adaption Klassischer Romane erklären andererseits Leser Freiheitsgrade visuell Ausgestaltung Szene erforderlich Integration Bild Text möglicherweise besonderer Aufgabe stellen interagier Bild Text lesen graphisch Literatur ermöglichen verstehen Gesamtwerk besonderer Herausforderung stellen multimedial Format Leser unterscheiden Narrativität graphisch herkömmlich Romane Kontext interdisziplinär ausgerichtet Nachwuchsgruppe hybrid Narrativität digital kognitiv Methode Leseverständnis graphisch Literatur vorliegend Beitrag illustrieren potenziell nutzen Kombination anhand Analyse Sammlung kürzer passage mehrere kanonisch graphisch romanen repräsentativ Korpus b Passage Paul Austers Roman city -- glass Auster Karasik Mazzucchelli Auster berichten Analyse relativ Anteil Fixation Text Bild effekte wortläng statistisch Worthäufigkeit geschrieben Sprache fixationsdauern zeigen Text tatsächlich lesen rezipieren Analyse Fixationsmuster zeigen zudem Text meist Bild anschauen Bild rein peripher Sehfeld analysieren interessant Objekt Person gesicht hoch Aufmerksamkeit bedacht Objekt Hintergrund Bild betrachten Informationsgehalt Bild abhängig wiederum Art übergang Panel unterscheiden Mccloud bestehend Handlung nächster Panel fortsetzen hoch Wahrscheinlichkeit überspringen Panel deutlich Vorgänger unterscheiden entscheidender Anteil Handlung beispielsweise Szenenwechsel b fokussieren insbesondere Frage unterscheiden beispielsweise blickverhalter Bild Text gemeinsam unterschiedlich Handlungssträng fokussieren zudem berichten deutlich Zusammenhäng graphisch Literatur explizit gemessen verständnistiefe speziell Werk implizit gemessen blickdauern lesen rein Text drücken expertise lesen graphisch Literatur geringeren hoch Betrachtungszeit speziell Bildanteil konzentrieren perspektivisch Material anhand Computationalem sehen abgeleitet deskriptoren beschreiben klassifizieren beispielsweise Lokale nutzen klassifikationsverfahren Bereich maschinell lernen anwenden sicherlich stilistisch Beschreibung anreichern Potenzielles Nebenprodukt Suche Bilddatenbanken explizit verbal Tagging vorbereiten Rahmen unser projektes erhoffen derartig Anreicherung daten Antwort Frage Wechselwirkung einfach Worthäufigkeit Narratologische Element blickbewegungsverhalt Rezeption Literatur auswirken letztlich empirisch Frage verhaltens verstehens Simple deskriptor erklären lässen Anteil Hinzunahme weit beispielsweise Konfigurationaler merkmal aufklären lassen zusammenfassend berichten kognitiv psychologisch Frage geleitet Analyse graphisch Literatur erhoben Blickbewegungsdat allgemein prinzipien anhand Sammlung verschieden kanonischer Werk Genr illustrieren beschreiben tief Analyse spezifisch exemplars Gattung,"[('graphisch', 0.3979744373992714), ('bild', 0.2956582723309533), ('literatur', 0.17015746655715613), ('lesen', 0.1569596306674915), ('unterscheiden', 0.1338406797928915), ('auster', 0.12853603675409508), ('berichten', 0.12710066459608427), ('gemessen', 0.1197215737006158), ('worthäufigkeit', 0.1197215737006158), ('panel', 0.11939233121978142)]"
2016,DHd2016,posters-065.xml,neonion - Kollaboratives Annotieren zur Erschließung von textuellen Quellen,"Claudia Müller-Birn (Freie Universität Berlin, Deutschland); Andre Breitenfeld (Freie Universität Berlin, Deutschland)","Annotieren, Software, Kollaboration, Textquellen","Gestaltung, Programmierung, Annotieren, Literatur, Manuskript, Software, Text","Im Rahmen einer Posterpräsentation stellen wir die kollaborative Annotationssoftware   neonion vor, dessen Entwicklung inspiriert wurde von der Vision des Memex. Vannevar   Bush führt in seinem Artikel dazu aus, dass ""[a] record if it is to be useful to   science, must be continuously extended, it must be stored, and above all it must be   consulted."" (Bush 1945: 37). Ein solcher ""record"" kann beispielsweise ein   historisches Dokument sein. Am Anfang des Forschungsprozesses, ist noch wenig   darüber bekannt, aber das Wissen um dieses Dokument wächst kontinuierlich durch die   Forschungsarbeit der Wissenschaftler_innen. Mit Hilfe von Die grundsätzlichen funktionalen Anforderungen an die Software neonion wurde  basierend auf den Erkenntnissen einer Interviewstudie durchgeführt. Ingesamt wurden  sechs Interviews durchgeführt. Diese Interviews waren semi-strukturiert und wurden  am Arbeitsplatz durchgeführt, um auch Informationen über das Arbeitsumfeld zu  erlangen und einen direkten Einblick in die genutzten Softwarewerkzeuge und Abläufe  zu erhalten. Die Interviews dauerten eine bis anderhalb Stunden und wurden  anschließend transkribiert. Alle Teilnehmer_innen waren Mitglieder der gleichen  Forschungsreinrichtung (für weitere Informationen s. Müller-Birn et al. 2015). Das  Ziel dieser Studie war es, den Kontext der Forschungsarbeit in den  Geisteswissenschaften besser zu verstehen. Solche Interviews sind zentral bei der  nutzerzentrierten Softwareentwicklung, einem Ansatz, der bei uns konsequent verfolgt  wird. Die Ergebnisse der Interviews wurden nach vier Gesichtspunkten ausgewertet:  Form, Funktion, Wert und Status (in Anlehnung an Marshall 1998). Wir nutzen diese  Ergebnisse im Folgenden, um die grundsätzlichen Funktionen von neonion vorzustellen. Der Bereich Form setzt sich mit der Struktur von Annotationen auseinander. Die Mehrzahl der Befragten gab an, vor allem basierend auf Textdokumenten zu forschen. Daher wurde entschieden, neonion zunächst für Textdokumente zu verwenden. Die Softwarearchitektur wurde mit Webframeworks umgesetzt. Hierzu findet Django im Bereich des Back-Ends und vorrangig Bootstrap und AngularJS im Front-End Anwendung. Die erzeugten Annotationen werden zur dauerhaften Aufbewahrung zum einen in den AnnotationStore, der auf Grundlage von ElasticSearch arbeitet, gespeichert, zum andern in den Sesame Triple Store eingespeist.  Im Bereich Funktion wurde der Frage der Verwendung nachgegangen. So wurde  ersichtlich, dass unterschiedliche Annotationsmodi (s. Abbildung 1) notwendig sind.  In neonion werden daher drei Arten der Annotation unterschieden: die Markierung für  Zitate, der Kommentar für Paraphrase und semantische Tags für das semantischen  Erschließen von Dokumenten (z. B. basierend auf einer zugrundeliegenden Ontologie).  Auch wenn diese drei Arten von Annotationen aus den Interviews entstanden sind,  können diese Annotationsmodi je nach Anwendungszweck sehr variabel eingesetzt  werden. Ein Einsatz von neonion im Bereich der Linguistik wäre beispielsweise  möglich, aber ein praktischer Anwendungsfall fehlt bisher. Zur Implementierung der Annotationskomponente kommt die quelloffene JavaScript Bibliothek Annotator.js zum Einsatz. Die Bibliothek der OKFN wurde zusätzlich durch eigene Plug-Ins, insbesondere zur Realisierung einer semantischen Annotation, erweitert.  Der längerfristige Wert der Annotation wurde im dritten Bereich untersucht. Hier wurde von allen Interviewteilnehmer_innen angegeben, dass eine Weiterverwendung der Annotationen in anderen Kontexten nicht möglich ist, da die verwendete Software den Export der Annotationen verhinderte. Diese Mangel sollte in neonion behoben werden. Alle Annotation werden einerseits innerhalb eines standardisierten Datenmodells 'dem Open Annotation Data Models (OADM) - gespeichert und anderseits haben Nutzende die Möglichkeit, alle ihre Annotationen nach unterschiedlichen Gesichtspunkten zu filtern und in ein Textdokument zur Weiterverarbeitung, z. B. in ein Textverarbeitungsprogramm zu exportieren (s. Abbildung 2). Mit Hinblick auf die Kollaboration besteht die Möglichkeit Annotationen innerhalb von Gruppen mit anderen Nutzer_innen zu teilen bzw. gemeinschaftlich Dokumente zu annotieren. Ebenfalls können die verwendeten strukturierten Vokabulare gemeinschaftlich erstellt werden. Es ist geplant, hier entsprechend benötigte Diskussionsfunktionen einzubauen. Darüber hinaus wurde das bestehende Open Annotation-Datenmodell um die Möglichkeit erweitert, semantische Tags über eine typisierte Verbindung in Relationen zueinander zu setzen. Die semantischen Tags stellen in diesem Zusammenhang Instanzen von vordefinierten Konzepten mit eigener URI (Unified Resource Identifier) dar und ermöglichen durch die Beziehung der Instanzen zueinander eine mehrstufige Analyse von Annotationen.  Im vierten Bereich wurde der Frage nachgegangen, wie Annotationen inhaltlich geteilt   werden. Unsere Interviewpartner führten aus, dass vor allem im Bereich der   strukturierten Annotationen (semantische Tags basieren auf einem vordefinierten   Begriffssystem) es sehr umständlich und zeitaufwändig ist, ein gemeinschaftliches   Begriffssystem zu erstellen. In neonion können solche Begriffssysteme, die zu einer   Ontologie weiterentwickelt werden können, einfach als sogenannte Concept Sets (s.   Abbildung 3) hinterlegt werden. Diese Concept Sets können dann auch wieder anderen   Personen in neuen Forschungskontexten zur Verfügung gestellt werden. Aus technischer   Sicht bietet das Backend von neonion verschiedene Dienste an, um Annotationen und   Concept Sets mit unterschiedlichen Systemen über eine spezifizierte REST API oder   SPARQL Endpoint auszutauschen.",de,Rahmen Posterpräsentation stellen kollaborativ annotationssoftwar Neonion Entwicklung inspirieren Vision memex Vannevar Bush führen Artikel record if -- -- to be useful to Science Must be Continuously extended -- Must be stored And above all -- Must be Consulted bush record beispielsweise historisch Dokument Anfang Forschungsprozesse wissen Dokument wachsen kontinuierlich Forschungsarbeit Hilfe grundsätzlich Funktional Anforderung softwar Neonion basierend Erkenntnis Interviewstudie durchführen Ingesamt Interview durchführen Interview Arbeitsplatz durchführen Information Arbeitsumfeld erlangen direkt Einblick genutzt Softwarewerkzeuge Ablauf erhalten Interview dauern anderhalb Stunde anschließend transkribieren Mitglied gleich Forschungsreinrichtung Information et Ziel Studie Kontext Forschungsarbeit geisteswissenschaften verstehen Interview zentral nutzerzentriert Softwareentwicklung Ansatz konsequent verfolgen Ergebnis Interview Gesichtspunkt auswerten Form Funktion Wert Status Anlehnung Marshall nutzen Ergebnis folgend grundsätzlich Funktion Neonion vorstellen Bereich Form setzen Struktur annotatio auseinander Mehrzahl befragter basierend textdokumenten forschen entscheiden Neonion Textdokumente verwenden Softwarearchitektur Webframeworks umsetzen hierzu finden django Bereich vorrangig bootstrap Angularjs Anwendung erzeugt annotationen dauerhaft Aufbewahrung Annotationstore Grundlage Elasticsearch arbeiten speichern Sesame Triple Store einspeisen Bereich Funktion Frage Verwendung nachgehen ersichtlich unterschiedlich Annotationsmodi Abbildung notwendig Neonion Art Annotation unterscheiden Markierung Zitat Kommentar Paraphrase semantisch tags semantisch erschließen dokument basierend zugrundeliegend Ontologie Art annotatio Interview entstehen Annotationsmodi Anwendungszweck variabel einsetzen Einsatz Neonion Bereich Linguistik beispielsweise praktisch Anwendungsfall fehlen Implementierung Annotationskomponente quelloffen Javascript Bibliothek Einsatz Bibliothek Okfn zusätzlich insbesondere Realisierung semantisch Annotation erweitern längerfristig Wert Annotation Bereich untersuchen angeben Weiterverwendung annotatio Kontext verwendet Software Export annotation verhindern Mangel Neonion beheben Annotation einerseits innerhalb standardisiert datenmodells op Annotation Data Model Oadm speichern anderseits Nutzende Möglichkeit annotatio unterschiedlich Gesichtspunkt Filter Textdokument Weiterverarbeitung Textverarbeitungsprogramm exportieren Abbildung Hinblick Kollaboration bestehen Möglichkeit annotation innerhalb Gruppe teilen gemeinschaftlich dokumenen annotieren ebenfalls verwendet strukturierten Vokabular gemeinschaftlich erstellen planen entsprechend benötigt diskussionsfunktionen einbauen hinaus bestehend Open Möglichkeit erweitern semantisch tags typisiert Verbindung relation Zueinander setzen semantisch tags Stelle Zusammenhang instanzen vordefiniert Konzept Uri unified resource Identifier dar ermöglichen Beziehung instanz Zueinander mehrstufig Analyse Annotation Bereich Frage nachgehen annotation inhaltlich teilen Interviewpartner führen Bereich strukturierten Annotation semantisch Tags basieren vordefiniert begriffssyst umständlich zeitaufwändig gemeinschaftlich begriffssyst erstellen Neonion Begriffssystem Ontologie weiterentwickeln einfach sogenannter Concept sets Abbildung hinterlegt Concept Set Person Forschungskontexte Verfügung stellen technisch Sicht bieten backend Neonion verschieden Dienst annotation Concept Set unterschiedlich Systeme spezifiziert Rest Api Sparql Endpoint austauschen,"[('neonion', 0.4781295178446362), ('interview', 0.2813852378996686), ('annotation', 0.1874190804923049), ('be', 0.16747745442470124), ('must', 0.15937650594821207), ('tags', 0.1452290605447104), ('gemeinschaftlich', 0.1406926189498343), ('semantisch', 0.13722107598996355), ('bereich', 0.13245713433096065), ('concept', 0.11883387242630919)]"
2016,DHd2016,posters-033.xml,Stefan George Digital,"Frederike Neuber (ZIM, Universität Graz, Österreich)","Typografie, Digitale Edition, Ontologien, Literaturwissenschaft, Modellierung","Transkription, Strukturanalyse, Modellierung, Annotieren, Theoretisierung, Archivierung, Veröffentlichung, Kommentierung, Webentwicklung, Visualisierung, Artefakte, Literatur, Text, texttragende Gegenstände","In der neueren deutschen Literatur steht Stefan George (1868-1933) wie kein anderer für die außergewöhnliche Beschäftigung eines Autors mit bzw. für die Verwendung von Typografie. Ab 1904 werden seine Werke in Eine serifenlose Schrift inmitten der in Deutschland tobenden Antiqua-Fraktur  Debatte zu verwenden, ihr Design an der eigenen Handschrift zu orientieren und  gleichzeitig auf historische Vorbilder zu referieren 'lediglich ausschnitthaft  verdeutlichen diese Aspekte die große Relevanz von Typografie für Georges Werk.  Umso verwunderlicher ist es, dass bisherige Editionen Das Editionskorpus StGDs besteht aus 29 Druckausgaben der insgesamt 11 lyrischen   Werke Die Modellierung der typografischen Detailformen erfolgt in Form einer Ontologie, welche ihre eindeutige Identifikation, formalisierte Beschreibung und Zitation ermöglicht. Damit wird eine netzwerkartige Erschließung und Verknüpfung unterschiedlicher Aspekte und Charakteristika von Schrift unternommen, welche sowohl für Mensch als auch für Maschine interpretierbar sind. Die Technologien des Semantic Web zur Wissensrepräsentation in Thesauri (SKOS), Klassenmodelle (RDFs) und Ontologien (OWL) können dafür ebenso verwendet werden wie Methoden der Daten- und Softwaremodellierung (UML). Sowohl die Digitale Edition als auch die Ontologie zur Beschreibung von Typografie werden unter CC BY-SA Lizenz auf der Das Projekt ist vorrangig für die digitale Editorik relevant, die seit geraumer Zeit  verstärkt auch die Materialität von Dokumenten zu erschließen versucht. Statt den Weg der  Abbildung von Schrift mittels Faksimiles oder ihrer Rekonstruktion im Rahmen der  Transkription (z. B. Schriftfaksimile ) zu gehen, wählt das Projekt die formale  Modellierung und macht die Informationen so analysierbar. In diesem Zusammenhang trägt  StGD auch zur Bildung einer Schließlich kann das StGD auch als exemplarisch für den Einsatz und Umgang mit projektspezifischen Datenmodellen gelten. Um die Ontologie für die breitere Forschungscommunity nutzbar zu gestalten, wird die Übertragbarkeit des Modells auf verschiedene Arten von Typen, wie beispielsweise bewegliche Lettern und frühneuzeitliche Typen, getestet. Außerdem ist der Versuch eines mappings des Modells auf Handschriften in Zusammenarbeit mit Neben einer Gesamtpräsentation des Projekts StGD, wird das Poster vorrangig drei aktuelle Herausforderungen illustrieren:",de,neu deutsch Literatur stehen Stefan George anderer außergewöhnlich Beschäftigung Autor Verwendung Typografie Werk serifenlos Schrift inmitten Deutschland tobend Debatte verwenden design Handschrift orientieren gleichzeitig historisch Vorbilder referieren lediglich ausschnitthafen verdeutlichen Aspekt Relevanz Typografie georg Werk umso verwunderlich bisherig Edition Editionskorpus Stgds bestehen Druckausgabe insgesamt lyrisch Werk Modellierung typografisch Detailform erfolgen Form Ontologie eindeutig Identifikation formalisieren Beschreibung Zitation ermöglichen netzwerkartig Erschließung Verknüpfung unterschiedlich Aspekt Charakteristika Schrift unternehmen sowohl Mensch Maschine interpretierbar Technologie Semantic Web Wissensrepräsentation Thesauri skos klassenmodelle Rdfs Ontologi owl verwenden Methode Softwaremodellierung uml sowohl digital Edition Ontologie Beschreibung Typografie cc Lizenz Projekt vorrangig digital Editorik relevant geraum verstärken Materialität Dokument erschließen versuchen Weg Abbildung Schrift mittels faksimile Rekonstruktion Rahmen Transkription Schriftfaksimile wählen Projekt formal Modellierung Information analysierbar Zusammenhang tragen stgd Bildung schließlich Stgd exemplarisch Einsatz Umgang projektspezifisch datenmodellen gelten Ontologie breit Forschungscommunity nutzbar gestalten Übertragbarkeit Modell verschieden Art Typ beispielsweise beweglich lettern frühneuzeitlich Typ testen Versuch Mapping Modell Handschrift Zusammenarbeit Gesamtpräsentation Projekt stgd Poster vorrangig aktuell Herausforderung illustrieren,"[('stgd', 0.3005396740340342), ('typografie', 0.27992992193824073), ('schrift', 0.22408756592273396), ('ontologie', 0.1981223490410199), ('vorrangig', 0.15338297746307178), ('handschrift', 0.13208156602734658), ('typ', 0.12067966527155177), ('werk', 0.11367366765650985), ('edition', 0.10148644725621699), ('beschreibung', 0.10148644725621699)]"
2016,DHd2016,posters-077.xml,Dramenwerkbank - Automatische Sprachverarbeitung zur Analyse von Figurenrede,"Andre Blessing (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Peggy Bockwinkel (Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland); Nils Reiter (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Marcus Willand (Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland)","Drama, Werkbank, Sprachverarbeitung, NLP, Textanalyse","Inhaltsanalyse, Strukturanalyse, Beziehungsanalyse, Annotieren, Netzwerkanalyse, Stilistische Analyse, Computer, Literatur, Metadaten, Personen, Software, Text","In diesem Beitrag stellen wir erste Einsichten aus einer quantitativen Analyse von Dramen vor, sowie unsere Konzeption für eine darauf aufbauende interaktive Werkbank, die einen Anstoß für eine Diskussion zur Tool-Unterstützung quantitativer Dramenanalyse geben soll. Die Werkbank unterstützt interessierte Forscherinnen und Forscher beim Einlesen von Dramen aus TEI-basierten Quellen und befindet sich noch in Entwicklung Um die Anwendungsgebiete der Werkbank aufzuzeigen, skizzieren wir 'anhand einer Dramentexte unterscheiden sich insbesondere durch zwei zusammenhängende  Eigenschaften von Prosatexten: a) Dramatische Texte sind im Gegensatz zu vielen  anderen Textsorten auf allen Ebenen (Akt- bis Redefolge) ausgesprochen gut  strukturiert und ermöglichen somit eine verhältnismäßig unaufwändige  Datenerhebung. Die Kehrseite der guten Strukturiertheit ist dass dramatische  Texte damit nicht dem Prototyp eines Textes entsprechen, wie er von vielen  Werkzeugen zur Sprachverarbeitung angenommen wird. Die maschinelle  Sprachverarbeitung auf dramatischen Texten ist damit nicht durch existierende  Werkzeuge ""out of the box"" zu leisten. b) Die dramatischen Figuren sprechen Eine automatisierte Erfassung der Oberflächenstruktur inklusiver aller relevanten  Metadaten dramatischer Texte ist die Grundvoraussatzung einer quantitativen  Textanalyse im oben genannten Sinne. TEI / XML ist als Standard etabliert, um  Texte und Korpora möglichst genau entsprechend der/einer gedruckten Edition  digital zu kodieren (cf. TextGrid; DTA). Insbesondere erlaubt TEI auch die  Kodierung von Seitenzahlen, Formatierungen, Zeilenumbrüchen, Kopf- und Fußzeilen  und vieles mehr, was über den reinen Textinhalt hinausgeht. Wie Trilcke et al. (2015) auch schon festgestellt haben, ist die Extraktion der inhaltlichen Textstruktur aus den TEI-Daten keineswegs trivial. Für Netzwerkanalyse ist die eindeutige Identifizierung von Figuren besonders relevant, für eine (maschinelle, computergestützte) Analyse des Inhaltes und der Häufigkeit der Figurenrede kommen o.g. Formatierungsmarkierungen noch als Herausforderung hinzu. In unserer Werkbank bieten wir einen Plausibilitätscheck an, der es erlaubt, Fehler im Importprozess (die sowohl durch Fehlannahmen im Importmodul als auch durch Fehlkodierungen in den Quelldaten verursacht werden können) direkt zu erkennen und zu beheben. Einmal identifizierte und behobene Fehler fließen in die Quelldaten zurück. In den bereits existierenden Arbeiten zur Stilometrie auf Dramen werden komplette Dramen verglichen (z. B. durch Vorverarbeitung mit DIGIVOY). Ein differenzierter Vergleich, bei dem einzelne Figuren oder Gruppen von Figuren betrachtet werden, ist so noch nicht möglich gewesen. Andere Projekte gehen genau den gegenteiligen Weg und verwerfen alle Dialoginhalte und beziehen ihre Netzwerkanalyse nur auf die Interaktion der jeweils in der Szene aktiven Figuren (cf. Trilcke et al.). Uns ist kein verfügbares System bekannt, das diese Lücke schließt und eine inhaltliche Analyse erlaubt, die sowohl die Interaktion der aktiven Figuren als auch deren Redeinhalt einbezieht. In unserer Werkbank erfolgt die Textanalyse mit computerlinguistischen Werkzeugen, welche durch die CLARIN-D Infrastruktur (Mahlow et al. 2014) bereitgestellt werden. Der Aufbau von Dramen erfordert eine spezielle Herangehensweise bei der Textanalyse, da die in der Computerlinguistik oft getroffene Annahme, dass Texte aus vollständigen und grammatikalisch wohlgeformten Sätzen bestehen, in Dramen nicht zutrifft (wie auch in Texten aus sozialen Medien oder in gesprochener Sprache). Daneben weisen Dramen die oben genannte spezifische Struktur auf, die eine adäquate Vorverarbeitung bedingt. Um eine Verarbeitung mit einer nicht modifizierten CL-Verarbeitungskette zu ermöglichen, wird das Drama vorher in passende Textsegmente zerlegt. Segmente, die zu einem Dialog gehören müssen nach der Verarbeitung wieder der jeweiligen Figur zugeordnet werden. Im Kontext der Figurenanalyse sind insbesondere Eigennamenerkennung und Koreferenzresolution von Interesse. Wenn man den stilometrischen Blick weitet und auch syntaktische Konstruktionen (verwendet eine Figur mehr oder weniger komplexe Satzstruktur?) untersuchen möchte, sind auch andere linguistische Verarbeitungsschritte möglich. Die Ergebnisse dieser Verarbeitung werden nicht fehlerfrei sein, deswegen bietet die Werkbank Möglichkeiten, die Ergebnisse zu korrigieren. Insbesondere die Zusammenführung von unterschiedlich genannten oder geschriebenen (z. B. ""Emilia"" vs. ""Emilie"" oder ""die Soldaten"" vs. ""erster Soldat"") Figuren ist nicht trivial und teilweise nur durch zusätzliches Weltwissen realisierbar. Damit dieser Schritt vereinfacht wird kommt hier ein halb-automatischer Figurenabgleich zum Einsatz. Das überarbeitete und manuell geprüfte Drama kann in einem TEI-konformen Format exportiert werden, damit die so kuratierte Ressource wieder der Community zur Verfügung gestellt werden kann. Linguistische Annotationen, die in TEI nicht direkt repräsentiert werden können, werden in einem geeigneten stand-off-Format exportiert. In einer Pilotstudie haben wir anhand eines einzelnen Dramas exploriert, wie  der Zusammenhang von (der zentralen) Dramenfigur zur dramatischen Handlung  automatisiert sichtbar gemacht werden kann. Die (zentrale) Stellung im  Figurennetzwerk wird dabei nicht (wie in der aktuellen Forschung gängig;  vgl. Moretti 2011) lediglich durch häufige Präsenz oder Interaktion auf der  Bühne repräsentiert, sondern durch differenziertere Analysen der  Figurenaktivität. Die Kombination von in Dramen vorhandenen strukturellen Informationen und durch automatische Verarbeitung ermittelte inhaltlich-semantische Information erlaubt neue, feinkörnige Analysen von Dramen. Die im Folgenden genannten sollen durch die Werkbank unterstützt werden, entweder durch Integration existierender oder durch Entwicklung neuer Tools. Möglich ist eine automatische Auswertung der Figurenreden nach inhaltlichen  Kriterien. Ohne Vorwissen bereitstellen zu müssen, lassen sich wichtige  Begriffe, durch deren Verwendung sich eine Figur von anderen unterscheidet,  mit Verfahren wie TF*IDF ermitteln und z. B. als Tabelle oder als Wortwolke  darstellen. Komplexere Verfahren wie topic modeling (Blei et al. 2003) oder  Wortfeldanalysen können natürlich auch auf den Redeinhalt einer Person (ggf.  auf Akte / Szenen o. ä. eingeschränkt) angewendet werden, erfordern aber  zumindest die Einstellung von Parametern (z. B. Anzahl der topics im topic  modelling) oder das Spezifizieren von Wortfeldern. Automatische Methoden zur  Erweiterung von Wortfeldern (angelehnt an z. B. Query Expansion, vgl.  Manning et al. 2008) können diesen Prozess unterstützen und sollen im  Rahmen der Werkbank erprobt und integriert werden. Abbildung 2 zeigt eine  visuelle Auswertung dieser Analyse. Stilometrische Analysen werden durch eine Schnittstelle ermöglicht, durch die  man Figurenrede als Datenstrukturen in R abrufen und dann nach diversen  Kriterien untersuchen kann, etwa mit Hilfe von stylo (Eder et al. 2013). Es  ließe sich z. B. untersuchen, ob Könige bei Schiller anders sprechen als bei  Lessing, oder ob Bürgerfiguren in einem bestimmten Dramenkorpus anders  sprechen als Adelsfiguren: Durch Methoden aus der Sentiment-Analyse (die zur automatisierten Analyse von Produktreviews eingesetzt wird) ließe sich z. B. analysieren, wie und ob bestimmte Figuren über andere sprechen. Neben positiv / negativ wären auch feinere, dramenspezifische Unterscheidungen denkbar (Feigling, Hahnrei, ...). Die Kombination dieser Techniken mit Netzwerkanalyseverfahren würde es erlauben, im Netzwerk auch Entitäten darzustellen über die geredet wird, ohne dass sie direkt im Drama vorkommen (z. B. Gott), Kanten zwischen Knoten können dann (z. B. durch Farben) auch inhaltliche, relationale Informationen kodieren (X spricht viel / positiv über Y). Eine Netzwerkdarstellung, in der die Position der Figuren nicht mehr zufällig (oder durch Layout-Algorithmen gesteuert) ist ist ebenfalls denkbar (Abbildung 4). Dabei werden prototypischen Figurenrollen feste Positionen in einem Raster zugewiesen, so dass große Mengen an Netzwerken schnell und direkt verglichen werden können.",de,Beitrag stellen Einsicht quantitativ Analyse Dram Konzeption aufbauend interaktiv Werkbank Anstoß Diskussion quantitativ Dramenanalyse geben Werkbank unterstützen interessierter Forscherinn Forscher Einlesen Dram quellen befinden Entwicklung anwendungsgebiete Werkbank aufzuzeigen skizzieren anhand dramentext unterscheiden insbesondere zusammenhängend eigenschaften Prosatext dramatisch Text Gegensatz Textsort eben redefolge ausgesprochen strukturieren ermöglichen somit verhältnismäßig unaufwändig Datenerhebung Kehrseite gut Strukturiertheit dramatisch Text Prototyp Text entsprechen werkzeugen Sprachverarbeitung annehmen maschinell Sprachverarbeitung dramatisch Text existierend werkzeuge out -- -- box leisten b dramatisch Figur sprechen automatisiert Erfassung Oberflächenstruktur inklusiv relevant metadaten dramatisch Text Grundvoraussatzung quantitativ Textanalyse genannt Sinn tei xml Standard etablieren Text Korpora möglichst genau entsprechend gedruckt Edition Digital kodieren cf Textgrid dta insbesondere erlauben tei Kodierung seitenzahlen formatierung zeilenumbrüch Fußzeile viele rein Textinhalt hinausgehen trilcke et feststellen Extraktion inhaltlich Textstruktur keineswegs Trivial Netzwerkanalyse eindeutig Identifizierung Figur relevant maschinell computergestützt Analyse inhaltes Häufigkeit Figurenrede Formatierungsmarkierunge Herausforderung hinzu Werkbank bieten Plausibilitätscheck erlauben Fehler Importprozess sowohl Fehlannahme importmodul Fehlkodierunge quelldaten verursachen direkt erkennen beheben identifiziert behoben Fehler fließen quelldaten existierend Arbeit Stilometrie Dram komplett Dram vergleichen Vorverarbeitung Digivoy differenziert Vergleich einzeln Figur Gruppe Figur betrachten Projekt genau gegenteilig Weg verwerfen dialoginhalt beziehen Netzwerkanalyse Interaktion jeweils Szene aktiv Figur cf trilcke et verfügbarer System Lücke schließen inhaltlich Analyse erlauben sowohl Interaktion aktiv Figur Redeinhalt einbeziehen Werkbank erfolgen Textanalyse computerlinguistisch Werkzeug Infrastruktur mahlow et bereitstellen Aufbau Dram erfordern speziell Herangehensweise Textanalyse Computerlinguistik getroffen Annahme Text Vollständig grammatikalisch wohlgeformt Satz bestehen Dram zutreffen Text sozial Medium gesprochen Sprache weisen dramen genannt spezifisch Struktur adäquat Vorverarbeitung bedingt Verarbeitung modifiziert ermöglichen Drama vorher passend Textsegmente zerlegt segmenen Dialog gehören Verarbeitung jeweilig Figur zuordnen Kontext Figurenanalyse insbesondere Eigennamenerkennung Koreferenzresolution Interesse stilometrisch Blick weiten syntaktisch Konstruktion verwenden Figur komplex Satzstruktur untersuchen linguistisch Verarbeitungsschritt Ergebnis Verarbeitung fehlerfrei bieten Werkbank möglichkeiten Ergebnis korrigieren insbesondere Zusammenführung unterschiedlich genannt geschrieben Emilia Emilie Soldat Soldat figuren trivial teilweise zusätzlich Weltwissen realisierbar Schritt vereinfachen Figurenabgleich Einsatz überarbeitet manuell geprüft Drama Format exportieren kuratieren Ressource Community Verfügung stellen linguistisch Annotation tei direkt repräsentieren geeignet exportieren Pilotstudie anhand einzeln Dramas explorieren Zusammenhang zentral Dramenfigur dramatisch Handlung automatisiert sichtbar zentral Stellung Figurennetzwerk aktuell Forschung gängig moretti lediglich häufig Präsenz Interaktion Bühne repräsentieren differenziert Analyse Figurenaktivität Kombination Dram vorhanden strukturell Information automatisch Verarbeitung ermittelt Information erlauben feinkörnig Analyse Dram folgend Genannt Werkbank unterstützen Integration existierend Entwicklung neu Tools automatisch Auswertung figurenred inhaltlich kriterien Vorwissen bereitstellen lassen wichtig Begriff Verwendung Figur unterscheiden Verfahren ermitteln Tabell wortwolke darstellen komplex Verfahren Topic Modeling blei et wortfeldanalysen Redeinhalt Person Akt Szene einschränken anwenden erfordern zumindest Einstellung Parameter Anzahl Topics Topic Modelling Spezifizieren Wortfelder automatisch Methode Erweiterung Wortfelder anlehnen Query Expansion Manning et Prozess unterstützen Rahmen Werkbank erproben integrieren Abbildung zeigen visuell Auswertung Analyse Stilometrische analysen Schnittstelle ermöglichen figurenreden datenstrukturen r abrufen diverser kriterien untersuchen Hilfe Stylo ed et lassen untersuchen könig Schiller sprechen Lessing Bürgerfigure bestimmt Dramenkorpus sprechen adelsfiguren Methode automatisiert Analyse Produktreviews einsetzen lassen analysieren bestimmt Figur sprechen positiv negativ sein feinere dramenspezifisch unterscheidungen denkbar Feigling Hahnrei Kombination Technik netzwerkanalyseverfahren erlauben netzwerk entität darzustellen reden direkt Drama vorkommen Gott kanen knoten Farbe inhaltlich relational Information Kodier x sprechen positiv y Netzwerkdarstellung Position Figur zufällig steuern ebenfalls denkbar Abbildung prototypisch figurenroll fest Position Raster zuweisen Menge netzwerken schnell direkt vergleichen,"[('werkbank', 0.40342847843887414), ('figur', 0.21982731566081304), ('dram', 0.21425098183774574), ('dramatisch', 0.17986261883979432), ('sprechen', 0.12771563499790897), ('verarbeitung', 0.12149547450472736), ('interaktion', 0.10544036053699458), ('erlauben', 0.10368670998073272), ('inhaltlich', 0.10126270262739785), ('redeinhalt', 0.10085711960971853)]"
2016,DHd2016,vortraege-009.xml,DH-Projekte Österreichischer Literaturarchive: Ein Problembericht,"Vanessa Hannesschläger (ACDH-OEAW Austrian Center for Digital Humanities, Österreich)","Archiv, best practices, Standards, Digitalisierung","Teilen, Sammlung, Transkription, Gestaltung, Programmierung, Modellierung, Annotieren, Kommunikation, Archivierung, Community-Bildung, Bewertung, Kollaboration, Lehre, Projektmanagement, Webentwicklung, Organisation, Visualisierung, Bibliographie, Curricula, Daten, Infrastruktur, Literatur, Manuskript, Metadaten, Methoden, Projekte, Forschung, Forschungsprozess, Forschungsergebnis, Software, Standards, Text, Visualisierung, virtuelle Forschungsumgebungen","In diesem Beitrag werden die Probleme skizziert, die sich aus Praktiken  Österreichischer Archive bei der Umsetzung von online-Projekten ergeben. Die  Beschränkung auf Österreich ergibt sich, um die Beispiel-Palette überschaubar zu  halten; die Problembereiche und Lösungsvorschläge lassen sich allerdings  allgemein anwenden. State-of-the-art Projekte von Bibliotheken und Archiven im  deutschsprachigen Raum, die den neusten Stand der Forschung umsetzen, werden in  einem ersten Schritt beschrieben. Der zweite Abschnitt skizziert Gründe dafür  und Konsequenzen daraus, dass diese Standards häufig nicht herangezogen werden.  Schließlich werden Lösungsvorschläge präsentiert und eine Agenda vorgeschlagen,  die die Situation nachhaltig verbessern könnte. Diese wird im Rahmen des  Vortrags auf der DHd 2016 im Zentrum stehen. Dort werden auch die hier allgemein  beschriebenen Schwierigkeiten anhand mehrerer Beispielprojekte illustriert. Vor allem im Bereich der digitalen Edition haben sich im deutschsprachigen Raum  auf breiter Ebene Standards und ""best practices"" entwickelt, die von einer  etablierten Community umgesetzt werden. Umfangreiche Bibliotheken publizierter /  rechtefreier Werke bieten etwa das Im Bereich der Beforschung von Archivbeständen überwiegen im digitalen Raum  ebenfalls Editionsprojekte, die die erwähnten Standards zur Anwendung bringen.  Beispiele hierfür sind das Wegweisend speziell für die Arbeit mit literarischen Nachlässen ist auch das Virtual Research Environment (VRE) Im Bereich der Archivierung und Bereitstellung von elektronischen Publikationen, Multimedia-Objekten und anderen digitalen Daten wird in Österreich das Projekt Die in Österreichischen Literaturarchiven aufbewahrten Bestände werden im Rahmen von wissenschaftlichen Projekten mit direkt an der Institution angesiedelten Mitarbeitenden erforscht und publiziert. Aufgrund der Vergabepolitik des FWF Forschungsfonds, der in den allermeisten Fällen Geldgeber dieser Unternehmen ist, haben die betreffenden Projekte mittlerweile häufig eine digitale Komponente. Projektleitende und Mitarbeitende sind zumeist literaturwissenschaftlich ausgebildet. Sie konzipieren und entwerfen, wie die digitale Repräsentation ihrer Arbeit strukturiert wird und erarbeiten das wissenschaftliche Konzept, das Inhalt und Funktionalität zugrundeliegt. Für die technische Umsetzung werden meist erst nach Abschluss der konzeptionellen Arbeit externe Auftragnehmende engagiert, oft privatwirtschaftliche IT-Unternehmen, die von den Möglichkeiten, die im Bereich der DH bereits verfügbar wären, nur eingeschränkte Kenntnis haben. Aus dieser Situation ergeben sich Probleme in mehreren Bereichen: Die im Rahmen von Projekten erstellten Scans sollten in einer digitalen Langzeitarchivierung der projekttragenden Institution abgelegt werden, was fallweise versäumt wird. Gründe: Die Projektzuständigen haben aufgrund ihrer Ausbildung meist einen editorischen oder von archivarischen Ordnungsprinzipien geprägten Zugang zur Modellierung und Strukturierung der Projektdaten. Gründe und Konsequenzen: Forschungsprojekte werden häufig in Zusammenarbeit mit Firmen umgesetzt, die nicht (primär) mit wissenschaftlicher Klientel arbeiten, deren Wünsche und Methoden daher nicht im Detail verstehen und nicht mit bereits existierenden DH-Tools und Ressourcen vertraut sind. Konsequenzen: An hostenden Institutionen werden kaum personelle Ressourcen zur Wartung abgeschlossener online-Projekte einkalkuliert. Projekt-Websites sterben daher oft nach wenigen Jahren, mit ihnen die Daten. Auch in Projektfinanzierungsplänen wird dieser Aspekt bislang nicht berücksichtigt. Konsequenzen: Der skizzierten Situation muss auf allen Ebenen begegnet werden: Seitens der hostenden Institutionen muss stärker daran gearbeitet werden, für langfristige Datensicherung Möglichkeiten zu entwickeln und anzubieten oder Kooperationen mit Langzeitdatenarchiven einzugehen. Dafür müssen sowohl substanzielle finanzielle als auch personelle Ressourcen explizit dieser Aufgabe zugeordnet werden. Die Postionen, die die Bei der Bewilligung von Projektanträgen sollten Fördergebende die skizzierten Probleme ernsthaft berücksichtigen und Projekte, die keine ausreichenden Ressourcen für die Arbeit an der digitalen Repräsentation der Projektergebnisse vorsehen, ablehnen - anstatt die Praxis, utopische Ziele in Projektanträge einzubauen, zu unterstützen. (Inter)Nationale DH-Plattformen sollten es sich zur Aufgabe machen, ein entsprechendes Empfehlungspapier zur Verfügung zu stellen. Bedenkenswert ist auch die Forderung nach einem ""offenen Lebenszyklus"" von Forschungsprojekten, die etwa von der Plattform Das größte Potential zur nachhaltigen Verbesserung der Situation liegt im  Bereich der Projektangestellten. Geisteswissenschaftlich Forschende erfahren  im Rahmen des Studiums unzureichende Ausbildung zur Arbeit im digitalen Raum  und haben in der Folge entsprechende Hemmungen, mit digital humanists in  Austausch zu treten. Deshalb werden DH von nicht primär im digitalen Raum  arbeitenden Forschenden noch immer als eigene Disziplin wahrgenommen anstatt  als Teil und Methode des geisteswissenschaftlichen Forschens an sich. Hier  muss Bewusstsein geschaffen und Skepsis abgebaut werden, indem die Lehrpläne  grundlegend überarbeitet und gegenwärtigen Standards angepasst werden.  Dadurch würden viele der umrissenen Probleme gar nicht erst entstehen. Neben  den Forschenden der Zukunft, die man so erreichen kann, müssen kurz- bis  mittelfristig auch die Forschenden der Gegenwart stärker animiert werden,  sich mit seriösen Methodiken und Frameworks für Forschungsprojekte im  digitalen Raum auseinanderzusetzen, indem sie dort, wo sie mit ihrem Wissen  stehen, abgeholt werden. Dafür kann etwa die Vorgehensweise des Die Arbeit in den Bereichen Helpdesk und Outreach zeigt, dass für eine zeitnahe Verbesserung der Situation Adaptionen in der Ausbildung der Forschenden der Zukunft alleine nicht ausreichen; um die Forschenden der Gegenwart zu erreichen, die nicht in digitaler Methodik ausgebildet wurden und sich (noch) nicht damit auseinandergesetzt haben, braucht es ""Übersetzende"", die die Kommunikation zwischen rein geisteswissenschaftlich und rein digital Denkenden erleichtern und Brücken bauen. Die Wichtigkeit solcher Bindeglieder, die ""beide Sprachen sprechen"", kann nicht hoch genug eingeschätzt werden, da sie allen Beteiligten Frustration, Zeit, überflüssige Arbeit und letztlich auch Geld ersparen können. Zentren, Institute und Verbände, die in den Digital Humanities arbeiten, sollten ihre Aufmerksamkeit vermehrt auf diesen neuen Arbeitsbereich der digital Übersetzenden richten und ihre Aktivitäten gezielt in diese Richtung lenken. Die unzureichende Vernetzung geisteswissenschaftlich Forschender mit der DH Community führt zu technischen Unzulänglichkeiten in abseits davon ambitionierten digitalen Projekten, die ihre Nachhaltigkeit gefährden. Gegenseitige Annäherung über Outreach-Programme und die Adaption der Lehrpläne geisteswissenschaftlicher Studienrichtungen, vor allem aber verbesserte interne und externe Kommunikation sind notwendig, um zu nachhaltiger Verbesserung der Situation zu gelangen.",de,Beitrag Problem skizzieren Praktik österreichisch Archiv Umsetzung ergeben Beschränkung Österreich ergeben überschaubar halten problembereich lösungsvorschläge lassen allgemein anwenden Projekt Bibliothek Archiv deutschsprachig Raum neu Stand Forschung umsetzen Schritt beschreiben Abschnitt Skizziert gründe Konsequenz Standard häufig heranziehen schließlich lösungsvorschlag präsentieren Agenda vorschlagen Situation nachhaltig verbessern Rahmen Vortrag dhd Zentrum stehen allgemein beschrieben Schwierigkeit anhand mehrere Beispielprojekt illustrieren Bereich digital Edition deutschsprachig Raum breit Ebene Standard best practices entwickeln etabliert Community umsetzen umfangreich bibliothek Publizierter Rechtefreier Werk bieten Bereich Beforschung Archivbeständ überwiegen digital Raum ebenfalls editionsprojeken erwähnt Standard Anwendung bringen Beispiel hierfür wegweisend speziell Arbeit literarisch Nachläss virtual research environment vre Bereich Archivierung Bereitstellung elektronisch publikation digital daten Österreich Projekt österreichisch literaturarchiven aufbewahrten Beständ Rahmen wissenschaftlich Projekt direkt Institution angesiedelt Mitarbeitende erforschen publizieren aufgrund Vergabepolitik fwf Forschungsfond allermeister Fall Geldgeber Unternehmen betreffend Projekt mittlerweile häufig digital Komponente projektleitend Mitarbeitende zumeist literaturwissenschaftlich ausbilden konzipieren entwerfen digital Repräsentation Arbeit strukturieren erarbeiten wissenschaftlich Konzept Inhalt Funktionalität zugrundeliegt technisch Umsetzung meist Abschluss konzeptionell Arbeit Externe auftragnehmend engagieren privatwirtschaftlich Möglichkeit Bereich dh verfügbar sein eingeschränkt Kenntnis Situation ergeben Problem mehrere Bereich Rahmen Projekt erstellt Scan digital Langzeitarchivierung projekttragend Institution ablegen fallweise versäumen gründe Projektzuständig aufgrund Ausbildung meist editorisch archivarisch ordnungsprinzipien geprägt Zugang Modellierung Strukturierung Projektdat gründe Konsequenz forschungsprojekte häufig Zusammenarbeit Firma umsetzen primär wissenschaftlich Klientel arbeiten wünsch Methode Detail verstehen existierend Ressource vertraut Konsequenz Hostende Institution personell Ressource Wartung Abgeschlossener einkalkulieren sterben weniger daten projektfinanzierungsplänen Aspekt bislang berücksichtigen Konsequenz skizziert Situation Ebene begegnen seitens Hostende Institution stark arbeiten langfristig Datensicherung Möglichkeit entwickeln anbieten kooperation langzeitdatenarchive eingehen sowohl substanziell Finanziell personell Ressource explizit Aufgabe zuordnen postion Bewilligung Projektanträg Fördergebend skizziert Problem ernsthaft berücksichtigen Projekt ausreichend Ressource Arbeit digital Repräsentation Projektergebnisse vorsehen ablehnen anstatt Praxis utopisch Ziel projektanträge einbauen unterstützen inter national Aufgabe entsprechend Empfehlungspapier Verfügung stellen bedenkensweren Forderung offen Lebenszyklus Forschungsprojekt Plattform groß Potential nachhaltig Verbesserung Situation liegen Bereich projektangestellten geisteswissenschaftlich forschend erfahren Rahmen Studium unzureichend Ausbildung Arbeit digital Raum Folge entsprechend Hemmunge Digital Humanist austausch treten dh primär digital Raum arbeitend Forschende Disziplin wahrnehmen anstatt Methode geisteswissenschaftlich Forschen Bewusstsein schaffen Skepsis abbauen Lehrpläne grundlegend überarbeitet gegenwärtig Standard angepasst umrissen Problem entstehen Forschende Zukunft erreichen mittelfristig Forschende Gegenwart stark animieren Seriös methodiken Framework forschungsprojekte digital Raum auseinandersetzen Wissen stehen abholen vorgehensweise Arbeit Bereich Helpdesk outreach zeigen zeitnah Verbesserung Situation adaptionen Ausbildung Forschende Zukunft alleine ausreichen Forschende Gegenwart erreichen Digitaler Methodik ausbilden auseinandergesetzen brauchen übersetzend Kommunikation rein geisteswissenschaftlich rein Digital denkend erleichtern brücken bauen Wichtigkeit Bindeglieder Sprache sprechen einschätzen beteiligt Frustration überflüssig Arbeit letztlich Geld ersparen zentren Institut Verband Digital Humanitie arbeiten Aufmerksamkeit vermehrt Arbeitsbereich Digital übersetzend Richt Aktivität gezielt Richtung lenken unzureichend Vernetzung geisteswissenschaftlich forschend dh Community führen technisch Unzulänglichkeit abseits ambitioniert digital Projekt Nachhaltigkeit gefährden gegenseitig Annäherung Adaption Lehrpläne geisteswissenschaftlich Studienrichtunge verbessert interne extern Kommunikation notwendig nachhaltig Verbesserung Situation gelangen,"[('forschende', 0.2304034079751153), ('situation', 0.2064689692381731), ('digital', 0.19702782990472678), ('raum', 0.15192869393964967), ('institution', 0.14859885191119282), ('konsequenz', 0.14552611495232032), ('arbeit', 0.14224698993575027), ('ausbildung', 0.13824204478506918), ('geisteswissenschaftlich', 0.1346554192997521), ('bereich', 0.13015000525942055)]"
2016,DHd2016,posters-020.xml,Mit der FinderApp durch Goethes Faust: Treffer im Faksimile visuell hervorgehoben und multimediale Ausgabe in Videoaufführung und Hörbuch.,"Maximilian Hadersbeck (Ludwig Maximilians Universität München, Deutschland); Elisabeth Eder (Ludwig Maximilians Universität München, Deutschland); Roman Capsamun (Ludwig Maximilians Universität München, Deutschland); Nora Eichfeldt (Ludwig Maximilians Universität München, Deutschland); Simeon Herteis (Ludwig Maximilians Universität München, Deutschland); Matthias Lindinger (Ludwig Maximilians Universität München, Deutschland); Raphael Höps (Ludwig Maximilians Universität München, Deutschland); Stefan Schweter (Ludwig Maximilians Universität München, Deutschland)","GoetheFind, anchor-key XML, No SQL-Database, Faksimile-Viewer, Multimedia","Bilderfassung, Transkription, Programmierung, Modellierung, Annotieren, Kontextsetzung, Bearbeitung, Webentwicklung, Visualisierung, Daten, Bilder, Sprache, Link, Metadaten, Methoden, Multimedia, Projekte, Forschung, Software, Ton, Text, Werkzeuge, Video, Visualisierung, virtuelle Forschungsumgebungen","In unserem Poster möchten wir unsere neueste FinderApp In unserer neuen FinderApp GoetheFind, setzen wir Ideen des ""Standoff-Markups"" um, damit ""overtagged""-XML vermieden wird. Wir entwickelten eine reduzierte ""XML-TEI-P5 anchor-key"" Edition und speichern die Metainformatione in einer ""NoSQL-mongo""-Datenbank. Alle relevanten Editions-, OCR- und Transkriptionsinformationen zur multimedialen Trefferausgabe sind in der Datenbank gespeichert. Grundlage unserer FinderApp GoetheFind ist die XML-TEI-P5 Textedition im  DTABf Format vom Deutschen Textarchiv (DTA) der Berlin-Brandenburgischen  Akademie der Wissenschaften (BBAW 2013; Haaf et al. 2015), dazu die  Bilddigitalisate der Staatsbibliothek zu Berlin (BBAW 2013) und dem freiem  Hochstift Frankfurt des Frankfurter Goethe-Hauses (Signatur: III B / 23).  Da wir zur multimedialen Ausgabe der Suchtreffer zahlreiche  Metainformationen benötigen und ""overtagged"" XML des Editionstexts vermeiden  wollten, verwenden wir Ideen des ""Standoff-Markups"" und lagern alle  notwendigen Meta-Informationen in der ""NoSQL"" mongo-Datenbank GoetheDB aus.  Eine eindeutige Referenz der Datenbankeinträge zum Editionstext lösen wir  über den XML-TEI-P5 Tag <anchor/>, der an Seitenanfängen in die  Edition eingefügt ist. Die Trefferpositionen werden über ein  XML-Attributtrippel (Seite, Zeile, Token) genau spezifiziert. Da unsere FinderApp die Suchanfragen regelbasiert mit Hilfe von lokalen Grammatiken im Kontext eines Satzes realisiert, verwenden wir als wichtigste Strukturierungsebene Sätze. Goethes Faustdrama bettet Sätze in Rede und Gegenrede, sogenannte Repliken ein, die die zweite Strukturierungsebene darstellt. Zur visuellen Hervorhebung und multimedialen Ausgabe der gefundenen Textstellen im Faksimile ermitteln wir für die Replike geometrische Informationen mit Hilfe eines von uns entwickelten semiautomatischem OCR-Correction Tools. Die Bühnen- und Audioaufnahme werden mit Hilfe des Clarin-Tools: ""Munich Automatic Segmentation System WebMAUS"" (CLARIN) semiautomatisch transkribiert. Mit Hilfe unseres Speziallexikons GoetheLEX, angereichert um historische Sprachvarianten, Part of Speech Tagging und lokalen Grammatiken implementierten wir eine Partikelverb- und Semantische Suche. Bei der Eingabe von Suchanfragen verwenden wir eine symmetrische Autovervollständigung mit Informationen zur Häufigkeit des Auftretens im Text. Öhnlich wie bei Google-Docs entwickelten wir einen Browser basierter Faksimile-Viewer mit dem man in einem doppelseitigen Buchlesemodus durch das Dokument blättern kann und die gefundenen Textstellen farblich hervorgehoben werden. GoetheFind vernetzt alle Treffer mit der entsprechenden Replik in der Bühnenaufführung und der Hörbuchausgabe. Sobald der Nutzer auf die Mulitmediabuttons des Treffers drückt, startet im Browser ein Videoviewer oder eine Ausdioausgabe ab dieser Stelle. Wir danken dem Deutschen Textarchiv für die gute Zusammenarbeit und die freundliche Verfügungsstellung der Editionsdaten von Goethes Faust (BBAW 2013). Der Staatsbibliothek zu Berlin 'Preußischer Kulturbesitz und dem Freien Deutschen Hochstift, in Frankfurt danken wir für die Wiedergaberechte der Bilddigitalisate der Originalausgabe Goethes Faust.",de,unser Poster möchten neu finderapp Finderapp geoethefin setzen Idee vermeiden entwickeln reduziert Edition Speicher Metainformatione relevanen Transkriptionsinformation multimedial Trefferausgabe Datenbank speichern Grundlage Finderapp goethefind Textedition Dtabf Format deutsch Textarchiv dta Akademie Wissenschaft bbaw Haaf et bilddigitalisaten Staatsbibliothek Berlin bbaw Freiem Hochstift Frankfurt Frankfurter Signatur iii -- multimedial Ausgabe suchtreffer zahlreich Metainformation benötigen overtagged xml editionstexts vermeiden verwenden Idee lagern notwendig Nosql Goethedb eindeutig Referenz datenbankeintrag editionstext lösen anchor Seitenanfäng Edition einfügen Trefferpositione Seite zeile token genau spezifizieren Finderapp Suchanfrag Regelbasiert Hilfe lokal Grammatike Kontext Satz realisieren verwenden wichtig strukturierungseben sätze goeth Faustdrama betten Sätz Rede gegenred sogenannter Replike Strukturierungsebene darstellen visuell Hervorhebung multimedial Ausgabe gefunden Textstell Faksimile ermitteln replike geometrisch Information Hilfe entwickelt semiautomatisch Tools audioaufnahme Hilfe munich automatic Segmentation System webmaus clarin semiautomatisch transkribieren Hilfe unser speziallexikons Goethelex angereicheren historisch sprachvarianen Part of speech Tagging lokal Grammatike implementierten semantisch Suche Eingabe Suchanfrag verwenden symmetrisch Autovervollständigung Information Häufigkeit Auftreten Text öhnlich entwickeln Browser Basierter doppelseitig Buchlesemodus Dokument blättern gefunden Textstelle farblich hervorheben goethefind vernetzen Treffer entsprechend Replik Bühnenaufführung Hörbuchausgabe sobald Nutzer Mulitmediabuttons Treffer drücken starten Browser videoviewer Ausdioausgabe Stelle danken deutsch Textarchiv Zusammenarbeit freundlich Verfügungsstellung editionsdat goeth Faust bbaw Staatsbibliothek Berlin preußisch Kulturbesitz frei deutsch Hochstift Frankfurt danken Wiedergaberechte Bilddigitalisat Originalausgabe goeth Faust,"[('finderapp', 0.29742506036533967), ('bbaw', 0.2077716715725133), ('multimedial', 0.18849957491023933), ('goeth', 0.1623489692173397), ('suchanfrag', 0.14871253018266983), ('goethefind', 0.14871253018266983), ('danken', 0.14871253018266983), ('hilfe', 0.14309238797521476), ('staatsbibliothek', 0.13851444771500887), ('semiautomatisch', 0.13851444771500887)]"
2016,DHd2016,vortraege-030.xml,"Annotation und Distant Reading: Probleme, Synergien, Perspektiven","Angelika Zirker (Eberhard Karls Universität Tübingen, Deutschland); Matthias Bauer (Eberhard Karls Universität Tübingen, Deutschland)","Kollaboration, distant reading, close reading, erklärende Annotation","Inhaltsanalyse, Strukturanalyse, Modellierung, Annotieren, Kommunikation, Kontextsetzung, Theoretisierung, Bearbeitung, Kollaboration, Kommentierung, Text","In unserem Vortrag möchten wir Methoden des Unsere Ausgangsfrage ist, inwiefern Methoden des Im zweiten Teil des Vortrags werden Synergieeffekte von Methoden der Annotation und des Aus diesen Synergieeffekten ergibt sich der Anschluss an Perspektiven zum Verhältnis von (erklärender) Annotation und quantitativen Methoden des Der Vortrag bewegt sich an der Schnittstelle von Automatisierung und individuellen hermeneutischen Akten und damit entlang des Problems, wie im Markup eines Textes Entscheidungen getroffen werden können, welche Aspekte in einem Text relevant sind und die dem individuellen Text gerecht werden können. Wir möchten verschiedene Fallstudien aus englischsprachigen literarischen Texten vorstellen, etwa anhand von automatisierten Annotationssystemen wie x-ray, die oben geschilderte Probleme exemplarisch aufzeigen, die aber zugleich auch Synergieeffekte deutlich machen. Letztlich sollten bei dem Verhältnis von qualitativen Methoden des",de,unser Vortrag möchten Methode Ausgangsfrage inwiefern Methode Vortrag Synergieeffekt Methode Annotation Synergieeffekte ergeben Anschluss Perspektive Verhältnis erklärend Annotation quantitativ Methode Vortrag bewegen Schnittstelle Automatisierung individuell hermeneutisch Akte entlang Problem Markup Text Entscheidung treffen Aspekt Text relevant individuell Text gerecht möchten verschieden Fallstudi englischsprachig literarisch Text vorstellen anhand automatisiert Annotationssystem geschildert Problem exemplarisch aufzeigen Synergieeffekt deutlich letztlich Verhältnis qualitativ Methode,"[('synergieeffekt', 0.37925087524323575), ('vortrag', 0.31496377575190787), ('methode', 0.2863181708294011), ('möchten', 0.22544452284741343), ('individuell', 0.20141084907569082), ('verhältnis', 0.19566292529656285), ('synergieeffekte', 0.18962543762161788), ('ausgangsfrage', 0.18962543762161788), ('geschildert', 0.16739543435067475), ('erklärend', 0.16023897174832022)]"
2016,DHd2016,vortraege-039.xml,Kollaboratives Annotieren literarischer Texte,"Janina Jacke (Universität Hamburg, Deutschland); Evelyn Gius (Universität Hamburg, Deutschland)","Annotation, Kollaboration, Literatur, Narratologie, best practice","Strukturanalyse, Modellierung, Annotieren, Theoretisierung, Kollaboration, Literatur, Metadaten, Forschungsprozess, Text","Kollaboratives Annotieren ist eine gute Möglichkeit, um mehr als bloß eine subjektive   Perspektive auf den Untersuchungsgegenstand 'beispielsweise einen Text '  abzubilden: Sobald mehr als ein Individuum MarkUp an einem digitalen Objekt   anbringt, kann deutlich werden, welche unterschiedlichen Aspekte des Objekts im   Zentrum des individuellen Interesses stehen oder welche verschiedenen Sichtweisen in   Bezug auf denselben Aspekt möglich sind. Soll die kollaborative Annotation jedoch   nicht bloß die Pluralität von Perspektiven und Meinungen aufzeigen, sondern einem   spezifischeren Erkenntnisinteresse dienen, so sollte die Annotationspraxis mithilfe   von Guidelines strukturiert und reguliert werden. Für die Annotation linguistischer   Phänomene (bspw. in Gebrauchstexten) werden solche bereits entwickelt (vgl. bspw.   Pyysalo / Ginter 2014; Mamoouri et al. 2008); dagegen existieren für die   kollaborative Annotation semantischer Phänomene in literarischen Texten kaum Im Folgenden möchten wir einen Die Annotationsguidelines sind im Kontext des Projekts heureCLéA entstanden (vgl.  Bögel et al. im Erscheinen). Ziel des Projekts ist die Entwicklung einer digitalen  Heuristik, d. h. eines Funktionsmoduls, das automatisch semantische Phänomene in  literarischen Texten 'hier: narratologische Phänomene der Zeitgestaltung Nach einigen Anläufen hat sich folgende Annotationspraxis als Wie deutlich geworden ist, werden die eingangs genannten drei Probleme literarischer  Annotation im Kontext dieses Ablaufschemas berücksichtigt: Die Möglichkeit, einen  literarischen Text unterschiedlich zu deuten, wird zum einen durch die individuelle  Annotationsphase (vgl. Schritt 2) gewährleistet, zum anderen dadurch, dass  widersprüchliche Annotationen erlaubt sind, sofern sie durch die Polyvalenz des  Textes bedingt sind (vgl. Grund d). Dass die Berücksichtigung der Polyvalenz nicht  in eine Beliebigkeit von Annotationsentscheidungen abgleitet, wird dadurch erreicht,  dass andere Gründe (d. h. mindestens Gründe a und b) für widersprüchliche  Annotationen ausgeschlossen werden. Die Spezifikation literaturwissenschaftlicher  Annotationskategorien wird schrittweise optimiert, um aussagekräftige  Annotationsergebnisse zu gewährleisten (vgl. Schritt 1 sowie ggf. weitere  Optimierungsschritte ausgehend von Schritt 2 und Grund b). Da klare Definiertheit  nicht notwendig mit einer Einschränkung der Perspektive auf Texte einhergeht (vgl.  Schritt 1), ist sie mit der literaturwissenschaftlichen Praxis bzw. mit der  Pluralität möglicher Erkenntnisinteressen kompatibel. Die Abhängigkeit  literaturwissenschaftlicher Kategorien untereinander wird schließlich, je nach  verfügbaren Ressourcen, entweder in einem reduzierten oder in einem ausführlichen  Ansatz explizit gemacht (vgl. Grund c).",de,kollaborativ annotieren Möglichkeit bloß subjektiv Perspektive Untersuchungsgegenstand beispielsweise Text abzubilden sobald Individuum Markup digital Objekt anbringen deutlich unterschiedlich Aspekt objekts Zentrum individuell Interesse stehen verschieden Sichtweise Bezug Aspekt kollaborativ Annotation bloß Pluralität Perspektive meinung aufzeigen spezifischer Erkenntnisinteresse dienen Annotationspraxis Mithilfe guidelin strukturieren regulieren Annotation linguistisch phänomen gebrauchstexten entwickeln Pyysalo Ginter Mamoouri et existieren kollaborativ Annotation semantisch Phänomen literarisch Text folgend möchten annotationsguidelin Kontext Projekt Heurecléa entstehen Bögel et erscheinen Ziel Projekt Entwicklung digital Heuristik funktionsmoduls automatisch semantisch Phänomen literarisch Text narratologisch Phänomen Zeitgestaltung Anläuf folgend Annotationspraxis deutlich eingangs genannt Problem literarisch Annotation Kontext Ablaufschema berücksichtigen Möglichkeit literarisch Text unterschiedlich deuten individuell Annotationsphase Schritt gewährleisten widersprüchlich annotationen erlauben sofern Polyvalenz Text bedingt Grund d Berücksichtigung Polyvalenz Beliebigkeit Annotationsentscheidung abgleitet erreichen gründe mindestens gründ b widersprüchlich annotation ausschließen Spezifikation literaturwissenschaftliche Annotationskategorie schrittweise optimieren aussagekräftig Annotationsergebnisse gewährleisten Schritt Optimierungsschritte ausgehend Schritt Grund b klar Definiertheit notwendig Einschränkung Perspektive Text einhergehen Schritt literaturwissenschaftlich Praxis Pluralität möglich Erkenntnisinteressen kompatibel Abhängigkeit literaturwissenschaftlich kategorien untereinander schließlich verfügbar Ressource reduziert ausführlich Ansatz explizit Grund c,"[('polyvalenz', 0.21329695447349628), ('kollaborativ', 0.19534800536547253), ('pluralität', 0.1882919126652897), ('phänomen', 0.18747770987347492), ('annotation', 0.1710183587031906), ('annotationspraxis', 0.16810396908517788), ('widersprüchlich', 0.15903788909175848), ('grund', 0.15650427662487995), ('schritt', 0.15522595483954482), ('bloß', 0.1430989272769713)]"
2016,DHd2016,vortraege-013.xml,Korpusanalyse in der computergestützten Komparatistik,"Christine Ivanovic (Universität Wien, Österreich); Andrew U. Frank (Technische Universität Wien)","Korpusanalyse, RDF, Big Data, Komparatistische Literturwissenschaft, Natural Language Processing","Datenerkennung, Programmierung, Inhaltsanalyse, Strukturanalyse, Beziehungsanalyse, Räumliche Analyse, Annotieren, Kontextsetzung, Netzwerkanalyse, Stilistische Analyse, Organisation, Literatur, Methoden","Komparatistische Forschung zielt weniger auf hermeneutische Auslegung von Einzeltexten, als darauf, (a) generalisierende Aussagen über (literarische) Texte, ihre Fomen und Funktionen zu machen, (b) deren historische Entwicklung innerhalb oder (c) im Austausch von kulturellen Systemen zu rekonstruieren, und (d) literarische Repräsentationen von 'Welterfahrung' mit anderen Repräsentationssystemen zu vergleichen. Zu diesem Zweck vergleicht die Komparatistik eine Vielzahl von Texten (resp. von Texten und anderen künstlerischen Repräsentationsformen) in unterschiedlichen Sprachen. Vergleiche erfordern die Annahme einer Anzahl von Eigenschaften der verglichenen Gegenstände als gleichwertig, während andere Eigenschaften desselben Gegenstands variieren. Die Bestimmung der Eigenschaften eines Textes ist demnach eine der unabdinglichen Voraussetzungen für den komparatistischen Vergleich. Um allgemeine Aussagen machen zu können, muss die komparatistische Forschung andererseits eine größere Anzahl von Texten untersuchen: sie muss Textkorpora bilden und evaluieren. Die Anzahl und Auswahl der in Betracht gezogenen Texte sowie der evaluierten Texteigenschaften bestimmen maßgeblich die Ergebnisse einer komparatistischen Untersuchung. Eine computergestützte korpusbasierte komparatistische Untersuchung unterscheidet sich von den bisher praktizierten Ansätzen nicht in der 'die Disziplin charakterisierenden 'Operation des Vergleichens, wohl aber in der Art und Weise, wie Auswahl und Anzahl der verglichenen Texte resp. Textkorpora begründet und dokumentiert werden. Potentiell sind alle jemals verfassten und mündlich oder schriftlich tradierten Texte aller Sprachen und aller Zeiten Gegenstand der Komparatistik. Ein umfassender systematischer Zugriff auf alle diese Texte ist (bisher) jedoch nicht möglich. Die Möglichkeiten der Evaluierung sind durch verschiedene Faktoren limitiert: nicht alle Texte sind faktisch (mehr) zugänglich, und die jeweilige Forscherperspektive beschränkt grundsätzlich die Erfassung der zum Vergleich herangezogenen Texte. Bisher sind die Kriterien für die Textauswahl in wesentlichem Maße abhängig gewesen von (a) der Subjektivität und (b) der natürlicherweise begrenzten Kapazitätder Forscher, die nur die ihnen bekannten Texte berücksichtigen können und die unter dem Credo arbeiten, nur Texte zu erforschen, die ihnen in der Originalfassung zugänglich sind. Dies führt dazu, dass die Komparatistik bisher mehrheitlich Texte aus den dominanten Sprachen (Englisch, Französisch,...) bearbeitet und Texte in 'kleinen' Sprachen (Finnisch, Urdu,....) oder Textvergleiche zwischen kaum verwandten Sprachen (Chinesisch gegen Arabisch) eher selten vorkommen. Ein weiteres Problem der Textauswahl stellt (c) die Gefahr des logischen Zirkelschlusses dar: Bei der Evaluation beispielsweise ""des"" europäischen Romans werden aus der Lesepraxis resp. -tradition herrührende Vorannahmen in die Auswahl einbezogen, wenn es darum geht, dieses Genre anhand verschiedener Beispiele zu bestimmen; sie haben unweigerlich Einfluss auf das erzielte Ergebnis. Schließlich beruhen, und auch dies bedeutet eine wesentliche Einschränkung, (d) generalisierende Aussagen wie über ""den europäischen Roman"" immer auf einer im Vergleich zur Gesamtmenge der je produzierten Texte verschwindend kleinen Auswahl. Die Auswahl der evaluierten Texte kann bei einer computergestützten korpusorientierten komparatistischen Untersuchung zumindest statistisch anders begründet werden: * durch einen definitiv bestimmten Korpus, der so angelegt ist, dass er Repräsentativität beanspruchen kann * durch einen Korpus, der in seinem Umfang weit über das Lesevermögen des Einzelnen hinausreicht und der große Textmengen in einer Vielfalt von Sprachen umfaßt, die kein Einzelleser je bewältigen könnte; * durch die Möglichkeit der Überprüfung der erzielten Ergebnisse in Wiederholungs- und Vergleichsstudien sowie mittles Vergleichskorpora; * durch die Trennung der Auswahlkriterien für die Erstellung des Korpus von den fokussierten Untersuchungsergebnissen; * durch die Möglichkeit von Negativabfragen (z. B. eine bestimmte Eigenschaft ist  in einigen der Texte des Korpus nachweisbar, während andere Eigenschaften in  keinem davon nachweisbar sind). Computergestützt korpusbasiert arbeitende Komparatistik sieht sich mit folgenden Aufgaben konfrontiert: Für den Aufbau und die Pflege großer Textkorpora bedarf es entsprechend  bearbeiteter Texte: alle in den Korpus aufgenommene Texte müssen  bibliographisch genau erfasst und mit Markups (Taggern) versehen sein.  Markups können gesetzt werden u. a. zur Auszeichnung der Sprachform  (insbesondere bei mehrspachigen Texten), der Textstruktur,  nicht-literarischer Elemente (z. B. Abbildungen im Text) etc. Bevorzugt  werden treebank getaggte Versionen mit verzweiger Struktur (parse tree), die  Koreferenzen, Personen- und Orstnamen u. a. erkennen lassen. Im Textmarkup  werden einzelne Elemente der Textstruktur identifiziert: Worte oder  Wortbestandteile, Sätze und deren Teile, Abschnitte, Kapitel und andere  Texteinteilungen bis hin zum Buchlayout. Es erscheint wichtig, auch die  Elemente zu erfassen, die nicht unmittelbar textimmantent sind, die aber zur  Identifizierung und Charakterisierung des Textes gehören wie Seitenangaben,  Verfassername und weitere Angaben, die im Rahmen einer Buchpublikation  vorkommen. Der Text sollte in UTF-8 codiert sein, um auch Texte in  nicht-alphabetischen Sprachen wie Chinesisch, Arabisch u.w.m. einbeziehen zu  können. Unserer Konzeption nach sollen die Markierungen in den Text  hineingesetzt werden, so dass der mit den Annotationen versehene Text mit  der Originalstruktur verbunden bleibt. Der mit Markups versehene Text und die POS-Annotationen werden in ein einziges Format zusammengefasst. Wir bevorzugen derzeit RDF (Manola / Miller 2004), das für die von uns avisierte Datenmenge auszureichen scheint. Unseren bisherigen Beobachtungen nach erhalten wir bei einem Text mit reichhaltiger linguistischer Auszeichnung für jedes Wort etwa 10 RDF Triples; bei einem literarischen Text von 100.000 Wörtern würde das eine Million Triples ergeben, bei einem Korpus von 10.000 Büchern wäre man mit 10 Milliarden Triples noch bei weitem innerhalb des Rahmens dessen, was das heutige RDF Depot erlaubt; Untersuchungen zur derzeitigen Kapazitätsgrenze haben für 1 Billiarde Triples eine Hochladezeit von wenigen Stunden ergeben ( Es müssen Methoden sein, die Abfragen und Analysen von Texten ermöglichen Die Anwendung aller Methoden auf alle Texte generiert eine Matrix evaluierbarer Werte; jeder Text läßt sich durch einen Vektor aus diesen Werten darstellen. Diese Darstellungsweise ermöglicht Textvergleiche mittels Clusteranalyse wie sie in der konventionellen Komparatistik aufgrund der o.g. Beschränkungen bisher nicht zugänglich waren. Korpusaufbau und Abfragemethoden müssen so gestaltet sein, dass Texte umstandslos dem Korpus hinzugefügt werden und die Methoden problemlos appliziert werden können. Dies setzt kontinuierliche Pflege und Aktualisierung des bestehenden Korpus resp. der Abfrageergebnisse voraus: wenn Texte hinzugefügt werden, müssen alle bisher angewandten Methoden automatisch darauf angewandt werden können; wenn Methoden hinzugefügt werden, müssen automatisch alle Texte einer entsprechenden Evaluierung unterzogen werden. In unserem Beitrag treten wir für die Etablierung eines computergestützten korpusbasierten Forschungsansatzes in der literaturwissenschaftlichen Komparatistik ein. Dazu wollen wir darstellen, (a) welche Vorteile die Erstellung umfangreicher Korpora literarischer Texte aus verschiedenen Sprachen für die komparatistische Analyse bietet, (b) wie sie konstruiert und gepflegt werden können, und (c) welche Abfragemöglichkeiten auf dem gegenwärtigen Stand der Technik sie bieten. In Betracht gezogen werden dafür sowohl bereits vorhandene und online zugängliche Korpora wie auch von einzelnen Forschergruppen erarbeitete, intern genutzte Korpora wie das Austrian Academy Corpus am ICLTT der ÖAW. Des weiteren wollen wir einen kursorischen Überblick über die bisher erprobten Ansätze computergestützter literaturwissenschaftlicher Analyse geben, um das gegenwärtige Spektrum der Methoden der Textevaluierung darstellen und zukünftige Desiderata aufzeigen zu können.",de,komparatistisch Forschung zielen hermeneutisch Auslegung Einzeltexte generalisierend Aussage literarisch Text Fomen Funktion -- historisch Entwicklung innerhalb c Austausch kulturell Systeme rekonstruieren d literarisch Repräsentation welterfahrung Repräsentationssysteme vergleichen Zweck vergleichen Komparatistik Vielzahl Text Resp Text künstlerisch Repräsentationsforme unterschiedlich Sprache Vergleich Erforder Annahme Anzahl Eigenschaft verglichen Gegenstand gleichwertig eigenschaft Gegenstand variieren Bestimmung Eigenschaft Text demnach unabdinglich Voraussetzung komparatistisch Vergleich allgemein Aussage komparatistisch Forschung andererseits groß Anzahl Text untersuchen Textkorpora bilden evaluieren Anzahl Auswahl betracht gezogen Text evaluierten texteigenschaften Bestimme maßgeblich Ergebnis komparatistisch Untersuchung computergestützt korpusbasiert komparatistisch Untersuchung unterscheiden praktiziert Ansatz Disziplin charakterisierend Operation Vergleichen Art Weise Auswahl Anzahl verglichen Text Resp Textkorpora begründen dokumentieren potentiell jemals verfassten mündlich schriftlich tradiert Text Sprache Zeit Gegenstand Komparatistik umfassend systematisch Zugriff Text Möglichkeit Evaluierung verschieden Faktor limitieren Text faktisch zugänglich jeweilig Forscherperspektiv beschränken grundsätzlich Erfassung Vergleich herangezogen Text kriterien Textauswahl wesentlich Maß abhängig Subjektivität b natürlicherweise begrenzt kapazitätd Forscher bekannt Text berücksichtigen Credo arbeiten Text erforschen Originalfassung zugänglich führen Komparatistik mehrheitlich Text dominanten Sprache englisch französisch bearbeiten Text Sprache finnisch Urdu Textvergleiche verwandt Sprache chinesisch arabisch eher selten vorkommen Problem Textauswahl stellen c Gefahr logisch zirkelschlusse dar Evaluation beispielsweise europäisch Roman Lesepraxis Resp herrührend vorannahmen Auswahl einbeziehen Genre anhand verschieden Beispiel bestimmen unweigerlich einfluss erzielt Ergebnis schließlich beruhen bedeuten wesentlich Einschränkung d generalisierend Aussage europäisch Roman Vergleich Gesamtmenge produziert Text verschwindend Auswahl Auswahl evaluiert Text computergestützt korpusorientiert komparatistisch Untersuchung zumindest statistisch begründen definitiv bestimmt Korpus anlegen Repräsentativität beanspruchen korpus Umfang Lesevermögen einzeln Hinausreicht Textmeng Vielfalt Sprache umfassen Einzelleser bewältigen Möglichkeit Überprüfung erzielt Ergebnis vergleichsstudien mittl Vergleichskorpora Trennung Auswahlkriterie Erstellung Korpus Fokussiert untersuchungsergebnisse Möglichkeit Negativabfrage bestimmt Eigenschaft Text Korpus nachweisbar eigenschaften nachweisbar Computergestützt korpusbasieren arbeitend Komparatistik sehen folgend Aufgabe konfrontieren Aufbau Pflege Textkorpora bedürfen entsprechend bearbeitet Text Korpus aufgenommen Text bibliographisch genau erfasst Markups Tagger versehen Markups setzen Auszeichnung Sprachform insbesondere mehrspachig Text Textstruktur element Abbildung Text bevorzugen Treebank getaggt Version verzweig Struktur Parse Tree koreferenzen Orstnamen erkennen lassen Textmarkup einzeln Element Textstruktur identifizieren Wort wortbestandteil Sätz Teil abschnitten Kapitel Texteinteilung Buchlayout erscheinen wichtig element erfassen unmittelbar textimmantent Identifizierung Charakterisierung Text gehören seitenangaben verfassernam Angabe Rahmen Buchpublikation vorkommen Text codieren Text Sprache chinesisch arabisch einbeziehen Konzeption Markierung Text hineinsetzen annotation versehen Text Originalstruktur verbinden bleiben Markups versehen Text einzig Format zusammengefassen bevorzugen derzeit rdf Manola Miller avisierte Datenmeng auszureichen scheinen unseren bisherig Beobachtung erhalten Text reichhaltig linguistisch Auszeichnung jeder Wort rdf triples literarisch Text wörtern Million Triples ergeben Korpus büchern Milliarde Triples weit innerhalb Rahmen heutig Rdf depot erlauben Untersuchung derzeitig Kapazitätsgrenze Billiarde Triples Hochladezeit weniger Stunde ergeben methode abfrag Analyse Text ermöglichen Anwendung Methode Text generieren Matrix evaluierbar wert Text lassen Vektor wert darstellen darstellungsweise ermöglichen Textvergleiche mittels Clusteranalyse konventionell Komparatistik aufgrund beschränkungen zugänglich Korpusaufbau abfragemethoden gestalten Text umstandslos Korpus hinzufügen Methode problemlos applizieren setzen kontinuierlich Pflege Aktualisierung bestehend Korpus Resp abfrageergebnisse voraus Text hinzufügen angewandt Methode automatisch anwenden Methode hinzufügen automatisch Text entsprechend Evaluierung unterziehen unser Beitrag treten Etablierung computergestützt korpusbasiert forschungsansatz literaturwissenschaftlich Komparatistik darstellen Vorteil Erstellung umfangreich Korpora literarisch Text verschieden Sprache komparatistisch Analyse bieten -- konstruieren pflegen c Abfragemöglichkeit gegenwärtig Stand Technik bieten betracht ziehen sowohl vorhanden Online zugänglich Korpora einzeln Forschergruppe erarbeiten intern genutzt Korpora Austrian Academy Corpus Icltt öaw kursorisch Überblick erprobt Ansatz Computergestützter literaturwissenschaftlich Analyse geben gegenwärtig Spektrum Methode Textevaluierung darstellen zukünftig Desiderata aufzeigen,"[('text', 0.34491418360718984), ('komparatistisch', 0.2884306611000711), ('komparatistik', 0.23665690286668706), ('triples', 0.18670517865760783), ('resp', 0.16481752062861205), ('sprache', 0.15259041041922547), ('markups', 0.13042628961152772), ('auswahl', 0.11929410777434851), ('korpus', 0.11863018372141759), ('eigenschaft', 0.10823874539504966)]"
2016,DHd2016,posters-079.xml,RuCoCo: Automatische Koreferenzannotationen fuÃàr Russisch,"Desislava Zhekova (Centrum für Informations- und Sprachverarbeitung, LMU, München); Alena Mikhaylova (Centrum für Informations- und Sprachverarbeitung, LMU, München)","Koreferenzresolution, Alignierung, Russisch, Deutsch","Sammlung, Programmierung, Strukturanalyse, Beziehungsanalyse, Modellierung, Annotieren, Veröffentlichung, Computer, Daten, Sprache, Software, Text","Koreferenzresolution beschäftigt sich mit der Aufgabe unterschiedliche sprachliche Ausdrücke, die die gleichen Entitäten im Text beschreiben, automatisch zu identifizieren. Die meisten state-of-the-art Koreferenzresolutionssysteme basieren auf statistischen Verfahren und verwenden große vorannotierte Trainingskorpora (Pradhan et al., 2011). Die Abhängigkeit von Trainingskorpora stellt ein Problem für die Sprachen dar, für die keine Korpora verfügbar sind, die mit Koreferenzinformationen annotiert sind (Recasens et al., 2010; Pradhan et al., 2012; Zhekova, 2013). Diese Arbeit beschreibt ein Verfahren zur Gewinnung automatischer Koreferenzannotationen durch parallele Korpora für das Russische. Russisch ist eine der Sprachen, für die es noch keine frei verfügbaren Koreferenzannotationen gibt und die daher nicht mit statistischen Koreferenzsystemen bearbeitet werden können. Unser Ziel ist es, Korpora die mehr als einen Zieltext haben zu benutzen (z.B. mehrere Übersetzungen des gleichen Texts/Quelltexts), um die Koreferenzketten von mehreren Zieltexten in den Quelltext zu projizieren. Unser Vorgehen basiert auf der These, dass die Verwendung mehrerer Zieltexte eine bessere Identifikation der Ketten und Grenzen der Mentions (die potentiell koreferenten Phrasen) ermöglicht. Zusätzlich stellen wir die automatisch gewonnenen Koreferenzannotationen (RuCoCo (aus dem Englischen: Russian Coreference Corpus)) für den russischen Originaltext zur freien Verfügung. Schon oft wurden parallele Korpora für die Gewinnung automatisch generierter Koreferenzannotationen benutzt (Kobdani et al., 2011; Souza and Ora≈°an, 2011; Rahman and Ng, 2012), das Sprachenpaar Russisch-Deutsch wird allerdings wenig bearbeitet (Grishina and Stede, 2015). Darüber hinaus sind parallele Korpora üblicherweise aus einem Quell- und einem Zieltext gebildet (Ganitkevitch et al., 2013; Dolan and Brockett, 2005) und konzentrieren sich hauptsächlich auf die Bearbeitung der englischen Sprache. In Bezug auf das Russische sind uns keine frei verfügbaren Datensätze bekannt, die auch mehrere Zieltexte enthalten. Ein derartiger Korpus für Russisch, der aus dem Roman Aus diesem Grund haben wir einige Ansätze für die Alignierung von Texten für das Sprachenpaar Deutsch-Russisch implementiert (Zhekova et al., 2014), die in das frei verfügbare und interaktive Online Zunächst wird der deutsche Teil mit Koreferenzinformationen versehen, um sie anschließend in den russischen Teil der Paralleltexte projizieren zu können. Dafür wird im ersten Schritt das beste frei verfügbare Koreferenzresolutionssystem (laut die CoNLL Evaluationen), IMSCoref (Björkelund and Farkas, 2012), an die deutsche Sprache angepasst. Wir verwenden die SemEval-Datensätze für Deutsch (Recasens et al., 2010), die in das CoNLL-Format umgewandelt werden. Für die Erzeugung von Syntaxbäumen (ParseBits in CoNLL-Daten), die in den SemEval-Daten fehlen, wird der Als Baseline-Features werden die Features verwendet, die in IMSCoref für Englisch entwickelt wurden. Das ursprüngliche Featureset für Englisch enthält allerdings Informationen, die in den SemEval-Daten nicht zur Verfügung stehen. Entsprechend werden diese Features ausgelassen (z.B. Speaker-, Genre-Features, usw). Agreement-Features für Deutsch sind zusätzlich integriert worden. Tabelle 1: Systemevaluation fuÃàr Englisch (en) und Deutsch (de) mit den original (Orig), reduzierter (Red) und erweiterter (Erw) Featureset. Um die Güte des für Deutsch adaptierten IMSCoref sicherzustellen, wurde das System für beide Sprachen (Englisch und Deutsch) einem Testlauf unterzogen. Dazu haben wir vier Featuresets getestet: Um das IMSCoref auf die Bearbeitung der deutschen UÃàbersetzungen der russisch-deutschen Paralleltexte (Zhekova et al., 2015) (aligniert auf Satzebene) vorzubereiten, werden diese auch in das CoNLL-Format transformiert.  Zuerst werden die Texte mit dem Stanford-Tokenizer tokenisiert. Der TreeTagger (Schmid, 1994; Schmid, 1995) wird danach für das POS-Tagging eingesetzt. Abschließend werden die Texte der Datenaufbereitung mit GIZA++ (Och and Ney, 2003) auf Wortebene aligniert. Wir untersuchen drei verschiedene Ansätze für die Übertragung der Annotationen, die im Folgenden beschrieben sind.  Tabelle 2: Ergebnisse der Projektion von Koreferenzinformationen vom Deutschen ins Russische Da keine Gold-Standard-Annotationen vorliegen, werden vorläufig die ersten 30 Sätze des russischen Textes als ein Dokument im Sinne der Koreferenz betrachtet und manuell (von nur einem Annotator) mit Koreferenzketten annotiert. Die Ergebnisse sind in Tabelle 2 dargestellt, wo alle Übersetzungen (markiert als 1924, 1956, 2010 in der Tabelle) in allen drei Settings (S1, S2 und S3) aufgeführt sind. Die Ergebnisse zeigen, dass durchaus bei allen drei Übersetzungen vergleichbare Zahlen erreicht werden, was innerhalb der drei Settings nicht der Fall ist. Als Erstes zeigt ein Vergleich zwischen der Identifikation der Mentions (IM) für das erweiterte IMSCoref-System im Deutschen ( Eine qualitative Analyse zeigt, dass die meisten Fehler durch falsche Wortalignierungen entstehen. Mentions werden nicht gefunden und übertragen, desweiteren haben die projizierten Mentions oft einen falschen Anfang oder ein falsches Ende und tragen so zu den niedrigen Genauigkeiten bei. Oft führt auch die falsche Alignierung zu einer falschen Identifikation des Kopfes im Russischen, wodurch die Ermittlung der Mentionspan im Setting  3 stark beeinflusst wird. Ein Anteil der Fehler ist auch auf Fehlerprojektion basiert 'falsche Ketten im deutschen Text werden auch falsch weitergegeben. Jedoch ist es unser Hauptziel zu untersuchen, ob Korpora, die mehr als einen Zieltext enthalten, hilfreicher für die Projektion sein können als traditionelle Korpora. Dafür haben wir zwei zusätzliche Experimente durchgeführt (jeweils 1924/1956 und 1924/1956/2010 in Tabelle 2) '1924/1956 fügt die Koreferenzketten aus den beiden Übersetzungen (1924 und 1956) zusammen, während in 1924/1956/2010 die Ketten aus alle drei Übersetzungen zusammengefügt werden (das wird manuell nur für die ersten 30 Sätze des Originalwerks gemacht womit wir entgegen den Testset evaluieren können). Die Ergebnisse zeigen, dass die Benutzung von mehreren Texten sehr hilfreich sein kann, da sich der CoNLL-Score von 20.11% für die Übersetzung 1924 auf 22.41% für 1924/1956/2010 erhöht. Das ist ein sehr positives Ergebnis. Wir vermuten, dass mit einer besseren Alignierung und qualitativ hochwertigeren Dependenzannotation für das Russische, diese Verbesserung noch größer ausfallen würde. In dieser Arbeit haben wir gezeigt, dass parallele Korpora mit mehr als einem Zieltext für die Gewinnung von automatischen Koreferenzannotationen sehr hilfreich sein können, und dass dadurch Sprachen, die bislang für state-of-the-art Koreferenzsysteme völlig unerreichbar waren, damit bearbeitet werden können. Zusätzlich werden die Koreferenzannotationen für den russischen Originaltext und das manuell annotierte Testset zur freien Verfügung",de,Koreferenzresolution beschäftigen Aufgabe unterschiedlich sprachlich Ausdrück gleich Entität Text beschreiben automatisch identifizieren meister Koreferenzresolutionssysteme basieren statistisch Verfahren verwend vorannotierte Trainingskorpora pradhan et Abhängigkeit Trainingskorpora stellen Problem Sprache dar Korpora verfügbar koreferenzinformation annotiert recasens et Pradhan et zhekova Arbeit beschreiben Verfahren Gewinnung automatisch Koreferenzannotation parallel Korpora russisch russisch Sprache frei verfügbar Koreferenzannotation statistisch Koreferenzsysteme bearbeiten Ziel Korpora Zieltext benutzen mehrere übersetzungen gleich texts quelltexts Koreferenzkett mehrere zieltexten quelltext projizieren vorgehen basieren These Verwendung mehrere Zieltexte gut Identifikation Kette Grenze Mention potentiell koreferent Phras ermöglichen zusätzlich stellen automatisch gewonnen Koreferenzannotation Rucoco englisch Russian Coreference Corpus russisch Originaltext frei Verfügung parallel Korpora Gewinnung automatisch generiert Koreferenzannotation benutzen Kobdani et Souza and rahman and ng sprachenpaar bearbeiten Grishina and steden hinaus parallel Korpora üblicherweise Zieltext bilden Ganitkevitch et Dolan and brockett konzentrieren hauptsächlich Bearbeitung englisch Sprache Bezug russisch frei verfügbar datensätze mehrere Zieltexte enthalten derartig Korpus russisch Roman Grund Ansatz Alignierung Text Sprachenpaar implementieren zhekova et frei verfügbar interaktiv Onlin deutsch koreferenzinformation versehen anschließend russisch Paralleltexte projizieren Schritt gut frei verfügbar Koreferenzresolutionssystem laut Conll evaluationen imscoref Björkelund and Farkas deutsch Sprache angepasst verwenden deutsch recasens et umwandeln Erzeugung syntaxbäumen parsebits fehlen Feature verwenden Imscoref englisch entwickeln ursprünglich Featureset Englisch enthalten informationen Verfügung stehen entsprechend Feature auslassen usw deutsch zusätzlich integrieren Tabell Systemevaluation Fuãàr Englisch deutsch de original orig Reduzierter red erweitert erw Featureset Güte deutsch adaptiert Imscoref sicherstellen System Sprache englisch deutsch testlauf unterzogen Featureset testen Imscoref Bearbeitung deutsch Uãàbersetzung Paralleltexte zhekova et aligniert satzeben vorbereiten transformieren Text tokenisieren Treetagger Schmid Schmid einsetzen abschließend Text Datenaufbereitung och and ney Wortebene aligniern untersuchen verschieden Ansatz Übertragung annotatio folgend beschreiben Tabell Ergebnis Projektion Koreferenzinformation deutschen russisch vorliegen vorläufig Sätz russisch Text Dokument Sinn Koreferenz betrachten manuell Annotator Koreferenzkett annotiert Ergebnis Tabelle darstellen übersetzung markieren Tabelle Setting aufführen Ergebnis zeigen übersetzung vergleichbar Zahl erreichen innerhalb Setting Fall zeigen Vergleich Identifikation Mention erweitert deutsch qualitativ Analyse zeigen meister Fehler falsch Wortalignierung entstehen Mention finden übertragen desweiteren projiziert mentions falsch Anfang falsch tragen niedrig Genauigkeit führen falsch Alignierung falsch Identifikation Kopf russisch wodurch Ermittlung Mentionspan Setting stark beeinflussen Anteil Fehler Fehlerprojektion basieren falsch Kette deutsch Text falsch weitergeben Hauptziel untersuchen Korpora Zieltext enthalten hilfreich Projektion traditionell Korpora zusätzlich experiment durchgeführt jeweils Tabelle fügen Koreferenzkett übersetzung Kette übersetzung zusammenfügen manuell Sätz Originalwerk womit entgegen Testset evaluieren Ergebnis zeigen Benutzung mehrere Text hilfreich Übersetzung erhöhen positiv Ergebnis vermuten gut Alignierung qualitativ hochwertiger Dependenzannotation russisch Verbesserung groß ausfallen Arbeit zeigen parallel Korpora Zieltext Gewinnung automatisch Koreferenzannotation hilfreich sprechen bislang koreferenzsystem völlig unerreichbar bearbeiten zusätzlich Koreferenzannotation russisch Originaltext manuell annotiert Testset frei Verfügung,"[('russisch', 0.361260859770016), ('koreferenzannotation', 0.27075018536934886), ('falsch', 0.21587683501945676), ('imscoref', 0.1805001235795659), ('zieltext', 0.16812218109263266), ('deutsch', 0.16158082843212537), ('korpora', 0.1597711975668646), ('englisch', 0.1499034126416826), ('frei', 0.1410126896219305), ('übersetzung', 0.1377591911572545)]"
2016,DHd2016,vortraege-049.xml,Classification of Literary Subgenres,"Lena Hettinger (Universität Würzburg, Deutschland); Isabella Reger (Universität Würzburg, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland); Andreas Hotho (Universität Würzburg, Deutschland)","Genreklassifikation, Topic Modelling","Inhaltsanalyse, Netzwerkanalyse, Stilistische Analyse, Literatur, Text","Literary scholars and common readers use labels like educational novel, crime  novel or adventure novel to organize the large domain of fiction. In both  discourses the use of these categories is well-established even though they are  evolving and tend to be inconsistent. The classification of genres is one of the  standard tasks in document classification and has been researched intensively  (cf. Biber 1989;  Santini 2004; Freund et al 2006; Sharoff et al. 2010). Some  results seem impressive, for example distinguishing clear-cut genres like poetry  from fiction (Underwood 2014), but most texts on literary genre classification  emphasize, as the literature on genre classification in general, the variability  of genre signals (Allison et al. 2011: 19; Underwood et al. 2013; Underwood  2014). The scores for genre classification over all categories are therefore  often not very high. Jockers for example reports an accuracy of 67% (Jockers  2013: 81). Genre classification in general works best with most frequent words,  all words or character tetragrams (Freund et al. 2006; Sharoff et al. 2010) and  most of the reported experiments for literary genre classification also use all  words or only the In the following we will describe the corpus and the features we use for the task of subgenre classification. Our corpus consists of 628 German novels mainly from the 19th century  (roughly 1745 to 1935) obtained from sources like the As mentioned in section 1 we use three types of features (stylometric, topic based and network) that are described in more detail in Hettinger et al. (2015). Features are extracted and normalized to a range of [0,1] based on the whole corpus consisting of 628 novels. We use word frequencies as well as character tetragrams to represent stylometric features. We tested different amounts of most frequent words and decided to work with the top 3000 (mfw3000). Additionally we use the top 1000 character tetragrams (4gram). We use Latent Dirichlet Allocation (LDA) by Blei et al. (2003) to extract topics from our data. In literary texts topics sometimes represent themes, but more often they represent topoi, often used ways of telling a story or parts of it. For each novel we derive a topic distribution, i.e. we calculate how strongly each topic is associated with each novel. We try different preprocessing approaches and topic numbers and build ten models for each setting to reduce the influence of randomness in LDA models. In every setting we first remove a set of predefined stop words from the novels and then use LDA on our corpus of 628 novels. The different forms of preprocessing we use are: We use the character recognition system described in Jannidis et al. (2015) to identify the characters of each novel. Although the NER tool may be employed with co-reference resolution we do not make use of this option here. We extract proper names to build a network where each node is a character and the number of co-occurrences of two characters in the same paragraph is the weight of the edge between these two. The network of each novel is reduced to the most central characters and the most frequent interactions in order to bring out their basic shape. The network feature set consists of the total number of characters in a novel and six network measures: maximum degree centrality, global efficiency, transitivity, average clustering coefficient, central point dominance and density. Classification is done by means of a linear Support Vector Machine (SVM) as we have already shown in Hettinger et al. (2015) that it works best in this setting (see also Yu 2008). In each experiment we apply 100 iterations of 10-fold cross validations to account for the small data sets. The depicted results are the average over 1000 classification accuracy values. We want to investigate the following subgenre constellations: Depending on the setting the label distribution is often imbalanced. To make results comparable we use undersampling where in each of the 100 iterations a new sample is drawn from the larger class while all instances of the smaller class are used. This accounts for a majority vote (MV) baseline that always yields an accuracy score of 0.50 as both classes have equal size. To determine the influence of the LDA topic parameter a) adventure/non-adventure b) educational/non-educational c) social/non-social d) adventure/educational e) educational/social  When comparing different feature sets across our subgenre constellations we can  see that semantic based features (mfw, 4grams, lda) all perform quite good while  network features perform rather poorly (see figure 2). With an accuracy score of  more than 90% adventure novels seem to be fairly easy to differentiate from  other genres. In contrast, the other genres don""t show such a distinct signal  using surface features. As the classification performance of adventure/educational is quite impressive we  take a closer look at the discriminating words of these genres (Figure 3). Some  of the most typical words of adventure novels include To test whether authorship of novels influences our results we removed the author signal by allowing only one document per author. In this way, we construct a new dataset called ""uni"". The sampling is done once so that the same novels are used in each setting. As shown in figure 4, we observe a much lower quality after removing the authorship information indicating an overemphasized focus of features and models on the hidden authorship signal. This varies for different settings as adventure/educational shows a loss of 0.09 (blue lines) and educational/social loses 0.23 (red lines). The relatively small loss in the first setting is remarkable as it contains 8 novels per author on average. One would expect the opposite given the weaker author signal of two novels per author for the other categories. In the next experiment we test whether the combination of feature sets changes our classification results. To balance the size of the feature sets we use Principal Component Analysis (PCA) and construct 100 features from the 3000 mfw and 1000 4gram features each. As shown in figure 5 some feature sets improve when combined (e.g. 4gram100 and lda100) and for others (e.g. lda100 and network) performance decreases. But classification results vary greatly in this setting as signaled by standard deviation bars so these differences should not be overrated. In this work we classified subgenres of German novels using different feature sets (mfw 3000, 4gram, lda etc.). Some subgenres, like adventure novels, are much easier to classify than others. Most of the applied feature sets showed a varying but comparable performance except the network features. The weak performance of network features might be caused by the weak link between the novel genre and character constellation. The variability of the subgenre signal could not be countered by using higher level features like topics and network characteristics. Interestingly, the author signal has a strong influence on the classification quality. The strength of influence seems to depend on category but is visible in all experiments. In the future, we would like to extend our work by using different network features, work on advanced topic models and find a reliable indicator for plot. Another challenge we have not faced yet is the development of subgenres over time.",en,literary scholar common reader use label like educational novel crime novel adventure novel organize large domain fiction discourse use category establish evolve tend inconsistent classification genre standard task document classification research intensively cf biber santini freund et al sharoff et al result impressive example distinguish clear cut genre like poetry fiction underwood text literary genre classification emphasize literature genre classification general variability genre signal allison et al underwood et al underwood score genre classification category high jocker example report accuracy jocker genre classification general work well frequent word word character tetragram freund et al sharoff et al report experiment literary genre classification use word following describe corpus feature use task subgenre classification corpus consist german novel mainly century roughly obtain source like mention section use type feature stylometric topic base network describe detail hettinger et al feature extract normalize range base corpus consist novel use word frequency character tetragram represent stylometric feature test different amount frequent word decide work additionally use character tetragram use latent dirichlet allocation lda blei et al extract topic datum literary text topic represent theme represent topoi way tell story part novel derive topic distribution calculate strongly topic associate novel try different preprocessing approach topic number build model setting reduce influence randomness lda model setting remove set predefine stop word novel use lda corpus novel different form preprocessing use use character recognition system describe jannidis et al identify character novel ner tool employ co reference resolution use option extract proper name build network node character number co occurrence character paragraph weight edge network novel reduce central character frequent interaction order bring basic shape network feature set consist total number character novel network measure maximum degree centrality global efficiency transitivity average clustering coefficient central point dominance density classification mean linear support vector machine svm show hettinger et al work good setting yu experiment apply iteration fold cross validation account small datum set depict result average classification accuracy value want investigate follow subgenre constellation depend set label distribution imbalance result comparable use undersample iteration new sample draw large class instance small class account majority vote mv baseline yield accuracy score class equal size determine influence lda topic parameter adventure non adventure b educational non educational c social non social d adventure educational e educational social compare different feature set subgenre constellation semantic base feature mfw lda perform good network feature perform poorly figure accuracy score adventure novel fairly easy differentiate genre contrast genre distinct signal surface feature classification performance adventure educational impressive close look discriminate word genre figure typical word adventure novel include test authorship novel influence result remove author signal allow document author way construct new dataset call uni sampling novel setting show figure observe low quality remove authorship information indicate overemphasize focus feature model hidden authorship signal vary different setting adventure educational show loss blue line educational social lose red line relatively small loss setting remarkable contain novel author average expect opposite give weak author signal novel author category experiment test combination feature set change classification result balance size feature set use principal component analysis pca construct feature mfw feature show figure feature set improve combine network performance decrease classification result vary greatly setting signal standard deviation bar difference overrate work classify subgenre german novel different feature set mfw lda etc subgenre like adventure novel easy classify apply feature set show varying comparable performance network feature weak performance network feature cause weak link novel genre character constellation variability subgenre signal counter high level feature like topic network characteristic interestingly author signal strong influence classification quality strength influence depend category visible experiment future like extend work different network feature work advanced topic model find reliable indicator plot challenge face development subgenre time,"[('novel', 0.3536459545404899), ('feature', 0.29732885745490406), ('classification', 0.25252884576736534), ('adventure', 0.22124959761111704), ('use', 0.21965121031618015), ('educational', 0.1966663089876596), ('network', 0.1882199328095292), ('character', 0.163437886037311), ('al', 0.16114822849636412), ('signal', 0.15743395409432487)]"
2016,DHd2016,vortraege-060.xml,Dramen als small worlds? Netzwerkdaten zur Geschichte und Typologie deutschsprachiger Dramen 1730-1930,"Peer Trilcke (Georg-August-Universität Göttingen, Deutschland); Frank Fischer (Niedersächsische Staats- und Universitätsbibliothek Göttingen); Mathias Göbel (Niedersächsische Staats- und Universitätsbibliothek Göttingen); Dario Kampkaspar (Herzog August Bibliothek Wolfenbüttel)","Netzwerkanalyse, Dramen, Literaturgeschichte, small worlds","Strukturanalyse, Netzwerkanalyse, Visualisierung, Literatur, Text","Neben dem ""klassischen"" strukturalistischen Paradigma, das sich wesentlich an  Theoremen der Linguistik orientiert (u. a. Lotman 1972; Titzmann 1977), gibt es  in der Literaturwissenschaft bereits seit Jahrzehnten Ansätze zu einer  Strukturanalyse, die sich auf die empirische Soziologie 'insbesondere auf die Unser derzeitiges Korpus umfasst 465 deutschsprachige Dramen (Zeitraum 1730 bis  1930), die aus dem Die diachrone Erstreckung unseres Dramenkorpus über ca. 200 Jahre deutscher  Literaturgeschichte macht es möglich, größere Entwicklungen im Bereich der  strukturellen Komposition von dramatischen Texten zu beobachten (erste  Überlegungen dazu haben wir in einem Blogpost skizziert:  Die von uns bisher erhobenen Werte zeigen, dass Dramen in dem untersuchten Zeitraum auf sehr unterschiedliche Weise strukturiert wurden. In der ""traditionellen"" Literaturwissenschaft wurden für solche unterschiedlichen ""Bauformen"" diverse Typologien entwickelt, in der Germanistik am bekanntesten ist Volker Klotz"" Unterscheidung in eine ""offene"" und eine ""geschlossen"" Dramenform (Klotz 1960). Diesen typologischen Impuls wollen wir aufgreifen und einen Vorschlag unterbreiten, wie sich mittels netzwerkanalytischer Daten bestimmte Typen der strukturellen Komposition von Dramen unterscheiden (und dann wiederum historisch verorten) lassen. Unser Vorschlag greift dabei Überlegungen aus der Forschung zu sog. Small-world-Netzwerken auf. Diese Forschungen setzen bei der Beobachtung an, dass die Werte von empirisch erhobenen Netzwerken nicht selten signifikant von entsprechenden Random-Netzwerken (also z. B. nach dem Erd≈ës-Rényi-Modell erstellten Graphen) abweichen. Abweichungen sind dabei insbesondere beim Clustering Coefficient, bei der Averge Path Length sowie bei der Degree Distribution zu beobachten (Albert / Barab√°si 2002). Für den hier projektierten Vortrag werden wir diese Werte 'sowie die Werte für die entsprechenden Random-Netzwerke 'für unser Gesamtkorpus erheben (sowie einen Workflow für die automatisierte Erhebung entwickeln) und diskutieren. Erste Testläufe deuten dabei darauf hin, dass sich auf diese Weise tatsächlich unterschiedliche Typen der strukturellen Komposition von Dramen beschreiben lassen könnten. So zeigen sich z. B. auffällige Unterschiede bei der Degree Distribution (s. exemplarisch die Tabellen für vier Dramen in Abbildung 2); und mit Blick auf den Clustering Coefficient zeigt sich, dass im Vergleich zu Random-Netzwerken signifikant höhere Werte, wie sie bei Small-world-Netzwerken zu erwarten sind, zwar in mehreren Fällen vorkommen, jedoch keineswegs für alle Dramennetzwerke charakteristisch sind (siehe exemplarisch die Werte in Abbildung 3). Im Vortrag werden wir diese Werte für alle Dramen unseres Korpus präsentieren; wir werden diskutieren, inwieweit sich hier 'aufbauend auf dem Small-world-Konzept 'netzwerkanalytisch basierte Typen der strukturellen Komposition von Dramen unterscheiden lassen und wir werden literarhistorisch fundiert erörtern, welche Eigenschaften der Dramen für die unterschiedlichen Werte verantwortlich sind. ",de,klassisch strukturalistisch Paradigma wesentlich Theorem Linguistik orientieren Lotman Titzmann Literaturwissenschaft Jahrzehnt Ansatz Strukturanalyse empirisch Soziologie insbesondere derzeitig Korpus umfassen deutschsprachig Dram Zeitraum Diachrone Erstreckung unser dramenkorpus deutsch Literaturgeschichte groß Entwicklung Bereich strukturell Komposition dramatisch Text beobachten Überlegung Blogpost skizzieren erhoben Wert zeigen dramen untersucht Zeitraum unterschiedlich Weise strukturiern traditionell Literaturwissenschaft unterschiedlich bauformen diverser Typologi entwickeln Germanistik bekannt Volker Klotz Unterscheidung offen schließen Dramenform klotz typologisch impuls aufgreifen Vorschlag unterbreiten mittels netzwerkanalytisch daten bestimmt Typ strukturell Komposition Dram unterscheiden wiederum historisch verorten lassen Vorschlag greifen Überlegung Forschung Forschung setzen Beobachtung Wert empirisch erhoben netzwerken selten signifikant entsprechend erstellt graph abweichen Abweichung insbesondere Clustering Coefficient Averge Path Length Degree Distribution beobachten Albert si projektiert Vortrag wert Wert entsprechend Gesamtkorpus erheben Workflow automatisiert Erhebung entwickeln diskutieren Testläuf deuten Weise tatsächlich unterschiedlich Typ strukturell Komposition Dram beschreiben lassen können zeigen auffällig Unterschied Degree Distribution exemplarisch Tabell Dram Abbildung Blick Clustering Coefficient zeigen Vergleich Signifikant hoch Wert erwarten mehrere Fall vorkommen keineswegs dramennetzwerk charakteristisch sehen exemplarisch wert Abbildung Vortrag Wert Dram unser Korpus präsentieren diskutieren inwieweit aufbauend netzwerkanalytisch basiert Typ strukturell Komposition Dram unterscheiden lassen literarhistorisch fundieren erörtern eigenschaften dramen unterschiedlich Wert verantwortlich,"[('komposition', 0.34662316171638796), ('wert', 0.3182086421443274), ('dram', 0.2761246045872245), ('strukturell', 0.177178163383291), ('klotz', 0.17331158085819398), ('coefficient', 0.1614265985233394), ('typ', 0.15658219891921518), ('degree', 0.14110908702439295), ('erhoben', 0.12613578009059317), ('netzwerkanalytisch', 0.12334205066565819)]"
2016,DHd2016,posters-073.xml,"Kollaboratives Schreiben gestern, heute und morgen: Nutzen und Grenzen eines Visualisierungs- und Analysemodels aus der digitalen Literaturforschung","Heiko Zimmermann (Universität Trier, Deutschland)","digitale Literatur, kollaboratives Schreiben, Autorschaft, Visualisierung, Modellbildung","Entdeckung, Modellierung, Theoretisierung, Community-Bildung, Bewertung, Visualisierung, Computer, Interaktion, Literatur, Multimedia, Multimodale Kommunikation, Personen, Werkzeuge, Visualisierung","Sieht man vom gemeinschaftlichen Schreiben aktueller 'Wirklichkeiten' in Facebook ab, ist Novalis' Prophetie weit entfernt von kreativen Schreibprozessen der Gegenwart. Dennoch hat es unterschiedlichste Formen kollaborativen Schreibens seit jeher gegeben. Dieses Schreiben hat die Literaturkritik und -wissenschaft oft vor Probleme gestellt (vgl. Ede / Lunsford 1990). In den letzten Jahren haben die Möglichkeiten des vernetzten Schreibens am Computer neue Formen und Dimensionen kollaborativer Schreibenprozesse gefördert. Werke wie die Enzyklopädie Zur selben Zeit ist in der englischsprachigen Welt das Genre der digitalen Literatur  aufgekommen, welches die Literaturwissenschaft ebenfalls vor große Herausforderungen  stellt. Ein Hauptproblem ist das der Rekonfigurationen von Autor- und Leserschaft,  das mittels poststrukturalistischer Metaphern (Landow 2006; Simanowski 2002; Winko  1999) nicht hinreichend beschrieben werden konnte. Auch Zwischenwesen wie das Modell  des Wreaders, also des schreibenden Lesers, konnten die Abweichungen von tradierten  Rollen in der Literaturproduktion und -rezeption nicht sinnvoll modellieren. Aus den  selben Gründen funktionieren auch buchgeschichtliche Modelle des Literaturmarktes  wie das von Robert Darnton (1982) nur bedingt, um die Wirklichkeit digitaler  Literatur zu beschreiben. Um das Problem der Autor- und Leserschaft und unzureichender tradierter  Modellierungen zu lösen, wurde das visuelle Beschreibungs- und Analysemodell des  textuellen Handlungsraums entwickelt (Zimmermann 2015a). Es basiert auf dem  Texton-Skripton-Modell von Espen Aarseth (1997: 62-65) und ordnet allen am Text  handelnden Akteuren einen eindeutigen Platz im Handlungsraum zu, der abhängig ist  von der Art und Weise und vom Zeitpunkt ihres Handelns am Text im Kontinuum von  Produktion und Rezeption (vgl. Abbildung 1). Anwendungen dieses Modells waren bisher  auf englische digitale Literatur beschränkt und haben in diesem Feld ergeben, dass  es bestimmte Konstellationen von Handelnden in der Literaturproduktion und  -rezeption, beispielsweise Foucaults Idee einer beherrschenden Stellung der  Autorfunktion in literarischen Diskursen, in Frage stellt (Zimmermann 2015b). Der vorgeschlagene Vortrag soll gleichsam mehrere Brücken zwischen akademischen Disziplinen und literarischen Traditionen schlagen. Das Modell des textuellen Handlungsraums, das aus dem Feld der elektronischen Literaturforschung - und damit aus einem Kerngebiet der digitalen Geisteswissenschaften, sofern diese nicht allein über Methoden und Werkzeuge definiert werden - stammt, soll nicht nur auf digitale englischsprachige Literatur angewendet werden, sondern auch auf nicht-digitale deutsche und englischsprachige literarische Texte der Gegenwart und des 20. Jahrhunderts. Damit werden Verbindungen zwischen verschiedenen Literaturen (zeitlich, sprachlich) und akademischen Feldern (digitale Literaturforschung, digitale Geisteswissenschaften, traditionelle Literaturwissenschaft) hergestellt. Nachdem das Modell im Vortrag kurz vorgestellt wurde, fragt dieser nach den Formen von Autor- und Leserschaft ausgewählter kollaborativ geschriebener Texte, nach Möglichkeiten solches Schreiben sinnvoll zu klassifizieren und danach, ob sich Rückschlüsse auf die (kommerzielle) Verwertbarkeit ebendieser Literatur ziehen lassen. Flankierend wird damit eine Fallstudie für die Permeabilität traditioneller Literaturanalyse für Modelle aus dem Bereich der digitalen Literaturwissenschaft vorgestellt, und es werden die Potentiale und Grenzen einer derartigen Visualisierung literarischen Schaffens aufgezeigt.",de,sehen gemeinschaftlich Schreiben aktuell wirklichkeiten Facebook Novalis Prophetie entfernt kreativ Schreibprozesse Gegenwart dennoch unterschiedlich Form kollaborativ Schreiben jeher geben schreiben Literaturkritik Problem stellen ede Lunsford letzter Möglichkeit vernetzt Schreiben Computer Form dimension kollaborativ Schreibenprozesse fördern Werk enzyklopädie selber englischsprachig Welt Genre digital Literatur aufkommen Literaturwissenschaft ebenfalls Herausforderung stellen Hauptproblem rekonfigurationen Leserschaft mittels poststrukturalistisch Metapher landow Simanowski Winko hinreichend beschreiben zwischenwesen Modell Wreader schreibend lesers Abweichung Tradiert Rolle Literaturproduktion sinnvoll modellieren selber Grund funktionieren buchgeschichtlich Modell literaturmarkter Robert Darnton bedingt Wirklichkeit Digitaler Literatur beschreiben Problem Leserschaft unzureichend Tradierter Modellierunge lösen visuell analysemodell textuell handlungsraums entwickeln zimmermann basieren espen Aarseth ordnen Text handelnd akteuren eindeutig Platz Handlungsraum abhängig Art Weise Zeitpunkt handelns Text Kontinuum Produktion Rezeption Abbildung anwendungen Modell englisch digital Literatur beschränken Feld ergeben bestimmt Konstellatione Handelnde Literaturproduktion beispielsweise Foucault Idee beherrschend Stellung Autorfunktion literarisch diskursen Frage stellen zimmermann vorgeschlagen Vortrag gleichsam mehrere Brücke akademisch disziplin literarisch Tradition schlagen Modell textuell handlungsraums Feld elektronisch Literaturforschung Kerngebiet digital geisteswissenschaften sofern Methode Werkzeug definieren stammen digital englischsprachig Literatur anwenden deutsch englischsprachig literarisch Text Gegenwart Jahrhundert Verbindung verschieden Literature zeitlich sprachlich akademisch feldern digital Literaturforschung digital geisteswissenschaften traditionell Literaturwissenschaft herstellen Modell Vortrag vorstellen fragen Form Leserschaft ausgewählt kollaborativ geschrieben Text Möglichkeit Schreiben sinnvoll klassifizieren rückschluß kommerziell verwertbarkeit ebendies Literatur ziehen lassen flankierend Fallstudie Permeabilität traditionell Literaturanalyse Modell Bereich digital Literaturwissenschaft vorstellen potential Grenze derartig Visualisierung literarisch Schaffen aufzeigen,"[('leserschaft', 0.24500389581249296), ('schreiben', 0.2260812246702512), ('modell', 0.20023678660192373), ('zimmermann', 0.16333593054166198), ('handlungsraums', 0.16333593054166198), ('literatur', 0.15444711538032305), ('literaturforschung', 0.1521350365245274), ('literaturproduktion', 0.1521350365245274), ('englischsprachig', 0.15171665481353835), ('kollaborativ', 0.14959120403095938)]"
2016,DHd2016,sektionen-002.xml,"""Delta"" in der stilometrischen Autorschaftsattribution","Stefan Evert (Universität Erlangen-Nürnberg, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland); Friedrich Michael Dimpel (Universität Erlangen-Nürnberg, Deutschland); Christof Schöch (Universität Würzburg, Deutschland); Steffen Pielström (Universität Würzburg, Deutschland); Thorsten Vitt (Universität Würzburg, Deutschland); Isabella Reger (Universität Würzburg, Deutschland); Andreas Büttner (Universität Würzburg, Deutschland); Thomas Proisl (Universität Erlangen-Nürnberg, Deutschland)","Stilometrie, Autorschaftsattribution, Delta","Stilistische Analyse, Literatur, Text","Stilometrische Verfahren der Autorschaftsattribution haben eine lange Tradition in den digitalen Geisteswissenschaften: Mit der Analyse der Ein jüngerer Meilenstein der stilometrischen Autorschaftsattribution ist ohne Zweifel das von John Burrows (2002) vorgeschlagene ""Delta""-Maß zur Bestimmung der stilistischen Öhnlichkeit zwischen Texten. Die beeindruckend gute Performance von Delta in verschiedenen Sprachen und Gattungen sollte allerdings nicht darüber hinwegtäuschen, dass die theoretischen Hintergründe weitgehend unverstanden geblieben sind (Argamon 2008). Anders ausgedrückt: Wir wissen, dass Delta funktioniert, aber nicht, warum es funktioniert. In diesem Kontext möchte die hier vorgeschlagene Sektion den aktuellen Stand der Forschung in der stilometrischen Autorschaftsattribution mit Delta vorstellen und neueste Entwicklungen anhand konkreter, eigener Untersuchungen demonstrieren. Jeder der drei Vorträge der Sektion leistet hierzu einen Beitrag: Die drei Beiträge demonstrieren auf diese Weise verschiedene aktuelle Entwicklungen in der stilometrischen Autorschaftsattribution mit Delta und seinen Varianten. Sie zeigen, wie bei der Anwendung stilometrischer Distanzmaße auf ganz unterschiedliche Gegenstandsbereiche ähnliche methodische Fragen zu berücksichtigen sind. Und sie partizipieren direkt an aktuellsten, internationalen Entwicklungen bei der Verwendung von Distanzmaßen wie Delta für die stilometrische Autorschaftsattribution. Burrows Delta ist einer der erfolgreichsten Algorithmen der Computational Stylistics (Burrows 2002). In einer ganzen Reihe von Studien wurde seine Brauchbarkeit nachgewiesen (z. B. Hoover 2004, Rybicki / Eder 2011). Im ersten Schritt bei der Berechnung von Delta werden in einer nach Häufigkeit sortierten Token-Dokument-Matrix alle Werte normalisiert, indem ihre relative Häufigkeit im Dokument berechnet wird, um Textlängenunterschiede auszugleichen. Im zweiten Schritt werden alle Werte durch eine z-Transformation standardisiert:  wobei Trotz seiner Einfachheit und seiner praktischen Nützlichkeit mangelt es bislang allerdings an einer Erklärung für die Funktionsweise des Algorithmus. Argamon (2008) zeigt, dass der dritte Schritt in Burrows Delta sich als Berechnung des Smith und Aldrige (2011) schlagen vor, wie im Information Retrieval üblich   (Baeza-Yates / Ribeiro-Neto 1999: 27), den Cosinus des Winkels zwischen den   Dokumentenverktoren zu verwenden. Die Cosinus-Variante von Delta übertrifft   Burrows Delta fast immer an Leistungsfähigkeit und weist, im Gegensatz zu   den anderen Varianten, auch bei der Verwendung sehr vieler MFWs keine   Verschlechterung auf (Jannidis et. al. 2015). Es stellt sich die Frage,   warum Delta Entscheidend für unsere weitere Analyse war die Erkenntnis, dass man die Verwendung des Cosinus-Abstands als eine Vektor-Normalisierung verstehen kann, da für die Berechnung des Winkels 'anders als bei Manhattan- und Euklidischem Abstand 'die Länge der Vektoren keine Rolle spielt (vgl. Abb. 1). Experimente haben gezeigt, dass eine explizite Vektor-Normalisierung auch die Ergebnisse der anderen Deltamaße erheblich verbessert und Leistungsunterschiede zwischen den Delta-Varianten weitgehend neutralisiert (Evert et al. 2015). Daraus wurden zwei Hypothesen abgeleitet: Für die hier präsentierten Untersuchungen verwenden wir drei vergleichbar  aufgebaute Korpora in Deutsch, Englisch und Französisch. Jedes Korpus  enthält je 3 Romane von 25 verschiedenen Autoren, insgesamt also jeweils  75 Texte. Die deutschen Romane aus dem 19. und dem Anfang des 20.  Jahrhunderts stammen aus der Digitalen Bibliothek von Um die Rolle von Ausreißern und damit die Plausibilität von H1 näher zu untersuchen, ergänzen wir Delta  Wir bezeichnen diese Abstandsmaße allgemein als L Abbildung 2 vergleicht vier unterschiedliche L Eine Vektor-Normalisierung verbessert die Qualität aller Delta-Maße  erheblich (vgl. Abb. 3). Argamons Delta Ein anderer Ansatz zur Abmilderung von Ausreißern besteht darin, besonders extreme Insgesamt erweist sich Hypothese H1 somit als nicht haltbar. H2 wird durch das gute Ergebnis der Vektor-Normalisierung unterstützt, kann aber nicht unmittelbar erklären, warum auch das Abschneiden von Ausreißern zu einer deutlichen Verbesserung führt. Um diese Hypothese weiter zu untersuchen, wurden reine ""Schlüsselprofil""-Vektoren erstellt, die nur noch zwischen überdurchschnittlicher (+1), unauffälliger (0) und unterdurchschnittlicher (–1) Häufigkeit der Wörter unterscheiden (vgl. Abb. 4, rechts unten). Abbildung 6 zeigt, dass solche Profil-Vektoren hervorragende Ergebnisse erzielen, die der Vektor-Normalisierung praktisch ebenbürtig sind. Selbst das besonders anfällige L H1, die Ausreißerhypothese, konnte widerlegt werden, da die Vektor-Normalisierung die Anzahl von Extremwerten kaum verringert und dennoch die Qualität aller L Burrows"" Delta (Burrows 2002) hat sich in Autorschaftsfragen etabliert; viele Studien zeigen, dass Delta für germanische Sprachen ausgezeichnet funktioniert (Hoover 2004b; Eder / Rybicki 2011; Eder 2013a; Eder 2013b; für das Neuhochdeutsche zuletzt Jannidis¬†/ Lauer 2014; Evert et al. 2015). Beim Mittelhochdeutschen ist jedoch die Schreibung nicht normiert: Das Wort ""und"" kann als ""unde"", ""unt"" oder ""vnt"" verschriftet sein. Ein Teil dieser Varianz wird zwar in normalisierten Ausgaben ausgeglichen, jedoch nicht vollständig. Viehhauser (2015) hat in einer ersten Delta-Studie zum Mittelhochdeutschen diese Probleme diskutiert: Wolfram von Eschenbach benutzt zum Wort ""kommen"" die Präteritalform ""kom"", Hartmann von Aue verwendet ""kam"", eine Form, die eher in den südwestdeutschen Raum gehört. Die Bedingungen für den Einsatz von Delta auf der Basis der Normalisierte Texte sind besser für Autorschaftsstudien geeignet, da hier die Zufälligkeiten von Schreibergraphien reduziert sind; Längenzeichen stellen dort meist weitere lexikalische Informationen zur Verfügung 'etwa zur Differenzierung von ""sin"" (""Sinn"") versus ""s√Æn"" (""sein""; allerdings ohne Disambiguierung von ""s√Æn"" als verbum substantivum oder Pronomen). In diplomatischen Transkriptionen sind dagegen etwa ""u-e"" Superskripte und andere diakritische Zeichen enthalten; die gleiche Flexionsform des gleichen Wortes kann in verschiedenen Graphien erscheinen. Anlass zu vorsichtigem Optimismus bietet allerdings eine Studie von Eder (2013a), die den Einfluss von Noise (wie z. B. Schreibervarianten) analysiert 'mit dem Ergebnis (u. a.) für das Neuhochdeutsche, dass ein zufälliger Buchstabentausch von 12% bei 100-400 MFWs die Ergebnisse kaum beeinträchtigt; bei einer mäßig randomisierten Manipulation der MFWs-Frequenzen verschlechtert sich die Quote der korrekten Attributionen bei 200-400 MFWs ebenfalls kaum. Ersetzt man im Autortext Passagen durch zufällig gewählte Passagen anderer Autoren, ergibt sich bei der Quote lediglich ein ""gentle decrease of performance""; im Lateinischen bleibt die Quote gut, selbst nachdem 40% des Originalvokabulars ausgetauscht wurden. Während die 17 Texte, die Viehhauser analysiert hat, in normalisierten Ausgaben vorliegen, habe ich zunächst 37 heterogene Texte von sieben Autoren mit Stylo-R (Eder / Kestemont et al. 2015) getestet sowie drei Texte mit fraglicher Autorzuschreibung zu Konrad von Würzburg. Ein Teil ist normalisiert (Hartmann, Wolfram, Gottfried, Ulrich, Wirnt, Konrad), andere liegen zum Teil in diplomatischen Transkriptionen vor: Bei Rudolf von Ems sind ""Gerhard"", ""Alexander"" und ""Barlaam"" normalisiert, nicht normalisiert sind ""Willehalm"" und ""Weltchronik"" (hier etwa ""ubir"" statt ""über""). Beim Stricker ist lediglich der ""Pfaffe Amis"" normalisiert. Per Skript wurden Längenzeichen eliminiert, damit nicht Texte mit und ohne Längenzeichen auseinander sortiert werden. Tustep-Kodierungen etwa für Superskripte habe ich in konventionelle Buchstaben transformiert. Dennoch bleiben große Unterschiede: Die Genitivform zu ""Gott"" lautet teils ""gotes"", teils ""gotis"", so dass eigentlich eine primäre Sortierung entlang der Unterscheidung normalisiert–nicht-normalisiert zu erwarten wäre. Das Ergebnis ist jedoch frappierend: Auf der Basis von 200 MFWs (diesen Parameter verwenden auch Eder 2013b und Viehhauser 2015) gelingt stylo-R ohne Pronomina und bei Culling=50% eine fehlerfreie Sortierung nach Autorschaft; Delta ordnet Rudolf zu Rudolf 'ob normalisiert oder nicht. Dieser Befund ist Anlass für eine Serie an automatisierten Tests in Anlehnung an Eder (2013b): Bei welchem Vektor und ab welcher Textlänge liefert Delta zuverlässige Ergebnisse? Wie wirkt sich das Einbringen von Noise aus? Per Perlskript wurde ein Delta-Test implementiert, der in einer großen Zahl an Iterationen (13.425 Delta-Berechnungen) verschiedene ""Ratetexte"" mit bekannter Autorschaft gegen ein Validierungskorpus mit bekannter Autorschaft jeweils daraufhin prüft, ob für jeden Text im Ratekorpus tatsächlich der niedrigste Delta-Wert bei einem Text des gleichen Autors im Validierungskorpus herauskommt. Gegen ein heterogenes Validierungskorpus mit 18 Texten wurden 19 normalisierte Ratetexte getestet; gegen ein heterogenes Validierungskorpus mit 15 Texten wurden 13 nicht-normalisierte Ratetexte getestet. Ermittelt wurde der Prozentsatz der richtig erkannten Autoren für jeweils eine Vektorlänge; die Vektorlänge wurde in 100er Schritten bis auf 2.500 MFWs erhöht. Pronomina wurden beseitigt. Bei den normalisierten Ratetexten ist die Erkennungsquote sehr gut bis 200–900 MFWs, bei den nicht-normalisierten sehr gut für 100–600 MFWs. Interessante Fehlattributionen 'etwa Strickers ""Pfaffe Amis"" und Konrads ""Herzmäre"" 'machen weitere Validierungsläufe nötig: Der normalisierte ""Pfaffe Amis"" wurde gegen einen nicht-normalisierten Stricker-Text getestet; das ""Herzmäre"" ist kurz (2991 Wörter). Während Burrows (2002) davon ausgeht, dass Delta ab einer Textlänge von 1.500 Wörtern anwendbar ist, zeigt Eder (2013b), dass Delta im Englischen ab 5.000 Wörtern sehr gute und unter 3.000 Wörtern teils desaströse Ergebnisse liefert; nur im Lateinischen werden ab 2.500 Wörtern gute Ergebnisse erreicht. Hier wurde die Textlänge linear begrenzt, die Texte wurden nach 1000, 2000 Wörtern usw. abgeschnitten. Das Korpus ist kleiner als zuvor, da zu kurze Texte herausgenommen wurden (normalisierte: 16 Texte Validierungskorpus, 15 Ratekorpus; nicht-normalisierte 14 Validierungskorpus, 6-7 Ratekorpus; 10.056 Delta-Berechnungen). Gleiches Korpus wie zuvor; 167.600 Delta-Berechnungen. Da die bag-of-words randomisiert zusammengestellt wird, schwankt die Erkennungsquote etwas, daher wurde jeder Test pro Textlänge und Wortlistenlänge 25x durchgeführt und der Mittelwert dieser 25 Erfolgsquoten verwendet. Aus einer Noise-Datei mit >18.000 mittelhochdeutschen und altfranzösischen Wortformen ohne Duplikate werden die Ratetexte prozentual aufsteigend randomisiert: Teile der bag-of-words werden gegen fremdes Sprachmaterial ausgetauscht, um Fehler in der √úberlieferungskette zu simulieren. Die Kurve verläuft nicht konstant linear, da für jede bag-of-words-Berechnung erneut Noise randomisiert hinzugefügt wird (hier 10 Iterationen pro Einzelwert; 1.179.360 Delta-Berechnungen). Beim Test der Vektorlänge (vgl. 3.2.1.) bleiben die Erkennungsquoten bei normalisierten Ratetexten sehr gut bis 200–900 MFWs. Bei den nicht-normalisierten Texten sind die Quoten nur für einen kleineren Bereich sehr gut: für 100–600 MFWs. Bei einer Begrenzung der Textlänge (Cutoff; vgl. 3.2.2.) bleiben die Ergebnisse bei normalisierten Texten nur ab 4000 Wörtern Textlänge weitgehend gut bis sehr gut. Schlecht sieht es bei den nicht-normalisierten Texten aus: Sehr gut ist die Quote nur bei 800 MFWs und 5000 Wörtern, ansonsten weithin desaströs. Bag-of-words (vgl. 3.2.3.) bieten dagegen stabilere Ergebnisse: Bei den normalisierten Texten sind bei einer Textlänge von 5000 die Quoten sehr gut bei 400-800 MFWs. Bei den nicht-normalisierten Texten ist die Quote wiederum nur bei Textlänge 5000 und 800 MFWs sehr gut. Bei kürzeren Texten und anderen Frequenzen verschlechtern sich die Quoten massiv, allerdings bleiben sie noch deutlich besser als beim Cutoff-Test. Bei normalisierten Texten werden durch das Eliminieren von Pronomina geringfügig bessere Quoten erreicht (vgl. 3.2.4.), bei nicht-normalisierten Texten etwas schlechtere Quoten. Stabil bleiben die Quoten bei normalisierten Texten nach dem Einbringen von Noise (vgl. 3.2.5.): Solange nicht mehr als 17% des Vokabulars ausgetauscht wurden, werden die Erkennungsquoten nur etwas schlechter. 600-800 MFWs liefern sehr gute Erkennungsquoten bis 20%. Auch die Quoten bei nicht-normalisierten Texte sind einigermaßen stabil, solange nicht mehr als 20% Noise eingebracht werden: Der Bereich von 600-800 MFWs liefert bis 9% Noise noch sehr gute und bis 22% noch gute Ergebnisse. Die Stabilität der Erkennungsquoten gibt Grund zum Optimismus für eine Anwendbarkeit bei normalisierten mittelhochdeutschen Texten. Am besten geeignet ist der Vektorbereich von 400-800 MFWs bei langen Texten mittels bag-of-words. Auch wenn die Ergebnisse für nicht-normalisierte Texte etwas zurückfallen, hat mich angesichts der wilden mittelhochdeutschen Graphien doch überrascht, dass die Delta-Performanz derart robust bleibt. Während jedoch etwa Eder Validierungsstudien mit über 60 Texten durchführen konnte, ist es um die digitale Verfügbarkeit von längeren mittelhochdeutschen Texten, von denen mindestens zwei Texte vom gleichen Autor verfasst wurden, derzeit noch deutlich schlechter bestellt. Die Aussagekraft der vorliegenden Studien wird daher durch die Korpusgröße v. a. bei den nicht-normalisierten Texten limitiert. Bei einem weiteren Versuch geht es darum, die Einflüsse von Schreibergraphie und Normalisierungsart zu reduzieren, indem nicht der Wortschatz, sondern abstraktere Daten verwendet werden: Nach Hirst und Feiguina (2007) erzielen Tests auf der Basis von Part-of-Speech–Bigrammen gute Ergebnisse; für das Mittelhochdeutsche ist jedoch noch kein Part-of-Speech-Tagger in Sicht. 2014 habe ich das Metrik-Modul aus meiner Dissertation grundlegend   überarbeitet, die Fehlerquote reduziert (nun unter 2%) und es ins Internet   zur freien Benutzung eingestellt (Dimpel 2015). Dieses Modul gibt Kadenzen   aus (etwa ""weiblich klingend""). Die metrische Struktur wird mit ""0""   (unbetonte Silben) und ""1"" (betonte Silben) ausgegeben; der dritte   ""Parzival""-Vers hat das Muster ""01010011"". Anstatt mit MFWs habe ich   Metrikmuster und Kadenzinformationen verwendet und so die Metrikdaten als   ""Worte"" testen lassen. Da ein weniger variationsreiches Ausgangsmaterial   verwendet wird, habe ich wie Hirst und Feiguina (2007) mit Bigrammen   gearbeitet. Auch ein Metrik-Delta-Plot mit Stylo-R clustert Autoren hier fehlerlos. Validierungstests sind bislang nur mit einem kleineren Korpus möglich, da das Metrikmodul Längenzeichen benötigt und nur für Texte mit vierhebigen Reimpaarversen konstruiert ist. Bei 13 Ratedateien und 11 Validierungsdateien ergibt sich bei 250–300 ""MFWs"" eine Erkennungsquote von 92,3%. Ein erfreuliches Ergebnis: (1) Bei Tests auf Grundlage von Metrik-Daten ist eine etwas geringere Abhängigkeit von Schreibergraphie und von Normalisierungsgewohnheiten gegeben. Zwar hat es mitunter metrische Eingriffe der Herausgeber gegeben, aber längst nicht immer. Wenn ein Herausgeber aus metrischen Gründen lieber das Wort ""unde"" statt ""unt"" verwendet, dann geht in den Metrik-Delta ein ähnlicher Fehler wie in den konventionellen Delta-Test ein. Immerhin immunisieren Metrik-Daten gegen Graphie-Varianten wie ""und"" oder ""unt"". (2) Zudem kann Autorschaft offenbar nicht nur mit dem vergleichsweise einfachen Parameter MFWs dargestellt werden: Nicht nur eine pure Wortstatistik führt zum Ziel, vielmehr erweist sich auch die Kompetenz zum philologischen Programmieren und zur filigranen Textanalyse als fruchtbar. (3) Bei der metrischen Struktur handelt es sich um ein Stilmerkmal, das Autoren oft intentional kunstvoll gestalten. Während es als communis opinio gilt, dass vor allem die unbewussten Textmerkmale wie MFWs autorspezifisch sind, gelingt es nun auch über ein wohl oft bewusst gestaltetes Stilmerkmal, Autorschaft zu unterscheiden. Man muss also den Dichtern nicht nur einen unbewussten stilistischen Fingerabdruck zutrauen, vielmehr lässt sich Autorschaft zumindest hier über ein Merkmal erfassen, das dem bewussten künstlerischen Zugriff unterliegen kann. Stilometrie ist der Versuch, sprachliche Besonderheiten durch statistische   Methoden herauszustellen und zu vergleichen, um damit unter anderem   Rückschlüsse auf die Urheberschaft eines Textes ziehen zu können. Als   probates Mittel bei der Autorschaftsattribuierung hat sich die Analyse der   Verwendung der häufigsten Wörter bewährt. Insbesondere Varianten des von   Burrows (2002) vorgeschlagenen Deltamaßes haben sich als sehr erfolgreich   erwiesen (Hoover 2004a; Eder / Rybicki 2011). Faktoren der Zusammensetzung   des Textkorpus, die sich negativ auf die Qualität der Ergebnisse auswirken   können, sind unter anderem zu kurze Texte (Eder 2015), unterschiedliche   Genres der Texte (Schöch 2013) und eine √úberlagerung von Autor- und   √úbersetzerstilen (Rybicki 2012). Gerade inhaltliche Unterschiede zwischen   Texten stellen ein Hindernis bei der Erkennung der Autoren dar, das nur mit   erheblichem technischen Aufwand überwunden werden kann (Stamatatos et al.   2000; Kestemont et al. 2012). In unserem Beitrag verwenden wir Deltamaße zur Identifikation von √úbersetzern. Textgrundlage ist eine Sammlung von im 12. Jahrhundert entstandenen arabisch-lateinischen √úbersetzungen wissenschaftlicher Texte aus verschiedenen Disziplinen. Wir zeigen eine Möglichkeit auf, wie die aus den oben genannten Faktoren resultierenden Limitierungen durch den Einsatz maschineller Lernverfahren kompensiert werden können. Gleichzeitig eröffnet sich dadurch eine Möglichkeit, unter den häufigsten Wörtern solche zu identifizieren, die eher Informationen zum √úbersetzer oder eher zur Disziplin tragen. Die hier verwendete Textsammlung wurde mit dem philologischen Ziel angelegt,   die √úbersetzer zu identifizieren, die im 12. Jahrhundert eine Vielzahl von   Texten aus dem Arabischen ins Lateinische übertragen und damit in den   verschiedensten Disziplinen die weitere Entwicklung der europäischen   Wissenschaften nachhaltig beeinflusst haben (Hasse / Büttner in   Vorbereitung) Für die Experimente wird ein Testkorpus so zusammengestellt, dass von jedem   √úbersetzer und aus jeder Disziplin mindestens drei Texte zur Verfügung   stehen. Dieses besteht aus insgesamt 37 Texten von 5 √úbersetzern, wobei die   Texte aus 4 Disziplinen stammen (siehe Abb. 18). Das daraus resultierende   Textkorpus ist nicht balanciert: Die Anzahl der Texte pro √úbersetzer ist   ungleich verteilt, die Länge der Texte liegt zwischen 500 und fast 200000   Wörtern; insgesamt sind die Texte auch deutlich kürzer als diejenigen der   oft verwendeten Romankorpora (vgl. etwa Jannidis et al. 2015). Weitere, die Analyse erschwerende Faktoren sind Doppelübersetzungen desselben  Originaltextes durch zwei √úbersetzer und die 'historisch nicht völlig klar  belegte 'Zusammenarbeit einiger √úbersetzer. Auf der anderen Seite sind die  unterschiedlichen Disziplinen prinzipiell klarer und eindeutiger  unterscheidbar als literarische Subgenres in Romankorpora. Ausgehend von Burrows ursprünglichem Deltamaß (Burrows 2002) wurde eine  ganze Reihe von Deltamaßen für die Autorschaftszuschreibung  vorgeschlagen (bspw. Hoover 2004b, Argamon 2008, Smith / Aldridge 2011,  Eder et al. 2013). Alle Maße operieren auf einer Term-Dokument-Matrix  der Für die folgenden Experimente verwenden wir Kosinus-Delta, das sich unter anderem bei Jannidi, Pielström, Schöch und Vitt (2015) sowie Evert, Proisel und Jannidis et al. (2015) als das robusteste Mitglied der Delta-Familie erwiesen hat. Rekursive Merkmalseliminierung (recursive feature elimination, RFE) ist eine von Guyon, Weston, Barnhill und Vapnik (2002) vorgeschlagene Methode zur Selektion einer möglichst kleinen Teilmenge von Merkmalen, mit der trotzdem möglichst optimale Ergebnisse mit einem überwachten maschinellen Lernverfahren erzielt werden können. Evert, Proisel und Jannidis et al. (2015) experimentieren zur Autorschaftszuschreibung mit durch RFE ermittelten Termen als Alternative zu den üblichen Da RFE auf einem überwachten Lernverfahren (üblicherweise einem In den folgenden Experimenten kombinieren wir beide Varianten und verkleinern die Merkmalsmenge (also die Menge der verwendeten Wörter) zunächst schrittweise auf die 500 besten Merkmale, um anschließend die optimale Merkmalsmenge zu bestimmen. Zunächst führen wir mit dem Testkorpus einige Versuche zur Anpassung der stilometrischen Methoden durch. Als Maß der Qualität des Clusterings dient dabei der Da das Hauptziel eine korrekte Zuordnung der √úbersetzer ist, soll die Menge der 500 häufigsten Wörter (im Folgenden Durch RFE wählen wir aus der Gesamtmenge weniger als 500 Wörter aus. Mit 483 Wörtern ist eine perfekte Klassifikation nach √úbersetzern möglich. Wenig überraschend erzielen wir mit diesen Wörtern auch ein perfektes Clustering der Texte nach √úbersetzern (ARI Die Analyse der Die 432 Wörter aus MFW500, die in der Menge der RFE-selektierten Wörter nicht enthalten sind, unterscheiden, wie erwartet, deutlich schlechter zwischen √úbersetzern (ARI Bei den Disziplinen erzielt die Schnittmenge der dafür mit RFE ausgewählten Wörter mit MFW500 sogar perfekte Ergebnisse (Anzahl der Merkmale: 109, ARI=1,0). Die Differenzmenge zeigt hier allerdings nicht den oben beschriebenen Effekt. Zwar ist die Clusteringqualität nach Disziplinen deutlich schlechter als der mit MFW500 erzielte Wert (ARI Um die Robustheit der Ergebnisse zu prüfen und insbesondere gegen ein Overfitting durch das RFE-Verfahren abzusichern, kann das bisher Beschriebene mit einem in ein Trainingsset und ein Testset aufgeteilten Korpus wiederholt werden, wobei die RFE-selektierten Wörter aus dem Trainingsset bestimmt und im Testset getestet werden. Dabei lassen sich die mit dem Gesamtkorpus beschriebenen Effekte reproduzieren, wenn auch 'aufgrund der dann sehr kleinen Textanzahl 'in schwächerer Ausprägung. Durch die Experimente wurde gezeigt, dass sich die Menge der Weitere Experimente in diesem Kontext werden dem Versuch dienen, die unterscheidenden Wörter besser zu charakterisieren, sodass idealerweise auch ohne maschinelles Lernen eine Auswahl der Merkmale möglich wird. Zudem steht eine Anwendung der Methode auf andere Textkorpora aus.",de,stilometrisch Verfahren Autorschaftsattribution Tradition Digital geisteswissenschaften Analyse Jüngerer Meilenstein stilometrisch Autorschaftsattribution Zweifel John burreoswn vorgeschlagen Bestimmung stilistisch Öhnlichkeit Text beeindruckend Performance Delta verschieden Sprache gattungen hinwegtäuschen theoretisch Hintergründ weitgehend unverstanden bleiben Argamon ausdrücken wissen Delta funktionieren funktionieren Kontext vorgeschlagen Sektion aktuell Stand Forschung stilometrisch Autorschaftsattribution Delta vorstellen neu Entwicklung anhand konkret Untersuchung demonstrieren vortrag Sektion leisten hierzu Beitrag beitrag demonstrieren Weise verschieden aktuell Entwicklung stilometrisch Autorschaftsattribution Delta Variant zeigen Anwendung stilometrisch distanzmaße unterschiedlich gegenstandsbereich ähnlich methodisch Frage berücksichtigen partizipieren direkt aktuellst international Entwicklung Verwendung Distanzmaß Delta stilometrisch Autorschaftsattribution burrows Delta erfolgreich algorithm Computational Stylistics burrows Reihe Studie Brauchbarkeit nachweisen Hoover Rybicki eder Schritt Berechnung Delta Häufigkeit sortiert Wert normalisieren relativ Häufigkeit Dokument berechnen Textlängenunterschiede ausgleichen Schritt Wert standardisieren wobei trotz Einfachheit praktisch Nützlichkeit mangeln bislang Erklärung Funktionsweise Algorithmus Argamon zeigen Schritt Burrow Delta Berechnung smith aldrig schlagen Information Retrieval üblich Cosinus Winkel dokumentenverktoren verwenden Delta übertreffen Burrow Delta fast Leistungsfähigkeit weisen Gegensatz Variant Verwendung vieler mfws Verschlechterung Jannidis et stellen Frage delta entscheidend Analyse Erkenntnis Verwendung verstehen Berechnung Winkel euklidisch Abstand Länge vektoren Rolle spielen abb experimente zeigen explizit Ergebnis deltamaße erheblich verbessern leistungsunterschied weitgehend neutralisieren everen Et Hypothese ableiten präsentiert Untersuchung verwenden vergleichbar aufgebaut Korpora deutsch Englisch französisch jeder Korpus enthalten Roman verschieden Autor insgesamt jeweils Text deutsch Roman Anfang Jahrhundert stammen digital Bibliothek Rolle Ausreißer Plausibilität nah untersuchen ergänzen delta bezeichnen Abstandsmaße allgemein l Abbildung vergleichen unterschiedlich l verbessern Qualität erheblich abb Argamons delta anderer Ansatz Abmilderung Ausreißer bestehen extrem insgesamt erweisen hypothese somit haltbar Ergebnis unterstützen unmittelbar erklären abschneiden Ausreißer deutlich Verbesserung führen Hypothese untersuchen rein erstellen überdurchschnittlich Unauffälliger unterdurchschnittlich Häufigkeit Wörter unterscheiden abb rechts unten Abbildung zeigen hervorragend Ergebnis erzielen praktisch ebenbürtig anfällig l Ausreißerhypothese widerlegen Anzahl Extremwert verringern dennoch Qualität l burrows Delta burrow Autorschaftsfragen etablieren Studie zeigen Delta germanisch Sprache ausgezeichnet funktionieren hoov eder Rybicki ed ed neuhochdeutsch zuletzt Lauer everen et mittelhochdeutsch Schreibung normiert Wort und unt vnt verschriften Varianz normalisiert Ausgabe ausgleichen vollständig viehhauser mittelhochdeutsch Problem diskutieren Wolfram eschenbach benutzen Wort Präteritalform kom Hartmann Aue verwenden Form eher südwestdeutsch Raum gehören Bedingung Einsatz Delta Basis normalisiert Text Autorschaftsstudie geeignet Zufälligkeit Schreibergraphi reduzieren längenzeich Stelle meist lexikalisch Information Verfügung Differenzierung sin Sinn versus Disambiguierung Verbum Substantivum Pronome diplomatisch Transkriptione superskript diakritisch Zeichen enthalten gleich Flexionsform gleich Wort verschieden Graphi erscheinen Anlass vorsichtig Optimismus bieten Studie ed einfluss noise Schreibervariante analysieren Ergebnis neuhochdeutsch zufällig Buchstabentausch mfws Ergebnis beeinträchtigen mäßig randomisiert Manipulation verschlechtern Quote Korrekt attributionen mfws ebenfalls ersetzen Autortext passag zufällig gewählt Passag anderer Autor ergeben Quote lediglich Gentle decrease -- performance lateinischer bleiben Quote Originalvokabular austauschen Text viehhauser analysieren normalisierten Ausgabe vorliegen heterogen Text Autor Eder Kestemont Et testen Text Fraglicher Autorzuschreibung Konrad Würzburg normalisieren Hartmann Wolfram Gottfried Ulrich Wirnt Konrad liegen diplomatisch Transkription Rudolf ems Gerhard Alexander Barlaam normalisieren normalisieren Willehalm Weltchronik ubir Stricker lediglich pfaff amis normalisieren per Skript Längenzeich eliminieren Text Längenzeich auseinander sortieren Superskript konventionell Buchstabe transformieren dennoch bleiben Unterschied Genitivform gott lauten teils got teils gotis eigentlich primär Sortierung entlang Unterscheidung erwarten Ergebnis frappierend Basis Mfws Parameter verwend ed viehhauser gelingen Pronomina fehlerfrei sortierung Autorschaft Delta ordnen Rudolf Rudolf normalisieren Befund Anlass Serie automatisiert Test Anlehnung eder Vektor Textlänge liefern delta zuverlässig Ergebnis wirken Einbringen noise per Perlskript implementieren Zahl iterationen verschieden Ratetext bekannt Autorschaft Validierungskorpus bekannt Autorschaft jeweils daraufhin prüfen Text Ratekorpus tatsächlich niedrig Text gleich Autor Validierungskorpus herauskommen heterogen Validierungskorpus Text normalisiert Ratetexte testen heterogen Validierungskorpus Text Ratetexte testen ermitteln Prozentsatz erkannt Autor jeweils vektorlänge vektorlänge Schritt Mfws erhöhen Pronomina beseitigen Normalisiert Ratetext Erkennungsquote mfws mfws interessant fehlattribution Stricker pfaff amis Konrad herzmär Validierungsläuf nötig normalisierter pfaff amis testen herzmär Wörter burrow ausgehen Delta Textlänge wörtern anwendbar zeigen eder Delta englisch wörtern wörtern teils desaströs Ergebnis liefern lateinisch wörtern Ergebnis erreichen Textlänge Linear begrenzen Text wörtern abschneiden korpus zuvor kurz Text herausnehmen normalisieren Text Validierungskorpus Ratekorpus Validierungskorpus Ratekorpus gleich Korpus zuvor randomisieren zusammenstellen schwanken Erkennungsquote Test pro Textläng wortlistenlänge durchführen mittelweren erfolgsquoter verwenden mittelhochdeutsch altfranzösisch Wortform Duplikate ratetexte prozentual aufsteigend randomisieren Teil fremd Sprachmaterial austauschen Fehler simulieren Kurve verlaufen konstant Linear erneut nois randomisiert hinzufügen iterationen pro einzelweren Test vektorlänge bleiben Erkennungsquot normalisierten Ratetext mfws texen quoten klein Bereich mfws Begrenzung Textlänge cutoff bleiben Ergebnis Normalisiert Text wörtern textlängen weitgehend sehen texen Quote Mfws wörtern ansonsten weithin desaströs bieten stabiler Ergebnis Normalisiert Text Textlänge quoten mfws texten Quote wiederum Textlänge mfws kurz Text frequenz verschlechtern quoten massiv bleiben deutlich Normalisiert Text eliminieren Pronomina geringfügig gut quoten erreichen texen schlecht quoten stabil bleiben quoten normalisierten Text Einbringen noise solange Vokabular austauschen erkennungsquoter schlecht mfws liefern erkennungsquot quoten Text einigermaßen stabil solange noise einbringen Bereich Mfws liefern noise Ergebnis Stabilität erkennungsquoter Grund Optimismus Anwendbarkeit normalisierten mittelhochdeutsch Text geeignet Vektorbereich Mfws lang texten mittels Ergebnis Text zurückfallen angesichts wild mittelhochdeutsch Graphi überraschen derart Robust bleiben ed Validierungsstudi Text durchführen digital Verfügbarkeit lang mittelhochdeutsch Text mindestens Text gleich Autor verfassen derzeit deutlich schlecht bestellen Aussagekraft vorliegend Studie Korpusgröße Text limitieren Versuch einflüß schreibergraphie normalisierungsart reduzieren Wortschatz abstrakt daten verwenden Hirst Feiguina erzielen Test Basis Ergebnis mittelhochdeutsch Sicht Dissertation grundlegend überarbeiten Fehlerquote reduzieren Internet frei Benutzung einstellen dimpel Modul Kadenz weiblich klingend metrisch Struktur unbetont Silbe betont Silbe ausgeben Muster anstatt Mfws Metrikmuster Kadenzinformation verwenden metrikdate wort testen lassen variationsreich ausgangsmaterial verwenden Hirst Feiguina Bigramm arbeiten clusteren Autor fehlerlos Validierungstests bislang klein Korpus Metrikmodul längenzeichen benötigen Text vierhebig Reimpaarverse konstruieren Ratedateie validierungsdateien ergeben mfws Erkennungsquote erfreulich Ergebnis Test Grundlage gering Abhängigkeit schreibergraphie normalisierungsgewohnheiten geben mitunter metrisch Eingriff Herausgeber geben längst Herausgeber metrisch Grund Wort und unt verwenden ähnlich Fehler konventionell immerhin immunisieren unt zudem Autorschaft offenbar vergleichsweise einfach Parameter Mfws darstellen pur Wortstatistik führen Ziel vielmehr erweisen Kompetenz philologisch Programmier filigran Textanalyse fruchtbar metrisch Struktur handeln Stilmerkmal Autor intentional kunstvoll gestalten Communis opinio gelten Unbewussten Textmerkmal mfws autorspezifisch gelingen bewusst gestaltet Stilmerkmal Autorschaft unterscheiden dichtern unbewusst stilistisch Fingerabdruck zutrauen vielmehr lässen Autorschaft zumindest Merkmal erfassen bewusst künstlerisch Zugriff unterliegen Stilometrie Versuch sprachlich Besonderheit statistisch Methode herausstellen vergleichen Rückschlüsse Urheberschaft Text ziehen probat Autorschaftsattribuierung Analyse Verwendung häufig Wörter bewähren insbesondere Variant Burrow vorgeschlagen deltamaßes erfolgreich erweisen hoover eder Rybicki Faktor Zusammensetzung Textkorpus negativ Qualität Ergebnis auswirken kurz Text eder unterschiedlich Genr Text schöch Rybicki inhaltlich Unterschied text Stelle Hindernis Erkennung Autor dar erheblich technisch Aufwand überwinden stamatatos et Kestemont et unser Beitrag verwenden deltamaßen Identifikation Textgrundlage Sammlung Jahrhundert entstanden wissenschaftlich Text verschieden Disziplin zeigen Möglichkeit genannt Faktor resultierenden limitierungen Einsatz Maschineller lernverfahren kompensieren gleichzeitig eröffnen Möglichkeit häufig wörtern identifizieren eher Information eher Disziplin tragen verwendet Textsammlung philologisch Ziel anlegen identifizieren Jahrhundert Vielzahl Text arabisch Lateinische übertragen verschieden disziplinen Entwicklung europäisch Wissenschaft nachhaltig beeinflussen hasse Büttner Vorbereitung experimente Testkorpus zusammenstellen Disziplin mindestens Text Verfügung stehen bestehen insgesamt Text wobei Text disziplin Stamme sehen abb resultierend Textkorpus balanciert Anzahl Text pro ungleich verteilen Länge Text liegen fast wörtern insgesamt Text deutlich kurz verwendet Romankorpora Jannidis et Analyse erschwerend Faktor Doppelübersetzunge originaltextes historisch völlig klar belegen Zusammenarbeit Seite unterschiedlich disziplin prinzipiell klar eindeutig unterscheidbar literarisch Subgenres Romankorpora ausgehend Burrow ursprünglich Deltamaß burrows Reihe Deltamaß Autorschaftszuschreibung vorschlagen hoov Argamon Smith Aldridge ed et Maß operieren folgend experiment verwenden Jannidi pielström schöch vitt everen Proisel Jannidis et robuste Mitglied erweisen rekursiv Merkmalseliminierung recursiv Feature Elimination rfe Guyon weston Barnhill Vapnik vorgeschlagen Methode Selektion möglichst Teilmenge Merkmale möglichst optimal Ergebnis überwacht Maschinellen lernverfahren erzielen evern Proisel Jannidis et experimentieren Autorschaftszuschreibung rfe ermittelt Terme Alternative üblich rfe überwacht lernverfahren üblicherweise folgend experimenten kombinieren Variant verkleinern merkmalsmenge Menge verwendet Wörter schrittweise Merkmal anschließend optimal Merkmalsmenge bestimmen fahren Testkorpus Versuch Anpassung stilometrisch Methode Maß Qualität Clustering dienen Hauptziel korrekt Zuordnung Menge häufig Wörter folgend rfe wählen Gesamtmenge Wörter wörtern perfekt Klassifikation überraschend erzielen wörtern perfekt Clustering Text ari Analyse Wörter Menge Wörter enthalten unterscheiden erwarten deutlich schlecht ari Disziplin erzielen Schnittmenge rfe ausgewählt Wörter sogar perfekt Ergebnis Anzahl Merkmal Differenzmeng zeigen beschrieben Effekt Clusteringqualität Disziplin deutlich schlecht erzielt Wert ari Robustheit Ergebnis prüfen insbesondere Overfitting absichern beschrieben trainingsset Testset aufgeteilt Korpus wiederholen wobei Wörter trainingsset bestimmen Testset testen lassen Gesamtkorpus beschrieben effekte reproduzieren aufgrund Textanzahl schwach Ausprägung experiment zeigen Menge experimente Kontext Versuch dienen unterscheidend Wörter charakterisieren sodass idealerweise Maschinelles lernen Auswahl Merkmal zudem stehen Anwendung Methode Textkorpora,"[('mfws', 0.35919849269810306), ('delta', 0.31843656308824925), ('text', 0.17872376285582245), ('quoten', 0.14586627862584184), ('validierungskorpus', 0.134233403419262), ('eder', 0.12750722367207856), ('wörtern', 0.125792472249591), ('normalisiert', 0.1234243089874622), ('ergebnis', 0.11912803662775637), ('normalisieren', 0.1139772318297484)]"
2016,DHd2016,posters-070.xml,"Visuelle Möglichkeiten der Textkollation anhand des Beispiels eines Vergleiches von Erich Kästners ""Fabian"" und ""Der Gang vor die Hunde""","Jan-Erik Stange (Fachhochschule Potsdam, Deutschland)","Visualisierung, Text, Kollation, Literatur","Gestaltung, Programmierung, Visualisierung, Computer, Literatur, Text, Visualisierung","Verschiedene Auflagen literarischer Werke zu unterschiedlichen Zeiten werden häufig durch Leser als originäres und statisches Erzeugnis von Autoren wahrgenommen. In der Realität führen unterschiedliche Einflussfaktoren, wie etwa das Einwirken eines repressiven staatlichen Zensurapparates auf die Veröffentlichung von Werken, das Nachbearbeiten durch Autoren selbst zu späteren Zeitpunkten oder in besonderem Maße auch die Wandlung durch Übersetzungen aus anderen Sprachen zu einer Varianz in den Fassungen, die von geringen Unterschieden in der Zeichensetzung oder unterschiedlichen Schreibweisen bis zu gänzlich anderen Inhalten reicht. Beispielhaft hierfür sind etwa die Unterschiede zwischen der 1931 veröffentlichten Fassung des Romans ""Fabian"" von Erich Kästner und der erst kürzlich wiederentdeckten und veröffentlichten Originalfassung des Romans ""Der Gang vor die Hunde"". Für die Deutsche Verlags-Anstalt war Kästners Originalmanuskript an vielen Stellen zu provokativ und sie stimmte einer Veröffentlichung nur zu unter der Bedingung, dass Kästner diese Stellen entschärfte. Gut lässt sich die Varianz auch anhand der zahllosen Übersetzungen von Shakespeare-Werken in andere Sprachen feststellen, die über die Jahrhunderte entstanden sind. In der Literaturwissenschaft erlauben sogenannte Kollationstools einen Vergleich verschiedener Fassungen eines Textes. Dieses Vorgehen bezeichnet man als textkritische Methode. Sie hat zum Ziel, oben genannte Einflussfaktoren auf die Gestalt eines Textes zu identifizieren, indem man verschiedene Textfassungen Satz für Satz miteinander vergleicht. Typische Tools dieser Art sind etwa ""CollateX"" oder ""JuXta"". Hinsichtlich des Interfaces bleiben diese Tools recht nah an der Textebene. Ohne Frage ist diese Nähe zur Textebene für eine Detailanalyse unerlässlich, erschwert es aber durch das Fehlen einer Überblicksdarstellung Zusammenhänge zwischen weiter auseinanderliegenden Textfragmenten zu erkennen, Muster zu identifizieren, die sich auf der Detailebene nicht erschließen und als unterstützende Navigationsebene, das schnelle Bewegen zwischen voneinander entfernten Positionen im Text. Auch ermöglicht der Überblick eine schnelle Einschätzung, welche Stellen des Textes für die Analyse besonders interessant sein könnten. Hierfür bietet sich eine visuelle Kodierung des Textes an, eine navigierbare interaktive Datenvisualisierung, da sie durch die Komplexitätsreduktion und die Konzentration auf bestimmte Attribute der Wörter eine komprimiertere Darstellung erlaubt. Der Vortrag zeigt anhand des oben genannten Kästner-Beispiels neuartige   Visualisierungsmöglichkeiten für Textkollationen auf und baut hierbei auf   bestehenden Visualisierungsansätzen auf. Hier ist vor allem Ben Frys Projekt zu   Darwins Werk ""On the Origin of Species: The Preservation of Favoured Traces"" zu   nennen. Die explorative Datenvisualisierung erlaubt es dem Betrachter, einen   Gesamtüberblick über die Entstehung des Werkes zu erhalten. Durch eine Farbkodierung   für die unterschiedlichen Ausgaben ist es auf einen Blick ersichtlich, welche   Abschnitte in welcher Auflage hinzugefügt wurden und welche Önderungen in den   verschiedenen Ausgaben vorgenommen worden sind. Zu jedem als farbigen Strich   dargestellten Absatz lässt sich in dieser Ansicht die entsprechende Textstelle   anzeigen, wenn man sich mit der Maus darüberbewegt. In der auf dem Poster vorgestellten Visualisierung der beiden Kästner-Fassungen dient die Farbkodierung dazu, den Grad der Abweichung zwischen Originalfassung und angepasster Fassung anzuzeigen. Satz für Satz können so die beiden Ausgaben miteinander verglichen werden. Als verwandtes Projekt, das Kollationen ebenfalls auf visuellem Wege zugänglich macht, ist hier außerdem die experimentelle Visualisierung ""TransVis"" zu nennen, entstanden innerhalb des Projektes ""Version Variation Visualization"", die es erlaubt, die insgesamt 37 deutschen Übersetzungen von Shakespeares Othello in einem visuellen Interface nebeneinanderzustellen und zu vergleichen. Öhnlich wie in diesem Projekt, gestattet es auch die Kästner-Visualisierung, Satz für Satz zu vergleichen, wobei visuell hervorgehoben wird, welche Teile des Satzes geändert wurden bzw. ob der Satz gänzlich ersetzt wurde. Eine weitere Ansicht ist geplant, die für einen durch den Nutzer zu definierenden Teilbereich des Buches eine typographische Übersicht bietet über die in diesem Bereich vorgenommenen Önderungen. Auf diese Weise kann schnell eingeschätzt werden, ob die Önderungen in einem Abschnitt einen thematischen Fokus haben (In Kästners Werk sind es häufig Formulierungen mit erotischem Bezug). Außerdem soll im Verlauf des Projektes eine weitere Variante entstehen, die sich nicht nur an Literaturwissenschaftler in Form einer analytischen Datenvisualisierung richtet, sondern auch an gewöhnliche Leser, bei denen zunächst der ununterbrochene Konsum des Textes im Vordergrund steht, für die aber zusätzliche Informationen, wie die Veränderung eines Textabschnittes über mehrere Fassungen hinweg auch von Interesse sein können. Die Textdaten für die Visualisierung basieren auf den beiden E-Book-Fassungen der  Texte ""Der Gang vor die Hunde"" (2013) und Fabian (2010). Unterschiede zwischen den  beiden Fassungen wurden mithilfe des Levenshtein-Algorithmus in Java vorberechnet  und dann als Datensatz in einer Webvisualisierung verwendet, die mit HTML, CSS,  Javascript und der Visualisierungsbibliothek D3 erstellt wurde.",de,verschieden auflagen literarisch Werk unterschiedlich Zeit häufig Leser originäres statisch Erzeugnis Autor wahrnehmen Realität führen unterschiedlich einflussfaktoren Einwirken repressiv staatlich zensurapparates Veröffentlichung Werk nachbearbeiter Autor spät Zeitpunkt besonderer Maß Wandlung übersetzung Sprache Varianz Fassung Geringe unterschiede Zeichensetzung unterschiedlich Schreibweise gänzlich inhalt reichen beispielhaft hierfür Unterschied veröffentlicht Fassung Roman Fabian Erich Kästner kürzlich wiederentdeckt veröffentlicht Originalfassung Roman Gang Hunde deutsch Kästner Originalmanuskript Stelle provokativ stimmen Veröffentlichung Bedingung kästn Stelle entschärfen lässen Varianz anhand zahllos übersetzungen Sprache feststellen jahrhunderte entstehen Literaturwissenschaft erlauben sogenannter kollationstools Vergleich verschieden Fassung Text vorgehen bezeichnen textkritisch Methode Ziel genannt Einflussfaktor Gestalt Text identifizieren verschieden textfassung Satz Satz miteinander vergleichen typisch Tools Art collatex Juxta hinsichtlich Interface bleiben Tool nah Textebene Frage Nähe Textebene Detailanalyse unerlässlich erschweren Fehlen Überblicksdarstellung zusammenhänge auseinanderliegend Textfragment erkennen Muster identifizieren detaileben erschließen unterstützend navigationsebene schnell Bewegen voneinander entfernt Position Text ermöglichen Überblick schnell Einschätzung Stelle Text Analyse interessant können hierfür bieten visuell Kodierung Text navigierbar interaktiv datenvisualisierung Komplexitätsreduktion Konzentration bestimmt Attribut Wörter komprimiert Darstellung erlauben Vortrag zeigen anhand genannt neuartig visualisierungsmöglichkeiten Textkollation bauen hierbei bestehend visualisierungsansätze ben Fry Projekt Darwins Werk -- -- origin of Species The Preservation -- favoured traces nennen explorativ Datenvisualisierung erlauben Betrachter Gesamtüberblick Entstehung Werk erhalten Farbkodierung unterschiedlich Ausgabe Blick ersichtlich abschnitte Auflage hinzufügen Önderungen verschieden Ausgabe vornehmen farbig Strich dargestellt Absatz lässen Ansicht entsprechend Textstelle anzeigen Maus darüberbewegen Poster vorgestellt Visualisierung dienen Farbkodierung Grad Abweichung Originalfassung angepasst Fassung anzeigen Satz Satz Ausgabe miteinander vergleichen verwandt Projekt Kollation ebenfalls visuell Weg zugänglich experimentell Visualisierung transvis nennen entstehen innerhalb projekt Version Variation Visualization erlauben insgesamt deutsch übersetzungen Shakespeare Othello visuell Interface nebeneinanderzustellen vergleichen öhnlich Projekt gestatten Satz Satz vergleichen wobei visuell hervorheben Teil satzes ändern Satz gänzlich ersetzen Ansicht planen Nutzer definierend Teilbereich Buch Typographisch übersicht bieten Bereich vorgenommenen önderungen Weise schnell einschätzen Önderungen Abschnitt thematisch Fokus kästner Werk häufig formulierungen erotisch Bezug Verlauf projektes variante entstehen Literaturwissenschaftler Form analytisch Datenvisualisierung richten gewöhnlich Leser ununterbrochen Konsum Text Vordergrund stehen zusätzlich Information Veränderung Textabschnitt mehrere Fassung hinweg Interesse Textdat Visualisierung basieren texte Gang Hunde Fabian Unterschied Fassung Mithilfe Java vorberechnen Datensatz Webvisualisierung verwenden html Css Javascript Visualisierungsbibliothek erstellen,"[('fassung', 0.2751041515656548), ('satz', 0.24968815436336766), ('kästner', 0.176036995192789), ('datenvisualisierung', 0.1597084843721613), ('önderungen', 0.14895316924915344), ('visuell', 0.13382937201449316), ('fabian', 0.1259984422454703), ('hunde', 0.1259984422454703), ('werk', 0.11914155007225091), ('farbkodierung', 0.11735799679519265)]"
2016,DHd2016,vortraege-021.xml,Digitale Editionen als Web-Services,"Immanuel Normann (pagina GmbH, Deutschland)","Semantic annotation, linked open data, REST API, digitale Editionen","Programmierung, Annotieren, Kommunikation, Kollaboration, Kommentierung, Webentwicklung, Software, Text, Werkzeuge, virtuelle Forschungsumgebungen","Verstehen wir unter einer digitalen Edition eine ""erschließende Wiedergabe   historischer Dokumente"", welche dem digitalen Paradigma folgt, indem sie die   gegenwärtigen technischen Möglichkeiten berücksichtigt (cf. Sahle 2013: 138,   148), dann stellt sich die Frage, welche   technischen Möglichkeiten zu welchem Zweck eingesetzt werden sollen. In diesem   Beitrag wird die Überzeugung vertreten, dass digitale Editionen als zentraler   Bestandteil von Forschungsumgebungen der Textwissenschaft von weit größerem Nutzen   sein können, wenn sie über standardisierte semantische Web-Schnittstellen verfügen.   Digitale Editionen wären dann primär als Web-Services zu verstehen, die über ihre   Web-Schnittstellen mit anderen Web-Services oder mit Web-Anwendungen kommunizieren.   Es wäre erst die Web-Anwendung (welche im Browser ausgeführt wird), mit der der   menschliche Nutzer interagiert, wogegen alle übrige Kommunikation von Maschine zu   Maschine liefe. Herkömmliche digitale Editionen sind primär auf eine Nutzung durch   den Menschen allein ausgerichtet. Die im Folgenden zu begründende These ist, dass   Werkzeuge der Forschungsumgebungen mit diesen herkömmlichen digitalen Editionen   deshalb nur unbefriedigend ineinandergreifen, weil sie programmatisch abgeschlossen   sind. Dieser Zustand ist insofern unbefriedigend, als dadurch Textforschung weit   weniger vernetzt und kollaborativ vonstatten geht als dies möglich wäre. Eine Verbesserung dieses Zustands kann natürlich nicht allein von technischen  Neuerungen digitaler Editionen erhofft werden. Es sind ebenso technische Neuerungen  bei allen Komponenten bestehender Forschungsumgebungen nötig (und bei Initiativen  wie TextGrid auch im Gange). Dabei besteht eine wechselseitige Abhängigkeit des  Entwicklungsfortschritts: Nur wenn die eine Komponente das eine neue Feature  anbietet, besteht bei der anderen Komponente die Chance eines Entwicklungssprungs.  Mit Blick auf diese Im Folgenden wird daher das Umfeld digitaler Editionen innerhalb einer textwissenschaftlichen Forschungsumgebung in den Blick kommen und zwar in einer Weise, die auch noch nicht existierende Systeme mitdenkt. Dies ist möglich, wenn man eine solche Umgebung zu diesem Zweck nicht als eine Ansammlung bestehender Tools auffasst, sondern die textwissenschaftlichen Tätigkeiten identifiziert, für die man sich ohne Rücksicht auf bestehende Fertiglösungen technische Unterstützung überhaupt vorstellen kann. Die aus informationstechnischer Sicht relevanten Tätigkeiten lassen sich in diesem Kontext sinnvoll unterteilen in: das Fragen wir uns nun, zu welchen dieser drei Tätigkeitsfeldern (Lesen, Schreiben,  Verwalten) eine digitale Edition eine unmittelbare und eine mittelbare Unterstützung  liefern kann. Traditionell dienen digitale Editionen (wie ihre gedruckten Vorfahren)  in erster Linie dazu gelesen zu werden. Zwar sind die in ihr enthaltenen Texte und  ihre Metadaten natürlich auch Ergebnis einer Textverwaltung. Jedoch bieten sie dem  Nutzer nur in seltenen Fällen und da auch nur rudimentär die Möglichkeit selbst Text  zu verwalten (cf. z. B. Arbeitsmappen bei All diesen digitalen Editionen ist jedoch gemeinsam, dass, sofern sie eine Textverwaltung unterstützen, diese dann nur für die im System vorhandenen (oder darin erzeugten) Texte ermöglichen. Im Allgemeinen ist der Textwissenschaftler aber nicht mit einem einzelnen Textkorpus befasst, sondern mit mehreren. Eine Textverwaltung kann dann nur ihren Nutzen entfalten, wenn sie als eigenständiger Service auf mehrere digitale Editionen zugreifen kann. Nehmen wir als einfaches Beispiel die Zusammenstellung der Literatur zu einem Germanistikseminar, in dem Texte verschiedener Autoren behandelt werden. Von einer komfortablen Textverwaltung würde man jetzt nicht die URL der jeweiligen digitalen Editionen erwarten, sondern man möchte am besten die Texte selbst per Mausklick zur Verfügung gestellt bekommen ohne dabei auf die Web-Seiten der jeweiligen digitalen Editionen gehen zu müssen. Schon dieser einfache Fall zeigt den Nutzen, den eine programmatische Schnittstelle von digitalen Editionen haben könnte: Ein eigenständiger Service zur Aggregation von Semesterapparaten ließe sich mit geringem Aufwand implementieren. Tatsächlich bieten manche digitale Editionen (z. B. das Deutsche Textarchiv) ihre  Texte (sogar in verschiedenen Formaten: TEI, HTML, plain text) zum Download an, so  dass man die entsprechenden Links schon als Web-API auffassen könnte. Allerdings  beschränkt sich diese Möglichkeit entweder auf den Download einer einzelnen Seite  oder des gesamten Textdokuments. Für eine brauchbare Textverwaltung wäre es jedoch  wesentlich praktischer, wenn man Texte nicht nach Paginierungsgrenzen sondern  bezüglich semantischer Sinneinheiten beziehen könnte. Es fällt nicht schwer, sich  entsprechende Szenarien vorzustellen: Für eine Anthologie möchte man etwa Balladen  einer bestimmten Epoche zusammenstellen.; für eine Theaterprobe möchte jeder  Schauspieler eine Zusammenstellung derjenigen Szenen, in der seine Rolle vorkommt;  ein Übersetzungsforscher möchte alle deutschen Übersetzungen des Monolog der ersten  Szene im dritten Aufzug von Shakespeares Hamlet. Die Zahl weiterer Szenarien ist  unbegrenzt. Als entscheidende Anforderung an eine digitale Edition wäre  festzuhalten: die Adressierbarkeit und Auffindbarkeit von Texten in allen üblichen  Struktureinheiten (z. B. Kapitel, Absatz, Drama, Akt, Szene, Gedicht, Strophe, Vers,  etc.). Da in den meisten digitalen Editionen die Texte im TEI-XML vorliegen, welche  die Kodierung solcher Struktureinheiten erlauben, dürfte es prinzipiell nicht  schwierig sein, diese auch über eine Web-API adressierbar zu machen. Was die  Auffindbarkeit betrifft, wäre es wünschenswert, die Möglichkeitender in der  Backend-Datenbank verwendeten Anfragesprachen weitgehend in der Web-API abzubilden.  Das ganze Feld der Suchmöglichkeiten ist allerdings so umfangreich, dass es einen  eigenen Beitrag rechtfertigen würde und daher hier nicht weiter vertieft werden  soll. Allein die Adressierbarkeit aller textspezifischen Struktureinheiten (s. o.)  mittels der Web-API von digitalen Editionen wäre eine große Chance zur Entwicklung  nützlicher Textverwaltungsdienste. Allerdings sollten neben den vorgegebenen  Struktureinheiten auch vom Nutzer frei definierte Textauswahlen von einer digitalen  Edition adressierbar sein. Damit soll die verbreitete Praxis, Textausschnitte mit  einem Textmarker zu markieren, im digitalen Medium nicht nur die Funktion erhalten,  etwas farblich hervorzuheben, sondern die so ausgezeichneten Textpassagen sollen  durch eine generierte Adresse permanent referenzierbar gemacht werden. Damit wäre  beispielsweise eine Sammlung von Exzerpten referenzierbar, die ein Benutzer mit  einem virtuellen Textmarker erzeugt hat. Bis hierin wurde die Adressierbarkeit von jeglichen Textausschnittenin den oben angeführten Szenarien ausschließlich für die Erstellung von Textsammlungen verwendet. Das ist aber nur eine einfache Form der Textverwaltung. Denn eine Textsammlung ist zunächst eine in sich unstrukturierte Menge von Texten. Ziel einer Textverwaltung ist es aber meist, in eine Textsammlung eine bestimmte Ordnung zu bringen. Das ist unter anderem der Fall, wenn man die gesammelten Texte nach forschungseigenen Kriterien klassifiziert; z. B. als Linguist nach grammatischen Eigenschaften, als Literaturwissenschaftler nach Motiven, als Übersetzer nach Idiomen, etc. Textklassifikation wäre eine Relation zwischen Texten und Sammelbegriffen. Darüber hinaus wäre es wichtig, in einer Textverwaltung die Beziehung der Texte untereinander explizit machen zu können. So könnte man beispielsweise explizit erfassen, dass eine bestimmte Textpassage eine Anspielung auf einen anderen Text ist; oder dass die eine Textfassung aus jener Skizze hervorgegangen ist, etc. Soweit würde man Textausschnitte aus digitalen Editionen in Beziehung zueinander setzen. Man würde aber in einer Textverwaltung insbesondere auch die Texte der digitalen Editionen in Beziehung zu selbstverfassten Texten setzten wollen. Auch würde man Texte zu nicht textartigen Gegenständen wie Personen, Orte oder Ereignissen in Beziehung setzen wollen; beispielsweise wenn man in historischen Romanen den Bezug zu historisch belegten Sachverhalten herstellen möchte. Eine Textverwaltung, die all die skizzierten Funktionalitäten bereitstellen würde, könnte einen Textwissenschaftler bei der Arbeit am Text bzw. der Organisation der eigenen Texte erheblich unterstützen. Sie würde darüber hinaus das kollaborative Arbeiten erleichtern, indem sie eine auf Austausch von Dokumenten basierte Arbeitsweise durch eine Praxis der direkten Vernetzung von Inhalten im Netz ersetzen würde. Sie könnte aber nur funktionieren, wenn die Texte digitaler Editionen in aller Granularität über Web-APIs adressierbar wären. Abschließend soll erwähnt werden, das eine ganze Reihe von Anstrengung von verschiedenen Seiten schon unternommen wurden, die durch eine geeignete Zusammenführung ein solides Fundament zur Umsetzung dieser Visionen bilden könnten. Allgemeine technische Grundlage wären die Semantic-Web-Technologien. Darauf aufbauend wären folgende theoretische und praktische Arbeiten hervorzuheben: Von Silvio Peroni (2014) zu ""Semantic Publishing"" , Fabio Ciottis und Francesca Tomasis (2014) Entwurf zu ""Formal ontologies, Linked Data and TEI semantics"", das semantic annotation Tool",de,verstehen digital Edition erschließend Wiedergabe Historischer dokument digital Paradigma folgen gegenwärtig technisch Möglichkeit berücksichtigen cf sahlen stellen Frage technisch Möglichkeit Zweck einsetzen Beitrag Überzeugung vertreten digital editionen Zentraler Bestandteil Forschungsumgebung Textwissenschaft größerem nutzen standardisiert semantisch verfügen digital editionen sein primär verstehen kommunizieren Browser ausführen menschlich Nutzer interagieren wogegen übrig Kommunikation Maschine Maschine liefe herkömmlich digital editionen primär Nutzung Mensch ausrichten folgend begründend These Werkzeug Forschungsumgebung herkömmlich digital Edition unbefriedigend ineinandergreifen programmatisch abschließen Zustand insofern unbefriedigend Textforschung vernetzen kollaborativ vonstatten Verbesserung Zustand technisch neuerung digitaler editionen erhoffen technisch Neuerung komponent bestehend Forschungsumgebung nötig Initiative Textgrid Gang bestehen wechselseitig Abhängigkeit Entwicklungsfortschritt Komponente Feature anbieten bestehen Komponente Chance Entwicklungssprung Blick folgend umfeld Digitaler editioner innerhalb textwissenschaftlich Forschungsumgebung Blick Weise existierend System mitdenken Umgebung Zweck Ansammlung bestehend Tools auffasst textwissenschaftlich Tätigkeit identifizieren Rücksicht bestehend Fertiglösunge technisch Unterstützung vorstellen informationstechnisch Sicht relevant Tätigkeit lassen Kontext sinnvoll unterteilen fragen Tätigkeitsfelder lesen schreiben verwalten digital Edition unmittelbar mittelbar Unterstützung liefern traditionell dienen digital editionen gedruckt vorfahren Linie lesen enthalten Text Metadat Ergebnis Textverwaltung bieten nutzer selten Fall Rudimentär Möglichkeit Text verwalten cf arbeitsmappe all digital editionen gemeinsam sofern Textverwaltung unterstützen System vorhanden erzeugt Text ermöglichen Textwissenschaftler einzeln Textkorpus befassen mehrere Textverwaltung Nutzen entfalten eigenständig Service mehrere digital editionen zugreifen nehmen einfach Zusammenstellung Literatur Germanistikseminar Text verschieden Autor behandeln komfortabl Textverwaltung url jeweilig digital editionen erwarten Text per Mausklick Verfügung stellen bekommen jeweilig digital editionen einfach Fall zeigen Nutzen programmatisch Schnittstelle digital editionen eigenständig Service Aggregation Semesterapparat lassen gering Aufwand implementieren tatsächlich bieten digital editioner deutsch Textarchiv Text sogar verschieden Format tei Html Plain Text Download entsprechend links auffassen beschränken Möglichkeit Download einzeln Seite gesamt Textdokument brauchbar Textverwaltung wesentlich praktisch texte Paginierungsgrenze bezüglich semantisch sinneinheit beziehen fallen schwer entsprechend szenarien vorstellen Anthologie ballad bestimmt Epoche zusammenstellen Theaterprobe Schauspieler Zusammenstellung Szene Rolle vorkommen übersetzungsforsch deutsch übersetzungen Monolog Szene Aufzug Shakespeare Hamlet Zahl weit Szenarie unbegrenzt entscheidend Anforderung digital Edition festhalten Adressierbarkeit Auffindbarkeit Text üblich Struktureinheit Kapitel Absatz Drama Akt Szene dichen Strophe vers meister Digital Edition Text vorliegen Kodierung Struktureinheit erlauben dürfen prinzipiell schwierig adressierbar Auffindbarkeit betreffen wünschenswert möglichkeitend verwendet Anfragesprach weitgehend abzubilden Feld Suchmöglichkeit umfangreich Beitrag rechtfertigen vertiefen Adressierbarkeit textspezifisch struktureinheiten mittels digital editionen Chance Entwicklung nützlich textverwaltungsdienste vorgegeben Struktureinheit Nutzer frei definiert Textauswahl digital Edition adressierbar verbreitet Praxis Textausschnitt Textmarker markieren digital Medium Funktion erhalten farblich hervorzuheben ausgezeichnet Textpassage generiert Adresse permanent referenzierbar beispielsweise Sammlung Exzerpt referenzierbar Benutzer virtuell Textmarker erzeugen Hierin Adressierbarkeit jeglicher Textausschnittenin angeführt Szenarie ausschließlich Erstellung Textsammlung verwenden einfach Form Textverwaltung Textsammlung unstrukturiert Menge Text Ziel Textverwaltung meist Textsammlung bestimmt Ordnung bringen Fall gesammelt Text forschungseigen kriterien klassifizieren linguist grammatischen eigenschaften Literaturwissenschaftler Motiv Übersetzer idiomen Textklassifikation Relation Text Sammelbegriff hinaus wichtig Textverwaltung Beziehung Text untereinander explizit beispielsweise explizit erfassen bestimmt Textpassage Anspielung Text Textfassung Skizze hervorgehen soweit Textausschnitt digital Edition Beziehung zueinander setzen Textverwaltung insbesondere Text digital Edition Beziehung selbstverfasst Text setzen texte Textartige genständen Person Ort Ereignissen Beziehung setzen beispielsweise historisch Roman Bezug historisch belegt Sachverhalt herstellen Textverwaltung all skizziert funktionalitäten bereitstellen Textwissenschaftler Arbeit Text Organisation Text erheblich unterstützen hinaus kollaborativ arbeiten erleichtern Austausch dokumenten basieren arbeitsweise Praxis direkt Vernetzung Inhalt Netz ersetzen funktionieren Text digitaler editionen Granularität adressierbar sein abschließend erwähnen Reihe Anstrengung verschieden Seite unternehmen geeignet Zusammenführung solid Fundament Umsetzung Vision bilden können allgemein technisch Grundlage sein aufbauend sein folgend theoretisch praktisch arbeiten hervorzuheben silvio Peroni semantic publishing Fabio Ciottis francesca Tomasis Entwurf formal ontologies Linked Data and tei semantics Semantic Annotation Tool,"[('textverwaltung', 0.42040688898358214), ('editionen', 0.33845644903030997), ('digital', 0.2221537005221582), ('edition', 0.17035595064956768), ('text', 0.1595278021452526), ('forschungsumgebung', 0.136916949206707), ('struktureinheit', 0.1261220666950746), ('adressierbarkeit', 0.1261220666950746), ('adressierbar', 0.1261220666950746), ('technisch', 0.10506166874055509)]"
2017,DHd2017,vortrag-REGER.xml,Analyzing Features for the Detection of Happy Endings in German Novels,"Fotis Jannidis (Universität Würzburg, Deutschland); Isabella Reger (Universität Würzburg, Deutschland); Albin Zehe (Universität Würzburg, Deutschland); Martin Becker (Universität Würzburg, Deutschland); Lena Hettinger (Universität Würzburg, Deutschland); Andreas Hotho (Universität Würzburg, Deutschland)","Happy End classification, plot representation","Inhaltsanalyse, Strukturanalyse, Literatur"," Der Plot ist ein grundlegendes Strukturelement literarischer Texte. Dementsprechend wären Methoden zur computergestützten Repräsentation von Plot oder bestimmten Plot-Elementen ein großer Gewinn für die quantitative Literaturanalyse. Dieses Paper betrachtet ein solches Plot-Element: das Ende; genauer gesagt untersuchen wir die Frage, ob ein Werk ein Happy End hat oder nicht. Dazu setzen wir Sentimentanalyse ein, wobei wir den Fokus auf die qualitative Betrachtung bestimmter Features und deren Performanz legen, um tiefere Einsicht in die Funktionsweise der automatischen Klassifikation zu erhalten. Außerdem zeigen wir, wie die beschriebene Vorgehensweise auf nachfolgende Forschungsfragen angewendet werden und dabei zu interessanten Ergebnissen hinsichtlich der Erscheinungszeit der Romane führen kann. In einer der ersten Arbeiten beschäftigt sich Mark Finlayson mit folkloristischen Erzählungen und entwickelt einen Algorithmus, der Ereignisse erkennt und daraus übergeordnete Konzepte wie Niedertracht oder Belohnung abstrahiert (Finlayson 2012). Reiter et al. identifizieren Ereignisse sowie deren Teilnehmer und Reihenfolge und nutzen maschinelle Lernverfahren, um strukturelle Öhnlichkeiten über Erzählungen hinweg aufzudecken (Reiter 2013, Reiter et al. 2014). In letzter Zeit richtet sich einige Aufmerksamkeit auf die Sentimentanalyse, insbesondere seit Matthew Jockers emotionale Erregung als Indikator für Plotstrukturen vorgeschlagen hat (Jockers 2014). Er unterteilt Romane in Segmente und bildet daraus emotionale Plot-Kurven (Jockers 2015). Obwohl die Idee, Sentimentanalyse in diesem Zusammenhang einzusetzen, gut aufgenommen wurde, wurde Jockers für seine Verwendung der Fourier-Transformation zur Glättung der resultierenden Plot-Kurven kritisiert (Swafford 2015, Schmidt 2015). Micha Elsner (Elsner 2015) verwendet, neben anderen Features, ebenfalls Sentimentkurven, um Repräsentationen des Plots romantischer Werke zu erstellen. Er verknüpft diese Kurven mit bestimmten Figuren und untersucht auch das gemeinsame Auftreten von Figuren. Die Auswertung seines Ansatzes zeigt, dass er echte Romane mit beachtlichem Erfolg von künstlich umgestellten Versionen unterscheiden kann, was darauf hindeutet, dass seine Methoden tatsächlich bestimmte Aspekte der Plotstruktur abbilden. In vorhergehenden Arbeiten haben wir Sentiment-Features verwendet, um Happy Ends, als ein wichtiges Plot-Element, in deutschsprachigen Romanen zu erkennen, wobei wir einen F1-score von 73% erreichen konnten (Zehe et al. 2016). Unser Datensatz besteht aus 212 deutschsprachigen Romanen, die hauptsächlich aus dem 19. Jahrhundert stammen. Unsere Sentimentanalyse erfordert eine Ressource, die auflistet, welche Gefühle Leser typischerweise mit bestimmten Worten oder Phrasen eines Textes assoziieren. Dieses Paper verwendet das NRC Sentiment Lexikon (Mohammad und Turney 2013), zu dem eine automatisch übersetzte deutsche Version verfügbar ist Tabelle 1: Beispieleinträge aus dem NRC Sentiment Lexikon Ziel dieses Papers ist es, Features, die zur Erkennung von Happy Ends in Romanen genutzt wurden, genauer zu untersuchen, um Einsichten in die Relevanz bestimmter Features zu erhalten. Dazu übernehmen wir die Features und Methoden, wie sie in Zehe et al. (2016) beschrieben sind. Die Parameter der linearen SVM sowie die Einteilung in 75 Segmente sind ebenfalls aus diesem Paper übernommen. Aufgrund unserer Annahme, dass die relevante Information zur Klassifikation von Happy Ends am Ende eines Romans zu finden ist, wurden zunächst die Sentiment-Werte des letzten Segments als einziges Feature-Set ( Um unserer Intuition gerecht zu werden, dass nicht nur das letzte Segment an sich, sondern auch sein Verhältnis zum Rest des Romans für die Klassifikation von Bedeutung ist, wurden sogenannte Sektionen ( Diese Beobachtung führte uns zu der Annahme, dass unser Begriff des ""Endes"" nicht differenziert genug ist, da die Anzahl an Segmenten für jeden Roman und damit auch die Grenzen des finalen Segments relativ willkürlich gewählt wurden. Daher wurde die Aufteilung in final section und main section im Folgenden variiert, sodass die final section mehr als nur das letzte Segment enthalten kann. Abbildung 1: Klassifikationsgenauigkeit für verschiedene Unterteilungen in main und final section. Die gestrichelte Linie gibt die Baseline an, die gepunktete Linie markiert die Aufteilung, bei der der maximale F1-score erreicht wird. Abbildung 1 zeigt, dass die Klassifikationsgenauigkeit steigt, wenn mindestens 75% der Segmente in der main section sind und ein Maximum bei ca. 95% erreicht (bei 75 Segmenten insgesamt bedeutet das 4 Segmente in der final section und 71 Segmente in der main section). Mit dieser Aufteilung verbessert sich der F1-Wert auf 68%, wenn nur das Feature-Set der final section ( Da sich die Ergebnisse durch die Einbeziehung des Verhältnisses zwischen der final section und der main section verbessert haben, war unser nächster Schritt, den Verlauf der Sentimentkurve gegen Ende eines Romans genauer zu modellieren. Beispielsweise könnte sich kurz vor dem Ende eine Katastrophe ereignen, die anschließend im Sinne eines Happy Ends aufgelöst wird. Um diese Intuition abzubilden, führten wir eine weitere Sektion ein, die sogenannte late-main section, die die letzten Segmente der main section umfasst. Die Differenzen zwischen den Feature-Sets für die late-main section und die final section wurden als zusätzliche Merkmale verwendet ( Tabelle 2: F1-score für die verschiedenen Feature-Sets Die beschriebenen Ergebnisse sind in Tabelle 2 zusammengefasst. Hier wird deutlich, dass die Aufnahme der einzelnen Feature-Sets jeweils zu einer kleinen Verbesserung geführt hat, bis hin zu einem F1-score von 73%. Obwohl die Aufteilung mit 4 Segmenten in der final section die besten Ergebnisse erzielte, konnten wir auch beobachten, dass einige Romane mit mehreren verschiedenen Unterteilungen korrekt klassifiziert werden konnten. Andere Romane hingegen konnten in keinem Setting korrekt vorhergesagt werden. Als Beispiel sei hier Jules Vernes Roman  Abbildung 2: F1-score für verschiedene Unterteilungen in main und final section. Die farbigen Kurven stehen für Romane aus verschiedenen Zeitperioden. Die gestrichelte Linie zeigt die Zufallsbaseline für die Zeitperiode ab 1871. Die Baselines für die anderen Zeitperioden liegen etwas darunter und werden daher nicht dargestellt. Die gepunkteten Linien zeigen jeweils den maximalen F1-Wert für die entsprechende Zeitperiode. Abbildung 2 zeigt, dass die Klassifikation erneut dann am besten funktioniert, wenn ca. 95-98% der Segmente in der Hauptsektion sind, unabhängig von der Zeitperiode. Die beste Aufteilung in Sektionen korreliert also nicht mit dem Erscheinungsjahr eines Romans. Es fällt jedoch auf, dass die Romane nach 1848 deutlich niedrigere Werte liefern als die vor diesem Jahr veröffentlichten Texte, meistens sogar unterhalb der Baseline. Das deutet auf eine Korrelation zwischen dem Erscheinungsdatum und der Klassifikationsgenauigkeit hin: Vor dem Realismus erschienene Romane sind hinsichtlich des Happy Ends leichter zu klassifizieren als realistische Romane. Eine mögliche Erklärung für diese Beobachtung könnte die stärker schematische Struktur der vor-realistischen Romane sein. Wir sind uns bewusst, dass die Anzahl der Romane für die einzelnen Zeitperioden relativ klein ist, sodass diese Beobachtungen zunächst als exploratorische Einblicke gesehen werden müssen. Nichtsdestotrotz zeigen diese vorläufigen Ergebnisse, dass die automatische Erkennung von Happy Ends, sogar mit nur einem recht einfachen Feature-Set, Zusammenhänge zu anderen Eigenschaften von Romanen aufdecken kann, die für die Literaturwissenschaft von großem Interesse sind. Die automatische Erkennung von Happy Ends als wesentlichem Plot-Element von Romanen ist ein nützlicher Schritt in Richtung einer umfassenden computergestützten Repräsentation des Plots literarischer Texte. Unsere Experimente zeigen, dass verschiedene Features auf Basis von Sentimentanalyse eine Erkennung von Happy Ends in Romanen mit unterschiedlicher, aber insgesamt solider Genauigkeit ermöglichen. Obwohl unser Ansatz relativ einfach gehalten ist, kann er zu substantiellen Erkenntnissen für die Literaturwissenschaft führen. In zukünftigen Arbeiten soll die Genauigkeit unserer Methode verbessert werden, indem die hohe Variabilität des Endes in Romanen differenzierter betrachtet wird. Außerdem könnte der Ansatz eingesetzt werden, um bestimmte Eigenschaften weiterer Romankorpora tiefergehend zu untersuchen.",de,Plot grundlegend Strukturelement literarisch Text sein methode computergestützt Repräsentation plot bestimmt Gewinn quantitativ Literaturanalyse Paper betrachten genau untersuchen Frage Werk Happy end setzen sentimentanalyse wobei Fokus qualitativ Betrachtung bestimmt Feature Performanz legen tief Einsicht Funktionsweise automatisch Klassifikation erhalten zeigen beschrieben Vorgehensweise nachfolgend Forschungsfrag anwenden interessant Ergebnis hinsichtlich Erscheinungszeit Roman führen Arbeit beschäftigen Mark Finlayson folkloristisch Erzählung entwickeln Algorithmus Ereignis erkennen übergeordnet Konzept niedertracht Belohnung abstrahieren Finlayson Reiter et identifizier Ereignis Teilnehmer Reihenfolge nutzen Maschinell lernverfahren strukturell öhnlichkeiten erzählung Hinweg aufdecken Reiter Reiter et letzter richten Aufmerksamkeit Sentimentanalyse insbesondere Matthew Jocker emotional Erregung Indikator plotstrukturen vorschlagen Jockers unterteilen Roman Segmente bilden emotional Jockers obwohl Idee Sentimentanalyse Zusammenhang einsetzen aufnehmen jockers Verwendung Glättung resultierend kritisieren Swafford Schmidt Micha elsner Elsner verwenden Features ebenfalls sentimentkurven repräsentatione plots romantisch Werk erstellen verknüpfen Kurve bestimmt Figur untersuchen gemeinsam auftreten Figur Auswertung Ansatz zeigen echt Roman beachtlich Erfolg künstlich umgestellt Version unterscheiden hindeuten Methode tatsächlich bestimmt Aspekt Plotstruktur abbilden vorhergehend arbeiten verwenden Happy End wichtig Deutschsprachig Roman erkennen wobei erreichen zehen et Datensatz bestehen Deutschsprachig romanen hauptsächlich Jahrhundert stammen Sentimentanalyse erfordern Ressource auflistet Gefühl Leser typischerweise bestimmt Wort Phrase Text assoziieren Paper verwenden Nrc Sentiment Lexikon Mohammad Turney automatisch übersetzt deutsch Version verfügbar Tabelle beispieleinträge Nrc Sentiment lexikon Ziel Paper Features Erkennung Happy End Roman nutzen genau untersuchen Einsicht Relevanz bestimmt Feature erhalten übernehmen features Methode zehen et beschreiben Parameter Linear svm Einteilung Segment ebenfalls Paper übernehmen aufgrund Annahme relevant Information Klassifikation Happy End Roman finden letzter Segment einzig Intuition gerecht letzter Segment Verhältnis Rest Roman Klassifikation Bedeutung sogenannter Sektione Beobachtung führen Annahme Begriff ende differenzieren Anzahl Segment Roman Grenze final Segment relativ willkürlich wählen Aufteilung Final Section Main Section folgend variieren sodass Final Section letzter Segment enthalten Abbildung Klassifikationsgenauigkeit verschieden unterteilungen Main Final Section gestrichelt Linie Baselin gepunktet Linie markieren Aufteilung maximal erreichen Abbildung zeigen Klassifikationsgenauigkeit steigen mindestens Segment Main Section maximum erreichen Segment insgesamt bedeuten Segment Final Section Segment Main Section Aufteilung verbessern Final Section Ergebnis Einbeziehung verhältnisses Final Section Main Section verbessern nächster Schritt Verlauf Sentimentkurve Roman genau modellieren beispielsweise Katastrophe ereignen anschließend Sinn Happy ends auflösen Intuition abzubilden führen Sektion sogenannter Section letzter Segment Main Section umfassen Differenz Section Final Section zusätzlich merkmale verwenden Tabelle verschieden beschrieben Ergebnis Tabelle zusammengefasst deutlich Aufnahme einzeln jeweils Verbesserung führen obwohl Aufteilung Segment Final Section Ergebnis erzielen beobachten Roman mehrere verschieden unterteilung korrekt klassifizieren Roman hingegen Setting korrekt vorhergesagen Jules vern Roman Abbildung verschieden unterteilungen Main Final Section farbig Kurv stehen Roman verschieden Zeitperiod gestrichelt Linie zeigen Zufallsbaseline Zeitperiode Baselin Zeitperiod liegen darstellen gepunkteten Linie zeigen jeweils maximal entsprechend Zeitperiode Abbildung zeigen Klassifikation erneut funktionieren Segment Hauptsektion unabhängig Zeitperiode gut Aufteilung Sektion korrelieren Erscheinungsjahr Roman fallen Roman deutlich niedrig Wert liefern veröffentlicht Text meistens sogar unterhalb Baselin deuten Korrelation erscheinungsdatum Klassifikationsgenauigkeit Realismus erschienen Roman hinsichtlich Happy End leicht klassifizieren realistisch Roman möglich Erklärung Beobachtung stark schematisch Struktur Roman bewussen Anzahl Roman einzeln Zeitperiod relativ klein sodass Beobachtung exploratorisch einblicke sehen nichtsdestotrotz zeigen vorläufig Ergebnis automatisch Erkennung Happy End sogar einfach zusammenhänge Eigenschaft romanen aufdecken Literaturwissenschaft groß Interesse automatisch Erkennung Happy End wesentlich romanen nützlich Schritt Richtung umfassend computergestützt Repräsentation plots literarisch Text experiment zeigen verschieden Feature Basis Sentimentanalyse Erkennung Happy End Roman unterschiedlich insgesamt solid Genauigkeit ermöglichen obwohl Ansatz relativ einfach halten substantiell Erkenntnis Literaturwissenschaft führen zukünftigen arbeiten Genauigkeit Methode verbessern hoch Variabilität endes Romanen differenziert betrachten Ansatz einsetzen bestimmt Eigenschaft weit Romankorpora tiefergehend untersuchen,"[('section', 0.41268087921552954), ('roman', 0.30512667022223583), ('happy', 0.2970801757418876), ('segment', 0.28129374212124975), ('final', 0.25221286277965793), ('end', 0.21139250536531706), ('main', 0.17654900394576054), ('aufteilung', 0.16504454207882643), ('sentimentanalyse', 0.13565014648736665), ('klassifikationsgenauigkeit', 0.09902672524729587)]"
2017,DHd2017,poster-PIELS.xml,"Einfaches Topic Modeling in Python - Eine Programmbibliothek für Preprocessing, Modellierung und Analyse","Fotis Jannidis (Universität Würzburg, Deutschland); Steffen Pielström (Universität Würzburg, Deutschland); Christof Schöch (Universität Würzburg, Deutschland); Thorsten Vitt (Universität Würzburg, Deutschland)","topic modeling, python","Programmierung, Inhaltsanalyse, Modellierung, Visualisierung, Literatur"," Ursprünglich entwickelt, um in größeren Sammlungen kürzerer Fachartikel schnell jene zu identifizieren, die für bestimmte Themen relevant sein könnten, kann diese Methode darüber hinaus für eine Reihe von Problem im Bereich der digitalen Literaturwissenschaft interessante neue Lösungsansätze bieten. Dazu gehört die automatische Identifikation von Romanen, die ähnliche Themen behandeln (wenngleich eine direkte Gleichsetzung probabilistischer ""Topics"" mit literarischen ""Themen"" durchaus problematisch ist), ebenso wie die Zuordnung zu bestimmten Genres anhand inhaltlicher Aspekte, oder die quantifizierende Betrachtung der zu- und abnehmenden Bedeutung einzelner Themenfelder über den Verlauf eines einzelnen Romans (vgl. Blevins 2012, Jockers 2011, Mit den Programmen ""Mallet"" (vgl. McCallum 2002) und ""Gensim"" (vgl. Rehurek 2010) stehen zur Zeit zwei State-of-the-Art Implementierungen von Topic Modeling-Algorithmen zur Verfügung. Um die Methode produktiv einzusetzen, sind aber neben der Erzeugung des Modells weitere Arbeitsschritte notwendig (Abb. 1). Im ""Preprocessing"" gilt es zunächst, die Textsammlungen in eine Form zu bringen, in der sie vom Modellierungsprogramm verarbeitet werden können. Darüber hinaus werden die Texte normalerweise durch das Herausfiltern häufiger Funktionswörter auf die potentiell inhaltsrelevanten Wörter reduziert, was in der Regel den vorhergehenden Einsatz von NLP-Tools (Natural Language Processing) erfordert. Sind die ""Topics"" dann erst einmal errechnet worden, kann sich eine Visualisierung der Ergebnisse anschließen, oder ihre statistische Evaluierung anhand interner oder externer Kriterien, ein Aspekt dem beim Einsatz von Topic Modeling-Verfahren im DH-Kontext bisher eher zu wenig Beachtung geschenkt wurde. Ziel unseres Projektes ist es, den Einstieg in aktuelle Topic Modeling-Verfahren für digital arbeitende Literaturwissenschaftler wesentlich zu vereinfachen, indem wir möglichst viele der notwendigen Arbeitsschritte in einer einheitlichen, umfangreichen und gut dokumentierten Programmbibliothek für die unter digital-quantitativ arbeitenden Geisteswissenschaftlern stark verbreitete Programmiersprache Python anbieten. Hierbei sollen Nutzerinnen und Nutzer bei allen Arbeitsschritten auf vorhandene, in einem ausführlichen Tutorial dokumentierte Funktionen zurückgreifen und so weit wie möglich wie mit einem Kommandozeilentool arbeiten können, ohne selbst programmieren zu müssen. Die Anforderungen an die Programmierkenntnisse der Forschenden, die diese Verfahren einsetzen möchten, werden damit minimiert und die Methode wird so einem größeren Nutzerkreis zugänglich gemacht. Für das NLP-Preprocessing steht mit dem DARIAH-DKPro-Wrapper (DDW) ein komfortables Einheitswerkzeug zur Verfügung, das ein großes Spektrum an NLP-Aufgaben abdeckt und linguistische Annotationen in einem Python-Pandas-kompatiblen Ausgabeformat erzeugt. Ein Ziel unserer Bibliothek ist die direkte Anbindung des DDW-Outputs an existierende Implementierungen verschiedener etablierter Varianten von Topic Modeling-Algorithmen. Für die Untersuchung der resultierenden Modelle möchten wir verschiedene Evaluierungsverfahren anbieten, sowohl interne Verfahren wie z.B. das Perplexity-Maß, als auch externe Vefahren, wie z.B. die Weglänge zwischen zwei Begriffen in einem Wörterbuch. Hieran schließen sich verschiedene Optionen zur Visualisierung der Ergebnisse an. Im Fokus der Entwicklung steht die Gestaltung schlüssig aufeinander aufbauender Programmbefehle, die einer einheitlichen Syntax folgen und deren Funktion sich schnell erschließen lässt. Sie sollen sich ohne längere Einarbeitung nutzen und zu einer Pipeline zusammenfügen lassen, die die spezifischen Arbeitsschritte eines bestimmten Topic Modeling-Projektes umsetzt. Hierbei können Nutzerinnen und Nutzer auf detaillierte Anleitungen aus einem umfangreichen Tutorial zurückgreifen, in dem alle Funktionen, alle Outputs, und potentielle Kombinationen detailliert dokumentiert und anhand von Beispielen erläutert werden.  ",de,ursprünglich entwickeln groß Sammlung kürzer Fachartikel schnell identifizieren bestimmt Thema relevant können Methode hinaus Reihe Problem Bereich digital Literaturwissenschaft interessant lösungsansätze bieten gehören automatisch Identifikation romanen ähnlich Thema behandeln wenngleich direkt Gleichsetzung probabilistisch Topics literarisch Thema problematisch Zuordnung bestimmt Genre anhand inhaltlich Aspekt quantifizierend Betrachtung abnehmend Bedeutung einzeln themenfeld Verlauf einzeln Roman Blevins Jockers programm mallet mccallum gensim Rehurek stehen implementierungen Topic Verfügung Methode produktiv einsetzen Erzeugung Modell Arbeitsschritte notwendig abb Preprocessing gelten Textsammlung Form bringen Modellierungsprogramm verarbeiten hinaus Text normalerweise Herausfilter häufig Funktionswörter potentiell inhaltsrelevant Wörter reduzieren Regel vorhergehend Einsatz natural language Processing erfordern topics errechnen Visualisierung Ergebnis anschließen statistisch Evaluierung anhand intern extern Kriterien Aspekt Einsatz Topic eher Beachtung schenken Ziel unser Projekte Einstieg aktuell Topic Digital arbeitend Literaturwissenschaftler wesentlich vereinfachen möglichst notwendig Arbeitsschritte einheitlich umfangreich dokumentiert Programmbibliothek arbeitend Geisteswissenschaftler stark verbreitet programmiersprache python anbieten hierbei nutzerinnen nutzer arbeitsschritten vorhanden ausführlich Tutorial dokumentiert Funktion zurückgreifen Kommandozeilentool arbeiten programmieren Anforderung Programmierkenntnisse Forschende Verfahren einsetzen möchten minimieren Methode groß Nutzerkreis zugänglich stehen ddw komfortabl Einheitswerkzeug Verfügung Spektrum abdecken linguistisch annotatio Ausgabeformat erzeugen Ziel Bibliothek direkt Anbindung existierend implementierungen verschieden Etablierter Variant Topic Untersuchung resultierenden Modell möchten verschieden evaluierungsverfahren anbieten sowohl intern Verfahren extern vefahren Weglänge begreifen Wörterbuch hieran schließen verschieden Optionen Visualisierung Ergebnis Fokus Entwicklung stehen Gestaltung schlüssig aufeinander aufbauend Programmbefehle einheitlich Syntax folgen Funktion schnell erschließen lässn lang Einarbeitung nutzen Pipeline Zusammenfüg lassen spezifisch Arbeitsschritte bestimmt Topic umsetzen hierbei nutzerinnen nutzer detailliert Anleitung umfangreich Tutorial zurückgreifen Funktion outputs potentiell Kombinatione detailliert dokumentieren anhand Beispiel erläutern,"[('topic', 0.22712034712415013), ('arbeitsschritte', 0.20020517891144932), ('implementierungen', 0.1577386964205488), ('nutzerinnen', 0.14310746502220612), ('dokumentiert', 0.13788535250034004), ('tutorial', 0.12964547028737375), ('einheitlich', 0.12627189079309753), ('arbeitend', 0.12627189079309753), ('zurückgreifen', 0.12627189079309753), ('thema', 0.12532114989236967)]"
2017,DHd2017,poster-HOHEN.xml,Raum und Zeit in Comics: Die Wirkung von Zwischenräumen auf Aufmerksamkeit und empfundene Zeit beim Lesen graphischer Literatur,"Sven Hohenstein (Universität Potsdam, Deutschland); Jochen Laubrock (Universität Potsdam, Deutschland)","Graphische Literatur, Zeitwahrnehmung, Blickbewegungen, Experimentalpsychologie","Visualisierung, Bilder, Literatur, Methoden, Text","Aufgrund der Kombination von Text und Bild stellen graphische Literatur und Comics komplexe Medien dar. Diese Hybridität stellt an die Aufmerksamkeit beim Lesen andere Anforderungen als bei rein textbasierten Romanen, da Informationen unterschiedlichen Typs erfasst und verarbeitet werden müssen. Wegen ihrer Konfiguration als eine Folge von Panels werden Comics auch als sequenzielle Kunst bezeichnet. Nach McCloud (1993) spielt der Raum zwischen den Panels, der als ""gutter"" bezeichnet wird, eine Rolle für die Verbindung der einzelnen Panels. Obwohl dieser Raum selbst leer ist, so vergeht doch nach McCloud Zeit zwischen zwei Panels. Diesem Postulat hinsichtlich der Empfindung, die durch den ""gutter"" ausgelöst wird, haben wir uns im Rahmen einer empirischen Studie gewidmet. Die Wirkung zusätzlichen, leeren Raums zwischen Panels für die subjektive Wahrnehmung von Zeit beim Lesen graphischer Literatur haben wir mit kognitionspychologischen Experimenten untersucht. Dieses Vorgehen erlaubt es über die reine Beschreibung des Materials hinaus den subjektiven Eindruck der Leserin bzw. des Lesers zu erfassen. Für diese Experimente stellten wir eine Sammlung von einzelnen Panels aus verschiedenen Comic-Reihen zusammen, beispielsweise ""Astérix"" und ""Donald Duck"". Die Auswahl der Panels erfolgte nach dem Kriterium, dass sie sich horizontal teilen lassen. Bei dieser Teilung wurde ein Panel per Bildbearbeitungssoftware in mehrere kleinere Unterpanels geteilt. Zusammenhängende Textabschnitte blieben dabei ungeteilt. Im ersten Experiment wurde das Material in zwei Bedingungen dargeboten. In der Kontrollbedingung wurden die Panels jeweils ohne Teilung in ihrer ursprünglichen Form auf einem Bildschirm präsentiert. In der zweiten Bedingung wurden die Subpanels hintereinander auf dem Bildschirm gezeigt. Jeder Durchgang endete damit, dass die Probanden gefragt wurde, wieviel Zeit während der Geschichte, die in dem Panel erzählt wird, vergangen ist. Die Antworten der Probanden spiegeln somit deren subjektive Einschätzung der Dauer wider. Obwohl in beiden Bedingungen letztlich dieselben Panels gezeigt wurden, gab es bedeutsame Unterschiede in den Antworten. Die Teilung der Panels führte zu längeren subjektiven Dauern als die Kontrollbedingung. Dieses Ergebnis verdeutlicht den Einfluss der Konfiguration visueller Information auf die Wahrnehmung der Leserin bzw. des Lesers. Um eine detailliertere Analyse der Aufmerksamkeit der Probanden vornehmen zu können, haben wir im zweiten Experiment zusätzlich Blickbewegungen erhoben. Für die Kontrolle der Auswirkungen der Panel-Teilung auf die wahrgenommene Dauer haben wir zudem das Material in einer Weise präsentiert, die ähnlicher zu tatsächlichen Comics ist. Die Subpanels wurden nebeneinander mit zusätzlichem, leerem Zwischenraum angeordnet, so dass das Aussehen einer kurzen Comic-Geschichte mit mehreren Panels gleicht. In der Kontrollbedingung wurden die Panels erneut ungeteilt dargeboten. Erneut wurden die Dauern länger eingeschätzt, wenn die Panels geteilt auf dem Bildschirm erschienen. Die Auswertung der Blickbewegungen ergab ein differenziertes Bild der Aufmerksamkeitsverteilung beim Betrachten der Panels. Die Blickbewegungsmuster unterschieden sich in Hinblick auf die experimentelle Bedingung. Waren die Panels geteilt, so machten die Versuchspersonen mehr Fixationen. Die höhere Anzahl an Fixationen ist somit eine mögliche Ursache für die subjektiv längere verstrichene Zeit. Außerdem zeigte sich eine leichte relative Tendenz zur Fixation nahe dem Zentrum eines jeden Subpanels, die bei geteilten Panels stärker ausgeprägt war. Diese und andere Befunde sprechen dafür, dass die Teilung von Panels die Aufmerksamkeit beim Lesen und Betrachten sowie die Wirkung graphischer Literatur beeinflussen kann.",de,aufgrund Kombination Text Bild stellen graphisch Literatur Comic komplex Medium dar Hybridität stellen Aufmerksamkeit lesen anforderungen rein textbasiert Roman information unterschiedlich Typ erfassen verarbeiten Konfiguration Folge Panel Comic sequenziell Kunst bezeichnen Mccloud spielen Raum Panel Gutter bezeichnen Rolle Verbindung einzeln Panel obwohl Raum leer vergehen Mccloud Panel Postulat hinsichtlich Empfindung Gutter auslösen Rahmen empirisch Studie widmen Wirkung zusätzlich leer Raum Panel subjektiv Wahrnehmung lesen graphisch Literatur kognitionspychologisch experimenten untersuchen vorgehen erlauben rein Beschreibung Material hinaus subjektiv Eindruck Leserin Leser erfassen experiment stellen Sammlung einzeln Panel verschieden beispielsweise astérix Donald duck Auswahl Panel erfolgen Kriterium horizontal teilen lassen Teilung Panel per Bildbearbeitungssoftwar mehrere klein unterpanel teilen zusammenhängende Textabschnitt bleiben ungeteilt Experiment Material Bedingung dargeboen Kontrollbedingung Panel jeweils Teilung ursprünglich Form Bildschirm präsentieren Bedingung Subpanel hintereinander Bildschirm zeigen durchgang enden Probande fragen wieviel Geschichte Panel erzählen vergehen Antwort proband spiegeln somit subjektiv Einschätzung Dauer wider obwohl Bedingung letztlich Panel zeigen bedeutsam Unterschied Antwort Teilung Panel führen lang Subjektiv Dauer Kontrollbedingung Ergebnis verdeutlichen Einfluss Konfiguration visuell Information Wahrnehmung Leserin Leser detaillierter Analyse Aufmerksamkeit Probande Vornehmen Experiment zusätzlich blickbewegung erheben Kontrolle Auswirkung wahrgenommen Dauer zudem Material Weise präsentieren Ähnlicher tatsächlich Comic Subpanel nebeneinander zusätzlich leer Zwischenraum anordnen aussehen kurz mehrere Panel gleichen Kontrollbedingung Panel erneut ungeteilt dargeboen erneut Dauer lang einschätzen Panel teilen Bildschirm erscheinen Auswertung blickbewegung ergeben differenziert Bild Aufmerksamkeitsverteilung Betrachten Panel Blickbewegungsmuster unterscheiden Hinblick experimentell Bedingung Panel teilen machen Versuchsperson Fixation hoch Anzahl fixationen somit möglich Ursache subjektiv lang verstrichen zeigen leicht relativ Tendenz Fixation nahe Zentrum Subpanel geteilt Panel stark ausprägen befunde sprechen Teilung Panel Aufmerksamkeit Lesen betrachten Wirkung graphisch Literatur beeinflussen,"[('panel', 0.6130190318363163), ('subjektiv', 0.21210821603047647), ('teilung', 0.20841073248187628), ('dauer', 0.16968657282438118), ('bildschirm', 0.15630804936140721), ('kontrollbedingung', 0.15630804936140721), ('subpanel', 0.15630804936140721), ('leer', 0.14558909800075978), ('teilen', 0.13511405895198084), ('bedingung', 0.13297714717827308)]"
2017,DHd2017,panel-REITE.xml,Aktuelle Herausforderungen der Digitalen Dramenanalyse,"Marcus Willand (Universität Stuttgart); Peer Trilcke (Universität Potsdam); Christof Schöch (Universität Würzburg); Nanette Rißler-Pipka (Katholische Universität Eichstätt-Ingolstadt); Nils Reiter (Universität Stuttgart); Frank Fischer (Higher School of Economics, Moskau)","Dramenanalyse, Methodenvergleich, Labor, Drama","Inhaltsanalyse, Strukturanalyse, Beziehungsanalyse, Modellierung, Annotieren, Bereinigung, Netzwerkanalyse, Literatur, Metadaten, Methoden, Forschungsprozess, Software, Standards, Text","Das hier vorgeschlagene Panel greift mit der Digitalen Dramenanalyse einen sich derzeit dynamisch entwickelnden Bereich der digitalen Literaturwissenschaften auf. Es setzt sich erstens zum Ziel, aktuelle Herausforderungen der Digitalen Dramenanalyse auf verschiedenen Ebenen vorzustellen, wobei insbesondere die Ebenen der dramatischen Gattung, der Netzwerkstrukturen und der dramatischen Figuren im Zentrum stehen werden. Zweitens möchte das Panel mit dem Publikum mögliche Lösungsansätze diskutieren, unter anderem durch Bezug auf vielfältige, vorhandene Erfahrungen mit der Analyse narrativer Texte. In der Summe wird das Panel einerseits eine Zwischenbilanz zum Stand der Forschung anbieten, andererseits auch im Sinne einer Konsolidierung des Forschungsfelds eine Agenda für die weitere Entwicklung formulieren, bei der es nicht zuletzt darum geht, Szenarien einer integrativen, mithin diverse methodische Ansätze synergetisch zusammenführenden Forschung, zu diskutieren. Dazu wird das Panel eine Art Laborsituation fingieren, in der die Erkenntnisziele, Möglichkeiten und Grenzen unterschiedlicher methodischer Zugänge zu dem titelgebenden Forschungsbereich der digitalen Dramenanalyse zu Tage treten sollen: In den In drei Kurzvorstellungen sollen die folgenden Methoden von jeweils einer Forschergruppe des Panels vorgestellt werden, wobei zur besseren Vergleichbarkeit der drei methodisch unterschiedlich aufgestellten Arbeitsgruppen zeitlich und gattungsbezogen vergleichbare Textsammlungen analysiert werden. Zwar werden diese Verfahren jeweils anhand eines individuellen Teilkorpus vorgestellt, es ist jedoch zu berücksichtigen, dass sie alle auf der statistischen Analyse größerer Textmengen basieren. Der Einsatz von Zweitens zeigt sich, dass sich einzelne dramatische Untergattungen wie Tragödie, Komödie oder Tragikomödie zwar in Bezug auf die jeweils dominanten Einzel Die in den quantitativen Sozialwissenschaften entwickelten Verfahren der Netzwerkforschung zielen auf eine formale Analyse sozialer Strukturen (Wasserman / Faust 1994). Angewandt auf literarische Texte ermöglichen sie Strukturbeschreibungen, die aus einer signifikant anderen Perspektive erfolgen als traditionelle literaturwissenschaftliche Verfahren der semantikbasierten Strukturanalyse (z.B. Titzmann 1977), insofern sie nicht die semantische Organisation literarischer Texte, sondern die ästhetische Modellierung sozialer Formationen im Medium der Literatur analysieren (Trilcke 2013). Ob ihres stark formalisierten Charakters operieren netzwerkanalytische Konzeptualisierungen dabei zunächst mit epistemischen Objekten, die sich erheblich von den Objekten der ""klassischen"" Literaturwissenschaft unterscheiden. Gerade deshalb aber bilden solche Konzeptualisierungen ein ebenso attraktives wie kontroverses Experimentierfeld für computerbasierte Zugänge zum Gegenstandsbereich ""Literatur"", die nicht nur neue Antworten auf alte Fragen finden, sondern dezidiert Computerlinguistische Methoden wie Named Entity Recognition und Koreferenzresolution (cf. Poesio et al. 2016) erlauben die Erkennung von Figurenreferenzen in der Rede dramatischer Figuren. Die erkannten Referenzen wiederum können genutzt werden, um den Stellenwert einer Figur innerhalb des Gesamttextes zu identifizieren. Neben der direkten Präsenz von Figuren (im Sinne von: Figur spricht; siehe auch das Problem der sog. Konfiguration, hierzu Ilsemann 1995, 2008) lässt sich damit auch die indirekte Präsenz (über eine Figur wird gesprochen) messen. Im Falle von Unser Beitrag zum Panel diskutiert zum einen die Herausforderungen an die maschinelle Sprachverarbeitung, wenn sie auf Dramentexte angewendet wird (Blessing et al. 2016). Zum anderen wollen wir untersuchen, inwiefern Autorinnen und Autoren sprachliche Eigenheiten der Figuren nutzen, um diese zu charakterisieren und z.B. als bestimmten Figurentypus (zärtlicher Vater, Hanswurst usw.; cf. Sørensen 1984, Aust 1989, Kord 2009) zu kennzeichnen. Die unterschiedlichen methodischen Zugänge zu dramatischen Texten erlauben zwar eine direkte Gegenüberstellung und Diskussion der drei Forschungsansätze, ihrer Prämissen, aber auch der Relevanz ihrer Ergebnisse für literaturtheoretische oder -historische Fragestellungen. Die vorgestellten Verfahren sollen letztlich aber nicht als konkurrierend oder unverbunden gedacht werden, sondern als Beiträge zu einem gemeinsamen Ziel: dem differenzierteren literaturwissenschaftlichen Verständnis dramatischer Texte. Vor dem Hintergrund der das Panel leitenden Idee einer Bilanzierung bisheriger und Konsolidierung aktueller Forschung auf dem Gebiet der ‚óè Jede der drei Methoden verfolgt spezifische Fragen und birgt spezifische Herausforderungen. In welchem Maße gibt es gemeinsame Forschungsziele, zu denen jede der Methoden einen Beitrag leisten kann? Können die verschiedenen Methoden beispielsweise einen Beitrag zu einer empirisch gesicherten Gattungsdifferenzierung oder für die literaturgeschichtliche Periodisierung leisten? ‚óè Wie können Ergebnisse, die mit unterschiedlichen methodischem Vorgehen gewonnen wurden, in Bezug zueinander gesetzt werden? ‚óè Welche Ressourcen (insbesondere Textsammlungen) liegen vor und wie kann die Verfügbarkeit geeigneter Ressourcen für die Digitale Dramenanalyse zukünftig verbessert werden? Wie können die teils unterschiedliche Anforderungen der Methoden an die Formate von Daten und Metadaten aufgefangen werden? ‚óè Welche konzeptuellen und datenbezogenen Standards für dokumentbezogene Metadaten und strukturelle oder semantische, lokale Annotationen liegen vor, wie kann die Standardisierung (bspw. durch Annotationsrichtlinien) weiter gefördert werden? ‚óè Welche Tools sind für die digitale Dramenanalyse derzeit verfügbar, wie könnte die Tool-Entwicklung zielgerichtet gefördert werden? Welche generischen Tools könnten produktiv eingesetzt werden, wie könnte der Einsatzbereich vorhandener Tools erweitert (Adaptierbarkeit, Übertragbarkeit) und so eine breitere Nutzerbasis geschaffen werden? Indem das Panel die Vielfalt digitaler Dramenanalysen vorführt und die explorative Kraft methodischer Innovation durch die Digital Humanities für die Literaturwissenschaften betont, möchten wir die fingierte ""Laborsituation"" im Sinne der theoretischen und wissenschaftspolitischen Implikationen einer auf Überprüfbarkeit und Wiederholbarkeit angelegten Wissenschaft verstanden wissen.",de,vorgeschlagen Panel greifen digital Dramenanalyse derzeit dynamisch entwickelnd Bereich digital Literaturwissenschaften setzen erstens Ziel aktuell Herausforderung digital Dramenanalyse verschieden eben Vorzustelle wobei insbesondere Ebene dramatisch Gattung Netzwerkstruktur dramatisch Figur Zentrum stehen zweitens Panel Publikum möglich Lösungsansätze diskutieren Bezug vielfältig vorhanden Erfahrung Analyse narrativ Text Summe Panel einerseits Zwischenbilanz Stand Forschung anbieten andererseits Sinn Konsolidierung Forschungsfeld Agenda Entwicklung formulieren zuletzt Szenarien integrativ mithin diverser methodisch Ansatz synergetisch zusammenführend Forschung diskutieren Panel Art Laborsituation fingieren erkenntnisziele Möglichkeit Grenze unterschiedlich methodisch Zugänge titelgebend forschungsbereich digital Dramenanalyse treten Kurzvorstellunge folgend Methode jeweils Forschergruppe Panel vorstellen wobei gut Vergleichbarkeit methodisch unterschiedlich aufgestellt Arbeitsgrupp zeitlich gattungsbezog vergleichbar Textsammlung analysieren Verfahren jeweils anhand individuell Teilkorpus vorstellen berücksichtigen statistisch Analyse groß Textmeng basieren Einsatz zweitens zeigen einzeln dramatisch untergattung Tragödie Komödie Tragikomödie Bezug jeweils dominanten einzel quantitativ sozialwissenschaften entwickelt Verfahren Netzwerkforschung zielen formal Analyse sozial Struktur Wasserman Faust angewandt literarisch Text ermöglichen strukturbeschreibungen Signifikant Perspektive erfolgen traditionell literaturwissenschaftlich Verfahren semantikbasiert Strukturanalyse Titzmann insofern semantisch Organisation literarisch Text ästhetisch Modellierung sozial formationen Medium Literatur analysieren Trilcke stark formalisiert Charakter operieren netzwerkanalytisch Konzeptualisierung epistemisch Objekt erheblich objekten klassisch literaturwissenschaft unterscheiden bilden Konzeptualisierung attraktiv kontrovers Experimentierfeld Computerbasierte zugänge gegenstandsbereich Literatur Antwort alt Frage finden dezidiert computerlinguistisch Methode named entity Recognition Koreferenzresolution cf Poesio et erlauben Erkennung Figurenreferenze Rede dramatisch Figur erkannt Referenz wiederum nutzen Stellenwert Figur innerhalb Gesamttext identifizieren direkt Präsenz Figur Sinn Figur sprechen sehen Problem Konfiguration hierzu Ilsemann lässen indirekt Präsenz Figur sprechen messen Fall Beitrag Panel diskutieren Herausforderung maschinell Sprachverarbeitung dramentexte anwenden blessing et untersuchen inwiefern autorinnen Autor sprachlich Eigenheit Figur nutzen Charakterisieren bestimmt Figurentypus zärtlich Vater hanswurst cf Sørense Aust Kord kennzeichnen unterschiedlich methodisch Zugänge dramatisch texten erlauben direkt Gegenüberstellung Diskussion Forschungsansätz Prämisse Relevanz Ergebnis literaturtheoretisch Fragestellung vorgestellt Verfahren letztlich konkurrierend unverbunden denken beiträge gemeinsam Ziel differenziert literaturwissenschaftlich Verständnis dramatisch Text Hintergrund Panel leitend Idee Bilanzierung bisherig Konsolidierung aktuell Forschung Gebiet óè Methode verfolgen spezifisch Frage bergen spezifisch Herausforderung Maß gemeinsam Forschungsziel Methode Beitrag leisten verschieden Methode beispielsweise Beitrag empirisch gesichert Gattungsdifferenzierung literaturgeschichtlich Periodisierung leisten óè Ergebnis unterschiedlich methodisch vorgehen gewinnen Bezug Zueinander setzen óè Ressource insbesondere Textsammlung liegen Verfügbarkeit geeignet Ressource digital Dramenanalyse zukünftig verbessern teils unterschiedlich Anforderung Methode Formate daten Metadat auffangen óè Konzeptuell datenbezogen Standard dokumentbezogen Metadat strukturell semantisch lokal Annotation liegen Standardisierung annotationsrichtlinien fördern óè Tools digital Dramenanalyse derzeit verfügbar zielgerichten fördern generisch Tools können produktiv einsetzen einsatzbereich vorhanden Tools erweitern Adaptierbarkeit übertragbarkeit breit Nutzerbasis schaffen Panel Vielfalt digital Dramenanalyse vorführen explorativ Kraft methodisch Innovation Digital Humanitie Literaturwissenschaften betonen möchten fingiert Laborsituation Sinn Theoretisch wissenschaftspolitisch implikationen überprüfbarkeit Wiederholbarkeit angelegt Wissenschaft verstehen wissen,"[('óè', 0.2933071757277827), ('panel', 0.29060490413283796), ('dramenanalyse', 0.2504883402487771), ('dramatisch', 0.20922666423568698), ('methodisch', 0.16933350861666127), ('figur', 0.16272831595882484), ('digital', 0.11808836345853592), ('konsolidierung', 0.11732287029111307), ('laborsituation', 0.11732287029111307), ('zugänge', 0.11601271645592093)]"
2017,DHd2017,poster-ZIRKE.xml,TEASys (Tübingen Explanatory Annotations System): Die erklärende Annotation literarischer Texte in den Digital Humanities,"Angelika Zirker (Eberhard Karls Universität Tübingen, Deutschland); Matthias Bauer (Eberhard Karls Universität Tübingen, Deutschland)","Annotation, Literaturwissenschaft, Hermeneutik, Lehrinstrument","Entdeckung, Gestaltung, Inhaltsanalyse, Strukturanalyse, Annotieren, Kontextsetzung, Theoretisierung, Community-Bildung, Stilistische Analyse, Kollaboration, Lehre, Text","Das Poster präsentiert das Lehr- und Forschungsprojekt TEASys (Tübingen Explanatory Annotations System) zur erklärenden Annotation literarischer Text in den Digital Humanities. Die erklärende Annotation wird dabei als Anreicherung bislang vor allem literarischer Texte um Informationen verstanden, die zum Textverständnis beitragen bzw. es überhaupt ermöglichen, d.h. sie dienen etwa der Überwindung von historischer Distanz (vgl. Hanna 1991). Eine Anwendung des Systems auf andere (nicht-literarische) Texte wird derzeit vorbereitet. TEASys arbeitet mit verschiedenen Kategorien der erklärenden Annotation sowie ihrer Präsentation auf mehreren Ebenen, die sich etwa bezüglich ihrer Komplexität unterscheiden und aufeinander aufbauen (vgl. Bauer & Zirker 2015). Die Kategorien der erklärenden Annotation sind Sprache, Form, Intratextualität, Intertextualität, Kontext und Interpretation. Die Interpretation ergibt sich dabei aus den Informationen, die aus den anderen Kategorien zum besseren Verständnis an den Text herangetragen werden. Weitere Kategorien, die auf einer Meta-Ebene angesiedelt sind, beinhalten philologische Informationen (z.B. zu Varianten) sowie Fragen oder Anmerkungen (z.B. zu Items, zu denen bislang keine Informationen gefunden werden konnten sowie zur bislang bereits stattgefundenen Recherche zu einzelnen Items). Letztere Kategorie ist vor allem auch im Hinblick auf Fragen der Nachhaltigkeit essentiell. Die Ebenen der Annotation bauen aufeinander auf, d.h. die erste von insgesamt drei Ebenen bietet Informationen an, die das Textverstehen grundsätzlich ermöglichen, und die weiteren Ebenen nennen weitere, meist komplexere und ausführliche Informationen. TEASys geht auf ein Peerlearning-Projekt zurück, das in Tübingen seit 2011 besteht und von Studierenden der englischen Literatur und weiteren geisteswissenschaftlichen Fächern getragen und von den Leitern des Forschungsprojekts (Prof. Dr. Matthias Bauer & PD Dr. Angelika Zirker) wissenschaftlich unterstützt wird. Es gibt derzeit vier Peerlearning-Gruppen, die sich mit Texten verschiedener Gattungen und Epochen beschäftigen und diese kollaborativ annotieren (zur Kollaboration in den DH s. z.B. McCarty 2012; Meister 2012; Stroud 2006). Das Forschungsprojekt widmet sich vor allem der Theoriebildung zur erklärenden Annotationen und der darauf aufbauenden Entwicklung eines best-practice-Modells, das wiederum auf die Theorie rückwirken soll (s. dazu Bauer & Zirker 2015). Die DH-Komponente liegt vor allem in der entsprechenden Aufbereitung und Visualisierung der erklärenden Annotationen für das digitale Medium sowie der darin möglichen Dynamik (s. Eggert 2009): Annotationen sind, entgegen ihrer Darstellung im Buch, ständig revidier- und erweiterbar und somit einer möglichst großen Rezipientengruppe offen, die umgekehrt für eine beständige Qualitätskontrolle sorgt. Ferner ermöglicht die digitale Repräsentation das Filtern von Informationen: je nach Bedarf können z.B. lediglich Annotationen zur Intertextualität angezeigt werden. Das Poster stellt sowohl den Aufbau von TEASys als best-practice-Modell vor wie auch seine theoretischen Grundlagen und Beispielannotationen aus dem Peerlearning-Projekt, die von Studierenden erstellt wurden. Es macht deutlich, wie grundlegende hermeneutische Fragestellungen in das digitale Medium übernommen und dort abgebildet werden können (vgl. Drucker 2012) 'und wie umgekehrt wiederum die digitale Präsentation aufgrund der theoretischen Überlegungen verbessert werden kann.",de,Poster präsentieren Forschungsprojekt Teasys tübingen explanatory annotation System erklärend Annotation literarisch Text Digital Humanitie erklärend Annotation Anreicherung bislang literarisch Text Information verstehen Textverständnis beitragen ermöglichen dienen Überwindung historisch Distanz Hanna Anwendung System Text derzeit vorbereiten Teasys arbeiten verschieden kategorien erklärend Annotation Präsentation mehrere Ebene bezüglich Komplexität unterscheiden aufeinander aufbauen Bauer Zirker kategorien erklärend Annotation Sprache Form Intratextualität Intertextualität Kontext Interpretation Interpretation ergeben Information Kategorie gut Verständnis Text herangetragen Kategorie ansiedeln beinhalen philologisch Information varianten Frage anmerkung items bislang Information finden bislang stattgefunden Recherche einzeln item letzterer Kategorie Hinblick Frage Nachhaltigkeit essentiell Ebene Annotation bauen aufeinander insgesamt Ebene bieten Information textverstehen grundsätzlich ermöglichen Ebene nennen meist komplex ausführlich Information Teasys Tübingen bestehen studierende englisch Literatur geisteswissenschaftlich fächern tragen leiter Forschungsprojekt Prof dr Matthias Bauer pd dr Angelika zirk wissenschaftlich unterstützen derzeit Text verschieden gattung epoch beschäftigen kollaborativ annotieren Kollaboration dh mccarty Meister stroud Forschungsprojekt widmen Theoriebildung erklärend Annotation aufbauend Entwicklung wiederum Theorie rückwirken Bauer Zirker liegen entsprechend Aufbereitung Visualisierung erklärend Annotation digital Medium möglich Dynamik eggern annotationen entgegen Darstellung Buch ständig erweiterbar somit möglichst Rezipientengruppe umgekehrt beständig Qualitätskontrolle sorgen ferner ermöglichen digital Repräsentation Filtern Information Bedarf lediglich annotation Intertextualität anzeigen Poster stellen sowohl Aufbau Teasys theoretisch Grundlage Beispielannotation studierender erstellen deutlich grundlegend hermeneutisch Fragestellung digital Medium übernehmen abbilden Drucker umgekehrt wiederum digital Präsentation aufgrund theoretisch Überlegung verbessern,"[('erklärend', 0.40401664332903325), ('teasys', 0.31873990853284484), ('annotation', 0.23000487058308647), ('information', 0.19660406249246548), ('bauer', 0.1946368670313028), ('zirker', 0.15936995426642242), ('forschungsprojekt', 0.14595896485245247), ('ebene', 0.13815299064125722), ('tübingen', 0.1297579113542019), ('dr', 0.1297579113542019)]"
2017,DHd2017,vortrag-HERRM.xml,"Das ""Was-bisher-geschah"" von KOLIMO. Ein Update zum Korpus der literarischen Moderne","Berenike Herrmann (Universität Göttingen, Deutschland); Gerhard Lauer (Universität Göttingen, Deutschland)","Korpus, Annotation, Moderne, POS, Stil","Teilen, Sammlung, Strukturanalyse, Modellierung, Annotieren, Kontextsetzung, Theoretisierung, Bereinigung, Bearbeitung, Archivierung, Veröffentlichung, Stilistische Analyse, Projektmanagement, Webentwicklung, Konservierung, Artefakte, Daten, Infrastruktur, Sprache, Link, Literatur, Metadaten, benannte Entitäten (named entities), Text","Der vorgeschlagene Beitrag dokumentiert den Fortschritt beim Aufbau unseres digitalen Korpus der literarischen Moderne (KOLIMO), das im Herbst 2016 in der Beta-Version veröffentlicht werden soll (abrufbar unter https://kolimo.uni-goettingen.de/). Im Fokus des Beitrags stehen das Verfahren zur Aufbereitung der Texte (insb. Format und Metadaten in TEI) und das linguistische Tagging (POS). Als Teil des laufenden Projektes Q-LIMO (Quantitative Analyse der literarischen Moderne) ist KOLIMO ein repräsentatives und computerlinguistisch solide aufbereitetes Korpus von narrativen fiktionalen Erzähltexten der literarischen Epoche der Moderne. Um durch stratifiziertes Sampling Repräsentativität (verstanden als ""extent to which a sample includes the full range of variability in a population""; vgl. Biber 1994) zu ermöglichen, umfasst das Korpus ein möglichst breites Spektrum der literarischen Moderne, verteilt über kanonische und nichtkanonische Texte. So wurden in das Korpus bislang ca. 596.000.000 Wörter aus frei zugänglichen Repositorien importiert (s. Abbildung 1). Abbildung 1 Gesamtanzahl Wörter aus den drei Hauptressourcen (Zwischenstand August 2016) Die Datenbank umfasst so neben Texten aus TextGrid und Gutenberg-DE (s. Abbildung 2) und dem DTA auch eine wachsende Zahl von Retrodigitalisaten. Das Sampling ist nicht zuletzt dadurch beeinflusst, dass KOLIMO auch das Kafka/Referenzkorpus (KAREK) beinhaltet, welches zum Ziel hat, Kafkas Texte und Texte, die Kafkas Schreibprozess beeinflusst haben könnten, möglichst umfangreich abzubilden (vgl. Herrmann / Lauer 2016a,b). Abbildung 2 Screenshot KOLIMO-WebApp: Anzahl Wörter, Autoren und Einträge aus TextGrid & Gutenberg-DE (ohne DTA und andere Quellen, Stand August 2016) Um philologischen Ansprüchen an den editorischen Status literarischer Texte und die Abbildung von Epochen sowie Gattungskonzepten zu genügen, war eine hohe Genauigkeit und Konsistenz bei der informatischen Vorverarbeitung Textmarkup (XML-TEI) inklusive der Metadaten (Autor, Entstehungszeitpunkt und Gattung) besonders wichtig. Gerade die Auszeichnung der genannten Metadaten stellt eine Schnittstelle zwischen den informatischen und philologischen Dimensionen unseres Projektes dar: so sind Metadaten (a) die unabhängigen Variablen unserer stilistischen Analyse und (b) variieren in den von uns importierten Korpus-Ressourcen stark in qualitativer und quantitativer Hinsicht (Fehler, missing entries, unterschiedliche Ontologien). Der vorgeschlagene Beitrag wird so erstens einen kurzen Einblick in unsere Vorgehensweise geben, wobei Kriterien der Nachhaltigkeit berücksichtigt werden: Zweitens wird der Beitrag unser Vorgehen bezüglich der linguistischen Anreicherung zusammenfassen: Unter der Annahme, dass Stil quantitativ beschreibbar ist (vgl. Herrmann / van Dalen-Oskam / Schöch 2015), und dass Wortarten verlässliche Indikatoren für Register und Genrevariation sind (vgl. z.B. Biber / Conrad 2009), haben wir uns für die linguistische Annotation auf POS (STTS Tagset; vgl. Schiller / Teufel / Thielen 1995) entschieden. POS sind im Vergleich mit anderen Variationsmarkern durch eine relativ akkurate automatische Annotation besonders praktikabel. Das Webinterface liefert variablen Zugriff auf die annotierten Daten, u.a. eine Volltextansicht (siehe Abbildung 3); geplant sind zur Veröffentlichung die Exportierbarkeit in .csv-Files und TCF-Format. Abbildung 3 Screenshot KOLIMO WebApp Textview POS-Tagging Zwar liefern bereits trainierte Modelle von einigen Taggern (z.B. TreeTagger) eine gute Genauigkeit für das gegenwärtige Standarddeutsch, angewendet auf ältere Sprachstufen oder vom Standarddeutschen abweichende Register wie ""Literatur"" sinkt die Genauigkeit jedoch. Ein bereits auf POS annotiertes Korpus ist das Deutsche Textarchiv (DTA, Berlin-Brandenburgische Akademie der Wissenschaften 2016), ein Referenzkorpus für das Deutsche, das sowohl historische Sprachstufen als auch das Register ""Literatur"" enthält. Die POS-Annotation baut hier auf fehlertoleranten linguistischen Analyse historischer Texte auf und verwendet ein Tool zur Morphologisierung (Jurish 2012), ist allerdings hinsichtlich ihrer Qualität noch nicht umfassend evaluiert worden. Ausgehend von diesem Datensatz haben wir zwei Strategien verfolgt: (1) Ein epochensensitives POS-Tagging, das verschiedene Tagger auf dem Datensatz des DTA, aber auf unterschiedlichen literarischen Epochen trainiert (vgl. Paluch et al. in Vorbereitung); (2) eine Überprüfung der Qualität der DTA-POS-Tags durch quantitative und qualitative Verfahren. In Strategie (1) machen wir uns zunutze, dass Annotationsgenauigkeit erhöht werden kann, wenn Tagger auf verschiedene Register/Sprachstände trainiert und diese trainierten Modelle dann auf noch nicht trainierte Texte des gleichen Registers angewendet werden (vgl. Giesbrecht / Evert). Für KOLIMO haben wir u.a. den TreeTagger (vgl. Schmid 1994), Perceptron (vgl. Rosenblatt 1958) und MarMoT (vgl. Müller / Schmid / Schütze 2013) verwendet. Durch die Wahl unterschiedlicher Tagger soll gewährleistet werden, dass die Genauigkeit der POS-Annotation maximiert werden kann, indem nur derjenige Tagger mit den besten Ergebnissen pro Register verwendet wird. Die Auswahl der Tagger basierte einerseits darauf, dass sie unterschiedliche Prinzipien benutzen: So funktioniert der TreeTagger nach dem Hidden Markov Model (HMM, vgl. Baum / Petrie 1966), MarMot nach dem Prinzip der Conditional Random Fields (DRF, vgl. Hammersly / Clifford 1971) und Perceptron nach dem neuronaler Netzwerke. Der Grund für die Wahl des TreeTaggers war zudem seine Prävalenz in der Forschungsliteratur, die nicht zuletzt durch gute Ergebnisse begründet scheint (vgl. Dipper 2012; Giesbrecht / Evert 2009). In einem ersten Schritt (vgl. Paluch et al. in Vorbereitung) wurden hier bereits getaggte Texte aus dem DTA in fünf Epochen geordnet. Neben der Moderne umfassten diese zu Vergleichszwecken auch Barock, Aufklärung, Romantik, und Realismus. Für die Einteilung der Epochen in Zeitperioden sowie der Einteilung von Autoren zu bestimmten Epochen wurden einschlägige Literaturgeschichten zu Rate gezogen (u.a. Beutin 2001; Jørgensen / Bohnen / Øhrgaard 1990; Meid 2009; Schulz 2000; Sprengel 1998, 2004). Anschließend wurden die Tagger auf jeweils eine Epoche trainiert, indem die Texte randomisiert in Trainings- und Evaluationstexte getrennt wurden und eine k-fold cross validation (vgl. Witten / Elbe 2005) für jeden Tagger durchgeführt wurde. Die Ergebnisse (vgl. auch Paluch et al. in Vorbereitung) weisen auf eine gute Genauigkeit insbesondere von Perceptron hin, müssen aber unter dem Vorbehalt betrachtet werden, dass der Status des DTA als Goldstandard für POS-Tagging noch fraglich ist. Hier setzen wir mit Strategie (2) an, mit der wir zunächst für alle POS-Tags Übereinstimmung und Abweichung (Matches und Missmatches) des Outputs des Tree-Taggers und MarMots mit dem DTA-Datensatz vergleichen. Aufbauend auf diese quantitative Überprüfung der einzelnen Tag-Zuweisung evaluieren wir zudem händisch Stichproben der Nichtübereinstimmungen in der Annotation der einzelnen Tags. Unsere quantitative Überprüfung ergibt eine generelle Übereinstimmung mit dem DTA-Datensatz in POS-Tags für den TreeTagger und den Marmot Tagger von jeweils 80%. Die generelle Übereinstimmung zwischen den Tags des TreeTaggers und denen des MarMot Taggers hingegen liegt bei 0.78%. Tabelle 1 zeigt Ergebnisse aus der Analyse der Übereinstimmungen (Matches) und Abweichungen (Missmatches) bei der POS-Tagzuweisung von TreeTagger (TT) und MarMot (MM) im Vergleich mit den Tags des DTA. Abgebildet sind hier solche Fälle pro POS-Tag, in denen TT und MM übereinstimmen, aber vom DTA abweichen. Die Tabelle listet die elf POS-Tags, die (von TT und MM gemeinsam) die proportional den höchsten Anteil der Abweichung vom DTA ausmachen. Tabelle 1 Abweichung zu POS-Tags des DTA (Übereinstimmung MM und TT) *STTS Tagset Aufbauend auf diesen Daten wird im nächsten Schritt die tatsächliche Qualität der bereits vorhandenen DTA-Tags für den Datensatz der literarischen Texte evaluiert. Auf der Grundlage von randomisiertem Sampling verbessern wir die POS-Annotationen bei tatsächlichen Fehlern händisch, um in der Folge u.a. eigene Sprachmodelle für unser spezifisches Korpus narrativer Texte zu trainieren. So soll schließlich unter Nutzung vorhandener Ressourcen ein Silber- oder sogar Goldstandard für das POS-Tagging historischer literarischer Texte des Deutschen erreicht werden. KOLIMO wird in der Beta-Version zur Tagung veröffentlicht (s. Gleichzeitig planen wir eine detaillierte Dokumentation der Arbeitsschritte zu veröffentlichen, die ähnlichen Projekten als Leitfaden zur Verfügung zu stehen soll. Unser Projekt dokumentiert in seinem gegenwärtigen Status Entscheidungen auf verschiedenen konzeptionellen, analytischen und prozeduralen Ebenen. Es zeigt, dass der Aufbau eines digitalen literarischen Korpus, das den synchronen und diachronen quantitativen Vergleich einer Schwerpunktepoche erlauben soll, bei Weitem keine triviale Aufgabe darstellt. So wurde zum Beispiel deutlich, wie Hypothesen zur Konstitution von Epochen, Autorschaft und Gattungen die Korpuskompilation steuern ‚Äì und deshalb auf einer möglichst präzisen Modellierung der zugrundeliegenden textwissenschaftlichen Theorien fußen sollten. Gleichzeitig sind Metadaten (u. a. Autor, Titel, Publikationsdatum, Publikationsort, Gattung) und linguistische Parameter (wie POS) gerade die Ansatzpunkte, an denen philologische Fragestellungen in präzise und praktikable Kategorien umgewandelt werden können. Nicht zuletzt deshalb sollten literarische Daten in flexiblen Architekturen gespeichert werden, die zusätzliche Annotationsebenen zulassen ‚Äì denn hermeneutische Erkenntnisprozesse stellen eine erwachsene Stärke der Geisteswissenschaften dar, die auch im digitalen Zeitalter einen explizit modellierten Platz einnehmen muss.",de,vorgeschlagen Beitrag dokumentieren fortschreiten Aufbau unser digital Korpus literarisch Moderne Kolimo Herbst veröffentlichen abrufbar Fokus Beitrag stehen Verfahren Aufbereitung Text Format metadaten tei linguistisch Tagging pos laufend projekt quantitativ Analyse literarisch Moderne Kolimo repräsentativ computerlinguistisch solide aufbereitet Korpus narrativ fiktionalen Erzähltext literarisch Epoche Moderne stratifiziertes Sampling Repräsentativität verstehen Extent to which Sample includ -- full range of variability population biber ermöglichen umfassen Korpus möglichst breit Spektrum literarisch Moderne verteilen kanonisch nichtkanonisch Text Korpus bislang wört frei zugänglich Repositorie importieren Abbildung Abbildung Gesamtanzahl Wörter Hauptressource Zwischenstand August Datenbank Umfasst Text Textgrid Abbildung Dta wachsend Zahl retrodigitalisaten Sampling zuletzt beeinflussen Kolimo Kafka Referenzkorpus Karek beinhalten Ziel Kafka Text Text Kafka Schreibprozess beeinflussen können möglichst umfangreich abzubild Herrmann Lauer -- Abbildung Screenshot Anzahl Wörter Autor einträge Textgrid Dta quellen stehen August philologisch Ansprüch editorisch status literarisch Text Abbildung epoche gattungskonzepen genügen hoch Genauigkeit Konsistenz informatisch Vorverarbeitung textmarkup inklusive Metadat Autor entstehungszeitpunken Gattung wichtig Auszeichnung genannt metadaten stellen Schnittstelle informatisch Philologische dimension unser projektes dar Metadat unabhängig Variable stilistisch Analyse b variieren importiert stark qualitativer quantitativer Hinsicht Fehler Missing entreisen unterschiedlich Ontologi vorgeschlagen Beitrag erstens kurz Einblick vorgehensweise geben wobei Kriterien Nachhaltigkeit berücksichtigen zweitens Beitrag vorgehen bezüglich linguistisch Anreicherung zusammenfassen Annahme Stil quantitativ beschreibbar Herrmann van schöch wortaren verlässlich indikatoren Register Genrevariation biber Conrad linguistisch Annotation pos stts Tagset Schiller Teufel Thielen entscheiden pos Vergleich variationsmarkern relativ akkurat automatisch Annotation praktikabel Webinterface liefern variabl Zugriff annotierter daten volltextansicht sehen Abbildung planen Veröffentlichung Exportierbarkeit Abbildung Screenshot Kolimo webapp textview liefern trainiert Modell Tagger treetagg Genauigkeit gegenwärtig standarddeutsch anwenden alt sprachstufen standarddeutsch abweichend Register Literatur sinken Genauigkeit pos annotiert Korpus deutsch Textarchiv dta Akademie Wissenschaft Referenzkorpus deutsch sowohl historisch sprachstufen Register Literatur enthalten bauen fehlertolerant linguistisch Analyse historisch Text verwenden Tool Morphologisierung Jurish hinsichtlich Qualität umfassend evaluieren ausgehend Datensatz strategien verfolgen epochensensitiv verschieden Tagger Datensatz dta unterschiedlich literarisch Epoche trainieren Paluch et Vorbereitung Überprüfung Qualität quantitativ qualitativ Verfahren Strategie zunutze Annotationsgenauigkeit erhöhen tagg verschieden Register Sprachstände trainieren trainiert Modell trainiert Text gleich Register anwenden Giesbrecht evern Kolimo Treetagger Schmid Perceptron Rosenblatt Marmot Müller Schmid schütze verwenden Wahl unterschiedlich Tagger gewährleisten Genauigkeit maximieren Tagger Ergebnis pro Register verwenden Auswahl Tagger basieren einerseits unterschiedlich prinzipien benutzen funktionieren Treetagger hidd Markov Model hmm Baum Petrie Marmot Prinzip Conditional Random Fields drf Hammersly Clifford Perceptron Neuronaler netzwerken Grund Wahl Treetagger zudem Prävalenz Forschungsliteratur zuletzt Ergebnis begründen scheinen Dipper Giesbrecht evern Schritt Paluch et Vorbereitung getaggt Text Dta Epoche ordnen modern Umfassten vergleichszwecken Barock Aufklärung Romantik Realismus Einteilung Epoche Zeitperiod Einteilung Autor bestimmt epochen einschlägig literaturgeschichten Rate ziehen Beutin jørgensen bohnen øhrgaard meid schulz sprengel anschließend Tagger jeweils epoche trainieren Text randomisieren evaluationstext trennen Cross Validation Witt Elbe Tagger durchführen Ergebnis paluch et Vorbereitung weisen Genauigkeit insbesondere Perceptron Vorbehalt betrachten Status dta Goldstandard fraglich setzen Strategie übereinstimmung Abweichung match missmatch Output Marmot vergleichen aufbauend quantitativ Überprüfung einzeln evaluieren zudem händisch Stichprobe nichtübereinstimmungen Annotation einzeln tags quantitativ Überprüfung ergeben generell Übereinstimmung Treetagger Marmot tagg jeweils generell Übereinstimmung tags Treetagger Marmot Tagger hingegen liegen Tabell zeigen Ergebnis Analyse übereinstimmung match abweichungen missmatch Treetagger tt Marmot mm Vergleich tags dta abbilden Fall pro tt mm übereinstimmen Dta abweichen Tabelle listen tt mm gemeinsam Proportional hoch Anteil Abweichung Dta ausmachen Tabelle Abweichung dta übereinstimmung mm tt Stts Tagset Aufbauend daten nächster Schritt tatsächlich Qualität vorhanden Datensatz literarisch Text evaluieren Grundlage Randomisiertem Sampling verbessern tatsächlich fehlern Händisch Folge Sprachmodelle spezifisch Korpus narrativ Text trainieren schließlich Nutzung vorhanden Ressource sogar Goldstandard Historischer literarisch Text deutsche erreichen Kolimo Tagung veröffentlichen gleichzeitig planen detailliert Dokumentation Arbeitsschritte veröffentlichen ähnlich Projekt Leitfad Verfügung stehen Projekt dokumentieren gegenwärtig Status Entscheidung verschieden konzeptionell analytischen Prozedurale Ebene zeigen Aufbau digital literarisch Korpus Synchrone diachron quantitativ Vergleich Schwerpunktepoche erlauben weit trivial Aufgabe darstellen deutlich hypothesen Konstitution epochen Autorschaft gattungen Korpuskompilation steuern äì möglichst präzise Modellierung zugrundeliegend textwissenschaftlich Theorie fußen gleichzeitig Metadat Autor Titel Publikationsdatum Publikationsort Gattung linguistisch parameter pos ansatzpunken philologisch Fragestellung präzise praktikabel Kategorie umwandeln zuletzt literarisch daten Flexibl architekturen speichern zusätzlich annotationseben zulass äì hermeneutisch erkenntnisprozesse stellen erwachsen Stärke geisteswissenschaften dar digital Zeitalter explizit modelliert Platz einnehmen,"[('dta', 0.2525856285757185), ('tagger', 0.2230887550960373), ('marmot', 0.21412912465099965), ('kolimo', 0.21412912465099965), ('treetagger', 0.18118470898905656), ('register', 0.16731656632202796), ('epoche', 0.1571153870348443), ('tt', 0.15326287588506435), ('mm', 0.15326287588506435), ('pos', 0.136342283857876)]"
2017,DHd2017,workshop-REITE.xml,CUTE: CRETA Unshared Task zu Entitätenreferenzen,"Nils Reiter (Universität Stuttgart, Deutschland); Andre Blessing (Universität Stuttgart, Deutschland); Nora Echelmeyer (Universität Stuttgart, Deutschland); Steffen Koch (Universität Stuttgart, Deutschland); Gerhard Kremer (Universität Stuttgart, Deutschland); Sandra Murr (Universität Stuttgart, Deutschland); Maximilian Overbeck (Universität Stuttgart, Deutschland); Axel Pichler (Universität Stuttgart, Deutschland)","shared task, unshared task, Entitätenreferenz, Annotation, Modularisierung von Forschungsfragen","Inhaltsanalyse, Beziehungsanalyse, Annotieren, Veröffentlichung, Kollaboration, Literatur, Methoden, Forschungsprozess, Standards, Text","Der Workshop zum CRETA Unshared Task (CUTE) verfolgt ein inhaltliches und ein methodisches Ziel. Das inhaltliche Ziel ist die Anregung eines Diskurses über Entitäten, deren Annotation und Kategorisierung entlang von geistes- und sozialwissenschaftlichen Forschungsfragen sowie deren Potential als disziplinübergreifende Textanalyseaufgabe. Methodisch möchten wir ein Workshop-Format erproben, das unseres Erachtens eine produktive Schnittstelle zwischen Geistes-/SozialwissenschaftlerInnen und InformatikerInnen bildet. Das genaue Programm des Workshops wird von den Teilnehmenden durch Beiträge gestaltet (durch Beiträge, siehe Call for Papers Das Konzept der Entität und ihrer Referenz ist ein bewusst weites, das anschlussfähig sein soll für verschiedene Forschungsfragen aus den Geistes- und Sozialwissenschaften. Wir möchten dabei explizit verschiedene Perspektiven auf Entitäten berücksichtigen. Figuren in literarischen Texten sind ""mit ihrer sinnkonstitutiven und handlungsprogressiven Funktion"" ein zentraler Bestandteil der fiktiven Welt (Platz-Waury 1997). Von besonderem Interesse dabei sind Figurenkonstellationen und Interaktionen, die Entwicklung von Figuren sowie die Funktionalisierung von Figuren als Handlungsträger. Die Erkennung von Figurenreferenzen ist grundlegend, um z.B. Figuren zu charakterisieren, ihre Relationen identifizieren und Netzwerkanalysen durchführen zu können (vgl. Jannidis 2015, Trilcke 2013). Neben der Figur rückt –- spätestens seit dem Politische Parteien, internationale Organisationen oder Institutionen sind seit jeher zentrale Analyseobjekte der empirischen sozialwissenschaftlichen Forschung und werden spätestens seit dem Im Unterschied zu den Literatur- und Sozialwissenschaften spielen Entitäten als Untersuchungsgegenstand in philosophischen Texten zunächst keine Rolle. Aufgrund ihrer metareflexiven Ausrichtung fragt Philosophie primär nicht nach individuell unterscheidbaren Objekten in der echten oder einer fiktiven Welt, sondern beschäftigt sich mit transzendentalen Fragen nach den Bedingungen und Möglichkeiten derartiger individueller Objekte. Dabei arbeitet sie mit abstrakten Konzepten, die sich ebenfalls als -- nicht-dingliche -- Objekte einer Welt auffassen lassen. Pragmatisch gesehen erfolgt die Referenz auf abstrakte Konzepte in Texten jedenfalls in ähnlicher Weise wie die Referenz auf Figuren, Organisationen und Orten (s.u.). Auch wenn die Interpretation von z.B. der Erwähnung von Organisationen in politischen und des Auftretens von Figuren in literarischen Texten anderen Regeln folgt und mit anderen Forschungsfragen zusammenhängt, gibt es Gemeinsamkeiten auf linguistisch-struktureller Ebene. Im Text realisiert werden Referenzen auf die o.g. Arten von Entitäten entweder als Eigennamen ( Abstrakt gesprochen verstehen wir unter Entitäten individuell unterscheidbare Objekte in der echten oder einer fiktiven Welt. Wir unterscheiden sechs verschiedene Typen von Entität: Personen, Orte, Ereignisse, Organisationen, kulturelle Artefakte und Konzepte. Die Bezeichnung als ""Objekt"" impliziert also Die Erstellung abstrakter Annotationsrichtlinien und deren systematische, kontrollierte Anwendung (Annotation) auf konkrete Texte verspricht im Wesentlichen zwei Ergebnisse: In diesem Sinne ist das zweite, methodische Ziel des Workshops zu verstehen: Wir möchten einen Community-Task veranstalten, der eine Beiträge zu Aufgabe 1 werden quantitativ evaluiert und im Wettbewerb mit den Evaluationsergebnissen der anderen Beiträge verglichen ( Das von uns im Rahmen des Workshops veröffentlichte Korpus umfasst vier Teilkorpora: Auch wenn jedes Teilkorpus seine eigenen Besonderheiten hat, wurden alle nach einheitlichen Annotationsrichtlinien annotiert, die wir ebenfalls veröffentlichen und zur Diskussion stellen möchten. Der Workshop wird ausgerichtet vom Centre for Reflected Text Analytics (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine ""black box"" sein, sondern auch für nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird.",de,Workshop Creta unshared Task Cute verfolgen inhaltlich methodisch Ziel inhaltlich Ziel Anregung Diskurse entität Annotation Kategorisierung entlang sozialwissenschaftlich Forschungsfrag Potential disziplinübergreifend Textanalyseaufgabe methodisch möchten erproben unser Erachten produktiv Schnittstelle Informatikerinn bilden genau Programm Workshop Teilnehmend beiträge gestalten beiträge sehen Call for Paper Konzept Entität Referenz bewusst weit anschlussfähig verschieden Forschungsfrag Sozialwissenschaft möchten explizit verschieden Perspektive Entität berücksichtigen Figur literarisch Text sinnkonstitutiv handlungsprogressiv Funktion zentral Bestandteil fiktiv Welt besonderer Interesse figurenkonstellationen interaktione Entwicklung Figur Funktionalisierung Figur Handlungsträger Erkennung Figurenreferenze grundlegend Figure Charakterisieren relation identifizier netzwerkanalys durchführen Jannidis trilcken Figur rücken spätestens politisch Partei international Organisation Institution jeher zentral Analyseobjekt empirisch sozialwissenschaftlich Forschung spätestens Unterschied Sozialwissenschaft spielen entitäen Untersuchungsgegenstand philosophisch Text Rolle aufgrund metareflexiv Ausrichtung fragen Philosophie primär individuell unterscheidbar objekten echt fiktiv Welt beschäftigen transzendental Frage Bedingung Möglichkeit derartig individuell objeken arbeiten abstrakt Konzept ebenfalls Objekt Welt auffassen lassen pragmatisch sehen erfolgen Referenz abstrakt Konzept Text jedenfalls ähnlich Weise Referenz Figur Organisation Ort Interpretation Erwähnung Organisation politisch Auftreten Figur literarisch Text Regel folgen Forschungsfrag zusammenhängen Gemeinsamkeit Ebene Text realisieren Referenz Art Entität eigennamen abstrakt sprechen verstehen entität individuell unterscheidbar Objekt echt fiktiv Welt unterscheiden verschieden Typ Entität Person oren Ereignis Organisation kulturell Artefakt Konzept Bezeichnung objekt implizieren Erstellung abstrakt Annotationsrichtlini systematisch kontrolliert Anwendung Annotation konkret Text versprechen wesentlich Ergebnis Sinn methodisch Ziel Workshop verstehen möchten veranstalten beiträge Aufgabe quantitativ evaluieren Wettbewerb Evaluationsergebniss beitrag vergleichen Rahmen Workshop veröffentlicht korpus umfassen Teilkorpora jeder Teilkorpus Besonderheit einheitlich Annotationsrichtlini annotiert ebenfalls veröffentlichen Diskussion stellen möchten Workshop ausrichten centre for reflected Text analytics Creta Universität Stuttgart Creta verbinden Literaturwissenschaft Linguistik Philosophie Sozialwissenschaft Maschineller Sprachverarbeitung Visualisierung Hauptaufgabe Creta Entwicklung reflektiert Methode Textanalyse wobei methode Gesamtpaket konzeptuell Rahmen annehmen technisch Implementierung Interpretationsanleitung verstehen Methode black box transparent reflektiert Einsatz Hinblick sozialwissenschaftlich Fragestellung,"[('entität', 0.25360849211637976), ('creta', 0.2096514036419669), ('organisation', 0.19325858005629928), ('workshop', 0.18405587723272282), ('abstrakt', 0.18379935891880625), ('möchten', 0.17123800423920096), ('figur', 0.17123428551115244), ('beiträge', 0.17027137267256973), ('referenz', 0.1669988367903194), ('sozialwissenschaftlich', 0.1610884016003242)]"
2017,DHd2017,vortrag-TRILC.xml,"Netzwerkdynamik, Plotanalyse 'Zur Visualisierung und Berechnung der ""progressiven Strukturierung"" literarischer Texte","Peer Trilcke (Universität Potsdam, Deutschland); Frank Fischer (Higher School of Economics, Moskau, Russland); Mathias Göbel (Staats- und Universitätsbibliothek Göttingen, Deutschland); Dario Kampkaspar (Herzog-August-Bibliothek Wolfenbüttel, Deutschland); Christopher Kittel (Universität Graz, Österreich)","Netzwerkanalyse, Literatur, Plot, Digitale Literaturwissenschaft, Strukturanalyse","Programmierung, Strukturanalyse, Modellierung, Theoretisierung, Netzwerkanalyse, Visualisierung, Daten, Sprache, Literatur, Text, Visualisierung","Die Anwendung von Methoden der Netzwerkanalyse auf literarische Texte hat sich in den letzten Jahren zu einem eigenständigen Forschungsfeld der Darüber hinaus wird ausgelotet, inwiefern sich mittels visueller und/oder statistischer Auswertung der Netzwerkdaten genuin literaturwissenschaftliche Erkenntnisse gewinnen bzw. neue Wege der literaturwissenschaftlichen Analyse entwickeln lassen: Neben Ansätzen zur quantitativen Beschreibung und Hierarchisierung des Figurenpersonals (Jannidis et al. 2016) werden hier, im Rahmen korpusbasierter Analysen, Optionen der literaturhistorischen Periodisierung auf Basis von quantitativen Strukturdaten diskutiert (Trilcke et al. 2015) sowie Typen der ästhetischen Modellierung sozialer Formationen in und durch literarische Texte differenziert (Stiller et al. 2003; Stiller & Hudson 2005; Trilcke et al. 2016). Nahezu keine Rolle spielte bisher jedoch ein durchaus hehres Erkenntnisversprechen, das 'bereits in der prä-automatisierten Zeit formuliert (de Nooy 2006) 'auch den Fluchtpunkt des einschlägigen ""Pamphlets"" von Franco Moretti steht: dass nämlich die Netzwerkanalyse als ein Instrumentarium der quantitativen ""plot analysis"" (Moretti 2011) fungieren könne. Tatsächlich lässt sich dieses Erkenntnisversprechen mit den derzeit verfolgten Ansätzen im Bereich der literaturwissenschaftlichen Netzwerkanalyse kaum aufgreifen, geschweige denn einlösen (so auch Prado et al. 2016). Denn die sequentielle Dimension literarischer Texte, mithin ihre Temporalität, bleibt hier in der Regel ausgeblendet: Erfasst, visualisiert und analysiert werden statische Netzwerke. Plot ist jedoch wesentlich ein Konzept, das die Temporalität narrativer (wie auch dramatischer) Versuche, die Netzwerkanalyse in Richtung einer quantitativen Plotanalyse weiterzuentwickeln, stehen also zunächst vor der Aufgabe, bei ihrer Modellierung des Untersuchungsgegenstandes die Zeitdimension zu berücksichtigen. Der Text ist entsprechend nicht lediglich als ein statisches Netzwerk zu modellieren, sondern als eine sich über die Zeit verändernde Folge von Netzwerkzuständen. Erst anhand dieser Netzwerkdynamiken lassen sich die Erkenntnispotenziale, die netzwerkanalytische Zugänge für die quantitative Plotanalyse bergen, überhaupt diskutieren. Der projektierte Vortrag wird 'in Anschluss an Prado et al. 2016 'aus theoretischer und methodischer Perspektive sowie anhand exemplarischer Fallstudien eine Erweiterung der bisherigen, auf die Analyse Entsprechend der zweigleisigen Auswertungsroutinen, die auf netzwerkanalytische Daten angewendet werden, wird der Vortrag zwei Szenarien der netzwerkbasierten Analyse der progressiven Strukturierung literarischer Texte diskutieren: zum einen (3.1) sind Möglichkeiten und Erkenntnispotenziale der Während die Visualisierung dynamischer Netzwerke in anderen Domänen bereits seit längerem gang und gäbe ist (vgl. exemplarisch Pohl et al. 2008; Frederico et al. 2011), wurde erst vor Kurzem der Versuch unternommen, entsprechende Visualisierungsverfahren auch auf literarische Netzwerke anzuwenden (Xanthos et al. 2016). Während Xanthos et al. ¬†u.a. auf didaktische Anwendungsszenarien hinweisen, wird ein literaturwissenschaftliches Erkenntnispotenzial lediglich angedeutet; eine Diskussion dessen, was durch eine solche Visualisierung nicht nur Hingegen zeigen erste, im Vortrag zu vertiefende Zwischenergebnisse unserer Analysen, dass die dynamische Visualisierung insbesondere dann erkenntnisrelevant wird, wenn es darum geht, multiplexe Netzwerke zu modellieren, d.¬†h. Netzwerke, die unterschiedliche Interaktionstypen zugleich erfassen. So zeigt eine statische Visualisierung von Lessings bürgerlichem Trauerspiel Abb.¬†1: Statisches Netzwerk zu Lessing: Zerlegt man das statische Dramennetzwerk (Abb.¬†1) nun nach Akten und dynamisiert es damit, so zeigt sich, dass die Familie Galotti zu keinem Zeitpunkt des Dramas gemeinsam auf der Bühne steht (vgl. Abb.¬†2). Abb.¬†2: Dynamisches Netzwerk zu Lessings Anschaulich und Dass dynamische Visualisierungen in diesem Sinne aus literaturwissenschaftlicher Sicht v.a. für die Analyse multiplexer Netzwerke produktiv gemacht werden können, werden wir im Vortrag anhand weiterer Beispiele aus dem dlina-Korpus (philologisch kuratierte Netzwerkdaten zu 465 deutschsprachige Dramen aus der Zeit 1730–1930, siehe Mehr noch als die Visualisierung statischer Netzwerke stellt diejenige dynamischer im Grunde keine Option eines korpusbasierten Von Carley (2003: 135–136) wurden dabei mehrere rudimentäre globale Maße (i.¬†e. Der Vortrag liefert einen Beitrag zur Methodenentwicklung und -reflektion im Bereich der",de,Anwendung Methode Netzwerkanalyse literarisch Text letzter eigenständig Forschungsfeld hinaus ausloten inwiefern mittels visuelle statistisch Auswertung netzwerkdaten genuin literaturwissenschaftlich Erkenntnis gewinnen Weg literaturwissenschaftlich Analyse entwickeln lassen Ansatz quantitativ Beschreibung Hierarchisierung Figurenpersonal Jannidis et Rahmen korpusbasiert analysen Optionen literaturhistorisch Periodisierung Basis Quantitativen strukturdat diskutieren trilcke et Type ästhetisch Modellierung sozial formationen literarisch Text differenzieren stiller et stiller Hudson trilcken et nahezu Rolle spielen hehres erkenntnisversprechen formulieren de nooy Fluchtpunkt einschlägig Pamphlet Franco moretti stehen nämlich Netzwerkanalyse Instrumentarium quantitativ plot Analysis moretti fungieren können tatsächlich lässen erkenntnisversprechen derzeit verfolgt Ansätz Bereich literaturwissenschaftlich Netzwerkanalyse aufgreifen einlösen prado et sequentiell Dimension literarisch Text mithin Temporalität bleiben Regel ausblenden erfasst visualisiern analysieren statisch netzwerke Plot wesentlich Konzept Temporalität narrativ dramatisch versuchen Netzwerkanalyse Richtung quantitativ Plotanalyse weiterentwickeln stehen Aufgabe Modellierung untersuchungsgegenstandes Zeitdimension berücksichtigen Text entsprechend lediglich statisch netzwerk Modelliere verändernd Folge netzwerkzuständen anhand netzwerkdynamiken lassen erkenntnispotenzial netzwerkanalytisch zugänge quantitativ Plotanalyse bergen diskutieren projektiert Vortrag Anschluss Prado Et theoretisch methodisch Perspektive anhand exemplarisch Fallstudi Erweiterung bisherig Analyse entsprechend zweigleisig Auswertungsroutin netzwerkanalytisch daten anwenden Vortrag Szenarie netzwerkbasiert Analyse progressiv Strukturierung literarisch Text diskutieren Möglichkeit erkenntnispotenzial Visualisierung dynamisch netzwerke Domän lang Gang geben exemplarisch Pohl et Frederico et kurz Versuch unternehmen entsprechend visualisierungsverfahren literarisch netzwerk anwenden xanthos et xanthos et didaktisch Anwendungsszenarie hinweisen literaturwissenschaftlich Erkenntnispotenzial lediglich andeuten Diskussion Visualisierung hingegen zeigen Vortrag vertiefend zwischenergebnisse Analyse dynamisch Visualisierung insbesondere erkenntnisrelevant Multiplexe netzwerken Modelliere netzwerk unterschiedlich Interaktionstype erfassen zeigen statisch Visualisierung lessings bürgerlich Trauerspiel statisch netzwerk Lessing zerlegen statisch Dramennetzwerk akte dynamisieren zeigen Familie Galotti Zeitpunkt Drama gemeinsam Bühne stehen dynamisch Netzwerk Lessings anschaulich dynamisch Visualisierung Sinn literaturwissenschaftlich Sicht Analyse multiplexer netzwerk produktiv Vortrag anhand weit Beispiel philologisch kuratiert netzwerkdaten deutschsprachig Dram sehen Visualisierung statisch netzwerke stellen Dynamischer Grund Option korpusbasierten Carley mehrere rudimentär global Maß Vortrag liefern Beitrag Methodenentwicklung Bereich,"[('statisch', 0.27853267348712585), ('netzwerk', 0.2362718834349135), ('erkenntnispotenzial', 0.20377723359357483), ('vortrag', 0.18803866925450782), ('et', 0.17948894291261572), ('netzwerkanalyse', 0.17336097081283533), ('visualisierung', 0.1719733611612143), ('dynamisch', 0.16589277285446635), ('netzwerke', 0.1483086138661423), ('prado', 0.13585148906238323)]"
2017,DHd2017,poster-REGER.xml,Comparison of Methods for Automatic Relation Extraction in German Novels,"Markus Krug (Universität Würzburg, Deutschland); Christoph Wick (Universität Würzburg, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland); Isabella Reger (Universität Würzburg, Deutschland); Lukas Weimer (Universität Würzburg, Deutschland); Nathalie Madarasz (Universität Würzburg, Deutschland); Frank Puppe (Universität Würzburg, Deutschland)",Relation Extraction,"Beziehungsanalyse, Modellierung, Annotieren, Netzwerkanalyse, Literatur, benannte Entitäten (named entities)","Die automatische Erkennung von spezifischen Relationen ermöglicht Einsichten über die Beziehungen zwischen Entitäten. Solche Informationen können nicht nur als Kantenbezeichner in sozialen Netzwerken fungieren, sondern auch als globale Constraints für das schwierige Problem der Coreference Resolution eingesetzt werden. Darüber hinaus kann eine Relationserkennung zur Beantwortung diverser literarischer Fragestellungen eingesetzt werden, z.B. ob eine Romangattung sich mit bestimmten Relationstypen befasst, oder ob die Arten der Relationen sich über die Jahrhunderte verändern. In dieser Arbeit stellen wir ein Label-Set für die Extraktion von binären Relationen zwischen Personen-Entitäten vor und vergleichen Feature-basierte Ansätze des maschinellen Lernens mit regelbasierten Ansätzen zur automatischen Erkennung dieser Relationen. Da Trainingsmaterial zur Verfügung steht, liegt der Fokus in dieser Arbeit auf dem Einsatz überwachter Methoden, d.h. unsere regelbasierten Verfahren sind ebenfalls auf einer zuvor abgetrennten Menge entwickelt worden. Wir verwenden ein neues Korpus, das manuell mit mehr als 50 verschiedenen, hierarchisch gegliederten Relationstypen annotiert wurde. Eine Übersicht über Arbeiten zur Relationserkennung findet sich in [Jung et al. 2012] sowie [Bach und Badaskar 2007]. Sowohl für den überwachten, als auch den halb-überwachten Fall wurden erfolgreiche Methoden entwickelt. Da dieses Paper sich hauptsächlich auf überwachte Algorithmen bezieht, geben wir nur einen knappen Überblick über halb-überwachte Verfahren. Algorithmen zur Relationsextraktion erhalten typischerweise zwei (oder mehr) Referenzen zu Entitäten (sogenannte Instanzen) als Input und sollen die Klasse, und das dazugehörige Label, vorhersagen, welche die Relation zwischen den Entitäten beschreibt. Die meisten Experimente wurden anhand englischer Texte und den Datensätzen der Automatic Content Extraction (ACE) Workshops 2004 und 2006 durchgeführt. Auf dem Datensatz von 2004 wurden Experimente zur Unterscheidung von 5 und 27 verschiedenen Klassen wie Arbeitsplatz-, körperliche, soziale, Mitgliedschafts- und Diskursrelationen (wobei manche Unterklassen von anderen sein können) betrachtet. Hierfür gibt es zahlreiche Ansätze, die jedoch alle versuchen, eine diskriminative Beschreibung der Instanzen zu erhalten und diese davon ausgehend zu klassifizieren: Im Folgenden vergleichen wir die genannten Methoden anhand eines Label-Sets zur Erkennung binärer Relationen zwischen Figuren in manuell annotierten Abschnitten von deutschsprachigen Romanen. Da Textstellen, an denen Relationen zwischen Entitäten explizit benannt werden, in Romanen typischerweise rar sind, ist es nicht sinnvoll, komplette Romane zu annotieren, da der Ertrag an Daten zu gering wäre. Aus diesem Grund wurde zunächst eine kleine Teilmenge per Hand annotiert und dann genutzt, um mit einem MaxEnt Classifier in einer Active Learning-Umgebung neue Sätze zum Labeln vorschlagen zu können. (Ein Überblick hierzu findet sich in Finn und Kushmerick [Finn und Kushmerick 2003]). Diese Umgebung erhielt Sätze aus 312 verschiedenen Romanen von Projekt Gutenberg und 215 Zusammenfassungen aus dem Kindler Literatur Lexikon Online. Daraus entstand ein Korpus mit 2412 Sätzen, die insgesamt 1265 Relationen enthalten (was wiederum die Knappheit an Daten illustriert). 33 Texte wurden zufällig für die Testmenge ausgewählt, sodass es feste Test- und Trainingsdaten gibt (1988 respektive 424 Sätze mit 1070 respektive 195 Relationen). Die verwendeten Label sind ähnlich zu Massey et al. [Massey et al. 2015]. Die Relationen werden durch eine Ontologie mit momentan 57 verschiedenen Relationstypen repräsentiert, die hierarchisch geordnet sind (beispielsweise ist die Relation ""Tochter"" der Relation ""Familie"" untergeordnet). Abbildung 1 zeigt die oberste Ebene des Label-Sets, mit den gleichen Kategorien wie in Massey et al. [Massey et al. 2015] und einer zusätzlichen Relation ""Liebe"". Abbildung 1: Die ersten beiden Ebenen unseres verwendeten Label-Sets mit den vier Haupttypen, die sich weiter in insgesamt 57 Relationstypen untergliedern lassen. Eine Relation wurde von einem Annotator als ein benannter, gerichteter Bogen zwischen zwei Entitäten in einem Satz gelabelt, sofern sie explizit im Text beschrieben ist. Es wurde immer das spezifischste Label verwendet, da die übergeordneten Relationstypen (vgl. Abbildung 1) daraus abgeleitet werden können. Abbildung 2 zeigt ein Beispiel einer Relation, wie sie in unserem Korpus annotiert ist. Abbildung 2: Zwei gelabelte Instanzen von Relationen in unserem Datensatz. Die erste zeigt die Relation ""hatTochter"" und die zweite die Relation ""hatVerehrer"". Um solche Relationen automatisch erkennen zu können, müssen die Texte eine große Zahl an Vorverarbeitungsschritten durchlaufen. Wir verwenden die Figurenerkennung von Jannidis et al. [Jannidis et al. 2015] und die gleiche Vorverarbeitung wie in [Krug et al. 2016]. Wir verwenden einen regelbasierten Ansatz mit manuell erstellten Regeln und zwei Feature-basierte Lernverfahren (Maximum Entropy, MaxEnt und Support Vector Machines, SVM). Der regelbasierte Ansatz nutzt sowohl die textuelle Repräsentation, als auch den kürzesten Pfad im Dependency-Baum und formuliert die Regel auf Basis dieser Repräsentationen und der Repräsentationen aus dem reinen Text. Das folgende Beispiel zeigt Regeln, die zu den Relationen aus Abbildung 2 passen: Die erste Regel basiert auf der angepassten Text-Repräsentation, während die zweite Regel sich auf den kürzesten Dependency-Pfad zwischen ""sich"" und ""sie"" bezieht. Die Zahlen in runden Klammern geben die Richtung an (in beiden Fällen von Entität 2 auf Entität 1). Die Regeln wurden manuell auf den zuvor gewählten Trainingsdaten erzeugt. Insgesamt wurden fast 500 solcher Regeln ermittelt. Der Großteil der Relationen konnte jedoch mit 3 Regeln (ab hier sogenannte Core-Regeln) abgedeckt werden, die Possessiv- und Genitivkonstruktionen abbilden. Die Feature-basierten Ansätze wurden in zwei Szenarien evaluiert: a) nur mit bereits bekannten Features aus Related Work und b) mit zusätzlichen Booleschen Features (eines pro Regel), falls eine der 500 Regeln passt. Tabelle 1 zeigt die Evaluationsergebnisse der verschiedenen Methoden für drei hierarchische Ebenen (alle Relationen, Relationen der obersten Ebene, alle 57 Relationstypen) und Tabelle 2 die Ergebnisse für die vier Relationstypen der obersten Ebene. Während die Verwendung aller Regeln zu einem F1-Score von 71% für alle Relationen und 59% für die vier übergeordneten Relationstypen führt, erreicht der Feature-basierte Ansatz mit MaxEnt mit einem Booleschen Feature für jede Regel etwas bessere Ergebnisse (F1 von 73,6% und 61,2%). Ohne die Regel-Features liegt der Score der Lernverfahren deutlich niedriger. Die SVM erreicht teilweise eine höhere Precision als MaxEnt, aber im Allgemeinen einen signifikant geringeren F1-Wert. Tabelle 1: Ergebnisse der verschiedenen Ansätze für drei verschiedene Evaluationsszenarien: binär (das reine Vorliegen einer Relation), für die 4 Haupttypen und für alle 57 Relationstypen insgesamt. Tabelle 2: Ergebnisse für die verschiedenen Ansätze, aufgeschlüsselt nach den 4 Haupttypen. Familienrelationen erreichen sehr gute Ergebnisse mit einem F1-Wert von fast 80% und einer Precision von bis zu 95%. Liebesrelationen sind schwerer zu erkennen, liegen aber dennoch bei 56,3% F1. Die anderen Relationstypen fallen in der Qualität ab, sind aber gleichzeitig weniger relevant. Sehr auffällig ist das gute Ergebnis für die drei Core-Regeln und dabei besonders die hervorragende Precision von 96,2% für Familien-Relationen. Eine genauere Betrachtung der False Positives (FP) in Tabelle 3 zeigt, dass diese Relationen fast immer syntaktisch korrekt erkannt wurden, aber semantisch irrelevant und daher nicht im Goldstandard annotiert sind (z.B. ""mein Gott""). Hier zeigt sich eine Schwachstelle dieser Arbeit: teilweise unpräzise Richtlinien für die Annotation von Relationen. Das ist jedoch ein sehr schwieriges Problem, das eventuell umgangen werden kann, wenn die Relationserkennung kein Ziel in sich, sondern eine untergeordnete Aufgabe im Zuge der Erkennung der Hauptfiguren und deren Beziehungen in Romanen ist. Tabelle 3: Auswertung der drei Core-Regeln auf unserem Datensatz Dieses Paper hat gezeigt, dass automatische Relationserkennung eine Herausforderung darstellt. Einfache Regeln können jedoch bereits einen wesentlichen Teil der Relationen mit hoher Precision erkennen. Dennoch ist der Bedarf an weiteren Verbesserungen durch fortschrittliche Methoden hier deutlich. Zudem ist die Evaluation der Relationserkennung an sich schwierig und kann besser im Kontext eines übergeordneten Ziels wie der automatischen Erstellung eines Netzwerks der Hauptfiguren eines Romans [Krug 2016] oder der Gattungsklassifikation [Hettinger et al. 2015] eingebracht werden.",de,automatisch Erkennung spezifisch relationen ermöglichen Einsicht Beziehung Entität Information Kantenbezeichner sozial netzwerken fungieren global Constraints schwierig Problem Coreference Resolution einsetzen hinaus Relationserkennung Beantwortung divers literarisch Fragestellung einsetzen Romangattung bestimmt Relationstype befassen Art Relation jahrhunderte verändern Arbeit stellen Extraktion binären Relation vergleich Ansatz maschinell Lernen regelbasierten Ansatz automatisch Erkennung Relation Trainingsmaterial Verfügung stehen liegen Fokus Arbeit Einsatz überwachter Methode regelbasierten Verfahren ebenfalls zuvor abgetrennt Menge entwickeln verwenden neu korpus manuell verschieden hierarchisch gegliedert Relationstype annotiert Übersicht Arbeit Relationserkennung finden jung et Bach Badaskar sowohl überwachten Fall erfolgreich Methode entwickeln Paper hauptsächlich überwacht algorithmen beziehen geben knapp Überblick Verfahren Algorithm Relationsextraktion erhalten typischerweise Referenz entität sogenannter instanzen Input Klasse dazugehörig Label vorhersagen Relation Entität beschreiben meister experiment anhand englisch Text datensätzen Automatic Content Extraction ace Workshop durchführen Datensatz Experimente Unterscheidung verschieden Klasse körperlich sozial Diskursrelation wobei unterklassen betrachten hierfür zahlreich Ansatz versuchen diskriminativ Beschreibung Instanz erhalten ausgehend klassifizieren folgend vergleichen genannt Methode anhand Erkennung binär Relation Figur manuell Annotiert Abschnitt Deutschsprachig romanen textstellen relationen entität Explizit benennen Roman typischerweise rar sinnvoll komplett Roman annotieren Ertrag daten gering Grund Teilmenge per Hand annotiert nutzen Maxent Classifier activ sätze Labeln vorschlagen Überblick hierzu finden Finn Kushmerick Finn Kushmerick Umgebung erhalten Sätz verschieden Roman Projekt Gutenberg Zusammenfassunge Kindler Literatur Lexikon Online entstehen Korpus Sätze insgesamt Relation enthalten wiederum Knappheit daten illustrieren Text zufällig Testmeng auswählen sodass fest trainingsdaten respektive Sätz respektive Relation verwendet label ähnlich massey et Massey et Relation Ontologie momentan verschieden Relationstype repräsentieren hierarchisch ordnen beispielsweise Relation Tochter Relation Familie untergeordnet Abbildung zeigen oberer Ebene gleich Kategorie Massey et Massey et zusätzlich Relation liebe Abbildung Ebene unser verwendet haupttype insgesamt Relationstype untergliedern lassen Relation Annotator benannt Gerichteter Bogen Entität Satz labeln sofern explizit Text beschreiben spezifisch Label verwenden übergeordnet Relationstype Abbildung ableiten Abbildung zeigen Relation unser Korpus annotiert Abbildung gelabelt Instanz relationen unser Datensatz zeigen Relation hattochter Relation hatverehrer relation automatisch erkennen Text Zahl vorverarbeitungsschritten durchlaufen verwenden Figurenerkennung Jannidis et Jannidis et gleich Vorverarbeitung krug et verwenden regelbasiert Ansatz manuell erstellt Regel lernverfahren maximum entropy Maxent support Vector machines svm regelbasiert Ansatz nutzen sowohl textuell Repräsentation kürzest Pfad formulieren Regel Basis repräsentatione repräsentatione rein Text folgend zeigen Regel relationen Abbildung passen Regel basieren angepasst Regel kürzest beziehen Zahl rund klammern geben Richtung Fall entität Entität Regel manuell zuvor gewählt Trainingsdate erzeugen insgesamt fast Regel ermitteln Großteil Relation Regel sogenannter abdecken genitivkonstruktion abbilden Ansatz Szenarie evaluieren bekannt Feature Related work b zusätzlich boolesch Feature pro Regel falls Regel passt Tabelle zeigen Evaluationsergebnis verschieden Methode hierarchisch eben Relatione Relation oberer Ebene relationstypen Tabell Ergebnis Relationstype oberer Ebene Verwendung Regel relation übergeordnet Relationstype führen erreichen Ansatz Maxent boolesch Feature Regel gut Ergebnis liegen Score lernverfahren deutlich niedrig svm erreichen teilweise hoch Precision Maxent Signifikant gering Tabell Ergebnis verschieden Ansatz verschieden evaluationsszenarie Binär rein vorliegen Relation haupttype Relationstype insgesamt Tabelle Ergebnis verschieden Ansatz Aufgeschlüsselt Haupttyp familienrelationen erreichen Ergebnis fast Precision liebesrelationen schwer erkennen liegen dennoch Relationstype fallen Qualität gleichzeitig relevant auffällig Ergebnis hervorragend Precision genau Betrachtung false Positives fp Tabelle zeigen Relation fast syntaktisch korrekt erkennen semantisch irrelevant Goldstandard annotiert Gott zeigen Schwachstelle Arbeit teilweise unpräzis Richtlinie Annotation relationen schwierig Problem eventuell umgangen Relationserkennung Ziel untergeordnet Aufgabe Zug Erkennung hauptfigur Beziehung romanen Tabell Auswertung unser Datensatz Paper zeigen automatisch Relationserkennung Herausforderung darstellen einfach Regel wesentlich Relation hoch Precision erkennen dennoch Bedarf Verbesserung fortschrittlich Methode deutlich zudem Evaluation Relationserkennung schwierig Kontext übergeordnet Ziel automatisch Erstellung Netzwerk hauptfiguren roman krug Gattungsklassifikation hettinger et einbringen,"[('relation', 0.44930616649576394), ('relationstype', 0.33183926227591504), ('regel', 0.24758257359589458), ('relationserkennung', 0.19792823508898083), ('entität', 0.162637713487019), ('massey', 0.15834258807118468), ('maxent', 0.14748411656707336), ('relationen', 0.1352689883597979), ('ansatz', 0.1230091162910817), ('precision', 0.10265454914189577)]"
2017,DHd2017,poster-BURGH.xml,Digitale Erschließung einer Sammlung von Volksliedern aus dem deutschsprachigen Raum,"Manuel Burghardt (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland); Sebastian Spanner (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland); Thomas Schmidt (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland); Florian Fuchs (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland); Katia Buchhop (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland); Miriam Nickl (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland); Christian Wolff (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland)","crowdsourcing, optical character recognition, optical music recognition, music transcription tool","Transkription, Crowdsourcing, Webentwicklung, Notenblätter, Werkzeuge","Dieser Beitrag beschreibt ein laufendes Projekt Die Datengrundlage des Projekts stellen umfangreichen Quellen zur Volksmusikforschung dar, die seit einigen Jahren von der Universitätsbibliothek Regensburg verwaltet werden. Die Regensburger Liedblattsammlung umfasst etwa 140.000 Blätter mündlich oder handschriftlich tradierter Volkslieder aus dem gesamten deutschsprachigen Raum, und ist, was Abdeckung und Umfang angeht, in dieser Form einzigartig (Krüger, 2013). Die losen Einzelblätter enthalten einerseits handschriftliche, monophone Melodien und andererseits Liedtexte, welche zumeist mit Schreibmaschine verfasst wurden (vgl. Abb. 1). Abbildung 1: Ausschnitt aus dem Liedblatt Nr. A23: ""Klana Mann wollt"" e grouß Fraa hou"". Zu den Liedblättern existieren darüber hinaus Metadaten wie Für die Transkription der Texte und Melodien wurden Tools für die automatische Erfassung evaluiert. Neben automatischer Texterkennung (OCR, Die Evaluation der Eignung bestehender OCR-Tools für den Kontext der Regensburger Liedblattsammlung lehnt sich an Kanungo, Marton und Bulbul (1999) an. Das Testkorpus umfasst 102 Liedblätter, die möglichst viele unterschiedliche typographische und orthographische Phänomene abdecken, etwa Druckschrift (mit unterschiedlich starkem Kontrast), Frakturschrift, aufgeklebte Korrekturen, Sonderzeichen, etc. Für die Evaluation wurde die Textzone unterhalb der Notenzeilen ausgewählt, da die Noten als unbekannte Sonderzeichen das Texterkennungsergebnis negativ verfälschen würden. Für jene Textzonen wurde eine manuelle Transkription erstellt, die in der weiteren Evaluation als Mithilfe des OCR-Evaluationstools Abbildung 2: OCR-Evaluationsergebnisse für die getesteten Tools hinsichtlich der korrekt erkannten, der falsch erkannten, der gar nicht erkannten sowie der überflüssigerweise erkannten Zeichen. Anhand dieser Parameter lassen sich Kennzahlen für die Tools berechnen, etwa die Abbildung 3: Boxplot zur Erkennungsgenauigkeit der einzelnen OCR-Tools. Dass In Anlehnung an eine OMR-Evaluationsstudie (Bellini, Bruno & Nesi, 2007) wurden drei der am weitesten verbreiteten OMR-Tools hinsichtlich ihrer Eignung für die Liedblattsammlung evaluiert: Anders als bei der OCR-Evaluation ist die Erstellung eines automatisch abgleichbaren Bei der Berechnung der Erkennungsgenauigkeit wurden dieselben Parameter verwendet wie schon bei der OCR-Evaluation (vgl. Abb. 2). Die Ergebnisse der OMR-Evaluation zeigen, dass hinsichtlich der durchschnittlichen Erkennungsgenauigkeit mit 36% bei Abbildung 4: Boxplot zur Erkennungsgenauigkeit der einzelnen OMR-Tools. Als alternative Erschließungsstrategie wurde ein Transkriptionstool namens Bei der Umsetzung des Tools für die Transkription der Regensburger Liedblätter wurde besonderes Augenmerk auf die einfache Bedienbarkeit durch iteratives Als erster Schritt wird in  Nach Angabe der Liedblattnummer sowie der Auswahl von Taktart und Tonart gelangt man in den eigentlichen Transkriptionsmodus, bei dem Takt für Takt auf einer interaktiven Notenzeile mit Maus und Tastatur (Shortcuts) transkribiert wird (vgl. Abb. 6). Jeder einzelne Takt kann im Browser abgespielt werden, um so ggf. auf auditiver Ebene schnell Transkriptionsfehler zu erkennen. Abbildung 6: Taktweise Transkription der Liedblätter mit dem Allegro-Tool. Im Hintergrund werden die Eingaben auf das virtuelle Notenblatt schließlich in ein maschinenlesbares Format ( Das Transkriptionstool befindet sich aktuell in der offenen Beta-Testphase und findet guten Zuspruch bei den Anwendern: Dieser Beitrag gibt einen Einblick in ein laufendes Projekt zur digitalen Erschließung einer großen Sammlung von Liedblättern. Während OCR-Tools für die automatische Erfassung der Liedtexte annehmbare Ergebnisse mit einer Erkennungsrate von bis zu 80% liefern, so liegt die Erkennungsgenauigkeit bestehender OMR-Tools für die handschriftlichen Notensätze bei lediglich maximal 36%. Im Falle der Notenerkennung wurde von Grund auf ein neues, intuitiv bedienbares Transkriptionstool entwickelt, welches über einen Crowdsourcing-Ansatz die sukzessive Erschließung der Notensätze sicherstellen soll. Aktuell liegt der Projektfokus auf der Erschließung der Liedblätter. Parallel entstehen zudem erste Prototypen (vgl. Burghardt et al., 2016) für das angedachte Informationssystem, das die Analyse der Liedblätter anhand der verfügbaren Metadaten, der Liedtexte sowie anhand verschiedener melodischer Parameter (vgl. Mongeau & Sankoff, 1990; Orio & Rod√°, 2009; Typke, 2007) erlaubt. Im Rahmen des weiteren Projektverlaufs sollen anhand der digital erschlossenen Liedblätter u.a. die folgenden Fragestellungen untersucht werden:",de,Beitrag beschreiben laufend Projekt Datengrundlage Projekt stellen umfangreich quellen Volksmusikforschung dar Universitätsbibliothek Regensburg verwalten Regensburger Liedblattsammlung umfassen Blätter mündlich Handschriftlich tradiert volkslied gesamt deutschsprachig Raum Abdeckung Umfang angehen Form einzigartig krüg losen Einzelblätter enthalten einerseits Handschriftliche Monophone Melodien andererseits liedtexte zumeist Schreibmaschine verfassen abb Abbildung Ausschnitt Liedblatt nr Klana Mann e Grouß fraa Hou liedblättern existieren hinaus metadaen Transkription Text Melodien Tool automatisch Erfassung evaluieren automatisch Texterkennung ocr Evaluation Eignung bestehend Kontext Regensburger Liedblattsammlung lehnen Kanungo Marton Bulbul Testkorpus umfassen Liedblätter möglichst unterschiedlich typographisch orthographisch phänomen abdecken druckschrifen unterschiedlich stark Kontrast frakturschrifen aufgeklebt korrekturen sonderzeichen Evaluation Textzone unterhalb Notenzeile auswählen Not unbekannt sonderzeichen Texterkennungsergebnis negativ verfälschen textzon Manuelle Transkription erstellen Evaluation Mithilfe Abbildung getesteter Tools hinsichtlich korrekt erkennen falsch erkennen erkannten überflüssigerweise erkannt Zeichen anhand parameter lassen Kennzahl Tools berechnen Abbildung Boxplot Erkennungsgenauigkeit einzeln Anlehnung Bellini Bruno Nesi weit verbreitet hinsichtlich Eignung Liedblattsammlung evaluieren Erstellung automatisch abgleichbaren Berechnung Erkennungsgenauigkeit parameter verwenden abb Ergebnis zeigen hinsichtlich durchschnittlich Erkennungsgenauigkeit Abbildung Boxplot Erkennungsgenauigkeit einzeln alternativ Erschließungsstrategie Transkriptionstool namens Umsetzung Tool Transkription Regensburger Liedblätter besonderer Augenmerk einfach Bedienbarkeit Iteratives Schritt Angabe Liedblattnummer Auswahl Taktart tonaren gelangen eigentlich Transkriptionsmodus Takt Takt interaktiv notenzeile Maus Tastatur shortcuts transkribieren abb einzeln Takt Browser abspieln auditiv Ebene schnell Transkriptionsfehler erkennen Abbildung Taktweise Transkription Liedblätter Hintergrund Eingabe virtuell Notenblatt schließlich maschinenlesbar Format Transkriptionstool befinden aktuell offen finden gut Zuspruch Anwender Beitrag Einblick laufend Projekt digital Erschließung Sammlung liedblättern automatisch Erfassung Liedtexte annehmbar Ergebnis erkennungsrat liefern liegen Erkennungsgenauigkeit bestehend handschriftlich Notensätz lediglich maximal Fall Notenerkennung Grund neu intuitiv bedienbar Transkriptionstool entwickeln sukzessiv Erschließung Notensätz sicherstellen aktuell liegen Projektfokus Erschließung Liedblätter parallel entstehen zudem Prototyp Burghardt Et angedacht informationssyst Analyse Liedblätter anhand verfügbar Metadat Liedtexte anhand verschieden Melodischer parameter Mongeau Sankoff orio Typke erlauben Rahmen Projektverlauf anhand Digital erschlossen Liedblätter folgend Fragestellung untersuchen,"[('liedblätter', 0.36900299026364675), ('erkennungsgenauigkeit', 0.24234940211537365), ('liedblattsammlung', 0.18450149513182337), ('regensburger', 0.18450149513182337), ('takt', 0.18450149513182337), ('transkriptionstool', 0.18450149513182337), ('liedtexte', 0.1628721774004203), ('transkription', 0.1545842401212123), ('notenzeile', 0.12300099675454891), ('liedblättern', 0.12300099675454891)]"
2017,DHd2017,poster-LAUBR.xml,Visuelle Elemente grafischer Literatur: Aufmerksamkeitszuwendung und objektive Beschreibung,"Jochen Laubrock (Universität Potsdam, Deutschland); Eike Richter (Universität Potsdam, Deutschland); Sven Hohenstein (Universität Potsdam, Deutschland)","grafische Literatur, Comics, Eyetracking, Korpus","Räumliche Analyse, Annotieren, Visualisierung, Bilder, Text","Graphische Romane vereinen als hybride Gattung Aspekte von Literatur und bildender Kunst (McCloud, 1993). Wie interagieren Bild und Text beim Lesen graphischer Literatur und ermöglichen das Verstehen des Gesamtwerkes? Worauf fokussiert die Aufmerksamkeit des Lesers? Als Methode zur Beantwortung dieser Fragen ist die Blickbewegungsmessung besonders geeignet. Blickbewegungen haben sich in einer Vielzahl an Studien als valides, nichtreaktives Maß für die Verarbeitung und das Verstehen von Text und Bild erwiesen, in dem sich zudem auch unbewusste Verarbeitungsprozesse niederschlagen (Findlay & Gilchrist, 2003; Wade & Tatler, 2005). In früheren Arbeiten (Laubrock, Hohenstein & Thoß, 2016; Dunst, Hartel, Hohenstein Laubrock, 2016) haben wir mit Eyetracking-Analysen gezeigt, dass beim Lesen grafischer Literatur der größte Teil der Aufmerksamkeit dem Text in Sprechblasen und Beschriftungen (Captions) zugewandt wird und nur ein relativ kleiner Teil den originär visuellen Gestaltungselementen alloziert wird. Wird der visuelle Inhalt gar nicht beachtet, oder kann er möglicherweise bereits im peripheren Sehen während der Fixationen auf dem Text verarbeitet werden? Wir hatten bereits berichtet, dass Comics-Experten den Bildanteil stärker beachten und darauf verstehensrelevante Information extrahieren. In einer neuen Serie von Studien untersuchen wir mittels blickkontingenter Präsentation, ob (a) den Bildanteilen mehr Aufmerksamkeit zugewandt wird, wenn die Vorschau verhindert wird, indem das Bild erst eingeblendet wird, wenn der Blick sich auf ein Panel bewegt und (b) die Aufmerksamkeit andere grafische Elemente auswählt, wenn zwar der visuelle Teil der Panels sichtbar ist, der Text aber erst nach Fokussierung eines Panels eingeblendet wird. Das visuelle Material wurde auf zweierlei Weise annotiert. Einerseits annotierten Menschen Personen und einzelne Objekte innerhalb der Panels. Andererseits versuchen wir eine objektiven Beschreibung des visuellen Materials mithilfe von Deskriptoren aus dem maschinellen Sehen (Computer Vision), z.B. mittels Farbhistogrammen, lokalem Fourier-Spektrum oder SIFT-Deskriptoren (Lowe, 1999). Der Vorteil dieser Beschreibung ist neben der Objektivität die skriptgesteuerte Anwendbarkeit auf große Datenmengen, etwa digitalisierte Korpora grafischer Literatur. Vergleichbare Arbeiten aus der Schnittstelle von Kunstgeschichte und Informatik ermöglichen beispielsweise eine automatisierte Klassifikation von Kunstrichtungen (Saleh & Elgammal, 2015) und zeigen das Potenzial eines solchen Ansatzes als Stilometrie visueller Merkmale. Für die Zuordnung der Blickbewegungsdaten auf das Stimulusmaterial nutzen wir die im Projekt entwickelte Graphic Novel Markup Language (GNML), eine Erweiterung der Comic Book Markup Language (CBML; Walsh, 2012). Das Material wurde mit unserem Editor annotiert, für Weiterverarbeitung und statistische Analyse der Daten nutzten wir ein in Entwicklung befindliches R-Paket. Die objektive Beschreibung des visuellen Materials mit Deskriptoren aus dem maschinellen Sehen wurde unter Nutzung von OpenCV (Bradski, 2000) und VLFEAT (Vedaldi & Fulkerson, 2008) teils in Python und teils in Matlab implementiert, da für R für diesen Anwendungsbereich keine hinreichend entwickelte Funktionsbibliothek existiert.",de,graphisch Roman vereinen hybrid Gattung Aspekt Literatur Bildender Kunst Mccloud interagier Bild Text lesen graphisch Literatur ermöglichen verstehen Gesamtwerk worauf fokussieren Aufmerksamkeit lesers Methode Beantwortung Frage Blickbewegungsmessung geeignet Blickbewegunge Vielzahl Studie valid nichtreaktiv Maß Verarbeitung verstehen Text Bild erweisen zudem unbewusst Verarbeitungsprozess niederschlagen Findlay Gilchrist wade Tatler früh Arbeit Laubrock Hohenstein thoß Dunst Hartel Hohenstein Laubrock zeigen les grafisch Literatur groß Aufmerksamkeit Text Sprechblase Beschriftung Captions zuwenden relativ originär visuell gestaltungselement alloziert visuell Inhalt beachten möglicherweise Peripher sehen Fixation Text verarbeiten berichten Bildanteil stark beachten verstehensrelevant Information extrahieren Serie Studie untersuchen mittels blickkontingent Präsentation bildanteil Aufmerksamkeit zuwenden Vorschau verhindern Bild eingeblenden Blick Panel bewegen b Aufmerksamkeit grafisch Element auswählen visuell Panel sichtbar Text Fokussierung Panel eingeblenden visuell Material zweierlei Weise annotiert einerseits annotiert Mensch Person einzeln Objekt innerhalb Panel andererseits versuchen objektiv Beschreibung visuell Material Mithilfe Deskriptor maschinell sehen Computer Vision mittels Farbhistogramme lokalem lowe Vorteil Beschreibung Objektivität Skriptgesteuerte Anwendbarkeit datenmeng digitalisiert Korpora grafisch Literatur vergleichbar arbeiten Schnittstelle Kunstgeschicht Informatik ermöglichen beispielsweise automatisiert Klassifikation Kunstrichtung Saleh elgammal zeigen Potenzial ansatzes Stilometrie visuell Merkmal Zuordnung Blickbewegungsdate stimulusmaterial nutzen Projekt entwickelt Graphic novel markup Language Gnml Erweiterung Comic Book Markup Language Cbml walsh Material unser Editor annotieren Weiterverarbeitung statistisch Analyse daten nutzen Entwicklung befindlich objektiv Beschreibung visuell Material Deskriptor maschinell sehen Nutzung Opencv Bradski Vlfeat vedaldi Fulkerson teils Python teils Matlab implementieren r anwendungsbereich hinreichend entwickelt Funktionsbibliothek existieren,"[('visuell', 0.29314916104477295), ('aufmerksamkeit', 0.20793423354767196), ('panel', 0.1953235489566423), ('grafisch', 0.18110156937857813), ('material', 0.16920751116437838), ('hohenstein', 0.15771185601578808), ('eingeblenden', 0.15771185601578808), ('bildanteil', 0.1468966374743426), ('deskriptor', 0.1468966374743426), ('zuwenden', 0.1468966374743426)]"
2017,DHd2017,workshop-KOLLA.xml,Annotieren und Publizieren mit DARIAH-DE und TextGrid,"Thomas Kollatz (Steinheim-Institut für deutsch-jüdische Geschichte Essen, Deutschland); Philipp Hegel (Technische Universität Darmstadt, Deutschland); Ubbo Veentjer (Niedersächsische Staat- und Universitätsbibliothek Göttingen, Deutschland); Sibylle Söring (Niedersächsische Staat- und Universitätsbibliothek Göttingen, Deutschland); Stefan E. Funk (Niedersächsische Staat- und Universitätsbibliothek Göttingen, Deutschland)","Annotation, Infrastruktur, Publizieren, Archivieren","Annotieren, Kommunikation, Bearbeitung, Archivierung, Veröffentlichung, Kollaboration, Kommentierung, Daten, Bilder, Infrastruktur, Interaktion, Metadaten, Text, Werkzeuge, virtuelle Forschungsumgebungen","Im Rahmen des halbtägigen Workshops werden den Teilnehmerinnen und Teilnehmern Werkzeuge zum Publizieren und Annotieren von Forschungsdaten demonstriert, die im Rahmen von Hands-On-Einheiten anhand eigener und / oder bereitgestellter Daten erprobt werden können. Vorgestellt und angewendet werden das TextGrid- und DARIAH-DE Repositorium, der DARIAH-DE Publikator und die DARIAH-DE Annotation Sandbox. Zudem wird in die Arbeit mit dem Text-Bild-Link-Editor des TextGrid Laboratoriums eingeführt und exemplarisch gezeigt, diese Text-Bild Relationen mit Hilfe des Web-Publikationstools ""SADE 'Scalable Architecture for Digital Editions"" in eine digitale Präsentation bzw. ein Web-Portal zu übernehmen. Der Workshop richtet sich an Geisteswissenschaftlerinnen und –wissenschaftler aus text- und bildbasierten Disziplinen aller Phasen des akademischen Werdegangs ebenso wie an Vertreterinnen und Vertreter von Institutionen 'etwa Bibliotheken, Forschungsverbünde oder Archive –, die im Rahmen ihrer Vorhaben digitale Forschungsinfrastruktur nutzen bzw. nutzen wollen, um ihre Forschungsdaten nachhaltig digital zu publizieren und zu annotieren. Der Workshop liefert durch Kurzvorträge und Hands-On-Einheiten Einblicke in verschiedene Verfahren, Anwendungen und Workflows liefern, um Geisteswissenschaftlerinnen und Geisteswissenschaftlern die maschinenlesbare Annotation von Text- und Bilddaten sowie die Publikation solcher Forschungsdaten in einem Repositorium zu ermöglichen. Nach einer kursorischen Einführung in die Angebote von TextGrid und DARIAH-DE liefert ein Überblick über das Annotieren in den digitalen Geisteswissenschaften verschiedene Anwendungsszenarien, -anforderungen, -modelle und -technologien. Dabei werden neben bereits bestehenden Angeboten wie dem TextGrid Text-Bild-Link-Editor auch neuere Entwicklungen wie die Annotation Sandbox und das DARIAH-DE Repositorium und seine Publish GUI (Publikator) demonstriert und in interaktiven Übungen durch die Teilnehmenden anhand eigener bzw. zur Verfügung gestellter Daten erprobt.  Digitales Annotieren ist zentrale Praxis bei der Wissensgenerierung und variiert je nach spezifischer wissenschaftlicher Zielsetzung und Forschungsgegenstand. Verfahren des fachwissenschaftlichen digitalen Annotierens bilden heute eine der Kernanwendungen der Digital Humanities. Im Zentrum steht dabei ein weites Spektrum von Daten und / oder Objekten, z.B. Texte, Bilder und Musik (Töne, Noten). Digitale Annotationen unterscheiden sich daher in Form, Funktion und Tragweite. Einführend werden die technischen Ebenen und theoretischen Dimensionen der digitalen Annotation in den Geisteswissenschaften exemplarisch erörtert. Die vermittelten Grundlagen können danach im Workshop praktisch angewandt werden. Forschungsinfrastrukturen wie TextGrid und DARIAH-DE haben zum Ziel, methodologische Fähigkeiten auf diesem Gebiet zu vermitteln, entsprechende Verfahren zu evaluieren bzw. bereitzustellen und die nachhaltige Anwendung dieser Verfahren in den Fachwissenschaften zu ermöglichen. Die DARIAH-DE Annotation Sandbox (Beta) ermoÃàglicht heute die Text- und Bildannotation der Bestände des TextGrid Repository. Darüber hinaus können beliebige Webseiten uÃàber den DARIAH-DE Annotationsdienst annotiert werden. Zudem lässt sich der DARIAH-DE Annotationsdienst in eigene Webseiten einbetten; hierzu wurden die digitalen Werkzeuge Annotator.js, Via und ein Annotation Manager über die DARIAH AAI (Authorization and Authentification Service) verfügbar gemacht. Die DARIAH-DE Annotation Sandbox gestattet die direkte Verbindung der in den Repositorien publizierten Forschungsdaten mit ihrer digitalen Annotation. Diese schließt sowohl die disziplinübergreifenden Nachnutzung als auch die Datenanreicherung oder die Analyse ein. Mittelfristig können Annotationen somit als Zwischenschritt des Forschungsprozesses, aber auch als genuines Forschungsergebnis - etwa im Sinne einer Mikropublikation - verstanden bzw. generiert, verfügbar gemacht und als solches nachgenutzt werden. Im Rahmen einer digitalen Infrastruktur fließen sie wie die Forschungsdaten, auf die sie Bezug nehmen, ebenfalls in die Archivierung ein, um ¬†weiterverarbeitet und nachgenutzt zu werden. Ein weiteres Anwendungsszenario digitaler Annotation stellt die Annotation von Bildern bzw. Bilddaten dar. Eine Vielzahl von Werkzeugen im TextGrid Laboratory erlaubt das Arbeiten mit Texten und Bildern, aber auch beispielsweise mit Noten und Digitalisaten. Eine dieser Komponenten, die auch für die Annotation von Bildbereichen dienen kann, ist der Text-Bild-Link-Editor. Er unterstützt den in TextGrid integrierten XML-Editor bei der Alignierung von Text- und Bildelementen. Ziel ist die Erstellung einer Ausgabedatei, die die Textelemente und die topographische Position von rechtwinkligen und polygonen Bildbereichen in SVG miteinander verknüpft, wie dies zum Beispiel bei der Verbindung von Faksimiles und Transkriptionen in kritischen Editionen der Fall ist. Auch können Bilder auf diese Weise im Rahmen kunsthistorischer Untersuchungen annotiert werden. Die Software SADE der Berlin-Brandenburgischen Akademie der Wissenschaften ist als ""Skalierbare Architektur für digitale Editionen"" in TextGrid eingebunden, um eigene Webportale für die Publikation gestalten zu können. Sie enthält ein Modul, mit dem die Verknüpfungen, die mit dem Text-Bild-Link-Editor erstellt wurden, in ein Web-Portal übernommen werden können. Dieses Modul basiert auf dem in DARIAH-DE integrierten Werkzeug ""Semantic Topological Notes"" (SemToNotes). Es erlaubt unter anderem, Zeilen auf einem Digitalisat auszuwählen und Transkriptionen anzuzeigen. Das DARIAH-DE Repositorium bildet eine zentrale Komponente der Infrastruktur, auf die mittels verschiedener Dienste und Anwendungen zugegriffen werden kann. Das Repositorium erlaubt es, Forschungsdaten zu speichern, diese mit Metadaten zu versehen und die Forschungsdaten durch die Generische Suche aufzufinden. Die Daten werden im DARIAH-DE Storage sicher gespeichert. Darüber hinaus ermöglicht das Repositorium die nachhaltige und sichere Archivierung von Datensammlungen bzw. Kollektionen. Abb.1: DARIAH-DE-Repositorium: Architektur Dies ist komfortabel und intuitiv über ein Web-Interface des DARIAH-DE Portals im Browser möglich, dem DARIAH-DE Publikator. Daten im Repositorium sind in Kollektionen organisiert, die zunächst vom Nutzer über den Publikator angelegt und mit Metadaten ausgezeichnet werden. Einer Kollektion können beliebig viele Dateien zugeordnet werden, die ebenfalls über den Publikator hochgeladen und mit Metadaten ausgezeichnet werden. Eine publizierte Kollektion sowie alle darin enthaltene Objekte können unmittelbar nach dem Publizieren per Persistent Identifier (PID) referenziert werden und sind damit öffentlich zugänglich und nachhaltig referenzier- und zitierbar. Im nächsten Schritt kann die Kollektion in der Collection Registry nachgewiesen und veröffentlicht werden. Sobald die Kollektion selbst ebenfalls in der Collection Registry publiziert wurde, sind die Daten auch mit der Generischen Suche recherchierbar. Abb. 2: DARIAH-DE Publikator: Übersicht über die Kollektionen Abb. 3: DARIAH-DE Publikator: Kollektion bearbeiten Im Workshop werden exemplarisch Annotationen an einem Digitalisat in TextGrid vorgenommen. Zu diesem Zweck ist ein eigener Rechner mitzubringen, auf dem im Idealfall TextGrid bereits installiert ist 'Eine Registrierung für TextGrid und DARIAH kann online beantragt werden unter  Bitte teilen Sie uns im Vorfeld des Workshops (möglichst bis zum 5. Februar 2017) mit, ob und welche eigenen Materialien Sie verwenden wollen. Für Rückfragen erreichen Sie uns unter workshop@de.dariah.eu Mirjam Blümm, Niedersächsische Staats- und Universitätsbibliothek Göttingen, Abt. Forschung und Entwicklung, Papendiek 14, 37073 Göttingen, Stefan E. Funk, Niedersächsische Staats- und Universitätsbibliothek Göttingen, Abt. Forschung und Entwicklung, Papendiek 14, 37073 Göttingen, Canan Hastik, Technische Universität Darmstadt,¬†Dolivostraße 15,¬†Institut für Sprach- und Literaturwissenschaft,¬†64293 Darmstadt, Philipp Hegel, Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Dolivostraße 15, 64293 Darmstadt, Thomas Kollatz, Salomon Ludwig Steinheim-Institut für deusch-jüdische Geschichte, Essen, Edmund-Körner-Platz 2, 42157 Essen, Sibylle Söring, Niedersächsische Staats- und Universitätsbibliothek Göttingen, Abt. Forschung und Entwicklung, Papendiek 14, 37073 Göttingen, Ubbo Veentjer, Niedersächsische Staats- und Universitätsbibliothek Göttingen, Abt. Forschung und Entwicklung, Papendiek 14, 37073 Göttingen, Aufgrund des hohen Praxisanteils soll die Zahl der Teilnehmerinnen und Teilnehmer auf möglichst 25 beschränkt bleiben. WLAN / Beamer / Stellwände / Verlängerungskabel",de,Rahmen halbtägig Workshop Teilnehmerinn Teilnehmer Werkzeug publizieren Annotiere Forschungsdat demonstrieren Rahmen anhand Bereitgestellter daten erproben vorstellen anwenden Repositorium Publikator Annotation Sandbox zudem Arbeit textgrid laboratoriums einführen exemplarisch zeigen Relation Hilfe sade scalabel Architecture for Digital editions digital Präsentation übernehmen Workshop richten geisteswissenschaftlerinnen Wissenschaftler bildbasiert disziplinen Phase akademisch Werdegang vertreterinnen Vertreter Institution bibliotheken forschungsverbünde Archiv Rahmen Vorhaben digital Forschungsinfrastruktur nutzen nutzen Forschungsdat nachhaltig Digital publizieren annotieren Workshop liefern kurzvorträge einblicken verschieden Verfahren Anwendung workflows liefern geisteswissenschaftlerinne geisteswissenschaftlern maschinenlesbar Annotation bilddaten Publikation Forschungsdat Repositorium ermöglichen kursorisch Einführung Angebot Textgrid liefern Überblick Annotiere digital Geisteswissenschaft verschieden Anwendungsszenarie bestehend anbieten Textgrid neuer Entwicklung Annotation sandbox Repositorium Publish Gui Publikator demonstrieren Interaktiv übungen Teilnehmende anhand Verfügung gestellt daten erproben digital annotieren zentral Praxis Wissensgenerierung variieren spezifisch wissenschaftlich Zielsetzung Forschungsgegenstand Verfahren fachwissenschaftlich digital Annotieren bilden Kernanwendung Digital Humanitie Zentrum stehen weit Spektrum daten objekt Text Bild Musik tön non digital annotationen unterscheiden Form Funktion Tragweit einführend technisch eben Theoretisch Dimension digital Annotation geisteswissenschaft exemplarisch erörtern vermittelt Grundlag Workshop praktisch anwenden forschungsinfrastrukturen Textgrid Ziel methodologisch Fähigkeit Gebiet vermitteln entsprechend Verfahren evaluieren bereitzustellen nachhaltig Anwendung Verfahren fachwissenschaften ermöglichen Annotation Sandbox Beta ermoãàglichen Bildannotation Beständ Textgrid Repository hinaus beliebig webseien uãàber Annotationsdienst annotiert zudem lässen Annotationsdienst Webseit einbeten hierzu digital werkzeug via Annotation Manager Dariah aai Authorization and Authentification Service verfügbar Annotation sandbox gestatten direkt Verbindung Repositorie publiziert Forschungsdat digital Annotation schließen sowohl disziplinübergreifend Nachnutzung Datenanreicherung Analyse mittelfristig annotationen somit Zwischenschritt Forschungsprozesse genuin Forschungsergebnis Sinn Mikropublikation verstehen neriern verfügbar nachgenutzen Rahmen digital Infrastruktur fließen forschungsdat Bezug nehmen ebenfalls Archivierung nachgenutzt Anwendungsszenario Digitaler Annotation stellen Annotation bildern bilddaten dar Vielzahl Werkzeug Textgrid Laboratory erlauben arbeiten Text bildern beispielsweise note Digitalisat Komponente Annotation bildbereich dienen unterstützen Textgrid integriert Alignierung Bildelement Ziel Erstellung Ausgabedatei Textelemente topographisch Position rechtwinklig polygon Bildbereich svg miteinander verknüpfen Verbindung faksimiles Transkription kritisch Edition Fall Bild Weise Rahmen kunsthistorischer Untersuchung annotiert Software Sade Akademie Wissenschaft skalierbar Architektur digital editionen Textgrid eingebunden Webportale Publikation gestalten enthalten modul Verknüpfung erstellen übernehmen Modul basieren integriert Werkzeug semantic Topological notes semtonot erlauben zeilen Digitalisat auswählen Transkription anzeigen Repositorium bilden zentral Komponente infrastruktur mittels verschieden dienst Anwendung zugegriffen Repositorium erlauben forschungsdaen speichern metadaten versehen Forschungsdat generisch Suche aufzufinden daten storage sicher speichern hinaus ermöglichen Repositorium nachhaltig sicher Archivierung datensammlung Kollektione Architektur komfortabel intuitiv Portal Browser Publikator daten Repositorium Kollektion organisieren Nutzer Publikator anlegen metadaten auszeichnen Kollektion beliebig dateien zuordnen ebenfalls Publikator hochgeladen metadaten auszeichnen publiziert Kollektion enthalten objekte unmittelbar Publizieren per Persistent Identifier pid referenzieren öffentlich zugänglich nachhaltig zitierbar nächster Schritt Kollektion Collection Registry nachweisen veröffentlichen sobald Kollektion ebenfalls Collection Registry publizieren daten generisch Suche recherchierbar abb Publikator übersicht Kollektion abb Publikator Kollektion bearbeiten Workshop exemplarisch annotation Digitalisat Textgrid vornehmen Zweck Rechner mitbringen Idealfall Textgrid installieren Registrierung Textgrid dariah Online beantragen bitte teilen Vorfeld Workshop möglichst Februar Materialium verwenden Rückfrage erreichen Mirjam Blümm niedersächsisch Universitätsbibliothek göttingen Abt Forschung Entwicklung Papendiek göttingen Stefan Funk niedersächsisch Universitätsbibliothek göttingen Abt Forschung Entwicklung Papendiek götting Canan Hastik technisch Universität Darmstadt Philipp Hegel technisch Universität Darmstadt Institut Literaturwissenschaft dolivostraß Darmstadt Thomas Kollatz salomon Ludwig Geschichte essen essen sibyllen Söring niedersächsisch Universitätsbibliothek göttingen Abt Forschung Entwicklung Papendiek Götting ubbo Veentjer niedersächsisch Universitätsbibliothek göttingen Abt Forschung Entwicklung Papendiek göttingen aufgrund hoch Praxisanteil Zahl Teilnehmerinn Teilnehmer möglichst beschränken bleiben Wlan Beamer Stellwände verlängerungskabel,"[('publikator', 0.2785804534395089), ('textgrid', 0.27108504183069515), ('kollektion', 0.2594765727203469), ('repositorium', 0.2195553145534906), ('göttingen', 0.21079041740697438), ('forschungsdat', 0.16642283410651657), ('niedersächsisch', 0.15918883053686222), ('papendiek', 0.15918883053686222), ('abt', 0.15918883053686222), ('sandbox', 0.15918883053686222)]"
2017,DHd2017,vortrag-WAGNE.xml,Ambige idiomatische Ausdrücke in kinderliterarischen Texten: Mehrwert einer Datenbankanalyse,"Wiltrud Wagner (Eberhard Karls Universität Tübingen, Deutschland)","Datenbank, Ambiguität, Idiom, interdisziplinär, Analysetool","Entdeckung, Sammlung, Gestaltung, Strukturanalyse, Annotieren, Theoretisierung, Archivierung, Community-Bildung, Veröffentlichung, Kollaboration, Organisation, Konservierung, Visualisierung, Sprache, Literatur, Metadaten, Methoden, Visualisierung","In meinem Vortrag setze ich mich mit der Frage auseinander, welchen Beitrag die Datenbank TInCAP (""Tübingen Interdisciplinary Corpus of Ambiguity Phenomena""), die bei der Tagung der Digital Humanities im deutschsprachigen Raum 2016 in Leipzig vorgestellt wurde und die der Sammlung und Annotation von Ambiguitätsbelegen dient, zur Erforschung des Phänomens ""Ambiguität"" leisten kann. Den Mehrwert, den TInCAP durch die innovative interdisziplinäre Annotation und die Zusammenführung von Belegen in einer durchsuchbaren Datenbank liefert, werde ich am Beispiel ambiger idiomatischer Ausdrücke in kinderliterarischen Texten illustrieren. Die Datenbank TInCAP entsteht im Rahmen des interdisziplinären Graduiertenkollegs Auch wenn alle an diesem Projekt beteiligten WissenschaftlerInnen das Interesse am Phänomen der Ambiguität verbindet, das hier als Doppel- oder Mehrdeutigkeit in ihren verschiedensten Formen verstanden wird, so sind die zu annotierenden Belege doch sehr divers: Durch die Vielzahl der beteiligten Disziplinen unterscheiden sich die Belege hinsichtlich Medium (aktuell: Schrift, Audio, Bild, Video) und Sprache (aktuell: Deutsch, Englisch, Französisch, Hebräisch, Italienisch, Latein, Spanisch, Griechisch), aber auch Umfang. Im Bestreben, eine gemeinsame Datenbank aufzubauen, sahen wir uns demnach zwei großen Herausforderungen gegenüber gestellt: (1) Der Erarbeitung einer disziplinenübergreifenden Terminologie, die einerseits präzise, andererseits aber nicht an das Vokabular einer der Disziplinen gebunden ist, und (2) der Entwicklung eines interdisziplinären Annotationsschemas, das 'trotz der notwendigen Komplexitätsreduktion 'den Anforderungen der einzelnen Disziplinen genügt und für alle Beteiligten profitabel ist. Das Ergebnis ist ein Annotationsschema, das die folgenden fünf Punkte fokussiert: Zusätzlich ist die Verknüpfung von Annotationen möglich, zum Beispiel, wenn ein Beleg auf verschiedenen Kommunikationsebenen (unterschiedlich) annotiert wird. Die Nachhaltigkeit der gesammelten Daten wird durch eine Kombination verschiedener Faktoren gewährleistet: Das von uns entwickelte XML-Schema ist soweit möglich TEI-konform, es wurde für die inhaltliche Annotation der Daten um ein eigenes Schema erweitert. Der gesamte Korpus bzw. Subkorpora können im XML-Format im- und exportiert werden. Diese XML-Dateien werden in Kooperation mit Clarin-D Tübingen im Rahmen der universitären Infrastruktur langfristig gespeichert, katalogisiert und mit PIDs zugänglich gemacht. Teilkorpora können dabei ebenso exportiert werden wie das Gesamtkorpus. Bei Video-, Audio- und Bilddateien halten wir uns an die üblichen Standards für nachhaltige Datenformate (nicht-proprietäre Formate, Formate mit gutem Nachnutzungswert). Nach der allgemeinen Vorstellung der Datenbank wende ich mich im zweiten Teil des Vortrags der Frage zu, was die Datenbank im Hinblick auf konkrete Fragestellungen leistet. Die von mir in die Datenbank eingebrachten Ambiguitätsbelege entstammen zum größten Teil meiner Dissertation, die einen interdisziplinären Beitrag zur Ambiguitätsforschung leistet: Der linguistische Teil der Arbeit untersucht, wie idiomatischen Ausdrücken das Potential zur Ambiguität inhärent sein kann. An der Schnittstelle zur Literaturwissenschaft zeigt die Arbeit, wann und wie idiomatische Ausdrücke in Interaktion mit unterschiedlichen Kotexten ihr Ambiguitätspotential entfalten. Am Beispiel von kinderliterarischen Texten wird schließlich dargestellt, wie die aus dieser Interaktion resultierende Bewusstmachung von Ambiguität als sprachspielerisches Potential für literarische Texte produktiv gemacht werden kann. (a)-(c) stellen typische Beispiele aus meinem Korpus dar, die jeweils annotierten Stellen sind fett markiert: (a)  (c) Die Annotation meiner Beispiele mit TInCAP ermöglicht die Sichtbarmachung von Aspekten, die bei der reinen linguistischen oder literaturwissenschaftlichen Analyse möglicherweise verborgen bleiben. Besonderes Gewicht kommt dabei der Möglichkeit zu, Ambiguitäten auf mehreren Kommunikationsebenen zu annotieren und die resultierenden Annotationen zu verknüpfen. Dies möchte ich anhand von Beispielen wie (a)-(c) illustrieren und mich dabei auf folgende Phänomene konzentrieren: Diese Phänomene, die erst durch die Annotation mit TInCAP und durch entsprechende Suchabfragen sichtbar werden, zeigen das Potential, das diese Datenbank innerhalb eines Projekts entfaltet. In einem abschließenden Ausblick möchte ich darüber hinaus auf den interdisziplinären Nutzen der Datenbank verweisen, der im Rahmen des GRK 1808 bereits zum Tragen kommt, insbesondere in der Vergleichbarkeit, die über Medien hinweg geschaffen wird.",de,Vortrag setzen Frage auseinander Beitrag Datenbank Tincap tübingen interdisciplinary Corpus of Ambiguity phenomena Tagung Digital Humanitie deutschsprachig Raum Leipzig vorstellen Sammlung Annotation Ambiguitätsbeleg dienen Erforschung Phänomen Ambiguität leisten mehrweren Tincap innovativ interdisziplinär Annotation Zusammenführung Belege durchsuchbar Datenbank liefern ambig idiomatisch Ausdrück kinderliterarisch Text illustrieren Datenbank Tincap entstehen Rahmen interdisziplinären graduiertenkollegs Projekt beteiligt wissenschaftlerinnen Interesse Phänomen Ambiguität verbinden Mehrdeutigkeit verschieden Form verstehen annotierend Beleg divers Vielzahl beteiligt Disziplin unterscheiden Beleg hinsichtlich Medium aktuell schrifen audio Bild Video Sprache aktuell deutsch Englisch französisch hebräisch Italienisch latein Spanisch griechisch Umfang bestreben gemeinsam datenbank aufbauen sehen demnach Herausforderung stellen Erarbeitung disziplinenübergreifend Terminologie einerseits präzise andererseits Vokabular disziplin binden Entwicklung interdisziplinär Annotationsschemas trotz notwendig Komplexitätsreduktion Anforderung einzeln Disziplin genügen beteiligt profitabel Ergebnis Annotationsschema folgend Punkt fokussieren zusätzlich Verknüpfung annotation Beleg verschieden kommunikationseben unterschiedlich annotiert Nachhaltigkeit gesammelt daten Kombination verschieden Faktor gewährleisten entwickelt soweit inhaltlich Annotation daten Schema erweitern gesamt Korpus Subkorpora exportieren Kooperation Tübinge Rahmen universitär Infrastruktur langfristig speichern katalogisieren Pid zugänglich Teilkorpora exportieren Gesamtkorpus Bilddateien halten üblich Standard nachhaltig Datenformate Formate Formate gut nachnutzungsweren Vorstellung Datenbank wenden Vortrag Frage datenbank Hinblick konkret Fragestellung leisten Datenbank eingebracht Ambiguitätsbeleg entstammen groß Dissertation interdisziplinär Beitrag Ambiguitätsforschung leisten linguistisch Arbeit untersuchen idiomatisch Ausdrücken Potential Ambiguität Inhärent Schnittstelle Literaturwissenschaft zeigen Arbeit idiomatisch Ausdrück Interaktion unterschiedlich Kotext Ambiguitätspotential entfalten kinderliterarisch Text schließlich darstellen Interaktion resultierend Bewusstmachung Ambiguität sprachspielerisch Potential literarisch Text produktiv stellen typisch Beispiel Korpus dar jeweils annotiert Stelle Fett markieren c Annotation Beispiel Tincap ermöglichen Sichtbarmachung Aspekt rein linguistisch literaturwissenschaftlich Analyse möglicherweise verbergen bleiben besonderer Gewicht Möglichkeit Ambiguität mehrere Kommunikationseben annotieren Resultierend Annotation verknüpfen anhand Beispiel illustrieren folgend phänomen konzentrieren phänomen Annotation Tincap entsprechend suchabfragen sichtbar zeigen Potential datenbank innerhalb Projekt entfalten abschließend Ausblick hinaus interdisziplinär Nutzen Datenbank verweisen Rahmen grk tragen insbesondere Vergleichbarkeit Medium hinweg schaffen,"[('datenbank', 0.3985792658331526), ('tincap', 0.3240049967024069), ('ambiguität', 0.25535538509469174), ('idiomatisch', 0.19440299802144412), ('interdisziplinär', 0.16050995133622406), ('beleg', 0.14882283392424941), ('annotation', 0.14547816500847208), ('ambiguitätsbeleg', 0.12960199868096275), ('kommunikationseben', 0.12960199868096275), ('kinderliterarisch', 0.12960199868096275)]"
2017,DHd2017,vortrag-GIUSE.xml,Datenvisualisierung als Aisthesis,"Evelyn Gius (Universität Hamburg, Deutschland); Rabea Kleymann (Universität Hamburg, Deutschland); Jan Christoph Meister (Universität Hamburg, Deutschland); Marco Petris (Universität Hamburg, Deutschland)","Datenvisualisierung, Hermeneutik, visuelle Grammatik, Interaktivität","Umwandlung, Gestaltung, Programmierung, Bearbeitung, Visualisierung, Daten, Bilder, Interaktion, Forschungsprozess, Werkzeuge, Visualisierung","Jede/r DH-Praktiker/in weiß: Computergestützte Forschung in den Geisteswissenschaften beginnt mit der Übersetzung relevanter Phänomene in digitale Daten. Seltener thematisiert wird dagegen, dass die digitale Derartige Expansionsverfahren systematisch zu beschreiben, ist in den Geisteswissenschaften Visualisierungen gelten heute disziplinübergreifend als probates Mittel der ""Expansion"" schwer überschaubarer Primär- und Sekundärdaten in intuitiv erfassbarer Form (Goodings 2003:281). In der disziplinspezifischen Perspektive ist allerdings zugleich zu fordern, dass den methodologischen Besonderheiten der Datenvisualisierungen, die als visuelle Expansionsverfahren Trotz der großen Vielfalt an bestehenden Visualisierungstools und Visualisierungsmetaphern gibt es allerdings bislang kein derartiges, theoretisch reflektiertes Gegenstand des Projekts ""3DH 'dreidimensionale dynamische Datenvisualisierung und Exploration fuÃàr Digital Humanities-Forschungen"" ist die Entwicklung und prototypische Implementierung eines solchen Konzepts der geisteswissenschaftlichen Datenvisualisierung. Grundlegendstes dieser Erfordernisse ist, die bildhafte Veranschaulichung von Daten konsequent bi-direktional zu denken. Die Zweites Erfordernis ist, dass hermeneutisch funktionale Visualisierungen neben generischen Anforderungen auch die Besonderheiten geisteswissenschaftlicher Praxis in den Das skizzierte Spannungsverhältnis zwischen den allen Geisteswissenschaften gemeinsamen und den disziplinspezifischen Anforderungen an eine visuelles ""Expansionskonzept"" hat Grinstein (2012) zur Formulierung einer ""grand challenge"" motiviert:. Er fordert ein Visualisierungssystem, das auf disziplinspezifische Anforderungen reagiert und die in Hinblick auf die jeweilige Forschungsfrage wie die verfügbaren Daten optimale Visualisierungslösung automatisch generieren kann. Diese Vision mag zwar in der Tat ""grand"" und unter dem Gesichtspunkt der Implementierbarkeit utopisch anmuten; als konzeptionelle Messlatte für das 3DH Projekt ist sie dennoch richtig. Denn nur Visualisierungslösungen, die den systematischen Zusammenhang zwischen den methodischen Anforderungen eines Forschungsvorhabens und den objektspezifischen Eigenschaften der in diesem Kontext erhobenen und generierten Daten konzeptionell reflektieren, haben zumindest eine theoretische Chance, die von Grinstein verlangten ""Passungen"" automatisch zu ermitteln. Das 3DH-Projekt erforscht den Phänomenbereich ""Datenvisualisierung"" vor diesem Hintergrund unter drei systematischen Aspekten, nämlich (1) einer Typologie hermeneutischer Routinen, Bedingungen und Zielsetzungen des begriffsorientierten (d.h. natürlich- bzw. fachsprachlich artikulierten) Interpretierens von Daten, die in ihrer für den geisteswissenschaftlichen Verstehensprozess kennzeichnenden Ausprägung zu definieren sind; (2) einer Syntax grafischer Strategien, die 'je nach Kontextbedingung und Prozessphase 'die ""bottom up""-definierten Grundlagen für ein erkenntnisproduktives visuelles ""mapping"" der vorgenannten hermeneutischen Operationen auf die jeweils behandelten Primär- und Sekundärdatensets bereitstellen; und (3) einer nach Designprinzipien geordneten Taxonomie konkreter Visualisierungstypen, die als ""top-down""-Determinanten und epistemologische Paradigmen aufgefasst werden können. Die Designprinzipien werden ihrerseits nicht auf die Funktion der bloßen Steuerung visueller Datenrepräsentation am Ende eines geisteswissenschaftlichen Arbeitszyklus reduziert; sie sollen vielmehr als eigenständige, komplementäre Verfahren nicht-sprachlicher, bildgebundener Verstehensoperationen aufgefasst werden. Die Bearbeitung der drei Aspekte soll neben der theoretischen Konzeptentwicklung auch zur Erarbeitung einer visuellen Grammatik für geisteswissenschaftliche Datenvisualisierung führen. Im ersten Schritt haben wir eine Reihe exemplarischer Use Cases der DH-Forschung Tabelle 1: epistemologische Gegensatzpaare Jedes dieser Gegensatzpaare markiert eine Dimension hermeneutischer Praxis, in der datenbasierte Erkenntnisprozesse in der Regel nicht auf normativ geregelte finite Auslegungen von Bedeutung und Wert, sondern auf kontextsensitive, skalierte dynamische Zuschreibungen von Informationsgehalt und Relevanz abzielen. Als epistemologische Matrix bildet diese Tabelle zugleich die Grundlage für die Entwicklung einer ""Grammar of Graphics"" in Anlehnung an Bertin (1983) und Wilkinson (2005). Wie von Satyanarayan et al. (2016) vorgeschlagen, müssen diese Ansätze allerdings um den Aspekt der Interaktivität erweitert werden. Graphische Merkmale sollen entsprechend durch sog. ""Aktivatoren"" visuell modalisierbar werden. Die so erweiterte visuelle Grammatik soll in eine Notation überführt werden, die möglichst allgemein verständlich, generisch und unabhängig von einer bestimmten Programmiersprache implementierbar sein muss; aufgrund der großen Verbreitung von XML in den Digital Humanities ist eine zusätzliche XML-Notation geplant. Daneben sollen für eine Reihe exemplarischer hermeneutischer Verstehens- und Interpretationsprozesse die systematischen Zusammenhängen zwischen Datenstrukturen und geeigneten Visualisierungsprinzipien erforscht und adäquate Vorschläge für eine (oder mehrere) Visualisierungen erarbeitet werden. Die Implementierung der entwickelten Visualisierungen wird eine webbasierte Browser-Anwendung sein, die kollaboratives Arbeiten ermöglicht und über ein Web Service Interface mit anderen Systemen verbunden werden kann. Die Spezifikation der Visualisierungen mit Hilfe einer von einer Grafik-Engine unabhängigen Grammatik erlaubt prinzipiell beliebige Ausgabeformate. Aufgrund der Interaktivität und Webfähigkeit ist zunächst SVG als Format geplant. Auch wenn die weiteren Schritte zur Erarbeitung der visuellen Grammatik und der prototypischen Implementierung geisteswissenschaftlich funktionaler Visualisierungsansätze vorgezeichnet scheinen: Die Frage nach der methodischen Adäquatheit des Vorgehens bleibt für unser Vorhaben weiterhin virulent. So stehen bei den epistemologischen Gegensatzpaaren in Tabelle 1 bislang logische Gegensätze des Typs A und non-A (z.¬†B. Reliablity vs. Unreliability) und phänomenologische Gegensätze (z.¬†B. Probability vs. Factuality) nebeneinander. Noch ist nicht geklärt, ob es sich hier um Kategorienfehler im analytischen Sinne handelt, oder ob nicht gerade dieses Nebeneinanderstehen kategorial unterschiedlicher Konzepte dem hermeneutischen Prozess gerecht wird. Welche Konsequenzen hätte es zum Beispiel für ein geisteswissenschaftliches Visualisierungskonzept, wenn sich strikt logische, binäre Modellierungen hermeneutischer Prozesse sogar als prinzipiell ungeeignet erweisen? Unter diesem kritischen Vorbehalt erscheinen zum einen konkrete, etablierte visuelle Verfahren in einem neuen Licht. Kann zum Beispiel Shneidermans (1996) bekanntes Erst das Nachdenken über die Erfordernisse eines geisteswissenschaftlichen Visualisierungs",de,r wissen computergestützt Forschung geisteswissenschaften beginnen Übersetzung relevant Phänomen digital daten selten thematisieren digital derartiger expansionsverfahren systematisch beschreiben geisteswissenschaft visualisierung gelten disziplinübergreifend probat Expansion schwer überschaubar sekundärdaen intuitiv erfassbar Form Goodings disziplinspezifisch Perspektive fordern methodologisch Besonderheit Datenvisualisierung visuell expansionsverfahren trotz Vielfalt bestehend Visualisierungstools Visualisierungsmetapher bislang derartig theoretisch reflektiert Gegenstand Projekt dreidimensional dynamisch Datenvisualisierung Exploration Fuãàr Digital Entwicklung prototypisch Implementierung Konzept geisteswissenschaftlich Datenvisualisierung grundlegendstes Erfordernis bildhaft Veranschaulichung daten konsequent denken Erfordernis hermeneutisch funktional Visualisierung generisch Anforderung Besonderheit geisteswissenschaftlich Praxis skizziert Spannungsverhältnis geisteswissenschaft gemeinsam disziplinspezifisch Anforderung visuell Expansionskonzept Grinstein Formulierung Grand Challenge motivieren fordern Visualisierungssystem disziplinspezifisch Anforderung reagieren Hinblick jeweilig Forschungsfrag verfügbar daten optimal Visualisierungslösung automatisch generieren Vision grand Gesichtspunkt Implementierbarkeit utopisch anmun konzeptionell Messlatte Projekt dennoch visualisierungslösungen systematisch Zusammenhang methodisch Anforderung Forschungsvorhaben objektspezifischer eigenschaften Kontext erhoben generiert daten konzeptionell reflektieren zumindest theoretisch Chance Grinstein verlangt Passung automatisch ermitteln erforschen phänomenbereich datenvisualisierung Hintergrund systematisch Aspekt nämlich Typologie hermeneutisch Routine Bedingung Zielsetzung begriffsorientiert fachsprachlich artikulierten interpretierens daten geisteswissenschaftlich verstehensprozess kennzeichnend Ausprägung definieren Syntax grafisch strategien Kontextbedingung Prozessphase bottom Grundlage erkenntnisproduktiv visuell Mapping vorgenannt hermeneutisch operationen jeweils Behandelten Sekundärdatenset bereitstellen Designprinzipien geordnet Taxonomie konkret visualisierungstypen epistemologisch Paradigm aufgefasst Designprinzipi ihrerseits Funktion bloß Steuerung visuell datenrepräsentation geisteswissenschaftlich Arbeitszyklu reduzieren vielmehr eigenständig komplementär Verfahren bildgebunden Verstehensoperation auffassen Bearbeitung Aspekt theoretisch Konzeptentwicklung Erarbeitung visuell Grammatik geisteswissenschaftlich Datenvisualisierung führen Schritt Reihe exemplarisch use Cases Tabelle epistemologisch gegensatzpaar jeder Gegensatzpaar markieren Dimension hermeneutisch Praxis datenbasierter erkenntnisprozesse Regel normativ geregelt Finit auslegungen Bedeutung wert kontextsensitiv skaliert dynamisch Zuschreibung Informationsgehalt Relevanz abzielen epistemologisch Matrix bilden Tabell Grundlage Entwicklung Grammar of graphics Anlehnung Bertin Wilkinson Satyanarayan Et vorschlagen Ansatz Aspekt Interaktivität erweitern graphisch Merkmal entsprechend aktivator visuell modalisierbar erweitert visuell Grammatik Notation überführen möglichst allgemein verständlich generisch unabhängig bestimmt Programmiersprache implementierbar aufgrund Verbreitung xml Digital Humanitie zusätzlich planen Reihe exemplarisch hermeneutisch interpretationsprozesse systematisch Zusammenhäng datenstrukturen geeignet Visualisierungsprinzipien erforschen adäquat Vorschlag mehrere visualisierung erarbeiten Implementierung entwickelt visualisierung webbasierter Kollaboratives arbeiten ermöglichen Web Service Interface System verbinden Spezifikation visualisierung Hilfe unabhängig Grammatik erlauben prinzipiell beliebig ausgabeformate aufgrund Interaktivität webfähigkeit svg Format planen Schritt Erarbeitung visuell Grammatik prototypisch Implementierung geisteswissenschaftlich funktional visualisierungsansätze vorgezeichnen scheinen Frage methodisch Adäquatheit Vorgehen bleiben Vorhaben weiterhin virulent stehen epistemologisch Gegensatzpaar Tabelle bislang logisch Gegensätz Typ Reliablity Unreliability phänomenologisch Gegensätz Probability Factuality nebeneinander klären Kategorienfehler analytisch Sinn handeln nebeneinandersteh Kategorial unterschiedlich Konzept hermeneutisch Prozess gerecht Konsequenz geisteswissenschaftlich Visualisierungskonzept strikt logisch binär Modellierunge hermeneutisch Prozesse sogar prinzipiell ungeeignet erweisen kritisch Vorbehalt erscheinen konkret etabliert visuell Verfahren Licht Shneiderman bekannt Nachdenken Erfordernis geisteswissenschaftlich visualisierungs,"[('visuell', 0.25733232512034865), ('datenvisualisierung', 0.22747680708982054), ('geisteswissenschaftlich', 0.22221199762676475), ('hermeneutisch', 0.2086586356726898), ('grammatik', 0.18198144567185642), ('epistemologisch', 0.18198144567185642), ('gegensatzpaar', 0.1615164723882263), ('erfordernis', 0.15044034919082294), ('disziplinspezifisch', 0.15044034919082294), ('anforderung', 0.12484810157298713)]"
2017,DHd2017,vortrag-PERNE.xml,Aufbau eines historisch-literarischen Metaphernkorpus für das Deutsche,"Stefan Pernes (Universität Würzburg, Deutschland); Lennart Keller (Universität Würzburg, Deutschland); Christoph Peterek (Universität Würzburg, Deutschland)","Metapher, Annotation, Klassifikation","Datenerkennung, Entdeckung, Sammlung, Inhaltsanalyse, Annotieren, Veröffentlichung, Stilistische Analyse, Identifizierung, Daten, Infrastruktur, Software","Metaphorischer Sprachgebrauch umfasst komplexe gedankliche Würfe genauso wie alltägliche Begrifflichkeiten. Die Metapher gilt als Untersuchungsgegenstand nicht nur in den Literaturwissenschaften, Sprachwissenschaften und der Anthropologie, sondern hat auch Relevanz für so disparate Forschungsprogramme wie das der Künstlichen Intelligenz und der Kritischen Diskursanalyse. Darüber hinaus stellt die Erkennung und Auflösung von Metaphern ein wichtiges Desiderat in sprachtechnologischen Anwendungen dar, deren Gegenstand die Disambiguierung von Wortbedeutungen umfasst. Korpusuntersuchungen zeigen, dass metaphorischer Sprachgebrauch in gängigen Textsorten durchschnittlich in jedem dritten Satz zu finden ist (vgl. Steen et al. 2010; Shutova und Teufel 2010) 'ein Beleg für die Ubiquität der Metapher, die in erster Linie darin begründet liegt, dass die idealtypische Karriere eines metaphorischen Ausdrucks als kühne Betonung beginnt und als konventionelle Form endet. Eine Sprachressource zum Metapherngebrauch kann also eine wichtige Ergänzung bei der automatischen inhaltlichen Erschließung von Textbeständen darstellen. Dabei stellt das hier entwickelte Korpus annotierter Sätze, dessen Grundlage eine Sammlung deutschsprachigen Romane aus dem 19. Jahrhunderts bildet, einen spezifischen Beitrag zur Erschließung von historischen Textbeständen dar. Die große Mehrheit der heute verfügbaren Metaphernkorpora basiert auf dem Prinzip, einige wenige Zielbegriffe sowie unter Umständen ausgewählte konzeptuelle Domänen zu definieren und alle passenden sprachlichen Realisierungen aus einem großen Textbestand zu extrahieren. Diese Herangehensweise lässt vermuten, dass die damit modellierten Eigenschaften sich nicht auf arbiträren Text in realen Anwendungsszenarien übertragen lassen, denn jedes vordefinierte lexikalische oder konzeptuelle Inventar wird dabei zu kurz greifen (vgl. Shutova 2015). Im Gegensatz dazu enthält das hier entwickelte Korpus keine Einschränkungen hinsichtlich der konzeptuellen Domänen oder der erfassten sprachlichen Konstruktionen, bis auf die Tatsache, dass es sich aus literarischen Prosatexten zusammensetzt. Es sollte noch darauf hingewiesen werden, dass mit der Hamburg Metaphor Database eine weitere deutschsprachige Ressource zur Metapher existiert, diese jedoch nach wesentlich anderen Gesichtspunkten erstellt wurde und lediglich eine kleine Zahl ausgewählter Beispielsätze enthält. Grundlage für die Erstellung des Korpus bildet die Romansammlung der Digitalen Bibliothek des Projektes TextGrid. Die Sammlung umfasst insgesamt 454 Werke vom frühen 16. bis zum frühen 20. Jahrhundert, wobei der Bedarf nach orthographisch normalisiertem Text die Datengrundlage auf 383 Romane aus den Jahren 1830 bis 1940 eingeschränkt hat. Zur Ziehung der zu annotierenden Sätze wird eine balancierte Sampling-Strategie hinsichtlich zeitlicher Streuung und Gender der AutorInnen eingesetzt. Es handelt sich dabei um eine Quotenstichprobe, die aus jedem 10-Jahres-Abschnitt und zu gleichen Teilen männlicher und weiblicher Autorinnen Sätze auswählt. Darüber hinaus wird im Rahmen des Samplings eine automatische Vorauswahl getroffen, sodass die Hälfte der Sätze Metaphern enthält. Möglich wird dies durch einen Classifier, der anhand von TF-IDF Scores 'auf Grundlage einer lemmatisieren Version des gesamten Romankorpus 'feststellen kann wie ""ungewöhnlich"" ein zu klassifizierender Satz ist. Anhand eines empirisch festgestellten, von der Größe des TF-IDF Korpus abhängigen, Schwellenwerts ist es anschließend möglich, eine Vorauswahl zu treffen, die indirekt Metaphorizität erfasst. Es handelt sich dabei um eine vereinfachte Form des von Schulder & Hovy (2014) entwickelten Klassifikationsansatzes. Ziel des hier entwickelten Korpus ist es, einen Gesamtumfang von bis zu 2000 annotierten Sätzen zu erreichen. Als Grundlage dafür wurden insgesamt 3000 Sätze ausgewählt. Wir orientieren uns an der Metaphor Identification Procedure (MIP) der Pragglejaz Group (Pragglejaz Group 2007; Steen et al. 2010) und sehen zunächst jedes Wort im Text als potentielle Metapher. Gegenstand der Metaphernannotation ist es somit, jedes Wort als metaphorisch beziehungsweise nicht metaphorisch zu klassifizieren. Die Aufgabe ist dabei auf metaphorische Öußerungen auf der Wortebene beschränkt, das heißt Satzmetapher und Textmetapher sowie Phänomene grammatischer Metapher sind ausgenommen. Aufgrund der Neigung des Deutschen zur Kompositabildung wird jedoch eine automatische Kompositazerlegung durchgeführt. Da der Umfang der zu annotierenden Sätze eine Herausforderung für eine solche detallierte Herangehensweise wie das MIP darstellt, wird eine automatische Vorselektion potentieller Metaphernkandidaten durchgeführt. Auf Grundlage von Part-of-Speech-Informationen und Dependency-Bäumen werden aus den Sätzen folgende Konstruktionstypen als Kandidaten für eine metaphorische Verwendung extrahiert (zu den Typen vgl. Skirl und Schwarz-Friesel 2007): Substantivmetapher 'dazu gehören Komposita, Kopulakonstruktionen (""X ist ein Y""), Simile (""X ist wie ein Y""), Genitivmetapher 'sowie Verb-, Adjektiv-, Präpositions- und 'als'-Metapher. Wir folgen mit diesem Ansatz der automatischen Vorauswahl Gandy et al. (2013), die dadurch bei der Metaphernauszeichnung eine Übereinstimmung der Annotatoren von Kappa = 0.80 erreichen. Die extrahierten Konstruktionen werden anschließend zusammen mit den Sätzen für die Annotation in ein geeignetes Format exportiert und in die Annotationsumgebung WebAnno (Yimam et al. 2014) geladen. Für den manuellen und bei weitem umfassendsten Teil der Arbeit wurde ein Annotationsleitfaden verfasst, der die Identifikationsstrategie MIP reproduziert, Hinweise zum Umgang mit lexikalisierten metaphorischen Ausdrücken und der Abgrenzung zur Metonymie enthält. Darüber hinaus ist dargestellt, welche Konstruktionstypen vormarkiert werden und welche Ausnahmen dabei zu erwarten sind (Eigennamen und Hilfsverben sind von der Markierung ausgenommen, vgl. Shutova und Teufel 2010). Schließlich wird festgelegt wie mit Ausnahmen und fehlerhaften Sätzen zu verfahren ist. Satzfragmente, starke dialektale Formen sowie Sätze, die ohne Kontext nicht interpretierbar sind, werden als Ausschuss markiert, fehlerhafte Vormarkierungen werden gekennzeichnet und im Rahmen der Kuration der annotierten Sätze verbessert. Es kann von den folgenden vorläufigen Ergebnissen der automatischen Vorauswahl und der manuellen Annotation berichtet werden: Die automatische Extraktion der möglicher Metaphernkandidaten hat es ermöglicht, ein korpusgestütztes Bild darüber zu erlangen wie die Konstruktionen in einer relativ offenen Domäne 'einem Romankorpus, das diverse Gattungen umfasst 'verteilt sind. Des Weiteren ist ausgehend von der Annotationspraxis festzustellen, dass die erhobenen Konstruktionen 'zumindest im Rahmen der hier zugrundeliegenden Texte 'theoretisch alle Vorkommen von metaphorischen Öußerungen auf der Wortebene abdecken. In der Praxis kommt es jedoch aufgrund komplexer Hypotaxen und fehlender automatischer Koreferenz-Auflösung zu fehlerhaften Vormarkierungen. Die vorbereitende, automatische Klassifikation der Sätze in Metapher beziehungsweise Nicht-Metapher führt zu einem Anteil von 48% an Sätzen, die lebendige Metaphern enthalten. Werden lexikalisierte metaphorische Ausdrücke mit eingerechnet, steigt der Anteil der Sätze, die Metaphern enthalten, auf 61%. Ein erheblicher Vorteil, der sich aus der Klassifikation der Sätze ergibt, ist die Fülle des Materials, die sich dadurch generieren lässt. Ohne Vorauswahl liegt die durchschnittliche Anzahl von metaphorischen Ausdrücken pro Satz 'je nach Textsorte 'zwischen 0.12 und 0.54 (vgl. Shutova & Teufel 2010), während mit dem hier vorgestellten Ansatz ein Wert von 1.91 Metaphern pro Satz erreicht wird. Eine genaue Auswertung der Präzision des Classifiers steht noch aus, in Bezug auf die Struktur der so ausgewählten Sätze kann jedoch festgestellt werden, dass die Klassifizierung keine Auswirkung auf die Verteilung der erhobenen Konstruktionstypen hat. Für die Übereinstimmung zwischen zwei Annotatoren beim Aufbau des hier vorgestellten Metaphernkorpus kann ein Wert von 0.87 (Cohen‚Äôs Kappa) berichtet werden. Werden lediglich die vormarkierten Konstruktionen als Grundlage der Berechnung herangezogen, schwankt Kappa je nach Einbezug lexikalisierter Öußerungen zwischen 0.77 bis 0.80.",de,metaphorisch Sprachgebrauch umfassen komplex gedanklich würf genauso alltäglich begrifflichkein Metapher gelten Untersuchungsgegenstand literaturwissenschaften Sprachwissenschaft Anthropologie Relevanz disparat Forschungsprogramm künstlich Intelligenz kritisch Diskursanalyse hinaus stellen Erkennung Auflösung Metapher wichtig Desiderat sprachtechnologisch Anwendung dar Gegenstand Disambiguierung Wortbedeutung umfassen Korpusuntersuchung zeigen metaphorisch Sprachgebrauch gängigen Textsort durchschnittlich Satz finden Steen et shutova Teufel Beleg Ubiquität Metapher Linie begründen liegen idealtypisch Karriere metaphorisch Ausdruck kühn Betonung beginnen konventionell Form enden sprachressource Metapherngebrauch wichtig Ergänzung automatisch inhaltlich Erschließung Textbeständ darstellen stellen entwickelt Korpus annotiert Sätz Grundlage Sammlung deutschsprachigen Roman Jahrhundert bilden spezifisch Beitrag Erschließung historisch Textbestände dar Mehrheit verfügbar Metaphernkorpora basieren Prinzip Zielbegriff umständ ausgewählt konzeptuell Domäne definieren passend sprachlich Realisierung Textbestand extrahieren Herangehensweise lässt vermuten modellierter eigenschaften arbiträrer Text real Anwendungsszenarie übertragen lassen jeder vordefiniert lexikalisch konzeptuell inventar greifen shutova Gegensatz enthalten entwickelt Korpus Einschränkung hinsichtlich konzeptuell Domän erfasst sprachlich Konstruktion Tatsache literarisch Prosatext zusammensetzen hinweisen Hamburg Metaphor Database deutschsprachig Ressource Metapher existieren wesentlich Gesichtspunkt erstellen lediglich Zahl ausgewählt Beispielsätz enthalten Grundlage Erstellung Korpus bilden Romansammlung digital Bibliothek projektes Textgrid Sammlung umfassen insgesamt Werk früh früh Jahrhundert wobei Bedarf orthographisch normalisiert Text Datengrundlage Roman einschränken Ziehung annotierend Sätz balanciert hinsichtlich zeitlich Streuung Gender Autorinne einsetzen handeln quotenstichprobe gleich Teile Männlicher weiblich autorinn Sätz auswählt hinaus Rahmen Sampling automatisch Vorauswahl treffen sodass Hälfte sätze metaphern enthalten Classifier anhand scores Grundlage lemmatisier Version gesamt Romankorpus feststellen ungewöhnlich klassifizierend Satz anhand empirisch festgestellt Größe Korpus abhängigen schwellenwerts anschließend Vorauswahl treffen indirekt Metaphorizität erfassen handeln vereinfacht Form Schulder hovy entwickelt Klassifikationsansatz Ziel entwickelt Korpus Gesamtumfang annotierter Satz erreichen Grundlage insgesamt Sätz auswählen orientieren Metaphor Identification Procedure mip Pragglejaz Group Pragglejaz Group steen et sehen jeder Wort Text potentiell Metapher Gegenstand Metaphernannotation somit jeder Wort metaphorisch beziehungsweise metaphorisch klassifizieren Aufgabe metaphorisch öußerunge Wortebene beschränken satzmetaph textmetaph phänomen Grammatischer metaph ausnehmen aufgrund Neigung deutsche Kompositabildung automatisch Kompositazerlegung durchführen Umfang annotierend Sätz Herausforderung detalliert Herangehensweise Mip darstellen automatisch Vorselektion Potentieller Metaphernkandidaten durchführen Grundlage sätzen folgend Konstruktionstype Kandidat metaphorisch Verwendung extrahiern Typ Skirl substantivmetaph gehören Komposita Kopulakonstruktionen x y similen x y genitivmetaph folgen Ansatz automatisch Vorauswahl Gandy et Metaphernauszeichnung Übereinstimmung Annotator Kappa erreichen extrahiert Konstruktion anschließend Satz Annotation geeignet Format exportieren Annotationsumgebung Webanno yimam et laden Manuelle weit umfassendst Arbeit Annotationsleitfaden verfassen Identifikationsstrategie Mip reproduzieren Hinweis Umgang lexikalisierter Metaphorischen Ausdrücken Abgrenzung Metonymie enthalten hinaus darstellen Konstruktionstype vormarkiern Ausnahme erwarten eigennamen Hilfsverben Markierung ausnehmen shutova Teufel schließlich festlegen Ausnahme fehlerhaft Satz verfahren satzfragment stark dialektal Form Sätz Kontext interpretierbar ausschuss markieren fehlerhaft vormarkierungen kennzeichnen Rahmen Kuration annotiert sätze verbessern folgend vorläufig Ergebnis automatisch Vorauswahl manuell Annotation berichten automatisch Extraktion Möglicher metaphernkandidaten ermöglichen korpusgestützt Bild erlangen Konstruktion relativ offen Domäne Romankorpus diverser Gattung umfassen verteilen ausgehend Annotationspraxis feststellen erhoben Konstruktion zumindest Rahmen zugrundeliegend Text theoretisch vorkommen metaphorisch öußerungen Wortebene abdecken Praxis aufgrund komplex hypotax fehlend automatisch Fehlerhaft Vormarkierung vorbereitend automatisch Klassifikation Sätz Metapher beziehungsweise führen Anteil sätzen lebendig Metapher enthalten lexikalisiert metaphorisch Ausdrück einrechnen steigen Anteil Sätz Metapher enthalten erheblich Vorteil Klassifikation Sätz ergeben Fülle Material generieren lässt Vorauswahl liegen durchschnittlich Anzahl metaphorisch Ausdrücken pro Satz textsoren shutova Teufel vorgestellt Ansatz Wert Metapher pro Satz erreichen genau Auswertung Präzision Classifier stehen Bezug Struktur ausgewählt Sätz feststellen Klassifizierung Auswirkung Verteilung erhoben Konstruktionstype Übereinstimmung Annotator Aufbau vorgestellt Metaphernkorpus Wert coh äôs Kappa berichten lediglich vormarkiert Konstruktion Grundlage Berechnung heranziehen schwanken Kappa Einbezug Lexikalisierter öußerungen,"[('metaphorisch', 0.3346517560964281), ('metapher', 0.28494317084870546), ('sätz', 0.2709297690480992), ('vorauswahl', 0.212309681838646), ('satz', 0.16829164445855233), ('konstruktion', 0.16253107638441724), ('shutova', 0.1499363032892253), ('automatisch', 0.12802854654965182), ('mip', 0.12738580910318759), ('konstruktionstype', 0.12738580910318759)]"
2017,DHd2017,vortrag-BARTH.xml,Digitale Modellierung literarischen Raums,"Florian Barth (Universität Stuttgart, Deutschland); Gabriel Viehhauser (Universität Stuttgart, Deutschland)","Raum, Narratologie, Digitale Textanalyse, Literaturkartographie","Datenerkennung, Entdeckung, Inhaltsanalyse, Strukturanalyse, Räumliche Analyse, Modellierung, Annotieren, Theoretisierung, Bereinigung, Netzwerkanalyse, Stilistische Analyse, Visualisierung, Daten, Sprache, Literatur, Karte, Methoden, benannte Entitäten (named entities), Forschung, Forschungsprozess, Forschungsergebnis, Text, Visualisierung","Problemstellung Im Anschluss an den 1990 durch den Humangeographen Edward Soja ausgerufenen ""Spatial Turn"" (Soja 1990) haben sich zahlreiche kulturwissenschaftliche Forschungsarbeiten mit einer Beschreibung des Raums beschäftigt. In der Literaturwissenschaft fanden dabei u.a. kartografische Darstellungen große Resonanz: Franco Moretti etwa untersuchte in seinem ""Atlas of the European Novel"" Orte der literarischen Produktion und Rezeption (Moretti 1998), Barbara Piattis Studie ""Die Geographie der Literatur"" richtete den Fokus auf die Illustration einer konkreten literarisch thematisierten Gegend (die Zentralschweiz, vgl. Piatti 2008). Besondere Aufmerksamkeit wurde literarischen Karten auch im Kontext der Digital Humanities zu Teil, in denen geografische Informationssysteme (GIS) zum Einsatz kommen (typische Workflows beschreiben Gregory et. al. 2015) Den meisten dieser Ansätze ist dabei gemein, dass sie für ihre Datengrundlage in erster Linie auf konkrete Nennungen von Ortsnamen (Toponymen) rekurrieren und weitere Ortsmarker weniger stark berücksichtigen. An der Konstitution literarischer Räume sind jedoch in der Regel auch komplexere Faktoren beteiligt, zu deren Beschreibung bereits erste narratologische Ansätze vorliegen (etwa von Kathrin Dennerlein [2009 und 2011] oder Gabriel Zoran [1984], vgl. auch die Überlegungen bei Piatti [2008]), die jedoch im Kontext der Digital Humanities bislang noch zu wenig Beachtung gefunden haben. In unserem Beitrag möchten wir diese Ansätze aufgreifen, um das Instrumentarium der digitalen Textanalyse hinsichtlich der Kategorie des Raums zu schärfen und zu erweitern. Dazu scheinen uns insbesondere zwei Aspekte von Bedeutung: Zum einem die Unterscheidung von Raummarkierungen hinsichtlich ihrer Handlungsrelevanz (I), zum anderen die Ausweitung der Analyse auf räumliche Begriffe, die über bloße Namensnennungen hinausgehen (II). Für beide Problemfelder präsentieren wir erste Verfahren zur automatischen Auswertung und geben Ausblicke auf die Möglichkeiten einer vergleichenden Analyse.  Im Anschluss an Dennerleins Narratologie des Raumes hat sich insbesondere der Terminus der Am Beispiel von Jules Vernes ""Reise um die Erde in 80 Tagen"" lässt sich die Wichtigkeit dieser Unterscheidung aufzeigen: So bietet z.B. Kapitel 14 eine Zugfahrt durch das Gangestal mit Aufenthalten in Allahabad und Benares sowie der Ankunft in Calcutta. Genannt werden im Text jedoch auch weitere Städte Indiens und das nächste Ziel Hongkong; eine Vielzahl der extrahierten Ortsnamen bezieht sich somit nicht auf den Handlungsort des Kapitels. Erst die kategoriale Trennung der Raummarkierungen in Ereignisregionen und erwähnte räumlichen Gegebenheiten ermöglicht die valide Rekonstruktion einer Reiseroute, die dann in einer GIS-Darstellung visualisiert werden kann (Abbildung 1). Eine automatische Unterscheidung dieser Kategorien muss daher das Fernziel computerunterstützter Raumuntersuchungen sein. Ansätze zu einer solchen Differenzierung suchen wir in der Anwendung von computerlinguistischen Methoden der Relationsextraktion, bei denen sich aus strukturellen Auffälligkeiten Regeln zur Klassifikation von Ereignisregionen und erwähnten räumlichen Gegebenheiten ableiten lassen. Hierzu zwei Beispiele: 1. Das 14. Kapitel der ""Reise um die Erde in 80 Tagen"" beginnt mit dem in Abbildung 2 dargestellten Satz: Vor allem der Hauptsatz mit dem Pfad [Figur 'SUBJ 'Bewegungsverb - OBJ 'Raumnomen] zwischen ""Phileas Fogg"" und ""Gangesthal"" kennzeichnet letzteres als Ereignisregion. Dabei stellt insbesondere die Verbkategorie ein Indiz für die Klassifikationsentscheidung dar: Statische Verben (""stehen"", ""sitzen"") oder Verben der Bewegung (""gehen"", ""fahren"") zeugen in spezifischen Satzstrukturen häufig von einer Ereignishaftigkeit im Gegensatz zu Verben der Kognition (""denken""). Zur Einteilung der Verben nutzen wir das von der Universität Tübingen entwickelte lexikalisch-semantische Netz 2. Findet ein erzähltes Ereignis innerhalb einer Bewegung im Raum statt, können mehrere Räume zu einem ""Ich fuhr [...] Im zweiten Satz findet sich zudem eine Referenz auf das Antezedens ""Gründerheim"". Eigentlich werden derartige deiktische Adverbialausdrücke (""hier"", ""da"" und ""dort"") bei der Koreferenz-Resolution nicht berücksichtigt. Deshalb planen wir eine Erweiterung der Trainingssets bestehender Koreferenz-Systeme hinsichtlich dieser Termini. Die hier an zwei Beispielfällen dargestellten Regeln zur Unterscheidung narratologischer Raumeinheiten sollen in Zukunft kontinuierlich erweitert und zunächst so gestaltet werden, dass sie eine hohe Präzision erzielen. Anschließend können sie als Features für spätere maschinelle Lernverfahren verwendet werden.  Netzwerkvisualisierung von Raumnomen Eine kartografische Darstellung, wie sie in Abbildung 1 ersichtlich ist, bleibt auf realweltliche Ortsnamen beschränkt und lässt wesentliche Aspekte der Raumdarstellung außer Acht. Einen ersten Ansatz, die Vielfältigkeit der tatsächlichen Handlungsräume und ihrer Zusammenhänge abzubilden, bietet das Netzwerk in Abbildung 3. Hier werden für das angesprochene Kapitel 14 neben den Toponymen auch unspezifische Raumnomen als Knoten berücksichtigt und Verbindungen immer dann etabliert, wenn zwei Raumbegriffe gemeinsam in einem Satz auftreten. Insbesondere landschaftliche (grün) und architektonische Raumnomen (grau) stellen relevante Klassen von Raummarkern dar, die neben den konkreten Ortsnamen zentrale Komponenten der literarischen Raumbeschreibung bilden. Als räumliche Gegebenheiten kommen aber auch bewegliche Objekte, in denen sich Figuren aufhalten können, in Frage, wie die in der Grafik blau markierten Fahrzeuge. Abbildung 3: Netzwerk Lexikon und Taxonomien für Raumbegriffe Zur lexikalischen Erfassung von realweltlichen Toponymen greifen wir auf die Named-Entitiy-Recognition von Innerhalb dieses Lexikons planen wir zudem eine Einordung der Raumbegriffe in spezifische Taxonomien:  Narratologisch wird unter dem Begriff Statt einer festen Zuschreibung nähern wir uns dem Verhältnis von Ort und Raum über eine vertikale Taxonomie von räumlichen Gegebenheiten an, die von der Planetenebene bis zu jenen Objekten reicht, in denen sich unter Annahme faktualer Gesetzmäßigkeiten keine Figur mehr aufhalten kann. Im Sinne des Abbildung 4 zeigt basierend auf den kapitelweise extrahierten Ereignisregionen in ""Reise um die Erde in 80 Tagen"" die obersten taxonomischen Stufen: 1. Kontinent, 2. Land, 3. Stadt bzw. landschaftliche Region (inklusive Transportmittel, markiert in blau). Während in den ersten beiden Ebenen dieses Beispiels ausschließlich Toponyme vorkommen, beinhaltet zumindest die dritte Stufe in der Nominalphrase ""Latenenwald bei Allahabad"" ein unspezifisches Raumnomen. Weitere allgemeine Begriffe wären vor allem auf einer hier nicht dargestellten vierten Ebene zu finden (z.B. ""Sumpf"", ""Bach"", ""Weizenfeld"" innerhalb der Landschaft ""Behar"", vgl. Abb. 3). Zur automatischen Erstellung einer solchen Hierarchie bieten sich bei Toponymen die in  Wie in Abb. 3 ersichtlich, speisen sich Raumnomen zu großen Teilen aus den Wortfeldern Architektur und Landschaft. Die folgende Analyse basiert auf semiautomatisch erstellten Wortlisten, die auf der Basis von Makroperspektive Das Potential digitaler korpusgestützter Raum-Analysen soll anhand des Vergleichs dreier ""Berlin-Romane"" exemplarisch aufgezeigt werden. Dazu werden die Texte in jeweils zehn Segmente aufgeteilt und hinsichtlich der Frequenz spezifischer Raumbegriffe aus den Wortfeldern Architektur und Landschaft untersucht: Dabei lassen sich deutlich höhere Anteile des architektonischen Vokabulars gegenüber dem Segmentmittelwerten eines Vergleichskorpus erkennen, das aus 451 im Textgrid-Repository enthaltenen Romanen besteht (Abbildung 5, oben). Während die Verteilung des architektonischen Wortschatzes in Hesses ""Heimliches Berlin"" nur temporäre Spitzen zeigt, sind die Segmentverteilungen von ""Berlin Alexanderplatz"" und Wilhelm Raabes ""Die Chronik der Sperlingsgasse"" gegenüber der mittleren Verteilung des Korpus signifikant verschieden. Dies wurde sowohl mit dem Wilcoxon-Rangsummentest (Annahme der Varianzhomogenität und Gleichverteilung zwischen den Sampleverteilungen) sowie dem Mood's Median-Test (keine Verteilungsannahme) überprüft (Abbildung 6). Das landschaftliche Vokabular hingegen liegt bei den Berlin-Romanen tendenziell etwas unter dem Mittel des Korpus, allerdings sind die Abweichungen nur im Fall von ""Berlin Alexanderplatz"" eindeutig signifikant (Abb. 5 unten, Abb. 6) Ungeachtet dieser Unterschiede sind die Zusammenhänge zwischen beiden Wortfeldern auffällig (Abbildung 7). Die Spearman-Korrelation zwischen architektonischen und landschaftlichen Begriffen bei ""Berlin Alexanderplatz"" beträgt 0.5030488 und bei ""Die Chronik der Sperlingsgasse"" sogar 0.7454545. So kann trotz abweichender Anteile der Wortfelder hinsichtlich ihrer Frequenz eine starke Verflechtung spezifischer Klassen von Räumen angenommen werden. Ausblick Die vorgestellten Ansätze verstehen sich als Anregung für die Entwicklung eines differenzierten Instrumentariums der digitalen Raumanalyse, das in Zukunft weiter ausgebaut werden soll und die Grundlage für die Behandlung weiterführender literaturwissenschaftlicher Fragestellungen bildet, die etwa Aspekte der Semantisierung von Räumen (Lotman 1972), des raumzeitlichen Entwurfs von Erzählwelten (Bachtin 1989) und der Bedeutung von Raumkonstellationen für die Gattungspoetik beinhalten (vgl. zusammenfassend Nünning 2009).",de,Problemstellung Anschluss humangeograph Edward Soja ausgerufen spatial Turn Soja zahlreich kulturwissenschaftlich Forschungsarbeit Beschreibung Raum beschäftigen Literaturwissenschaft finden kartografisch Darstellung Resonanz Franco moretti untersuchen atlas -- the european novel orte literarisch Produktion Rezeption moretti Barbara Piattis Studie Geographie Literatur richten Fokus Illustration konkret literarisch thematisierten gegend Zentralschweiz Piatti besonderer Aufmerksamkeit literarisch Karte Kontext Digital humaniteisen geografisch Informationssystem gis Einsatz Typisch workflows beschreiben Gregory et meister Ansatz gemein Datengrundlage Linie konkret Nennung Ortsnamen Toponymen rekurrieren Ortsmarker stark berücksichtigen Konstitution literarisch räume Regel komplex Faktor beteiligen Beschreibung narratologisch Ansatz vorliegen Kathrin dennerlein gabriel zoran Überlegung piatti Kontext Digital Humanitie bislang Beachtung finden unser Beitrag möchten Ansatz aufgreifen Instrumentarium digital Textanalyse hinsichtlich Kategorie Raum schärfen erweitern scheinen insbesondere Aspekt Bedeutung Unterscheidung raummarkierungen hinsichtlich Handlungsrelevanz i Ausweitung Analyse räumlich begriffe bloß Namensnennung hinausgehen ii Problemfelder präsentieren Verfahren automatisch Auswertung geben Ausblicke Möglichkeit vergleichend Analyse Anschluss dennerleinsr Narratologie Raum insbesondere Terminus Jules vernes Reise Erde lässt Wichtigkeit Unterscheidung aufzeigen bieten Kapitel Zugfahrt Gangestal Aufenthalt Allahabad Benar Ankunft calcutta nennen Text Stadt indien nächster Ziel Hongkong Vielzahl extrahiert Ortsnamen beziehen somit Handlungsort Kapitel kategorial Trennung raummarkierunge ereignisregion erwähnt räumlich Gegebenheit ermöglichen valid Rekonstruktion Reiseroute visualisieren Abbildung automatisch Unterscheidung Kategorie Fernziel Computerunterstützter raumuntersuchungen Ansatz Differenzierung suchen Anwendung computerlinguistisch Methode Relationsextraktion strukturell auffälligkeit Regel Klassifikation ereignisregion erwähnt räumlich Gegebenheite ableiten lassen hierzu Beispiel Kapitel Reise Erde beginnen Abbildung dargestellt Satz Hauptsatz Pfad Figur subj bewegungsverb obj raumnomen Phileas Fogg gangesthal kennzeichnen letzterer Ereignisregion stellen insbesondere Verbkategorie Indiz Klassifikationsentscheidung dar statisch verben stehen sitzen verben Bewegung fahren Zeuge Spezifisch Satzstrukture häufig Ereignishaftigkeit Gegensatz verben Kognition denken Einteilung verben nutzen Universität tübingen entwickelt Netz finden erzählt Ereignis innerhalb Bewegung Raum mehrere räume fahren Satz finden zudem Referenz Antezedens gründerheim eigentlich derartig deiktisch Adverbialausdrücke berücksichtigen planen Erweiterung Trainingsset bestehend hinsichtlich Termini beispielfällen dargestellt Regel Unterscheidung Narratologischer Raumeinheit Zukunft kontinuierlich erweitern gestalten hoch Präzision erzielen anschließend Features spät maschinell lernverfahren verwenden Netzwerkvisualisierung raumnomen kartografisch Darstellung Abbildung ersichtlich bleiben realweltlich Ortsnam beschränken lässt wesentlich Aspekt Raumdarstellung Ansatz Vielfältigkeit tatsächlich handlungsräume zusammenhänge abzubilden bieten Netzwerk Abbildung angesprochen Kapitel Toponyme unspezifisch Raumnome knoten berücksichtigen Verbindung etablieren raumbegriffe gemeinsam Satz auftreten insbesondere Landschaftliche grün architektonisch raumnomen Grau stellen relevant Klasse Raummarker dar konkret Ortsnamen zentral Komponente literarisch Raumbeschreibung bilden räumlich Gegebenheit beweglich Objekt Figur aufhalten Frage Grafik blau markiert Fahrzeug Abbildung netzwerk Lexikon Taxonomien raumbegriffe lexikalisch Erfassung realweltlichen Toponyme greifen innerhalb Lexikon planen zudem Einordung raumbegriffe spezifisch Taxonomien narratologisch Begriff fest Zuschreibung nähern Verhältnis Ort Raum vertikal Taxonomie räumlich Gegebenheit Planetenebene Objekt reichen annahme Faktualer gesetzmäßigkeiten Figur aufhalten Sinn Abbildung zeigen basierend kapitelweise extrahiert Ereignisregion Reise Erde oberer taxonomische Stufe Kontinent Land Stadt landschaftlich Region inklusive Transportmittel markieren blau Ebene Beispiel ausschließlich toponyme vorkommen beinhalten zumindest Stufe Nominalphrase Latenenwald allahabad unspezifisch raumnomen allgemein begriffe sein dargestellt Ebene finden sumpf bach Weizenfeld innerhalb Landschaft behar abb automatisch Erstellung Hierarchie bieten Toponyme abb ersichtlich speisen raumnomen Teil Wortfelder Architektur Landschaft folgend Analyse basieren semiautomatisch erstellt wortlisen Basis Makroperspektive Potential Digitaler Korpusgestützter anhand Vergleichs Dreier exemplarisch aufzeigen Text jeweils Segment aufteilen hinsichtlich Frequenz spezifisch raumbegriffe Wortfelder Architektur Landschaft untersuchen lassen deutlich hoch Anteil Architektonische Vokabular Segmentmittelwerten Vergleichskorpus erkennen enthalten Roman bestehen Abbildung Verteilung architektonisch Wortschatz hesses heimlich Berlin temporär Spitze zeigen Segmentverteilung Berlin Alexanderplatz Wilhelm Raabes Chronik Sperlingsgasse mittlerer Verteilung Korpus signifikant verschieden sowohl Annahme Varianzhomogenität Gleichverteilung sampleverteilung Verteilungsannahme überprüfen Abbildung Landschaftliche Vokabular hingegen liegen tendenziell Korpus Abweichung Fall Berlin Alexanderplatz eindeutig Signifikant abb unten abb ungeachtet Unterschied zusammenhänge Wortfelder auffällig Abbildung architektonischen landschaftlich begreifen Berlin Alexanderplatz betragen Chronik Sperlingsgass sogar trotz abweichend Anteil wortfelder hinsichtlich Frequenz stark Verflechtung spezifisch Klasse räumen annehmen Ausblick vorgestellt Ansatz verstehen Anregung Entwicklung differenziert Instrumentariums digital Raumanalyse Zukunft ausbauen Grundlage Behandlung weiterführend literaturwissenschaftlich Fragestellung bilden aspekte Semantisierung räumen lotman raumzeitlich entwurfs Erzählwelt Bachtin Bedeutung Raumkonstellation Gattungspoetik beinhalen zusammenfassend nünning,"[('raumnomen', 0.21497517383841658), ('raumbegriffe', 0.17198013907073326), ('ereignisregion', 0.17198013907073326), ('toponyme', 0.17198013907073326), ('abbildung', 0.15063780448373493), ('räumlich', 0.14691928250924532), ('wortfelder', 0.14532818482305626), ('alexanderplatz', 0.12898510430304994), ('kapitel', 0.12516675974195307), ('verben', 0.12239449274948105)]"
2018,DHd2018,PIELSTRÖM_Steffen_LDA_Topic_Modeling_über_ein_graphisches_In.xml,LDA Topic Modeling über ein graphisches Interface,"Severin Simmler (Universität Würzburg, Deutschland); Thorsten Vitt (Universität Würzburg, Deutschland); Steffen Pielström (Universität Würzburg, Deutschland)","Topic Modeling, Python","Inhaltsanalyse, Modellierung, Visualisierung"," In den letzten Jahren ist das Interesse an LDA als Verfahren für die Analyse literarischer Textcorpora auf Seiten der digitalen Geisteswissenschaften stark gestiegen. Im Kontrast zu diesem gesteigerten Interesse ist die Anwendung der Methode allerdings nicht wesentlich leichter geworden. Gängige Implementierungen des LDA-Algorithmus werden entweder über ein kommandozeilenbasiertes Java-Programm (MALLET von McCallum 2002) oder über Skripte in der Programmiersprache Python (Gensim von Rehurek und Sojka 2010) angesprochen. Die Aufbereitung der Daten vor dem Topic Modeling, das sog. ""Preprocessing"" und die Analyse der Ergebnisse hinterher geschieht dann zumindest in Teilen häufig unter Verwendung weiterer Programme bzw. Arbeitsumgebungen. Alles in allem erfordert die Durchführung einer LDA-basierten Inhaltsanalyse damit zur Zeit relativ umfangreiche technische Kenntnisse. Um den Zugang zu dieser Methode zu erleichtern entwickeln wir im Rahmen von DARIAH-DE (https://de.dariah.eu/) zur Zeit eine ausführlich dokumentierte Python-Programmbibliothek, die es ermöglichen soll, den gesamten Arbeitsprozess einer LDA-basierten Analyse in einer einzigen Umgebung durchzuführen ( Um einen leichtgewichtigen Einstieg in diese Thematik zu bieten haben wir auf Basis unserer Programmbibliothek, der Python-nativen LDA-Implementierung von Allan Riddell (https://pypi.python.org/pypi/lda) und dem Python-Microframework ""Flask"" ( Der GUI-Demonstrator übernimmt und erklärt hierbei exemplarisch alle Arbeitsschritte einer einfachen Analyse. Zunächst werden Textdateien über ein Auswahlmenü eingelesen und tokenisiert. Nutzerinnen und Nutzer können zur Reduktion des Vokabulars auf die Funktionswörter vorgeben, wie viele der häufigsten Wörter aus den Texten entfernt werden sollen, oder alternativ über ein weiteres Auswahlmenü eine externe Stopwortliste einbinden. Die Anzahl der zu berechnenden Topics und die Zahl der Iterationen, über die die Berechnung durchgeführt werden soll, ein Faktor, der die Qualität der Ergebnisse entscheidend beeinflusst, können ebenfalls über das Interface gesteuert werden. In der derzeitigen Form generiert das Programm als Output eine Tabelle mit den zehn am stärksten gewichteten Wörtern in jedem Topic, sowie ein Heatmap als Übersicht über die Verteilung der Topics über die Texte. Im Fokus der gegenwärtigen Weiterentwicklung steht die Gestaltung interaktiver Outputs mit Hilfe von Bokeh ( Das Ziel dieser Entwicklung bleibt aber in erster Linie ein didaktisches: Der GUI-Demonstrator führt die ¬†grundsätzlichen Möglichkeiten der Methode vor und informiert gleichzeitig über die Abläufe im Hintergrund, so dass der Schritt hin zur Verwendung der gleichen Funktionalitäten in einem vorbereiteten Notebook mit interaktiven Codeblöcken, das schnell an die spezifischen Bedürfnisse eine bestimmten Forschungsfrage angepasst werden kann, nur noch klein ist. ",de,letzter Interesse lda Verfahren Analyse literarisch Textcorpora Seite digital geisteswissenschaften stark steigen Kontrast gesteigert Interesse Anwendung Methode wesentlich leicht gängig Implementierunge kommandozeilenbasiert mallet Mccallum Skript Programmiersprache Python gensim Rehurek Sojka ansprechen Aufbereitung daten Topic Modeling Preprocessing Analyse Ergebnis hinterher geschehen zumindest Teil häufig Verwendung weit Programm arbeitsumgebungen erfordern Durchführung Inhaltsanalyse relativ umfangreich technisch Kenntnisse Zugang Methode erleichtern entwickeln Rahmen ausführlich dokumentiert ermöglichen gesamt Arbeitsprozess Analyse einzig Umgebung durchführen leichtgewichtig Einstieg Thematik bieten Basis Programmbibliothek Allan Riddell Flask übernehmen erklären hierbei exemplarisch Arbeitsschritte einfach Analyse textdateien Auswahlmenü eingelesen tokenisieren nutzerinnen nutzer Reduktion Vokabular Funktionswörter vorgeben häufig Wörter Text entfernen alternativ Auswahlmenü extern stopwortlister einbinden Anzahl berechnenden Topic Zahl Iteration Berechnung durchführen Faktor Qualität Ergebnis entscheidend beeinflussen ebenfalls Interface steuern derzeitig Form neriern Programm Output Tabelle stärk gewichtet wörtern Topic Heatmap übersichen Verteilung Topics Text Fokus gegenwärtig Weiterentwicklung stehen Gestaltung interaktiver Output Hilfe Bokeh Ziel Entwicklung bleiben Linie didaktisch führen Möglichkeit Methode informieren gleichzeitig Ablauf Hintergrund Schritt Verwendung gleich Funktionalität vorbereitet Notebook interaktiv Codeblöcken schnell spezifisch bedürfnis bestimmt Forschungsfrage angepasst klein,"[('auswahlmenü', 0.22112382206838144), ('topic', 0.1779311930150645), ('programm', 0.14834989937608828), ('output', 0.1457696764925455), ('analyse', 0.11931336440969345), ('stärk', 0.11056191103419072), ('kommandozeilenbasiert', 0.11056191103419072), ('sojka', 0.11056191103419072), ('implementierunge', 0.11056191103419072), ('arbeitsumgebungen', 0.11056191103419072)]"
2018,DHd2018,FISCHER_Frank_Netzwerkanalytischer_Blick_auf_die_Dramen_Anto.xml,Netzwerkanalytischer Blick auf die Dramen Anton Tschechows,"Veronika Faynberg (Higher School of Economics, Moskau, Russland); Frank Fischer (Higher School of Economics, Moskau, Russland); Svetlana Lashchuk (Higher School of Economics, Moskau, Russland); Tatyana Orlova (Higher School of Economics, Moskau, Russland); German Palchikov (Higher School of Economics, Moskau, Russland); Evgenia Shlosman (Higher School of Economics, Moskau, Russland)","Netzwerkanalyse, Anton Tschechow, Russische Literatur, Drama","Netzwerkanalyse, Visualisierung, Literatur"," In diesem Kontext siedelt sich auch unser Posterprojekt an, in dessen Mittelpunkt die extrahierten Netzwerkdaten zu den Stücken des russischen Dramatikers Anton Tschechow (1860–1904) stehen. Die Datengrundlage bildet das von uns aufgebaute und betriebene Russian Drama Corpus (RusDraCor), das es sich zur Aufgabe gestellt hat, russischsprachige Stücke in der Zeitspanne zwischen den 1740er-Jahren (Sumarokow, Lomonossow u.¬†a.) und den 1930er-Jahren (mit Texten von Autoren wie Majakowski oder Gorki) im TEI-Format zur Verfügung zu stellen (Fischer u. a. 2017). Neben Large-Scale-Analysen zur strukturellen Evolution des russischen Dramas ergibt sich so auch die Möglichkeit zur Betrachtung von nach verschiedenen Kriterien portionierten Teilkorpora, etwa der Stücke einzelner Autoren. Anton Tschechow gehört zu den meistgespielten russischen Dramatikern, dessen Werke bis heute inszeniert werden, gerade auch an deutschsprachigen Bühnen, vor allem seine vier letzten Stücke, ""Die Möwe"", ""Onkel Wanja"", ""Drei Schwestern"" und ""Der Kirschgarten"". Von der Figurenkonstellation her haben diese Werke einen hohen Wiedererkennungswert: Es gibt keine wirklichen Protagonisten; die Redeanteile und Gesprächssituationen sind relativ gleichmäßig über eine Gruppe von Figuren verteilt. Dies zeigt sich sofort auch in den Netzwerkgraphen: Die Knoten (von denen jeder für eine Figur des jeweiligen Dramas steht) bilden einen Die Beschaffenheit des Russian Drama Corpus erlaubt es, quantitative Analysen auch zugeschnitten auf bestimmte Figurengruppen zu beschränken, etwa gesondert nach Geschlecht oder sozialem Status. Bereits eine simple Worthäufigkeitsanalyse kann so etwa zeigen, dass weibliche und männliche Rollen in Tschechow-Stücken von den Redeanteilen und dem Vernetzungsgrad her vergleichbar sind (anders als etwa bei allen anderen Autoren im Korpus). Diese Verteilungsdiagramme sowie netzwerktheoretische Werte wie Dichte, Diameter, Clustering-Koeffizient und Average Path Length ergänzen die chronologisch sortierten Netzwerkvisualisierungen. Die im Poster geschaffene Übersicht über alle Tschechow-Dramen hat auch enzyklopädischen Charakter, enthält sie doch etwa alle Figuren im Kontext ihres Auftretens im Tschechow""schen Dramenkosmos. Der netzwerkanalytische Blick ist somit durchaus geeignet, als Brücke zur inhaltlichen Auseinandersetzung mit den Werken Tschechows zu dienen.",de,Kontext siedeln posterprojekt Mittelpunkt extrahiert netzwerkdaten Stück russisch Dramatikers Anton Tschechow stehen Datengrundlage bilden aufgebaut betrieben Russian Drama Corpus Rusdracor Aufgabe stellen russischsprachig Stück Zeitspanne Sumarokow Lomonossow Text Autor Majakowski gorki Verfügung stellen Fischer strukturell Evolution russisch Dramas ergeben Möglichkeit Betrachtung verschieden kriterien portioniert Teilkorpora Stück einzeln Autor Anton Tschechow gehören meistgespielt russisch dramatikern Werk inszenieren Deutschsprachig Bühne letzter Stück Möwe Onkel wanja Schwester kirschgarten Figurenkonstellation Werk hoch wiedererkennungswern wirklich Protagonist Redeanteil gesprächssituationen relativ gleichmäßig Gruppe Figur verteilen zeigen sofort netzwerkgraph knoten Figur jeweilig Dramas stehen bilden Beschaffenheit russian Drama Corpus erlauben quantitativ analysen zuschneiden bestimmt Figurengruppe beschränken gesondert Geschlecht sozial Status Simple Worthäufigkeitsanalyse zeigen weiblich männlich Rolle Redeanteile Vernetzungsgrad vergleichbar Autor Korpus verteilungsdiagramm netzwerktheoretisch wert Dichte Diameter Average Path Length ergänzen chronologisch sortiert Netzwerkvisualisierung Poster Geschaffene übersichen enzyklopädisch Charakter enthalten Figur Kontext Auftreten Tschechow sch dramenkosmos netzwerkanalytisch Blick somit geeignet Brücke inhaltlich Auseinandersetzung Werk Tschechow dienen,"[('tschechow', 0.386482825460011), ('stück', 0.25928770465015055), ('russisch', 0.21096100058470316), ('anton', 0.1932414127300055), ('russian', 0.17998972599744015), ('dramas', 0.14793359653246005), ('figur', 0.11486927212924034), ('autor', 0.11486927212924034), ('werk', 0.10963507662714209), ('drama', 0.10698994422630256)]"
2018,DHd2018,KONLE_Leonard_Analysing_Direct_Speech_in_German_Novels.xml,Analysing Direct Speech in German Novels,"Fotis Jannidis (Universität Würzburg, Deutschland); Leonard Konle (Universität Würzburg, Deutschland); Albin Zehe (Universität Würzburg, Deutschland); Andreas Hotho (Universität Würzburg, Deutschland); Markus Krug (Universität Würzburg, Deutschland)","Direct Speech, Deep Learning, Machine Learning","Datenerkennung, Programmierung, Stilistische Analyse, Text"," This paper aims to provide a detailed analysis of the use of direct speech across different time periods and domains. To create a reliable database for these analyses, we need to measure the usage of direct speech in a large and representative corpus. This task is more challenging than it may sound: While, nowadays, direct speech is often marked very explicitly by the use of quotes, this has not always been consistently the case. Many historical novels are not available in a well-edited form, meaning that there may be inconsistent use of quotation, or no quotation at all (Brunner, 2013). In this case, a more robust method for detecting direct speech is necessary. Our first contribution is therefore a deep learning-based method to detect direct speech using large amounts of rule-based, but slightly flawed, labelled data extracted from raw text. ¬†This has multiple advantages over the use of manually annotated training data: First, manually annotating large amounts of text is very time-intensive and therefore costly. Furthermore, annotations for one type of texts may not be transferable to other types, leading to the necessity of new annotated data for new corpora. Being able to learn from the already existing weakly labelled data is therefore desirable, as this data can automatically be extracted for a new corpus. Our second contribution is the application of this approach on curated texts to gain insight in trends of direct speech distribution. On one hand we try to look for development of direct speech over time, analysing a large dataset of novels from the nineteenth century, on the other hand we focus on differences in genre comparing contemporary high and low brow literature.  For example, Brunner (2013) tests rule-based and machine learning driven classification, as well as combinations of both, on German novels. She recommends using a pure machine learning approach (Random Forest), reaching an F1 score of 0.87. Scheible et al. (2016) employ a simple greedy algorithm and a semi-Markov model, showing that the latter outperforms the previous state-of-the-art by achieving a precision of 0.88. Although the results seem quite satisfying, these systems require a relatively large amount of labelled data for training. As stated above, this is problematic because of the need for expensive annotation and lack of transferability to other domains. Thus, our goal in this paper differs from that in previous work. We do not aim to set a new state-of-the-art in direct speech detection, but instead: a) present a method that can leverage large amounts of weakly labelled data extracted from raw text, and b) use this model for the analysis of different distributions of direct speech across genres or time-periods. To the best of our knowledge, the second task has never been done on a large collection of texts.  In order to train and evaluate our classifiers, we need to obtain labels specifying which parts of the texts contain direct speeches. To this end, we chose two strategies: For training our classifiers, we decided to extract weak labels using a simple rule based on quotation, implying everything written between quotation marks is direct speech. To yield high accuracy for this approach, it is necessary to use a well-edited collection of texts. Our PD corpus contains such a subset, which we refer to as our Using our quotation rule on the For further evaluation, we chose to annotate a smaller subset of the corpus  We conducted experiments on two different levels, starting with a sentence classification task, which is then refined to detect direct speech on word-level. In our first classification task, documents are split into sentences and vectorised by storing each sentence in a bag-of-words representation. To create a baseline for measuring the advantage using deep learning for direct speech recognition, we compared the performance of traditional machine learning algorithms on our labelled datasets. Training and testing some of the most common machine learning classifiers to detect sentences containing at least one word of direct speech leads to an accuracy of Since we noticed that three of our classifiers all ended up with about the same score, we decided to give the task to two human annotators to establish an upper bound. We selected 250 sentences for manual annotation and again removed all quotation marks. Both annotators ended up with an accuracy comparable to that of the best machine learning methods, 84% and 82.8% respectively. From this result we concluded that it is not expedient to further optimise the sentence classification task, as we had already reached human-level accuracy. Because of the results from the previous section, we decided to modify our task to a word-level prediction, which enables us to include more context by ignoring sentence boundaries and at the same time make more fine-grained predictions. In this second classification task, each word is to be classified separately as inside or outside a direct speech. As baseline for this task, we trained a Linear Chain Conditional Random Field (CRF) that was only given the word itself and its part-of-speech tag. This CRF stagnated at a comparably low accuracy of 0 Since our goal was to provide the classifier with more context, we chose to use an architecture based on recurrent neural networks, which are able to deal with relatively large contexts. Our assumption here is that, for a good classification, we need context from both before and after the target word itself, as markers for direct speech can be found at the beginning or the end of the direct speech. We thus designed a two-branch network, visualised in Figure 1. This network receives as input a text-segment, specifically the target word in its context. The words of the input are then passed through an embedding layer and split into two parts, where the first part contains the context up to the target word and the second part contains the context following the target word. The target word itself is contained in both parts. Each part is passed through three separate LSTM-layers. In the future-branch, the context is passed through the layers in reverse, so that the target word is the last word to be read in both branches. The LSTM-layers in the past-branch are stateful and can therefore theoretically retain the entire context of the novel up to the target word. The outputs of the final LSTM-layer of both branches are concatenated. The final prediction is made based on this concatenation by a fully connected layer. In our best setup, we used 60 words before and after the target word as context. Training on one half of the   This finding is contrary to the assumption mentioned above. We propose that, while there is no clear difference in the average use of direct speech between high and low brow literature, authors in high brow literature are far more flexible in choosing how much direct speech they use in their novels. Low brow literature, on the other hand, is expected to have a rather constant amount of dialogue.  While an accuracy of 0.9 is remarkable, there is still need for optimisation. Recent developments in the performance of neural networks by adding an attention mechanism (see Rush 2015) could improve the results. We used our neural network to analyse the distribution of direct speech over time and genres. Besides algorithmic refinements, there is a lot of potential in adding more text to our corpus and refining metadata to allow more sophisticated research questions like differences between or development of direct speech in certain genres.",en,paper aim provide detailed analysis use direct speech different time period domain create reliable database analysis need measure usage direct speech large representative corpus task challenging sound nowadays direct speech mark explicitly use quote consistently case historical novel available edit form mean inconsistent use quotation quotation brunner case robust method detect direct speech necessary contribution deep learning base method detect direct speech large amount rule base slightly flawed label datum extract raw text multiple advantage use manually annotate training datum manually annotate large amount text time intensive costly furthermore annotation type text transferable type lead necessity new annotated datum new corpora able learn exist weakly label datum desirable datum automatically extract new corpus second contribution application approach curate text gain insight trend direct speech distribution hand try look development direct speech time analyse large dataset novel nineteenth century hand focus difference genre compare contemporary high low brow literature example brunner test rule base machine learn drive classification combination german novel recommend pure machine learning approach random forest reach score scheible et al employ simple greedy algorithm semi markov model show outperform previous state art achieve precision result satisfying system require relatively large label datum training state problematic need expensive annotation lack transferability domain goal paper differ previous work aim set new state art direct speech detection instead present method leverage large amount weakly label datum extract raw text b use model analysis different distribution direct speech genre time period good knowledge second task large collection text order train evaluate classifier need obtain label specify part text contain direct speech end choose strategy train classifier decide extract weak label simple rule base quotation imply write quotation mark direct speech yield high accuracy approach necessary use edit collection text pd corpus contain subset refer quotation rule evaluation choose annotate small subset corpus conduct experiment different level start sentence classification task refine detect direct speech word level classification task document split sentence vectorise store sentence bag word representation create baseline measure advantage deep learning direct speech recognition compare performance traditional machine learn algorithm label dataset training test common machine learn classifier detect sentence contain word direct speech lead accuracy notice classifier end score decide task human annotator establish upper bind select sentence manual annotation remove quotation mark annotator end accuracy comparable good machine learning method respectively result conclude expedient optimise sentence classification task reach human level accuracy result previous section decide modify task word level prediction enable include context ignore sentence boundary time fine grain prediction second classification task word classify separately inside outside direct speech baseline task train linear chain conditional random field crf give word speech tag crf stagnate comparably low accuracy goal provide classifier context choose use architecture base recurrent neural network able deal relatively large context assumption good classification need context target word marker direct speech find beginning end direct speech design branch network visualise figure network receive input text segment specifically target word context word input pass embed layer split part contain context target word second contain context follow target word target word contain part pass separate lstm layer future branch context pass layer reverse target word word read branch lstm layer past branch stateful theoretically retain entire context novel target word output final lstm layer branch concatenate final prediction base concatenation fully connect layer good setup word target word context training half finding contrary assumption mention propose clear difference average use direct speech high low brow literature author high brow literature far flexible choose direct speech use novel low brow literature hand expect constant dialogue accuracy remarkable need optimisation recent development performance neural network add attention mechanism rush improve result neural network analyse distribution direct speech time genre algorithmic refinement lot potential add text corpus refining metadata allow sophisticated research question like difference development direct speech certain genre,"[('speech', 0.41863122884148896), ('direct', 0.4102786333753353), ('word', 0.24852216210039246), ('context', 0.16545254590753006), ('target', 0.15222953776054143), ('quotation', 0.14262371870076046), ('use', 0.1365364477830844), ('task', 0.1331563519474693), ('sentence', 0.13001748088206688), ('layer', 0.1293944941759208)]"
2018,DHd2018,LEBHERZ_Daniel_Text_Mining_und_Computersimulation_zur_Analys.xml,Text Mining und Computersimulation zur Analyse autobiographischer Texte: Einflüsse auf das literarische Schaffen Klaus Manns,Jan Hess (Trier Center for Digital Humanities (TCDH); Universität Trier); Daniel Lebherz (Center for Informatics Research and Technology (CIRT); Universität Trier); Christian Zeyen (Center for Informatics Research and Technology (CIRT); Universität Trier),"Klaus Mann, autobiographische Texte, Text Mining, Workflows, agentenbasierte Sozialsimulation","Inhaltsanalyse, Strukturanalyse, Beziehungsanalyse, Modellierung, Annotieren, Literatur","""Ist das alles?"" (Mann 1995, S. 151) 'Die vorwurfsvoll anmutende (rhetorische) Frage, die Klaus Mann Ende Juni 1933 unter einer Auflistung seiner Werke des zurückliegenden Halbjahrs in seinem Tagebuch notiert, gibt einen Hinweis darauf, wie selbstkritisch sich der Schriftsteller mit dem eigenen Schaffen auseinandersetzt. Angesichts der persönlichen und politischen Umstände sowie der Vielzahl an Kontakten und Aktivitäten, die er in seinem Journal verzeichnet, erscheint es fast etwas überraschend, dass die gut drei Monate nach seiner Emigration erstellte Werkliste immerhin 21 'wenn auch größtenteils kürzere 'Texte umfasst. In Anbetracht seiner ständigen Rastlosigkeit stellt sich nicht nur die Frage, wann Klaus Mann überhaupt seiner eigentlichen schriftstellerischen Arbeit nachgeht, sondern auch, welche Faktoren zum Ge- oder Misslingen seines Schaffens beitragen. Wie wirken sich beispielsweise die beinahe allabendlichen Theater-, Kino- und Barbesuche oder die mehr oder weniger regelmäßig eingenommenen Rauschmittel auf seine literarische Produktivität aus? Welche Rolle spielt das tägliche Lektürepensum? Haben die zahlreichen Treffen, Telefonate und Korrespondenzen Einfluss auf seine schriftstellerische Arbeit? Oder erweisen sich die politischen Umstände, Angst, Verzweiflung und Hass gegenüber dem Nationalsozialismus als entscheidendes Vehikel literarischer Produktivität? Aufgrund des Umfangs und der Detailgenauigkeit seiner alltäglichen Schilderungen von Gedanken und Aktivitäten 'insbesondere auch der teilweise bis auf die Tageszeit genauen Protokollierung seiner Arbeitsprozesse 'erscheinen Klaus Manns Tagebücher als idealer Untersuchungsgegenstand zur Beantwortung der aufgeworfenen Fragen. Gerade die Vielzahl und Komplexität der darin enthaltenen Informationen machen es jedoch schwierig, sich potentiellen Faktoren literarischer Produktivität tiefergehend analytisch zu nähern. Um diese große Anzahl an Informationen und deren Zusammenhänge besser bzw. überhaupt untersuchen zu können, sollen im Rahmen von Methoden der Computersimulation haben sich bereits seit längerer Zeit in verschiedenen¬†Wissenschaftsdisziplinen etabliert. Seit den 1990er Jahren halten diese auch vermehrt Einzug in sozialwissenschaftliche Forschungsbereiche und bieten dort verschiedene Möglichkeiten zur Darstellung und Analyse von gesellschaftlichen Systemzusammenhängen (Gilbert 2007). Eine ähnliche Entwicklung lässt sich auch in den Digital Humanities nachvollziehen, wenngleich diese Methoden 'insbesondere im literaturwissenschaftlichen Bereich 'eine eher untergeordnete Rolle spielen (Kohle 2017). Die jedoch insgesamt zunehmende Anwendung von Computersimulation als Forschungsmethode liegt darin begründet, dass sie die Möglichkeit bietet, komplexe, unzugängliche Systeme zu untersuchen und experimentell zu analysieren. In solchen Experimenten lassen sich u.a. durch die Betrachtung verschiedener Szenarien Resultate erzielen, die Rückschlüsse auf Plausibilitäten von Handlungen, Strukturen und Zusammenhängen im zugrundeliegenden System erlauben. Agentenbasierte Computersimulation bietet darüber hinaus insbesondere bei der Analyse von individuellem Entscheidungsverhalten die Möglichkeit, sogenannte emergente Effekte aufzudecken. Diese ergeben sich aus dem Zusammenspiel individueller Handlungen und Interaktionen von einzelnen Akteuren, lassen sich jedoch nicht ausschließlich durch diese erklären (Bonabeau 2002). Ein direkter Zusammenhang zwischen Systeminput und -output muss bei agentenbasierter Simulation also nicht bestehen. Dies steht im Gegensatz zu anderen statistischen Analyseverfahren, bei denen i.d.R. eine (lineare) Abhängigkeit zwischen verschiedenen Objekten vorausgesetzt wird. Ferner ermöglicht agentenbasierte Computersimulation detaillierte Analysen von Komponenten der Makroebene. Der Weg zu verlässlichen Ergebnissen einer Simulationsstudie führt in einem ersten, wesentlichen Schritt über die Modellbildung und Sammlung dafür relevanter Daten (Law 2015). Zu diesem Zweck soll das zugrundeliegende System, also konkret Klaus Mann, seine sozialen Kontakte und sein Umfeld möglichst realitätsnah mit allen wesentlichen Eigenschaften, Aktivitäten und Zusammenhängen in einer agentenbasierten Sozialsimulation (Davidsson 2002) abgebildet werden (Abb. 2). Die dafür notwendige Generierung einer geeigneten Datengrundlage ist ein anspruchsvoller Prozess, in dem anhand von Methoden aus dem Bereich des Text Mining zunächst die wesentlichen, auf den Untersuchungsgegenstand einwirkenden Faktoren identifiziert und analysiert werden müssen. Konkret sollen dabei u.a. Methoden wie Named Entity Recognition, Worthäufigkeits- oder Kookkurrenzanalysen angewendet werden, um vorab Kontakte, Aktivitäten, Aufenthalte, Tätigkeiten, persönliche oder politische Ereignisse zu identifizieren, welche mit Klaus Manns literarischer Produktivität in Zusammenhang stehen könnten. Anders als im Falle der Computersimulation kommen Text Mining-Methoden, -Werkzeuge und Programmbibliotheken im literaturwissenschaftlichen Kontext bereits ungleich häufiger zum Einsatz. Zur Komplexitätsreduktion werden die zugrundeliegenden Text Mining-Prozesse dabei jedoch oft als Black Box betrachtet, was den Nachteil birgt, dass das Zustandekommen der erzielten Ergebnisse nur schwer nachvollzogen werden kann. Um dem entgegenzuwirken sollen die im Rahmen von Im Kontext von",de,Mann vorwurfsvoll anmutend rhetorisch Frage Klaus Mann Juni Auflistung Werk zurückliegend Halbjahr Tagebuch notieren Hinweis selbstkritisch Schriftsteller Schaffen auseinandersetzt angesichts persönlich politisch umstände Vielzahl Kontakt Aktivität Journal verzeichnen erscheinen fast überraschend Monat Emigration erstellt werkliste immerhin größtenteils kurz Text umfassen Anbetracht ständig Rastlosigkeit stellen Frage Klaus Mann eigentlich schriftstellerisch Arbeit nachgehen Faktor Misslinge Schaffen beitragen wirken beispielsweise beinahe allabendlich Barbesuch regelmäßig eingenommen Rauschmittel literarisch Produktivität Rolle spielen täglich Lektürepensum zahlreich treffen Telefonat Korrespondenz einfluss schriftstellerisch Arbeit erweisen politisch umstände Angst Verzweiflung Hass Nationalsozialismus entscheidend Vehikel literarisch Produktivität aufgrund Umfang Detailgenauigkeit alltäglich Schilderung Gedanke Aktivität insbesondere teilweise Tageszeit genau Protokollierung arbeitsprozeß erscheinen Klaus Mann Tagebücher ideal Untersuchungsgegenstand Beantwortung aufgeworfen Frage Vielzahl Komplexität enthalten Information schwierig potentiell Faktor literarisch Produktivität tiefergehend analytisch nähern Anzahl Information zusammenhänge untersuchen Rahmen Methode Computersimulation lang etablieren halten vermehrt Einzug sozialwissenschaftlich Forschungsbereich bieten verschieden Möglichkeit Darstellung Analyse gesellschaftlich systemzusammenhänger gilbert ähnlich Entwicklung lässen Digital Humanitie nachvollziehen wenngleich Methode insbesondere literaturwissenschaftlich Bereich eher untergeordnet Rolle spielen Kohle insgesamt zunehmend Anwendung Computersimulation Forschungsmethode liegen begründen Möglichkeit bieten komplex unzugänglich System untersuchen experimentell analysieren experimenten lassen Betrachtung verschieden Szenarien Resultat erzielen rückschluß Plausibilität handlungen Struktur Zusammenhäng zugrundeliegend System erlauben agentenbasiert Computersimulation bieten hinaus insbesondere Analyse individuellem entscheidungsverhalten Möglichkeit sogenannter emergent effekte aufdecken ergeben Zusammenspiel individuell handlungen Interaktion einzeln Akteur lassen ausschließlich erklären Bonabeau direkt Zusammenhang Systeminput agentenbasiert Simulation bestehen stehen Gegensatz statistisch analyseverfahren linear Abhängigkeit verschieden Objekt voraussetzen ferner ermöglichen agentenbasiert Computersimulation detailliert Analyse Komponente Makroebene Weg verlässlich Ergebnis Simulationsstudie führen wesentlich Schritt Modellbildung Sammlung relevant daten law Zweck zugrundeliegend System konkret Klaus Mann sozial Kontakt Umfeld möglichst realitätsnah wesentlich eigenschaften Aktivität zusammenhängen agentenbasiert Sozialsimulation Davidsson abbilden abb notwendig Generierung geeignet Datengrundlage Anspruchsvoller prozess Anhand Methode Bereich Text mining wesentliche Untersuchungsgegenstand einwirkend Faktor identifizieren analysieren konkret Methode named entity Recognition Kookkurrenzanalysen anwenden vorab Kontakt Aktivität aufenthalt Tätigkeit persönlich politisch Ereignis identifizieren Klaus Mann literarisch Produktivität Zusammenhang stehen können Fall Computersimulation Text Programmbibliotheke literaturwissenschaftlich Kontext ungleich häufig Einsatz Komplexitätsreduktion zugrundeliegend Text Black box betrachten Nachteil bergen Zustandekommen erzielt Ergebnis schwer nachvollziehen entgegenwirken Rahmen Kontext,"[('klaus', 0.2842487518849536), ('computersimulation', 0.2842487518849536), ('mann', 0.25027806603720176), ('agentenbasiert', 0.24414118117728767), ('produktivität', 0.22739900150796288), ('aktivität', 0.18689929080419673), ('kontakt', 0.14430973126533286), ('politisch', 0.13031244272265957), ('schriftstellerisch', 0.12207059058864383), ('umstände', 0.1077601179953711)]"
2018,DHd2018,BURGHARDT_Manuel_Digital_Dylan___Computergestützte_Analyse_d.xml,Digital Dylan 'Computergestützte Analyse der Liedtexte von Bob Dylan (1962 '2016),"Colin Sippl (Lehrstuhl für Medieninformatik, Universität Regensburg); Florian Fuchs (Lehrstuhl für Medieninformatik, Universität Regensburg); Manuel Burghardt (Lehrstuhl für Medieninformatik, Universität Regensburg)","Liedtexte, Korpusvergleich, Frequenzanalyse, Signifikanztest","Entdeckung, Inhaltsanalyse, Webentwicklung, Text, Visualisierung","Am 13. Oktober 2016 gab die Schwedische Akademie bekannt, dass sie den Nobelpreis in Literatur an Bob Dylan ""für seine poetischen Neuschöpfungen in der großen amerikanischen Songtradition"" verleihen werde. Die (welt-)politischen Entwicklungen, die das Schaffen Dylans inspirierten, sind im Kontext seines Wirkens umfassend diskutiert worden, unter anderem in ""Bob Dylan und die sechziger Jahre: Aufbruch und Abkehr"", Dieser Beitrag erprobt, inwiefern mithilfe digitaler Methoden im Sinne des Bereits vor der Auszeichnung Dylans mit dem Nobelpreis in Literatur, waren seine Texte Gegenstand wissenschaftlicher Betrachtungen im Sinne des Eine umfassende Untersuchung Dylans Werks mithilfe computerbasierter Methoden fand sich bis zum Abfassungszeitpunkt des vorliegenden Texts nicht. Allerdings sind quantitative Verfahren zur stilistischen und inhaltlichen Analyse von Liedtexten in den Digital Humanities durchaus verbreitet. So beschreiben etwa, Den analytischen Bezugsrahmen dieser Studie stellt die phasenweise Einteilung von Dylans Schaffen nach Brown (2014) In dieser Arbeit wurde ein Korpus bestehend aus 452 Liedtexten mit einem Umfang von 133.045 Tokens untersucht, die Bob Dylan zwischen den Jahren 1962 und 2016 auf Studio-Alben veröffentlicht hat. Die Liedtexte und Metainformationen wie etwa Titel, Album und Jahr stammen von der Plattform Das Korpus wurde weiterhin mit Methoden der Computerlinguistik aufbereitet, insbesondere unter Verwendung des Ein etabliertes Verfahren, um aus einem Korpus spezifische Wörter zu extrahieren, ist ein direkter Korpusvergleich mit dem Als Referenzkorpus dient das mündliche Subkorpus des Beim Korpusvergleich kann entweder das gesamte Dylan-Korpus mit dem Referenzkorpus verglichen werden, oder mit den jeweiligen Dylan-Subkorpora, also bspw. all seinen Texten aus den 1970er-Jahren oder aus der ersten Schaffensperiode ""Becoming Bob Dylan"" (1960-1964). Ein Vergleich der einzelnen Dylan-Subkorpora zum Gesamtwerk ist ebenso möglich. Letztere Option wird z.B. genutzt, um anhand jeweils signifikanter Wörter die einzelnen Schaffensperioden nach Brown (2014) zu überprüfen und damit die grundsätzliche Eignung solch quantitativer Verfahren zur Identifikation thematischer Verschiebungen zu untersuchen. Die Ergebnisse dieses Korpusvergleichs sind, zusammen mit allen anderen Ergebnissen der angewandten Analyseverfahren, in einer interaktiven Webanwendung über unterschiedliche Visualisierungen (Balkendiagramm, Im direkten Vergleich des gesamten Dylan-Korpus (1962-2016) mit dem OANC-Referenzkorpus treten einige interessante, signifikant-häufige Wörter im Werk Dylans hervor. Die von Bob Dylan verwendeten Adjektive erzeugen in der Gesamtschau tendenziell eher eine bedrückende Stimmung ( Die Analyse signifikant-häufiger Wörter für die einzelnen Schaffensphasen Dylans liefert Ergebnisse mit hoher Aussagekraft. So fällt etwa für die Phase ""The Changing of the Guard"" (1978-1981), in der sich Dylan dem Christentum hinwendet, auf, dass das Vokabular tatsächlich viele christliche Motive aufweist ( Ein differenziertes Bild ergibt sich für die N-Gramm-Analyse, was einerseits der Vielfalt an verfügbaren Methoden zur Berechnung Im Sinne einer Kritik der Digitalen Vernunft bleibt demnach festzuhalten, dass sich Methoden der computergestützten Textanalyse und des statistischen Korpusvergleichs grundsätzlich dafür eignen, einen inhaltlichen Gesamtüberblick zu einem Liedtext-Korpus zu erhalten. Es können damit diachrone Entwicklungen des Wortschatzes und Verlagerungen thematischer Schwerpunkte als grobe Tendenzen aufgezeigt werden, um das Bild des Gesamtwerks zu ergänzen. Ein solcher Ansatz eignet sich demnach gut für die initiale Thesengenerierung und kann in gewisser Weise die Funktion eines Empfehlungs- bzw. Hinweissystems für erklärungsbedürftige Stellen Die Identifikation konkreter Schaffensperioden, ausschließlich auf Basis signifikant häufiger Wörter ist aber 'zumindest für das Werk Dylans 'nicht ohne Weiteres erfassbar. Bei den N-Grammen zeigt sich, dass im Falle von Dylans Texten methodenübergreifend und mit zunehmender N-Gramm-Länge meist keine brauchbaren Ergebnisse erzielt werden konnten. Dies ist ein Hinweis darauf, dass die hier präsentierten Analysemethoden, die für andere Textsorten wie bspw. Parlamentsprotokolle bereits erfolgreich eingesetzt werden konnten (vgl. Sippl et al. 2016), auf Liedtexte nur eingeschränkt anwendbar sind. Ein möglicher Kritikpunkt am hier beschriebenen Vorgehen mag zudem das verwendete OANC-Referenzkorpus sein, welches trotz hoher Anteile mündlicher Kommunikation doch nur beschränkt vergleichbar mit der Textsorte ""Liedtext"" ist. Für künftige Vergleichsstudien böte sich ggf. ein Vergleich mehrerer unterschiedlicher Künstler und deren Liedtexte an, also bspw. Bob Dylan vs. Johnny Cash.",de,Oktober schwedisch Akademie Nobelpreis Literatur Bob Dylan Poetische neuschöpfungen amerikanisch songtradition verleihen Entwicklung schaffen dylanr inspirieren Kontext wirkens umfassend diskutieren Bob Dylan sechziger Aufbruch Abkehr Beitrag erproben inwiefern Mithilf Digitaler Methode Sinn Auszeichnung dylan Nobelpreis Literatur Text Gegenstand wissenschaftlich Betrachtung Sinn umfassend Untersuchung dylanr werks Mithilfe Computerbasierter Methoden finden Abfassungszeitpunkt vorliegend texts quantitativ Verfahren stilistisch inhaltlich Analyse Liedtexte Digital Humanitie verbreiten beschreiben analytisch bezugsrahmen Studie stellen phasenweis Einteilung dylan Schaffe brown Arbeit Korpus bestehend Liedtexte Umfang Token untersuchen Bob Dylan veröffentlichen Liedtexte Metainformation Titel Album stammen Plattform korpus weiterhin Methode Computerlinguistik aufbereiten insbesondere Verwendung etabliert Verfahren Korpus spezifisch Wörter extrahieren direkt korpusvergleich Referenzkorpus dienen Mündliche Subkorpus korpusvergleich gesamt Referenzkorpus vergleichen jeweilig all Text Schaffensperiode Becoming Bob Dylan Vergleich einzeln Gesamtwerk letzterer Option nutzen anhand jeweils signifikant Wörter einzeln Schaffensperioden brown überprüfen grundsätzlich Eignung solch quantitativ Verfahren Identifikation thematisch Verschiebung untersuchen Ergebnis Korpusvergleich Ergebnis angewandt analyseverfahren interaktiv Webanwendung unterschiedlich visualisierung Balkendiagramm direkt Vergleich gesamt treten interessant Wörter Werk dylans hervor Bob Dylan verwendet adjektiv erzeugen Gesamtschau tendenziell eher bedrückend Stimmung Analyse Wörter einzelner Schaffensphase dylanr Liefert Ergebnis hoch Aussagekraft fallen Phase The Changing of -- guard Dylan Christentum hinwenden vokabular tatsächlich christlich Motiv Aufweist differenziert Bild ergeben einerseits Vielfalt verfügbar Methode Berechnung Sinn Kritik digital Vernunft bleiben demnach festhalten Methode computergestützt Textanalyse statistisch Korpusvergleich grundsätzlich eignen inhaltlich Gesamtüberblick erhalten diachron Entwicklung Wortschatz verlagerung thematisch Schwerpunkt grob Tendenz aufzeigen Bild Gesamtwerk ergänzen Ansatz eignen demnach initial Thesengenerierung gewiß Weise Funktion hinweissystems erklärungsbedürftig Stelle Identifikation konkret schaffensperioden ausschließlich Basis Signifikant häufig Wörter zumindest Werk dylanr erfassbar zeigen Fall dylan Text Methodenübergreifend zunehmend meist brauchbar Ergebnis erzielen Hinweis Präsentiert analysemethoden Textsort parlamentsprotokoll erfolgreich einsetzen Sippl et Liedtexte eingeschränkt anwendbar möglich kritikpunken beschrieben vorgehen zudem verwendet trotz hoch Anteil mündlich Kommunikation beschränkt vergleichbar Textsorte Liedtext künftig vergleichsstudien böte Vergleich mehrere unterschiedlich Künstler Liedtexte Bob Dylan Johnny cash,"[('dylan', 0.5537482021771787), ('bob', 0.3322489213063072), ('liedtexte', 0.24441583889530488), ('dylanr', 0.22149928087087145), ('korpusvergleich', 0.19553267111624392), ('wörter', 0.12521529426033184), ('schaffensperioden', 0.11074964043543573), ('nobelpreis', 0.10315489394677707), ('brown', 0.09358664495362654), ('gesamtwerk', 0.08728420046084817)]"
2018,DHd2018,BÖRNER_Ingo_Cäsar_Flaischlens__Graphische_Litteratur_Tafel__.xml,"Cäsar Flaischlens ""Graphische Litteratur-Tafel"" 'digitale Erschließung einer großformatigen Karte zur Deutschen Literatur","Ingo Börner (Universität Wien, Österreich); Frank Fischer (National Research University Higher School of Economics, Moskau, Russland); Angelika Hechtl (Wirtschaftsuniversität Wien, Österreich); Robert Jäschke (University of Sheffield, UK); Peer Trilcke (Universität Potsdam, Deutschland)","Literaturkarte, Literaturgeschichtsschreibung, TEI Encoding, Bildanalyse","Transkription, Inhaltsanalyse, Annotieren, Literatur, Karte, Visualisierung","Cäsar Flaischlens ""Graphische Litteratur-Tafel"" von 1890 stellt den Versuch dar, die Entwicklung der Deutschen Literatur mit ihren Einflüssen aus anderen Nationalliteraturen graphisch in der Form eines Flusses darzustellen. Gegenstand des Vortrags ist die digitale Edition und Bereitstellung des Vorwortes und der Karte sowie der entwickelte Workflow: Für die Edition wurde das graphische Karteninventar kodiert. Mithilfe computergestützter Bildanalyse können nicht-textuelle Informationen der Visualisierung erfasst und einer quantitativen Analyse zugeführt werden. In den letzten Jahren lässt sich ein Trend innerhalb der Literaturwissenschaft 'u.a. der Literaturgeschichtsschreibung 'ausmachen, Fragestellungen auf der Grundlage von großen Datenkorpora zu beantworten. Charakteristisch für diese Art von Literaturwissenschaft ist ein Methodenimport aus Natur- und Sozialwissenschaften, der sich nicht zuletzt in den Darstellungsformen deutlich zeigt. Auch wenn sich diese Zugänge gegenwärtig großer Beliebtheit erfreuen, sind Darstellungsweisen wie jene in Morettis einflussreichem Buch ""Kurven, Karten, Stammbäume: Abstrakte Modelle für die Literaturgeschichte"" (Moretti 2007) keineswegs ein Phänomen der Gegenwart, denn Literaturgeschichtsschreibung bedient sich bereits seit der Antike Bildmedien und anderer 'nicht rein textueller 'Präsentationsformen. Darstellungen von AutorInnen, wie etwa jene auf Raffaels berühmtem Parnassfresko in den Vatikanischen Museen lassen sich aus heutiger Perspektive wie Diagramme lesen. Information zu Relevanz sowie Verbindungen einzelner AutorInnen sind hier im Bildmedium kodiert. (vgl. Hölter/Schmitz-Emans 2013, Hölter 2005) Das Parnassfresko ist nur eine Art, wie sich Kanonbildung, Rezeption und die Zugehörigkeit zu einer AutorInnengruppe darstellen lassen. Häufig gewählte Darstellungsformen sind (Stamm-)Baum (Lima 2014) und Fluss. Die Literaturwissenschaft greift damit Formen auf, die erst mit Fortschritten in der Buchproduktion durch die Entwicklung der Lithographie möglich geworden sind und zunächst in der Geschichtsschreibung Anwendung gefunden haben (vgl. Rosenberg/Grafton 2010). Wie produktiv sich diese tradierten Denkbilder auf die Konzeptualisierung von (Literatur-)Karten auswirken können, zeigt die ""Graphische Litteratur-Tafel"" (1890) des deutschen Autors Cäsar Flaischlen (1864–1920). Flaischlens großformatige Karte (58x86,5 cm) visualisiert den 'wie es im Untertitel heißt '""Einfluss fremder Literaturen"" auf die deutsche Literatur und bedient dafür die Denkfigur geschichtlicher Prozesse als Fluss, bestehend aus einer Summe von Einflüssen. Er knüpft damit an eine Darstellungstradition von Weltgeschichte an, wie sie durch die Graphik ""Strom der Zeiten"" (1804) des österreichischen Historiographen Friedrich Strass maßgeblich geprägt wurde (vgl. Rosenberg und Grafton, 2010). In Cäsar Flaischlens ≈íuvre nimmt die aufwendig gestaltete Litteratur-Tafel eine Sonderstellung ein: Die für ihre Zeit ungewöhnliche literaturwissenschaftliche Arbeit erschien beinahe zeitgleich mit seiner Promotion 1899 und sollte Flaischlens einzige Publikation zur Literaturgeschichte bleiben. Flaischlen verließ die akademische Welt und war als Mitherausgeber und Redakteur von Literatur- und Kunstzeitschriften tätig. Heute ist der Autor hauptsächlich für seine Mundartgedichte und Erzählungen bekannt. Auf der Karte wird die deutschsprachige Literatur von ihren Anfängen bis in Flaischlens Gegenwart dargestellt. Was in der Grafik zunächst als zwei sich schlängelnde Bäche der ""Volks- und Kunstpoesie"" um 750 beginnt, entwickelt sich im Laufe der Jahrhunderte zu einem breiten Strom, in welchen über den gesamten (Zeit-)Verlauf Zuflüsse aus anderen (hauptsächlich) europäischen Nationalliteraturen einmünden. In der Legende der Karte führt Flaischlen folgende Einflüsse auf: Altes- und Neues Testament, Englisch, Französisch, Nordisch, Orientalisch, Klassisches Altertum, Spätlateinisch, Niederländisch, Italienisch, Spanisch, Schwedisch/Dänisch/Norwegisch, Russisch. Als Vertreter positivistischer Denkrichtung unternimmt Cäsar Flaischlen also den Versuch, die Darstellung der Zeit als Fluss mit den exakten Wissenschaften zu verbinden. Die Tatsache, dass er keine Quellen für die Zusammenstellung seiner Tafel nennt, spricht dafür, dass er den gängigen Kanon abbildet. Im 8-spaltigen Vorwort zur Tafel spielt Flaischlen zwar den Zusammenhang von quantitativem Befund und Visualisierung herunter 'so gibt er etwa zu bedenken, dass die Breite des Flusses ""nicht mathematisch berechnet"" sei, die Platzierung der Autoren folgt jedoch einem gewissen Prinzip: Die Autoren habe er am ""Höhepunkt"" ihres Schaffens eingezeichnet. Der Informationsgehalt der ""Litteraturtafel"" ist sehr hoch: Auswahl, Platzierung und Größe der beeinflussenden und beeinflussten Autoren ermöglichen die Rekonstruktion von Flaischlens Datengrundlage und lassen Rückschlüsse auf die hierfür verwendeten Quellen zu. So sind etwa in der Tafel Namen von Autoren, kanonischen Texten, literarischen Gruppierungen und literarischen Schulen enthalten. Ferner nutzt Cäsar Flaischlen typographische Gestaltungsmöglichkeiten wie Schriftart, Schriftgröße, Farbwahl und Unterstreichung), Symbole (Kreise in verschiedenen Größen, römische und lateinische Ziffern) und die farbliche Schraffur der (Zu-)Flüsse. Am unteren Rand der Karte ist zwar eine Legende angebracht, diese weist jedoch lediglich die Bedeutung der Schraffur aus (z.B. blau für Einflüsse aus der Englischen Literatur, rot für Einflüsse aus der Französischen Literatur, etc.). Weitere Angaben, insbesondere Erläuterungen zu verwendeten Schriftarten und -größen fehlen jedoch. Das vorgestellte Projekt ""Cäsar Flaischlens Graphische Litteratur-Tafel digital"" unternimmt den Versuch eines ""reverse engineering"" und erschließt dieses Dokument früher Visualisierung literaturgeschichtlicher Daten mit Methoden der Digital Humanities. Dazu wurden die Koordinaten von angeführten Personen, Texten, literarischen Strömungen und Schulen unter Verwendung des GIMP ImageMap-Editors ermittelt und entsprechend den in den TEI P5 Guidelines (TEI Consortium 2017) definierten Transkriptionskonventionen (""Advanced Uses of surface and zone"") erfasst, um die spatiale Dimension (Kodierung von Information über räumliche Anordnung) ebenfalls zugänglich machen zu können. Die Personen- und Werkreferenzen wurden mit den entsprechenden Normdaten (GND, VIAF, wikidata) verknüpft. Das kartographische Inventar der Karte wurde unter Rückgriff auf CSS innerhalb von @style erfasst. Dies ermöglicht nun, neben den textuellen Informationen zusätzlich die typographische Gestaltung des Textes auszuwerten. Die TEI-Daten werden über ein github-repository bereitgestellt und als Webseite aufbereitet. Das Interface fügt die drei separaten Abschnitte von Flaischlens Karte zusammen. In einer Print- Ausgabe wäre der zusammengefügte Fluss beinahe drei Meter lang und somit nur schwer lesbar. Die Web-Version erlaubt über das Scroll-Interface jedoch einen komfortablen Zugang. Die kodierten Informationen sind über Register erschlossen. Ein Prototyp der Edition ist unter http://litteratur-tafel.weltliteratur.net zugänglich. Durch eine Analyse des kartographischen Inventars lässt sich das Zeichensystem rekonstruieren, das zudem gängigen kartographischen Konventionen folgt. Beeinflusste und beeinflussende Autoren werden durch Unterstreichung unterschieden. Die Farbe der Unterstreichung entspricht der farblichen Kennzeichnung der Zuflüsse und ordnet somit die Autoren einer der beeinflussenden Nationalliteraturen zu. Durch die Verwendung unterschiedlicher Schriftgrößen wird die ""Relevanz"" eines Autors bzw. Textes für die deutsche Literatur zum Ausdruck gebracht. Besonders wichtige deutsche Autoren sind in Konturschrift ausgeführt, vergleichbar der Beschriftung von Städten in Abhängigkeit von ihrer Einwohnerzahl (vgl. Kohlstock 2004: 100). EIne Analyse der Typografie erlaubt es somit, Flaischlens ""Verständnis"" der deutschen Literatur zu rekonstruieren. Um die farblich kodierten Informationen der Karte ebenfalls berücksichtigen zu können, wurde ein Zugang über Verfahren aus dem Bereich computergestützter Bildanalyse gewählt. Mittels der Open Source Computer Vision Library (openCV) wurden die Pixel nach Farbbereichen klassifiziert und quantitativ ausgewertet, um die ""Einflüsse"" zu messen und ihre Veränderungen im Laufe der Zeit visualisieren zu können. Abbildung 1 zeigt jene Pixel, der Kategorie ""rot"" und somit dem französischen Einfluss zugeordnet wurden. Für Abbildung 2 wurde wurden die Pixel nach Jahren gruppiert und als Liniendiagramm visualisiert. Deutlich erkennbar sind Spitzen um 1620, 1665, 1715 und 1800, die sich verschiedenen Epochen der französischen Literaturgeschichte zuordnen lassen: Dem Französischen Klassizismus und der Aufklärung. Der Peak in den 1860er Jahren ist mit dem französischen Naturalismus verbunden. Das Projekt erschließt nicht nur einen inspirierenden Vorläufer gegenwärtiger Versuche von Visualisierung literaturgeschichtlicher Daten mithilfe von ""Graphen, Karten und Stammbäumen‚Äô, sondern erprobt auch auf einer methodologischen Ebene Möglichkeiten und Workflows zur Erschließung und Kodierung älterer graphischer Darstellungen von (Literatur-)geschichte. Darüber hinaus liefert es Impulse für die Arbeit mit der TEI für das Encoding von Bildmaterial, indem die Eignung von primär zur Kodierung von Manuskripten eingesetzten Tags für Grafiken und Diagramme überprüft werden.",de,Cäsar Flaischlen graphisch stellen Versuch dar Entwicklung deutsch Literatur einflüssen Nationalliteratur Graphisch Form Flusses darstellen Gegenstand Vortrag digital Edition Bereitstellung vorwortes Karte entwickelt Workflow Edition graphisch Karteninventar kodieren Mithilfe Computergestützter Bildanalyse Information Visualisierung erfassen quantitativ Analyse zuführen letzter lässen Trend innerhalb Literaturwissenschaft Literaturgeschichtsschreibung ausmachen Fragestellung Grundlage Datenkorpora beantworten charakteristisch Art Literaturwissenschaft Methodenimport Sozialwissenschaft zuletzt darstellungsformen deutlich zeigen zugänge gegenwärtig Beliebtheit erfreuen darstellungsweisen Moretti einflussreich Buch kurv Karte Stammbäume abstrakt Modell Literaturgeschichte moretti keineswegs Phänomen Gegenwart Literaturgeschichtsschreibung bedienen Antike bildmedien anderer rein textuell präsentationsformen Darstellung Autorinn raffaels berühmt Parnassfresko vatikanisch Museum lassen heutig Perspektive Diagramm lesen Information Relevanz Verbindung einzeln Autorinn Bildmedium kodieren hölter hölt Parnassfresko Art kanonbildung Rezeption Zugehörigkeit Autorinnengruppe darstellen lassen häufig gewählt darstellungsformen Lima Fluss Literaturwissenschaft greifen Form Fortschritt Buchproduktion Entwicklung Lithographie Geschichtsschreibung Anwendung finden Rosenberg Grafton produktiv tradiert denkbilder Konzeptualisierung auswirken zeigen graphisch deutsch Autor Cäsar flaischlen Flaischlen großformatig Karte cm visualisiern untertitel einfluss fremd Literatur deutsch Literatur bedienen Denkfigur geschichtlich prozesse Fluss bestehend Summe einflüssen knüpfen Darstellungstradition Weltgeschichte Graphik Strom Zeit österreichisch Historiographen Friedrich Strass maßgeblich prägen Rosenberg Grafton Cäsar Flaischlen nehmen aufwendig gestaltet Sonderstellung ungewöhnlich literaturwissenschaftlich Arbeit erscheinen beinahe zeitgleich Promotion Flaischlen einzig Publikation Literaturgeschichte bleiben flaischlen verlassen akademisch Welt Mitherausgeber Redakteur Kunstzeitschrift tätig Autor hauptsächlich mundartgedicht erzählung Karte deutschsprachig Literatur anfängen Flaischlen Gegenwart darstellen Grafik schlängelnd bäche Kunstpoesie beginnen entwickeln Lauf jahrhunderte breit Strom gesamt zuflüssen hauptsächlich europäisch Nationalliteratur einmünden Legende Karte führen flaischlen folgend einflüsse neu Testament Englisch französisch nordisch orientalisch klassisch Altertum spätlateinisch niederländisch Italienisch Spanisch schwedisch dänisch norwegisch russisch Vertreter positivistisch Denkrichtung unternehmen Cäsar flaischlen Versuch Darstellung fluss exakt Wissenschaft verbinden Tatsache quellen Zusammenstellung Tafel nennen sprechen gängig kanon abbilden Vorwort Tafel spielen flaischlen Zusammenhang quantitativ Befund Visualisierung Herunter bedenken Breite Flusses mathematisch berechnen Platzierung Autor folgen gewiß Prinzip Autor höhepunken schaffens eingezeichnen Informationsgehalt Litteraturtafel Auswahl Platzierung Größe beeinflussend beeinflusst Autor ermöglichen Rekonstruktion flaischlens Datengrundlage lassen rückschluß hierfür verwendet quellen Tafel Name Autor kanonisch text literarisch Gruppierung literarisch Schule enthalten ferner nutzen Cäsar flaischlen Typographisch Gestaltungsmöglichkeit schriftart Schriftgröße Farbwahl Unterstreichung Symbol Kreis verschieden Größe römisch lateinisch ziffern farblich Schraffur unterer Rand Karte Legende anbringen weisen lediglich Bedeutung Schraffur blau einflüsse englisch Literatur rot einflüsse französisch Literatur Angabe insbesondere erläuterung verwendet Schriftart fehlen vorgestellt Projekt Cäsar flaischlens graphisch Digital unternehmen Versuch reverse engineering erschließen Dokument Visualisierung literaturgeschichtlich daten Methode Digital Humanitie Koordinate angeführt Person text literarisch Strömung Schule Verwendung Gimp ermitteln entsprechend tei guidelines tei Consortium definiert Transkriptionskonvention advanced uses of surface and zone erfasst spatial Dimension Kodierung Information räumlich Anordnung ebenfalls zugänglich Werkreferenz entsprechend Normdate gnd viaf Wikidata verknüpfen kartographisch Inventar Karte Rückgriff Css innerhalb erfassen ermöglichen textuell Information zusätzlich typographisch Gestaltung Text auswerten bereitstellen webseit aufbereiten Interface fügen Separat Abschnitt Flaischlen Karte Ausgabe zusammengefügter fluss beinahe Meter somit schwer lesbar erlauben Komfortablen Zugang Kodiert Information Register erschließen Prototyp Edition zugänglich Analyse kartographisch inventars lässen zeichensyst rekonstruieren zudem gängig kartographisch Konvention folgen beeinflus beeinflussend Autor Unterstreichung unterscheiden Farbe Unterstreichung entsprechen farblich Kennzeichnung Zuflüsse ordnen somit Autor beeinflussend nationalliteraturen Verwendung unterschiedlich Schriftgröße Relevanz Autor Text deutsch Literatur Ausdruck bringen wichtig deutsch Autor Konturschrift ausführen vergleichbar Beschriftung städten Abhängigkeit Einwohnerzahl Kohlstock Analyse Typografie erlauben somit flaischlen Verständnis deutsch Literatur rekonstruieren farblich kodiert Information Karte ebenfalls berücksichtigen Zugang Verfahren Bereich Computergestützter Bildanalyse wählen mittels op source Computer vision Library opencv Pixel farbbereich klassifizieren quantitativ auswerten einflüsse messen Veränderung Lauf visualisieren Abbildung zeigen Pixel Kategorie rot somit französisch einfluss zuordnen Abbildung Pixel gruppieren liniendiagramm visualisieren deutlich erkennbar Spitze verschieden Epoche französisch Literaturgeschichte zuordnen lassen französisch Klassizismus Aufklärung Peak französisch Naturalismus verbinden Projekt erschließen inspirierend Vorläufer gegenwärtig Versuch Visualisierung literaturgeschichtlich daten Mithilfe graph Kart stammbäumen äô erproben methodologisch Ebene Möglichkeit workflow Erschließung Kodierung alt graphisch Darstellung hinaus liefern Impuls Arbeit tei Encoding bildmaterial Eignung primär Kodierung manuskripter eingesetzt tags grafik diagramm überprüfen,"[('flaischlen', 0.47640532120424606), ('karte', 0.2459187875891846), ('cäsar', 0.2198793790173443), ('fluss', 0.14658625267822956), ('autor', 0.14522642870576383), ('graphisch', 0.13615850366934684), ('einflüsse', 0.12940178145230344), ('französisch', 0.12747085160784416), ('literatur', 0.11088717004511867), ('unterstreichung', 0.10993968950867215)]"
2018,DHd2018,JACKE_Janina_Digital_vs__Humanities__Didaktische_Aufbereitun.xml,Digital vs. Humanities. Didaktische Aufbereitung digitaler Methoden für die klassischen Geisteswissenschaften im Projekt forTEXT,"Janina Jacke (Universität Hamburg, Deutschland); Jan Horstmann (Universität Hamburg, Deutschland); Jan Christoph Meister (Universität Hamburg, Deutschland)","Literatur, Interpretation, Forschungsumgebung, Annotation, Didaktik","Sammlung, Modellierung, Annotieren, Kollaboration, Lehre, Literatur","Computergestütztes Arbeiten kann geisteswissenschaftliches Forschen auf unterschiedlichste Weise befördern und bereichern. Dennoch müssen wir in unserem Arbeitsalltag und in Gesprächen mit Kolleginnen und Kollegen Das lässt sich gut am Beispiel der Literaturwissenschaft illustrieren: Digitale Methoden werden bisher vornehmlich von Literaturwissenschaftlern genutzt, die an strukturellen oder anderen formalen Aspekten literarischer Texte interessiert sind (beispielsweise an narrativen Strukturen, Figurennetzwerken etc.). Ihrem traditionellen Selbstverständnis nach ist die Literaturwissenschaft allerdings zentral an komplexen und innovativen Damit digitale Methoden eine breitere Akzeptanz finden, ist es deswegen notwendig, den Nutzen dieser Methoden auch für stärker hermeneutisch ausgerichtete geisteswissenschaftliche Forschungsfragen zu reflektieren. Unserem (weiten) Verständnis von ""Hermeneutik"" entsprechend handelt es sich bei hermeneutischen Forschungsfragen um Fragen, die auf die (holistische) Auslegung bzw. Deutung von Texten gerichtet sind (vgl. bspw. Spörl 2004: 128). In literaturwissenschaftlichen Zusammenhängen spielen dabei insbesondere Fragen nach Funktion bzw. Wirkung bestimmter Textelemente oder des Gesamttextes eine Rolle, ebenso wie die In-Beziehung-Setzung des Textes mit bestimmten Kontexten. Diese Forschungsfragen sollten exponiert im Zusammenhang mit der Entwicklung von Tools, digitaler Forschungsumgebungen und vor allem didaktischer Konzepte zur Vermittlung von DH-Methoden berücksichtigt werden. Diese Forderungen werden bisher jedoch nicht in zureichendem Maße erfüllt. Wir möchten in diesem Beitrag das aktuelle Projekt forTEXT (2017–2020) vorstellen, das der Vermittlung, Aufbereitung und Bereitstellung von Mitteln zur computergestützten Textanalyse insbesondere für hermeneutisch arbeitende Geisteswissenschaftler gewidmet ist. Im Folgenden sollen in diesem Zusammenhang zunächst die unterschiedlichen konzeptionellen Dimensionen (Abschnitt 2) sowie anschließend erste inhaltliche Ergebnisse des Projekts präsentiert werden (Abschnitt 3). Das Anfang 2017 gestartete DFG-Projekt (a) Orientierung an genuin geisteswissenschaftlichen Arbeitsweisen: Es geht, ganz im Sinne des geisteswissenschaftlichen Selbstverständnisses, um die Unterstützung der genuin (b) Niedrigschwelliger Zugang: Geisteswissenschaftler sollen die digitale Forschungsumgebung In den folgenden Unterabschnitten sollen sowohl forTEXTs Empfehlungssystem als auch die drei Komponenten des Informationsrepositoriums ( Für Geisteswissenschaftler, die noch nicht wissen, auf welche Weise digitale Methoden der Textanalyse und -interpretation ihre eigene Forschung unterstützen können, bietet forTEXT ein individualisiertes Empfehlungssystem in Form eines digitalen Fragebogens an (siehe Abb. 1). Hier können die Nutzer beispielsweise angeben, ob sie schon Vorerfahrungen mit digitalen Methoden der Textanalyse gemacht haben, in welchem Zustand sich ihr Textkorpus befindet, unter welcher Fragestellung sie ihre Texte untersuchen wollen und welcher literaturtheoretischen Schule sie sich zuordnen. forTEXTs digitale Forschungsumgebung ist in drei Bereiche gegliedert. (a) Routinen: Im Teilbereich Zum anderen werden unter (b) Ressourcen: Unter (c) Tools: Im Bereich Alle Verzeichnisse und Einträge aus den drei forTEXT-Bereichen Routinen, Ressourcen und Tools können von Nutzern eigenständig durchsucht und aufgerufen werden 'oder es erfolgt ein angeleiteter Zugriff durch die Nutzung des Empfehlungssystems. Im verbleibenden Teil dieses Beitrags möchten wir etwas genauer auf einen Lehrmodulentwurf eingehen, das dem forTEXT-Bereich Zwei von forTEXTs 90-minütigen Lehrmodulen sind der Vermittlung der digitalen Methode des Um diese Anforderungen umzusetzen, sieht forTEXT zwei Lehreinheiten zum manuellen Annotieren vor, von denen wir die erste im Folgenden kurz vorstellen möchten. In der Einheit Das verwendete Programm CATMA ist hierbei in zweifacher Hinsicht auf die forTEXT-Paradigmen abgestimmt: Es bietet eine intuitiv bedienbare Benutzeroberfläche und unterstützt den freien, undogmatischen und genuin interpretativen Zugang zu Texten, während es zugleich Optionen stärkerer Formalisierung bereithält (vgl. Abb. 2). Die im Rahmen des Lehrmoduls verfolgte didaktische Strategie hat mehrere Vorteile: Der erste Schritt, d.h. das digitale Anbringen freier Kommentare, stellt einen vollkommen explorativen und potenziell unstrukturierten Zugang zu literarischen Texten dar. Er erzwingt also kein formalistisches Umdenken und bildet die traditionellere geisteswissenschaftlich-hermeneutische Arbeitsweise gut ab. Im Vergleich zum analogen Arbeiten birgt er aber dennoch den Vorteil, dass die freien Kommentare durch digitale Unterstützung effektiver Als zusätzliches Angebot an Literaturwissenschaftler, die für eine etwas stärkere Formalisierung ihres Zugangs offen sind, zeigt das Lehrmodul, welche weiteren Vorteile und Optionen Im Lehrmodul zum manuellen digitalen Annotieren sind also die Paradigmen umgesetzt, die auch die weitere Arbeit am forTEXT-Projekt bestimmen sollen: Durch stärkere Orientierung an der traditionell-geisteswissenschaftlichen Arbeitsweise und erleichterten Zugang können digitale Methoden einer breiteren Nutzergemeinschaft nähergebracht werden.",de,computergestütztes arbeiten geisteswissenschaftlich Forschen unterschiedlich Weise befördern bereichern dennoch unser Arbeitsalltag Gespräch kolleginn Kollege Lässt Literaturwissenschaft illustrieren digital Methode vornehmlich Literaturwissenschaftler nutzen strukturell formal Aspekt literarisch Text interessiert beispielsweise narrativ Struktur figurennetzwerken traditionell Selbstverständnis Literaturwissenschaft zentral komplex innovativen digital Methode breit Akzeptanz finden notwendig nutzen Methode stark hermeneutisch ausgerichtet geisteswissenschaftlich forschungsfragen reflektieren unser weit Verständnis Hermeneutik entsprechend handeln hermeneutisch Forschungsfrag Frage holistisch Auslegung Deutung Text richten Spörl literaturwissenschaftlich Zusammenhäng spielen insbesondere Frage Funktion Wirkung bestimmt Textelemente Gesamttext Rolle Text bestimmt Kontext forschungsfragen exponiert Zusammenhang Entwicklung Tools digitaler forschungsumgebungen didaktischer Konzept Vermittlung berücksichtigen Forderung zureichend Maß erfüllen möchten Beitrag aktuell Projekt fortext vorstellen Vermittlung Aufbereitung Bereitstellung Mittel computergestützt Textanalyse insbesondere hermeneutisch arbeitend geisteswissenschaftl widmen folgend Zusammenhang unterschiedlich Konzeptionelle dimension Abschnitt anschließend inhaltlich Ergebnis Projekt präsentieren Abschnitt Anfang gestartet Orientierung genuin geisteswissenschaftlich Arbeitsweise Sinn geisteswissenschaftlich Selbstverständnisse Unterstützung genuin b niedrigschwellig Zugang geisteswissenschaftler digital Forschungsumgebung folgend Unterabschnitt sowohl fortexts empfehlungssyst Komponente informationsrepositoriums Geisteswissenschaftler wissen Weise digital Methode Textanalyse Forschung unterstützen bieten fortext individualisiert empfehlungssystem Form digital Fragebogen sehen abb Nutzer beispielsweise angeben Vorerfahrung digital Methode Textanalyse Zustand Textkorpus befinden Fragestellung Text untersuchen literaturtheoretisch Schule zuordnen Fortexts digital Forschungsumgebung Bereich gliedern routinen Teilbereich b Ressource c Tools Bereich Verzeichnisse einträge Routine Ressource Tools Nutzer eigenständig durchsuchen aufrufen erfolgen angeleitet Zugriff Nutzung Empfehlungssystem verbleibend Beitrag möchten genau Lehrmodulentwurf eingehen fortexts Lehrmodule Vermittlung digital Methode Anforderung umsetzen sehen fortext lehreinheiten Manuelle Annotiere folgend vorstellen möchten Einheit verwendet Programm Catma hierbei zweifach Hinsicht abstimmen bieten intuitiv bedienbar benutzeroberfläche unterstützen frei undogmatisch genuin interpretativ Zugang texten option stark Formalisierung bereithalten abb Rahmen lehrmoduls verfolgt didaktisch Strategie mehrere Vorteil Schritt digital anbringen frei Kommentar stellen vollkommen explorativ potenziell unstrukturiert Zugang literarisch Text dar erzwingen formalistisch umdenken bilden traditioneller arbeitsweise Vergleich Analog arbeiten bergen dennoch Vorteil frei Kommentar digital Unterstützung effektiv zusätzlich Angebot Literaturwissenschaftler stark Formalisierung Zugang zeigen Lehrmodul Vorteil Option Lehrmodul manuell digital Annotier Paradigm umsetzen Arbeit bestimmen stark Orientierung Arbeitsweise erleichtert Zugang digital Methode breit nutzergemeinschaft nähergebrachen,"[('digital', 0.20639573162684208), ('fortexts', 0.2050577968996743), ('zugang', 0.1763221964463761), ('vermittlung', 0.17327976107164303), ('arbeitsweise', 0.16695663592972412), ('genuin', 0.15697948469656012), ('fortext', 0.15697948469656012), ('methode', 0.1444891987815942), ('geisteswissenschaftlich', 0.14105775715710087), ('lehrmodul', 0.1367051979331162)]"
2018,DHd2018,BRUNNER_Annelen_Projektvorstellung___Redewiedergabe__Eine_li.xml,Projektvorstellung 'Redewiedergabe. Eine literatur- und sprachwissenschaftliche Korpusanalyse,"Annelen Brunner (Institut für Deutsche Sprache, Deutschland); Stefan Engelberg (Institut für Deutsche Sprache, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland); Ngoc Duyen Tanja Tu (Institut für Deutsche Sprache, Deutschland); Lukas Weimer (Universität Würzburg, Deutschland)","Redewiedergabe, Annotation, Maschinelles Lernen, Linguistik, Narratologie","Programmierung, Strukturanalyse, Annotieren, Veröffentlichung, Stilistische Analyse","Das laufende DFG-Projekt ""Redewiedergabe"" stellt einen Anwendungsfall quantitativer Sprach- und Literaturwissenschaft dar und beschäftigt sich mit dem Phänomen ""Redewiedergabe"" auf der Grundlage großer Datenmengen. Zu diesem Zweck wird zum einen ein Korpus manuell mit Redewiedergabeformen annotiert, zum anderen werden Verfahren zur automatischen Erkennung des Phänomens entwickelt. Ziel ist es, Forschungsfragen nach der Entwicklung von Redewiedergabe vor allem im 19. Jahrhundert zu beantworten. Das Poster präsentiert einen Überblick über das Gesamtprojekt sowie erste Projektergebnisse. Sowohl aus linguistischer als auch aus literaturwissenschaftlicher Perspektive ist Redewiedergabe ein interessantes Phänomen. Die Art und Weise, wie die Figurenstimme in die Erzählung eingebunden ist, steht in engem Zusammenhang mit Erzählweise und -haltung, sowie der Konstruktion der erzählten Welt. Folglich wird dem Phänomen in der Erzählforschung viel Aufmerk¬≠samkeit geschenkt und es liegen zahlreiche systematische Analysen vor (vgl. z.B. Genette 1998; Mart√≠nez / Scheffel 2007). Zu Phänomenen wie der erlebten Rede, dem Bewusstseinsstrom usw. gibt es eine umfangreiche Spezialforschung (Überblick bei McHale 2014). Aus linguistischer Perspektive ist Redewiedergabe vor allem in Bezug auf den Funktionswandel des Konjunktivs im Zusammenhang mit seinem Auftreten in indirekter Rede untersucht worden (vgl. z.B. Übersicht in √Ågel 2000). In geringem Umfang sind auch Redewiedergabeverben und ihr Verhältnis zur wiedergegebenen Rede in das Blickfeld der Forschung gerückt (eine kurze Synopse bei Fritz 2005). Ein Vorbild für die ausführliche, manuelle Annotation von Redewiedergabe ist v.a. Semino / Short 2004. Implementierungen der automatischen Erkennung stammen vor allem aus dem Bereich der Computerlinguistik und werden oft als Vorverarbeitungsschritt für andere Anwendungen durchgeführt (z.B. Wissensextraktion, Sprechererkennung oder dem Aufbau von sozialen Netzwerken literarischer Figuren, vgl. z.B. Krestel / Bergler / Witte 2008; Elson / Dames / McKeown 2010; Iosif / Mishra 2014). Eine literaturwissenschaftlich motivierte Anwendung ist die Untersuchung von Schöch et al. 2016 zur Erkennung von direkter Wiedergabe in französischen Romantexten. Die wichtigste Vorarbeit für das vorgestellte Projekt ist die Studie Brunner 2015, auf deren Ergebnissen es aufbaut. In dieser Studie wurde ein Korpus von 13 Erzähltexten manuell annotiert und Prototypen für die automatische Erkennung (sowohl regelbasiert als auch mit Hilfe von maschinellem Lernen) wurden entwickelt und ausgewertet. Das Untersuchungskorpus umfasst die Jahre 1840-1920 und enthält sowohl fiktionale als auch nicht-fiktionale Texte. Der nicht-fiktionale Teil setzt sich zusammen aus Texten des ""Mannheimer Korpus Historischer Zeitungen und Zeitschriften"" und der Zeitschrift ""Die Grenzboten"" (digitalisiert durch die Staats- und Universitätsbibliothek Bremen), der fiktionale Teil aus Erzählungen der Sammlung der Digitalen Bibliothek (textgrid). So sind sowohl Beobachtungen von Entwicklungen über die Zeit hinweg als auch Vergleiche zwischen Textsorten möglich. Auszüge aus den Texten werden manuell annotiert. Das in Brunner 2015 vorgestellte und an Kategoriensystemen der Literaturwissenschaft orientierte Annotationssystem wurde für das Projekt erweitert und präzisiert. Es unterscheidet zwischen Wiedergabe von gesprochener Sprache, von Schrift und von Gedanken sowie den Typen direkte Wiedergabe ( Die Annotatoren arbeiten mit dem im Projekt Kallimachos ( Die zweite Projektphase, welche zum Einreichungszeitpunkt dieses Posters gerade beginnt, umfasst die Entwicklung eines automatischen Erkenners für Redewiedergabeformen. Hierbei dient das manuell annotierte Material als Test- und Trainingsmaterial. Die in Brunner 2015 implementierten Prototypen dienen als Ausgangspunkt, die Implementierung erfolgt unter Nutzung des UIMA-Frameworks sowie in Python. Geplant ist eine Verbesserung des maschinellen Lernens durch Optimierung der Attributauswahl sowie Tests mit verschiedenen Lernalgorithmen (RandomForest, SVM, eventuell Conditional Random Fields und Deep Learning) und verschiedenen Parametereinstellungen. Auch regelbasierte Ansätze sollen weiter verfolgt werden, eventuell auf Grundlage einer aufwendigeren Vorverarbeitung (z.B. Parsing). Zudem ist eine Ergänzung und Verfeinerung einer Liste von Wörtern geplant, die auf Redewiedergabe hinweisen, welche sich bereits in Brunner 2015 als wertvolles Werkzeug bei der automatischen Erkennung erwiesen hat. Der Redewiedergabe-Erkenner wird dann auf weitere Texte in unserem Untersuchungszeitraum angewendet, um größere Entwicklungslinien beobachten zu können und verschiedene offene narratologische und linguistische Forschungsfragen auf quantitativer Basis zu untersuchen, z.B.: Welche Entwicklungen in der Verwendung und Form von Redewiedergabe lassen sich im Untersuchungszeitraum beobachten? Welche Rolle spielen Textsortenunterschiede bei der Entwicklung von Redewiedergabeformen? Wie kommt die Dynamik im Bestand an Verben zustande, die als Redeeinleiter gebraucht werden? Sowohl das manuell annotierte Korpus als auch der automatische Erkenner werden am Ende des Projekts der Forschungsgemeinschaft zur Verfügung gestellt. Es werden dafür sowohl das CLARIN-D-Forschungsdatenrepositorium des Instituts für Deutsche Sprache als auch das DARIAH-DE-Repository genutzt.",de,laufend Redewiedergabe stellen Anwendungsfall quantitativ Literaturwissenschaft dar beschäftigen phänom Redewiedergabe Grundlage datenmengen zweck Korpus manuell Redewiedergabeformen annotiert Verfahren automatisch Erkennung Phänomen entwickeln Ziel forschungsfragen Entwicklung Redewiedergabe Jahrhundert beantworten Poster präsentieren Überblick gesamtprojekt Projektergebnisse sowohl linguistisch literaturwissenschaftlich Perspektive Redewiedergabe interessant Phänomen Art Weise Figurenstimme Erzählung einbinden stehen eng Zusammenhang Erzählweise Konstruktion erzählt Welt folglich Phänomen Erzählforschung schenken liegen zahlreich systematisch Analyse Genette Scheffel phänomenen erlebt Rede Bewusstseinsstrom umfangreich Spezialforschung Überblick mchal linguistisch Perspektive Redewiedergabe Bezug Funktionswandel Konjunktiv Zusammenhang auftret indirekt Rede untersuchen übersichen gering Umfang redewiedergabeverben Verhältnis wiedergegeben Rede Blickfeld Forschung rücken kurz Synopse Fritz Vorbild ausführlich manuell Annotation Redewiedergabe Semino Short implementierungen automatisch Erkennung stammen Bereich Computerlinguistik Vorverarbeitungsschritt Anwendung durchgeführt Wissensextraktion Sprechererkennung Aufbau sozial netzwerken literarisch Figur Krestel Bergler Witte elson dames Mckeown iosif Mishra literaturwissenschaftlich motiviert Anwendung Untersuchung Schöch et Erkennung direkt Wiedergabe französisch Romantext wichtig vorarbeien vorgestellt Projekt Studie Brunner Ergebnis aufbauen Studie Korpus Erzähltext Manuell annotiert Prototyp automatisch Erkennung sowohl regelbasieren Hilfe Maschinellem lernen entwickeln auswerten Untersuchungskorpus umfassen enthalten sowohl Fiktional Text setzen Text mannheimer Korpus historisch Zeitung Zeitschrift Zeitschrift grenzbot digitalisieren Universitätsbibliothek Bremen fiktional Erzählung Sammlung digital Bibliothek Textgrid sowohl Beobachtung Entwicklung hinweg Vergleich textsorten auszug Text manuell annotiert Brunner vorgestellt Kategoriensystemen Literaturwissenschaft orientiert Annotationssystem Projekt erweitern präzisieren unterscheiden Wiedergabe gesprochen Sprache Schrift Gedanke Typ direkt Wiedergabe Annotator arbeiten Projekt kallimachos Projektphase einreichungszeitpunkt Poster beginnen umfassen Entwicklung automatisch Erkenner Redewiedergabeformen hierbei dienen manuell annotiert Material Trainingsmaterial Brunner implementiert Prototyp dienen Ausgangspunkt Implementierung erfolgen Nutzung Python planen Verbesserung maschinell Lernen Optimierung Attributauswahl Test verschieden Lernalgorithme Randomforest svm eventuell conditional Random Field Deep Learning verschieden Parametereinstellung regelbasiert Ansatz verfolgen eventuell Grundlage aufwendiger Vorverarbeitung Parsing zudem Ergänzung Verfeinerung Liste wörtern planen Redewiedergabe hinweisen Brunner wertvoll Werkzeug automatisch Erkennung erweisen Text unser Untersuchungszeitraum anwenden groß Entwicklungslinie beobachten verschieden offen narratologisch linguistisch Forschungsfrag quantitativ Basis untersuchen Entwicklung Verwendung Form Redewiedergabe lassen Untersuchungszeitraum beobachten Rolle spielen Textsortenunterschiede Entwicklung Redewiedergabeformen Dynamik Bestand verben zustande Redeeinleiter brauchen sowohl manuell annotiert Korpus automatisch Erkenner Projekt Forschungsgemeinschaft Verfügung stellen sowohl Institut deutsch Sprache nutzen,"[('redewiedergabe', 0.40584055820870557), ('redewiedergabeformen', 0.1988017038970319), ('brunner', 0.1886437495064645), ('manuell', 0.15756599790274617), ('wiedergabe', 0.15667994683037043), ('erkennung', 0.1509552751885356), ('sowohl', 0.1386957197023561), ('annotiert', 0.1353972155807212), ('automatisch', 0.13320318477502135), ('erkenner', 0.12344581045625569)]"
2018,DHd2018,REITER_Nils_Maschinelles_Lernen_lernen__Ein_CRETA_Hackatoria.xml,Maschinelles Lernen lernen: Ein CRETA-Hackatorial zur reflektierten automatischen Textanalyse,"Nils Reiter (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Nora Ketschik (Institut für Literaturwissenschaft, Universtität Stuttgart, Deutschland); Gerhard Kremer (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Sarah Schulz (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland)","Shared task, training, maschinelles Lernverfahren","Programmierung, Modellierung, Annotieren, Methoden, Text","Das Ziel dieses Tutorials ist es, den Teilnehmerinnen und Teilnehmern konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten. Es werden dabei keinerlei Vorkenntnisse zu maschinellem Lernen oder Programmierkenntnisse vorausgesetzt. Es gibt keinen Grund, den Ergebnissen von maschinellen Lernverfahren im Allgemeinen und NLP-Tools im Besonderen blind zu vertrauen. Durch die konkreten Einblicke in den ""Maschinenraum"" von maschinellen Lernverfahren wird den Teilnehmenden ermöglicht, das Potenzial und die Grenzen statistischer Textanalysewerkzeuge realistischer einzuschätzen. Mittelfristig hoffen wir dadurch, den immer wieder auftretenden Frustrationen beim Einsatz automatischer Verfahren für die Textanalyse und deren teilweise wenig zufriedenstellender Ergebnis-Daten zu begegnen, aber auch die Nutzung und Interpretation der Ergebnisse von maschinellen Lernverfahren (d.h. in erster Linie von automatisch erzeugten Annotationen) zu fördern. Zu deren adäquater Nutzung, etwa in hermeneutischen Interpretationsschritten, ist der Einblick in die Funktionsweise der maschinellen Methoden unerlässlich. Insbesondere ist die Art und Herkunft der Trainingsdaten für die Qualität der maschinell produzierten Daten von Bedeutung, wie wir im Tutorial deutlich machen werden. Neben einem Python-Programm für die automatische Annotierung von Entitätenreferenzen, mit und an dem während des Tutorials gearbeitet werden wird, stellen wir ein heterogenes, manuell annotiertes Korpus sowie die Routinen zur Evaluation und zum Vergleich von Annotationen zu Verfügung. Das Korpus enthält Entitätenreferenzen, die im ""Center for Reflected Text Analytics"" (CRETA) Als empirisches Phänomen befassen wir uns mit dem Konzept der Entität und ihrer Referenz. Das Konzept steht für verschiedene linguistische und semantische Kategorien, die im Rahmen der Digital Humanities von Interesse sind. Es ist bewusst weit gefasst und damit anschlussfähig für verschiedene Forschungsfragen aus den geistes- und sozialwissenschaftlichen Disziplinen. Auf diese Weise können unterschiedliche Perspektiven auf Entitäten berücksichtigt werden. Insgesamt werden in den ausgewählten Texten fünf verschiedene Entitätenklassen betrachtet: PER (Personen/Figuren), LOC (Orte), ORG (Organisationen), EVT (Ereignisse) und WRK (Werke). Unter Entitätenreferenzen verstehen wir Ausdrücke, die auf eine Entität in der realen oder fiktiven Welt referieren. Das sind zum einen Eigennamen (Named Entities, z.B. ""Peter""), zum anderen Gattungsnamen (z.B. ""der Bauer""), sofern diese sich auf eine konkrete Instanz der Gattung beziehen. Dabei wird als Referenzausdruck immer die maximale Nominalphrase (inkl. Artikel, Attribut) annotiert. Pronominale Entitätenreferenzen werden hingegen nicht annotiert. In In den Die Grundlage für (überwachte) maschinelle Lernverfahren bilden Annotationen. Um die Annotierung von Entitätenreferenzen automatisieren zu können, bedarf es Textdaten, die die Vielfalt des Entitätenkonzepts abdecken. Bei diesem Tutorial werden wir auf Annotationen zurückgreifen, die im Rahmen von CRETA an der Universität Stuttgart entstanden sind (cf. Blessing et al., 2017; Reiter et al., 2017a). Das Korpus enthält literarische Texte aus zwei Sprachstufen des Deutschen (Neuhochdeutsch und Mittelhochdeutsch) sowie ein sozialwissenschaftliches Teilkorpus. Der  Das Der Ablauf des Tutorials orientiert sich an sog. Zunächst diskutieren wir kurz die zugrundeliegenden Texte und deren Annotierung. Annotationsrichtlinien werden den Teilnehmerinnen und Teilnehmern im Vorfeld zur Verfügung gestellt. Im Rahmen der Einführung wird auch auf die konkrete Organisation der Annotationsarbeit eingegangen, so dass das Tutorial als Blaupause für zukünftige Tätigkeiten der Teilnehmenden in diesem und ähnlichen Arbeitsfeldern dienen kann. Die Teilnehmerinnen und Teilnehmer versuchen selbständig und unabhängig voneinander, eine Kombination aus maschinellen Lernverfahren, Merkmalsmenge und Parametersetzungen zu finden, die auf einem neuen, vom automatischen Lernverfahren ungesehenen Datensatz zu den Ergebnissen führt, die dem Goldstandard der manuellen Annotation am Öhnlichsten sind. Das bedeutet konkret, dass der Einfluss von berücksichtigten Features (z.B. Groß- und Kleinschreibung oder Wortlänge) auf die Erkennung von Entitätenreferenzen empirisch getestet werden kann. Dabei sind Intuitionen über die Daten und das annotierte Phänomen hilfreich, da simplem Durchprobieren aller möglichen Kombinationen (``brute force'') zeitlich Grenzen gesetzt sind. Wir verzichten bewusst auf eine graphische Benutzerschnittstelle (cf. Reiter et al., 2017b) -- stattdessen editieren die Teilnehmerinnen und Teilnehmer das (Python)-Programm direkt, nach einer Einführung und unter Anleitung. Vorkenntnisse in Python sind dabei nicht nötig: Das von uns zur Verfügung gestellte Programm ist so aufgebaut, dass auch Python-Neulinge relativ schnell die zu bearbeitenden Teile davon verstehen und damit experimentieren können. Wer bereits Erfahrung im Python-Programmieren hat, kann fortgeschrittene Funktionalitäten des Programms verwenden. Wie am Ende jedes maschinellen Lernprozesses wird auch bei uns abschließend eine Evaluation der automatisch generierten Annotationen durchgeführt. Hierfür werden den Teilnehmerinnen und Teilnehmern nach Ablauf einer begrenzten Zeit des Experimentierens und Testens (etwa 60 Minuten) die finalen, vorher unbekannten Testdaten zur Verfügung gestellt. Auf diese Daten werden die erstellten Modelle angewendet, um automatisch Annotationen zu erzeugen. Diese wiederum werden dann mit dem Goldstandard verglichen, wobei die verschiedenen Entitätenklassen sowie Teilkorpora getrennt evaluiert werden. Auch das Programm zur Evaluation stellen wir bereit. Am hier verwendeten Beispiel der automatischen Annotation von Entitätenreferenzen demonstrieren wir, welche Schritte für die Automatisierung einer Textanalyseaufgabe mittels maschinellen Lernverfahren nötig sind und wie diese konkret implementiert werden können. Die Teilnehmerinnen und Teilnehmer bekommen einen zusammenhängenden Überblick von der manuellen Annotation ausgewählter Texte über die Feinjustierung der Lernverfahren bis zur Evaluation der Ergebnisse. Die vorgestellte Vorgehensweise für den gesamten Ablauf ist grundsätzlich auf ähnliche Projekte übertragbar. Das Tutorial schärft dabei das Verständnis für den Zusammenhang zwischen untersuchtem Konzept und den dafür relevanten Features, die in ein statistisches Lernverfahren einfließen. Durch Einblick in die technische Umsetzung bekommen die Teilnehmerinnen und Teilnehmer ein Verständnis für die Grenzen und Möglichkeiten der Automatisierung, das sie dazu befähigt, zum einen das Potenzial solcher Verfahren für eigene Vorhaben realistisch(er) einschätzen zu können, zum anderen aber auch Ergebnisse, die auf Basis solcher Verfahren erzielt wurden, angemessen hinterfragen und deuten zu können. Der Workshop wird ausgerichtet von Mitarbeiterinnen und Mitarbeitern des ""Center for Reflected Text Analytics"" (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine ""black box"" sein, sondern auch für Nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird.  Die Forschungsinteressen von Nils Reiter liegen generell in der Anwendung computerlinguistischer Methoden auf Fragen aus den Geistes- und Sozialwissenschaften. Insbesondere die Operationalisierung literarischer Forschungsfragen und die adäquate Interpretation von Ergebnissen ist dabei ein Schwerpunkt, neben der regelgeleiteten Annotation und damit zusammenhängenden Fragen.  Nora Ketschik ist Promotionsstudentin in der Abteilung für Germanistische Mediävistik. Im Rahmen des CRETA-Projekts nimmt sie Analysen narratologischer Kategorien (u.a. Figur, Raum) an ausgewählten mittelhochdeutschen Romanen vor und setzt sich dabei mit der Verwendung computergestützter Methoden für literaturwissenschaftliche Analysezwecke auseinander.  Der Interessenschwerpunkt Gerhard Kremers ist der reflektierte Einsatz von Werkzeugen der Computerlinguistik für geistes- und sozialwissenschaftliche Fragestellungen. Damit zusammenhängend gehören die Entwicklung übertragbarer Arbeitsmethoden und die angepasste, nutzerfreundliche Bedienbarkeit automatischer linguistischer Analysetools zu seinen Forschungsthemen.  Sarah Schulz beschäftigt sich überwiegend mit der automatischen Verarbeitung von Texten, die syntaktischen oder lexikalischen Eigenschaften aufweisen und damit vom Zwischen 15 und 25. Es wird außer einem Beamer keine besondere technische Ausstattung benötigt. Es sollte sich um einen Raum handeln, in dem es möglich ist, den Teilnehmenden über die Schulter zu blicken und durch die Reihen zu gehen.",de,Ziel Tutorial teilnehmerinn Teilnehmer konkret praktisch einblicken Standardfall automatisch Textanalyse geben automatisch Erkennung Entitätenreferenze allgemein annehmen verfahrensweisen methodisch Standard maschinell lernverfahren Teilnehmerinn Teilnehmer bearbeiter lauffähig Programmiercode entscheidungsraum Verfahren ausleuchten austesten keinerlei Vorkenntnisse Maschinellem lernen Programmierkenntnisse voraussetzen Grund Ergebnis maschinell lernverfahren besonderer Blind vertrauen konkret einblick maschinenraum maschinell lernverfahren Teilnehmend ermöglichen Potenzial Grenze statistisch Textanalysewerkzeug Realistischer einschätzen mittelfristig hoffen auftretende Frustration Einsatz automatisch Verfahren Textanalyse teilweise zufriedenstellend begegnen Nutzung Interpretation Ergebnis Maschinelle lernverfahren Linie automatisch erzeugt Annotation fördern adäquat Nutzung hermeneutisch interpretationsschritten Einblick Funktionsweise maschinell Methode Unerlässlich insbesondere Art Herkunft Trainingsdat Qualität maschinell produziert daten Bedeutung Tutorial deutlich automatisch Annotierung entitätenreferenzen tutorials arbeiten stellen heterogen manuell annotiert korpus Routin Evaluation Vergleich Annotation Verfügung korpus enthalten entitätenreferenzen center for reflected Text analytics creta empirisch Phänomen befassen Konzept Entität Referenz Konzept stehen verschieden linguistisch semantisch Kategorie Rahmen Digital Humanitie Interesse bewusst fassen anschlussfähig verschieden Forschungsfrag sozialwissenschaftlich Disziplin Weise unterschiedlich Perspektive entitäten berücksichtigen insgesamt ausgewählt Text verschieden Entitätenklasse betrachten per Person Figur loc ort org organisation Evt Ereignis wrk werk Entitätenreferenze verstehen ausdrücken Entität real fiktiv Welt referieren Eigennam named entities Peter Gattungsnamen Bauer sofern konkret Instanz Gattung beziehen Referenzausdruck maximal Nominalphrase Artikel Attribut annotiert pronominale entitätenreferenzen hingegen annotiert Grundlage überwacht maschinell lernverfahren bilden Annotation Annotierung Entitätenreferenze automatisieren bedürfen textdan Vielfalt Entitätenkonzept abdecken Tutorial annotation zurückgreifen Rahmen Creta Universität Stuttgart entstehen cf blessing et Reiter et korpus enthalten literarisch Text Sprachstuf deutsch neuhochdeutsch mittelhochdeutsch sozialwissenschaftlich Teilkorpus Ablauf tutorials orientieren diskutieren zugrundeliegend Text Annotierung Annotationsrichtlini Teilnehmerinn Teilnehmer Vorfeld Verfügung stellen Rahmen Einführung konkret Organisation Annotationsarbeit eingehen Tutorial Blaupause zukünftig Tätigkeit Teilnehmende ähnlich Arbeitsfelder dienen Teilnehmerinn Teilnehmer versuchen selbständig unabhängig voneinander Kombination Maschinellen lernverfahren merkmalsmenge Parametersetzunge finden automatisch lernverfahren ungesehen Datensatz ergebnissen führen Goldstandard manuell Annotation öhnlichsten bedeuten konkret einfluss berücksichtigt Features Kleinschreibung wortlängen Erkennung Entitätenreferenze empirisch testen intuitioner daten annotiert Phänomen hilfreich simpl Durchprobier möglich Kombination brute force zeitlich Grenze setzen verzichten bewussen graphisch Benutzerschnittstelle cf Reiter et stattdessen editieren Teilnehmerinn Teilnehmer direkt Einführung Anleitung Vorkenntnisse Python nötig Verfügung gestellt Programm aufbauen relativ schnell bearbeitenden Teil verstehen experimentieren Erfahrung fortgeschritten funktionalitäten Programm verwenden jeder maschinell Lernprozesses abschließend Evaluation automatisch generiert Annotation durchführen hierfür Teilnehmerinn Teilnehmer Ablauf begrenzt experimentierens Testen Minute Finale vorher unbekannt Testdate Verfügung stellen daten erstellt Modell anwenden automatisch annotation erzeugen wiederum Goldstandard vergleichen wobei verschieden Entitätenklasse Teilkorpora trennen evaluieren Programm Evaluation stellen bereit verwendet automatisch Annotation Entitätenreferenze demonstrieren Schritt Automatisierung Textanalyseaufgabe mittels maschinell lernverfahren nötig konkret implementieren Teilnehmerinn Teilnehmer bekommen zusammenhängend Überblick manuell Annotation ausgewählter Text Feinjustierung lernverfahren Evaluation Ergebnis vorgestellt vorgehensweise gesamt Ablauf grundsätzlich ähnlich Projekt übertragbar Tutorial schärfen Verständnis Zusammenhang untersucht Konzept relevant Feature statistisch lernverfahren einfließen Einblick technisch Umsetzung bekommen Teilnehmerinn Teilnehmer Verständnis Grenze Möglichkeit Automatisierung befähigen Potenzial Verfahren Vorhaben realistisch einschätzen Ergebnis Basis Verfahren erzielen angemessen Hinterfrag deuten Workshop ausrichten mitarbeiterinn Mitarbeiter center for reflected Text analytics Creta Universität Stuttgart Creta verbinden Literaturwissenschaft Linguistik Philosophie Sozialwissenschaft Maschineller Sprachverarbeitung Visualisierung Hauptaufgabe Creta Entwicklung reflektiert Methode Textanalyse wobei methode Gesamtpaket konzeptuell Rahmen annehmen technisch Implementierung Interpretationsanleitung verstehen Methode black box transparent reflektiert Einsatz Hinblick sozialwissenschaftlich Fragestellung Forschungsinteresse Nils Reiter liegen generell Anwendung computerlinguistisch Methode Frage Sozialwissenschaft insbesondere Operationalisierung literarisch Forschungsfrag adäquat Interpretation Ergebnis schwerpunken regelgeleitet Annotation zusammenhängend Frage Nora Ketschik Promotionsstudentin Abteilung germanistisch Mediävistik Rahmen nehmen analysen narratologischer kategorien Figur Raum ausgewählt mittelhochdeutsch Roman setzen Verwendung Computergestützter Methode literaturwissenschaftlich Analysezweck auseinander Interessenschwerpunkt Gerhard Kremer reflektiert Einsatz Werkzeug Computerlinguistik sozialwissenschaftlich Fragestellung zusammenhängend gehören Entwicklung übertragbar arbeitsmethoden angepas nutzerfreundlich Bedienbarkeit automatisch linguistisch Analysetools Forschungsthem Sarah Schulz beschäftigen überwiegend automatisch Verarbeitung Text syntaktisch lexikalisch eigenschaften aufweisen Beamer besonderer technisch Ausstattung benötigen Raum handeln Teilnehmend Schulter blicken Reihe,"[('teilnehmerinn', 0.286936321791324), ('lernverfahren', 0.2778659264356755), ('teilnehmer', 0.2488185206364265), ('entitätenreferenze', 0.18526766557594426), ('tutorial', 0.17419626665985385), ('automatisch', 0.16771017897386048), ('maschinell', 0.1671154524186794), ('creta', 0.16560862249037991), ('annotation', 0.14595544374154076), ('sozialwissenschaftlich', 0.13573072417559398)]"
2018,DHd2018,GIUS_Evelyn_hermA__Zur_Rolle_von_Annotationen_in_hermeneutis.xml,hermA. Zur Rolle von Annotationen in hermeneutischen Prozessen,"Benedikt Adelmann (Universität Hamburg); Melanie Andresen (Universität Hamburg); Anke Begerow (Hochschule für angewandte Wissenschaften, Hamburg); Uta Gaidys (Hochschule für angewandte Wissenschaften, Hamburg); Evelyn Gius (Universität Hamburg); Gertraud Koch (Universität Hamburg); Wolfgang Menzel (Universität Hamburg); Dominik Orth (Bergische Universität Wuppertal); Sebastian Topp (Universität Hamburg); Michael Vauth (Technische Universität Hamburg); Heike Zinsmeister (Universität Hamburg)","Hermeneutik, Automatisierung, Annotation, Codierung","Modellierung, Annotieren, Kollaboration, Methoden, Forschungsprozess, Text","Der Forschungsverbund ""Automatisierte Modellierung hermeneutischer Prozesse"" (hermA) befasst sich im Rahmen einer interdisziplinären Zusammenarbeit von Literaturwissenschaft, Pflegewissenschaft, Kulturanthropologie, Computerlinguistik und Informatik mit der Frage, ob und inwieweit hermeneutisches Arbeiten im Bereich der sozial- und geisteswissenschaftlichen Textanalyse computergestützt automatisiert werden kann. Von der Auseinandersetzung mit dieser Frage sind zum einen Erkenntnisse über die Verwendung und Funktion von Annotationen in den jeweiligen hermeneutischen Prozessen zu erwarten, zum anderen sollen erste Ansätze zur Automatisierung des Analyseprozesses entwickelt werden, die die Auswertung größerer Textmengen unterstützen. Die fünf Teilprojekte von hermA folgen in ihrer hermeneutischen Arbeit an und mit Texten unterschiedlichen Forschungslogiken (deduktiv, induktiv und/oder abduktiv); sie arbeiten außerdem jeweils eigenständig zu einem Thema im Gesundheitsbereich und stellen damit thematisch verbundene, fachdisziplinäre Forschungsszenarien zur Evaluation der automatisierten Modelle zur Verfügung: Die Teilprojekte im Projekt hermA decken die gesamte Bandbreite an hermeneutischen Vorgehensweisen ab, die sich auf deduktive, induktive und abduktive Schlussverfahren zurückführen lassen. Damit geht es um alle drei darauf basierende Forschungslogiken, die Charles Sanders Peirce folgendermaßen zusammenfasst: ""Deduction proves that something Es werden also jeweils bestimmte Strategien genutzt, um unterschiedliche Arten von Erkenntnissen zu erlangen: Hinzu kommt: In jedem hermeneutischen Erkenntnisprozess werden laufend neue Erkenntnisse generiert. Wenn diese sich nicht in die jeweilige Forschungslogik integrieren lassen, müssen die entsprechenden Hypothesen und/oder Vorhersagen revidiert oder erweitert und anschließend erneut angewendet werden. Dabei ist es zum Teil nötig, auf andere Forschungslogiken zurückzugreifen (etwa durch eine induktive Herleitung einer neuen Regel, die im deduktiven Prozess angewendet werden kann). Ein erstes Zwischenergebnis des Projekts hermA ist, dass die Rolle von Annotationen in hermeneutischen Prozessen von der jeweilig zur Anwendung kommenden Forschungslogik abhängt. Diese Zusammenhänge zwischen den Forschungslogiken und dem Einsatz von Annotationen sollen auf dem vorgeschlagenen Poster mit Blick auf die von den Teilprojekten verfolgten Forschungsfragen dargestellt werden. Dabei geht es um folgende Aspekte: Bei der Betrachtung der Rolle von Annotationen muss zusätzlich zwischen manuellen und automatischen Annotationen differenziert werden. Während manuelle Annotationen eher zur Entwicklung oder Überprüfung von Hypothesen genutzt werden, unterstützen die automatisierten Zugänge jene Aspekte der hermeneutischen Prozesse, die bereits klar definiert werden können 'etwa die Erkennung bereits operationalisierter Phänomene oder die Identifikation relevanter Texte oder Textstellen für die weitere Analyse und Interpretation.",de,Forschungsverbund automatisiert Modellierung hermeneutisch prozesse Herma befassen Rahmen interdisziplinär Zusammenarbeit Literaturwissenschaft Pflegewissenschaft Kulturanthropologie Computerlinguistik Informatik Frage inwieweit hermeneutisch arbeiten Bereich geisteswissenschaftlich Textanalyse Computergestützt automatisieren Auseinandersetzung Frage Erkenntnis Verwendung Funktion annotatio jeweilig hermeneutisch Prozessen erwarten Ansatz Automatisierung analyseprozes entwickeln Auswertung groß Textmeng unterstützen Teilprojekt Herma Folge hermeneutisch Arbeit text unterschiedlich Forschungslogike deduktiv induktiv abduktiv arbeiten jeweils eigenständig Thema gesundheitsbereich stellen thematisch verbunden fachdisziplinär forschungsszenarie Evaluation automatisiert Modell Verfügung teilprojekte Projekt Herma decken gesamt Bandbreit hermeneutisch vorgehensweisen deduktiv induktiv abduktiv Schlussverfahr zurückführen lassen basierend Forschungslogike Charles sander Peirce folgendermaßen zusammenfasst Deduction proves thaen something jeweils bestimmt Strategie nutzen unterschiedlich Art Erkenntnis erlangen hinzu hermeneutisch erkenntnisprozess laufend erkenntnis neriern jeweilig forschungslogik integrieren lassen entsprechend Hypothese Vorhersag revidieren erweitern anschließend erneut anwenden nötig Forschungslogike zurückzugreifen induktiv Herleitung Regel deduktiv Prozess anwenden Zwischenergebnis Projekt Herma Rolle annotatio hermeneutisch Prozessen Jeweilig Anwendung kommend forschungslogik abhängen zusammenhänge Forschungslogik Einsatz Annotation vorgeschlagen Poster Blick Teilprojekte verfolgt Forschungsfrag darstellen folgend Aspekt Betrachtung Rolle annotation zusätzlich Manuelle automatisch annotatio differenzieren Manuelle Annotation eher Entwicklung Überprüfung Hypothesen nutzen unterstützen automatisiert zugänge Aspekt hermeneutisch Prozesse klar definieren Erkennung operationalisiert Phänomen Identifikation relevant Text Textstell Analyse Interpretation,"[('hermeneutisch', 0.3782008926665446), ('herma', 0.2886166290576202), ('forschungslogik', 0.25615984984549073), ('forschungslogike', 0.25615984984549073), ('deduktiv', 0.20856357298811068), ('induktiv', 0.19099720335927628), ('abduktiv', 0.15906232014443758), ('teilprojekte', 0.1507532950779634), ('prozessen', 0.1390423819920738), ('annotatio', 0.1347070696454808)]"
2018,DHd2018,FISCHER_Frank__Liebe_und_Tod_in_der_Deutschen_Nationalbiblio.xml,Liebe und Tod in der Deutschen Nationalbibliothek.   Der DNB-Katalog als Forschungsobjekt der digitalen Literaturwissenschaft,"Frank Fischer (Higher School of Economics, Moskau); Robert Jäschke (Humboldt-Universität, Berlin)","Deutsche Nationalbibliothek, Metadaten, Katalogdaten, Literaturwissenschaft","Programmierung, Visualisierung, Literatur, Metadaten","Der Sammelauftrag der Deutschen Nationalbibliothek (DNB) beginnt 1913 und bezieht sich auf ""lückenlos alle deutschen und deutschsprachigen Publikationen"" (""Wir über uns"", 16.03.2017). Der DNB-Katalog ist natürlich längst digitalisiert und die Arbeit mit ihm mittlerweile sehr komfortabel, da der Datendienst der DNB unter http://www.dnb.de/datendienst vierteljährlich einen Komplettabzug der Katalogdaten im RDF-Format bereitstellt, unter der freien Lizenz CC0 1.0. Momentan (Stand vom 23.06.2017) enthält er 14¬†102¬†309 Datensätze, also Metadaten zu von der DNB gesammelten Medien. Bisher gibt es aus geisteswissenschaftlicher Sicht nur wenige Versuche, diese Quelle nutzbar zu machen (eine Ausnahme bilden etwa Häntzschel u.¬†a. 2009). Wir präsentieren ein einfaches Framework, mit dem verschiedene Aspekte des DNB-Katalogs untersucht werden können, seine Entwicklung über die knapp 105 Jahre seit Bestehen der Nationalbibliothek (vgl. auch Schmidt 2017, der für die Library of Congress einen ähnlichen Ansatz vorgestellt hat). Wir konzentrieren uns dabei auf Romane als Untersuchungsobjekt, von denen in der DNB rund 180¬†000 als solche rubriziert sind (dies entspricht nicht der Gesamtanzahl an Romanen, denn Nachauflagen und Übersetzungen zählen dort mit hinein 'außerdem fehlen auch einige Romane, da sie nicht entsprechend verschlagwortet worden sind. Dieser Vortrag ist methoden-, nicht vorderhand ergebniszentriert, wobei wir an zwei Anwendungsszenarien aus der Praxis der digitalen Literaturwissenschaft demonstrieren, wie Katalogmetadaten bei der Bearbeitung konkreter Forschungsfragen behilflich sein können bzw. diese überhaupt erst ermöglichen. Die Titeldaten der DNB werden in typischen Linked-Data-Formaten (RDF/XML, JSON-LD usw.) angeboten. Der übliche Ansatz mit solchen Daten zu arbeiten ist, diese in eine geeignete Datenbank (Triple-Store) einzuladen und Anfragen mit Hilfe der entsprechenden Anfragesprache (i.¬†A. SPARQL) zu stellen. Prinzipiell sind auch andere Systeme (z.¬†B. relationale Datenbank, Suchmaschine) geeignet. Dies ermöglicht sehr flexible Anfragen und die leichte Einbindung weiterer Datenquellen. Da die Größe der Daten (unkomprimiert ca. 21¬†GB) jedoch gewisse Anforderungen an die Hardware stellt und die Konfiguration und Optimierung der Datenbank aufwendig ist, haben wir uns für eine andere, kompakte und leichter nachzuvollziehende Lösung entschieden. Langfristiges Ziel ist jedoch die Bereitstellung einer fertig konfigurierten Arbeitsumgebung in Form eines Docker-Containers, in der die Daten in einer Datenbank ad hoc verfüg- und analysierbar sind. Der Titeldatensatz ist mit 14¬†102¬†309 Datensätzen und 227¬†212¬†707 Tripeln (""Fakten"") sehr umfangreich und enthält neben Angaben zu Büchern auch Angaben zu weiteren Medientypen wie etwa Zeitschriften. Neben den üblichen Metadatenfeldern wie Titel und Erscheinungsjahr ist bei Buchobjekten meist auch die Seitenanzahl sowie das Format vermerkt. Ganz im Sinne von Linked Data werden viele Angaben mit Hilfe von standardisierten Vokabularien (z.¬†B. Dublin Core oder Bibo) beschrieben und ermöglichen so die Verlinkung mit weiteren Datensätzen. Insbesondere ermöglicht die Angabe der Autor*innen durch die numerische Kennung aus der Gemeinsamen Normdatei (GND) die Verknüpfung der Daten mit Wikidata, der (zukünftig) hinter Wikipedia stehenden Faktendatenbank. Wikidata verwendet ein auf Linked Data basierendes Datenmodell und ermöglicht, ähnlich wie Wikipedia, jedermann das Hinzufügen und Bearbeiten von Daten. Neben Angaben zu Städten und Ländern (z.¬†B. Fläche, Einwohnerzahl) sind in Wikidata auch Daten zu zahlreichen Persönlichkeiten gespeichert, etwa deren Namen, Geburtsdaten, Berufe, Werke und, falls vorhanden, GND-Kennung (als Beispiel sei auf die Seite zu Johann Wolfgang von Goethe verwiesen: https://www.wikidata.org/wiki/Q5879). Unser Framework umfasst derzeit vier Schritte, die im Folgenden beschrieben werden: RDF/XML wird von den üblichen Softwaretools im Allgemeinen nicht als Datenstrom verarbeitet, sondern im Hauptspeicher abgelegt und dann weiterverarbeitet. Aufgrund der Größe der Daten scheidet diese Möglichkeit aus. Da jedoch alle wesentlichen Daten zu einem Medium typischerweise innerhalb eines XML-Tags ""rdf:Description"" abgelegt sind, können wir die Daten auch mit Hilfe eines SAX-Parsers als XML verarbeiten. Wir extrahieren die für die Analyse wesentlichen Metadaten (z.¬†B. dcterms:contributor, dcterms:language, dc:title, dcterms:extent, rdau:P60493) und speichern diese als JSON ab. JSON ist im Allgemeinen platzsparender als RDF/XML und kann leicht in Elasticsearch eingeladen werden, was ein geplanter nächster Schritt ist. Unser Ziel ist die Anreicherung der Autorenangaben im DNB-Datensatz mit Informationen aus Wikidata, beispielsweise Geburtsdatum- und ‚Äëort, Beruf und Verweis auf einen etwa vorhandenen Artikel in Wikipedia. Da die Python-Softwarebibliothek zur Verarbeitung von Wikidata-Datensätzen veraltet ist, greifen wir auf das Java-basierte Wikidata Toolkit zurück. Nach Herunterladen des aktuell (14.08.2017) 16¬†GB großen komprimierten Wikidata-Datensatzes extrahieren wir in zwei Durchgängen zunächst alle Elemente mit einer GND-Kennung einschließlich ausgewählter Merkmale und ergänzen im zweiten Durchlauf die Werte der Merkmale. Das Ergebnis speichern wir im JSON-Format. Unser Python-Skript implementiert eine Pipeline, die alle in den vorherigen Schritten extrahierten Daten einliest und mit Hilfe der GND-Kennung verknüpft, Metadatenangaben (wie z.¬†B. Seitenanzahlen) extrahiert, vereinfacht und normalisiert, Datensätze mit fehlenden Angaben filtert und schließlich die gewünschten Datenfelder spaltenbasiert ausgibt. Die Vereinfachung umfasst vor Allem das Entfernen von Namespace-Präfixen (etwa http://id.loc.gov/vocabulary/iso639-2/ bei der Angabe der Sprache); Seitenanzahlen werden mit Hilfe eines regulären Ausdrucks extrahiert, der die häufigsten Fälle abdeckt; Jahreszahlen ebenso; Verlagsnamen können mit Hilfe einer Normtabelle normiert werden (dies ist nötig, da die Schreibung dieser Namen innerhalb des Katalogs nicht standardisiert ist). Die entstandenen Dateien im TSV-Format können mit den üblichen Unix-Kommandozeilen-Werkzeugen wie awk, sort, uniq etc. leicht verarbeitet und analysiert werden; Visualisierungen wurden mit gnuplot erzeugt. Alle Schritte sind im GitHub-Repository dokumentiert. Abbildung¬†1 zeigt die zeitliche Verteilung einiger Subdatensätze des Katalogs. Von den etwa 14,1¬†Mio. Objekten im originalen DNB-Datensatz weisen etwa 8,3¬†Mio. extrahierbare Seitenanzahlen auf (59¬†%). Beschränken wir diese Anzahl auf ""Romane"" (über das Datenfeld ""rdau:P60493""), bleiben 353¬†498 übrig, von denen wiederum 316¬†518 Umfangsangaben aufweisen und 180¬†219 einen Verfasser, der mindestens einen Wikipedia-Eintrag (in egal welcher Sprache) besitzt. Dieses Datenset ist die Grundlage für die unten folgenden Anwendungsszenarien.  Als möglicher Plausibilitäts- bzw. Repräsentativitätstest kann das Auszählen derjenigen Romanciers dienen, die mit den meisten Romanen im Katalog vertreten sind. Da der DNB-Katalog Vollständigkeit anstrebt, kann ein entsprechendes Ranking etwas über vergangene Realitäten auf dem deutschsprachigen Buchmarkt aussagen (Tab.¬†1), und tatsächlich stehen die Verfasser*innen von Romanbestsellern im Unterhaltungsbereich ganz oben (die Anzahl der Bücher umfasst von der DNB mitgesammelte Neuauflagen, Konsalik hat also nicht über 2¬†000 Romane geschrieben). Tabelle 1: Romanautor*innen geordnet nach Anzahl der Werke (inkl. Nachauflagen) im DNB-Katalog. Die Verfügbarkeit großer digitalisierter Kataloge ermöglicht Large-Scale-Analysen bibliografischer Metadaten, etwa die Entwicklung von Romantiteln. Ein Vorläufer auf diesem Gebiet, Werner Bergengruens immer noch zu empfehlende Bibliothekarsfantasie ""Titulus"" von 1960, musste sich noch auf eine manuelle Sammlung des Autors stützen. Mittlerweile gibt es mit Franco Morettis Studie ""Style Inc."" (2009) ein prominentes datengestütztes Beispiel (wobei sich Moretti bei seiner Analyse von um die 7¬†000 Romantiteln auf Fachbibliografien stützte, nicht auf Katalogdaten). Um einen ersten Einblick in das Vokabular von Romantiteln zu bekommen, seien in Tabelle¬†2 die am häufigsten vorkommenden Substantive aufgelistet. Tabelle 2: Häufigste Substantive in Romantiteln im gesamten DNB-Katalog. Überzeitliche Konzepte 'Liebe, Tod usw. 'dominieren das Feld. Und nebenbei bemerkt: Ein wenig erinnert diese Liste an Jan Böhmermanns satirischen Song ""Menschen, Leben, Tanzen, Welt"", mit dem auf die Beliebig- und Austauschbarkeit kontemporärer deutschsprachiger Liedproduktion angespielt wird (vgl. Pandzko/Böhmermann 2017), ein Befund, der sich analog auch auf Romantitel projizieren ließe. Diese Anfragetechnik kann 'wie beim Google Ngram Viewer 'auf n-Gramme ausgedehnt werden, die Top-10 der häufigsten Trigramme findet sich in Tabelle¬†3. Tabelle 3: Häufigste Trigramme in Romantiteln im DNB-Katalog. Ebenfalls analog zum Ngram Viewer lässt sich die zeitliche Entwicklung von n-Gramm-Frequenzen darstellen. Die unterschiedlichen Darstellungen in absoluten (Abb.¬†2) und relativen Zahlen (Abb.¬†3) kann etwa zeigen, dass sich zwischen Mitte der 1970er-Jahre und Mitte der 1990er-Jahre die Zahl an Romanen mit ""Liebes""-Titeln zwar nahezu verdoppelt, dass sich diese Titel aber in relativen Zahlen nicht großartig vermehren. Für genauere Analysen auf Grundlage dieser Extraktions- und Visualisierungsmethoden stellt das von uns vorgestellte Framework eine ideale Basis dar. Unser zweites Anwendungsszenario betrifft die Erforschung des literarischen Textumfangs. Abbildung¬†4 zeigt die durchschnittliche Seitenanzahl von Romanen im Katalog der DNB. Als Zuarbeit zu einer Theorie des literarischen Textumfangs haben wir mit dem von uns hier vorgestellten Framework in einer umfangreicheren Studie untersucht, wie sich der Umfang von Romanen etwa auf die Kanonbildung auswirkt (längere Romane, speziell solche von mehr als 1¬†000 Seiten Umfang, haben es leichter, in Kanonlisten zu landen). Außerdem ist es uns gelungen zu zeigen, wie umfangreiche Romane die DNA von Verlagen bestimmen können (vgl. Fischer/Jäschke 2018).  Katalogdaten als Untersuchungsobjekt der quantifizierenden Literaturwissenschaften sind keine sich selbst erklärende Quelle, sondern ein über Jahrhunderte gewachsenes, überaus komplexes System. Die bibliothekarische Betreuung dieser Daten zielt nicht per se auf literaturwissenschaftliche Anwendungsfälle. Die Verschlagwortung kann lückenbehaftet sein, bestimmte Angaben wie etwa zum Textumfang können Fehler aufweisen. Die literaturwissenschaftliche Beschäftigung mit Katalogdaten setzt deren Explorier- und Kontrollierbarkeit voraus, wozu das hier vorgestellte Framework einen ersten Beitrag leisten soll. Zwei konkrete Anwendungsfälle sollten als Praxisbeispiele und ausdrücklich als Anreiz für weitere Szenarien dienen.",de,Sammelauftrag deutsch Nationalbibliothek dnb beginnen beziehen lückenlos deutsch deutschsprachig Publikation längst digitalisieren Arbeit mittlerweile komfortabel Datendienst Dnb vierteljährlich Komplettabzug Katalogdat bereitstellen frei Lizenz momentan stehen enthalten datensätz metadaen Dnb gesammelt Medium geisteswissenschaftlich Sicht Versuch Quelle nutzbar Ausnahme bilden Häntzschel präsentieren einfach Framework verschieden Aspekt untersuchen Entwicklung knapp Besteh Nationalbibliothek Schmidt Library -- congress ähnlich Ansatz vorstellen konzentrieren Roman Untersuchungsobjekt Dnb rubrizieren entsprechen Gesamtanzahl romanen nachauflag übersetzung zählen hinein fehlen Roman entsprechend verschlagworten Vortrag Vorderhand ergebniszentrieren wobei Anwendungsszenarie Praxis digital Literaturwissenschaft demonstrieren Katalogmetadat Bearbeitung konkret forschungsfrag behilflich ermöglichen Titeldat Dnb typisch rdf xml anbieten üblich Ansatz daten arbeiten geeignet datenbank einladen Anfrage Hilfe entsprechend Anfragesprach Sparql stellen prinzipiell System relational Datenbank Suchmaschin eignen ermöglichen flexibel Anfrage leicht Einbindung weit datenquellen Größe daten unkomprimiert gewiß anforderungen Hardware stellen Konfiguration Optimierung Datenbank aufwendig kompaken leicht nachzuvollziehend Lösung entscheiden langfristig Ziel Bereitstellung fertig konfiguriert Arbeitsumgebung Form daten datenbank ad hoc analysierbar Titeldatensatz datensätzen Tripel fakten umfangreich enthalten Angabe büchern Angabe Medientype zeitschriften üblich Metadatenfelder Titel Erscheinungsjahr buchobjekter meist Seitenanzahl Format vermerken Sinn Linked data Angabe Hilfe Standardisiert vokabularien Dublin Core bibo beschreiben ermöglichen Verlinkung Datensätze insbesondere ermöglichen Angabe numerisch Kennung gemeinsam Normdatei gnd Verknüpfung daten Wikidata zukünftig wikipedia stehend Faktendatenbank Wikidata verwenden Linked Data basierendes datenmodell ermöglichen ähnlich wikipedia Hinzufügen bearbeiten daten Angabe städten Land Fläche Einwohnerzahl Wikidata daten zahlreich Persönlichkeit speichern Name burtsdaten beruf Werk falls vorhanden Seite Johann Wolfgang Goethe verweisen Framework umfassen derzeit Schritt folgend beschreiben rdf xml üblich Softwaretools Datenstrom verarbeiten Hauptspeicher ablegen weiterverarbeiten aufgrund Größe daten scheiden Möglichkeit wesentlich daten Medium typischerweise innerhalb rdf description ablegen daten Hilfe xml verarbeiten extrahieren Analyse wesentlich Metadat Dcterms Contributor Dcterms Language dc title Dcterms Extent Rdau speichern Json Json Platzsparender rdf xml Elasticsearch einladen Geplanter Nächster Schritt Ziel Anreicherung Autorenangabe Information Wikidata beispielsweise äëort Beruf Verweis vorhanden Artikel wikipedia Verarbeitung veralten greifen Wikidata Toolkit herunterlad Aktuell Komprimiert extrahieren Durchgäng Element einschließlich ausgewählt merkmal ergänzen Durchlauf Wert Merkmal Ergebnis speichern implementieren pipeline vorherig Schritt extrahiert daten Einliest Hilfe verknüpfen metadatenangaben seitenanzahlen extrahiern vereinfachen normalisieren datensätzen fehlend Angabe filtern schließlich gewünschten datenfeld spaltenbasiern ausgeben Vereinfachung umfassen Entfern Angabe Sprache seitenanzahlen Hilfe regulär Ausdruck extrahiern häufig Fall abdecken jahreszahlen verlagsnamen Hilfe Normtabelle normiert nötig Schreibung Name innerhalb Katalog standardisieren entstanden Datei üblich awk Sort uniq verarbeiten analysieren visualisierung Gnuplot erzeugen Schritt dokumentieren zeigen zeitlich Verteilung Subdatensätz Katalog objekten original weisen extrahierbar seitenanzahlen beschränken Anzahl Roman Datenfeld rdau bleiben übrig wiederum umfangsangaben aufweisen verfasser mindestens egal Sprache besitzen datenset Grundlage unten folgend Anwendungsszenarie möglich Repräsentativitätstest auszähl Romanciers dienen meister Roman Katalog vertreten Vollständigkeit anstreben entsprechend Ranking Realität deutschsprachig buchmarken Aussage tatsächlich stehen Romanbestseller Unterhaltungsbereich Anzahl bücher Umfasst Dnb Mitgesammelte neuauflagen Konsalik Roman schreiben Tabell ordnen Anzahl Werk nachauflag Verfügbarkeit Digitalisierter Kataloge ermöglichen bibliografisch metadaten Entwicklung Romantitel Vorläufer Gebiet Werner bergengruens empfehlend Bibliothekarsfantasie titulus manuell Sammlung Autor stützen mittlerweile Franco Moretti Studie style inc prominent datengestützt wobei moretti Analyse Romantitel fachbibliografien stützen Katalogdat Einblick Vokabular Romantiteln bekommen häufig vorkommend Substantive auflisten Tabell häufigst Substantiv Romantiteln gesamt überzeitlich Konzept liebe Tod dominieren Feld nebenbei bemerken erinnern Liste Jan Böhmermann satirisch Song Mensch leben Tanze Welt austauschbarkeit kontemporärer deutschsprachig Liedproduktion angespielt Pandzko Böhmermann Befund analog Romantitel projizieren lassen Anfragetechnik Google ngram viewer ausdehnen häufig Trigramme finden Tabell häufigst Trigramme Romantiteln ebenfalls analog Ngram viewer lässen zeitlich Entwicklung darstellen unterschiedlich Darstellung absolut Relativen zahlen zeigen Mitte Mitte Zahl Roman nahezu verdoppeln Titel Relativen Zahl großartig vermehren genau Analyse Grundlage visualisierungsmethoden stellen vorgestellt Framework ideal Basis dar Anwendungsszenario betreffen Erforschung literarisch Textumfang zeigen durchschnittlich Seitenanzahl Roman Katalog dnb Zuarbeit Theorie literarisch Textumfang vorgestellt Framework umfangreich Studie untersuchen Umfang Roman Kanonbildung auswirken lang Roman speziell Seite umfingen leicht kanonlisten landen gelingen zeigen umfangreich Roman dna Verlag bestimmen Fischer jäschke Katalogdat Untersuchungsobjekt quantifizierende Literaturwissenschaften erklärend Quelle jahrhundert Gewachsen überaus komplex System bibliothekarisch Betreuung daten zielen per se Literaturwissenschaftliche anwendungsfällen Verschlagwortung lückenbehaften bestimmt Angabe Textumfang Fehler aufweisen literaturwissenschaftlich Beschäftigung Katalogdat setzen Kontrollierbarkeit voraus wozu vorgestellt Framework Beitrag leisten konkret anwendungsfäll praxisbeispiel ausdrücklich Anreiz Szenarie dienen,"[('dnb', 0.24443679883502345), ('angabe', 0.19165200481346423), ('roman', 0.1778949081238423), ('katalogdat', 0.16529396043989567), ('katalog', 0.15395879293315123), ('wikidata', 0.14120741035657022), ('daten', 0.13789135284887769), ('framework', 0.12615372134823893), ('seitenanzahlen', 0.12397047032992173), ('romantiteln', 0.12397047032992173)]"
2018,DHd2018,SCHÖCH_Christof_Burrows_Zeta__Varianten_und_Evaluation.xml,Burrows Zeta: Varianten und Evaluation,"Christof Schöch (Universität Trier, Deutschland); Albin Zehe (Universität Würzburg, Deutschland); José Calvo Tello (Universität Würzburg, Deutschland); Andreas Hotho (Universität Würzburg, Deutschland)","Zeta, Distinktivität, Keyness, Stilometrie","Programmierung, Inhaltsanalyse, Beziehungsanalyse, Theoretisierung, Text","Der vorliegende Beitrag enthält methodische Überlegungen und Experimente zu ""Zeta"", einem von John Burrows (2007) vorgeschlagenen Maß für die Distinktivität oder ""keyness"" von textuellen Merkmalen (Wortformen, Lemmata, etc.). Mit solchen Maßen werden Merkmale ermittelt, die für eine bestimmte Gruppe von Texten gegenüber einer Vergleichsgruppe charakteristisch sind. Das Exposé gibt einen Überblick zu solchen Maßen, bevor die Funktionsweise von Zeta erläutert wird. Aufbauend auf einer Neu-Implementierung in Python (""pyzeta"", https://github.com/cligs/pyzeta) und Vorarbeiten (Schöch im Druck) liegt der spezifische Forschungsbeitrag dann in den folgenden Schritten: erstens werden mehrere Varianten von Zeta vorgeschlagen und implementiert; zweitens werden Verfahren zum Vergleich und der Evaluation der Ergebnisse erprobt. Ziel ist es, Zeta in seiner Funktionsweise und in seiner Beziehung zu vergleichbaren Maßen besser zu verstehen und vorhandene Nachteile des Maßes durch gezielte Modifikationen zu beheben. Die vergleichende, kontrastierende Analyse zweier Gruppen von Texten ist ein in den Sprach- und Literaturwissenschaften weit verbreitetes Verfahren. Entsprechend wurden zahlreiche Maße der Distinktivität oder ""keyness"" von Merkmalen entwickelt und für vielfältige Fragestellungen eingesetzt. Die grundlegende Annahme solcher Maße ist, dass ein Merkmal nicht schon durch seine reine Häufigkeit in einer Textgruppe für diese charakteristisch ist, sondern dass dies auch davon abhängt, wie häufig das Merkmal in einer Vergleichsgruppe ist. Diejenigen Merkmale bekommen einen besonders hohen Wert zugewiesen, die in der einen Gruppe sehr häufig sind und zugleich in der Vergleichsgruppe sehr selten sind (Scott 1997, 236). Man kann vier Arten von Verfahren unterscheiden: Die praktische Bedeutung von Distinktivitätsmaßen ist daran erkennbar, dass Korpusanalyse-Software meist eine entsprechende Funktion anbietet, so ""keyness"" in WordCruncher (Scott 1997) oder ""spécificity"" in TXM (Heiden et al. 2012). Kilgariff 2004 und Lijfijt et al. 2014 sind wichtige Arbeiten zur Evaluation von Distinktivitätsmaßen. Das von John Burrows (2007) vorgeschlagene ""Zeta"" beruht auf einem Dispersionsmaß. Vor der Berechnung werden die Texte in kleinere Segmente gesplittet, wobei die Segmentlänge ein wichtiger Parameter ist. Dann wird für jedes Merkmal der Anteile der Segmente erhoben, in denen das Merkmal mindestens einmal vorkommt (die ""document proportion""). Von diesem Anteil in der untersuchten Gruppe wird der entsprechende Anteil in der Vergleichsgruppe subtrahiert, woraus sich ein Zeta-Wert zwischen -1 und 1 ergibt. Ein Effekt dieser Berechnungsweise ist, dass Zeta Inhaltswörter als distinktive Wörter favorisiert, Funktionswörter sowie Eigennamen hingegen penalisiert. Daraus ergibt sich eine hohe Interpretierbarkeit der Ergebnisse, die Zeta im Vergleich zu anderen Maßen für die (digitalen) Literaturwissenschaften besonders attraktiv macht. Ein Nachteil ist, dass Merkmale durch die Subtraktion niemals einen Zeta-Wert bekommen können, der höher ist als ihre ""document proportion"" in der untersuchten Textgruppe, selbst wenn sie gegenüber der Vergleichsgruppe deutlich überrepräsentiert sind (Abbildung 1, Wörter in den roten Rahmen; Schöch im Druck).  Eine bekannte Implementierung von Zeta existiert im stylo-Paket für Rin der Funktion ""oppose()"" (Eder et al. 2016). Abbildung 2 zeigt für ein Beispiel die Ergebnisdarstellung in der hier verwendeten ""pyzeta""-Implementierung.  Anwendungsbeispiele von Zeta gibt es in der Shakespeare-Forschung (Craig und Kinney 2009), der modernen englischsprachigen Literatur (Hoover 2010; Weidman und O""Sullivan 2017) und der Romanistik (Schöch im Druck). In der zuletzt genannten Arbeit zum französischen Theater der Klassik und Aufklärung konnte nicht nur die erwartbare, klare Differenzierung von Komödien und Tragödien gezeigt werden. Vielmehr wurde auch die spezifische Verortung der Tragikomödien deutlich, die nicht als Mischform zwischen Komödien und Tragödien zu verstehen sind, sondern eine besondere Affinität zur Tragödie aufweisen (Abbildung 3). Ausgehend von der ursprünglichen Formulierung von Zeta durch Burrows als Subtraktion der ""document proportions"" lassen sich mehrere Faktoren identifizieren, die zur Formulierung von Varianten von Zeta geeignet erscheinen: Die Kombination dieser Faktoren ergibt 8 Varianten von Zeta (Tabelle 1). Tabelle 1: Übersicht über die getesteten Varianten von Zeta. Die Variante mit Label ""sd0"" entspricht Burrows"" Zeta. Einige der Varianten sind mathematisch gut motivierbar und versprechen, den oben genannten Nachteil der begrenzten Werte für bestimmte Wörter auszugleichen und damit Zeta zu verbessern, es wurden aber alle implementiert und auf zwei Datensätzen evaluiert. Es wurden zwei unterschiedliche Korpora verwendet. Erstens ein Korpus aus der textbox-Sammlung (Schöch et al. 2017), das Romane enthält, die zwischen 1880 und 1940 veröffentlicht wurden: jeweils 24 Texte aus Spanien und aus Lateinamerika (ca. 2,8 Millionen Tokens). Zweitens, ein Teil der Sammlung Die 8 Varianten führen zu unterschiedlichen Wortlisten, geordnet nach absteigenden Zeta-Werten. Vergleicht man den Beginn der Wortlisten für zwei Varianten, fällt auf, dass es wie erwartet zu Verschiebungen im Rang der distinktivsten Wörter kommt. Um den Grad der Abweichung der Ergebnisse für alle Varianten zueinander auf der Grundlage längerer Wortlisten zu erheben, ist ein quantifizierendes Verfahren unerlässlich. Ein Ansatz ist, ein Clustering der Maße auf Basis der Zeta-Werte ihrer Wörter vorzunehmen (Abbildung 4).  Abbildung 4 zeigt, dass der wichtigste Faktor für die Unterschiedlichkeit der Varianten ist, ob subtrahiert oder dividiert wird (zwei Haupt-Cluster). Die beiden anderen Variablen spielen eine viel kleinere Rolle. (Die Ergebnisse weiterer Analysen, u.a. auf Basis der RBO-Öhnlichkeit (""ranked biased order"", Webber et al. 2010), werden aus Platzgründen hier nicht diskutiert.) Unabhängig von den Beziehungen der Varianten zueinander stellt sich die Frage, welche der Varianten von Zeta besonders gut distinktive Wörter identifiziert. Dabei kann zur Evaluation nicht auf einen Goldstandard zurückgegriffen werden: eine händische Annotation der Wörter nach dem Grad ihrer Distinktivität ist nicht möglich, weil niemand das zugrunde liegende Korpus überblicken kann. Die Qualität eines Distinktivitätsmaßes kann aber evaluiert werden, indem es als Merkmalsselektor für einen Klassifikationstask verwendet wird. Wenn die durch Zeta am höchsten bewerteten Wörter als Features für einen Klassifikator verwendet werden, sollte dieser Klassifikator eine höhere Genauigkeit erreichen als bei einfacher Verwendung der häufigsten Wörter. Tatsächlich lässt sich dieser Effekt auf dem Korpus der spanisch-sprachigen Romane nachweisen (Tabelle 2). Zur Ermittlung einer Baseline wurde für die Klassifikation in spanische und lateinamerikanische Romane ein linearer SVM-Classifier auf den häufigsten 80, nach TF-IDF gewichteten Wörtern (ohne Stoppwörter) trainiert. Dieser Classifier erreichte lediglich eine Klassifikationsgüte (F1-Score) von 0.49, ist also nicht vom Zufall zu unterscheiden. Trainiert man stattdessen auf den 40 distinktivsten Wörtern nach Zeta (oder einer der Varianten), lassen sich Genauigkeiten von deutlich über 90% erzielen. Diese Genauigkeit kann nicht als tatsächliches Klassifikationsergebnis gesehen werden, da die distinktivsten Merkmale auf dem gesamten Korpus extrahiert wurden, ohne Aufteilung in Trainings- und Testdaten. Dennoch zeigt das Ergebnis, dass die von Zeta selektierten Merkmale tatsächlich sehr nützlich für eine Klassifikation sind. Zudem zeigen sich deutliche Unterschiede in der Performanz je nach verwendeter Variante: während mit ""sd0"" (=Burrows Zeta) 81% Genauigkeit erreicht wird, erhöht sich dieser Wert bei der Variante mit log2-Transformation, ""sd2"", auf 98%.  Wichtigste Ergebnisse dieses Beitrags sind ein differenziertes Verständnis davon, wie Zeta im Kontext anderer Distinktivitätsmaße einzuordnen ist und wie bestimmte mathematischen Parameter sich auf die Ergebnislisten auswirken: als ein auf dem Grad der Dispersion der Merkmale beruhendes Maß, dessen entscheidende Eigenschaft die Subtraktion der Werte ist. Ein weiteres wesentliches Ergebnis sind die beiden vorgeschlagenen Strategien zum Vergleich und der Evaluation von Distinktivitätsmaßen, wenn eine direkte Evaluation auf Goldstandard-Daten nicht möglich ist. Nächste Schritte: Wir möchten als weitere Evaluationsstrategie künstliche Texte generieren, in denen wir kontrolliert einzelne Wörter mit unterschiedlich stark abweichender Verteilung einfügen. So können verschieden Zeta-Varianten direkt dahingehend evaluiert werden, wie gut sie diese Wörter korrekt identifizieren. Zudem möchten wir neben der ""document proportion"" von Zeta ein weiteres Dispersionsmaß, die von Gries (2008) vorgeschlagene ""deviation of proportions"" als Grundlage für eine weitere Zeta-Variante verwenden. Schließlich möchten wir untersuchen, ob die hohe Interpretierbarkeit des Original-Zeta bei den Varianten mit noch höherer Klassifikationsgüte erhalten bleibt. Eine separate Untersuchung ist in Vorbereitung zu zwei eng zusammenhängenden Fragen: wie sich unterschiedliche Segmentlängen einerseits auf die Ergebnisse auswirken, und wie sich die Ergebnisse verändern, wenn unterschiedlich lange Texte nicht mit allen Segmenten in die Berechnung eingehen, sondern aus jedem Einzeltext zufällig eine identische Anzahl von Segmenten gesampelt wird. Übergeordnetes Ziel all dieser Arbeiten zu Zeta ist es letztlich weniger, ein perfektes Distinktivitätsmaß zu identifizieren, als ein justierbares Maß vorzuschlagen, bei dem in Abhängigkeit von Daten und Forschungsfragen dynamisch Parameter verändert und die resultierenden Verschiebungen in den Ergebnissen visualisiert werden können.",de,vorliegend Beitrag enthalten methodisch Überlegung experiment zeta John Burrow vorgeschlagen Maß Distinktivität keyness textuell Merkmale Wortform Lemmata maß merkmal ermitteln bestimmt Gruppe Text Vergleichsgruppe charakteristisch Exposé Überblick maßen bevor Funktionsweise Zeta erläutern aufbauend Python Pyzeta vorarbeien schöch Druck liegen spezifisch Forschungsbeitrag folgend Schritt erstens mehrere Variant zeta vorschlagen implementieren zweitens Verfahren Vergleich Evaluation Ergebnis erproben Ziel zeta Funktionsweise Beziehung vergleichbar maßen verstehen vorhanden Nachteil maßer gezielt Modifikationen beheben vergleichend kontrastierend Analyse zwei Gruppe Text literaturwissenschaften verbreitet Verfahren entsprechend zahlreich Maß Distinktivität keyness merkmale entwickeln vielfältig Fragestellung einsetzen grundlegend Annahme Maß Merkmal rein Häufigkeit Textgruppe charakteristisch abhängen häufig Merkmal Vergleichsgruppe merkmal bekommen hoch Wert zuweisen Gruppe häufig Vergleichsgruppe selten Scott Art Verfahren unterscheiden praktisch Bedeutung distinktivitätsmaße erkennbar meist entsprechend Funktion anbieten keyness Wordcruncher Scott Spécificity txm heid et Kilgariff Lijfijt Et wichtig Arbeit Evaluation distinktivitätsmaßen John Burrow vorgeschlagen zeta beruhen dispersionsmaß Berechnung Text klein Segment splitten wobei Segmentlänge wichtig parameter jeder Merkmal Anteil Segment erheben Merkmal mindestens vorkommen Document Proportion Anteil untersucht Gruppe entsprechend Anteil Vergleichsgruppe subtrahieren woraus ergeben Effekt berechnungsweise zeta inhaltswörter Distinktive Wörter favorisieren Funktionswörter eigennamen hingegen penalisieren ergeben hoch Interpretierbarkeit Ergebnis zeta Vergleich maeß digital literaturwissenschafn attraktiv Nachteil merkmal Subtraktion niemals bekommen hoch Document Proportion untersucht Textgruppe Vergleichsgruppe deutlich überrepräsentieren Abbildung Wörter rot Rahmen schöch Druck bekannt Implementierung zeta existieren Rin Funktion oppos ed et Abbildung zeigen Ergebnisdarstellung verwendet Anwendungsbeispiel zeta Craig Kinney modern englischsprachig Literatur hoov Weidman o sullivan Romanistik schöch Druck zuletzt genannt Arbeit französisch Theater Klassik Aufklärung erwartbar klar Differenzierung Komödie Tragödie zeigen vielmehr spezifisch Verortung Tragikomödie deutlich Mischform Komödie Tragödie verstehen besonderer Affinität Tragödie aufweisen Abbildung ausgehend ursprünglich Formulierung Zeta Burrow Subtraktion Document Proportion lassen mehrere Faktor identifizieren Formulierung Variant zeta geeignet erscheinen Kombination Faktor ergeben Variant Zeta Tabelle Tabell übersicht getesteter Variant Zeta Variante label entsprechen burrow zeta Variant mathematisch motivierbar versprechen genannt Nachteil begrenzt Wert bestimmt Wörter ausgleichen Zeta verbessern implementieren datensätzen evaluieren unterschiedlich Korpora verwenden erstens Korpus schöch et Roman enthalten veröffentlichen jeweils Text Spanien Lateinamerika Million tokens zweitens Sammlung Variant fahren unterschiedlich Wortlist ordnen absteigend vergleichen Beginn Wortlisten Variant fallen erwarten Verschiebung Rang distinktiv Wörter Grad Abweichung Ergebnis variant zueinander Grundlage lang Wortliste erheben quantifizierend Verfahren unerlässlich Ansatz Clustering Maß Basis Wörter vornehmen Abbildung Abbildung zeigen wichtig Faktor Unterschiedlichkeit Variant subtrahieren dividieren Variable spielen klein Rolle Ergebnis weit Analyse Basis ranked biased order Webber et Platzgründe diskutieren unabhängig Beziehung Variant Zueinander stellen Frage Variant Zeta distinktiv Wörter identifizieren Evaluation Goldstandard zurückgegriffen händisch Annotation Wörter Grad Distinktivität zugrunde liegend Korpus überblicken Qualität distinktivitätsmaß evaluieren Merkmalsselektor Klassifikationstask verwenden zeta hoch bewerteten Wörter Feature Klassifikator verwenden Klassifikator hoch Genauigkeit erreichen einfach Verwendung häufig Wörter tatsächlich lässen Effekt Korpus Roman nachweisen Tabelle Ermittlung Baselin Klassifikation spanisch lateinamerikanisch Roman linearer häufig gewichtet wörtern Stoppwörter trainieren Classifier erreichen lediglich Klassifikationsgüte Zufall unterscheiden trainieren stattdessen distinktiv wörtern Zeta Variant lassen genauigkeit deutlich erzielen Genauigkeit tatsächlich Klassifikationsergebnis sehen distinktiv Merkmal gesamt Korpus extrahiern Aufteilung Testdat dennoch zeigen Ergebnis Zeta selektiert merkmal tatsächlich nützlich Klassifikation zudem zeigen deutlich Unterschied Performanz Verwendeter variante Burrow zeta genauigkeit erreichen erhöhen Wert Variante wichtig ergebnisse Beitrag differenziert Verständnis zeta Kontext anderer Distinktivitätsmaße einordnen bestimmt Mathematisch parameter ergebnislist auswirken Grad Dispersion merkmal Beruhend Maß entscheidend Eigenschaft Subtraktion Wert wesentlich Ergebnis vorgeschlagen strategien Vergleich Evaluation distinktivitätsmaßen direkt Evaluation nächster Schritt möchten evaluationsstrategie künstlich Text generieren kontrollieren einzeln Wörter unterschiedlich stark abweichend Verteilung einfügen verschieden direkt dahingehend evaluieren Wörter korrekt identifizieren zudem möchten Document Proportion zeta dispersionsmaß gries vorgeschlagen Deviation -- proportions Grundlage verwenden schließlich möchten untersuchen hoch Interpretierbarkeit Variant hoch Klassifikationsgüte erhalten bleiben separat Untersuchung Vorbereitung eng zusammenhängend Frage unterschiedlich segmentlängen einerseits ergebniss auswirken Ergebnis verändern unterschiedlich Text Segment Berechnung eingehen Einzeltext zufällig identisch Anzahl Segment gesampelt übergeordnet Ziel all arbeiten Zeta letztlich perfekt Distinktivitätsmaß identifizieren Justierbares Maß vorschlagen Abhängigkeit daten forschungsfrag dynamisch parameter verändern resultierend Verschiebung ergebnissen visualisiern,"[('zeta', 0.620631158661026), ('variant', 0.24679985594428067), ('merkmal', 0.17778848957087845), ('wörter', 0.1648716829902099), ('vergleichsgruppe', 0.15434617009544627), ('proportion', 0.1325678864987392), ('burrow', 0.12685715213297752), ('maß', 0.12199884243254629), ('distinktiv', 0.10148572170638201), ('subtraktion', 0.0994259148740544)]"
2018,DHd2018,HERRMANN_J__Berenike_Praktische_Tagger_Kritik__Zur_Evaluatio.xml,Praktische Tagger-Kritik. Zur Evaluation des POS-Tagging des Deutschen Textarchivs,"Berenike Herrmann (Universität Basel, Schweiz)","Korpus, POS, Tagger, DTA, Intercoder-Reliabilität","Annotieren, Bearbeitung, Stilistische Analyse, Kollaboration, Literatur, Text","Der vorliegende Beitrag leistet eine Tool- und Methoden-Kritik der automatischen Auszeichnung von Wortarten (Part of Speech-, bzw. POS-Taggern) an literarischen Texten des 19. und frühen 20. Jahrhunderts. Er geht über eine rein intellektuelle Reflektion hinaus, indem er erste Schritte einer empirischen Evaluation des POS-Tagging des Deutschen Textarchivs (DTA, Berlin-Brandenburgische Akademie der Wissenschaften) und seiner praktischen Verbesserung vorlegt. Aus der Perspektive der Digitalen Literaturstilistik und des Distant Reading sind Wortarten besonders interessante lexiko-grammatikalische Merkmale, ist ihre Verteilung doch ein wichtiger Indikator für Dimensionen wie Autorstil, Gattung und Register (z.B. Biber / Conrad 2009). POS sind vergleichsweise leicht und scheinbar valide zu bestimmen, gilt doch in der Computerlinguistik das Problem der automatischen Wortartenannotation als gelöst 'auch für das Deutsche, wo eine durchschnittliche Erkennungsgenauigkeit bei 95-97% liegt (vgl. Giesbrecht / Evert 2009). Für DH-Anwender scheint es also nahe zu liegen, ihre Korpora komfortabel mit out-of-the-box-Taggern zu annotieren, oder sich bereits annotierter Korpora zu bedienen, wie zum Beispiel des DTA. Ein genauerer Blick zeigt jedoch, dass Korpora der Geisteswissenschaft, historische wie literarische, von der sprachlichen Varietät abweichen, die den Sprachmodellen der verfügbaren Tagger zugrunde liegt, also Zeitungtexten der Gegenwart (der für das Deutsche frei verfügbare Goldstandard ist derzeit TIGER, ein Korpus von 900.000 Wörtern aus der Frankfurter Rundschau, vgl. Brants et al. 2004). In Nichtstandardvarietäten sinkt die Genauigkeit des POS-Taggings rapide (vgl. z.B. Scheible et al. 2011), und teilweise sind Aussagen über die Annotationsgenauigkeit mangels Referenzstandards gar nicht möglich. Dies betrifft auch das DTA, dessen POS-Tagging bislang nicht systematisch evaluiert wurde. Insgesamt ist die DH-Community also noch recht weit von einem Goldstandard für historische literarische narrative Texte des Zeitraums entfernt. Unser Beitrag leistet hier einen wichtigen Schritt, indem er erste Ergebnisse zur Einschätzung der Qualität ebenso wie zur Verbesserung des Annotationstools vorlegt. Ausgehend von dem Ziel unser Korpus der Literarischen Moderne (KOLIMO http://kolimo.uni-goettingen.de/) valide mit POS auszuzeichnen, haben wir eine Stichprobe (N= 9.065 ) des DTA manuell nachannotiert. Unsere Methode verbindet einen Tagger-Vergleich mit einer händischen Analyse. Dabei werden folgende Ziele verfolgt: Die Evaluation des POS-Taggings wurde durchgeführt auf einer randomisierten Stichprobe des DTA, die aufgrund unseres Forschungsinteresses auf narrative Texte mit Publikationsdatum ab 1800 beschränkt war, wobei sowohl fiktionale wie auch nicht-fiktionale Texte berücksichtigt wurden (ausschlaggebend waren die Metadaten zur Erstveröffentlichung und Gattung im Header des DTA). Die Grundgesamtheit der aus dem DTA entnommenen Stichprobe umfasste N= 64.924.458 Tokens, die der händisch annotierten Tokens umfasste n= 9.065 Tokens/POS-Tags, also 0,014%). Die Stichprobe wurde in ihrer tokenisierten und normalisierten Form aus dem DTA übernommen (vgl. DTA). Der Taggervergleich nutzte neben dem DTA-Tagger Das Tagging wurde durch vier studentische Hilfskräfte besorgt, wobei iterative Analysen und finale Annotation durch die PI betreut wurden. Mit einem Skript wurden csv-Tabellen erstellt, die die Tokens (fortlaufende Wortformen, inklusive Interpunktion) und POS-Tags in einem Dem automatischen wie händischen Tagging lag das Tagset des STTS (Schiller et al. 1999) zugrunde. Wo angezeigt, wurden im Projekt zusätzliche Regeln für der Handhabung des STTS-Manuals vereinbart und dokumentiert. Hierzu gehört auch u.a. die systematische Einbindung eines korpusbasierten Wörterbuchs (http://www.duden.de/) bei Eigennamen und Fremdwörtern. Das Tagging wurde in drei Phasen durchgeführt. Primäres Ziel des Taggings war es, die Genauigkeit von In Phase II wurden dieselben Coder, Tagger und dieselbe Software eingesetzt, ebenso wie die in Phase 1 erarbeiteten Tagging-Guidelines. Anders als in der ersten Phase wurden jedoch nicht ganze Sätze, sondern jeweils einhundert Token pro POS-Kategorie aus dem DTA extrahiert. Dadurch wurde eine Ungleichverteilung der einzelnen POS-Kategorien, welche in natürlichen Sätzen gegeben ist (vgl. Evert 2006; Kilgarriff 2005), vermieden. Für fünfundfünfzig POS-Kategorien des STTS wurden jeweils n=100 Wort/Token-POS-Paare sortiert nach STTS-Tag annotiert. Dies entspricht einer Grundgesamtheit von N=5.500 Tokens. Jedes Token wurde von zwei Codern unabhängig annotiert. In der anschließenden Diskussionsphase wurden die strittigen Fälle besprochen und finale Annotationen erarbeitet. Zu diesem Zweck wurden Statistiken für die Tags (über Coder und Tagger) analysiert und Nichtübereinstimmung der vergebenen Tags identifiziert. Für die statistische Evaluation wurde die Interrater-Reliabilität als Die Ergebnisse in Tabelle 1 basieren für Phase I auf den finalen Annotationen und für Phase II zum momentanen Zeitpunkt auf etwa der Hälfte der finalen Annotation. Sie zeigen für Tabelle 1: *Es handelt sich nicht um den Mittelwert von Phase I und II sondern um eine gewichtete Statistik, die die unterschiedlichen Stichprobengrößen (zum Zeitpunkt der Rechnung) einbezieht. Die Übereinstimmung zwischen Codern vor der Diskussion und Referenzstandard ist hingegen vergleichsweise hoch, auch wenn sie in der zweiten Tagging-Phase etwas abfällt ( Obwohl die Gesamtergebnisse noch ausstehen, könnte der Unterschied zwischen den Phasen tentativ damit erklärt werden, dass Phase II mehr problematische Tags annotiert, die eine niedrigere Distribution haben und im per-Satz-Tagging seltener auftreten. Für die einzelnen POS-Kategorien variiert die Genauigkeit zwischen 0% und 100%, wobei Tabelle 2: In den Gruppendiskussionen konnten Probleme identifiziert werden, die vornehmlich bei den Taggern lagen (z.B. bei Abkürzungen, Relativpronomen). Es traten aber auch Fälle auf, in denen die STTS-Guideline nicht präzise genug ist (z.B. bei Vergleichspartikeln, Possessivpronomen, Indefinitpronomina). Dabei war die Analyse der Disagreements eine produktive Heuristik, um (computer-)linguistisch und literarturwissenschaftlich interessante Fälle aufzuwerfen. So scheint gerade in literarischen Fällen eine Ambiguität (etwa zwischen Adjektiv und Verb bei Partizipien) geradezu intentional. Öhnlich Insgesamt zeigt unsere praktische Taggerkritik, dass auch eine scheinbar gelöste NLP-Aufgabe wie die Wortartenauszeichnung kein Solitär ist, auf den geistes- und literaturwissenschaftliche Projekte ohne genauere Prüfung bauen sollten. Unsere Ergebnisse zeigen trotz der hochqualitativen Vorverarbeitung des DTA eine Fehlerrate von ca. 9% an, die allerdings stark nach POS-Tag variiert. Die diachrone wie synchrone Heterogenität des literarischen Diskurses führt generische POS-Tagger bislang fast zwangsläufig an ihre Grenzen, durch historische Sprachformen, aber auch die Vielfalt der Gattungen, Erzähltechniken und kreative Lexik und Syntax. Zukünftig bieten sich hier wohl zwei Wege an: zum einen die fortlaufende Verbesserung von generischen Tools, zum anderen gerade aber auch die Feinabstimmung der Tools für spezifische Anwendungen, mit flexibel ansprechbaren Tagging- und Sprachmodellen. So haben wir unsere Annotation an",de,vorliegend Beitrag leisten automatisch Auszeichnung Wortarten Part -- literarisch Text früh Jahrhundert rein intellektuell Reflektion hinaus Schritt empirisch Evaluation deutsch Textarchivs dta Akademie Wissenschaft praktisch Verbesserung vorlegen Perspektive digital Literaturstilistik distant Reading wortarter interessant merkmal Verteilung wichtig Indikator dimension autorstil Gattung Register biber Conrad pos vergleichsweise scheinbar Valide bestimmen gelten Computerlinguistik Problem automatisch Wortartenannotation lösen deutsch durchschnittlich Erkennungsgenauigkeit liegen Giesbrecht evern scheinen nahe liegen Korpora komfortabel annotieren annotiert Korpora bedienen dta genauer Blick zeigen Korpora Geisteswissenschaft historisch literarisch sprachlich Varietät abweichen Sprachmodelle verfügbar Tagger zugrunde liegen zeitungtexen Gegenwart deutsch frei verfügbar Goldstandard derzeit Tiger Korpus wörtern Frankfurter Rundschau Brants et Nichtstandardvarietäte sinken Genauigkeit rapide scheibel et teilweise Aussage Annotationsgenauigkeit mangels Referenzstandard betreffen Dta bislang systematisch evaluieren insgesamt Goldstandard historisch literarisch narrativ Text Zeitraum entfernen Beitrag leisten wichtig Schritt Ergebnis Einschätzung Qualität Verbesserung Annotationstool vorlegen ausgehend Ziel Korpus literarisch Moderne Kolimo validen pos auszuzeichnen Stichprobe dta manuell nachannotieren Methode verbinden händisch Analyse folgend Ziel verfolgen evaluation durchführen randomisiert Stichprobe dta aufgrund unser Forschungsinteresse narrativ Text Publikationsdatum beschränken wobei sowohl Fiktional Text berücksichtigen ausschlaggebend metadaten Erstveröffentlichung Gattung Header dta Grundgesamtheit Dta entnommen Stichprobe umfas Token Händisch annotiert Token umfas tokens Stichprobe tokenisiert normalisiert Form Dta übernehmen dta Taggervergleich nutzen Tagging studentisch Hilfskräfte besorgt wobei iterativ Analyse final Annotation pi betreuen Skript erstellen Token fortlaufend Wortform inklusive interpunktion automatisch Händischen Tagging liegen Tagset Stt Schiller et zugrunde anzeigen Projekt zusätzlich Regel Handhabung vereinbaren dokumentieren hierzu gehören systematisch Einbindung korpusbasiert Wörterbuch eigennam fremdwörtern Tagging Phase durchführen primäres Ziel Tagging Genauigkeit Phase ii coder Tagger Software einsetzen Phase erarbeitet Phase Sätz jeweils einhundert token pro Dta extrahiern Ungleichverteilung einzeln natürlich satz geben evern kilgarriff vermeiden fünfundfünfzig Stt jeweils Wort sortieren annotiert entsprechen Grundgesamtheit Token jeder token codern unabhängig annotieren anschließend Diskussionsphase strittig Fall besprechen final Annotation erarbeiten zweck Statistik tags cod Tagger analysieren Nichtübereinstimmung vergeben tags identifizieren statistisch Evaluation Ergebnis Tabelle basieren Phase i final annotation Phase ii momentan Zeitpunkt Hälfte final Annotation zeigen Tabelle handeln Mittelwert Phase i ii gewichtet Statistik unterschiedlich stichprobengröß Zeitpunkt Rechnung einbeziehen Übereinstimmung codern Diskussion Referenzstandard hingegen vergleichsweise abfallen obwohl Gesamtergebnisse ausstehen Unterschied phas Tentativ erklären Phase ii problematisch Tags annotiert niedrig Distribution selten auftreten einzeln variieren Genauigkeit wobei Tabell Gruppendiskussion Problem identifizieren vornehmlich Tagger liegen Abkürzung relativpronomen treten Fall präzise vergleichspartikeln possessivpronomen Indefinitpronomina Analyse Disagreement produktiv Heuristik literarturwissenschaftlich interessant Fall aufzuwerfen scheinen literarisch Fall Ambiguität adjektiv verb partizipien geradezu intentional öhnlich insgesamt zeigen praktisch Taggerkritik scheinbar gelöst Wortartenauszeichnung Solitär literaturwissenschaftlich Projekt genau Prüfung bauen Ergebnis zeigen trotz hochqualitativ Vorverarbeitung dta Fehlerrate stark variieren Diachron Synchrone Heterogenität literarisch Diskurse führen generisch bislang fast zwangsläufig Grenze historisch sprachform Vielfalt gattung erzähltechniken kreativ Lexik Syntax zukünftig bieten Weg fortlaufend Verbesserung generisch Tools Feinabstimmung Tools spezifisch Anwendung flexibel ansprechbaren Sprachmodelle Annotation,"[('dta', 0.3809567173047987), ('phase', 0.2531423602168746), ('token', 0.17264927169557565), ('tagger', 0.15294032128327892), ('final', 0.14955290911901253), ('stichprobe', 0.14955290911901253), ('tagging', 0.14098178600815303), ('ii', 0.12491781561823256), ('tags', 0.10771173903996846), ('referenzstandard', 0.1050706185014585)]"
2018,DHd2018,DIMPEL_Friedrich_Michael_Die_guten_ins_Töpfchen__Zur_Anwendb.xml,"Die guten ins Töpfchen: Zur Anwendbarkeit von Burrows"" Delta bei kurzen mittelhochdeutschen Texten nebst eines Attributionstests zu Konrads ""Halber Birne""","Friedrich Michael Dimpel (FAU Erlangen-Nürnberg, Deutschland, Germanistik, und TU Darmstadt, Computerphilologie und Mediävistik)","Stilometrie; Autorschaftsattribution, Quantitative Verfahren","Stilistische Analyse, Visualisierung, Sprache, Literatur, Methoden","Die Anwendbarkeit von Burrows"" Delta (Burrows 2002) als Autorschaftstest für das Deutsche ist in Validierungstestreihen wiederholt eindrucksvoll demonstriert worden (Büttner et alia 2017, Eder 2013a/b, Evert et alia 2015. Evert et alia 2016); auch im Mittelhochdeutschen ist Delta anwendbar (Dimpel 2016/2018). Die Stabilität des Verfahrens wurde in Noise-Tests belegt: Wenn man etwa 12% aller Wörter durch Fremdmaterial austauscht, sinkt die Erkennungsquote kaum (Dimpel 2017a/2018). Bei nicht-normalisierten mittelhochdeutschen Texten steigt die Erkennungsquote in einem Validierungstest von 80% auf 91%, wenn man die bei Evert et alia (2016) entwickelte Methode der Z-Wert-Begrenzung mit einem von mir zusammengestellten Normalisierungswörterbuch kombiniert (Dimpel 2017a). Kontraintuitiv ist, dass nur die Kombination dieser Optimierungsverfahren zu einer Verbesserung um 11% führt, während in diesem Setting nur der Einsatz der Z-Wert-Begrenzung zu einer minimalen Verschlechterung führt; der Einsatz nur des Normalisierungswörterbuchs führt nur zu einer Verbesserung um 5,6%. Dieser Befund wird unter dem Stichwort ""Delta-Rätsel"" in einem Dariah-de-Working-Paper (Dimpel 2017b) ausführlich analysiert. Bei der Rätsel-Analyse wurde 'ein Serendipitätseffekt 'eine Möglichkeit entdeckt, wie man bei einem konkreten Vergleich von drei Texten die Wortformen identifizieren kann, die eine korrekte Autorschaftserkennung begünstigen oder behindern 'dazu im Weiteren. Beim Delta-Test berechnet man aus den Wortfrequenzen für ein Korpus jeweils die zugehörigen Z-Werte. Beim Vergleich von zwei Wortformen aus zwei Texten wird die Differenz der jeweiligen Z-Werte gebildet und der Betrag dieser Differenz genommen. Delta ist schließlich der Mittelwert der absoluten Z-Wert-Differenzen für alle Wortformen. Abb. 1. zeigt oben die Z-Werte der Handschrift M von Wolframs ""Parzival"" in einem Test, in dem sich im Vergleichskorpus neben Wolframs ""Willehalm"" noch weitere 19 Distraktortexte von anderen Autoren befinden (ausführlich zum Testverfahren Dimpel 2017b). Der ""Parzival"" soll dem Autor-Vergleichstext (Wolframs ""Willehalm"") zugeordnet werden und nicht etwa Konrads ""Partonopier"". Im oberen linken Viertel sind positive Z-Werte blau aufgetragen und nach der Höhe der Z-Werte sortiert. Ab der Stelle, an der die blauen Balken auf 0 zurückgehen, folgt rechts der Betrag der negativen Z-Werte (blau). Unten stehen (orange) die absoluten Z-Wert-Differenzen zwischen dem Ratetext und dem Autor-Vergleichstext (Differenzen der Z-Werte von Wolframs ""Parzival"" und Wolframs ""Willehalm""). Man könnte A) den Verdacht haben, dass Wortformen bei hohen blauen Balken ""gut"" sind, um einen Text von Distraktortexten zu unterscheiden, da hohe Z-Werte auf erhebliche Abweichung von den übrigen Korpusfrequenzen hindeuten. Man könnte auch B) den Verdacht haben, dass Wortformen bei hohen orangen Balken ""schlecht"" für die Autorerkennung sind: Unterschiede zwischen dem Ratetext und Autor-Vergleichstext (also Unterschiede von zwei Texten des gleichen Autors) sollten eher niedrig sein, damit die Erkennung funktioniert. Allerdings sind bei hohen blauen Balken relativ oft auch hohe orange Balken vorhanden 'auch in anderen Tests (Dimpel 2017b). Dieses Diagramm erlaubt also keine Aussage darüber, welche Wortformen gut für die Autorerkennung sind; hohe Z-Werte allein erlauben noch keine Aussage darüber, ob ein Wort hier gut geeignet ist, um einen Autor zu charakterisieren. Neu ist in Abb. 2 nur die obere Hälfte: Sie enthält Z-Wert-Differenzen des Ratetexts zum Distraktortext (""Partonopier""). Diese grauen Unterschiede sollten bei funktionierender Autorerkennung eher groß sein; gleichzeitig sollten die orangen Unterschiede der Texte vom gleichen Autor niedriger sein als die grauen. Dort, wo die grauen Balken genauso hoch sind wie die orangen, hilft das Wort nicht bei der Autorerkennung 'dies ist bei sehr hohen positiven Z-Werten der Fall. Sind die orangen Balken höher als die grauen, stört die Wortform die Autorerkennung: Die Differenzen zwischen Texten verschiedener Autoren müssen größer sein als die Differenzen zwischen Texten gleicher Autoren, wenn die Autorschaftserkennung funktioniert. Die Differenz zwischen orange und grau sei ""Level-2-Differenz"" genannt: ""Differenz aus der Z-Wert-Differenz zwischen Ratetext und Distraktortext einerseits und der Z-Wert-Differenz zwischen Ratetext und Autor-Vergleichstext andererseits"". Bei positiven Level-2-Differenzen ist eine Wortform vorteilhaft für die Autorerkennung 'mit Blick auf den einen untersuchten Distraktortext. Bei negativen Level-2-Differenzen ist die Wortform schlecht für die Autorerkennung. Über diese Differenz kann man ""gute"" und ""schlechte"" Wortformen einzeln identifizieren. Konrads Autorschaft wurde der ""Halben Birne"" trotz Selbstnennung im Epilog ( Die stilometrische Analyse ist in mehrfacher Hinsicht eine Herausforderung: Eine gattungsübergreifende Attribution ist mangels anderer Vergleichstexte nötig (nach Schöch 2014 wäre eine Gattungsmischung möglichst zu meiden). In Konrads Oevre herrscht eine Vielfalt an Themen, Frivoles wie in der ""Halben Birne"" ist eher selten 'auch im einzigen anderen Märentext Konrads: im ""Herzmäre"" bleibt die Liebe unerfüllt, es kommt zum doppelten Minnetod. Zudem ist die ""Halbe Birne"" recht kurz: sehr gute Quoten erreicht Delta ab 5.000 Wortformen in einer Bag-of-Words (vgl. Abb. 3 sowie Eder 2013a und Eder 2013b). Die ""Halbe Birne"" enthält jedoch nur 2.469 Wortformen. Wenn man nun die ""Birne"" gegen ein Konrad-Korpus testet, kann man entweder die Wörter mit hoher Level-2-Differenz, die einer Erkennung von Konrad entgegenstehen, aus der Liste der untersuchten Most-Frequent-Words (MFWs) streichen. Oder man kann eine Positivliste mit ""guten"" Wörtern bevorzugt verwenden 'Wörter mit hoher positiver Level-2-Differenz. Vorab wird das Verfahren validiert: In einer Ermittlungsgruppe (vier Konrad-Texte) werden ""gute"" und ""schlechte"" Wörter identifiziert. ""Gute Wörter"": Level-2-Differenzen >+2,31 in 6 von 7 Ermittlungsgruppen-Ratetexten, 304 items ""Schlechte Wörter"": Level-2-Differenzen <-1,2 in 2 von 7 Ermittlungsgruppen-Ratetexten, 174 items Im Attributionstest 1 wird die ""Halbe Birne"" als Autor-Vergleichstext verwendet, als Ratetexte werden die acht Konrad-Texte sowie das ""Herzmäre"" verwendet; im ""Herzmäre""-Test bleibt es bei acht Konrad-Ratetexten; das ""Herzmäre"" ist Autor-Vergleichstext. Hier erreicht das ""Herzmäre"" nur 4,5%, ein schlechter Wert, obwohl hier die Autorschaft nicht infrage gestellt wurde. Dagegen liegt die Erkennungsquote bei der ""Halben Birne"" auch ohne zusätzliche Wortlisten bereits über dem Zufallswert: Wenn ein Konrad-Text aus dem Ratekorpus nun nicht einem der 20 Texte von anderen Autoren zuordnet wird, sondern der ""Halben Birne"", dann stehen die Chancen dafür 1 zu 21. Wenn es also auf den Zufall zurückzuführen wäre, dass ein Text dem richtigen Autor zugeordnet wird, dann müsste die Erkennungsquote bei 5% liegen 'so beim ""Herzmäre"". 83,8% bei der ""Halben Birne"" sind ein ordentlicher Wert, wenn man bedenkt, dass nur kurze Bag-of-Words mit 2.000 Wortformen getestet werden können und dass gattungsübergreifend getestet wird. Beim Attributionstest 1 befand sich die ""Halbe Birne"" im Vergleichskorpus. Im Ratekorpus waren inklusive ""Herzmäre"" 9 Konrad-Texte. Nun werden umgekehrt ""Halbe Birne"" bzw. ""Herzmäre"" als Ratetexte verwendet. Ins Vergleichskorpus gebe ich zu den 20 Distraktortexten in separaten Tests jeweils einen Konrad-Text als Autor-Vergleichstext ins Vergleichskorpus. Attributionstest 2: Im Attributionstest 2 übersteigen die meisten Werte 86%. Es gibt lediglich zwei deutliche Ausreißer, an denen jeweils das ""Herzmäre"" beteiligt ist. Dieses Minneleid-und-Minnetod-Märe fügt sich nicht zur politischen Propagandadichtung ""Turnier von Nantes"". Auch zur ""Halben Birne"" passt das ""Herzmäre"" nicht: Dort geht es um eine Dame, die einen Ritter abweist, weil er beim Birnenverzehr keine Tischmanieren an den Tag legt. Die Dame schläft mit einem vermeintlich taubstummen Hofnarren, der sich jedoch später als der abgewiesene Birnen-Ritter entpuppt. Interessante Fehlattributionen (etwa ""Birne"" zu ""Häslein"" statt zum ""Herzmäre"") werden im Vortrag vorgestellt. Als Katharina Zeppezauer-Wachauer (Salzburg) mir einige Mären aus der Mittelhochdeutschen Begriffsdatenbank überlassen hat (vielen Dank dafür!), hat sie notiert: ""Vielleicht können Sie ja wirklich, wie Edith Feistner gefordert hat, ""Konrad seine Birne wiedergeben""!"" Auch wenn die Zahlen in beiden Attributionstests trotz der geringen Textlänge und trotz der Gattungsproblematik überraschend eindeutig sind, möchte ich bei einer vorsichtigen Interpretation bleiben. Zwar ist die Wahrscheinlichkeit sehr gering, dass die gefundene Nähe der ""Halben Birne"" zum Konrad-Korpus auf dem Zufall beruht. Allerdings wären ""Kontrollpeilungen"" (Eibl 2013) wünschenswert: Eine Attribution sollte nicht auf einem einzelnen Test mit einer Methode erfolgen, wünschenswert wären Bestätigungen mit anderen Methoden. Immerhin aber geht es hier nicht um eine blinde Attribution, sondern lediglich um Widerspruch gegen eine Athetese der Forschung. Eine Attribution stünde in Einklang mit Konrads Selbstnennung in fünf von sieben überlieferten Textzeugen. Zudem würde ich den Test gerne mit einem größeren Mären-Korpus wiederholen, in dem idealerweise längere Texte wären und mehr Texte, die näher an Konrads Schaffenszeit liegen. Dass die Birne nicht zu Kaufringer clustert, könnte auch dem zeitlichen Abstand geschuldet sein, der durch gemeinsame groteske oder frivole Inhaltselemente nicht überlagert wird. Wichtig ist mir auch das Verfahren: Bislang ist eine Feature-Eliminierung oder Feature-Selektion häufig auf dem Weg des maschinellen Lernens erfolgt (Büttner et alia 2016) 'mit dem Nachteil, dass der Weg der Kategorisierung teilweise im Dunklen bleibt. Ermittelt man ""gute"" oder ""schlechte"" Wörter via Level-2-Differenzen, so ist transparent, wie man zu den Parametern kommt und wie auf dieser Basis die weiteren Berechnungen erfolgen.",de,Anwendbarkeit burrow delta burrow autorschaftstest deutsch Validierungstestreih wiederholt eindrucksvoll demonstrieren Büttner et Alia ed b evern Et Alia everen et Alia mittelhochdeutsch Delta anwendbar dimpel Stabilität verfahren belegen Wörter Fremdmaterial austauscht sinken Erkennungsquote dimpel mittelhochdeutsch Text steigen Erkennungsquote Validierungstest evert et Alia entwickelt Methode zusammengestellt normalisierungswörterbuch kombinieren dimpel kontraintuitiv Kombination optimierungsverfahren Verbesserung führen Setting Einsatz minimal Verschlechterung führen Einsatz Normalisierungswörterbuch führen Verbesserung Befund Stichwort dimpel ausführlich analysieren Serendipitätseffekt Möglichkeit entdecken konkret Vergleich Text Wortform identifizieren korrekt Autorschaftserkennung begünstigen behindern berechnen Wortfrequenz Korpus jeweils zugehörig Vergleich Wortforme Text Differenz jeweilig bilden Betrag Differenz nehmen Delta schließlich Mittelwert absolut wortformen abb zeigen Handschrift m wolfram parzival Test Vergleichskorpus wolframs Willehalm Distraktortexte Autor befinden ausführlich testverfahr dimpel parzival wolfram Willehalm zuordnen Konrad Partonopier oberer linker Viertel positiv blau auftragen Höhe sortieren Stelle blau Balken zurückgehen folgen rechts Betrag negativ blau unten stehen Orange absolut Ratetext Differenz wolfram parzival wolframs Willehalm verdachen Wortforme hoch blau Balk Text Distraktortext unterscheiden hoch erheblich Abweichung übrig Korpusfrequenz hindeuen b verdachen Wortforme hoch Orang Balk Autorerkennung Unterschied Ratetext Unterschied Text gleich Autor eher niedrig Erkennung funktionieren hoch Blaue Balken relativ hoch Orange balken vorhanden Test dimpel Diagramm erlauben Aussage Wortform Autorerkennung hoch erlauben Aussage Wort geeignet Autor Charakterisieren neu abb oberer Hälfte enthalten Ratetexts Distraktortext Partonopier grau Unterschied funktionierend Autorerkennung eher gleichzeitig orang Unterschiede Text gleich Autor niedrig grauen grau Balke genauso orang helfen Wort Autorerkennung hoch positiv Fall Orangen Balke hoch Graue stören Wortform Autorerkennung Differenz Text verschieden Autor groß Differenz Text gleich Autor Autorschaftserkennung funktionieren Differenz Orange grau nennen Differenz Ratetext Distraktortext einerseits Ratetext andererseits positiv Wortform Vorteilhaft Autorerkennung Blick untersucht distraktortext negativ Wortform Autorerkennung Differenz schlecht Wortformen einzeln identifizieren Konrad Autorschaft halb Birn trotz Selbstnennung Epilog stilometrisch Analyse mehrfach Hinsicht Herausforderung gattungsübergreifend Attribution mangels anderer Vergleichstexte nötig schöch Gattungsmischung möglichst meiden Konrad oevre herrschen Vielfalt Thema Frivole halb birne eher selten einzig Märentext Konrad Herzmäre bleiben Liebe unerfüllen doppelt Minnetod zudem halb birn quoten erreichen Delta Wortforme abb ed ed halb birne enthalten Wortforme birn testen Wörter hoch Erkennung Konrad entgegenstehen Liste untersucht mfws streichen Positivlist gut wörtern bevorzugt verwend Wörter hoch Positiver vorab Verfahren validiern ermittlungsgruppe schlecht Wörter identifizieren Wörter item schlecht Wörter item Attributionstest halb birne verwenden ratetexte Herzmäre verwenden bleiben herzmär erreichen Herzmäre schlecht Wert obwohl Autorschaft Infrage stellen liegen Erkennungsquote halb birne zusätzlich Wortliste zufallswert Ratekorpus Text Autor zuordnen halb birne stehen Chance Zufall zurückführen Text richtig Autor zuordnen müsste Erkennungsquote liegen herzmär halb Birn ordentlich Wert bedenken kurz Wortforme testen gattungsübergreifend testen Attributionstest befinden halb birne Vergleichskorpus Ratekorpus inklusive herzmär umgekehrt halb birn herzmär Ratetexte verwenden Vergleichskorpus geben Distraktortext Separat Test jeweils Vergleichskorpus Attributionstest Attributionstest übersteigen meister wert lediglich deutlich Ausreißer jeweils Herzmäre beteiligen fügen politisch Propagandadichtung Turnier nantes halb birne passen Herzmäre dame Ritter abweist birnenverzehr Tischmanier legen dam schläft vermeintlich taubstumm hofnarren abgewiesen entpuppen interessant fehlattribution birn Häslein herzmär Vortrag vorstellen Katharina Salzburg mären mittelhochdeutsch Begriffsdatenbank überlassen notieren Edith Feistner fordern Konrad Birn Wiedergeben Zahl attributionstests trotz gering Textlänge trotz Gattungsproblematik überraschend eindeutig vorsichtig interpretation bleiben Wahrscheinlichkeit gering gefunden Nähe halb birne Zufall beruhen sein kontrollpeilung eibl wünschenswert Attribution einzeln Test Methode erfolgen wünschenswert sein bestätigungen Methode immerhin blind Attribution lediglich Widerspruch Athetese Forschung Attribution stehen einklang Konrad Selbstnennung überlieferten Textzeuge zudem Test gerne groß wiederholen idealerweise lang Text sein Text nah Konrad Schaffenszeit liegen Birn Kaufringer clusteren zeitlich Abstand schulden gemeinsam grotesk Frivole Inhaltselement überlageren wichtig Verfahren bislang häufig Weg maschinell lernen erfolgen büttner et Alia Nachteil Weg Kategorisierung teilweise dunkel bleiben ermitteln schlecht Wörter via transparent Parameter Basis Berechnunge erfolgen,"[('halb', 0.34009044290920787), ('birn', 0.23922340527775077), ('birne', 0.23922340527775077), ('autorerkennung', 0.22473163382273326), ('konrad', 0.2091142142769561), ('differenz', 0.17552809768062425), ('alia', 0.16052259558766663), ('herzmäre', 0.16052259558766663), ('dimpel', 0.15683566070771707), ('wortforme', 0.15181361878935581)]"
2018,DHd2018,PROISL_Thomas_Delta_vs__N_Gram_Tracing__Wie_robust_ist_die_A.xml,Delta vs. N-Gram-Tracing: Wie robust ist die Autorschaftsattribuierung?,"Thomas Proisl (Friedrich-Alexander-Universität Erlangen-Nürnberg, Deutschland); Stefan Evert (Friedrich-Alexander-Universität Erlangen-Nürnberg, Deutschland)",Autorschaftsattribuierung,"Stilistische Analyse, Sprache, Methoden, Text","Die Autorschaftsattribuierung, also die Zuweisung von Texten unbekannter oder umstrittener Autorschaft zu ihrem wahren Autor, hat vielfältige Anwendungen beispielsweise in der Literatur- und Geschichtswissenschaft oder der forensischen Sprachwissenschaft. Eine populäre Methode zur Autorschaftsattribuierung ist die Anwendung von Deltamaßen (Burrows 2002; Argamon 2008) wie zum Beispiel Cosine-Delta (Smith und Aldridge 2011). Deltamaße verwenden die n häufigsten Wörter im Korpus, standardisieren die Frequenzen auf z-Werte und wenden ein Abstandsmaß, im Fall von Cosine-Delta den Kosinusabstand, an. Typischerweise schließt sich die Anwendung eines (hierarchischen) Clusterverfahrens an, das Texte des selben Autors zusammengruppiert. Eine neue Methode zur Autorschafts-attribuierung ist das sogenannte N-Gram-Tracing (Grieve et al., in Begutachtung). Hierbei werden aus dem zu klassifizierenden Text alle Wort- oder Buchstaben-N-Gramme einer bestimmten Länge extrahiert. Der Text wird dann dem Autor zugewiesen, der im Vergleichskorpus die meisten dieser N-Grammtypen verwendet. Die Häufigkeit der N-Gramme spielt dabei keine Rolle, es geht nur darum, wie viele N-Gramme aus dem zu klassifizierenden Text auch im Vergleichskorpus auftauchen. Wenn Methoden zur Autorschaftsattribuierung angewandt werden sollen um tatsächlich eine strittige Autorschaftsfrage zu klären, ist es sehr wichtig die Zuverlässigkeit und Robustheit der Verfahren abschätzen zu können, schließlich gibt es eine ganze Reihe von Einflussfaktoren. Kritisch sind zum Beispiel die folgenden Fragen: Welchen Einfluss haben die Länge des zu klassifizierenden Textes und die Größe des Vergleichskorpus auf die Genauigkeit der Autorschaftsattribuierung? Gibt es für die beiden Verfahren eine Mindesttextlänge, die nicht unterschritten werden sollte? Wie stark werden die Verfahren durch autor- und werkspezifische Eigenheiten beeinflusst? Ist die Genauigkeit der Autorschaftsattribuierung robust in Bezug auf die Zusammensetzung des Vergleichskorpus oder kann die Auswahl der Autoren und Texte das Ergebnis beeinträchtigen? Um diese Fragen zumindest teilweise beantworten zu können, führen wir eine Reihe von Evaluationsexperimenten durch. Um die Ergebnisse des N-Gram-Tracings besser mit denen von Delta vergleichen zu können, führen wir auf den Deltaabständen zwischen den Texten kein Clustering sondern eine nearest-neighbor-Klassifikation durch, d.h. wir weisen den zu klassifizierenden Text dem Autor des Textes mit dem geringsten Abstand zu. Im Einzelnen handelt es sich um zwei Kürzungs- und zwei Samplingexperimente. Datengrundlage für die Kürzungsexperimente sind die deutschen, englischen und französischen Romankorpora, die unter anderem von Jannidis et al. (2015) und Evert et al. (2017) verwendet wurden. Jedes Korpus besteht aus je drei Romanen von 25 Autoren, also aus 75 Romanen. Für das erste Kürzungsexperiment wird die Größe des Vergleichskorpus stabil gehalten und nur der zu klassifizierende Text gekürzt. Für Delta wird zusätzlich die Anzahl der verwendeten häufigsten Wörter variiert. Im zweiten Kürzungsexperiment werden sowohl der zu klassifizierende Text als auch das Vergleichskorpus gekürzt. Über ein leave-one-out-Verfahren werden alle Texte im Korpus klassifiziert um die Genauigkeit der Verfahren zu ermitteln. Für die Samplingexperimente verwenden wir eine Sammlung von 1018 deutschen Romanen aus dem langen 19. Jahrhundert. Alle Texte wurden von Muttersprachlern verfasst. Für das erste Samplingexperiment ziehen wir 5000 zufällige Stichproben von 25 Autoren und je drei Romanen (die Zusammensetzung der einzelnen Stichproben ist also vergleichbar mit den oben erwähnten Romankorpora). Für das zweite Samplingexperiment beschränken wir uns auf die 25 Autoren, die in unserer Sammlung mit den meisten Romanen vertreten sind, und ziehen 5000 zufällige Stichproben von je drei Romanen pro Autor (also ebenfalls 25√ó3 Texte). Für jede Stichprobe ermitteln wir über ein leave-one-out-Verfahren die Genauigkeit der beiden Verfahren. Aus Platzgründen berichten wir an dieser Stelle nur knapp die Ergebnisse des ersten Kürzungsexperiments und des ersten Samplingexperiments und beschränken und dabei auf auf die deutschen Daten. Die Ergebnisse des ersten Kürzungsexperiments, in dem nur der zu klassifizierende Text gekürzt wird, sind in der folgenden Abbildung dargestellt:  Wir vergleichen Cosine-Delta auf Basis der 3000 häufigsten Wörter mit N-Gram-Tracing auf Basis von Wort-1-bis-3-Grammen, Zeichen-4-bis-10-Grammen und Zeichen-11-Grammen. Bis zu einer Textlänge von 5000 Tokens liefern alle N-Gram-Tracing-Varianten bessere Ergebnisse als Delta, für längere Texte funktionieren Delta und N-Gram-Tracing auf Wort-N-Grammen am besten. Bei weniger als 2000 Tokens brechen die Ergebnisse für Delta ein, bei weniger als 1000 Tokens auch die für N-Gram-Tracing. Die Ergebnisse des ersten Samplingexperiments zeigen, dass die Klassifikationsgenauigkeit bei beiden Verfahren großen Schwankungen unterworfen ist. Hier die Ergebnisse für Cosine-Delta:  Die Grafik zeigt, dass ab ca. den 1000 häufigsten Wörtern zwar im Mittel eine Klassifikationsgenauigkeit von rund 85% erreicht wird, allerdings mit enormen Schwankungen zwischen knapp über 60% und knapp unter 100%. Die Ergebnisse für N-Gram-Tracing auf Basis von Wort-N-Grammen sehen ähnlich aus:  Durch die Kombination von Wort-1- bis Wort-3-Grammen wird zwar eine mittlere Klassifikationsgenauigkeit von über 70% erreicht, aber auch hier mit enormen Schwankungen. Die Ergebnisse zeigen, dass N-Gram-Tracing auf kurzen Texten besser funktioniert als Cosine-Delta, allerdings werden für beide Verfahren längere Texte benötigt, als häufig verwendet werden. Die Wahl der Autoren im Vergleichskorpus und auch, wie das Poster zeigen wird, die Wahl der einzelnen Werke haben einen enormen und schwer vorhersehbaren Einfluss auf die Qualität der Autorschaftszuschreibung, deren Genauigkeit ohne weiteres um 20 Prozentpunkte schwanken kann. Im Licht dieser Erkenntnisse ist es durchaus fraglich, wie valide und generalisierbar bisherige Forschungsergebnisse auf dem Gebiet der Autorschaftsattribuierung sind.",de,Autorschaftsattribuierung Zuweisung Text unbekannt umstritten Autorschaft wahr Autor vielfältig Anwendung beispielsweise Geschichtswissenschaft forensisch Sprachwissenschaft populär Methode Autorschaftsattribuierung Anwendung Deltamaß burrows Argamon Smith aldridg Deltamaße verwenden n häufig Wörter korpus standardisieren Frequenz wenden Abstandsmaß Fall Kosinusabstand typischerweise schließen Anwendung hierarchisch Clusterverfahren Text selber Autor zusammengruppieren Methode sogenannter grieve et Begutachtung hierbei klassifizierend Text bestimmt Länge extrahiern Text Autor zuweisen Vergleichskorpus meister verwenden Häufigkeit spielen Rolle klassifizierend Text Vergleichskorpus auftauchen Methode Autorschaftsattribuierung anwenden tatsächlich strittig Autorschaftsfrage klären wichtig Zuverlässigkeit Robustheit Verfahren abschätzen schließlich Reihe Einflussfaktor kritisch folgend Frage einfluss Länge klassifizierend Text Größe Vergleichskorpus Genauigkeit Autorschaftsattribuierung Verfahren Mindesttextlänge unterschreiten stark Verfahren werkspezifisch eigenheiten beeinflussen Genauigkeit Autorschaftsattribuierung Robust Bezug Zusammensetzung Vergleichskorpus Auswahl Autor Text Ergebnis beeinträchtigen Frage zumindest teilweise beantworten fahren Reihe evaluationsexperimenten Ergebnis Delta vergleichen fahren deltaabständ Text Clustering weisen klassifizierend Text Autor Text geringster Abstand einzelner handeln Samplingexperiment datengrundlage Kürzungsexperimente deutsch englisch französisch Romankorpora Jannidis et everen Et verwenden jeder korpus bestehen Romane Autor romanen Kürzungsexperiment Größe Vergleichskorpus stabil halten klassifizierend Text kürzen Delta zusätzlich Anzahl verwendet häufig Wörter variieren Kürzungsexperiment sowohl klassifizierend Text Vergleichskorpus kürzen Text Korpus klassifizieren Genauigkeit Verfahren ermitteln Samplingexperimente verwenden Sammlung deutsch Roman lang Jahrhundert Text Muttersprachler verfassen Samplingexperiment ziehen zufällig Stichprobe Autor Romane Zusammensetzung einzeln Stichprobe vergleichbar erwähnt Romankorpora Samplingexperiment beschränken Autor Sammlung meister Roman vertreten zeihen zufällig Stichprobe Romane pro Autor ebenfalls Text Stichprobe ermitteln Genauigkeit Verfahren platzgründ berichten Stelle knapp Ergebnis Kürzungsexperiment Samplingexperiment Beschränke deutsch daten Ergebnis Kürzungsexperiment klassifizierend Text kürzen folgend Abbildung darstellen vergleichen Basis häufig Wörter Basis Textlänge Token liefern gut Ergebnis Delta lang Text funktionieren Delta Token brechen Ergebnis Delta Token Ergebnis Samplingexperiment zeigen Klassifikationsgenauigkeit Verfahren Schwankung unterwerfen Ergebnis Grafik zeigen häufig wörtern Klassifikationsgenauigkeit erreichen enorm Schwankung knapp knapp Ergebnis Basis sehen ähnlich Kombination mittlerer Klassifikationsgenauigkeit erreichen enorm Schwankung Ergebnis zeigen kurz Text funktionieren Verfahren lang Text benötigen häufig verwenden Wahl Autor Vergleichskorpus Poster zeigen Wahl einzeln Werk enorm schwer vorhersehbar einfluss Qualität Autorschaftszuschreibung Genauigkeit prozentpunkt schwanken Licht Erkenntnis fraglich valide generalisierbar bisherig Forschungsergebnisse Gebiet Autorschaftsattribuierung,"[('klassifizierend', 0.3387669204699196), ('autorschaftsattribuierung', 0.29037164611707394), ('vergleichskorpus', 0.2784326086037478), ('samplingexperiment', 0.2597918057148746), ('kürzungsexperiment', 0.20783344457189964), ('autor', 0.2059054540881715), ('text', 0.19716144938176267), ('delta', 0.1848881298314689), ('genauigkeit', 0.17126045995514882), ('kürzen', 0.15587508342892473)]"
2018,DHd2018,HENNY_KRAHMER_Ulrike_Alternative_Gattungstheorien__Das_Proto.xml,Alternative Gattungstheorien: Das Prototypenmodell am Beispiel hispanoamerikanischer Romane,"Ulrike Henny-Krahmer (Universität Würzburg, Deutschland); Katrin Betz (Universität Würzburg, Deutschland); Daniel Schlör (Universität Würzburg, Deutschland); Andreas Hotho (Universität Würzburg, Deutschland)","Literarische Gattungen, Prototypentheorie, Topic Modeling, MFW, Hispanoamerikanischer Roman","Inhaltsanalyse, Modellierung, Stilistische Analyse, Literatur, Methoden, Text","Die Definition von Gattungen sowohl im Sinne allgemeiner Gattungskonzepte als auch konkreter einzelner Gattungen ist ein altes und nach wie vor zentrales Problem der Literaturwissenschaft und wird immer noch debattiert (Kayser 1956: 330-387, Zymner 2003). Allgemein können Gattungsbegriffe als Sammelbegriffe verstanden werden, deren Aufgabe es ist, zu beschreiben, in welcher Hinsicht Texte zu Textgruppen zusammengefasst werden können. Oft ist dabei auf das Konzept von Gattungen als logischen Klassen zurückgegriffen worden, auch in vielen Untersuchungen zu literarischen Gattungen im Bereich der Digital Humanities, obwohl in der literaturwissenschaftlichen Gattungstheorie bereits seit den 60er-Jahren andere Vorschläge für das Verständnis von Gattungskategorien gemacht worden sind. Im Sinne der ""Kritik der digitalen Vernunft"" ist das Ziel dieses Beitrags, die Problematik der Gattungsbeschreibung auf der theoretischen Basis alternativer Gattungstheorien und mit Hilfe informatischer Mittel aus einer neuen Perspektive zu betrachten. Dazu wird exemplarisch die Anwendbarkeit des Prototypenmodells als Gattungskonzept für digitale gattungsstilistische Studien überprüft. Als Testkorpus dient eine Sammlung von Texten aus der hispanoamerikanischen Romanliteratur des 19. Jahrhunderts. Viele Gattungstheorien beruhen auf der Grundannahme, dass sich konkrete Texte anhand von hinreichenden und notwendigen Attributen eindeutig disjunkten Klassen zuordnen lassen und oft auch, dass sich für Gattungen eine Taxonomie entwickeln lässt (vgl. Zymner 2003: 102-104). Allerdings weisen nicht alle Texte einer bestimmten Gattung gemeinsame Merkmale auf, noch sind die Beziehungen zwischen den einzelnen Gattungen und Texten statisch. Zudem gibt es die Vorstellung von Werken, die initiale, prägende Wirkung haben (z. B. In den Digital Humanities gibt es bereits einige Untersuchungen zu literarischen Gattungen, bei denen jedoch üblicherweise die Annahme zugrunde liegt, dass Gattungen als Klassen im logischen Sinn zu verstehen sind (Calvo Tello et al. 2017, Hettinger el al. 2016a, Hettinger et al. 2016b, Schöch et al. 2016, Schöch 2015, Schöch 2013). Im vorliegenden Beitrag wird exemplarisch dargestellt, inwiefern es möglich ist, das Prototypenmodell zu verwenden, um Einsichten in die Anordnung maschinell gruppierter Texte zu gewinnen, die über den klassischen Ansatz der Klassifikation nicht möglich wären. Der Beitrag leistet damit zum einen, einen konkreten Vorschlag für die Formalisierung des Prototypen-Ansatzes für gattungsstilistische Untersuchungen zu machen. Zum anderen zielt er darauf, durch die Berücksichtigung der internen Strukturierung von Gattungskategorien eine bessere Anbindung der computergestützten Verfahren an literaturgeschichtliche Forschungsergebnisse zu ermöglichen.  Für die Zuordnung der Romane zu den verschiedenen Untergattungen wurde einschlägige Sekundärliteratur ausgewertet. Abb. 1 und 2 zeigen die Verteilung der Romane über die Zeit nach Untergattungen sowie nach Ländern. Bestandteil des Korpus sind neben hispanoamerikanischen Romanen auch fünf Texte aus Spanien sowie je ein Text aus England und Frankreich (in spanischer Übersetzung), welche aufgrund ihres Prototypen-Status einbezogen wurden. Das Korpus umfasst insgesamt 2,3 Mio. Token. Beim historischen Roman und beim Liebesroman handelt es sich bei den Prototyp-Texten um Übersetzungen der ursprünglich auf englisch und französisch verfassten Romane, die auch in Hispanoamerika als Vorbilder für diese Romantypen eingeschätzt werden. Kontroverser wird diskutiert, welche Texte Einfluss auf die kostumbristischen Romane ausgeübt haben. Auf der einen Seite wird der mexikanische Autor Lizardi als Pionier genannt, auf der anderen Seite werden Romane spanischer Autoren (Caballero, Alarcón, Valera, Pereda) angeführt (Calderón 2005). Eine Prototypenanalyse könnte hier Argumente für eine der beiden Thesen liefern. Im Falle des Gaucho-Romans und des Antisklaverei-Romans sind die gesetzten Prototypen als Höhepunkte der jeweiligen Gattung zu verstehen, da die weiteren diesen Untergattungen zugeordneten Romane im Korpus entweder als Vorstufen oder Nachfolger der besonders repräsentativen Gattungsvertreter beschrieben worden sind (Lichtblau 1959: 121-135, Rivas 1990).  Das Topic Modeling (vgl. Blei 2012) wurde mit MALLET  Die zweite Dokument-Repräsentation wurde auf Basis der 10.000 häufigsten Wörter (MFW) in den Roman-Volltexten erstellt (nicht lemmatisiert, gewichtet mit TF-IDF, maximale Dokument-Frequenz von 90 %). Anhand dieser beiden Repräsentationen kann überprüft werden, welche Merkmale zentral für die Abbildung von Gattungsunterschieden zwischen den Texten sind. Die exemplarische Anwendung des Prototypenmodells auf die hispanoamerikanischen Romane verschiedener Untergattungen hat gezeigt, dass Ansätze zur Modellierung literarischer Gattungen, die über das Prinzip logischer Klassen hinausgehen, informatisch umgesetzt werden können. Fragen wie diejenige nach der Prototypizität einzelner Texte, Gattungsmischungen und nach differenzierten Nähe- und Distanzverhältnissen lassen sich erst auf dieser Grundlage angehen. Literaturgeschichtliche Aussagen zu Gattungszugehörigkeiten und Gattungsentwicklungen mit prototypensemantischem Bezug (wie hier die Entwicklung des Gaucho-Romans oder die Frage nach den die hispanoamerikanische Tradition prägenden kostumbristischen Romanen) können so hinsichtlich relevanter Textmerkmale genauer untersucht werden. Künftig sollen weitere theoretische Gattungsmodelle, insbesondere das Prinzip der Familienähnlichkeit, auf ihre Anwendbarkeit für gattungstilitische Untersuchungen hin getestet werden. Außerdem soll geprüft werden, welche Ergebnisse andere Textrepräsentationen als Topic-Modelle und MFW liefern, z.B. Word Embeddings. Zu diskutieren bleibt, wie offene Kategorisierungsmodelle evaluiert werden können.",de,Definition Gattung sowohl Sinn allgemein gattungskonzepte konkret einzeln Gattung alt zentral Problem Literaturwissenschaft debattieren Kayser zymn allgemein gattungsbegriffe Sammelbegriff verstehen Aufgabe beschreiben Hinsicht Text Textgruppe zusammengefassen Konzept Gattung logisch klasse zurückgegriffen Untersuchung literarisch Gattung Bereich Digital humaniteisen obwohl literaturwissenschaftlich Gattungstheorie Vorschlag Verständnis Gattungskategorie Sinn Kritik digital Vernunft Ziel Beitrag Problematik Gattungsbeschreibung theoretisch Basis alternativ Gattungstheorien Hilfe informatisch Perspektive betrachten exemplarisch Anwendbarkeit Prototypenmodell Gattungskonzept digital gattungsstilistisch Studie überprüfen Testkorpus dienen Sammlung Text hispanoamerikanisch Romanliteratur Jahrhundert Gattungstheorien beruhen Grundannahme konkret Text anhand Hinreichend notwendig attribut eindeutig disjunkten Klasse Zuordn lassen gattung Taxonomie entwickeln lässt zymn weisen Text bestimmt Gattung gemeinsam Merkmal Beziehung einzeln gattung Text statisch zudem Vorstellung Werk initial prägend Wirkung Digital Humanitie Untersuchung literarisch Gattung üblicherweise Annahme zugrunde liegen Gattunge Klasse logisch Sinn verstehen calvo Tello et hettinger El hettinger et schöch et schöch schöch vorliegend Beitrag exemplarisch darstellen inwiefern prototypenmodell verwenden Einsicht Anordnung maschinell Gruppierter Text gewinnen klassisch Ansatz Klassifikation sein Beitrag leisten konkret Vorschlag Formalisierung gattungsstilistisch Untersuchung zielen Berücksichtigung intern Strukturierung Gattungskategorien gut Anbindung computergestützt Verfahren literaturgeschichtlich Forschungsergebnisse ermöglichen Zuordnung Roman verschieden Untergattung einschlägig Sekundärliteratur auswerten abb zeigen Verteilung Roman untergattungen Land Bestandteil Korpus hispanoamerikanisch Roman Text Spanien Text England Frankreich spanisch Übersetzung aufgrund einbeziehen Korpus umfassen insgesamt Mio token historisch Roman Liebesroman handeln übersetzungen ursprünglich englisch französisch verfasst Roman Hispanoamerika Vorbilder Romantype einschätzen kontroverser diskutieren Text einfluss kostumbristisch Roman ausüben Seite mexikanisch Autor Lizardi Pionier nennen Seite roman spanisch Autor Caballero Alarcón Valera Pereda anführen Calderón Prototypenanalyse Argument Thes liefern Fall gesetzt Prototyp Höhepunkt jeweilig Gattung verstehen untergattung zugeordneten Roman Korpus vorstufen Nachfolger repräsentativ Gattungsvertreter beschreiben lichtblau rivas Topic Modeling blei Mallet Basis häufig Wörter mfw erstellen lemmatisieren wichten maximal anhand Repräsentation überprüfen merkmal Zentral Abbildung Gattungsunterschied Text exemplarisch Anwendung Prototypenmodell hispanoamerikanisch Roman verschieden untergattungen zeigen Ansatz Modellierung literarisch Gattung Prinzip logisch Klasse hinausgehen informatisch umsetzen fragen Prototypizität einzeln Text Gattungsmischung differenziert distanzverhältnisse lassen Grundlage angehen literaturgeschichtlich Aussage gattungszugehörigkeiten Gattungsentwicklung prototypensemantisch Bezug Entwicklung Frage hispanoamerikanisch Tradition prägend kostumbristisch Roman hinsichtlich Relevanter Textmerkmal genau untersuchen künftig theoretisch gattungsmodelle insbesondere Prinzip Familienähnlichkeit Anwendbarkeit gattungstilitisch Untersuchung testen prüfen Ergebnis Textrepräsentation mfw liefern Word Embedding diskutieren bleiben offen Kategorisierungsmodelle evaluieren,"[('gattung', 0.3021123600040416), ('roman', 0.2703311200455681), ('hispanoamerikanisch', 0.2339575279215335), ('prototypenmodell', 0.1883869327700675), ('klasse', 0.15129098719692852), ('text', 0.1379542748074735), ('logisch', 0.1371075874223851), ('kostumbristisch', 0.12559128851337834), ('gattungsstilistisch', 0.12559128851337834), ('gattungstheorien', 0.12559128851337834)]"
2018,DHd2018,LAUBROCK_Jochen_Computationale_Beschreibung_visuellen_Materi.xml,Computationale Beschreibung visuellen Materials am Beispiel des Graphic Narrative Corpus,"Jochen Laubrock (Universität Potsdam, Deutschland); David Dubray (Universität Potsdam, Deutschland); André Krügel (Universität Potsdam, Deutschland)","Neuronale Netze, Bildverarbeitung, Graphic Novels","Modellierung, Annotieren, Stilistische Analyse, Bilder, Text","Die digitale Revolution in den Geisteswissenschaften hat diesen eine Reihe neuer Methoden eröffnet. Durch die heute verfügbaren großen Datenmengen und intelligenten Algorithmen haben sich zwar einige bisher offengeliebene geisteswissenschaftliche Kernfragen beantworten lassen, jedoch wird mancherorts eine Methodenfokussiertheit und Theoriemangel kritisiert (Gumbrecht, 2014). Ob die Digitalen Geisteswissenschaften zu einer darüber hinausgehenden tiefergreifenden Veränderung der geisteswissenschaftlichen Erkenntnis führen werden, bleibt abzuwarten; derzeit scheint es, als werde das Potenzial der neuen methodischen Zugänge erst noch ausgelotet. In anderen Wissenschaften hat aber die Verfügbarkeit computationaler Modelle und der damit einhergehende Zwang, implizite Annahmen zu explizieren und Theorien formal testbar zu machen, signifikant zur Theoriebildung und -prüfung beigetragen (cf. Myung & Pitt, 2002; Lewandowsky & Farrell, 2011). Deshalb besteht die begründete Hoffnung, dass computationale Modellierung auch die Geisteswissenschaften bereichern wird. Ein Großteil der Forschung in den digitalen Geisteswissenschaften beschäftigt sich mit Text. Es gibt hier eine fruchtbare interdisziplinäre Zusammenarbeit von Literaturwissenschaften und Computerlinguistik; im Umfeld des ""Distant Reading"" sind umfangreiche Werkzeuge entstanden, mit denen sich etwa stilometrische Analysen oder Topic Modeling computergestützt vornehmen lassen (Blei, 2012; Juola, 2006). Auch netzwerkanalytische Methoden aus der theoretischen Physik und computationalen Soziologie haben hier interessante neue Perspektiven eröffnet (Schich et al., 2014). Dagegen ist die digitale Analyse visuellen Materials noch relativ wenig entwickelt oder standardisiert, obwohl dieses für Disziplinen wie z.B. Kunstgeschichte oder Archäologie von zentralem Interesse ist. In den letzten Jahren wurden durch Entwicklungen im Bereich der Convolutional Neural Networks (CNN) die Möglichkeiten automatisierter Bildanalyse revolutioniert. Während in klassischen Ansätzen der maschinellen Bildverarbeitung ein hohes Ausmaß an Expertenwissen notwendig war, um Merkmale zu definieren, mit denen sich das Material sinnvoll beschreiben ließ (""engineered features""), lernen CNNs die Merkmale durch Fehlerrückführung (Backpropagation) selbst. Convolutional Neural Networks sind eine besondere Klasse künstlicher neuronaler Netze, die sich durch eine 2D-Anordnung der Neuronen, innerhalb einer Schicht geteilte Gewichte und lokale Konnektivität auszeichnen. Sie eignen sich insbesondere für die Analyse von Bildmaterial. Die Netze sind typischerweise auf einer großen Anzahl von Fotos in Objektklassifikationsaufgaben trainiert worden, dabei bilden sich auf verschiedenen Ebenen der CNNs Repräsentationen aus, die denen im menschlichen visuellen System ähnlich sind. Neuronen auf niedrigen Ebenen des Netzwerks haben oft eine Filterantwort, die relativ einfache Merkmale kodiert, vergleichbar z.B. mit Kantendetektoren im frühen visuellen Kortex, während Neuronen auf höheren Ebenen recht komplexe Merkmale kodieren können, z.B. Texturen oder Teile von Gesichtern. Da diese Merkmale relativ generisch sind, ist zu erwarten, dass Transfer auf neuartiges Material gelingt. Es sind heute einige derart vortrainierte Netzwerke verfügbar, die sich mit relativ wenig Aufwand an neues Material anpassen lassen. Der Vergleich der Gewichte für verschiedene Materialtypen erlaubt dann auch Rückschlüsse über deren Unterschiede. Generalisieren die auf Fotos vortrainierten Netzwerke auch auf zeichnerisches Material? Wir berichten von Experimenten, in denen wir das Material des Graphic Narrative Corpus (GNC, Dunst et al., 2017) mit CNNs beschreiben. Der Graphic Narrative Corpus repräsentiert das erste digitale Korpus von englischsprachigen Graphic Novels mit derzeit 130 Titeln. Die ersten Kapitel dieser Werke werden von menschlichen Kodierern annotiert, dabei werden u.a. die Identität und der Ort zentraler Charaktere, Orte von Panels, Sprechblasen und Textboxen (Captions) und Onomatopeia sowie der Text selbst notiert. Außerdem werden Blickbewegungen von Lesern erhoben (Eye-Tracking), um Aufschluss über die Aufmerksamkeitsverteilung auf Seite der Rezipienten zu erhalten. Die Beschreibung des GNC mit CNNs hat verschiedene Ziele. Erstens erhoffen wir uns Aufschluss über stilistische Unterschiede zwischen Werken und Genres, z.B. mittels Berechnung von Distanzmaßen basierend auf den Modellparametern. Allgemeiner könnte so der Weg zu einer visuellen Stilometrie aufgezeigt werden, die auch für inhaltliche Bereiche außerhalb der Graphic Novels relevant ist, etwa im Sinne einer computationalen Kunstgeschichte (Saleh & Elgammal, 2015; Manovich, 2015). Zweitens ermöglicht die Beschreibung mit Hilfe der Merkmale tiefer CNNs durch sogenannte Region Proposal Networks (Girshick et al., 2013) die Detektion von Objektklassen. Beispielsweise könnten sich Sprechblasen oder handelnde Charaktere lokalisieren lassen. Wenn Klassen von Objekten automatisiert lokalisiert werden können, erleichtert dies die Arbeit der Annotatoren sehr. Die Ergebnisse können also zurück in das Annotationswerkzeug fließen, um eine Teilautomatisierung zu ermöglichen. Drittens ist aus kognitionspsychologischer Perspektive interessant, welche Merkmale die Aufmerksamkeit auf sich ziehen. Die Korrelation der Netzwerkbeschreibung mit den Blickbewegungsdaten ermöglicht eine Modellierung der Aufmerksamkeitssteuerung auf einem deutlich höheren Auflösungsgrad als die subjektive Beschreibung. Für die Modellierung des Materials nutzen wir die Architektur VGG der Visual Geometry Group in Oxford (Simonyan & Zisserman, 2014), insbesondere VGG-16 und VGG-19. Diese Wahl ist motiviert durch die Einfachheit der Architektur, die die Interpretation der Gewichte erleichtert. Das zugrundeliegende Netzwerk lässt sich aber prinzipiell austauschen; andere Architekturen wie ResNet (He et al., 2015) oder Inception (Szegedy et al., 2015) sind denkbar und sollten ähnlich gute Ergebnisse liefern. Für die Vorhersage der Aufmerksamkeitsverteilung der Leser nutzen wir die Architektur Deep Gaze II (Kümmer et al., 2016). Deep Gaze II ist ein neuronales Netz, das auf VGG-19 aufsetzt und die Antwort einiger dessen Schichten nutzt, um ""empirische Salienz"" vorherzusagen. Empirische Salienz ist operationalisiert durch Messung von Mauspositionen beim Aufdecken eines verschwommenen Bildes bzw. Messung von Blickbewegungsdaten beim Betrachten von Fotos natürlicher Szenen. Die Fotos sind andere, als die für das Training von VGG-19 benutzten. Man beachte, dass sowohl VGG-19 als auch Deep Gaze II auf Fotos trainiert wurden, also nie Graphic Novels gesehen haben. Da sie jedoch Merkmale und Gewichte herausgebildet haben, die für die Interpretation (von Bildern) der menschlichen Umwelt nützlich sind, kann man vermuten, dass sie sich auch für die Analyse von Zeichnungen eignen. Zwar sind Zeichnungen Abstraktionen, haben aber als solche einen Bezug zur visuellen (Photo-)Realität. Die Ergebnisse zeigen, dass sich mit Hilfe von Neuronen auf höheren Ebenen der tiefen CNNs recht gut bestimmte Klassen von Objekten lokalisieren lassen. Beispielsweise eignen sich einige Kombinationen von Merkmalen zuverlässig als Sprechblasendetektoren (Abb. 1). Dies ist insofern bemerkenswert, als die Detektion von Sprechblasen sich für klassische Ansätzen der maschinellen Bildbverarbeitung als schwieriges Problem dargestellt hat (Rigaud et al., 2013). Auch für die Erkennung gezeichneter Gesichter eignen sich CNNs, allerdings ist hier ein Training auf Ansichten in verschiedenen Perspektiven (Frontal, Profil) notwendig. Und schließlich lässt sich die empirische Fixationsverteilung mit Deep Gaze II insgesamt sehr überzeugend reproduzieren (Abb. 2). Die CNN-Features kodieren also aufmerksamkeitsrelevante Merkmale. Insgesamt eigenen sich auf Fotos trainierte CNNs schon ohne spezifisches weiteres Training recht gut zur Beschreibung gezeichneten Materials in Graphic Novels. Die ""objektive"" Beschreibung eröffnet vielfältige Anwendungen. Einerseits kann, wie oben skizziert, die Annotation visuellen Materials durch Nutzung von vorgeschlagener Regionen deutlich erleichtert werden, etwa vergleichbar mit dem bei verbalen Material durch Verwendung von Optical Character Recognition (OCR) ermöglichten √úbergang von kompletter Transkription zum Korrekturlesen. Hier soll angemerkt werden, dass vielversprechende CNN-basierte Ansätze zur Textlokalisation (Sudholt & Fink, 2016) und OCR existieren (Lee & Osindero, 2016). Andererseits sind durch das Vorliegen visueller Merkmale (Features) vielfältige stilometrische Anwendungen denkbar. Zum Beispiel lassen sich aufgrund der Merkmale √Ñhnlichkeiten verschiedener Zeichner und Künstler berechnen und durch Gruppierung (Clustering) im Merkmalsraum auch Stile definieren. Auch die weitergehende Exploration der Repräsentation auf verschiendenen Schichten des Netzwerks scheint eine vielversprechende Aufgabe weiterer Forschung. Beispielsweise könnte der Vergleich der Antworten auf fotographische versus zeichnerisch abstrahierte Abbilder von Exemplaren einer Kategorie Hinweise auf das Wesen der Abstraktion geben, oder es lassen sich visuelle Merkmale identifzieren, die in besonderem Ausmaß die Aufmerksamkeitszuwendung im Leseprozess und bei der Rezeption von Zeichnungen leiten. Wir haben beispielhaft aufgezeigt, wie sich Werkzeuge der mathematisch-computationalen Modellierung eignen, um grafisches Material zu analysieren und zu beschreiben. Die Hoffnung ist, dass eine visuelle Stilometrie die Digitalen Geisteswissenschaften im Bereich visuellen Materials in ähnlicher Art und Weise bereichert wie computerlinguistische Ansätze im Bereich der Textanalyse. Digitale Analysen liefern mächtige neue Werkzeuge, die mittel- bis längerfristig auch eine neue Theoriebildung fördern könnten.",de,digital Revolution geisteswissenschaften Reihe neu Methode eröffnen verfügbar datenmenger intelligenten algorithmen offengelieben geisteswissenschaftlich Kernfrage beantworten lassen mancherorts Methodenfokussiertheit Theoriemangel kritisieren Gumbrecht digital geisteswissenschaften hinausgehend tiefergreifend Veränderung geisteswissenschaftlich Erkenntnis führen bleiben abwarten derzeit scheinen Potenzial methodisch zugängen ausloten Wissenschaft Verfügbarkeit computationaler Modell einhergehend Zwang implizit annehmen explizieren Theorie formal testbar signifikant Theoriebildung beitragen cf Myung Pitt Lewandowsky farrell bestehen begründet Hoffnung computational Modellierung geisteswissenschaft Bereichern Großteil Forschung digital geisteswissenschaften beschäftigen Text fruchtbar interdisziplinär Zusammenarbeit Literaturwissenschaft Computerlinguistik Umfeld distant Reading umfangreich werkzeuge entstehen stilometrisch Analyse Topic Modeling Computergestützt vornehmen lassen blei Juola netzwerkanalytisch Methode theoretisch Physik computational Soziologie interessant Perspektive eröffnen schich et digital Analyse visuellen Material relativ entwickeln standardisieren obwohl disziplin kunstgeschicht Archäologie Zentralem Interesse letzter Entwicklung Bereich Convolutional Neural Network cnn Möglichkeit automatisierter Bildanalyse revolutionieren klassisch Ansatz maschinell Bildverarbeitung hoch Ausmaß Expertenwisse notwendig merkmal definieren Material sinnvoll beschreiben lassen engineered features lernen Cnn merkmal Fehlerrückführung Backpropagation Convolutional Neural Network besonderer Klasse künstlich Neuronaler Netz neuron innerhalb Schicht geteilt Gewicht lokal Konnektivität auszeichnen eignen insbesondere Analyse Bildmaterial Netz typischerweise Anzahl Foto objektklassifikationsaufgaben trainieren bilden verschieden Ebene cnns repräsentatione menschlich visuell System ähnlich Neuron niedrig Ebene Netzwerk Filterantwort relativ einfach Merkmal kodieren vergleichbar Kantendetektor früh visuell Kortex Neuron hoch eben komplex merkmal Kodier texturen Teil gesichtern merkmal relativ generisch erwarten Transfer neuartig Material gelingen derart vortrainiert netzwerke verfügbar relativ Aufwand neu Material anpassen lassen Vergleich Gewichte verschieden Materialtype erlauben rückschluß Unterschied generalisieren fotos vortrainiert netzwerk zeichnerisch Material berichten experimenten Material graphic Narrative Corpus Gnc Dunst et Cnn beschreiben Graphic Narrative Corpus repräsentieren digital Korpus englischsprachig Graphic Novels derzeit Titel Kapitel Werk menschlich Kodierer annotiert Identität Ort zentral charaktere Ort Panel Sprechblase textboxen Captions Onomatopeia Text notieren Blickbewegung Leser erheben aufschluss Aufmerksamkeitsverteilung Seite Rezipient erhalten Beschreibung gnc Cnn verschieden Ziel erstens erhoffen aufschluss stilistisch Unterschied Werk genre mittels Berechnung Distanzmaß basierend modellparametern allgemein Weg visuell Stilometrie aufzeigen inhaltlich Bereich außerhalb Graphic Novels relevant Sinn computational Kunstgeschichte Saleh elgammal manovich zweitens ermöglichen Beschreibung Hilfe Merkmal tief Cnn sogenannter Region Proposal Network Girshick et Detektion objektklassen beispielsweise können Sprechblase handelnd Charaktere lokalisieren lassen klasse Objekt automatisiert lokalisiern erleichtern Arbeit annotatoren Ergebnis Annotationswerkzeug fließen Teilautomatisierung ermöglichen drittens kognitionspsychologisch Perspektive interessant merkmal Aufmerksamkeit ziehen Korrelation Netzwerkbeschreibung Blickbewegungsdat ermöglichen Modellierung Aufmerksamkeitssteuerung deutlich hoch Auflösungsgrad subjektiv Beschreibung Modellierung Material nutzen Architektur vgg Visual Geometry Group Oxford Simonyan Zisserman insbesondere Wahl motivieren Einfachheit Architektur Interpretation Gewichte erleichtern zugrundeliegend Netzwerk lässen prinzipiell austauschen architekturen resnen he et Inception Szegedy et denkbar ähnlich Ergebnis liefern Vorhersage Aufmerksamkeitsverteilung Leser nutzen Architektur Deep gaze ii Kümmer et Deep gaze ii neuronal Netz aufsetzen Antwort Schicht nutzen empirisch Salienz vorherzusagen empirisch Salienz Operationalisiert Messung Mausposition Aufdecken verschwommen Bild Messung Blickbewegungsdate Betrachten fotos natürlich Szene Foto Training benutzn beachen sowohl Deep gaze ii Foto trainieren Graphic Novels sehen merkmal Gewichte herausgebilden Interpretation bildern menschlich Umwelt nützlich vermuten Analyse Zeichnung eignen Zeichnung abstraktion Bezug visuell Ergebnis zeigen Hilfe Neuron hoch Ebene tief Cnn bestimmt Klasse objekt lokalisieren lassen beispielsweise eignen Kombination Merkmale zuverlässig Sprechblasendetektoren abb insofern bemerkenswert Detektion Sprechblase klassisch Ansätz maschinell Bildbverarbeitung schwierig Problem darstellen rigaud et Erkennung Gezeichneter gesicht eignen Cnn training Ansicht verschieden Perspektive frontal Profil notwendig schließlich lässen empirisch Fixationsverteilung Deep Gaze ii insgesamt überzeugend reproduzieren abb kodieren aufmerksamkeitsrelevant Merkmal insgesamt Foto Trainierte Cnns spezifisch Training Beschreibung gezeichnet Material Graphic Novels objektiv Beschreibung eröffnen vielfältig anwendungen einerseits skizzieren Annotation visuell Material Nutzung vorgeschlagen Region deutlich erleichtern vergleichbar verbal Material Verwendung Optical character Recognition ocr ermöglicht Kompletter Transkription Korrekturlesen anmerken vielversprechend Ansatz Textlokalisation sudholen fink ocr existieren lee Osindero andererseits vorliegen visueller merkmal features vielfältig stilometrisch Anwendung denkbar lassen aufgrund merkmal verschieden zeichn Künstler berechnen Gruppierung Clustering Merkmalsraum Stil definieren weitergehend Exploration Repräsentation verschienden Schicht Netzwerk scheinen vielversprechend Aufgabe weit Forschung beispielsweise Vergleich Antwort Fotographisch versus zeichnerisch abstrahiert Abbilder Exemplar Kategorie Hinweis Wesen Abstraktion geben lassen visuell merkmal identifzieren besonderer Ausmaß Aufmerksamkeitszuwendung Leseprozess Rezeption Zeichnung leiten beispielhaft aufzeigen Werkzeug Modellierung eignen grafisch Material analysieren beschreiben Hoffnung visuell Stilometrie digital geisteswissenschaften Bereich visuell Material ähnlich Art Weise bereichern computerlinguistisch Ansatz Bereich Textanalyse digital analysen liefern mächtig Werkzeug längerfristig Theoriebildung fördern können,"[('merkmal', 0.24986065354846007), ('material', 0.24986065354846007), ('cnn', 0.23984815325632755), ('graphic', 0.17828269431697819), ('visuell', 0.16490642696773553), ('foto', 0.1552570456054365), ('gaze', 0.1552570456054365), ('neuron', 0.14461016768045745), ('novels', 0.11885512954465213), ('zeichnung', 0.11644278420407736)]"
2018,DHd2018,DU_Keli_Sentimentanalyse_in_unstrukturierten_Texten__am_Bsp_.xml,Sentimentanalyse in unstrukturierten Texten (am Bsp. literaturgeschichtlicher Rezeptionsanalyse),"Katja Mellmann (Universität Göttingen, Deutschland); Keli Du (Universität Göttingen, Deutschland)","Sentimentanalyse, unstrukturierte Texte, literaturgeschichtliche Rezeptionsanalyse","Inhaltsanalyse, Methoden, Forschungsprozess, Text","Die fortschreitende Retrodigitalisierung von Kulturzeitschriften und anderen Publikationsformen mit literaturkritischen Inhalten eröffnet der literaturgeschichtlichen Rezeptionsanalyse die Möglichkeit, mit historisch repräsentativen Korpora zu arbeiten. Dabei stellt sich jedoch das Problem, dass insbesondere Zeitschriftendigitalisate in der Regel nicht als edierte Texte von standardisierter Qualität vorliegen, sondern mit ""schmutzigen Texten"", also Texten mit fehlerhafter OCR und ohne linguistische Strukturierung gearbeitet werden muss. Wir wollen im Rahmen des Themas ""Kritik der digitalen Vernunft"" einen konstruktiven Umgang mit diesem Problem vorstellen. Wir unterscheiden dazu grundsätzlich zwei Zielperspektiven: Die erste Perspektive ist auf qualitativ hochwertige Textkorpora angewiesen, um statistisch aussagekräftige Ergebnisse zu erzielen. Sie ist die Standardperspektive, wenn Korpusanalysen als Forschungsinstrument eingesetzt werden. Wir nehmen hingegen die zweite Perspektive ein: In ihr werden qualitativ minderwertige Textkorpora nicht als bedauerliche Abweichung vom eigentlich Gewünschten aufgefasst, sondern als Forschungsgegenstand eigener Art, der auch eine Methodik eigener Art erfordert. Diese Methodik hebt auf vorläufige Trenddarstellungen ab, die nicht als (noch unvollkommene) Vorstufe einer validen Korpusanalyse, sondern als eigenständige Heuristik zur Identifikation potentieller Ereignisse in einem diachronen Korpus aufgefasst werden. Die Methode soll sozusagen grobe Bewegungsprofile liefern, von denen aus anschließend wieder gezielte hermeneutische Tiefensondierungen unternommen werden können. Digitale und hermeneutische ""Vernunft"" stehen hier also in einem komplementären Verhältnis; nicht versucht wird, die eine durch die andere möglichst perfekt nachzubilden. Bei den angezielten Bewegungsprofilen handelt es sich im Rahmen unseres Forschungsprojekts Historische Rezeptionsanalyse (Mellmann/Willand 2013) rekonstruiert die Aufnahme literarischer Werke durch das originale zeitgenössische Publikum. Dazu zählen insbesondere (a) Inhaltsverständnis, (b) ästhetische Wertung und (c) Kontextualisierung mit außerliterarischen zeitgenössischen Wissensformationen. Wir befassen uns in unserer Studie ausschließlich mit der ästhetischen Wertung (b). Diese ist symptomatisch für einen umfassenden literaturgeschichtlichen Wandel in der zweiten Hälfte des 19. Jahrhunderts: Auf dem Übergang vom Bürgerlichen Realismus zur Klassischen Moderne verlieren ehemals reputierte Autoren noch während ihrer aktien Schaffenszeit ihren führenden Status; neue Stile aus dem Bereich des gesamteuropäischen Naturalismus und Östhetizismus gewinnen an Reputation. Für einzelne Autoren und insbesondere für poetologische Programmatiken wurde dies bereits vielfach gezeigt. Was noch aussteht, ist eine Einschätzung, wie repräsentativ die in Einzelstudien ermittelten Entwicklungen für die Gesamtentwicklung sind, insbesondere unter Einschluss auch der wenig erforschten nichtkanonischen Literatur. Abhilfe schaffen könnte hier die Analyse eines großen Korpus repräsentativer Literaturzeitschriften, die diachrone Wertungsprofile zu symptomatischen Autorengruppen liefert. Langfristiges Ziel ist eine Analyse von sieben repräsentativen Zeitschriften über einen Erscheinungszeitraum von ca. 1860 bis 1900: ""Die Grenzboten"", ""Die Gegenwart"", ""Deutsche Rundschau"", ""Nord und Süd"", ""Blätter für literarische Unterhaltung"", ""Westermanns Monatshefte"" und ""Magazin für Literatur"". Als Volltext verfügbar ist derzeit nur die Zeitschrift ""Die Grenzboten"". Sie dient uns als erster Anwendungsfall nach den überwachten Optimierungsläufen an einem auf der Basis einer Anthologie (Kreuzer 2006, Bd. I und II) erstellten Testkorpus. Literaturkritisches Schrifttum im ausgehenden 19. Jahrhundert ist ein außergewöhnlich stark rhetorisiertes Textgenre, das durch einen hohen Grad an Ironie, Intertextualität, Euphemismus und Understatement besondere Herausforderungen an die Methode der digitalen Sentimentanalyse stellt, zumal in unstrukturierten Texten, die keine Berücksichtigung von grammatischen Komplexitäten (wie z.B. doppelter Verneinung, Konjunktiven, indirekter Rede) zulassen. Die Optimierung der Sentimentwortliste ist deshalb weniger auf eine Verfeinerung als auf eine Vergröberung hin ausgelegt. Die Korpusanalyse soll vor allem eklatante Veränderungen identifizieren. Der Volltext des Testkorpus wurde durch NLTK Punkt Sentence Tokenizer Der Grundwert jedes Sentimentworts wurde mit 1 angesetzt. Die Polarität eines Satzes wurde in zwei Schritten festgelegt: Zuerst wurde das Vorkommen der positiven und negativen Sentimentwörter im Satz gezählt. Anschließend wurde die ""Aggregation function"" (Abb. 1) für die Berechnung des Sentiment-Werts des Satzes verwendet: "" Ist der Sentiment-Wert größer als 0, gilt der Satz als positiv; kleiner als 0 gilt als negativ; ist der Wert gleich 0 (z.B., weil kein Sentimentwort im Satz auftaucht), gilt der Satz als neutral. Die im Satz ermittelten Sentimentwörter wurden in die Ergebnisdarstellung übernommen, um die digitale Analyse anschließend manuell überprüfen und die Sentimentwortliste optimieren zu können. Wörter, die sich als überwiegend dysfunktional erweisen, werden von der Sentimentwortliste gelöscht, fehlende Sentimentwörter werden ergänzt. Im ersten Testlauf wurden nur ca. 47.4% der Sätze richtig erkannt (Tab. 1). Unsere Wortliste konnte vor allem die negativen Sätze schwer identifizieren. Ca. 77% der positiven Sätze wurden richtig identifiziert. Aber auch der Anteil der fälschlich als positiv klassifizierten Sätze war sehr hoch. Außerdem war der Sentiment-Wert von vielen als neutral eingestuften Sätzen nicht gleich 0.  In einem zweiten Testlauf haben wir eine automatische Klassifikation ausprobiert. Dabei wurden die Anzahl der Sentimentwörter und der Sentiment-Wert eines Satzes als Feature verwendet. Im Verhältnis 80% zu 20% wurden die Daten in einen Trainings- und einen Testdatensatz aufgespalten. Es wurde ein Support Vector Machine (SVM) Modell trainiert; die Evaluation erfolgte als 10-fache Kreuzvalidierung (Cross-Validation). Dadurch verbesserte sich das Ergebnis um ca. 10%: Die Trefferquote der Klassifikation lag bei 57% (+/- 7%). Für einen dritten Testlauf wurde die Sentimentwortliste bearbeitet und die Textsnippets wurden einem zweiten manuellen Rating mit mehr als nur 3 Kategorien unterzogen. Insbesondere sollte zwischen tatsächlich neutralen Sätzen (z.B. ""X wurde 1826 in Berlin geboren."" = 0) und Sätzen mit (einander ausgleichenden) positiven und negativen Bewertungen (z.B. ""Trotz dieser erheblichen Schwächen ist X ein Werk gelungen, das ‚Ä¶"" = 0,###) unterschieden werden. Auch Problemfälle (wie z.B. erwartbare Artefakte durch Zitation oder Ironie) wurden separiert, um die Analyseergebnisse gesondert evaluieren zu können. Von den 1687 eindeutig positiven, negativen oder neutralen Sätzen wurden 65,6% richtig erkannt (Tab. 2).  In unserer Präsentation werden wir die ausführliche Ergebnisevaluation des dritten Testlaufs vorstellen und die sich stellenden Probleme im Hinblick auf die eingangs dargestellte Zielsetzung diskutieren. Außerdem soll ein erster Probelauf über das inzwischen provisorisch erstellte Satzkorpus aus den ""Grenzboten"" präsentiert werden, der die angezielte Methodik der diachronen Trenddarstellung illustriert.",de,fortschreitend Retrodigitalisierung Kulturzeitschrift Publikationsform literaturkritisch Inhalt eröffnen literaturgeschichtlich Rezeptionsanalyse Möglichkeit historisch repräsentativ Korpora arbeiten stellen Problem insbesondere zeitschriftendigitalisat Regel ediert Text standardisiert Qualität vorliegen schmutzig Text Text Fehlerhafter Ocr linguistisch Strukturierung arbeiten Rahmen Thema Kritik digital Vernunft konstruktiv Umgang Problem vorstellen unterscheiden grundsätzlich Zielperspektiv Perspektive qualitativ hochwertig Textkorpora angewiesen statistisch aussagekräftig Ergebnis erzielen Standardperspektive Korpusanalyse Forschungsinstrument einsetzen nehmen hingegen Perspektive qualitativ minderwertig Textkorpora bedauerlich Abweichung eigentlich gewünschten aufgefassen Forschungsgegenstand Art Methodik Art erfordern Methodik heben vorläufig Trenddarstellunge unvollkommene vorstufen valid Korpusanalyse eigenständig Heuristik Identifikation potentiell Ereignis diachron Korpus aufgefasst Methode sozusagen grob Bewegungsprofil liefern anschließend gezielt hermeneutisch Tiefensondierung unternehmen digital hermeneutisch Vernunft stehen komplementär Verhältnis versuchen möglichst perfekt nachzubilden angezielt Bewegungsprofile handeln Rahmen unser Forschungsprojekt historisch Rezeptionsanalyse Mellmann Willand rekonstruieren Aufnahme literarisch Werk original zeitgenössisch Publikum zählen insbesondere Inhaltsverständnis b ästhetisch Wertung c Kontextualisierung außerliterarisch zeitgenössisch wissensformationen befassen Studie ausschließlich ästhetisch Wertung b symptomatisch umfassend literaturgeschichtlich Wandel Hälfte Jahrhundert Übergang bürgerlich Realismus klassisch modern verlieren ehemals reputiert Autor Aktie Schaffenszeit führend Status Stil Bereich gesamteuropäisch naturalismus östhetizismus gewinnen Reputation einzeln Autor insbesondere poetologisch Programmatike vielfach zeigen ausstehen Einschätzung repräsentativ Einzelstudien ermittelt Entwicklung Gesamtentwicklung insbesondere einschluss erforscht nichtkanonisch Literatur Abhilfe schaffen Analyse Korpus repräsentativ literaturzeitschrifen Diachron Wertungsprofil symptomatisch Autorengrupp liefern langfristig Ziel Analyse repräsentativ Zeitschrift Erscheinungszeitraum grenzboen Gegenwart deutsch Rundschau Nord süd blätt literarisch Unterhaltung westermann monatsheft Magazin Literatur Volltext verfügbar derzeit Zeitschrift grenzboen dienen Anwendungsfall überwacht Optimierungsläufen Basis Anthologie Kreuzer bd i ii erstellt Testkorpus literaturkritisch Schrifttum ausgehend Jahrhundert außergewöhnlich stark rhetorisiert Textgenre hoch Grad Ironie Intertextualität Euphemismus understatement besonderer Herausforderung Methode digital Sentimentanalyse stellen zumal unstrukturiert Text Berücksichtigung grammatisch komplexitäten doppelt Verneinung konjunktiv indirekt Rede zulassen Optimierung sentimentwortli Verfeinerung Vergröberung auslegen Korpusanalyse eklatant Veränderung identifizieren Volltext Testkorpus nltk Punkt Sentence Tokenizer grundwert jeder Sentimentwort ansetzen Polarität Satz Schritt festlegen vorkommen positiv negativ Sentimentwörter Satz zählen anschließend Aggregation Function abb Berechnung Satz verwenden groß gelten Satz positiv gelten negativ Wert Sentimentwort Satz auftauchen gelten Satz Neutral Satz ermittelt Sentimentwörter Ergebnisdarstellung übernehmen digital Analyse anschließend manuell überprüfen sentimentwortli optimieren Wörter überwiegend dysfunktional erweisen sentimentwortli löschen fehlend Sentimentwörter ergänzen Testlauf Sätz erkennen tab Wortliste negativ Sätz schwer identifizieren positiv Sätz identifizieren Anteil fälschlich positiv klassifiziert Sätz neutral eingestuft Satz Testlauf automatisch Klassifikation ausprobieren Anzahl Sentimentwörter satzes Feature verwenden Verhältnis daten Testdatensatz aufgespalten Support vector Machine svm Modell trainieren Evaluation erfolgen Kreuzvalidierung verbessern Ergebnis Trefferquote Klassifikation liegen testlauf sentimentwortli bearbeiten textsnippets manuell Rating kategori unterziehen insbesondere tatsächlich neutral Satz x Berlin gebären Satz ausgleichend positiv negativ Bewertung trotz erheblich Schwäche x Werk gelingen unterscheiden problemfäll erwartbar artefakte Zitation Ironie separieren Analyseergebnisse gesondert evaluieren eindeutig positiv negativ neutral Sätze erkennen tab Präsentation ausführlich Ergebnisevaluation Testlauf vorstellen stellend Problem Hinblick eingangs dargestellt Zielsetzung diskutieren Probelauf inzwischen provisorisch erstellt Satzkorpus grenzboten präsentieren angezielt Methodik diachron Trenddarstellung illustrieren,"[('satz', 0.29288727144627374), ('sentimentwortli', 0.206916808167329), ('sentimentwörter', 0.206916808167329), ('testlauf', 0.17485067928669779), ('positiv', 0.1718421852688329), ('neutral', 0.15059358920392482), ('negativ', 0.14815932540411703), ('repräsentativ', 0.13881854692866158), ('sätz', 0.13202394389990532), ('korpusanalyse', 0.11571055645086115)]"
2018,DHd2018,SCHLUPKOTHEN_Frederik_Zwischen_Polysemie_und_Formalisierung_.xml,"Zwischen Polysemie und Formalisierung: Mehrstufige Modellierung komplexer intertextueller Relationen als Annäherung an ein ""literarisches"" Semantic Web","Julia Nantke (Bergische Universität Wuppertal, Deutschland); Frederik Schlupkothen (Bergische Universität Wuppertal, Deutschland)","Literaturwissenschaft, Intertextualität, Semantic Web, Situationstheorie","Beziehungsanalyse, Modellierung, Annotieren, Kontextsetzung, Theoretisierung, Text","Die Modellierung textueller und transbiblionomer Relationen mithilfe von Semantic Web-Technologien bildet mittlerweile eines der zentralen Forschungsfelder der Digital Humanities. Die Struktur und Funktionsweise literarischer Texte erfordern in Bezug auf die formale Beschreibung, Erklärung und Kategorisierung von semantischen Strukturen ein besonders differenziertes Vorgehen: Interne und externe textuelle Beziehungen bestehen in Form komplexer, häufig ambiger Zeichenrelationen, die plurale, sich auf verschiedenen Ebenen überlagernde Bedeutungsangebote stiften. Letztere können zudem nicht auf verortbare Ereignisse, stabile Relationen zwischen (bibliografischen, historischen) Artefakten oder ein konkretes argumentatives Ziel bezogen werden. Die Kategorisierung literarischer ""Daten"" erfolgt deshalb systematisch im Spannungsfeld zwischen dem Text als linguistisch-materieller Zeichenformation und deren interpretierender Auffassung. Die Aktualisierung bestimmter Codes hängt hierbei immer auch von kulturellen und historischen Faktoren, methodologischen Vorannahmen sowie der Kontingenz interpretativer Schlussfolgerung ab. Die daraus resultierende Bedingtheit und potentielle Vielfalt semantischer Zuschreibungen muss daher in einer Modellierung transparent abbildbar sein. Das Projekt möchte damit in zweierlei Hinsicht einen Beitrag zur methodologischen Reflexion leisten: Zum einen streben wir mit dieser zunächst auf das genauere Verständnis intertextueller Phänomene gerichteten stufenweisen Modellierung Der Beitrag diskutiert zum einen konkrete Probleme, welche sich im Spannungsfeld zwischen literarischer Polysemie, der Literaturwissenschaft inhärenter Perspektivenvielfalt und technischer Normierung ergeben, denn das Projekt dient nicht zuletzt auch der Reflexion der Möglichkeiten zur Formalisierung literaturwissenschaftlicher Erkenntnisse sowie dem Ausloten der Grenzen für den Einsatz formaler Beschreibungssprachen im Hinblick auf literaturwissenschaftliche Forschungsfragen. Zum anderen wird dargestellt, wie durch die spezifische Anlage der Modellierung auf verschiedenen Ebenen Desideraten bisheriger Ansätze begegnet und gleichzeitig ein Beitrag zum literaturtheoretisch fundierten Einsatz von DH-Methoden geleistet werden kann. Erste Ergebnisse werden in dem vorgeschlagenen Beitrag anhand konkreter Beispiele wie etwa des intertextuellen Netzes um Matthias Claudius"" In vielen bisherigen Projekten zur Erfassung literarischer Beziehungen schränkt die Konzentration auf automatisierbare Analysevorgänge das heuristische Potential digitaler Modellierung für die literaturwissenschaftliche Forschung in verschiedener Hinsicht ein: Erstens werden Modelle zur Beschreibung (innertextueller) literarischer Strukturen an stark Plot-lastigen Texten entwickelt, Zweitens erfolgt eine Modellierung intertextueller Beziehungen anhand einer ""historische[n] Positivtät von Kontext-Dokumenten"" (Wagner/Mehler/Biber 2016, S. 90 mit Bezug auf das Projekt Wikidition), deren Verknüpfungen auf linguistischer Ebene modelliert werden. Auf diese Weise werden zwar viele Probleme im Hinblick auf die Intersubjektivierbarkeit der Modellierung und die Differenzen zwischen verschiedenen Intertextualitätskonzepten vermieden, gleichzeitig wird aber aus literaturwissenschaftlicher Sicht die Aussagekraft der Ergebnisse stark eingeschränkt, indem der literaturwissenschaftlich relevante Fokus auf die Kategorisierung, Funktion und Wirkung von Intertextualität und die hierbei produktiven Schreibweisen und Markierungen (vgl. Kocher 2007, 179) zugunsten einer eher enzyklopädischen Perspektive verloren geht. Unser Projekt richtet sich hingegen weder auf die automatisierte Textanalyse noch beschränkt es sich auf die linguistische Ebene konkreter Wortäquivalenz. Vielmehr steht die Entwicklung eines formalisierten Vokabulars zur semantischen Repräsentation literarischer Intertextualität im Zentrum des Forschungsinteresses. Die Isolierung und formale Beschreibung intertextueller Phänomene dient der Beobachtung und Darstellung des Zusammenwirkens jener Schreibweisen und Markierungen bei der Erzeugung von Intertextualität. Die intertextuellen Beziehungen werden dabei also nicht deduktiv im Sinne einer Qualifizierung als Parodie, Kontrafaktur, Nachahmung, Hommage etc. modelliert, da derartige ""Gattungszuschreibungen"" in systematischen literaturwissenschaftlichen Untersuchungen zur Typisierung von Intertextualität oftmals den Blick für die spezifischen, bei der Erzeugung von Intertextualität wirksamen Faktoren verstellen. Die angeführten literarischen Beispiele dienen dann eher der selektiven Untermauerung der jeweils präfigurierten Typologie (vgl. einschlägig Broich/Pfister 1985; Genette 1993). Im Gegensatz dazu bildet die Modellierung von Beziehungen zwischen konkreten Schreibweisen den Ausgangspunkt unseres Projekts, welcher im Anschluss Schlussfolgerungen über die Relationen der verschiedenen Ebenen, auf denen intertextuelle Verknüpfungen stattfinden, sowie über die jeweils erzeugten Wirkungen ermöglichen soll. Für das angestrebte Forschungsziel stellt die mehrstufige Modellierung einen expliziten heuristischen Gewinn gegenüber bisherigen Ansätzen der Systematisierung dar, indem die umfassende induktive Erfassung und Beschreibung sowie die anschließend abgeleiteten strukturellen Konstanten nicht unverbunden nebeneinander stehen, sondern Mikro- und Makrostrukturen durch das Modell in ihrem Zusammenhang beobachtbar gemacht werden (s. Abbildung¬†1). Indem zunächst Einzeltextphänomene modelliert, darauf aufbauend deren Funktionen im Sinne ihres Beitrags zur ""Bedeutungskonstitution"" (Hempfer 1991, S.¬†19) erfasst und daraus übergreifende Kategorien abgeleitet werden, können zwei bislang getrennt voneinander verhandelte Bereiche der Untersuchung von Intertextualität in einer ganzheitlichen Modellierung verbunden werden: die ""Entwirrung"" des intertextuellen Gefüges eines einzelnen Textes (vgl. hierfür exemplarisch Bauer Lucca 2001; Dudzik 2017) sowie die übergeordnete Suche nach gemeinsamen Strukturen und Funktionsweisen intertextueller Verweise. Das vorgestellte Modell verknüpft also die in der Literaturwissenschaft seit den 1980er Jahren unternommenen Bestrebungen zur Typologisierung intertextueller Strukturen und Funktionsweisen mit einer umfassenden Detailuntersuchung literarischer Texte. Die Differenzierung in Phänomenbeschreibung und ‚Äëbewertung, welche der eingesetzte Formalismus unterstützt (s.¬†u.), sieht explizit die Modellierung funktionaler Überlagerungen und alternativer Forschungsmeinungen vor, sodass im Rahmen der Formalisierung sowohl der Multifunktionalität intertextueller Schreibweisen (vgl. Kocher 2010, S. 179) als auch der maßgeblich auf produktivem Dissens basierenden Dynamik des literaturwissenschaftlichen Diskurses Rechnung getragen wird. Als Ausgangspunkt zur formalen Beschreibung intertextueller Phänomene dient ein situationstheoretischer Ansatz, welcher die Brücke zwischen literaturwissenschaftlicher Analyse und technischer Modellierung darstellt. Die Situationstheorie bietet sich an, da sie im Sinne der angestrebten Beschreibung der Intertextualität einen mehrstufigen Formalismus zur Verfügung stellt, welcher Informationen und Informationsflüsse in Kontextabhängigkeit beschreibt: Basale Phänomene werden durch basale Informationseinheiten (sog. ""Infone"") beschrieben, welche wiederum ""Situationen"" als Phänomene einer höheren Ordnung aus der Perspektive eines oder mehrerer ""Agenten"" zu bilden erlauben. Somit kann formal unterschieden werden zwischen der Modellierung konkreter (sprachlicher, inhaltlicher, stilistischer) Texteigenschaften und -relationen (beschrieben als Infone) und der Klassifizierung der modellierten Informationseinheiten im Sinne ihrer Funktion sowie einer durch sie indizierten, Kontext-abhängigen Wirkung (beschrieben als Situationen). Im Gegensatz zu technischen Beschreibungssprachen (wie etwa RDF oder OWL) liefert die Situationstheorie einen Formalismus, welcher zunächst frei von umsetzungsspezifischen Einschränkungen (wie etwa festgelegten Datentypen oder Objekthierarchien) ist, die sich ungewollt perspektivierend auf die Modellierung auswirken können. Das Modell wird sukzessive unter Einbezug literarischer Texte verschiedener Textsorten und Publikationszeiträume getestet und weiterentwickelt. An diese sukzessive formale Strukturierung anknüpfend wird geprüft, inwieweit etablierte Beschreibungssprachen bei einer technischen Umsetzung des Modells Anwendung finden können. Insbesondere etablierte Sprachen aus dem Umfeld elektronischer Publikation sollen auf ihre Anwendbarkeit bzw. Möglichkeiten der Erweiterung hin betrachtet werden. Dies sind im Rahmen der durch das W3C beschriebene Standards für Verweisstrukturen Sprachen wie XPath, XLink oder XPointer (vgl. einschlägig Wilde/Lowe 2003), für semantische Auszeichnungen die Sprachen des Semantic Web wie RDF oder OWL. Dies schließt ‚Äì unabhängig von der konkreten Sprache ‚Äì die Berücksichtigung unterschiedlicher Auszeichnungskonzepte wie bspw. Standoff- in Abgrenzung zu Inline-Markup ein (vgl. Banski 2010).",de,Modellierung textuell Transbiblionomer relation Mithilfe Semantic bilden mittlerweile zentral forschungsfelder Digital Humanitie Struktur funktionsweise literarisch Text erfordern Bezug formal Beschreibung Erklärung Kategorisierung semantisch Struktur differenziert vorgehen Interne extern textuell Beziehung bestehen Form komplex häufig ambig zeichenrelationen plural verschieden Ebene überlagernd Bedeutungsangebote Stift letzterer zudem verortbar ereignis stabil relationen bibliografisch historisch artefakten konkret argumentativ Ziel beziehen Kategorisierung literarisch daten erfolgen systematisch Spannungsfeld Text Zeichenformation interpretierend Auffassung Aktualisierung bestimmt Codes hängen hierbei kulturell historisch Faktor methodologischen vorannahmen Kontingenz interpretativ Schlussfolgerung resultierend Bedingtheit potentiell Vielfalt semantisch Zuschreibung Modellierung Transparent abbildbar Projekt zweierlei Hinsicht Beitrag methodologisch Reflexion leisten streben genau Verständnis intertextuell phänomen gerichtet Stufenweise Modellierung Beitrag diskutieren konkret Problem Spannungsfeld literarisch Polysemie Literaturwissenschaft inhärent Perspektivenvielfalt technisch Normierung ergeben Projekt dienen zuletzt Reflexion Möglichkeit Formalisierung literaturwissenschaftlich erkenntnis Auslot Grenze Einsatz formal beschreibungssprachen Hinblick literaturwissenschaftlich forschungsfragen darstellen spezifisch Anlage Modellierung verschieden eben Desiderat bisherig Ansatz begegnen gleichzeitig Beitrag literaturtheoretisch fundiert Einsatz leisten Ergebnis vorgeschlagen Beitrag anhand konkret Beispiel intertextuell netzes Matthias Claudius bisherig Projekt Erfassung literarisch Beziehung schränken Konzentration automatisierbar Analysevorgang heuristisch Potential Digitaler Modellierung literaturwissenschaftlich Forschung verschieden Hinsicht erstens Modell Beschreibung innertextueller literarisch Struktur stark Text entwickeln zweitens erfolgen Modellierung intertextuell Beziehung anhand historisch n Positivtät Wagner Mehler biber Bezug Projekt Wikidition Verknüpfung linguistisch Ebene modellieren Weise Problem Hinblick Intersubjektivierbarkeit Modellierung Differenz verschieden intertextualitätskonzept vermeiden gleichzeitig literaturwissenschaftlich Sicht Aussagekraft Ergebnis stark einschränken literaturwissenschaftlich relevant Fokus Kategorisierung Funktion Wirkung Intertextualität hierbei produktiv Schreibweisen markierung koch zugunsten eher enzyklopädisch Perspektive verlieren Projekt richten hingegen weder automatisiert Textanalyse beschränken linguistisch Ebene konkret Wortäquivalenz vielmehr stehen Entwicklung formalisiert Vokabular semantisch Repräsentation literarisch Intertextualität Zentrum Forschungsinteresses Isolierung formal Beschreibung intertextuell phänomen dienen Beobachtung Darstellung Zusammenwirken Schreibweise markierungen Erzeugung Intertextualität intertextuell Beziehung deduktiv Sinn Qualifizierung Parodie Kontrafaktur Nachahmung Hommage modellieren derartig Gattungszuschreibunge systematisch literaturwissenschaftlich Untersuchung Typisierung Intertextualität oftmals Blick spezifisch Erzeugung Intertextualität wirksam Faktor verstellen angeführt literarisch Beispiel dienen eher selektiv Untermauerung jeweils präfiguriert Typologie einschlägig broich Pfister genetter Gegensatz bilden Modellierung Beziehung konkret Schreibweisen Ausgangspunkt unser Projekt Anschluss schlussfolgerungen Relation verschieden Ebene intertextuell Verknüpfung stattfinden jeweils erzeugt wirkung ermöglichen angestrebt Forschungsziel stellen mehrstufig Modellierung explizit heuristisch Gewinn bisherig Ansatz Systematisierung dar umfassend induktiv Erfassung Beschreibung anschließend abgeleitet strukturell Konstant Unverbunde nebeneinander stehen makrostrukturen Modell Zusammenhang beobachtbar einzeltextphänomen modellieren aufbauend Funktion Sinn beitrags Bedeutungskonstitution Hempfer erfassen übergreifend Kategori ableiten bislang trennen voneinander verhandelt Bereich Untersuchung Intertextualität ganzheitlich Modellierung verbinden Entwirrung intertextuell Gefüg einzeln Text hierfür exemplarisch Bauer Lucca dudzik übergeordnet Suche gemeinsam Struktur Funktionsweise intertextuell verweise vorgestellt Modell verknüpfen Literaturwissenschaft unternommen Bestrebung Typologisierung intertextuell Struktur Funktionsweise umfassend Detailuntersuchung literarisch Text Differenzierung Phänomenbeschreibung äëbewertung eingesetzt Formalismus unterstützen sehen explizit Modellierung funktional überlagerungen alternativ Forschungsmeinunge sodass Rahmen Formalisierung sowohl Multifunktionalität intertextuell Schreibweisen koch maßgeblich produktiv dissens basierend Dynamik literaturwissenschaftlich diskurs Rechnung tragen Ausgangspunkt formal Beschreibung Intertextueller Phänomen dienen situationstheoretisch Ansatz Brücke literaturwissenschaftlich Analyse technisch Modellierung darstellen Situationstheorie bieten Sinn angestrebt Beschreibung Intertextualität mehrstufig Formalismus Verfügung stellen Information informationsflüsse Kontextabhängigkeit beschreiben basal Phänomen basal informationseinheit Infone beschreiben wiederum situation Phänomen hoch Ordnung Perspektive mehrere Agent bilden erlauben somit formal unterscheiden Modellierung konkret sprachlich inhaltlich stilistisch texteigenschafen beschreiben Infone Klassifizierung modelliert informationseinheiten Sinn Funktion indizieren Wirkung beschreiben Situation Gegensatz technisch beschreibungssprachen rdf owl liefern Situationstheorie Formalismus frei umsetzungsspezifisch Einschränkung festgelegt Datentypen objekthierarchien ungewollt perspektivierend Modellierung auswirken Modell sukzessive Einbezug literarisch Text verschieden Textsort Publikationszeiträume testen weiterentwickeln sukzessive formal Strukturierung anknüpfend prüfen inwieweit etabliert beschreibungssprachen technisch Umsetzung Modell Anwendung finden insbesondere etabliert Sprache Umfeld elektronisch Publikation Anwendbarkeit Möglichkeit Erweiterung betrachten Rahmen beschrieben Standard verweisstrukturer sprechen xpath xlink xpoint einschlägig Wilde lowe semantisch Auszeichnung Sprache semantic Web rdf owl schließen äì unabhängig konkret Sprache Äì Berücksichtigung unterschiedlich auszeichnungskonzepte Abgrenzung Banski,"[('intertextuell', 0.3151081020733645), ('modellierung', 0.29115382518336574), ('intertextualität', 0.21601459302322729), ('formal', 0.1410858658789338), ('literaturwissenschaftlich', 0.1378920217879043), ('beschreibung', 0.13456271365778819), ('schreibweisen', 0.1328303275731868), ('konkret', 0.12432270168348089), ('beschreibungssprachen', 0.12372138004110025), ('literarisch', 0.11511031425252106)]"
2018,DHd2018,GODLER_Katharina_musilonline___integral_lösen__Dialogfeld_Di.xml,musilonline - integral lösen. Dialogfeld Digitale Edition,"Anke Bosse (Robert-Musil-Institut, AAU Klagenfurt); Walter Fanta (Robert-Musil-Institut, AAU Klagenfurt); Katharina Godler (Robert-Musil-Institut, AAU Klagenfurt); Gerrit Brüning (Goethe-Universität Frankfurt / Freies Deutsches Hochstift); Artur Boelderl (Institut für Germanistik, AAU Klagenfurt)","Digitale Edition, Interface, Manuskript, XML/TEI, Online-Kommentar","Umwandlung, Gestaltung, Annotieren, Archivierung, Kommentierung, Literatur","Digitale Editionen haben sich bereits als geeignete Publikationsform für die Präsentation von umfangreichen Textbeständen im Bereich des kulturellen Erbes etabliert. Für ein so umfangreiches und komplexes textgenetisches Korpus wie Robert Musils literarischen Nachlass und die daran entwickelte Datenstruktur der Klagenfurter Ausgabe stehen jedoch keine fertigen Modelle zur Verfügung. Im Rahmen des Panels soll einerseits diskutiert werden, welche Kriterien eine digitale Edition erfüllen muss, um eine Grundlage zur Erforschung von Robert Musils Gesamtwerk zu erstellen und andererseits, ob die derzeit geltenden Standards zur Langzeitarchivierung, interoperablen Repräsentation und Online-Kommentierung von digitalen Textkorpora noch zeitgemäß sind. Der umfangreiche literarische Nachlass des österreichischen Schriftstellers Robert Musil umfasst 12.000 Manuskriptseiten und wird bereits seit 1985 digital ediert. Die wichtigsten bisherigen Publikationsetappen markieren die CD-ROM-Ausgabe a) Die b) Das künftige Im Kurzvortrag werden noch keine fertigen Lösungen vorgestellt, sondern es erfolgt ein kritischer Aufriss der Problemlage in Folge der komplexen Struktur von Musils Manuskripten und die Präsentation eines Grundkonzepts an Hand von exemplarischen Ausschnitten aus dem Manuskriptbestand. (Walter Fanta, Robert-Musil-Institut/Kärntner Literaturarchiv, AAU Klagenfurt) c) Die fachgerechte d) Die e) Der Das Panel integriert höchst unterschiedliche Bereiche und Aspekte der Digital Humanities, die sich im Projekt",de,digital editionen geeignet Publikationsform Präsentation umfangreich Textbestände Bereich kulturell Erbe etablieren umfangreich komplex textgenetisch Korpus Robert Musil literarisch Nachlass entwickelt Datenstruktur Klagenfurter Ausgabe stehen fertig Modell Verfügung Rahmen Panel einerseits diskutieren kriterien digital Edition erfüllen Grundlage Erforschung Robert Musil Gesamtwerk erstellen andererseits derzeit geltend Standard Langzeitarchivierung interoperabl Repräsentation digital Textkorpora zeitgemäß umfangreich literarisch Nachlass österreichisch Schriftsteller Robert Musil umfassen Manuskriptseit digital edieren wichtig bisherig Publikationsetappe markieren -- Künftige Kurzvortrag fertig Lösung vorstellen erfolgen Kritischer Aufriss Problemlage Folge komplex Struktur Musil manuskripten Präsentation grundkonzept Hand exemplarisch Ausschnitt Manuskriptbestand Walter Fanta Kärntner literaturarchiv aau Klagenfurt c fachgerecht d e Panel integrieren höchst unterschiedlich bereich Aspekt Digital Humanitie Projekt,"[('musil', 0.4604287653661266), ('robert', 0.27215544332344704), ('fertig', 0.21442724452490197), ('nachlass', 0.20322609174356113), ('umfangreich', 0.17985111947849233), ('präsentation', 0.14924950986455673), ('digital', 0.14482278491506156), ('panel', 0.14255836999983393), ('publikationsetappe', 0.11510719134153165), ('kurzvortrag', 0.11510719134153165)]"
2018,DHd2018,KESSLER_Linda_Entitäten_im_Fokus_am_Beispiel_von_Captivity_N.xml,Entitäten im Fokus am Beispiel von Captivity Narratives,"Linda Kessler (Universität Stuttgart, Deutschland); Tamara Braun (Universität Stuttgart, Deutschland); Tanja Preuß (Universität Stuttgart, Deutschland)","maschinelles Lernverfahren, Sentimentanalyse, Captivity Narratives","Annotieren, Literatur, benannte Entitäten (named entities)","Eigennamenerkennung (NER) ist im Bereich der maschinellen Sprachverarbeitung bereits viel behandelt worden. Eine Übersicht hierzu findet sich bei Nadeau und Sekine (2007). In den Digital Humanities dient die Erkennung von benannten Entitäten der Identifikation zentraler Akteure und Elemente in Texten, welche unter anderem die Grundlage für tiefergehende Analysen bezüglich Beziehungen, Strukturen und Emotionen in diesen Texten bilden. Jannidis et al. (2015) thematisieren allerdings, dass die reine NER beispielsweise für eine Analyse von Figurennetzwerken in literarischen Texten unzureichend ist, da dabei nur Figurenreferenzen durch konkrete Namensnennung erfasst werden. Um spezifisch auf die Bedürfnisse von Textanalysen im Kontext der Digital Humanities einzugehen, wurden im ""Center for Reflected Text Analytics"" (CRETA) (Kuhn et al. 2016) der Universität Stuttgart Annotationsrichtlinien entworfen, die über die Annotation reiner Eigennamen hinausgehen und sich auf verschiedenartige Entitätsreferenzen in deutschsprachigen Texten unterschiedlicher Genres fokussieren. Ein Beispiel für den Mehrwert der Annotation solcher Entitätsreferenzen findet sich bei Blessing et al. (2017). Um die Übertragbarkeit der Richtlinien nicht nur zwischen verschiedenen Textsorten, sondern auch sprachübergreifend zu evaluieren, stellen wir unser Projekt mit dem Ziel der Annotation von Erzähltexten in englischer Sprache vor. Ausgehend von der durch CRETA geschaffenen Grundlage teilt sich unser Projekt in drei Phasen auf: die manuelle Annotation und Überprüfung der Übertragbarkeit der CRETA-Richtlinien auf die gegebene Textsorte, die Automatisierung der Entitätserkennung und die Einbindung der Entitäten in eine literaturwissenschaftliche Analyse. Als Textgrundlage dient eine Sammlung von englischsprachigen Captivity Narratives. Der so entstandene Goldstandard dient als Trainingsdatensatz zur Entwicklung eines maschinellen Lernverfahrens. Ein Naive Bayes Classifier wurde mit Features trainiert, die sich u.a. auf die äußere Gestalt (z.B. Großschreibung), die Wortart und die Zugehörigkeit zu Wortlisten (Namen und amerikanische Orte) beziehen. Im Kreuzvalidierungsverfahren kann damit ein Micro-Fscore von 0,29 erzielt werden. Für die am häufigsten im Trainingsmaterial vorhandene Klasse PER wurde ein Precision-Wert von 0,45 erzielt. Dies bedeutet, dass fast die Hälfte der automatisch mit PER annotierten Entitäten wirklich Personen sind. Der Recall von 0,3 zeigt, wie unvollständig die Erkennung mit einem knappen Drittel aller relevanten Personen noch ist. Eine Auswertung der Ergebnisse zeigt, dass die Länge und Verschachtelung vieler Entitäten die automatische Klassifizierung erschwert. Da sich im manuellen Annotationsprozess der Kontext häufig als Entscheidungshilfe herausstellte, sollte dieser bei der automatischen NER zukünftig berücksichtigt werden. Darüber hinaus könnte die Erweiterung der verwendeten Features durch syntaktische Informationen und die Verwendung einer größeren Menge an Trainingsdaten zu Verbesserungen führen. Um den Mehrwert der Entitätsreferenzen für eine inhaltliche Fragestellung bezüglich der Captivity Narratives zu veranschaulichen, zeigen wir die textstatistische Analyse von Emotionen im Umfeld bestimmter Entitäten bzw. Entitätsgruppen. Basierend auf den manuell annotierten Texten, lassen sich Personenentitäten mithilfe von Clusteranalysen gruppieren. Anhand von positiven und negativen Wortlisten lassen sich zwei Gruppen bilden, die sich grob als Abschließend lässt sich festhalten, dass auf Grundlage unserer Annotationen eine Abgrenzung der im Text auftretenden Gruppen anhand von emotionsgeladenen Wörtern möglich ist, die der erwarteten negativen Haltung der Verfasser gegenüber den Eingeborenen Nordamerikas entspricht. Die von CRETA entwickelten Annotationsrichtlinien sind grundsätzlich auf die von uns analysierten Texte anwendbar, trotz abweichender Sprache und spezifischer Erzählweise. Um die Breite der enthaltenen Entitätsreferenzen vollständig abbilden zu können, bedarf es allerdings einzelner Spezifizierungen der Annotationsrichtlinien für diese Textsorte. ",de,Eigennamenerkennung ner Bereich maschinell Sprachverarbeitung behandeln Übersicht hierzu finden Nadeau Sekine Digital Humanitie dienen Erkennung benannt entitäten Identifikation zentral Akteur elemenen Text Grundlage tiefergehend Analyse bezüglich Beziehung Struktur Emotion Text bilden Jannidis et thematisieren Reine ner beispielsweise Analyse Figurennetzwerke literarisch Text unzureichend Figurenreferenze konkret Namensnennung erfasst spezifisch Bedürfniss textanalyse Kontext Digital Humanitie eingehen center for reflected Text analytics creta kuhn et Universität Stuttgart Annotationsrichtlini entwerfen Annotation Reiner eigennamen hinausgehen verschiedenartig entitätsreferenzen Deutschsprachig Text unterschiedlich Genre fokussieren Mehrwert Annotation entitätsreferenzen finden Blessing Et Übertragbarkeit Richtlinie verschieden Textsort sprachübergreifend evaluieren stellen Projekt Ziel Annotation Erzähltext englisch Sprache ausgehend Creta geschaffen Grundlage teilen Projekt Phase Manuelle Annotation Überprüfung Übertragbarkeit Gegebene textsoren Automatisierung Entitätserkennung Einbindung Entität literaturwissenschaftlich Analyse Textgrundlage dienen Sammlung englischsprachig Captivity narratives entstanden Goldstandard dienen Trainingsdatensatz Entwicklung maschinell lernverfahrens naiv Bayes Classifier Features trainieren äußerer Gestalt Großschreibung Wortart Zugehörigkeit wortli Name amerikanisch Ort beziehen Kreuzvalidierungsverfahren erzielen häufig Trainingsmaterial vorhanden Klasse per erzielen bedeuten fast Hälfte automatisch per Annotierte entitäen Person Recall zeigen unvollständig Erkennung knapp Drittel relevant Person Auswertung Ergebnis zeigen Länge Verschachtelung vieler Entität automatisch Klassifizierung erschweren Manuelle annotationsprozess Kontext häufig Entscheidungshilfe herausstellen automatisch ner zukünftig berücksichtigen hinaus Erweiterung verwendet Feature syntaktisch Information Verwendung groß Menge Trainingsdat Verbesserung führen Mehrwert entitätsreferenzen inhaltlich Fragestellung bezüglich Captivity narratives veranschaulichen zeigen textstatistisch Analyse Emotion Umfeld bestimmt entität entitätsgruppen basierend manuell annotierten Text lassen personenentität Mithilfe Clusteranalysen gruppieren anhand positiv negativ Wortliste lassen Gruppe bilden grob abschließend lässn festhalten Grundlage annotatio Abgrenzung Text auftretend Gruppe anhand emotionsgeladen wörtern erwartet negativ Haltung Verfasser eingeboren Nordamerikas entsprechen Creta entwickelt Annotationsrichtlini grundsätzlich analysiert Text anwendbar trotz abweichend Sprache spezifisch Erzählweise Breit enthalten entitätsreferenzen vollständig abbilden bedürfen einzeln spezifizierungen Annotationsrichtlinie textsoren,"[('entitätsreferenzen', 0.3198280316983739), ('ner', 0.18904757109369802), ('creta', 0.1745775934592113), ('captivity', 0.15991401584918696), ('narratives', 0.14894778240966675), ('entität', 0.1407872289123224), ('textsoren', 0.13513196219459658), ('übertragbarkeit', 0.13513196219459658), ('text', 0.12774957909702142), ('mehrwert', 0.11638506230614087)]"
2018,DHd2018,KLINGER_Roman_Digitale_Modellierung_von_Figurenkomplexität_a.xml,Digitale Modellierung von Figurenkomplexität am Beispiel des Parzival von Wolfram von Eschenbach,"Manuel Braun (Institut für Literaturwissenschaft, Universität Stuttgart); Roman Klinger (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart); Sebastian Padó (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart); Gabriel Viehhauser (Institut für Literaturwissenschaft, Universität Stuttgart)","Figurenanalyse, Distributionelle Semantik, Distant Reading","Strukturanalyse, Beziehungsanalyse, Modellierung, Stilistische Analyse, Visualisierung","Figuren gehören zu den wichtigsten Bestandteilen literarischer Erzählungen. Narratologische Analysen haben sich bislang insbesondere mit zwei Aspekten von Figuren beschäftigt: ihrer strukturellen Bedeutung und ihrer Charakterisierung. Während der erste Aspekt in den Digital Humanities bereits modelliert worden ist (etwa im Rahmen von Netzwerkanalysen, vgl. Jannidis et. al. 2016, Piper et. al. 2017), steht die datengetriebene Untersuchung des zweiten noch ganz am Anfang, obwohl die hermeneutisch arbeitende Literaturwissenschaft wiederholt auf seine Bedeutung hingewiesen hat (etwa Jannidis 2004, 2009). Dieses Desiderat lässt sich darauf zurückführen, dass Figuren auf die unterschiedlichste Art gekennzeichnet werden 'etwa durch ihr Handeln und Reden, aber auch durch Beschreibungen und Bewertungen des Erzählers 'und dass neben expliziten Charakterisierungen schwerer zu greifende implizite stehen. Diese Informationen lassen sich nur schwer erheben und in einem umgreifenden Modell verrechnen. Um dennoch einen Einstieg in die digitale Erfassung der Figurencharakteristik zu ermöglichen, nehmen wir eine vergleichsweise einfach zu modellierende Facette der Figurendarstellung in den Blick, und zwar das Konzept der Figurenkomplexität, das auch in der traditionellen Theoriebildung eine prominente Rolle spielt. Nach der ebenso verbreiteten wie vielkritisierten Kategorisierung von Forster (1927) lassen sich Figuren grundsätzlich in Als Paradefall für Figurenkomplexität kann der um 1200/1210 abgefasste Artusroman ""Parzival"" angesehen Im Folgenden schlagen wir eine Methodik vor, mit der diese Einschätzungen der hermeneutischen Forschung digital modelliert und überprüft werden kann. Wir untersuchen, ob man bereits mit extrem einfachen Methoden Vorhersagen zur Figurenkomplexität treffen kann, und verwenden dazu Spezifisch betrachten wir zwei Arten frequenzbasierter Kontextvektoren: (a) In der Computerlinguistik werden Kontextvektoren typischerweise miteinander verglichen, um die semantische Öhnlichkeit der Zielwörter zu modellieren und finden breite Anwendung (Pantel und Turney 2010). In unserer Studie betrachten wir stattdessen den Dies entspricht der (aus theoretischer Perspektive natürlich stark vereinfachenden) Annahme, wonach Figuren als komplexer wahrgenommen werden, wenn sie in reicheren und verschiedeneren Kontexten vorkommen. Bei lexikalischen Kontexten (s.o.) sagen wir also voraus, dass höhere lexikalische Variabilität auf Komplexität hinweist 'wobei Entropie freilich nicht zwischen lexikalisch reichen und semantisch widersprüchlichen Kontexten unterscheiden kann. Bei entitätsbasierten Kontexten übernimmt diese Rolle das gemeinsame Auftreten einer Figur mit mehreren anderen Figuren. Methodisch ist anzumerken, dass Frequenz einen Störfaktor bei der Interpretation von Entropie darstellt: Häufigere Zielwörter kommen 'ceteris paribus 'häufiger in verschiedenen Kontexten vor und erhalten damit eine höhere Entropie. Da dieser Zusammenhang aber nicht linear ist, ist eine einfache Normalisierung nicht möglich. In einem ersten Schritt sind also nur Zielwörter mit ähnlicher Frequenz hinsichtlich ihrer Entropie gut vergleichbar. Als Textgrundlage verwenden wir den ""Parzival"" nach der 5. Auflage der Ausgabe Lachmanns (1891) in der digitalisierten, mit Lemmata und der Auszeichnung von Eigennamen versehenen Fassung von Yeandle (2014). Für unser Hauptexperiment beschränken wir uns auf die sieben im Text am häufigsten namentlich genannten Frauenfiguren Cundr√Æe, Herzeloyde, Jesch√ªte, Cunnew√¢re, Arn√Æve, B√™ne und Itonj√™. Hinzu kommt Sig√ªne als ein in der Forschung oft genanntes Beispiel für eine weniger komplexe Figur im ""Parzival"". Sieben der acht Figuren liegen in einem engen Frequenzband zwischen 30 und 40 Nennungen, während Sig√ªne nur 14 Mal vorkommt. Wir präsentieren die Ergebnisse per Streudiagramm, mit Entropie und Frequenz als den beiden Achsen. Abbildung 1 und 2 zeigen die Ergebnisse für die wichtigsten weiblichen Hauptfiguren. Diese lassen sich zwanglos mit den Einschätzungen der hermeneutischen Literaturwissenschaft in Einklang bringen. Die höchsten Werte weisen Cundr√Æe und Herzeloyde auf, auf deren Komplexität die ""Parzival""-Forschung immer wieder hingewiesen hat: Bei Cundr√Æe handelt es sich um die Gralsbotin, die zwar kultiviert und nach höfischen Sitten gekleidet auftritt, zugleich aber monströs hässlich ist. Sie klagt Parzival zwar berechtigterweise an, fällt dabei aber aus dem höfischen Rahmen. Herzeloyde zieht ihren Sohn Parzival aus (übermäßiger?) Trauer um ihren im Kampf gefallenen Ehemann fern von der höfischen Welt in einer Waldeinöde auf, um zu verhindern, dass auch er einst Ritter wird 'ein Verhalten, das die Forschung unterschiedlich bewertet. Mittelwerte erreichen Cunnew√¢re und Jeschute, die in der Forschung durchaus kontrovers diskutiert werden. B√™ne, Arn√Æve und Itonj√™ lassen sich hingegen eher als Nebenfiguren bezeichnen, für die keine hohe Komplexität zu erwarten war. Die in der Trauer um ihren Geliebten Schionatulander verharrende Sig√ªne, die gerade in ihrer Geradlinigkeit einen Gegenentwurf zu den auf Ruhm und Ehre versessenen Artusrittern darstellt, zeigt erwartungsgemäß keine hohen Entropiewerte. Allerdings erlaubt dieser Befund keine starke Interpretation, da für Sig√ªne aufgrund der niedrigeren Frequenz von vorneherein niedrigere Entropiewerte zu erwartet sind. Um dennoch eine Vorhersage zur Komplexität Sig√ªnes zu erhalten, vergleichen wir in den Abbildungen 3 und 4 ein breiteres Spektrum an Figuren im Frequenzbereich zwischen 10 und 20 Nennungen bezüglich ihrer Entropie. Dies entspricht einer erweiterten Version unserer Hypothese: Figuren, die Interessanterweise liefern die beiden Kontextdefinitionen für Sig√ªne abweichende Vorhersagen: Die unterdurchschnittliche soziale Kontextvielfalt entspricht der Isoliertheit der Figur, die sich aus Trauer von der Welt zurückzieht und wenn, dann fast nur noch mit Parzival interagiert. Die vergleichsweise hohe lexikalische Vielfalt könnte demgegenüber darauf zurückzuführen sein, dass Sig√ªne neben ihrer Trauer auch die weitere Funktion hat, Parzival über seine Herkunft aufzuklären: Während Sig√ªne als Figur eher statisch erscheint, sind ihre Erzählungen über die Gralswelt informationshaltig. Für eingehendere Untersuchungen wären daher weitere Faktoren der Figurendarstellung wie Figurenbeschreibung und Figurenrede mit einzubeziehen sowie das Verhältnis von lexikalischer und sozialer Kontextvielfalt näher zu bestimmen. Unsere Ergebnisse legen nahe, dass sich die Analyse der lexikalischen und ""sozialen"" Vielfalt von Figurenkontexten durchaus als Maß für eine erste Annäherung an das Konzept der Figurenkomplexität eignet. Damit konnte mit erstaunlich einfachen Mitteln ein Zugang zum komplexen narratologischen Themenfeld der Figurencharakterisierung gewonnen werden. Auf der technischen Ebene bleiben zwei Fragen offen, zum einen die nach den sprachlichen Mechanismen, die zur Korrelation von Entropie und Figurenkomplexität führen, zum anderen die nach dem tatsächlichen Zusammenhang von Frequenz und Entropie: Gibt es also auch selten auftretende Auf der konzeptuellen Ebene bieten unsere Ergebnisse den Einstieg in eine differenziertere Modellierung von Figurenkomplexität, deren Erarbeitung auf die traditionelle narratologische Theoriebildung zurückwirken kann: Die digitale Modellierung macht es notwendig, die Faktoren, aus denen sich das Konzept der Figurenkomplexität zusammensetzt (etwa Figurenhandeln, -rede, -beschreibung und -bewertung), genauer und expliziter zu bestimmen sowie über ihre Gewichtung nachzudenken. Außerdem wäre zu klären, was das Konzept der Komplexität genau meint und wie es sich zu verwandten Konzepten wie denen der Hybridität, der Dynamik oder der Aktantenhaftigkeit von Figuren verhält. Im Sinne einer kritisch reflektierten digitalen Methodik werden somit vielschichtige literaturwissenschaftliche Phänomene wie die Charakterisierung von Figuren durch den formalisierenden Zugang nicht nivelliert, vielmehr führt die Verbindung von quantitativen und qualitativen Methoden zur wechselseitigen Erhellung.",de,Figur gehören wichtig bestandteilen literarisch erzählungen narratologisch Analyse bislang insbesondere Aspekt Figur beschäftigen strukturell Bedeutung Charakterisierung Aspekt Digital Humanitie modellieren Rahmen Netzwerkanalyse Jannidis et Piper et stehen datengetrieben Untersuchung Anfang obwohl hermeneutisch arbeitend Literaturwissenschaft wiederholt Bedeutung hinweisen Jannidis Desiderat lässen zurückführen Figur unterschiedlich Art kennzeichnen handeln reden Beschreibung Bewertung Erzähler explizit Charakterisierung Schwerer greifend Implizit stehen Information lassen schwer erheben umgreifend Modell verrechnen dennoch Einstieg digital Erfassung Figurencharakteristik ermöglichen nehmen vergleichsweise einfach modellierend Facette Figurendarstellung Blick Konzept Figurenkomplexität traditionell Theoriebildung prominent Rolle spielen verbreitet vielkritisiert Kategorisierung Forster lassen Figur grundsätzlich Paradefall Figurenkomplexität abgefasste Artusroman parzival ansehen folgend schlagen Methodik Einschätzung hermeneutisch Forschung Digital modellieren überprüfen untersuchen extrem einfach Methode vorhersagen Figurenkomplexität treffen verwenden spezifisch betrachten Art frequenzbasiert kontextvektoren Computerlinguistik kontextvektoren typischerweise miteinander vergleichen semantisch Öhnlichkeit Zielwörter Modelliere finden breit Anwendung Pantel Turney Studie betrachten stattdessen entsprechen theoretisch Perspektive stark vereinfachend annahmen wonach Figur Komplexer wahrnehmen Reicheren verschiedener Kontext vorkommen lexikalisch Kontext sagen voraus hoch lexikalisch Variabilität Komplexität hinweisen wobei entropie freilich lexikalisch reichen semantisch widersprüchlich Kontext unterscheiden entitätsbasiert Kontext übernehmen Rolle gemeinsam Auftret Figur mehrere Figur methodisch anzumerken Frequenz Störfaktor Interpretation Entropie darstellen häufigere zielwörter Ceteris paribus häufig verschieden Kontext erhalten hoch Entropie Zusammenhang linear einfach Normalisierung Schritt zielwörter ähnlich Frequenz hinsichtlich Entropie vergleichbar Textgrundlage verwenden Parzival Auflage Ausgabe Lachmann digitalisiert Lemmata Auszeichnung Eigennam versehen Fassung Yeandle Hauptexperiment beschränken Text häufig namentlich genannt Frauenfigur Herzeloyde ne hinzu Forschung genannt komplex Figur parzival Figur liegen eng Frequenzband nennungen mal vorkommen präsentieren Ergebnis per Streudiagramm Entropie Frequenz Achse Abbildung zeigen Ergebnis wichtig weiblich hauptfiguren lassen zwanglos Einschätzung hermeneutisch Literaturwissenschaft einklang bringen hoch wert weise Herzeloyde Komplexität hinweisen handeln Gralsbotin kultivieren höfisch Sitte kleiden auftreten Monströs hässlich klagen Parzival berechtigterweise fallen höfisch Rahmen Herzeloyde ziehen Sohn Parzival Übermäßiger Trauer Kampf gefallen Ehemann fern höfisch Welt Waldeinöde verhindern einst Ritter verhalten Forschung unterschiedlich bewerten Mittelwerte erreichen jeschute Forschung kontrovers diskutieren ne lassen hingegen eher nebenfigurer bezeichnen hoch Komplexität erwarten Trauer geliebt schionatuland verharrend Geradlinigkeit Gegenentwurf Ruhm ehre versessen artusrittern darstellen zeigen erwartungsgemäß hoch entropiewern erlauben Befund stark Interpretation aufgrund niedrig Frequenz Vorneherein niedrig Entropiewert erwarten dennoch Vorhersage Komplexität erhalten vergleichen abbildung breiter Spektrum Figur Frequenzbereich Nennung bezüglich Entropie entsprechen erweitert Version Hypothese Figur interessanterweise liefern kontextdefinitionen abweichend vorhersagen unterdurchschnittlich sozial Kontextvielfalt entsprechen Isoliertheit Figur Trauer Welt zurückziehen fast Parzival interagieren vergleichsweise hoch lexikalisch Vielfalt zurückführen Trauer Funktion Parzival Herkunft aufklären Figur eher statisch erscheinen erzählungen gralswelt informationshaltig eingehend Untersuchung sein Faktor Figurendarstellung Figurenbeschreibung Figurenrede einzubezieh Verhältnis lexikalisch sozial Kontextvielfalt nah bestimmen Ergebnis legen nahe Analyse lexikalisch sozial Vielfalt Figurenkontext Maß Annäherung Konzept Figurenkomplexität eignen erstaunlich einfach Mittel Zugang komplex narratologisch Themenfeld Figurencharakterisierung gewinnen technisch Ebene bleiben Frage sprachlich mechanismen Korrelation Entropie Figurenkomplexität führen tatsächlich Zusammenhang Frequenz entropie selten auftretend konzeptuell Ebene bieten Ergebnis Einstieg differenziert Modellierung Figurenkomplexität Erarbeitung traditionell narratologisch Theoriebildung zurückwirken digital Modellierung notwendig Faktor Konzept Figurenkomplexität zusammensetzen figurenhandeln genau explizit bestimmen Gewichtung nachdenken klären Konzept Komplexität genau meinen verwandt Konzept Hybridität Dynamik Aktantenhaftigkeit Figur verhalten Sinn kritisch reflektierten Digital methodik somit vielschichtig literaturwissenschaftlich Phänomen Charakterisierung Figur formalisierender Zugang nivellieren vielmehr führen Verbindung quantitativen qualitativ Methode wechselseitig Erhellung,"[('entropie', 0.3289564022197072), ('figurenkomplexität', 0.3090287492624311), ('parzival', 0.2728009615084102), ('figur', 0.2624245792074321), ('lexikalisch', 0.15954222315063), ('frequenz', 0.14310384714312027), ('zielwörter', 0.1324408925410419), ('herzeloyde', 0.1324408925410419), ('höfisch', 0.1324408925410419), ('trauer', 0.13166695249602028)]"
2018,DHd2018,ODEBRECHT_Carolin_Suche_und_Visualisierung_von_Annotationen_.xml,Suche und Visualisierung von Annotationen historischer Korpora mit ANNIS. Kritik der korpuslinguistischen Analysemethoden in einem erweiterten Nutzungskontext,"Carolin Odebrecht (Humboldt-Universität zu Berlin, Deutschland); Thomas Krause (Humboldt-Universität zu Berlin, Deutschland); Rolf Guescini (Humboldt-Universität zu Berlin, Deutschland); Frank Kühnlenz (Humboldt-Universität zu Berlin, Deutschland); Anke Lüdeling (Humboldt-Universität zu Berlin, Deutschland); Malte Dreyer (Humboldt-Universität zu Berlin, Deutschland)","Suche und Visualisierung, Analyse, historische Korpora, Annotation, Wiederverwendung","Teilen, Inhaltsanalyse, Visualisierung, Daten, Forschungsprozess, Werkzeuge","Historische Korpora (Gippert und Gehrke 2015; Claridge 2008; Rissanen 2008) dienen in vielen geisteswissenschaftlichen Disziplinen als Analysegrundlage und können sehr unterschiedlich aufbereitet sein. Mit korpusbasierten Studien können qualitative und quantitative Analysen, die für die Überprüfung von Hypothesen über ein bestimmtes Phänomen notwendig sind, durchgeführt werden. Dem gegenüber steht methodisch die korpusgetriebene Studie, die das Korpus selbst nutzt, um Hypothesen über ein Phänomen zu generieren (vgl. McEnery und Hardie 2012; Lüdeling und Zeldes 2007). Neben diesen zwei Studientypen können mit Hilfe von Korpora auch einzelne Belege und Kontexte für die Beantwortung verschiedenster Forschungsfragen ermittelt werden. Eine andere methodische Unterscheidung wird mit dem In den digitalen Geisteswissenschaften müssen daher für die jeweiligen Methoden und Forschungsdaten Analyse- und Visualisierungswerkzeuge entwickelt werden, die es den Forscherinnen und Forschern ermöglichen, für ihren jeweiligen Forschungskontext aus einem breiten methodischen Spektrum wählen zu können (vgl. für einen Überblick z.B. Kupietz und Geyken 2016). Ein solches Werkzeug ist ANNIS (Krause und Zeldes 2016), das Such- und Visualisierungstool für Annotationen, das wir in unserem Workshop den Forscherinnen und Forschern aus den Digital Humanities vorstellen möchten. ANNIS erlaubt das Durchsuchen von Korpora, die unterschiedliche Arten von Annotationen, die möglicherweise durch unterschiedliche Forschergruppen unter verschiedenen Gesichtspunkten annotiert worden, in einem Korpus vereinen. Diese Flexibilität erlaubt es, annotierte Phänomene in der Suche zu kombinieren und damit komplexere Strukturen zu finden. Neben der Unterstützung der vielfältigen Analysemethoden ist eine weitere Herausforderung für die Analysewerkzeuge, dass historische Korpora je nach Forschungskontext und -frage unterschiedlich erstellt und aufbereitet werden (Lüdeling 2011). Dies zeigt sich unter anderen in den vielfältigen Transkriptions- und Normalisierungsverfahren (vgl. z.B. Odebrecht et al. 2016; Krasselt et al. 2015; Archer et al. 2015; Bollmann et al. 2012; Jurish 2010) und Annotationsguidelines (für z.B. Annotation von Wortarten für historisches Deutsch Coniglio et al. 2016; Dipper et al. 2013) sowie verschiedenen Formaten (z.B. Romary et al. 2015; Schmidt und Wörner 2009; Burnard und Baumann 2008; Wittenburg et al. 2006; Dipper 2005), die allein für die Erstellung von historischen Korpora eingesetzt werden. Damit historische Korpora mit verschiedenen Methoden analysiert werden können, muss deren Wiederverwendung ermöglicht werden. Die Wiederverwendung von historischen Korpora wird durch u.a. deren freie Veröffentlichung und umfassende Dokumentation möglich (Odebrecht 2014; Borgmann 2012; Büttner et al. 2011). Weiterhin erhöht eine Wiederverwendung ihre Sichtbarkeit und stellt eine Chance zur engeren Vernetzung und Zusammenarbeit in den digitalen Geisteswissenschaften dar. So können auch historische Korpora in unterschiedlichen Wiederverwendungsszenarien gedacht werden (vgl. Simons und Bird 2008) und als empirische Grundlage für die verschiedenen Analysemethoden dienen. Dieser Workshop möchte ausgehend von diesen Themenkomplex mit den Teilnehmerinnen und Teilnehmern folgende Fragen diskutieren: Wie können Analysewerkzeuge den Forscherinnen und Forschern vielfältige Analysemethoden und Visualisierungsmethoden für verschiedene historische Korpora ermöglichen? Wie kann ANNIS die verschiedenen Analysemethoden bislang unterstützen? Wie kann es gelingen, auch die Vielfältigkeit der Forschungsdaten als solche zu berücksichtigen und deren Wiederverwendung zu ermöglichen? Wie können Werkzeuge spezifisch genug entwickelt werden, um genaue und für den Forschungskontext und die Forschungsdaten angepasste Analysen zu ermöglichen? Der Workshop hat das Ziel, anhand mehrerer historischer Korpora des Deutschen das generische Such- und Visualisierungstool ANNIS (Krause und Zeldes 2016) für den Einsatz in den Digital Humanities zu diskutieren und anzuwenden, da es bislang überwiegend für korpusbasierte und korpusgetriebene Studien sowie für das Auffinden von sprachlichen Belegen eingesetzt wird. ANNIS wird seit 2009 als ein generisches webbasiertes Such- und Visualisierungstool für verschiedene Korpustypen und Annotationskonzepte in verschiedenen Kooperationen mit der Humboldt-Universität zu Berlin und der Georgetown University und in mehreren Projekten entwickelt. Der Quellcode von ANNIS ist frei zugänglich veröffentlicht und bietet gleichzeitig eine Desktop- sowie Server-Installation. In ANNIS können Korpora mit Token-, Spannen-, Baum- und Pointingannotationen unabhängig von den einzelnen, jeweils korpusspezifischen Annotationsguidelines in ANNIS analysiert werden. ANNIS bietet weiterhin den Korpuserstellerinnen und -erstellern annotations- oder fachspezifische Visualisierungen für Korpora. Mit einer wiederum generischen und mächtigen Anfragesprache (ANNIS Query Language 'AQL) können alle Korpora in ANNIS nach Annotationen und Kombinationen von Annotationen durchsucht werden. Weiterhin können die Suchergebnisse für bspw. weitere statistische Auswertungen exportiert werden. Jedes Korpus, jede Suchanfrage und jeder Beleg kann über einen permanenten Link stabil referenziert werden. Mit dem Konverterframework Pepper (Zipser und Romary 2010) werden Korpora, die in verschiedenen Formaten vorliegen können, in das ANNIS-Format überführt. Repositorien wie das LAUDATIO-Repository (Odebrecht et al. 2015) ermöglichen einen Open Access Zugang zu verschiedensten historischen Korpora und stellen eine umfassende Korpusdokumentation (Odebrecht 2014) zur Verfügung, die eine Erschließung dieser heterogenen Datengrundlage unabhängig von den Korpuserstellerinnen und -erstellern ermöglicht. Damit wird eine Voraussetzung für die Wiederverwendung der historischen Korpora erfüllt. Für den Workshop werden aus LAUDATIO beispielhaft die Korpora ""Referenzkorpus Altdeutsch"" (Donhauser 2015) und ""RIDGES Herbology Korpus"" (Odebrecht et al. 2016) verwendet. Das Referenzkorpus Altdeutsch ist ein historisches Mehrebenenkorpus der ganzen Sprachperiode des Althochdeutschen mit ca. 650.000 Wörtern (von den ersten Überlieferungen bis Mitte des 11. Jahrhunderts). Als Grundlage für die diplomatischen Transkription sind Editionen der jeweiligen Handschriften, die mit weiteren Annotationen zur Textstruktur sowie mit komplexen Wortartenannotation (Dipper et al. 2013), Annotation zu Flexionsklassen und Lemmatisierung versehen sind. Das RIDGES Korpus ist ein tief annotiertes Korpus mit Auszügen aus gedruckten Kräuterbüchern aus der Zeit zwischen 1487 und 1910, anhand derer die Entwicklung der deutschen Wissenschaftssprache auf vielen Ebenen untersucht wird. Die Drucke sind diplomatisch transkribiert (wo möglich, nach vorher digitalisierten oder durch OCR-Verfahren erstellte Vorlagen, vgl. Springmann und Lüdeling 2017). Die Daten sind mehrfach normalisiert und auf vielen Ebenen annotiert (unter anderem mit Wortart, Lemma, Informationen zu Kompositionstypen (Perlitz 2014), Dependenzsyntax, Informationen zur graphischen Struktur nach den TEI Guidelines). Dabei werden automatische und manuelle Annotationsverfahren und Prüfverfahren genutzt. Um die eingangs formulierten Fragen adressieren zu können, wird der Workshop zwei Schwerpunkte enthalten. Der erste Schwerpunkt wird die Einführung in die Funktionen und Suchanfragesprache von ANNIS sowie die damit verbundene Vorstellung der zwei historischen Beispielkorpora umfassen. Wir wollen den Teilnehmerinnen und Teilnehmern die verschiedenen Analyse- und Visualisierungsmöglichkeiten online und hands-on vorstellen. Über die Vorstellung zweier historischer Korpora mit dem generischen ANNIS können bereits die Herausforderungen der heterogenen Datengrundlage in den digitalen Geisteswissenschaften für Analysetools herausgearbeitet werden. Der zweite Schwerpunkt soll Raum für eine Diskussion mit den Teilnehmerinnen und Teilnehmern sowie auch die Möglichkeit geben, weitere Korpora in ANNIS 'geleitet von den Forschungsinteressen der Teilnehmerinnen und Teilnehmern 'zu durchsuchen. Mit diesem Workshop wollen wir uns gemeinsam mit den Teilnehmerinnen und Teilnehmern kritisch mit den Anforderungen an ein Analysetool für verschiedene Methoden zur Analyse und Visualisierung von historischen Korpora auseinandersetzen und prüfen, in wie weit ANNIS bereits einige dieser Anforderungen erfüllen kann. So wollen wir ANNIS in einem neuen Forschungskontext der Digital Humanities diskutieren und dabei neue Nutzerszenarien für die weitere Entwicklung erarbeiten.",de,historisch Korpora gippert gehrken Claridge rissan dienen geisteswissenschaftlich Disziplin Analysegrundlage unterschiedlich aufbereiten korpusbasiert Studie qualitativ quantitativ analysen Überprüfung Hypothesen bestimmt Phänomen notwendig durchführen stehen methodisch korpusgetrieben Studie Korpus nutzen Hypothesen Phänomen generieren Mcenery Hardie lüdeling zeld Studientype Hilfe Korpora einzeln Beleg Kontexte Beantwortung verschiedenst Forschungsfrag ermitteln methodisch Unterscheidung digital geisteswissenschaften jeweilig methode forschungsdat visualisierungswerkzeuge entwickeln forscherinnen forschern ermöglichen jeweilig Forschungskontext breit methodisch Spektrum wählen Überblick Kupietz geyken Werkzeug Annis Krause zeld Visualisierungstool annotation unser Workshop Forscherinn Forscher Digital Humanitie vorstellen möchten Annis erlauben Durchsuchen Korpora unterschiedlich Art Annotation möglicherweise unterschiedlich Forschergruppe verschieden Gesichtspunkt annotiert Korpus vereinen Flexibilität erlauben annotiert Phänomen Suche kombinieren komplex Struktur finden Unterstützung vielfältig analysemethoden Herausforderung Analysewerkzeuge historisch Korpora forschungskontext unterschiedlich erstellen aufbereitet lüdeling zeigen vielfältig normalisierungsverfahren odebrechen et krasselt Et arch et Bollmann Et jurish annotationsguidelin Annotation Wortarten Historische deutsch Coniglio et Dipper et verschieden Format romary Et Schmidt Wörner Burnard Baumann wittenburg et dipper Erstellung historisch Korpora einsetzen historisch Korpora verschieden Methode analysieren Wiederverwendung ermöglichen Wiederverwendung historisch Korpora frei Veröffentlichung umfassend Dokumentation odebrechen Borgmann Büttner et weiterhin erhöhen Wiederverwendung Sichtbarkeit stellen Chance eng Vernetzung Zusammenarbeit digital geisteswissenschaften dar historisch Korpora unterschiedlich Wiederverwendungsszenarien denken simons bird empirisch Grundlage verschieden analysemethoden dienen Workshop ausgehend Themenkomplex teilnehmerinn Teilnehmer folgend Frage diskutieren Analysewerkzeuge Forscherinn forschern vielfältig analysemethoden visualisierungsmethoden verschieden historisch Korpora ermöglichen Annis verschieden analysemethoden bislang unterstützen gelingen Vielfältigkeit forschungsdaten berücksichtigen Wiederverwendung ermöglichen Werkzeug spezifisch entwickeln genau Forschungskontext forschungsdat angepas Analyse ermöglichen Workshop Ziel anhand mehrerer historisch Korpora deutscher generisch visualisierungstool Annis Krause zeld Einsatz Digital Humanitie diskutieren anwenden bislang überwiegend Korpusbasiert korpusgetrieben Studie auffind sprachlich Belege einsetzen Annis generisch webbasiert Visualisierungstool verschieden Korpustype annotationskonzepte verschieden Kooperation Berlin Georgetown university mehrere Projekt entwickeln Quellcode Annis frei zugänglich veröffentlichen bieten gleichzeitig Annis Korpora Pointingannotation unabhängig einzeln jeweils korpusspezifisch annotationsguidelin Annis analysieren Annis bieten weiterhin korpuserstellerinn fachspezifisch visualisierungen Korpora wiederum generisch mächtig Anfragesprach Annis query language aql Korpora Annis annotation Kombination annotation durchsuchen weiterhin suchergebnisse statistisch Auswertung exportieren jeder korpus Suchanfrage Beleg permanent link stabil referenzieren Konverterframework Pepper zipser romary Korpora verschieden Format vorliegen überführen Repositorie odebrechen et ermöglichen op Access Zugang verschieden historisch Korpora stellen umfassend Korpusdokumentation odebrechen Verfügung Erschließung heterogen Datengrundlage unabhängig korpuserstellerinn ermöglichen Voraussetzung Wiederverwendung historisch Korpora erfüllen Workshop Laudatio Beispielhaft Korpora Referenzkorpus altdeutsch donhauser ridg Herbology Korpus odebrechen et verwenden Referenzkorpus altdeutsch historisch Mehrebenenkorpus Sprachperiode althochdeutsch wörtern überlieferungen Mitte Jahrhundert Grundlage diplomatisch Transkription editionen jeweilig Handschrift Annotation Textstruktur komplex Wortartenannotation Dipper et Annotation flexionsklassen Lemmatisierung versehen ridg korpus tief annotiert Korpus Auszüge gedruckt Kräuterbücher anhand der Entwicklung deutsch Wissenschaftssprach Ebene untersuchen Druck diplomatisch transkribieren vorher digitalisieren erstellt vorliegen Springmann Lüdeling daten mehrfach normalisieren eben Annotiert Wortart Lemma Information Kompositionstype Perlitz dependenzsyntaxn Information graphisch Struktur tei guidelin automatisch Manuelle annotationsverfahren prüfverfahren nutzen eingangs formuliert Frage adressieren Workshop Schwerpunkt enthalten schwerpunken Einführung Funktion suchanfragesprach Annis verbunden Vorstellung historisch Beispielkorpora umfassen Teilnehmerinn Teilnehmer verschieden visualisierungsmöglichkeit Online vorstellen Vorstellung zwei Historischer Korpora generisch Annis Herausforderung heterogen Datengrundlage digital geisteswissenschaften Analysetools herausgearbeiteen Schwerpunkt Raum Diskussion Teilnehmerinn Teilnehmer Möglichkeit geben Korpora Annis leiten Forschungsinteresse Teilnehmerinn Teilnehmer durchsuchen Workshop gemeinsam teilnehmerinn Teilnehmer kritisch Anforderung Analysetool verschieden Methode Analyse Visualisierung historisch Korpora auseinandersetzen prüfen Annis Anforderunge erfüllen Annis Forschungskontext Digital Humanitie diskutieren Nutzerszenari Entwicklung erarbeiten,"[('annis', 0.547153884559725), ('korpora', 0.32498474777884356), ('historisch', 0.18801723093857212), ('wiederverwendung', 0.1835743489091833), ('odebrechen', 0.1620537215719617), ('teilnehmerinn', 0.14467893716548844), ('forschungskontext', 0.12964297725756938), ('teilnehmer', 0.12545919208843714), ('et', 0.11641971470696388), ('verschieden', 0.11546438770354539)]"
2018,DHd2018,HAAF_Susanne__Perspektiven_auf_ein_Korpus__Kombinationen_qua.xml,Perspektiven auf ein Korpus. Kombinationen quantitativ-qualitativer Analysemethoden zur Ermittlung von Textgliederungsprinzipien,"Susanne Haaf (Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland)",quantitativ-qualitative Analyse; Methodendiskussion; Korpuslinguistik; TEI-Analyse; Textgliederungsprinzipien,"Strukturanalyse, Kontextsetzung, Visualisierung, Text","Im Bereich digital basierter Untersuchungen wird zunehmend eine Verzahnung quantitativen und qualitativen Arbeitens gefordert. In der konkreten Arbeit der Korpusanalyse wird aus dieser scheinbaren Dichotomie jedoch schnell eine Methodenvielfalt, denn gerade durch Kombinationen verschiedener Perspektiven auf die Daten werden unterschiedliche Phänomene greifbar und entfaltet sich das volle Potential quantitativ-qualitativen Arbeitens. Das hier präsentierte Poster soll dies an einem konkreten Beispiel veranschaulichen. Inhaltlicher Ausgangspunkt ist die Frage nach Textgliederungsprinzipien, welche für bestimmte erbauliche Textsorten kennzeichnend sind. Mittel der Textgliederung können als Mittel der Markierung von Teiltexten innerhalb eines Gesamttextes beschrieben werden (Hausendorf/Kesselheim 2008: 41) und können wiederum für Textsorten charakteristisch sein. Sie finden sich auf verschiedenen Ebenen des Textes, u.a. im Bereich der Typographie (Stein 2003: 422). Gerade diese typographischen Gliederungsmerkmale stellen einen guten Ausgangspunkt für eine quantitative Analyse von Textgliederungsmerkmalen dar, da sie in TEI-annotierten Korpora z.B. durch die XML-Strukturierung automatisch greifbar werden. Anders als bei dem Verfahren, textbezogene Phänomene reduziert auf bestimmte TEI-Strukturen zu untersuchen (z.B. Schöch 2016: 351ff., Haaf 2016), gelangen hier die im Korpusvergleich möglicherweise signifikanten Häufigkeiten der TEI-Strukturierungen selbst in den Blick. Die inhaltliche Frage nach Textgliederungsprinzipien erbaulicher Textsorten wird ausführlich behandelt in Haaf (in Vorber.). Im vorliegenden Beitrag stehen 'der thematischen Ausrichtung der Konferenz entgegenkommend 'Überlegungen zur adäquaten Methodik einer solchen Untersuchung im Vordergrund. Der hier präsentierten Studie liegen drei Teilkorpora des 17. Jahrhunderts aus dem Deutschen Textarchiv (2017) zugrunde: Die Texte des DTA-Korpus wurden nach einheitlichen Richtlinien und mittels eines TEI-Subsets, das Ambiguitäten der Auszeichnung möglichst reduziert, ausgezeichnet (Haaf et al. 2014/15). Für die vorliegende Untersuchung wurden einzelne TEI-Strukturen hinsichtlich der Häufigkeit ihres Auftretens (relativ zur Token-Anzahl) und ihrer Verteilung im jeweiligen Korpus verglichen, um speziell die Unterschiede in der Textgliederung zwischen den untersuchten Textsorten herauszuarbeiten. Dabei wurden solche Tags einbezogen, die voraussichtlich Textgliederungsmerkmale repräsentieren. So kann z.B. tei:div die Kapitelstruktur eines Textes anzeigen, durch tei:l wird der Wechsel zwischen Prosatext und Lyrik greifbar, tei:note zeigt Metatexte in Form von Anmerkungen, z.B. Marginalien, an, tei:hi repräsentiert Hervorhebungen von Textpassagen gegenüber dem Grundtext. Die Ergebnisse wurden einer qualitativen Beurteilung unterzogen. Zur Evaluation eines Merkmals war hier nicht allein der Blick auf seine relativen Häufigkeiten in und deren signifikante Unterschiede zwischen den untersuchten Korpora relevant. Die signifikant erhöhte Häufigkeit eines Merkmals kann vielmehr unterschiedliche Gründe haben. So kann sie einerseits zwar durchaus (1) auf die höhere Relevanz des Merkmals im Korpus hindeuten, wie sich am Merkmal der Marginalie zeigt, das signifikant häufig und regelmäßig verteilt im Korpus der Funeralschriften auftritt (Abb. 1). Sie kann andererseits aber auch (2) aufgrund der unausgewogenen Verteilung des Merkmals im Korpus gar nicht aussagekräftig sein, entweder weil (2a) sich das Korpus selbst als in sich unausgewogen und nicht repräsentativ für den zu beschreibenden Gegenstand erweist oder weil (2b) das Merkmal im gegebenen Kontext nicht relevant ist, wie etwa die horizontale Trennlinie zwischen Textteilen, die in allen drei Vergleichskorpora unregelmäßig verteilt war (Abb. 2). Andererseits kann es (3) auch vorkommen, dass die bestehende Ausgewogenheit der Verteilung eines Merkmals in einem Korpus letztlich nicht aussagekräftig für dessen Relevanz ist. Weiterhin konnten im gegebenen Kontext (4) auch Merkmale mit geringeren Häufigkeiten relevant sein, und schließlich ist nicht zuletzt (5) auch der Ort, an dem ein Merkmal im Dokument auftritt, zu berücksichtigen. Beide Aspekte (4 und 5) zeigen sich z.B. am Merkmal der Liste, die in den erbaulichen Prosawerken erwartungsgemäß selten, aber relativ regelmäßig auftritt, und zwar in Form von Registern am Buchbeginn oder Buchende (in tei:front oder tei:back). Methodisch zeigte sich also, dass für eine adäquate Beurteilung der untersuchten Merkmale verschiedene Blickwinkel notwendig sind. Das Poster veranschaulicht anhand der erwähnten und weiterer Beispiele diese genannten methodischen Aspekte. Inhaltlich führte die Untersuchung zutage, dass z.B. Merkmale, die den Zugang zum Text erleichtern und Hilfe zur Orientierung im Text geben, für die erbaulichen Textsorten relevant sind (Näheres vgl. Haaf (in Vorber.)).",de,Bereich Digital basiert Untersuchung zunehmend Verzahnung quantitativen qualitativ Arbeiten fordern konkret Arbeit Korpusanalyse scheinbar Dichotomie schnell Methodenvielfalt Kombination verschieden Perspektive daten unterschiedlich Phänomen greifbar entfalten voll Potential Arbeiten präsentiert Poster konkret veranschaulichen inhaltlich Ausgangspunkt Frage Textgliederungsprinzipie bestimmt erbaulich Textsort Kennzeichnend Textgliederung Markierung teiltexten innerhalb Gesamttext beschreiben Hausendorf Kesselheim wiederum Textsorte charakteristisch finden verschieden Ebene Text Bereich Typographie Stein typographisch gliederungsmerkmal Stelle gut Ausgangspunkt quantitativ Analyse Textgliederungsmerkmal dar Korpora automatisch greifbar Verfahren textbezogen phänomen reduzieren bestimmt untersuchen schöch Haaf gelangen korpusvergleich möglicherweise signifikanten häufigkeien Blick inhaltlich Frage Textgliederungsprinzipie erbaulich Textsort ausführlich behandeln Haaf Vorber vorliegend Beitrag stehen thematisch Ausrichtung Konferenz Entgegenkommend Überlegung adäquat Methodik Untersuchung vordergrund präsentiert Studie liegen Teilkorpora Jahrhundert deutsch Textarchiv zugrunde Text einheitlich Richtlinie mittels Ambiguität Auszeichnung möglichst reduzieren auszeichnen Haaf et vorliegend Untersuchung einzeln hinsichtlich Häufigkeit Auftreten relativ Verteilung jeweilig Korpus vergleichen speziell Unterschied Textgliederung untersuchten textsorten Herauszuarbeit tags einbeziehen voraussichtlich textgliederungsmerkmal repräsentieren tei div Kapitelstruktur Text anzeigen tei l Wechsel Prosatext Lyrik greifbar tei Note zeigen Metatexte Form Anmerkung marginalien tei hi repräsentieren Hervorhebung Textpassagen Grundtext Ergebnis qualitativ Beurteilung unterziehen Evaluation Merkmal Blick relativ häufigkeien signifikant Unterschied untersucht Korpora relevant Signifikant erhöht Häufigkeit Merkmal vielmehr unterschiedlich gründe einerseits hoch Relevanz Merkmal Korpus hindeuen Merkmal Marginalie zeigen signifikant häufig regelmäßig verteilen Korpus funeralschrift Auftritt abb andererseits aufgrund unausgewogen Verteilung Merkmal Korpus aussagekräftig Korpus unausgewogen repräsentativ beschreibend Gegenstand erweisen Merkmal gegeben Kontext relevant horizontal Trennlinie Textteile Vergleichskorpora unregelmäßig verteilen abb andererseits vorkommen bestehend Ausgewogenheit Verteilung Merkmal Korpus letztlich aussagekräftig Relevanz weiterhin gegeben Kontext merkmal gering häufigkeiten relevant schließlich zuletzt Ort Merkmal Dokument auftreten berücksichtigen Aspekt zeigen Merkmal Liste erbaulich Prosawerk erwartungsgemäß selten relativ regelmäßig auftreten Form Register Buchbeginn Buchende tei Front tei back methodisch zeigen adäquat Beurteilung untersucht merkmal verschieden Blickwinkel notwendig Poster veranschaulichen anhand erwähnt weit Beispiel genannt methodischen aspeken inhaltlich führen Untersuchung zutage merkmal Zugang Text erleichtern Hilfe Orientierung Text geben erbaulich Textsort relevant näher Haaf Vorber,"[('merkmal', 0.4070085218153929), ('erbaulich', 0.2529047280388945), ('tei', 0.20778319251864244), ('haaf', 0.20591327539110352), ('greifbar', 0.16744231679989421), ('textgliederung', 0.12645236401944726), ('textgliederungsmerkmal', 0.12645236401944726), ('vorber', 0.12645236401944726), ('textgliederungsprinzipie', 0.12645236401944726), ('korpus', 0.12051942493098607)]"
2018,DHd2018,FICHTNER_Mark_Von_Drupal_8_zur_virtuellen_Forschungsumgebung.xml,Von Drupal 8 zur virtuellen Forschungsumgebung - Der WissKI-Ansatz,"Mark Fichtner (Germanisches Nationalmuseum, Deutschland)","WissKI, CIDOC CRM, Ontologie, OWL DL, Drupal","Modellierung, Kollaboration, Organisation, Visualisierung, Metadaten, virtuelle Forschungsumgebungen","Im Rahmen des von der DFG finanzierten Projekts ""WissKI"" entstand in zwei Projektphasen eine digitale Forschungsumgebung für die Anwendung im Bereich der Digital Humanities. Mit dem Ende der zweiten Projektphase 2017 wurde die Forschungsumgebung grundlegend aktualisiert und setzt nun auf das Open Source Content Management System Drupal 8 auf. Damit ging eine Aktualisierung der gesamten zugrundeliegenden Frameworks und Technologien (php 7, SPARQL 1.1) einher. Die aktuelle Version der Forschungsumgebung steht nun der wissenschaftlichen Öffentlichkeit als Open Source zur freien Verfügung. Auch die aktuelle Fassung der Software setzt auf die bewährten Kernaspekte: Die Datenerfassung und -haltung in WissKI wird zentral bestimmt durch die semantischen Zusammenhänge zwischen einzelnen Fakten und Datensätzen. Dies wird durch umfassende Unterstützung aktueller Semantic Web Technologien erreicht. Die Einordnung und Speicherung der erhobenen Daten erfolgt auf Grundlage einer Domänenontologie, deren Konzepte und Relationen - zu sogenannten Pfaden verbunden - als Vorlage für die Masken und Felder im System dienen. Auf Basis dieser Technologie werden solitär erscheinende Daten zu einem gemeinsamen, semantischen Netzwerk verbunden und damit die unmittelbare Sichtbarkeit weiterer, tiefergehender Zusammenhänge ermöglicht. Hierdurch werden intuitiv Zusammenhänge in den Daten sichtbar, die sich für den Nutzer als Mehrwert anbieten. Das Webbasierte Systemdesign und der dadurch ermöglichte Zugriff über das Internet, die Anbindung von externen kuratierten Datenquellen (sog. Authority Files) und die Möglichkeit zur Bereitstellung ausgewählter Daten über gängige Online-Schnittstellen (Web-Frontend, SPARQL-Endpoint, ...) betonen den Semantic-Web-Gedanken hinter der Infrastruktur. Die Speicherung der Daten erfolgt in einem TripleStore, der die eingegebenen Fakten in einer Subjekt-Prädikat-Objekt-Satzform ablegt. Die Aneinanderreihung der hier verwendeten Prädikate zu Pfaden erfolgt im Kern des Systems, dem sogenannten Pathbuilder, mit dem die semantische Bedeutung der einzelnen Einträge in Bezug auf das beschriebene Objekt (auch Person, Ort o. Ö.) anhand der Ontologie festgelegt wird. Die Eingabe der Daten erfolgt über eine, mit den gängigen Datenbankoberflächen vergleichbare, Editier-Oberfläche. Sie ist aus Feldern aufgebaut, die wiederum je einem bestimmten Feldtyp zugeordnet sind. Feldtypen bestimmen die Ein- und Ausgabemodalitäten der Daten. Dabei verzichtet die Software nicht auf die aus dem Bereich der Content Management Systeme bekannten Funktionalitäten wie z. B. die Generierung von Websites, Foren, Wikis oder auch die detaillierte Verwaltung der Nutzer und ihrer Zugriffsrechte. Inzwischen ist die Software in verschiedenen Forschungsprojekten an unterschiedlichen, namhaften Institutionen im kunst- und kulturhistorischen, sowie biologischen und technischen Bereich erfolgreich im Einsatz. Als Domänenontologie im Museums- und Sammlungsbetrieb kommen individuelle Erweiterungen des ""Conceptual Reference Model"" des Comité international pour la documentation zum Einsatz (CIDOC-CRM: ISO 21127), dessen Umsetzung in der Web-Ontology-Language OWL ebenfalls vom Projekt besorgt wurde und über die Website http://erlangen-crm.org frei zur Verfügung steht. Das Poster stellt den aktuellen Stand der WissKI-Software nach Vollendung der beiden Projektphasen dar. Neben dem bewährten Modell der Anpassung der Software durch die beiden am DFG-Projekt beteiligten Museen und der Friedrich-Alexander-Universität Erlangen-Nürnberg unterstützt die Interessengemeinschaft für semantische Datenverarbeitung e.V. (http://www.igsd-ev.de/) die gemeinnützigen Aspekte der Software weiter. Darüber hinaus werden bewusst auch Dritte zum Einsatz und zur Anpassung von WissKI eingeladen. Daraus resultierte im vergangenen Jahr der zahlreiche Einsatz der Software in Forschungsprojekten z.B. in Kooperation mit der Landesstelle der Nichtstaatlichen Museen in Bayern oder dem Zentralinstitut für Kunstgeschichte. Das System stellte v.a. durch die Nutzung aller Drupal-Basis-Funktionalitäten wie z.B. ""Views"" seine Stärken unter Beweis. So können neben den altbewährten Textfeldern und -bereichen und Bildern (incl. Zoomviewer für sehr hochauflösende Bilder) auch interaktive Landkarten, 3D-Animationen, Zeitstrahlen und alle denkbaren Medientypen, sowohl zur direkten Ansicht als auch zum Download als Funktionalität genutzt werden. Zusätzlich zu diesen gängigen Formaten ermöglicht die Standardkonformität von WissKI-D8 auch die Einbindung anderer, gängiger Feldtypmodule, die für Drupal 8 zur Verfügung stehen. Zu den erwähnten Erleichterungen zählt ebenso ein Update des System-Kerns, dem Pathbuilder, mit dem die Pfadschablonen durch die Domänenontologie auf einer graphischen Oberfläche ausgewählt bzw. erzeugt werden können. Daneben wird eine umfassende Bibliothek mit Musterontologien, -masken und -pfaden bereitgestellt, die die Einstiegshürde für Erstbenutzer minimal zu halten.",de,Rahmen Dfg finanziert Projekt Wisski entstehen Projektphas digital Forschungsumgebung Anwendung Bereich Digital Humanitie Projektphase Forschungsumgebung grundlegend aktualisieren setzen Open Source Content Management System Drupal Aktualisierung gesamt zugrundeliegend framework Technologie php sparql einher aktuell Version Forschungsumgebung stehen wissenschaftlich Öffentlichkeit Open Source frei Verfügung aktuell Fassung Software setzen bewährt kernaspeken Datenerfassung Wisski zentral bestimmen semantisch Zusammenhäng einzeln Fakt datensätzen umfassend Unterstützung aktuell Semantic web Technologie erreichen Einordnung Speicherung erhoben daten erfolgen Grundlage Domänenontologie Konzept relationen sogenannter Pfad verbinden Vorlage masken feld System dienen Basis Technologie solitär erscheinend daten gemeinsam semantisch netzwerk verbinden unmittelbar Sichtbarkeit weit Tiefergehender Zusammenhänge ermöglichen hierdurch intuitiv zusammenhang daten sichtbar Nutzer Mehrwert anbieten webbasierter Systemdesign ermöglicht Zugriff Internet Anbindung extern kuratiert Datenquell Authority Files Möglichkeit Bereitstellung ausgewählt daten gängig betonen Infrastruktur Speicherung daten erfolgen Triplestore eingegeben Fakt ablegen Aneinanderreihung verwendet Prädikat Pfad erfolgen Kern System sogenannter Pathbuilder semantisch Bedeutung einzeln einträge Bezug beschrieben Objekt Person Ort anhand Ontologie festlegen Eingabe daten erfolgen gängig datenbankoberfläch vergleichbar feldern aufbauen wiederum bestimmt feldtyp zuordnen Feldtype bestimmen ausgabemodalitäter daten verzichten Software Bereich Content Management system bekannt Funktionalität Generierung websit For wiki detailliert Verwaltung Nutzer zugriffsrechten inzwischen Software verschieden Forschungsprojekt unterschiedlich namhaft Institution kulturhistorisch biologisch technisch Bereich erfolgreich Einsatz Domänenontologie Sammlungsbetrieb individuell Erweiterunge conceptual Reference Model comité international Pour -- Documentation Einsatz iso Umsetzung owl ebenfalls Projekt besorgt websiter frei Verfügung stehen Poster stellen aktuell Stand Vollendung Projektphasen dar bewährt Modell Anpassung Software beteiligt Museum unterstützen Interessengemeinschaft semantisch Datenverarbeitung gemeinnützig Aspekt Software hinaus bewussen Einsatz Anpassung Wisski einladen Resultierte zahlreich Einsatz Software Forschungsprojekt Kooperation Landesstelle nichtstaatlich Museum Bayern Zentralinstitut Kunstgeschicht System stellen Nutzung views Stärke Beweis altbewährt textfeldern bildern zoomviewer hochauflösend Bild interaktiv Landkarte zeitstrahlen denkbar Medientype sowohl direkt Ansicht download Funktionalität nutzen zusätzlich gängigen formaten ermöglichen Standardkonformität Einbindung anderer gängig Feldtypmodule Drupal Verfügung stehen erwähnt erleichterung zählen Update Pathbuilder Pfadschablon Domänenontologie graphisch Oberfläche auswählen erzeugen umfassend Bibliothek Musterontologien bereitstellen Einstiegshürde Erstbenutzer minimal halten,"[('software', 0.2400009674892648), ('domänenontologie', 0.19923216385784892), ('wisski', 0.19923216385784892), ('forschungsumgebung', 0.16221344591437412), ('system', 0.1559648303284048), ('technologie', 0.14178916104221606), ('daten', 0.1410209351584746), ('pathbuilder', 0.1328214425718993), ('drupal', 0.1328214425718993), ('gängig', 0.12164451171012486)]"
2018,DHd2018,DECLERCK_Thierry_Formalisierung_von_Märchen.xml,Formalisierung von Märchen,"Thierry Declerck (DFKI GmbH, Deutschland); Anastasija Aman (Universität des Saarlandes, Deutschland); Stefan Grünewald (Universität des Saarlandes, Deutschland); Matthias Lindemann (Universität des Saarlandes, Deutschland); Lisa Schäfer (Universität des Saarlandes, Deutschland); Natalia Skachkova (Universität des Saarlandes, Deutschland)","Formalisierung, XML, Python, Märchen","Programmierung, Inhaltsanalyse, Modellierung, Projektmanagement","Im Rahmen eines Softwareprojektes Wir beschreiben in diesem Beitrag zum einen, welche Informationen in dieser formalen Repräsentation enthalten sind, und zum anderen, wie diese Informationen in XML bzw. Python konkret codiert werden. Ein Märchen besteht im Sinne unseres Projektes aus den folgenden Bestandteilen: Im Folgenden werden die verschiedenen Bestandteile, sowie ihre Eigenschaften und Beziehungen untereinander, näher beschrieben.   Die oben beschriebenen Informationen lassen sich im XML-Format darstellen. Dabei wird eine XML-Baumstruktur genutzt, um die Hierarchie der verschiedenen Objekte zu repräsentieren. Das Wurzelelement des Dokuments hat stets den Bezeichner Tale und die Attribute ""title"" und ""annotator"", welche Titel und den Namen des Annotators des jeweiligen Märchens enthalten: <Tale title=""Froschkönig"" annotator=""Lisa Schäfer""> ... </Tale> Diesem Element untergeordnet sind die Elemente Characters, Locations und Text. Das Characters-Element enthält Character-Subelemente, die jeweils die gesammelten Informationen für einen Charakter speichern: <Character id=""ch1"" name=""Frosch"" age=""adult"" gender=""male"" type=""animal_monster"" subtype=""small"" attitude=""neutral"" archetype=""hero""> </Character> Analog dazu enthält das Locations-Element untergeordnete Location-Elemente, die jeweils einen Ort codieren: <Location id=""loc1"" type=""WALD""> </Location> Das Text-Element enthält schließlich den eigentlichen Märchentext. Dieser ist auf die verschiedenen Szenen 'repräsentiert durch Szene-Elemente 'aufgeteilt, welche wiederum die verschiedenen Dialogakte (Dialog-Elemente) enthalten: <Text><Scene id=""s2"" time=""t2"" location=""loc1"" characters=""ch1,ch2"" propp_functions=""d|e"" propp_subfunctions=""D7|E10"" transition=""gehen""> ... <Dialogue id=""d5"" time=""t2.4"" speaker=""ch2"" receiver=""ch1""> Ach, du bist""s, alter Wasserpatscher, </Dialogue> ... </Scene></Text> Beim Entwurf des XML-Schemas wurde besonders Wert auf Übersichtlichkeit und Leserlichkeit gelegt. Trotz der Vielzahl der kodierten Informationen sind die resultierenden XML-Dateien daher vergleichsweise kompakt; so besteht die XML-Repräsentation des (vergleichsweise langen) Märchens "" Diese XML Repräsentation basiert auf und erweitert das Annotation Schema, das in (Scheidel & Declerck, 2010) beschrieben wird. Auf der Grundlage der oben beschriebenen XML-Struktur kann eine Python-Klassenstruktur aufgebaut werden, die ein Märchen sowie seine einzelnen Teile als Python-Objekte repräsentiert. Neben einer Oberklasse Tale gibt es für jeden der oben beschriebenen Teile eine eigene Python-Klasse, d. h. die Klassen Location, Character, Scene und Dialogue. (Insgesamt bestehen die Dateien zur Märchen-Repräsentation aus 288 Zeilen Code.) Jede Klasse enthält dabei als Attribute die oben beschriebenen Eigenschaften, wobei diese auch Verweise auf andere Elemente darstellen können. So verweisen bspw. Dialogue-Objekte auf die Character-Objekte von Sprecher und Empfängern. Der Python-Code dient als Interface für drei Anwendungen. Erstens können Märchen aus bestehenden XML-Dateien eingelesen werden; zweitens können XML-Dateien anhand einer anderweitig (z. B. durch automatische Klassifizierung) erzeugten Python-Märchenstruktur generiert werden; und drittens kann anderer Python-Code auf die Märchen-Information zugreifen, was die Grundlage für Anwendungen wie Text-to-Speech oder Visualisierung bildet. Sowohl die XML Kodierung als auch die Python Objekte interagieren mit einer Märchen-Ontologie interagieren, die eine Erweiterung der in (Koleva et al., 2012) beschriebenen Ontologie ist. Somit haben wir eine formale Repräsentation von Märchen, die in verschiedenen Anwendungen zum Tragen kommen kann.",de,Rahmen softwareprojekt beschreiben Beitrag Information formal Repräsentation enthalten Information xml python konkret codieren märchen bestehen Sinn unser Projekte folgend bestandteilen folgend verschieden Bestandteil eigenschaften Beziehung untereinander nah beschreiben beschrieben Information lassen darstellen nutzen Hierarchie verschieden Objekt repräsentieren Wurzelelement Dokument stets Bezeichner Tale attribut title Annotator Titel Name Annotator jeweilig Märchen enthalten tale Schäfer Element untergeordnen element Character Location Text enthalten jeweils gesammelt Information Charakter Speicher character analog enthalten untergeordnet jeweils Ort Codier Location enthalten schließlich eigentlich Märchentext verschieden Szene repräsentieren aufteilen wiederum verschieden dialogakt enthalten dialogue s alt wasserpatsch Entwurf wert übersichtlichkeit Leserlichkeit legen trotz Vielzahl kodierter Information resultierend vergleichsweise kompakt bestehen vergleichsweise lang Märchen xml Repräsentation basieren erweitern Annotation Schema Scheidel Declerck beschreiben Grundlage beschrieben aufbauen märch einzeln Teil repräsentieren Oberklasse tale beschrieben Teil klass Location character Scene dialogue insgesamt bestehen datei zeil Code Klasse enthalten attribut beschriebenen eigenschaften wobei verweise element darstellen verweisen Sprecher empfängern dienen Interface Anwendung erstens märcher bestehend eingelesen zweitens anhand anderweitig automatisch Klassifizierung erzeugt neriern drittens anderer zugreifen Grundlage Anwendung Visualisierung bilden sowohl xml Kodierung python objekt interagieren interagieren Erweiterung Koleva et beschrieben Ontologie somit formal Repräsentation Märchen verschieden Anwendung tragen,"[('märchen', 0.2899164146493135), ('location', 0.24874489832552688), ('tale', 0.22567228362066807), ('enthalten', 0.21849558417355353), ('beschrieben', 0.17875506836226246), ('character', 0.16537388861023855), ('information', 0.15688210757920829), ('dialogue', 0.15044818908044538), ('xml', 0.1478595661554811), ('element', 0.14627458595123569)]"
2018,DHd2018,REITER_Nils_SANTA__Systematische_Analyse_Narrativer_Texte_du.xml,SANTA: Systematische Analyse Narrativer Texte durch Annotation,"Evelyn Gius (Universität Hamburg, Deutschland); Nils Reiter (Universität Stuttgart, Deutschland); Jannik Strötgen (Max-Planck-Institut für Informatik, Saarbrücken, Deutschland); Marcus Willand (Universität Stuttgart, Deutschland)","shared task, narratologie, annotationsrichtlinien","Inhaltsanalyse, Modellierung, Annotieren, Theoretisierung, Community-Bildung, Literatur","In diesem Beitrag wollen wir ein Vorhaben zur Diskussion stellen, das an zwei zentralen Herausforderungen in den Digital Humanities ansetzt: Der Erstellung adäquater Annotationsrichtlinien für geisteswissenschaftlich relevante textuelle Konzepte und der Schnittstelle in der Kooperation zwischen beteiligten Wissenschaftlerinnen und Wissenschaftlern aus Geisteswissenschaft und Informatik. Für DH-Projekte sind Kooperationen unerlässlich, wenn fortgeschrittene Techniken zur Textanalyse eingesetzt werden und/oder es um eine Zusammenführung von Konzepten oder Zugangsweisen geht, die bereits intradisziplinär als komplex gelten. Dabei wird ein signifikanter Anteil der Projektlaufzeit auf die Entwicklung einer ""gemeinsamen Sprache"" und die Identifikation der exakten, gemeinsamen wissenschaftlichen Fragestellung verwendet. Dies ist zweifellos ein produktiver Prozess, dessen erfolgreiche Durchführung allerdings voraussetzt, dass auf beiden Seiten Forscherinnen und Forscher beteiligt sind, die sich auf das interdisziplinäre Vorgehen voll einlassen und auch den nötigen Zeitaufwand tragen. Methodisch-technisch ist ein substanzielles Nadelöhr bei der Entwicklung automatischer Werkzeuge das Fehlen von annotierten Goldstandards, an/auf denen Werkzeuge trainiert, verglichen und feinjustiert werden können. Das Fehlen der Goldstandards ist jedoch eigentlich ein nachgelagertes Problem, wie sich z.B. in narratologisch orientierten Projekten zeigt (heureCLéA: Bögel et al., 2015; Propp annotation: Fisseni et al., 2014): Die Umsetzung narratologischer Theorien als Annotationen ist alles andere als trivial, da narratologische Konzepte nicht im Hinblick auf Annotation entwickelt wurden. Leerstellen in den Definitionen müssen gefüllt, Voraussetzungen geklärt und Unterkategorien geklärt werden. Die Annotation solcher Kategorien ist also kein reiner Umsetzungs- oder Implementierungsprozess, sondern einer bei dem sich tiefe, konzeptionelle Fragen stellen. Als Ergebnis solcher Prozesse stehen dann Annotationsrichtlinien, die die Brücke zwischen Theorie und Praxis schlagen. Erst wenn Annotationsrichtlinien für ein Phänomen (oder eine Gruppe von Phänomenen) etabliert sind, können größere Annotationsprojekte mit Aussicht auf Erfolg durchgeführt werden. Das von uns vorgeschlagene Vorgehen erlaubt den Beteiligten Forscherinnen und Forschern ihre Expertise einzubringen, ohne in einem gemeinsamen Projektkontext zu arbeiten. Die Schnittstelle zwischen D und H wird hierbei von annotierten Daten und Annotationsrichtlinien gebildet, wobei die Richtlinien ohne Kompromisse bezüglich möglicher Automatisierungen erstellt werden. Das Vorhaben gibt somit auch narratologisch/literaturwissenschaftlich anspruchsvoller Konzeptentwicklung und damit Theoriebildung einen Rahmen. Verfügbare annotierte Daten wiederum erlauben Informatikerinnen und Informatikern ohne Expertise in narratologischen Fragen die Entwicklung von Werkzeugen für komplexe technische Probleme. Shared Tasks sind in der Computerlinguistik weit verbreitet und haben für viele Bereiche gezeigt, dass sie ein geeignetes Instrument sind, Forschungsbemühungen verschiedener Gruppen zum gleichen Thema zu bündeln und zu verstärken. In einem Als Rahmen für die Entwicklung von Annotationsrichtlinien organisieren wir einen shared task der sich genau auf dieses Ziel konzentriert (Phase 1: Erstellung von Guidelines). Sind die Richtlinien etabliert, kann anschließend ein großes Korpus annotiert werden, das wiederum in einem NLP-shared task eingesetzt werden kann, um Verfahren zu erproben, die die annotierten Phänomene automatisch finden (Phase 2: Automatisierung). Als Phänomen haben wir uns dabei auf Erzählebenen in englischen und deutschen Texten festgelegt, da diese für zahlreiche, komplexere literaturwissenschaftliche Fragestellungen hilfreich sind, ohne selbst (für einen ersten Während Details zum Gesamtaufbau des Shared Tasks bereits in einem anderen Artikel beschrieben wurden (Reiter et al., 2017), fokussieren wir uns in diesem Beitrag auf die genauere Beschreibung der ersten Phase des (bis Mitte Juni 2018) Im ersten Schritt wird allen Teilnehmerinnen und Teilnehmern ein Die Texte können und sollen von den Teilnehmerinnen und Teilnehmern benutzt werden, um Richtlinien für die Annotation von Erzählebenen zu entwickeln und zu testen. Ob die Texte in einer oder in beiden Sprachen verwendet werden, ist dabei den Teilnehmerinnen und Teilnehmern überlassen. Sie sollten dabei das Ziel verfolgen, eine möglichst breite Anwendbarkeit der Richtlinien sicherzustellen (auch jenseits des Wie genau die Gruppen dabei vorgehen, bleibt ihnen überlassen. In vergangenen Annotationsprojekten (mit und ohne Bezug zu Literaturwissenschaft bzw. literarischen Texten) hat sich aber ein iterativer Prozess als fruchtbar erwiesen. Sobald eine erste Version der Richtlinien erstellt wurde, werden sie auf neuen Texten getestet, um ihre Definitionslücken oder Vagheiten zu identifizieren. Aus dem Schließen der Lücken ergibt sich dann eine weitere Version der Richtlinien, die wiederum auf Texten getestet werden können. (bis Ende Juni 2018) Im zweiten Schritt sollen die Arbeitsgruppen ihre eigenen Richtlinien auf neuen Texten testen. Nach dem Einreichen ihrer Richtlinien erhalten die Teilnehmerinnen und Teilnehmer hierzu sechs neue literarische Texte, die vom Organisationsteams des (bis Mitte Juli 2018) Im dritten Schritt erhält jede teilnehmende Gruppe Richtlinien anderer Gruppen, auf deren Basis Erzählebenen in den sechs Texten erneut annotiert werden, wobei alle Richtlinien von uns zuvor anonymisiert werden. Zusätzlich wird auch eine vom Organisationsteam betreute Gruppe von studentischen Hilfskräften alle eingereichten Annotationsrichtlinien auf den sechs Texten anwenden. (August/September 2018) Im letzten Schritt der ersten Phase des Zur vergleichenden Evaluation von Annotationsrichtlinien sind bisher Ansätze aus der Computer- und Korpuslinguistik zur quantitativen Messung des Inter-Annotator-Agreement (IAA) bekannt (vgl. Artstein, 2017), die im Bereich der Digital Humanities angewendet wurden und werden. Da es aber bei der Erstellung von Annotationsrichtlinien für narratologische Phänomene eben nicht Im Rahmen des Vortrags wollen wir insbesondere zwei der o.g. Aspekte in den Fokus rücken und diskutieren: Die iterative Entwicklung von Annotationsrichtlinien als verteiltes, kollaboratives Projekt sowie die Evaluation und Vergleichbarkeit von Annotationsrichtlinien für literarische Phänomene.",de,Beitrag Vorhaben Diskussion stellen zentral Herausforderung Digital Humanitie ansetzt Erstellung adäquat Annotationsrichtlinie geisteswissenschaftlich relevant textuell Konzept Schnittstelle Kooperation beteiligt wissenschaftlerinn Wissenschaftler Geisteswissenschaft Informatik kooperation unerlässlich fortgeschritten Technik Textanalyse einsetzen Zusammenführung konzepten zugangsweisen intradisziplinär Komplex gelten signifikant Anteil Projektlaufzeit Entwicklung gemeinsam Sprache Identifikation exakt gemeinsam wissenschaftlich Fragestellung verwenden zweifellos produktiv prozess erfolgreich Durchführung voraussetzen Seite Forscherinn Forscher beteiligen interdisziplinär vorgehen voll einlassen nötig Zeitaufwand tragen substanzielle Nadelöhr Entwicklung automatisch Werkzeug fehlen Annotiert goldstandards Werkzeug trainieren vergleichen feinjustieren fehlen goldstandards eigentlich nachgelagert Problem narratologisch orientiert Projekt zeigen Heurecléa Bögel et Propp Annotation fisseni et Umsetzung narratologischer Theorie annotation trivial narratologisch Konzept Hinblick Annotation entwickeln leerstellen Definition füllen Voraussetzung klären unterkategorien klären Annotation Kategori Reiner Implementierungsprozess tief konzeptionell Frage stellen Ergebnis prozesse stehen Annotationsrichtlinien Brücke Theorie Praxis schlagen Annotationsrichtlinien Phänomen Gruppe Phänomen etablieren groß Annotationsprojekt Aussicht Erfolg durchführen vorgeschlagen vorgehen erlauben beteiligt Forscherinn forschern Expertis einzubringen gemeinsam Projektkontext arbeiten Schnittstelle d h hierbei annotierter daten Annotationsrichtlini bilden wobei Richtlinie kompromisse bezüglich möglich Automatisierung erstellen Vorhaben somit narratologisch literaturwissenschaftlich anspruchsvoll Konzeptentwicklung Theoriebildung Rahmen verfügbar annotiert daten wiederum erlauben Informatikerinn Informatikern Expertise narratologisch Frage Entwicklung Werkzeug Komplex technisch Problem Shared Tasks Computerlinguistik verbreiten Bereich zeigen geeignet Instrument forschungsbemühung verschieden Gruppe gleich Thema bündeln verstärken Rahmen Entwicklung Annotationsrichtlini organisieren shared Task genau Ziel konzentrieren Phase Erstellung guidelin Richtlini etablieren anschließend korpus annotiert wiederum Task einsetzen Verfahren erproben annotiert phänomen automatisch finden Phase Automatisierung Phänom Erzähleben englisch deutsch Text festlegen zahlreich komplex literaturwissenschaftlich Fragestellung hilfreich Detail Gesamtaufbau Shared Tasks Artikel beschreiben Reiter et fokussieren Beitrag genau Beschreibung Phase Mitte Juni Schritt Teilnehmerinn Teilnehmer Text teilnehmerinn teilnehmern benutzen Richtlinie Annotation erzählebenen entwickeln testen Text Sprache verwenden teilnehmerinn Teilnehmer überlassen Ziel verfolgen möglichst breit Anwendbarkeit Richtlinie sicherstellen jenseits genau Gruppe vorgehen bleiben überlassen Annotationsprojekte Bezug Literaturwissenschaft literarisch Text iterativ Prozess fruchtbar erweisen sobald Version Richtlinie erstellen Text testen definitionslücken vagheiten identifizieren Schließe Lücke ergeben Version Richtlinie wiederum Text testen Juni Schritt arbeitsgruppen Richtlinie Text testen einreichen Richtlinie erhalten Teilnehmerinn Teilnehmer hierzu literarisch Text Organisationsteams Mitte Juli Schritt erhalten teilnehmend Gruppe Richtlinie anderer Gruppe Basis Erzähleben Text erneut annotiert wobei Richtlinie zuvor anonymisiert zusätzlich Organisationsteam betreut Gruppe studentisch Hilfskräften eingereicht Annotationsrichtlinie texten anwenden August September letzter Schritt Phase vergleichend Evaluation Annotationsrichtlini Ansatz Korpuslinguistik quantitativ Messung iaa artstein Bereich Digital Humanitie anwenden Erstellung Annotationsrichtlinien narratologisch Phänomen Rahmen Vortrag insbesondere Aspekt Fokus rücken diskutieren iterativ Entwicklung Annotationsrichtlinie verteilt kollaborativ Projekt evaluation Vergleichbarkeit Annotationsrichtlinien literarisch Phänomen,"[('richtlinie', 0.3374611114673152), ('gruppe', 0.18725430478150945), ('teilnehmerinn', 0.18525826652185567), ('annotationsrichtlinien', 0.18525826652185567), ('narratologisch', 0.18195104915394159), ('shared', 0.14354000087355173), ('phase', 0.14158192986150256), ('annotationsrichtlinie', 0.13145027362280232), ('phänomen', 0.12913063464406077), ('testen', 0.1274019911420233)]"
2018,DHd2018,PIELSTRÖM_Steffen__Embedded_Humanities.xml,Embedded Humanities,"Fotis Jannidis (Universität Würzburg, Deutschland); Mike Kestemont (Universität Antwerpen, Belgien)","distributional semantics, word2vec, topic modeling","Inhaltsanalyse, Strukturanalyse, Modellierung","In recent years, the Digital Humanities have witnessed the steadily growing popularity of models from distributional semantics which can be used to model the meaning of documents and words in large digital text collections. Well-known examples of influential distributional models include Latent Dirichlet Allocation for topic modelling (Blei et al.) or Word2vec for estimating word vectors (Mikolov et al. 2013). Such distributional models have recently gained much prominence in the fields of Natural Language Processing and, more recently, Deep Representation Learning (Manning 2016). Humanities data is typically sparse and distributional models help scholars obtain smoother estimations of them. Whereas, for instance, words are conventionally encoded as binary ""one-hot vectors"" in digital text analysis, embedding techniques from distributional semantics allow scholars to obtain dense, yet rich representations of vocabularies. These embedded representations are known to capture all sorts of valuable relationships between data points, although embedding techniques are typically trained using unsupervised objectives and require relatively little parameter tuning from scholars. Inspiring applications of this emergent technology in DH have ranged from more technical work in cultural studies at large (Bamman et al. 2014), case studies in literary history (Mimno 2012; Schoech 2017) or valuable DH-oriented web apps, such as ShiCo (Martinez-Ortiz et al. 2016). The availability of high-quality implementations in the public domain, in software suites as gensim, word2vec, or mallet etc. has greatly added these methods"" popularity. In spite of their huge potential for Digital Humanities, multiple aspects of their application still remain untapped. Unsupervised models such as Word2vec, for instance, are notoriously hard to evaluate directly 'often researchers have to resort to indirect evaluations in this respect. This renders it intriguing to which extent the output of distributional models should play a decisive role in hermeneutical debates or controversies in the Humanities. With other techniques for Distant Reading, distributional models moreover share the drawback that they typically only yield a The DARIAH working group on Text and Data Analytics (@dariahtdawg), in collaboration with the FWO-sponsored scientific community Digital Humanities Flanders (DHuF) proposes to collocate a one-day workshop with the 2018 DHd conference in Cologne. The workshop aims to bring together ca.  10-12 practitioners from the Digital Humanities to present and discuss recent advances in the field, through 30-minute presentations on focused case studies, including work-in-progress or theoretical contributions. Additionally, the workshop aims to reach an audience of non-presenting participants who take an active interest in distributional models and who are planning to apply distributional models to their own data in the near future. We aim to bring together a diverse group of both junior and senior stakeholders in this nascent subfield of DH. The goal of the workshop is to identify the state of the art in the field, identify common challenges and share recommendations for a best practice. Special attention will be given to the (both hermeneutic and quantitative) evaluation of distributional models in the context of Humanities research, which remains a challenging issue. The workshop is open to scholars from all backgrounds with an interest in semantic representation learning and encourages submissions that deal with under-researched resource-scarce and/or historic languages. Abstracts (between 250 and 300 words, not including references) can be submitted to mike.kestemont@uantwerp.be. The¬†workshop¬†also explicitly welcomes submissions presenting previously published research which is of interest to the DH community (although this work should not overlap strongly with work presented at the main conference). Topics which seem of special interest to the DH community nowadays include, but are not limited to: In terms of technical requirements, the workshop would need a beamer to project from a laptop. As keynote speaker, we have found David Bamman (University of California, Berkeley) willing to join our workshop and give a plenary lecture. Bamman is an authority in the field and will certainly increase the attractiveness of the workshop to potential participants. Fotis Jannidis (University of Würzburg, Germany) Fotis.jannidis@uni-wuerzburg.de, Fotis holds the chair for literary computing in the department of German studies at the University of Würzburg. In the last years, the main focus of his work is the computational analysis of larger collections of literature, especially narrative texts. He is interested in developing new research methods for this new subfield of literary studies, but also in new applications for established methods and also in a better understanding, why successful algorithms in this field work. Mike Kestemont (University of Antwerp, Belgium) mike.kestemont@uantwerp.be,  Mike is a tenure track research professor in the department of Literature the University of Antwerp. He specializes in computational text analysis for the Humanities, in particular stylometry or computational stylistics. He has published on the topic of authorship attribution in various fields, such as Classics or medieval European literature. Mike actively engages in the debate surrounding the Digital Humanities and attempts to merge methods from Artificial Intelligence with traditional scholarship in the Humanities. He recently took up an interest in so-called ""deep"" representation learning using neural networks.",en,recent year digital humanity witness steadily grow popularity model distributional semantic model meaning document word large digital text collection know example influential distributional model include latent dirichlet allocation topic modelling blei et al estimate word vector mikolov et al distributional model recently gain prominence field natural language processing recently deep representation learning man humanity datum typically sparse distributional model help scholar obtain smoother estimation instance word conventionally encode binary hot vector digital text analysis embed technique distributional semantic allow scholar obtain dense rich representation vocabulary embed representation know capture sort valuable relationship datum point embed technique typically train unsupervised objective require relatively little parameter tune scholar inspire application emergent technology dh range technical work cultural study large bamman et al case study literary history mimno schoech valuable dh orient web app shico martinez ortiz et al availability high quality implementation public domain software suite gensim mallet etc greatly add method popularity spite huge potential digital humanity multiple aspect application remain untapped unsupervise model instance notoriously hard evaluate directly researcher resort indirect evaluation respect render intriguing extent output distributional model play decisive role hermeneutical debate controversy humanity technique distant reading distributional model share drawback typically yield dariah working group text data analytic collaboration fwo sponsor scientific community digital humanity flander dhuf propose collocate day workshop dhd conference cologne workshop aim bring practitioner digital humanity present discuss recent advance field minute presentation focus case study include work progress theoretical contribution additionally workshop aim reach audience non presenting participant active interest distributional model plan apply distributional model datum near future aim bring diverse group junior senior stakeholder nascent subfield dh goal workshop identify state art field identify common challenge share recommendation good practice special attention give hermeneutic quantitative evaluation distributional model context humanity research remain challenging issue workshop open scholar background interest semantic representation learning encourage submission deal research resource scarce historic language abstract word include reference submit explicitly welcome submission present previously publish research interest dh community work overlap strongly work present main conference topic special interest dh community nowadays include limit term technical requirement workshop need beamer project laptop keynote speaker find david bamman university california berkeley willing join workshop plenary lecture bamman authority field certainly increase attractiveness workshop potential participant fotis jannidis university würzburg germany fotis hold chair literary computing department german study university würzburg year main focus work computational analysis large collection literature especially narrative text interested develop new research method new subfield literary study new application establish method well understanding successful algorithm field work mike kestemont university antwerp belgium mike tenure track research professor department literature university antwerp specialize computational text analysis humanity particular stylometry computational stylistic publish topic authorship attribution field classic medieval european literature mike actively engage debate surround digital humanity attempt merge method artificial intelligence traditional scholarship humanity recently take interest call deep representation learning neural network,"[('distributional', 0.36313079876262144), ('humanity', 0.28374373861455704), ('model', 0.23005599623114847), ('interest', 0.1720808687062031), ('field', 0.16303313718653029), ('workshop', 0.15942564256748917), ('work', 0.1390528582351935), ('representation', 0.13872960772559564), ('study', 0.13077878581285654), ('university', 0.1263764169803343)]"
2018,DHd2018,KRAUTTER_Benjamin_Quantitatives__close_reading___Vier_mikroa.xml,"Quantitatives ""close reading""? Vier mikroanalytische Methoden der digitalen Dramenanalyse im Vergleich.","Benjamin Krautter (Universität Stuttgart, Deutschland)","Stilometrie, Kopräsenz, Drama, Wortfeldsemantik, Sentiment","Inhaltsanalyse, Strukturanalyse, Kontextsetzung, Stilistische Analyse, Literatur","Jüngste Ergebnisse der computergestützten Forschung legen nahe, dass Romanfiguren 'gemessen an ihrer Figurenrede 'von den jeweiligen Autoren stilistisch distinktiv angelegt werden können (Hoover 2017; Fields, Bassist, Roper 2017). Versierte Autoren könnten ihren Figuren also sogenannte ""distinctive voices"" einschreiben, die sich stilometrisch identifizieren lassen. Anders als bei Autorschafts-, Gattungs- oder Epochensignalen handelt es sich hierbei um ein Erstaunlicherweise beschränken sich die Studien zur stilistischen Differenzierung von Figurenrede größtenteils auf Romane. Dabei ist es doch gerade die Struktur dramatischer Texte, die eine quantitative Untersuchung der Figurenrede plausibel erscheinen lässt 'die Rede wird nicht von einem Erzähler sortiert, kommentiert und in einen Rahmen gebettet. Auch erste Forschungsansätze sind durchaus vorhanden: John Burrows und Hugh Craig zeigen etwa, dass einzelne Dramenfiguren sehr wohl erfolgreich einem Autorsignal zugeordnet werden können (Burrows, Craig 2012). Sie reagieren damit interessanterweise auf Kritiker, die die erfolgreiche Autorschaftsattribution von Dramentexten aufgrund der vielen verschiedenartigen Stimmen 'weil es also keinen Erzähler gibt 'in Frage stellen (Masten 1997). Nachfolgend soll geprüft werden, inwieweit sich Hoovers Vorgehen (2017) zur Ermittlung distinktiver Figurenrede auch auf dramatische Texte übertragen lässt. Die Ergebnisse der stilometrischen Untersuchung werden im Anschluss durch drei weitere quantitative Analyseverfahren kontextualisiert und zugleich kritisch hinterfragt.  Die stilometrische Analyse von Ein einzelnes Dendrogram darf für die Bewertung der Hypothese jedoch nicht mehr sein als ein erstes Indiz, zumal die Segmentierung nicht gemäß einer festen Länge mit vorgegebener Wortanzahl, sondern nach den Aktgrenzen vorgenommen wurde. Um ein potentielles ""Cherry Picking""-Problem an dieser Stelle zu vermeiden, ergänzt  Ein Öhnlichkeitssignal, das Figurentypen, etwa den zärtlichen Vater in den bürgerlichen Trauerspielen Lessings, über das einzelne Drama hinweg verbinden würde, ist zumindest auf diese Weise nicht auszumachen. Die Vermutung liegt nahe, schreibt Lessing seine Dramen doch dezidiert für die am Theater üblichen Rollenfächer des 18. Jahrhunderts (Harris 1992). Sie scheint jedoch nicht auf diese Weise stilometrisch operationalisierbar zu sein. Stilometrische Analysen sind nicht das einzige Verfahren, um relative Öhnlichkeiten innerhalb eines Textkorpus zu bestimmen. Inwieweit sie geeignet sind, offene Fragestellungen 'im Gegensatz etwa zur Autorschaftsattribution 'zu erörtern, ist überhaupt noch zu prüfen. Sollten Parameter wie Distanzmaß, Wortumfang oder Culling tatsächlich je nach Textkorpus neu zu bestimmen sein, wären ""Cherry Picking""-Probleme der Methode inhärent (Schöch 2014; Jannidis 2014; Eder 2013). Nachfolgend ist es deshalb geboten, die bisherigen Beobachtungen weiteren quantitativen Verfahren gegenüberzustellen. Dazu dienen Analysen der Kopräsenz, der Figurensemantik und der Empfindung, sogenannte Sentiment-Analysen.  Die Tabelle in Um diesen Befund weiter zu spezifizieren, soll eine semantische Wortfeldanalyse die thematische Konzeption der Figurenrede operationalisieren (Willand, Reiter 2017). Während also Die Sentiment-Analyse in  Die nähere Betrachtung der Figurenrede in Die vorliegende Arbeit wurde im Rahmen des Projekts ""Quantitative Drama Analytics"" (QuaDramA) durchgeführt, das von der VolkswagenStiftung finanziert wird.",de,jung Ergebnis computergestützt Forschung legen nahe romanfiguren messen Figurenrede jeweilig Autor stilistisch distinktiv anlegen hoov Field bassisen Roper versiert Autor können Figur sogenannter distinctiv Voices einschreiben stilometrisch identifizieren lassen Epochensignale handeln hierbei erstaunlicherweise beschränken Studie stilistisch Differenzierung Figurenrede größtenteils Romane Struktur dramatisch Text quantitativ Untersuchung Figurenrede Plausibel erscheinen lässn Rede Erzähler sortieren kommentieren Rahmen betten Forschungsansätz vorhanden john Burrow Hugh Craig zeigen einzeln dramenfiguren erfolgreich autorsignal zuordnen burrow Craig reagieren interessanterweise Kritiker erfolgreich Autorschaftsattribution dramentexten aufgrund verschiedenartig Stimme Erzähler Frage stellen Mast nachfolgend prüfen inwieweit hoover vorgehen Ermittlung distinktiv Figurenrede dramatisch Text übertragen lässn Ergebnis stilometrisch Untersuchung Anschluss quantitativ analyseverfahren kontextualisieren kritisch hinterfragen stilometrisch Analyse einzeln Dendrogram Bewertung Hypothese Indiz zumal Segmentierung gemäß fest Länge vorgegebener Wortanzahl Aktgrenze vornehmen potentiell Cherry Stelle vermeiden ergänzen Öhnlichkeitssignal figurentypen zärtlich Vater bürgerlich trauerspiel Lessings einzeln Drama Hinweg verbinden zumindest Weise ausmachen Vermutung liegen nahe schreiben Lessing Dram dezidiert Theater üblich Rollenfächer Jahrhundert harris scheinen Weise stilometrisch operationalisierbar stilometrisch Analyse einzig Verfahren relativ öhnlichkeiten innerhalb Textkorpus bestimmen inwieweit eignen offen Fragestellung Gegensatz Autorschaftsattribution erörtern prüfen parameter distanzmaß Wortumfang Culling tatsächlich Textkorpus neu bestimmen sein Cherry Methode Inhärent schöch jannidis ed nachfolgend gebieten bisherig Beobachtung quantitativ Verfahren gegenüberzustellen dienen Analyse Kopräsenz Figurensemantik Empfindung sogenannter Tabell Befund spezifizieren semantisch Wortfeldanalyse thematisch Konzeption Figurenrede Operationalisieren Willand Reiter nah Betrachtung Figurenrede vorliegend Arbeit Rahmen Projekt quantitativ Drama analytics quadrama durchführen Volkswagenstiftung finanzieren,"[('figurenrede', 0.33079744293891533), ('stilometrisch', 0.26638491942097386), ('cherry', 0.16435736328040154), ('craig', 0.14508956477639753), ('autorschaftsattribution', 0.133818625180104), ('quantitativ', 0.13266630871716467), ('distinktiv', 0.12582176627239355), ('burrow', 0.12582176627239355), ('nachfolgend', 0.12254768558381048), ('erzähler', 0.12254768558381048)]"
2018,DHd2018,SCHMIDT_Thomas__Kann_man_denn_auch_nicht_lachend_sehr_ernsth.xml,"""Kann man denn auch nicht lachend sehr ernsthaft sein?"" 'Zum Einsatz von Sentiment Analyse-Verfahren für die quantitative Untersuchung von Lessings Dramen","Thomas Schmidt (Lehrstuhl für Medieninformatik, Universität Regensburg); Manuel Burghardt (Lehrstuhl für Medieninformatik, Universität Regensburg); Katrin Dennerlein (Institut für Deutsche Philologie, Julius-Maximilians-Universität Würzburg)",Sentiment Analysis; Dramenanalyse;,"Inhaltsanalyse, Literatur, Methoden, Text, Werkzeuge","Sentiment Analyse (SA) beschreibt eine Reihe von computergestützten Methoden zur Prädiktion der Polarität eines Texts, versucht also vereinfacht gesagt automatisiert herauszufinden, ob ein Text ein positives oder negatives Gefühl ausdrückt (Liu 2016). Darüber hinaus werden teilweise auch komplexere emotionale Kategorien (wie z.B. Zorn und Freude) betrachtet (Mohammad & Turney 2010). Zentrale Anwendungsfelder der SA sind bislang vor allem die Analyse von Online-Reviews (McGlohan, Glance & Reiter 2010) und Social Media-Daten (Kouloumpis, Wilson & Moore 2011). Zur Analyse von literarischen Texten mittels SA-Techniken finden sich bislang nur wenige Studien, z.B. zu Märchen (Alm, Roth & Sproat 2005) und Romanen (Kakkonen & Kakkonen 2011; Elsner 2012; Jannidis et al. 2016). Auf größeren Textkorpora wurde getestet, inwiefern SA-Werte eines Textes und Emotionskurven von Texten zur Genreklassifikation verwendet werden können (Kim, Padó & Klinger 2017) und wie begriffsgeschichtliche Bedeutungsverschiebungen in literarischen Texten mithilfe von erweiterten SA-Methoden erforscht werden können (Buechel, Hellrich & Hahn 2017). In Dramentexten hat man bisher die Verteilung von emotionalen Kategorien (Mohammad 2011) oder die Entwicklung von Figurenbeziehungen (Nalisnick & Baird 2013) in Shakespeare-Dramen untersucht. Auch der vorliegende Beitrag beschäftigt sich mit dem Einsatz von SA im Bereich der Dramenanalyse. Es werden erstmals systematisch verschiedene Methoden der SA für Dramen getestet und evaluiert. Zudem wird exploriert, inwiefern bisher in der Literaturwissenschaft erforschte Aspekte von Dramen mithilfe der SA erfasst werden und inwiefern die SA auch für die Gewinnung neuer literaturwissenschaftlicher Erkenntnisse eingesetzt werden kann. Das im Rahmen dieser Studie verwendete Lessing-Korpus umfasst ein mit Strukturinformationen annotiertes Dramenkorpus mit 11 Dramen, bestehend aus insgesamt 8224 Einzelrepliken. Sämtliche Dramen wurden über die Plattform Innerhalb der SA unterscheidet man zwei wesentliche Ansätze: (1) die Nutzung maschinellen Lernens und (2) die Verwendung lexikonbasierter Verfahren. Für das erstgenannte Vorgehen ist typischerweise ein mit Sentiment-Informationen annotiertes Trainingskorpus notwendig (D""Andrea et al. 2015), welches für die Dramenanalyse bislang nicht vorliegt. Aus diesem Grund werden in der vorliegenden Arbeit lexikonbasierte Verfahren eingesetzt. Ein Sentiment-Lexikon ist dabei eine Wortliste, in der für jedes Wort Sentiment-Informationen angegeben sind (Liu 2016: 10), also z.B. ob es positiv oder negativ konnotiert ist und in welchem Ausmaß (Polaritätsstärke). Ein derartiges Wort nennt man auch Folgende SA-Optionen wurden in unterschiedlichen Kombinationen systematisch evaluiert:   Alle nachfolgenden Berechnungen wurden bezüglich aller kombinatorischen Möglichkeiten der soeben beschriebenen SA-Parameter durchgeführt. Dabei werden die jeweiligen SA-Metriken nach Term-Zähl-Methodik (Kennedy & Inkpen 2006) berechnet, d.h. ein Text wird hinsichtlich vorhandener SBWs untersucht, positive und negative Wörter ausgezählt und für einen Polaritätswert die positive von der negativen Zahl subtrahiert. SA-Metriken wurden auf folgenden Ebenen über die jeweils zugehörigen Texte kalkuliert: Drama, Akte, Szenen, Repliken sowie Sprecher und Sprecherbeziehungen pro Drama, Akt, Szene und Replik. Die Beziehungen zwischen den Figuren wurden nach einer Heuristik von Nalisnick & Baird (2013) berechnet. Zur systematischen Evaluation der Prädiktionsleistung der verschiedenen SA-Ansätze wurde ein Evaluationskorpus bestehend aus 200 Repliken erstellt. Bei der Auswahl der Repliken wurde darauf geachtet, dass die dramenspezifische Verteilung berücksichtigt wird, längere Dramen sind also mit mehr Repliken vertreten. Ferner wurden nur solche Repliken aufgenommen, die mindestens 19 Wörter umfassen. Diese Länge entspricht etwa -25% des Mittelwerts des Gesamtkorpus und vermeidet damit die Selektion von zu kurzen Repliken. Es wurde insgesamt auf eine gleichmäßige Längenverteilung geachtet. Die Repliken wurden von insgesamt fünf Personen (4 weiblich, 1 männlich; alle jeweils mit Deutsch als Muttersprache) jeweils unabhängig voneinander bezüglich deren Polaritätswirkung bewertet. Die Polarität jeder Replik wurde jeweils sechswertig (sehr negativ, negativ, neutral, gemischt, positiv, sehr positiv) und binär (positiv, negativ) bewertet. Die Annotationen wurden bezüglich des Übereinstimmungsgrades analysiert. Dazu wurden das Übereinstimmungsmaß Fleiss"" Kappa (Fleiss 1971) sowie der Durchschnittswert der prozentualen Übereinstimmung aller Annotatoren und Annotatorinnen berechnet (vgl. Tabelle 1). Man erkennt eine geringe Übereinstimmung für die Bewertungsskala mit sechsstufiger Polarität und eine moderate Übereinstimmung für die binäre Variante. Die Ergebnisse verhalten sich konform zu verwandten Studien bei der Interpretation literarischer Texte (Alm & Sproat 2005). Als finale Annotation für eine Replik wird die binäre Polarität gewählt, die die Mehrheit der Annotatoren und Annotatorinnen ausgewählt haben (Endresultat: 139 negativ, 61 positiv). Als Evaluationsmaße wurden Genauigkeit (accuracy), Recall, Precision und F-Werte (Gon√ßalves et al. 2013) herangezogen. Abb. 1 zeigt einen Ausschnitt aus den je fünf besten Kombinationen pro Lexikon, geordnet nach Genauigkeit. Nachfolgend erfolgt eine überblicksartige Zusammenstellung einiger zentraler Ergebnisse aus der Evaluation: Aufgrund der Tatsache, dass hier ein verhältnismäßig simpler SA-Ansatz gewählt wurde und bereits menschliche Annotatoren und Annotatorinnen Schwierigkeiten mit der Polaritätsbestimmung haben, sind die Ergebnisse insgesamt durchaus positiv zu bewerten. Abschließend wurde auf Basis des besten SA-Ansatzes ein Web-Tool für die SA bei Dramen entwickelt. Dieses bietet interaktive Visualisierungen der Sentiment-Verteilungen und -Verläufe für alle berechneten Ebenen. Neben den SentiWS-Metriken wurden auch die Emotionskategorien des NRC integriert. Über das Tool kann man erste Fallstudien auf Dramen-, Akt-, Szenen-, Repliken-, Sprecher- und Sprecherbeziehungsebene durchführen. Die SA-Komponente ist online verfügbar. Trotz der historischen Differenz stimmen die Ergebnisse der automatischen SA tendenziell mit dem überein, was man in der Dramengeschichte über Bewertungen von Figuren und deren Verhalten weiß. Zusätzlich ist aber ein wichtiger heuristischer Mehrwert zu beobachten: eine Analyse allein auf der Basis von Sentiment-Zuschreibungen führt dazu, dass man das Augenmerk gezielt auf Fakten des Textes richtet, die bisher nicht berücksichtigt wurden. Im Folgenden einige Beispiele für die Bestätigung bekannter Ergebnisse und für Entscheidungen von Analysefragen: Die Analyse von Minna von Barnhelm zeigt, dass die negativen emotionalen Bewertungen insgesamt gegenüber den positiven deutlich überwiegen (vgl. Abb. 2). Dieser Befund bestätigt die bekannte Erkenntnis, dass Lessing das Schema des rührenden Lustspiels verwendet hat. Während die Komik im Stück eher das Ergebnis von Schlussprozessen ist, geht es auf der wörtlichen Ebene überwiegend um ernste Vorwürfe und drohenden Identitäts- und Beziehungsverlust. Es ist verschiedentlich behauptet worden (Saße 1993), Minna und nicht Tellheim sei die lächerliche Figur des Stücks. Die Sympathielenkung auf der wörtlichen Ebene des Textes, die in der unten stehenden Sentimentverteilung pro Akt abgebildet ist, kann dazu herangezogen werden, diese Frage negativ zu bescheiden (vgl. Abb. 3). Es ist eine auffällige Abweichung der Polarität im zweiten Akt erkennbar. In diesem Akt tritt Minna von Barnhelm zum ersten Mal auf, Tellheim jedoch nicht. Die letzte Visualisierung kann genutzt werden die Frage zu diskutieren, warum Emilia in Lessings Drama ""Emilia Galotti"" sterben muss (vgl. Abb. 4). Auffällig ist hier die starke negative Bewertung Emilias im zweiten Akt. Entgegen bisheriger Interpretationen, in denen nur die Intrige des Prinzen und Marinelli dafür verantwortlich gemacht werden, dass Emilia um ihre Tugend fürchten und ihren Vater dazu bringen muss, sie umzubringen, wird dadurch die Abwertung allein durch die Avancen des Prinzen sichtbar, die später sowohl Emilias als auch für Odoardos Einschätzung der Ehrbarkeit Emilias in ihrem zukünftigen Leben bestimmen. Insgesamt sind die ersten Analyse-Ergebnisse über das Web-Tool sehr vielversprechend. Dabei ist zu bedenken, dass über die Verwendung von SA-Lexika ein sehr einfacher SA-Ansatz gewählt wurde. Über ML- oder Hybrid-Ansätze können Besonderheiten der poetischen und veralteten Sprache möglicherweise besser beachtet werden. Ferner ist fraglich, ob eine Reduktion auf das sonst in der SA übliche binäre System positiv/negativ ausreichend ist für komplexe Interpretationen von Emotionen in Dramen. Durch Optimierung des SA-Verfahrens, Ausbau der Funktionen im Front-End und Erweiterung des Tools mit zusätzlichen Dramen sollen künftig Möglichkeiten und Nutzen der SA in der Dramenanalyse weiter exploriert werden.",de,Sentiment Analyse sa beschreiben Reihe computergestützt Methode Prädiktion Polarität texts versuchen vereinfacht automatisieren herausfinden Text positiv negativ Gefühl ausdrücken liu hinaus teilweise komplex emotional Kategorie Zorn Freude betrachten Mohammad Turney zentral Anwendungsfelder Sa bislang Analyse Mcglohan Glance Reiter social kouloumpis Wilson Moore Analyse literarisch Text mittels finden bislang Studie märchen alm Roth sproat Roman Kakkon Kakkon Elsner Jannidis et groß Textkorpora testen inwiefern Text Emotionskurve Text Genreklassifikation verwenden Kim Padó Klinger begriffsgeschichtlich Bedeutungsverschiebung literarisch text Mithilfe erweitert erforschen Buechel hellrich Hahn dramentexten Verteilung emotional kategorien Mohammad Entwicklung Figurenbeziehung Nalisnick baird untersuchen vorliegend Beitrag beschäftigen Einsatz sa Bereich Dramenanalyse erstmals systematisch verschieden Methode Sa Dram testen evaluieren zudem explorieren inwiefern Literaturwissenschaft erforscht Aspekt Dram Mithilfe Sa erfassen inwiefern sa Gewinnung neu literaturwissenschaftlich erkenntnis einsetzen Rahmen Studie verwendet umfassen strukturinformation annotiert Dramenkorpus Dram bestehend insgesamt einzelrepliken sämtlicher Dram Plattform innerhalb sa unterscheiden wesentlich ansätze Nutzung maschinell Lernen Verwendung lexikonbasiert Verfahren erstgenannt vorgehen typischerweise annotiert Trainingskorpus notwendig d andrea et Dramenanalyse bislang vorliegen Grund vorliegend Arbeit lexikonbasiert Verfahren einsetzen Wortliste jeder Wort angeben liu positiv negativ konnotieren Ausmaß polaritätsstärk derartig Wort nennen folgend unterschiedlich kombination systematisch evaluieren nachfolgend Berechnung bezüglich kombinatorisch Möglichkeit soeben beschrieben durchführen jeweilig Kennedy inkpe berechnen Text hinsichtlich Vorhandener Sbws untersuchen positiv negativ Wörter auszählen Polaritätswert positiv negativ Zahl subtrahieren folgend Ebene jeweils zugehörig Text kalkulieren Drama aken Szene Replik Sprecher sprecherbeziehungen pro Drama Akt Szene Replik Beziehung Figur Heuristik Nalisnick Baird berechnen systematisch Evaluation Prädiktionsleistung verschieden Evaluationskorpus bestehend Replike erstellen Auswahl Replike achten dramenspezifisch Verteilung berücksichtigen lang dramen Replike vertreten ferner Replike aufnehmen mindestens Wörter umfassen Länge entsprechen Mittelwert Gesamtkorpus vermeiden Selektion kurz Replike insgesamt gleichmäßig Längenverteilung achten Replike insgesamt Person weiblich männlich jeweils deutsch Muttersprache jeweils unabhängig voneinander bezüglich Polaritätswirkung bewerten Polarität Replik jeweils sechswertig negativ negativ neutral mischen positiv positiv binär positiv negativ bewerten Annotation bezüglich übereinstimmungsgrad analysieren übereinstimmungsmaß fleiss Kappa fleiss durchschnittsweren prozentual Übereinstimmung annotatoren Annotatorinn berechnen Tabelle erkennen gering übereinstimmung Bewertungsskala sechsstufig Polarität moderat Übereinstimmung binär Variante Ergebnis verhalten konform verwandt Studie Interpretation literarisch Text alm sproat final Annotation Replik binär Polarität wählen Mehrheit annotatoren annotatorinnen auswählen Endresultat negativ positiv Evaluationsmaße Genauigkeit accuracy Recall Precision et heranziehen abb zeigen Ausschnitt Kombinatione pro Lexikon ordnen Genauigkeit nachfolgend erfolgen überblicksartig Zusammenstellung Zentraler Ergebnis evaluation aufgrund Tatsache verhältnismäßig simpl wählen menschlich annotatoren annotatorinn Schwierigkeit Polaritätsbestimmung Ergebnis insgesamt positiv bewerten abschließend Basis sa Dram entwickeln bieten interaktiv Visualisierung berechneter ebenen Emotionskategorie Nrc integrieren Tool Fallstudie sprecherbeziehungsebener Durchführ online verfügbar trotz historisch Differenz stimmen Ergebnis automatisch sa tendenziell überein dramengeschichte Bewertung Figur verhalten wissen zusätzlich wichtig Heuristischer mehrweren beobachten Analyse Basis führen Augenmerk gezielt Fakt Text richten berücksichtigen folgend Beispiel Bestätigung bekannt Ergebnis Entscheidung Analysefrag Analyse Minna barnhelm zeigen negativ emotional Bewertung insgesamt positiv deutlich überwiegen abb Befund bestätigen bekannt Erkenntnis Lessing Schema rührend Lustspiel verwenden Komik Stück eher Ergebnis Schlussprozesse wörtlich Ebene überwiegend Ernste Vorwurf drohend Beziehungsverlust verschiedentlich behaupten saß minna tellheim lächerlich Figur Stück Sympathielenkung wörtlich Ebene Text unten stehend Sentimentverteilung pro Akt abbilden heranziehen Frage negativ bescheiden abb auffällig Abweichung Polarität Akt erkennbar Akt treten minna barnhelm Mal Tellheim letzter Visualisierung nutzen Frage diskutieren Emilia Lessings Drama emilian Galotti sterben abb auffällig stark negativ Bewertung emilias Akt entgegen bisherig Interpretation Intrig Prinz marinelli verantwortlich emilian Tugend fürchten Vater bringen umbringen Abwertung Avance prinzen sichtbar sowohl emilias Odoardos Einschätzung Ehrbarkeit emilias zukünftig Leben bestimmen insgesamt vielversprechend bedenken Verwendung einfach wählen Besonderheit poetisch veraltet Sprache möglicherweise beachten ferner fraglich Reduktion Sa üblich binär System positiv negativ ausreichend komplex Interpretation Emotion Dram Optimierung Ausbau Funktion Erweiterung Tool zusätzlich Dram künftig Möglichkeit nutzen sa Dramenanalyse explorieren,"[('sa', 0.43574420204338266), ('negativ', 0.27229733222693037), ('positiv', 0.24125391765114787), ('replike', 0.20084532891352933), ('polarität', 0.15609976508222356), ('dram', 0.14726286335338748), ('replik', 0.13389688594235288), ('akt', 0.13056915510302877), ('binär', 0.11276712397944581), ('minna', 0.11068982446184934)]"
2018,DHd2018,DUNST_Alexander_Hin_zu_einer_Visuellen_Stilometrie__Automati.xml,Hin zu einer Visuellen Stilometrie: Automatische Genre- und Autorunterscheidung in graphischen Narrativen,"Alexander Dunst (Universität Paderborn, Deutschland); Rita Hartel (Universität Paderborn, Deutschland)","Bildanalyse, Stilometrie, Corpus, multimodale Medien, Comics","Bilderfassung, Programmierung, Stilistische Analyse, Bilder, Multimodale Kommunikation","Stilometrische Untersuchungen haben eine lange Tradition in der Literaturwissenschaft und erleben mit der Digitalisierung von Textkorpora und Analysemethoden in den letzten drei Jahrzehnten einen erneuten Aufschwung (Holmes und Calle-Mart√≠n & Miranda-Garc√≠a). Öhnliche Tendenzen sind in den vergangenen Jahren auch in der Kunstgeschichte zu verzeichnen (Kohle; Qu, Taeb & Hughes; Manovich). Im Gegensatz dazu sind stilometrische Untersuchen visueller Erzählungen noch nicht etabliert. In den Bereich visueller, oder multimodaler, Narrative fallen etwa Comics, Filme und Fernsehserien, aber auch zu einem gewissen Grad Computerspiele, und damit viele der populärsten Erzählformen des 20. und 21. Jahrhunderts. Das Fehlen einschlägiger Forschung gründet einerseits auf die technischen Herausforderungen digitaler Bildanalyse, andererseits auf den Fokus auf qualitative und ideologiekritische Zugänge in großen Teilen der Medien- und Kulturwissenschaft. Auf Basis eines Corpus von 138 graphischen Romanen (Comicbüchern in Romanlänge) stellt unser Vortrag eine Methode zur automatischen Genre- und Autorunterscheidung vor. Weiters erörtern wir die Herausforderungen stilometrischer Methoden für Erzählformen, die Text und Bild kombinieren, und diskutieren abschließend die potenzielle Übertragbarkeit des vorgestellten Zugangs auf andere Medien. Die Basis der Untersuchungen stellt das erste repräsentative Corpus graphischer Romane dar, dass sich derzeit im Aufbau befindet (Dunst, Hartel & Laubrock). Zum Zeitpunkt der Untersuchungen im Mai 2017 waren 160 Volltexte digitalisiert; davon wurden aufgrund mangelnder Scanqualität bei einigen Titeln 138 Bücher mit einer Gesamtmenge von rund 33.000 Seiten für die Corpusstudie herangezogen. Der Fokus auf Bildanalyse ergab sich dabei sowohl aus methodischen als auch aus praktischen Überlegungen: einerseits sind Methoden zur stilometrischen Textanalyse bereits etabliert und werden laufend weiterentwickelt. Viele davon lassen sich direkt auf Comicstext anwenden oder dafür adaptieren. Andererseits bereitet eine zuverlässige, automatisierte Textlokalisierung und -Erkennung in Comics weiterhin Probleme. Damit bleibt derzeit nur der Weg über eine semi-automatische und zeitintensive Textannotation. Für größere Korpora bietet sich diese Methode daher nicht an. Drei visuelle Maße stellten die Basis unserer Berechnungen dar. Für all diese Maße wurde das Bild zunächst mithilfe einer linearen Näherung der Luminanz in eine Graustufen-Darstellung des Bildes umgewandelt. Im Anschluss an die Kalkulation dieser Grundmaße berechneten wir aus den Ergebnissen pro Seite die Mediane für jedes einzelne der 138 Werke. Um stilistische Abweichungen innerhalb eines Werkes zu messen, berechneten wir die Standardabweichung von den drei Maßen. Um die stilistische Entwicklung innerhalb eines Werkes zu berechnen, wurden in einem ersten Schritt manuell Inhalts- von Funktionsseiten getrennt und letztere von der Kalkulation ausgeschlossen. Hierzu betrachten wir die Kurvenverläufe der drei visuellen Maße über alle Seiten eines graphischen Romans. Für jedes visuelle Maß ermittelten wir: die Anzahl Extrema je 100 Seiten, die Standardabweichung aller Messwerte, die Standardabweichung innerhalb der lokalen Minima bzw. Maxima, die Regelmäßigkeit einer Kurve (also die Standardabweichung des Abstands in Seiten zweier aufeinanderfolgender Extrema) und eine Klassifikation des Kurven-Beginns und -Endes (die erste/letzte Seite hat einen größeren/kleineren Messwert als die nächste/vorherige Seite). Letztere Berechnungen für die stilistische Entwicklung ergaben sieben Werte je Maß, also insgesamt 21 Ergebnisse pro graphischem Roman. Diese reduzierten wir mit Hilfe von skalierter Hauptkomponentenanalyse (Principal Component Analysis [PCA]). Die errechneten Maße zogen wir im Anschluss für die Analyse folgender literatur-, bzw. kulturwissenschaftlicher Konzepte heran: Genre, Autorschaft, und Publikationsformat. Aus Zeitgründen stellen wir hier nur Teilergebnisse für die ersten beiden Kategorien vor. Unser Korpus enthält sowohl fiktionale als auch nicht-fiktionale Texte, die trotz dieser Unterschiede oftmals als graphische Romane bezeichnet werden. Tatsächlich erscheint es sinnvoller hier von graphischen Narrativen zu sprechen. Dabei handelts es sich um durchgehende Erzählungen im Medium Comics, die sich an ein erwachsenes Publikum wenden und einen Umfang von mehr als 64 Seiten haben, womit sie sich vom traditionellen Seitenformat von Comicbüchern abheben. Einzelnen Büchern wurde auf Basis von Klappentexten, Verlagsinformaionen und Zusammenfassungen eines der folgenden Genres zugewiesen: Graphic Novel, Graphic Memoir, andere nicht-fiktionale Texte (Sachbücher). Texte, denen die Subgenres Superhelden, Science Fiction, Märchen, Fantasy, Mystery und Horror zugewiesen wurden, fassten wir in der Sammelkategorie Graphic Fantasy zusammen. Eine kleine Zahl an Texten, die unter keine dieser Klassifikationen fielen, fassten wir unter dem Begriff ""miscellaneous"". Die Ergebnisse der Untersuchen wurden mit Hilfe von Welch Two Sample T-Tests mit p<0,05 auf ihre statistische Signifikanz untersucht. Dabei zeigte sich, dass sich die Genres Graphic Novel, Graphic Memoir und Graphic Fantasy signifikant voneinander unterscheiden. Im speziellen zeichnen sich graphische Romane und Memoiren durch einen deutlich regelmäßigeren visuellen Stil aus. Titel mit der niedrigsten Anzahl an Flächen und niedriger Standardabweichung von der durchschnittlichen Helligkeit gehören meist zu diesen beiden Genres. Graphische Memoiren sind außerdem deutlich heller als die Erzählungen, die wir unter Graphic Fantasy zusammengefasst hatten. Illustration 1 zeigt, dass sich auch eine historische Entwicklung nachweisen lässt. So werden graphische Memoiren seit Mitte des letzten Jahrzehnts deutlich heller 'eine Entwicklung, die möglicherweise durch die beispielgebende Wirkung von mittlerweile kanonischen Titeln wie Alison Bechdels Öhnlich wie im Fall der Genreunterscheidungen zeigten sich signifikante visuelle Unterschiede zwischen unterschiedlichen Autoren. Illustration 2 visualisiert die stilistische Handschrift von Künstlern, die mir mehr als drei Titeln in unserem Corpus vertreten sind. Jene, die einen von Werk zu Werk vergleichsweise konstanten Stil bevorzugen, nehmen dabei eine kleinere Fläche in der Matrix ein. Darunter fallen etwa die bekannten Autoren Jason Lutes, Chester Brown und der japanische Manga-Autor Ozamu Tezuka. Hingegen nehmen Frank Miller, Dave McKean und David Mazzuchelli aufgrund ihrer visuellen Bandbreite eine größere Fläche im stilistischen Raum (Manovichs ""style space"") graphischer Erzählungen ein. Die Einbeziehung von Tezuka zeigt als Testfall aber auch die momentanen Grenzen dieser Methode auf. Die verwendeten Maße erlauben keine Differenzierung zwischen dem angloamerikanischen graphischen Roman und dem japanischen Manga, zwei Erzählformen, die trotz gegenseitiger Beeinflussung deutliche stilistische Unterschiede aufweisen. In diesem Vortrag haben wir automatische Bildanalysen vorgestellt, die stilometrische Unterscheidungen zwischen Genres und Autoren auf den Bereich visueller Erzählungen übertragen. Dazu ist anzumerken, dass sich dieser Zugang in einem experimentellen Stadium befindet. So werden wir demnächst zusätzliche visuelle Maße zu den hier verwendeten erproben. In einem ersten Schritt ist dabei an Farbigkeitsmaße gedacht. Die Datenbasis wird mit Fortschreiten der Retrodigitalisierung unseres Corpus auf rund 250 Werke erweitert. Bei den Genreunterscheidungen hat sich die relativ aufwändige Berechnung der internen Entwicklung eines Werkes als wenig aussagekräftig herausgestellt. Allerdings fließt diese bei den Autorenunterscheidungen in die PCA ein. Auch die Genrekategorien bedürfen einer weiteren Verfeinerung: bei Versuchen, die interne Kohärenz der Genres zu berechnen (Distanzmaße vom Zentroiden eines Clusters) hat sich die Sammelkategorie Graphic Fantasy als weniger kohärent als eine zufällige Vergleichsgruppe herausgestellt. Hier sollten bei neuerlichen Berechnungen Subgenres wie Superheldennarrative herangezogen werden. Als graphische Kunstwerke, die von Hand gezeichnet werden, unterscheiden sich Comics deutlich von Filmen, Fernsehserien, aber auch von Computerspielen. Dennoch birgt die hier präsentierte Methode unserer Meinung nach Potenzial für diese Medien, da Fragen zur Stilistik von Genre und Autorschaft auch dort eine Rolle spielen. So könnte etwa erforscht werden, ob sich das Genre des Film Noir tatächlich stilistische Kohärenz aufweist, oder ob sich komplexe Qualitätsserien wie",de,stilometrisch Untersuchung Tradition Literaturwissenschaft erleben Digitalisierung Textkorpora analysemethoden letzter Jahrzehnt erneut Aufschwung holmes öhnlich Tendenze Kunstgeschichte verzeichnen Kohle qu taeb hughes manovich Gegensatz stilometrisch untersuch Visueller Erzählung etablieren Bereich visuell Multimodaler Narrative fallen Comics Film Fernsehserie gewiß Grad Computerspiele populär Erzählforme Jahrhundert fehlen einschlägig Forschung gründen einerseits technisch Herausforderung Digitaler Bildanalyse andererseits Fokus qualitativ ideologiekritisch Zugäng Teil Kulturwissenschaft Basis Corpus graphisch Roman Comicbüchern romanlänge stellen Vortrag Methode automatisch Autorunterscheidung weiters erörtern Herausforderung stilometrisch Methode Erzählforme Text Bild kombinieren diskutieren abschließend potenziell übertragbarkeit vorgestellt Zugang Medium Basis Untersuchung stellen repräsentativ Corpus graphisch Roman dar derzeit Aufbau befinden Dunst Hartel Laubrock Zeitpunkt Untersuchung Mai volltext digitalisieren aufgrund mangelnd Scanqualität titeln bücher Gesamtmenge Seite Corpusstudie heranziehen Fokus Bildanalyse ergeben sowohl methodisch praktisch Überlegung einerseits Methode stilometrisch Textanalyse etablieren laufend weiterentwickeln lassen direkt Comicstext anwenden adaptieren andererseits bereiten zuverlässig automatisiert Textlokalisierung Comics weiterhin Problem bleiben derzeit Weg zeitintensiv Textannotation groß Korpora bieten Methode visuell Maß stellen Basis Berechnung dar all Maß Bild Mithilfe linear Näherung Luminanz Bild umwandeln Anschluss Kalkulation Grundmaße berechneen Ergebnis pro Seite Mediane jeder einzelner Werk Stilistische abweichungen innerhalb Werk messen berechneen Standardabweichung maßen stilistisch Entwicklung innerhalb Werk berechnen Schritt Manuell Funktionsseit trennen letzterer Kalkulation ausschließen hierzu betrachten kurvenverläufe visuell Maß Seite graphisch Roman jeder visuell Maß ermittelen Anzahl Extrema Seite Standardabweichung Messwert Standardabweichung innerhalb lokal Minima Maxima Regelmäßigkeit Kurve Standardabweichung Abstand Seite zwei Aufeinanderfolgender Extrema Klassifikation letzter Seite groß klein messweren nächster vorherig Seite letzterer Berechnung stilistisch Entwicklung ergaben Wert Maß insgesamt Ergebnis pro graphisch Roman reduziern Hilfe Skalierter Hauptkomponentenanalyse Principal Component Analysis pca errechnet Maß ziehen Anschluss Analyse folgend kulturwissenschaftlich Konzept heran Genre Autorschaft Publikationsformat zeitgründ stellen teilergebnisse Kategorie Korpus enthalten sowohl Fiktional Text trotz Unterschied oftmals graphisch Roman bezeichnen tatsächlich erscheinen sinnvoll graphisch narrativen sprechen handelts durchgehend Erzählung Medium Comics erwachsen Publikum wenden Umfang Seite womit traditionell Seitenformat Comicbücher abheben einzeln Büchern Basis Klappentext Verlagsinformaion Zusammenfassunge folgend Genre zuweisen graphic novel graphic Memoir Text Sachbücher Text subgenres Superhelde Science Fiction märchen Fantasy Mystery Horror zugewiesen fassen Sammelkategorie Graphic Fantasy Zahl Text Klassifikatione fallen fassen Begriff Miscellaneous Ergebnis untersuch Hilfe welcher two sample statistisch Signifikanz untersuchen zeigen genr Graphic novel graphic Memoir Graphic Fantasy Signifikant voneinander unterscheiden speziell zeichnen graphisch Roman Memoire deutlich regelmäßig visuell Stil Titel niedrig Anzahl Fläche niedrig Standardabweichung durchschnittlich Helligkeit gehören meist Genre graphisch Memoire deutlich hell erzählungen Graphic Fantasy zusammengefassen Illustration zeigen historisch Entwicklung nachweisen lässt graphisch Memoire Mitte letzter Jahrzehnt deutlich Heller Entwicklung möglicherweise beispielgebend Wirkung mittlerweile kanonisch Titel Alison Bechdels öhnlich Fall Genreunterscheidung zeigen signifikant visuell Unterschied unterschiedlich Autor Illustration visualisiern stilistisch Handschrift künstlern Titel unser Corpus vertreten Werk Werk vergleichsweise konstant Stil bevorzugen nehmen klein Fläche Matrix fallen bekannt Autor Jason lut Chester Brown japanisch Ozamu Tezuka hingegen nehmen Frank Miller Dave Mckean David Mazzuchelli aufgrund visuell Bandbreit groß Fläche stilistisch Raum Manovichs Style space graphisch Erzählung Einbeziehung Tezuka zeigen Testfall momentan Grenze Methode verwendet Maß erlauben Differenzierung angloamerikanisch graphisch Roman japanisch Manga Erzählforme trotz gegenseitig Beeinflussung deutlich stilistisch Unterschied aufweisen Vortrag automatisch bildanalyse vorstellen stilometrisch unterscheidungen genr Autor Bereich visuell erzählungen übertragen anzumerken Zugang experimentell Stadium befinden demnächst zusätzlich visuell Maß verwendet erproben Schritt Farbigkeitsmaße denken Datenbasis Fortschreit Retrodigitalisierung unser Corpus Werk erweitern Genreunterscheidunge relativ aufwändig Berechnung intern Entwicklung Werk aussagekräftig herausstellen fließen autorenunterscheidunge Pca Genrekategorien Bedürfen Verfeinerung versuchen intern Kohärenz Genre berechnen distanzmaße Zentroiden Cluster Sammelkategorie Graphic fantasy Kohärent zufällig Vergleichsgruppe herausstellen neuerlich berechnung subgenres Superheldennarrative heranziehen graphisch Kunstwerk Hand zeichnen unterscheiden Comic deutlich Film Fernsehserie computerspielen dennoch bergen präsentiert Methode Meinung Potenzial Medium Frage Stilistik Genre Autorschaft Rolle spielen erforschen Genre Film Noir tatächlich stilistisch Kohärenz aufweist komplex qualitätsserien,"[('graphisch', 0.3045813542351217), ('graphic', 0.2510258220843501), ('visuell', 0.19591179194181932), ('fantasy', 0.19088830222775977), ('maß', 0.1724372605612033), ('seite', 0.16917404615090978), ('standardabweichung', 0.16686267503246127), ('stilistisch', 0.15229067711756086), ('stilometrisch', 0.13286551160742033), ('roman', 0.12351689403554979)]"
2018,DHd2018,KLINGER_Roman_A_Reporting_Tool_for_Relational_Visualization_.xml,A Reporting Tool for Relational Visualization and Analysis of Character Mentions in Literature,"Florian Barth (Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland); Evgeny Kim (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Sandra Murr (Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland); Roman Klinger (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland)","reporting tool, network analysis, visualization, text mining","Inhaltsanalyse, Beziehungsanalyse, Netzwerkanalyse, Visualisierung, Literatur, Text","The emergence of computational methods of text processing has created new paradigms of research in literary studies in recent years (Jockers & Underwood, 2016), for instance Existing tools of text analysis and network visualization such as Voyant We present our ongoing effort on closing this gap by developing a literary analysis reporting tool As a use-case study, we apply Previous research on social networks in literary fiction generally fall into one of the two categories: (1) works that explore methods for extracting and formalizing character networks ( Building on graph theory extensively elaborated in the past fifty years (e.g., Bondy and Murty, 1976 or West, 2001), our work is similar to Beveridge & Shan (2016), in particular, in terms of the weighted degree measure, and to Park et al. (2012), in terms of distance measure for detecting closely related characters in a text. In the following, we explain the different components in To detect character mentions in the text we use a fundamental named-entity recognition approach based on dictionaries. This approach is suitable for scholars who analyze texts they already know. Consequently, we opt for a transparent and simple character recognition procedure: The user provides a list of character names to be included in the analysis specifying a canonical name form and all variations thereof she would like to take into account ( We define the closeness of relationship between two characters using a We visualize the network of characters with an undirected graph Word clouds are an approach to visualize the vocabulary of a text. The size of one word corresponds to its frequency. We use two different kinds of word clouds: For each character in the character list, we show word clouds based on the context of a window size We plot the timeline of multiple predefined world fields (specified by word lists) in the text. This feature is helpful in representing how certain fields ( The tool was developed using Python v.3.6 and the Flask For a use-case analysis, we apply In Goethe's epistolary novel, the protagonist Werther describes his unhappy love for Lotte, who is engaged to Albert. The characteristic triangular relationship in the novel arises from this constellation (protagonist - beloved woman - antagonist). With The protagonist Werther shows a degree of 21, which is the number of characters with whom he interacts. The closest relationship measured by edge weight (Figure 2) is observed between Werther and Lotte (81 interactions). The antagonist Albert has a low degree of 3. However, his weighted degree is 36 (third highest after Werther and Lotte), which confirms his important role in the triangular relationship.  Highlighted in red is the typical triangular relationship in Goethe""s novel, which corresponds to the three highest weighted degrees. In further steps, we will use To better characterize the edges, the tool outputs top-  The word clouds enable first conclusions about the relationships of the characters. Werther and Lotte's word cloud characterizes their ambivalent relationship. The key words ""Leidenschaft"" and ""Freude"" reflect Werther's love, whereas the mentions of ""sterben"" and ""Verblendung"" are characteristic of the unrequited love, which leads Werther into his ""disease unto death"". As Werther and Albert""s word cloud reveals, their relationship is dominated by the ""Unruhe"" that Werther feels through his adversary. Additionally, the tool plots the development of the narrative (not bound to specific characters) based on the word fields, an example of which is shown on Figure 6. In this case we used words from the emotion domain (with emotion dictionaries by Klinger et al. (2016)).  The word field development can highlight the prevalence of individual emotion domains across the text. The accumulation of the negative emotion words (Wut,Trauer, Furcht) towards the end suggests, for example, that Goethe""s novel has no ""happy ending"". The striking rash on ""Freude"", however, captures the last happy hours Werther spends with Lotte in the second part of the narration before he kills himself. The next version of the tool will include a character-oriented word field development calculated and plotted for the main characters of the stories. In addition, future releases will include more analysis features and bulk file processing.",en,emergence computational method text processing create new paradigm research literary study recent year jockers underwood instance exist tool text analysis network visualization voyant present ongoing effort close gap develop literary analysis report tool use case study apply previous research social network literary fiction generally fall category work explore method extract formalize character network build graph theory extensively elaborate past year bondy murty west work similar beveridge shan particular term weight degree measure park et al term distance measure detect closely relate character text following explain different component detect character mention text use fundamental name entity recognition approach base dictionary approach suitable scholar analyze text know consequently opt transparent simple character recognition procedure user provide list character name include analysis specify canonical form variation thereof like account define closeness relationship character visualize network character undirected graph word cloud approach visualize vocabulary text size word correspond frequency use different kind word cloud character character list word cloud base context window size plot timeline multiple predefine world field specify word list text feature helpful represent certain field tool develop python flask use case analysis apply goethe epistolary novel protagonist werther describe unhappy love lotte engage albert characteristic triangular relationship novel arise constellation protagonist beloved woman antagonist protagonist werther show degree number character interact close relationship measure edge weight figure observe werther lotte interaction antagonist albert low degree weight degree high werther lotte confirm important role triangular relationship highlight red typical triangular relationship novel correspond high weighted degree step use well characterize edge tool output word cloud enable conclusion relationship character werther lotte word cloud characterize ambivalent relationship key word leidenschaft freude reflect werther love mention sterben verblendung characteristic unrequited love lead werther disease unto death werther word cloud reveal relationship dominate unruhe werther feel adversary additionally tool plot development narrative bind specific character base word field example show figure case word emotion domain emotion dictionary klinger et al word field development highlight prevalence individual emotion domain text accumulation negative emotion word wut trauer furcht end suggest example novel happy ending strike rash freude capture happy hour werther spend lotte second narration kill version tool include character orient word field development calculate plot main character story addition future release include analysis feature bulk file processing,"[('werther', 0.36862137931661937), ('character', 0.34310046905868335), ('word', 0.3173834319207253), ('relationship', 0.25778069670727805), ('cloud', 0.19333552253045852), ('lotte', 0.18431068965830968), ('degree', 0.16111293544204877), ('field', 0.13791518122578786), ('tool', 0.12367265713961217), ('triangular', 0.11872830358546106)]"
2018,DHd2018,KRUG_Markus_Annotation_and_beyond___Using_ATHEN.xml,Annotation and beyond 'Using ATHEN   Annotation and Text Highlighting Environment,"Markus Krug (Universität Würzburg, Deutschland); Ngoc Duyen Tanja Tu (Institut für Deutsche Sprache, Deutschland); Lukas Weimer (Universität Würzburg, Deutschland); Isabella Reger (Universität Würzburg, Deutschland); Leonard Konle (Universität Würzburg, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland); Frank Puppe (Universität Würzburg, Deutschland)","Annotation, Query, NLP, Preprocessing, Extensibility","Programmierung, Annotieren, Bearbeitung, Visualisierung, Metadaten, Software","The workshop presents ATHEN We compare ATHEN to three web-based and four desktop applications in 12 categories by adapting most criteria defined by Neves and Leser (Neves / Leser 2012) to compare different annotation tools: We explicitly do not want to compare subjective features like usability or how the annotations are presented. All of the listed tools have an accessible documentation, either web-based or as a PDF, available for download. Besides UAM (O'Donnell 2008), every other application is listed as open source, so at least extensions based on code level can be made. WebAnno (Yimam et al. 2013) is the only application having a tutorial supporting a new developer to make changes in their project. ATHEN stands out in the sense that extensions to its UI can be made at runtime, therefore easing the process of adding functionality to it. WebAnno supports the largest number of formats and comes with a machine learning based automatic annotation, however lacks integrated NLP-preprocessing. CATMA (Meister 2017) is the only project that has a very good visualization component and also supports TEI-XML (Wittern et al 2009), the unspoken standard of text processing. Being a standalone web application, CATMA itself does not support NLP-preprocessing. ATHEN comes with the support of the execution of UIMA analysis engines, accessible from web repositories or a local repository, giving the user a chance to integrate her custom-made annotators. Four tools, ATHEN, UAM, MMAX2 (Müller / Strube 2006) and CATMA feature an integrated query language which helps to analyze existing corpora. Most tools allow the annotation of user-defined annotation schemas earning therefore the title ""generic annotation tool"". Alongside UAM, ATHEN supports the annotation based on queries, while UAM defines its own language, ATHEN supports the annotation using Apache UIMA Ruta (Kluegl et al. 2016) rules. Three of the listed tools, MMAX2, Knowtator (Ogren 2006) and WordFreak (Morton / LaCivita 2003) are currently no longer in active development. ATHEN is a Java-based desktop application with the vision to be extensible. Therefore, it makes use of the flexible plugin architecture of the eclipse The program is split into four sections: The first section is a presentation which shows the main differences between the existing annotation tools. The second section defines an ordinary annotation scenario and it is used to introduce the participants to the general-purpose annotation view of ATHEN. Afterwards, for tasks to which ATHEN has special support (annotating character references and their coreferences, annotating direct speech and their speaker) an introduction to the special purpose views of ATHEN is given. The third panel introduces the participants to the functionality of ATHEN beyond regular text annotation. It starts with the definition of an OWL ontology (and its utilization for texts). This is centered on relation detection of character references, as well as an attribution of those references. To speed up manual annotation it is helpful to have it preprocessed with existing tools. The task definition is then changed from pure annotation to an application with a consecutive correction of the output of the automatic engines. In this context, Nappi, a submodule of ATHEN is presented and it is shown how to define, execute and integrate custom analysis engines. The next part is dedicated towards extracting knowledge from annotated data, for this purpose, an Apache Lucene Index is created using ATHEN and is queried in a live fashion. This feature allows rapid insight into an existing corpus and enables the user to answer their own hypothesis. The tutorial continues with the presentation of how images can be annotated with polygon-based annotations to show, that ATHEN is not only limited to textual resources. The last part is directed towards Java developers who are interested in developing their own annotation component. Each section starts with a set of slides which introduce the features in focus and presents the participants with one or more tasks that can be fulfilled by using ATHEN. The participants need their own laptops with an active internet connection. The number of participants is limited to 15 to 20. The last section requires knowledge of Java. Data which is necessary for the tutorials will be hosted on our own server and will be made accessible for download. ATHEN is mainly developed in the context of the project Kallimachos at the University of Wuerzburg. Its main purpose was to support the annotation process of DROC ( An extension to ATHEN was made in the project ""Redewiedergabe"" to manually annotate different forms of speech, thought and writing representation (STWR). These annotations will then be used to train an automatic recognizer for STWR.",en,workshop present athen compare athen web base desktop application category adapt criterion define neve leser neve leser compare different annotation tool explicitly want compare subjective feature like usability annotation present list tool accessible documentation web base pdf available download uam application list open source extension base code level webanno yimam et al application have tutorial support new developer change project athen stand sense extension ui runtime ease process add functionality webanno support large number format come machine learning base automatic annotation lack integrate nlp preprocessing catma meister project good visualization component support tei xml wittern et al unspoken standard text processing standalone web application catma support nlp preprocessing athen come support execution uima analysis engine accessible web repository local repository give user chance integrate custom annotator tool athen uam müller strube catma feature integrate query language help analyze exist corpora tool allow annotation user define annotation schema earn title generic annotation tool alongside uam athen support annotation base query uam define language athen support annotation apache uima ruta kluegl et al rule list tool knowtator ogren wordfreak morton lacivita currently long active development athen java base desktop application vision extensible make use flexible plugin architecture eclipse program split section section presentation show main difference exist annotation tool second section define ordinary annotation scenario introduce participant general purpose annotation view athen task athen special support annotate character reference coreference annotate direct speech speaker introduction special purpose view athen give panel introduce participant functionality athen regular text annotation start definition owl ontology utilization text center relation detection character reference attribution reference speed manual annotation helpful preprocesse exist tool task definition change pure annotation application consecutive correction output automatic engine context nappi submodule athen present show define execute integrate custom analysis engine dedicate extract knowledge annotated datum purpose apache lucene index create athen query live fashion feature allow rapid insight exist corpus enable user answer hypothesis tutorial continue presentation image annotate polygon base annotation athen limit textual resource direct java developer interested develop annotation component section start set slide introduce feature focus present participant task fulfil athen participant need laptop active internet connection number participant limit section require knowledge java datum necessary tutorial host server accessible download athen mainly develop context project kallimachos university wuerzburg main purpose support annotation process droc extension athen project redewiedergabe manually annotate different form speech think write representation stwr annotation train automatic recognizer stwr,"[('athen', 0.5741798216930387), ('support', 0.21438903794904612), ('annotation', 0.20860224810820727), ('participant', 0.1682852006123682), ('base', 0.15235264323593073), ('tool', 0.1505593839444891), ('application', 0.1429260252993641), ('uam', 0.1346281604898946), ('define', 0.13149496583039266), ('section', 0.13149496583039266)]"
2018,DHd2018,TRILCKE_Peer_Dramenquartett___Eine_didaktische_Intervention.xml,Dramenquartett 'Eine didaktische Intervention,"Frank Fischer (Higher School of Economics, Moskau); Christopher Kittel (Karl-Franzens-Universität Graz, Open Knowledge Forum Österreich); Carsten Milling (Berlin); Peer Trilcke (Universität Potsdam, Deutschland); Jana Wolf (Mittelbayerische Zeitung Regensburg)","Literatur, Netzwerkanalyse, Gamification","Modellierung, Kommunikation, Netzwerkanalyse, Einführung, Lehre, Text","Ziel dieses Posters ist es, anhand von 32 deutschsprachigen Dramen in die Netzwerkanalyse literarischer Texte einzuführen, eine didaktische Intervention für eine zwar mittlerweile etablierte Methode der literaturwissenschaftlichen Analyse, die aber nicht immer genügend reflektiert wird: Der Errechnung teils komplexer netzwerktheoretischer Maße entspricht nicht immer ein entsprechender Sprung zur Bedeutungsebene. Was bedeutet es zum Beispiel wirklich, dass die durchschnittliche Pfadlänge in Goethes ""Faust. Der Tragödie erster Theil"" genau 1,79 beträgt? Wenn man jedoch diesen Wert in Beziehung zu entsprechenden Werten anderer Stücke setzt, gewinnt er an komparatistischer Bedeutung. Die Anschaulichkeit der Wert und ihre spielerisch erfahrene Dimensionierung ermöglichen so die Einübung in die strukturalistische Betrachtung von Netzwerken am Beispiel von Dramen, wobei damit zugleich kulturelles Grundwissen über die Strukturation von Netzwerken 'immerhin ubiquitäre Gegenstände der sozialen und technischen Welt 'erworben werden kann. Um den komparatischen Blick im Kontext der literaturwissenschaftlichen Netzwerkanalyse zu schulen, setzen wir mit unserem Poster auf einen Gamification-Ansatz. Anders als bei unserem ersten Experiment in dieser Richtung 'der auf der DHd2016 präsentierten Android-App ""Play(s)"" (vgl. Göbel/Meiners 2016), in deren Mittelpunkt die spielerische Korrektur und Anreicherung unserer Korpusdaten stand –, handelt es sich diesmal um eine nicht-technische Anwendung, die auf spielerische Weise netzwerkanalytisches Datenmaterial explorierbar macht. Dabei wird das Posterformat in zweierlei Hinsicht bespielt: Das Poster ist einerseits eine Datenvisualisierung auf Grundlage eines selbst gepflegten größeren Dramenkorpus. Andererseits ist es ein in 32 Teile zerlegbares Dramenquartett, das spielerisch mit den Bedeutungshorizonten verschiedener netzwerktheoretischer Größen bekannt macht und ein Bewusstsein für komparatistische Möglichkeiten trainiert. Dieser Ansatz ist in den Geisteswissenschaften nicht neu, verwiesen sei etwa auf das architekturgeschichtliche Quartettspiel ""Plattenbauten. Berliner Betonerzeugnisse"" (Mangold u.¬†a. 2001), in dem technische Daten verschiedener Plattenbautypen gegenübergestellt werden (vgl. auch Richter 2006). Die Didaxe des Dramenquartetts bezieht sich auf mehrere Dimensionen: eine literaturgeschichtliche, eine quantitative, eine netzwerktheoretische. Die 32 Stücke bilden einen Minimalkanon, der von der Zeit der Gottschedischen Theaterreformen bis in die Moderne reicht. Statt der lexikonartigen Beschreibung eines solchen Kanons (wie etwa im ""Dramenlexikon des 18.¬†Jahrhunderts"", Hollmer/Meier 2001), besteht das Beschreibungsinstrument hier in visuellen und quantitativen Werten, die Vergleichbarkeit herstellen 'erst dieser Umstand vereint die verschiedenen Karten zu einem kompetitiven Spiel. Als visueller Catch der Quartettkarten dient eine Visualisierung des jeweiligen extrahierten sozialen Netzwerks (vgl. Fischer u.¬†a. 2016). Die weiteren Informationen auf den Karten setzen sich aus (Kanonwissen präsentierenden) Metadaten (Autor*in 'Titel 'Untertitel 'Genre 'Jahr) und vor allem aus statischen und dynamischen Netzwerkdaten zusammen (Anzahl von Subgraphen 'Netzwerkgröße 'Netzwerkdichte 'Clustering-Koeffizient 'Durchschnittliche Pfadlänge 'Höchster Degreewert und Name der entsprechenden Figur –), wie sie im Rahmen des dlina-Projekts berechnet wurden. Das Poster wird mit unserer Python-Skriptsammlung ""dramavis"" generiert, die in der neuen Version 0.4 eine entsprechende Funktion erhalten hat (Kittel/Fischer 2017). Für das Konferenzposter haben wir einen Fallback-Kanon zusammengestellt (Stücke von Johann Christoph und Luise Adelgunde Victorie Gottsched, von Gellert, J.¬†E. Schlegel, Caroline Neuber, Klopstock, Lessing, Gerstenberg, Goethe, Lenz, Klinger, Schiller, Kotzebue, Kleist, Zacharias Werner, Müllner, Grillparzer, Grabbe, Büchner, Hebbel, Gustav Freytag, Anzengruber, Arno Holz, Wedekind, Schnitzler, Erich Mühsam). Über eine individualisierbare Kanon-Datei können aber auch eigene Quartette zusammengestellt werden, sodass sich etwa auch epochenspezifische Sets (Dramen der Aufklärung, Dramen der Klassik, Romantische vs. Klassische Dramen, Dramen des Sturm und Drang vs. Dramen des Naturalismus) oder gattungsspezifische Sets erstellen lassen. Auf der Konferenz werden wir neben einem Poster, das das didaktisch-interventionistische Konzept veranschaulicht, auch diverse Quartett-Sets präsentieren.",de,Ziel Poster anhand Deutschsprachig dramen Netzwerkanalyse literarisch Text einführen didaktisch Intervention mittlerweile etabliert Methode literaturwissenschaftlich Analyse genügend reflektieren Errechnung teils Komplexer netzwerktheoretischer Maß entsprechen entsprechend Sprung bedeutungsebene bedeuten durchschnittlich pfadlänge goeth Faust Tragödie Theil genau betragen Wert Beziehung entsprechend Wert anderer Stück setzen gewinnen komparatistisch Bedeutung Anschaulichkeit Wert spielerisch erfahren Dimensionierung ermöglichen Einübung strukturalistisch Betrachtung netzwerken Dram wobei kulturell Grundwisse Strukturation netzwerken immerhin ubiquitär Gegenstände sozial technisch Welt erwerben komparatisch Blick Kontext literaturwissenschaftlich Netzwerkanalyse schulen setzen unser Poster unser Experiment Richtung präsentieren play s Göbel Meiners Mittelpunkt spielerisch Korrektur Anreicherung Korpusdat stehen handeln diesmal Anwendung spielerisch Weise netzwerkanalytisch Datenmaterial explorierbar Posterformat zweierlei Hinsicht bespieln Poster einerseits Datenvisualisierung Grundlage gepflegt groß dramenkorpus andererseits Teil zerlegbarer Dramenquartett spielerisch bedeutungshorizonten Verschiedener netzwerktheoretischer Größe Bewusstsein komparatistisch Möglichkeit trainieren Ansatz geisteswissenschaften neu verweisen architekturgeschichtlich Quartettspiel plattenbaun Berliner Betonerzeugnisse mangold technisch daten verschieden Plattenbautyp genübergestellen Richter Didaxe Dramenquartett beziehen mehrere dimension literaturgeschichtlich quantitativ netzwerktheoretisch Stück bilden Minimalkanon gottschedisch Theaterreformen modern reichen lexikonartig Beschreibung Kanon Dramenlexikon Hollmer meier bestehen Beschreibungsinstrument Visuell Quantitativen werten vergleichbarkeit herstellen Umstand vereinen verschieden Karte kompetitiv Spiel visueller Catch quartettkarten dienen Visualisierung jeweilig extrahiert sozial Netzwerks Fischer Information Karte setzen kanonwiss präsentierend metadaten Titel untertitel Genre statisch dynamisch Netzwerkdat Anzahl subgraph netzwerkgröße netzwerkdicht durchschnittlich pfadlänge hoch degreewert Name entsprechend Figur Rahmen berechnen Poster dramavis neriern Version entsprechend Funktion erhalten Kittel Fischer konferenzposter zusammenstellen Stück Johann Christoph luise adelgund Victorie gottsched llern Schlegel Caroline Neuber Klopstock Lessing Gerstenberg Goethe Lenz Klinger Schiller Kotzebue Kleist Zacharias Werner Müllner Grillparzer Grabbe Büchner hebbel Gustav Freytag anzengruber arno Holz wedekind schnitzl Erich mühsam individualisierbar Quartette zusammenstellen sodass epochenspezifisch Set Dram Aufklärung dramen Klassik romantisch klassisch Dram Dram Sturm Drang Dram Naturalismus gattungsspezifisch Set erstellen lassen Konferenz Poster Konzept veranschaulichen diverser präsentieren,"[('spielerisch', 0.2409963914815917), ('dram', 0.18932382406292578), ('poster', 0.18222233843837518), ('stück', 0.14350002392185698), ('netzwerktheoretischer', 0.14259654097039795), ('pfadlänge', 0.14259654097039795), ('dramenquartett', 0.12587978813409104), ('komparatistisch', 0.12587978813409104), ('karte', 0.10632244104796389), ('entsprechend', 0.10490900679778785)]"
2018,DHd2018,LAUER_Gerhard_Kulturelle_Evolution__Zur_Kritik_der_literatur.xml,Kulturelle Evolution. Zur Kritik der literaturhistorischen Methode,"Gerhard Lauer (Universität Basel, Schweiz)","Kulturelle Evolution, Literaturgeschichte, Phylogenetik","Beziehungsanalyse, Räumliche Analyse, Modellierung, Theoretisierung, Netzwerkanalyse","Zu den Provokationen der Literaturwissenschaft durch die Digital Humanities gehört ihre Arbeit an großen Datenmengen. Nicht das besondere Buch, der Kanon oder der Großschriftsteller, sondern die vielen Bücher und Literaturen sind der ""andere"" Gegenstand der computergestützten Literaturwissenschaft. Geradezu typisiert werden Digital Humanities und Distant Reading zusammen genannt. Tatsächlich war (Moretti, 2000) als Kritik an der herkömmlichen Literaturwissenschaft angelegt, genauer an ihrer methodischen Beschränkung, die Vielfalt der Literaturen methodisch in den Griff zu bekommen. Die Kritiker Morettis haben sein Anliegen konzediert, ohne konkrete Vorschläge zu machen, wie mit dem ""Mengenproblem"" in der Literaturgeschichtsschreibung besser umgegangen werden könnte (Ross, 2014). Ein radikaler Ansatz, die Geschichte der Literatur anders als bisher zu modellieren, ist der Ansatz der kulturellen Evolution (Lewens, 2013). Meine These lautet: Evolutionäre Modelle und Theorien sind für die Beantwortung literaturhistorische Fragestellung geeignet, besonders um das ""Mengenproblem"" in den Griff zu bekommen. Im Folgenden skizziere ich Theorie und Methodologie eines solchen Ansatzes und frage nach den Folgen für ein Fach wie die Literaturgeschichte. Wie der Name schon andeutet, verschiebt der Ansatz den Akzent von der Geschichte auf die Evolution. In den Blick rückt nicht weniger als die Menschheitsgeschichte. Der Ansatz kommt denn auch aus der biologischen Anthropologie, nicht aus den geisteswissenschaftlichen Fächern. Das leitende Paradigma ist Darwins Theorie der Evolution. Die Theorie der kulturellen Evolution überträgt dieses Modell auf die Kulturgeschichte der Menschheit. Sie geht von der These aus, dass die Entwicklung der menschlichen Kulturen der gleichen evolutionären Entwicklungslogik folgt, der auch die Natur unterliegt (Mesoudi, 2016). Dabei geht es nicht um eine Analogie, vielmehr lautet die Dual-Heritance-These dieser Theorie, dass die Kultur die Natur des Menschen ist, kulturelle Entwicklungen die Natur des Menschen bis in seine genetische und biologische Veranlagung beeinflussen, wie umgekehrt die Naturgeschichte des Menschen seine Kultur bestimmt (Henrich & McElreath, 2007). So generell angelegt versteht sich kulturelle Evolution als eine Supertheorie, die verspricht, die Sozial- und Geisteswissenschaften mit den biologischen Wissenschaften zusammenzuführen (Mesoudi, Whiten & Laland, 2006; Laubichler & Renn, 2015). Aus der Sicht der Digital Humanities ist die Theorie der kulturellen Evolution daher am extremen Ende des Distant Reading angesiedelt. Statt das einzelne Buch versucht die Theorie die Kultur der Menschheit in den Blick der Untersuchung zu nehmen. Kulturelle Evolution ist aber neben dem ""Distant Reading"" auch noch aus einem zweiten Grund von Interesse für Digital Humanities. Sie arbeitet mit computergestützten Modellen. Bereits die Arbeiten aus den 70er und 80er Jahren haben computer-basierte Modelle genutzt, als (Cavalli-Sforza & Marc Feldman, 1981), dann auch (Boyd & Richerson, 1985) begonnen haben, die Evolutionsbiologie für das Verstehen von kulturellen Prozessen zu nutzen. Ihr Modell der kulturellen Evolution geht von Populationen aus, die ihrerseits aus Gruppen von Individuen bestehen, von denen jedes Individuum über variierende kulturelle Eigenschaften verfügt. Soziale Transmission von Informationen ist der wesentliche kulturelle ""Vererbungs""-Mechanismus zwischen Individuen und Populationen. Nur dann, wenn man annimmt, dass durch Prozesse des kulturellen Lernens Wissen vertikal, aber auch horizontal zwischen gleichzeitig lebenden Generationen weitergegeben wird, etwa das Lernen von Sprachen durch Kinder, kann man verstehen, warum sich die horizontale Weitergabe langfristig auch auf die vertikale, genetische Weitergabe von Eigenschaften auswirken kann und sich so etwas wie komplexe Sprachen entwickeln konnten. Hier kommen mathematische Ansätze und Computersimulationen ins Spiel, um die langfristigen Veränderungen mikroevolutionärer Veränderungen in Populationen, die Rate der Ausbreitung und räumlichen Verteilung neuer kultureller Eigenschaften zu modellieren. Quantitative Ansätze waren für Cavalli-Sforza, Feldman, Boyd und Richerson trotz der damals noch bescheidenen Speicherraten notwendig geworden, weil die Prozesses, die Veränderungen in der kulturellen Variationen verursachen, so vielfältig sind, dass sie mit herkömmlichen Methoden nicht mehr gehandhabt werden konnten. Die Linguistik hat die Theorie der kulturellen Evolution rasch adaptiert, um die Evolution der Sprache untersuchen zu können, wie etwa die Befunde zur Diversität von Sprachen, nämlich dass größere Populationen ein größeres Inventar an Wörtern besitzen, ihre Sprachen stärker grammatikalisiert sind, mehr Phoneme haben, aber ihre Morphologie zugleich einfacher ist und ihre Wörter kürzer sind (Atkinson, 2011; Nettle, 2012). Auch kann ein solcher Ansatz zeigen, wie sich wärmere Klimata auf das Klangspektrum der in einer Sprache genutzten Laute auswirken (Munroe, Fought & Macaulay, 2009) oder wie zerklüftete Landschaften bestimmte Distanzsprachen wie etwa Zeichen- oder Pfeifsprachen präferieren (Meyer, 2015). Immer geht es dabei um Einsichten in die Struktur der Kultur und die Prozesse ihrer Veränderung der langen Dauer, die nicht der Granularität etablierter historischer Beschreibungen entsprechen. In der Literaturgeschichte sind kulturevolutionäre Modelle nicht etabliert. Das hat zunächst damit zu tun, dass generell der Aufbau von Korpora und die formale Modellierung von Fragestellungen innerhalb des historisch-hermeneutischen Paradigmas keine Rolle spielen und kaum eine Tradition haben. Linguisten dagegen wie (Labov, 1963) haben variationstheoretische und funktionalistischen Methoden genutzt, um die Systematizität in der sozialen und individuellen Variation des Sprachgebrauchs zu verstehen. Neuere Arbeiten in der Linguistik fragen danach, ob selbst solche fundamentalen Unterscheidungen wie die Unterscheidung der Wortklassen Nomen und Verben nicht in allen Sprachen zu finden sein könnte, die Wortordnung viel variabler als bislang angenommen sein dürfte, sprachliche Register höchst unterschiedlich gebraucht werden, Sprachen in einer höchst unterschiedlichen Interaktion zwischen Kindern und Eltern erworben werden (Evans, 2013; Lieven, 2013). Wenn aber Sprache in ihrer Entwicklungsgeschichte selbst in ihren Grundkategorien diverser sein könnten, als lange angenommen, und die Evolutionsrate ihrerseits je nach Sprache und Umwelt stark zu variieren scheint (Gray et al., 2013), müsste nicht Öhnliches auch für die Literaturen der Welt und ihre Entwicklung gelten, so dass man annehmen könnte, dass die Rate der Evolution von literarischen Formen mit Faktoren wie Gruppengröße, Dichte des sozialen Netzwerks, Menge der geteilten Informationen, soziale Stabilität und das Niveau des Austausches mit anderen Gruppen korreliert (Trudgill, 2011)? Das wäre eine innovative Forschungsagenda. Die wenigen literaturhistorischen Arbeiten, die bislang vorliegen, nutzen verschiedene statistische Modelle und Methoden, um eine kulturevolutionäre Literaturgeschichte zu untersuchen. Besonders naheliegend ist der Ansatz einer Weiterentwicklung von Stemma zur Rekonstruktion von Manuskript-Kulturen. 1998 druckte die Zeitschrift Ein weiterer kulturevolutionärer Ansatz nutzt die Klassifikation von Motiven, wie sie in der historisch-geographischen Schule der Märchenforschung mit dem Aarne-Uther-Thompson-Index vorliegt. Mit Methoden der Kladistik (Most Parsimonious Trees), Bayesian und phylogentischen Netzwerkanalysen konnten (2013; Tehrani & d""Huy, 2106) die weltweite Verwandtschaft des Rotkäppchen-Märchens bestimmen. Von Interesse sind dabei auch Zusammenhänge mit der Populationsstruktur (Ross, Greenhill & Atkinson, 2013), wenn dabei gezeigt werden kann, wie geographische Distanz, Genetik und Variation der Märchenmotive zusammenhängen (Bortolini et al., 2017). Die Annahme dabei ist, dass genetische, linguistische und motivliche Distanz korrelieren. Statt über textuelle Merkmale wie Abschriftenfehler oder Motive zu gehen nutzen andere Ansätze innerhalb dieses Paradigmas Spielexperimente, wie sie besonders in der Verhaltensökonomie gängig sind. So wählen Probanden aus mehr als 60 Geschichten diejenigen aus, die ihnen besonders erzählenswert erscheinen und schreiben eine der Geschichten in kurzen Abschnitten (120-160 Buchstaben) weiter, bevor anderen Probanden die Geschichte weiterschreiben. Jeder Proband kennt jeweils nur die ""Eltern"" der Geschichte, die sie gerade fortschreiben (Cuskley et al., 2016). Gemessen werden qualitativ und quantitativ narrative Innovationen, um so experimentell die Ausbreitung von Geschichten zu messen. Ein weiterer Ansatz nutzt das Konzept der ""Minimally Counterintuitive Narratives"", demzufolge Geschichten dann eher geteilt und weitergegeben werden, wenn sie eine realistische Ontologie leicht durchbrechen, wie das etwa bei Märchen der Fall ist (Porubanova-Norquist, Shaw & Xygalatas, 2013). Auch hier werden phylogenetische Methoden verwendet, um zu berechnen, welche Texteigenschaften in welcher Umwelt evolutionär vorteilhaft sind und daher eher geteilt und weitergegeben werden (Stubbersfield & Tehrani, 2013). Es liegt auf der Hand, dass noch sehr viele andere Dimensionen von (literarischen) Texten eine Rolle im Prozess der Evolution spielen dürften. Systematisch gewendet heißt das: Eine kulturevolutionäre Literaturgeschichtsschreibung hat ein anderes Gegenstandsfeld, nicht den Viktorianischen Roman, sondern die evolutionäre Logik seiner Entwicklung und die Schreibmuster dieses Genres. Literatur ist kein Werk, sondern Teil von Populationen. Sie benutzt zweitens andere Methodensets als die herkömmliche Literaturgeschichte, aber teilweise auch andere als sie sonst in den Digital Humanities gängig sind. Drittens leistet der Ansatz anderes. Er ist auf Erkenntnisse zur Logik der langen Dauer ausgerichtet. Und viertens rückt der Ansatz die Literaturgeschichte nahe an die Biologie heran mit Folgen für die Theoriebildung und Methodenentwicklung. Die Theorie der kulturellen Evolution ist nicht weniger als ein Ansatz, Digital Humanities als Teil eines größeren Forschungsprogramms zu betreiben. Mein Vortrag ist ein Plädoyer für ein solches Forschungsprogramm einer Literaturgeschichte der langen Dauer.",de,Provokation Literaturwissenschaft Digital Humanitie gehören Arbeit datenmenger besonderer Buch Kanon Großschriftsteller büch Literatur Gegenstand computergestützt Literaturwissenschaft geradezu typisieren Digital Humaniti distant Reading nennen tatsächlich moretti Kritik herkömmlich Literaturwissenschaft anlegen genau methodisch Beschränkung Vielfalt literaturen methodisch Griff bekommen Kritiker Moretti Anliegen konzedieren konkret Vorschlag mengenprobl Literaturgeschichtsschreibung umgehen ross radikal Ansatz Geschichte Literatur modellieren Ansatz kulturell Evolution lewens These lauten evolutionär Modell Theorie Beantwortung literaturhistorisch Fragestellung geeignet mengenprobl Griff bekommen folgend skizziern Theorie Methodologie ansatzes Frage Folge Fach Literaturgeschicht Name andeuten verschieben Ansatz Akzent Geschichte Evolution Blick rücken Menschheitsgeschichte Ansatz biologisch Anthropologie geisteswissenschaftlich fächern leitend Paradigma darwins Theorie Evolution Theorie kulturell Evolution übertragen Modell Kulturgeschichte Menschheit These Entwicklung menschlich Kultur gleich evolutionären Entwicklungslogik folgen Natur unterliegen Mesoudi Analogie vielmehr lauten Theorie Kultur Natur Mensch kulturell Entwicklung Natur Mensch genetisch biologisch Veranlagung beeinflussen umgekehrt neturgeschichen Mensch Kultur bestimmen henrich Mcelreath generell anlegen verstehen kulturell Evolution Supertheorie versprechen geisteswissenschaften biologisch Wissenschaft Zusammenzuführ Mesoudi whiten Laland Laubichler renn Sicht Digital Humanitie Theorie kulturell Evolution extrem distant Reading ansiedeln einzeln Buch versuchen Theorie Kultur Menschheit Blick Untersuchung nehmen kulturell Evolution distant Reading Grund Interesse Digital Humanitie arbeiten Computergestützt modellen Arbeit Modell nutzen Marc Feldman boyd Richerson beginnen Evolutionsbiologie verstehen kulturell Prozessen nutzen Modell kulturell Evolution populationen ihrerseits Gruppe individuen bestehen jeder Individuum variierend Kulturelle eigenschaften verfügen sozial Transmission Information wesentlich kulturell individuen populationen annehmen prozesse kulturell lernen wissen Vertikal Horizontal gleichzeitig lebend generationen weitergeben lernen Sprache Kind verstehen horizontal Weitergabe langfristig vertikal genetisch Weitergabe eigenschaften auswirken komplex Sprache entwickeln mathematisch Ansatz Computersimulation Spiel langfristig Veränderung Mikroevolutionärer Veränderung populationen Rate Ausbreitung räumlich Verteilung neu Kultureller eigenschaften Modelliere quantitativ Ansatz Feldman Boyd Richerson trotz bescheiden Speicherrat notwendig prozesses Veränderung kulturell Variation verursachen vielfältig herkömmlich Methode handhaben Linguistik Theorie kulturell Evolution rasch adaptieren Evolution Sprache untersuchen befunde Diversität Sprache nämlich groß populationen größeres inventar wörtern besitzen Sprache stark grammatikalisiert Phonem Morphologie einfach Wörter kurz Atkinson nettle Ansatz zeigen wärm Klimata Klangspektrum Sprache genutzt laute auswirken Munroe Fought Macaulay zerklüftet landschafen bestimmt distanzsprache pfeifsprach Präferier Meyer Einsicht Struktur Kultur prozeß Veränderung lang Dauer Granularität etabliert Historischer Beschreibung entsprechen Literaturgeschichte kulturevolutionär Modell etablieren generell Aufbau Korpora formal Modellierung Fragestellung innerhalb Paradigmas Rolle spielen Tradition linguisten Labov variationstheoretisch funktionalistisch Methode nutzen Systematizität sozial individuell Variation Sprachgebrauch verstehen neuerer arbeiten Linguistik fragen fundamental unterscheidungen Unterscheidung Wortklasse nomen verben Sprache finden Wortordnung variabl bislang annehmen dürfen sprachlich Register höchst unterschiedlich brauchen sprechen höchst unterschiedlich Interaktion Kind Eltern erwerben Evan lieven Sprache entwicklungsgeschichen Grundkategorie diverser können annehmen evolutionsrat ihrerseits Sprache Umwelt stark variieren scheinen Gray et müsste öhnlich Literatur Welt Entwicklung gelten annehmen Rate Evolution literarisch Form Faktor Gruppengröße Dichte sozial Netzwerks Menge geteilt Information sozial Stabilität Niveau Austausch Gruppe korrelieren Trudgill innovativ Forschungsagenda weniger literaturhistorisch arbeiten bislang vorliegen nutzen verschieden statistisch Modell Methode Kulturevolutionäre Literaturgeschichte untersuchen naheliegend Ansatz Weiterentwicklung Stemma Rekonstruktion drucken Zeitschrift weit kulturevolutionär Ansatz nutzen Klassifikation Motiv Schule Märchenforschung vorliegen Methode Kladistik Most Parsimonious Trees Bayesian phylogentisch netzwerkanalysen Tehrani d huy weltweit Verwandtschaft bestimmen Interesse zusammenhang Populationsstruktur Ross Greenhill Atkinson zeigen geographisch Distanz Genetik Variation Märchenmotiv Zusammenhäng Bortolini et Annahme genetisch linguistisch motivlich Distanz korrelieren textuell merkmal abschriftenfehl Motiv nutzen Ansatz innerhalb Paradigmas spielexperimenen Verhaltensökonomie gängig wählen Probande geschicht erzählensweren erscheinen schreiben geschichte kurz Abschnitt Buchstabe bevor Proband Geschichte weiterschreiben Proband kennen jeweils Eltern Geschichte fortschreiben Cuskley et messen qualitativ quantitativ narrativ Innovation experimentell Ausbreitung Geschicht messen weit Ansatz nutzen Konzept Minimally counterintuitiv narratives geschicht eher teilen weitergeben realistisch Ontologie durchbrechen märche Fall shaw xygalatas phylogenetisch Methode verwenden berechnen Texteigenschaft Umwelt evolutionär vorteilhaft eher teilen weitergeben Stubbersfield Tehrani liegen Hand dimension literarisch texen Rolle Prozess Evolution spielen dürfen systematisch wenden kulturevolutionär Literaturgeschichtsschreibung anderer Gegenstandsfeld viktorianisch Roman evolutionär logik Entwicklung Schreibmuster genr Literatur Werk populationen benutzen zweitens Methodenset herkömmlich Literaturgeschicht teilweise Digital Humanitie gängig drittens leisten Ansatz anderer erkenntnis Logik lang Dauer ausrichten viertens rücken Ansatz Literaturgeschichte nahe Biologie heran Folge Theoriebildung Methodenentwicklung Theorie kulturell Evolution Ansatz Digital Humanitie groß Forschungsprogramm betreiben Vortrag Plädoyer Forschungsprogramm Literaturgeschichte lang Dauer,"[('evolution', 0.37506307010857565), ('kulturell', 0.2946939767337304), ('populationen', 0.18303647490619993), ('theorie', 0.1824117901897055), ('ansatz', 0.17695087550840083), ('sprache', 0.13463284696007288), ('kultur', 0.12509159586238133), ('evolutionär', 0.10982188494371994), ('kulturevolutionär', 0.10982188494371994), ('literaturgeschichte', 0.10421043556236934)]"
2018,DHd2018,BIGALKE_Ben_Personen__und_Figurennetzwerke_in_Fernando_Pesso.xml,Personenund Figurennetzwerke in Fernando Pessoas Publikationsplänen,"Ben Bigalke (Universität zu Köln, Deutschland); Sviatoslav Drach (Universität zu Köln, Deutschland); Ulrike Henny-Krahmer (Universität Würzburg, Deutschland); Pedro Sep‚àöé¬¨‚à´lveda (Neue Universität Lissabon, Portugal); Christian Theisen (Universität zu Köln, Deutschland)","Fernando Pessoa, Netzwerk, Visualisierung, D3, TEI","Beziehungsanalyse, Netzwerkanalyse, Visualisierung, Manuskript, Personen","  Abstract zur Konferenz Digital Humanities im deutschsprachigen Raum 2018 Personenund Figurennetzwerke in Fernando Pessoas Publikationsplänen Bigalke, Ben bbigalke@smail.uni-koeln.de Universität zu Köln, Deutschland Drach, Sviatoslav sdrach@smail.uni-koeln.de Universität zu Köln, Deutschland ständige Bedeutungsverschiebung hin, die sowohl an fragmentarischen Schriften seines Nachlasses als auch an seinen Publikationsplänen zu erkennen ist (vgl. dazu Cunha 1987, Martins 2003, Gusm√£o 2003 und Sep√∫lveda 2013). Diese Bedeutungsverschiebung hängt stark mit dem Wahl der Autorennamen zusammen, die ebenfalls einem ständigen Wechsel unterlag und in den Plänen zur Edition und Publikation des Werkes eine hohe Bedeutung gewann. Zur Definition eines Publikationsvorhabens gehörte für Pessoa die Zuordnung einer bestimmten Autorfigur, deren fiktionale Persönlichkeit sowohl durch ein bestimmtes Werk konstruiert werden sollte als auch dieses Werk in seiner Besonderheit definieren würde. Die Spannung zwischen Planung und Publikation des Werkes wird daher noch dadurch verstärkt, dass Pessoa unter verschiedenen, insgesamt etwa 120 Autorennamen geschrieben hat oder geplant hat zu schreiben (vgl. dazu Pessoa 2012). Eine besonders wichtige Rolle in Pessoas Werk und seinen Werkplänen spielen dabei die Namen Alberto Caeiro, √Ålvaro de Campos und Ricardo Reis, die er (in Abgrenzung zu den weiteren Pseudonymen) Heteronyme genannt hat. Digitale Edition ""Projekte und Publikationen"" Henny-Krahmer, Ulrike ulrike.henny@uni-wuerzburg.de Universität Würzburg, Deutschland Sep√∫lveda, Pedro pmpsepulveda@gmail.com Neue Universität Lissabon, Portugal Theisen, Christian ctheise2@uni-koeln.de Universität zu Köln, Deutschland Im Nachlass des portugiesischen Dichters Fernando Pessoa (1888-1935) finden sich zahlreiche Listen geplanter Publikationen. Diesen stehen nur wenige zu Lebzeiten tatsächlich realisierte Veröffentlichungen gegenüber. Daraus ergibt sich ein Kontrast zwischen der Ebene des Möglichen und des Verwirklichten in der Literatur und Literaturproduktion. Vor diesem Hintergrund entsteht die digitale Edition ""Fernando Pessoa. Projekte und Publikationen"" und mit ihr die im Folgenden vorgestellte Netzwerkvisualisierung. Mit ihr wird ein Werkzeug zur Verfügung gestellt, das die Exploration des Personenund Figurenkosmos in Pessoas Publikationsplänen über die Zeit ermöglicht. Pessoas Werk zwischen Planung und Publikation Die Notizzettel, Seiten aus Notizbüchern und andere Papiere aus Pessoas Nachlass, auf denen er die Pläne für seine Werke handschriftlich oder mit Schreibmaschine geschrieben festgehalten hat, werden in einer Kooperation zwischen dem Institut für Literatur und Tradition (IELT) der Neuen Universität Lissabon und dem Cologne Center for eHumanities (CCeH) der Universität zu Köln digital ediert (Sep√∫lveda und Henny-Krahmer 2017). 1 Neben den Dokumenten aus dem Nachlass umfasst die digitale Edition auch die zu Lebzeiten von Pessoa publizierten Gedichte. Gegenstand des hier vorgestellten Netzwerkes sind jedoch ausschließlich die Dokumente, auf denen die Figuren und Personen genannt sind und deren Beziehungen ausgehend von ihrer gemeinsamen Erwähnung über die Zeit untersucht werden. Die Dokumente sind in der Edition in TEI codiert, wobei die Namensvorkommen erfasst werden und eine Identifikation der hinter den Namen stehenden Personen und Figuren in einem zentralen Index erfolgt. Transkriptionen und Index bilden zusammen mit den für jedes Dokument festgehaltenen Metadaten die Datengrundlage für das Figurenund Personennetzwerk, wobei insbesondere die für die Chronologie relevante Datierung zu nennen ist. Die Dynamik der Schriften Pessoas ist im Spannungsverhältnis zwischen dem projizierten Werk, das der Dichter im Sinne eines vollkommen Ganzen konzipiert hat, und dem tatsächlich Geschriebenen und nur in geringem Maße Publizierten zu verstehen. Pessoa war bei der Veröffentlichung seiner Werke extrem selektiv und die Dynamik seines Schreibens weist auf eine Abstract zur Konferenz Digital Humanities im deutschsprachigen Raum 2018 Netzwerkvisualisierung zu Personen und Figuren Die Vorkommen von Namen in Pessoas Publikationsplänen werden hier mit Hilfe einer interaktiven Netzwerkvisualisierung analysiert, um zu untersuchen, wie sich der von ihm in den Dokumenten entworfene Personenund Figurenkosmos über die Zeit entwickelt. Für dynamische Netzwerkvisualisierungen gibt es in den DH bereits verschiedene Ansätze (vgl. u. a. Rigal et al. 2016, Xanthos et al. 2016). Für das Pessoa-Netzwerk ergibt sich die Dynamik aus der Möglichkeit, das Gesamtnetzwerk der Personen und Figuren über alle Dokumente hinweg auf Dokumente aus bestimmten Zeiträumen oder Jahren einzugrenzen. Es ist als heuristisches Instrument gedacht, um Hypothesen zur Chronologie von Personenund Figurenkonstellationen zu generieren und im Ansatz überprüfen zu können. Für das Netzwerk, das unter http://www.pessoadigital.pt/ de/network verfügbar ist, sind 249 Dokumente ausgewertet worden, die insgesamt 369 Namen enthalten. Es werden sowohl historische Personen als auch fiktive Figuren aus Pessoas Werkwelt gezeigt. Analysiert wird das Vorkommen der Namen auf Pessoas Publikationsplänen von 1913 bis zu seinem Tod im Jahr 1935. Die frühen Dokumente (vor 1913) werden hier nicht berücksichtigt, da sie noch in Bearbeitung sind. Die Netzwerkdaten sind mit Hilfe von XSLT aus den TEI-Dokumenten generiert worden und liegen im JSON-Format vor. 2 Die interaktive Visualisierung ist mit der Bibliothek D3 erstellt worden, wobei ein Netzwerklayout von Mike Bostock adaptiert und um weitere Funktionalitäten ergänzt wurde. 3 Erweiterungen, die für die vorliegende Anwendung vorgenommen wurden, sind u. a. die Möglichkeit, Teile des Netzwerks einund auszublenden (nach Chronologie; Teilnetzwerke für die Verbindungen, die von einzelnen Personen ausgehen; nur Knoten mit oder auch Knoten ohne Verbindungen) sowie Optionen für die Darstellung (Einblenden von Labels; Dichte bzw. Weite der Anzeige des Netzwerks). Abbildung 1: Optionen für die Anzeige des Netzwerks Die Größe der Knoten im Netzwerk zeigt an, wie häufig einzelne Namen auf den Dokumenten erwähnt werden. Zur Ermittlung der Knotengröße wurde die Formel 2 + log2(size) angewandt, wobei size für die tatsächliche Häufigkeit des Vorkommens steht. Um zwischen fiktiven Figuren und historischen Personen unterscheiden zu können, sind die Knoten unterschiedlich eingefärbt (türkis = fiktiv; dunkelbau = historisch). Bei den Netzwerkkanten verdeutlicht die Dicke, wie häufig Namen gemeinsam auf Dokumenten vorkommen: je häufiger das gemeinsame Auftreten, umso dicker die Linien in der Visualisierung. Dabei ist die minimale Kantendicke 1 Pixel (bei einer gemeinsamen Erwähnung). Pro weiterer gemeinsamer Erwähnung nimmt die Kantendicke um 1 Pixel zu. Abstract zur Konferenz Digital Humanities im deutschsprachigen Raum 2018 Abbildung 2: Teilnetzwerk mit Fernando Pessoa als zentralem Knoten Zentrale Funktionalitäten in der Netzwerkanwendung sind Auswahloptionen, welche die Chronologie der Namenserwähnungen betreffen. So kann das Netzwerk neben einer Gesamtdarstellung auch für Vorkommen in einzelnen Jahren angezeigt werden. Darüber hinaus ist eine Anzeige nach Perioden möglich (z. B. 1919-1927). Da es in der Pessoa-Forschung konkurrierende Vorschläge für eine Periodisierung des Werkes gibt, werden zwei verschiedene Einteilungen in Perioden zur Auswahl angeboten. Auf diese Weise wird es möglich, die Entwicklung von Pessoas Personenund Figurenkosmos, wie er sich in den Publikationsplänen darstellt, kritisch zu untersuchen. Betrachtet man das Netzwerk chronologisch anhand ausgewählter Jahre und Perioden, innerhalb derer die Publikationspläne verfasst wurden, so sind Tendenzen zu erkennen, die historisch, editorisch und auch poetisch für das Werk von maßgeblicher Bedeutung sind. So kann man beispielsweise sehen, wie in den Jahren von 1913 bis 1919, und besonders zwischen 1914 und 1915, die Namen der Heteronyme zusammen mit dem von Fernando Pessoa am häufigsten vorkommen und vor allem untereinander verbunden sind (vgl. Abb. 4). Abbildung 4: Teilnetzwerk 1915 Abbildung 3: Gesamtnetzwerk mit Pessoa als häufigstem Namen Wenn man das Vorkommen aller Namen im gesamten Netzwerk betrachtet, so ist zu erkennen, dass Fernando Pessoa als Name am häufigsten vorkommt, direkt gefolgt von den Namen der Heteronyme Alberto Caeiro, Ricardo Reis und √Ålvaro de Campos, was etabliertes Wissen zu der Bedeutung der Heteronyme in Pessoas Werk bestätigt (vgl. Abb. 2 und 3). An zweiter Stelle stehen dann William Shakespeare, Jos√© Almada-Negreiros, Edgar Allan Poe, M√°rio de S√°-Carneiro und Ant√≥nio Mora. Dabei ist beispielsweise interessant zu sehen, dass die Heteronyme (und Mora, der zeitweise als starker Kandidat für die Rolle eines Heteronyms galt) auch zusammen mit Fernando Pessoa selbst, aber besonders unter sich verbunden sind, was auf eine gewisse Autonomie des heteronymischen Universums hindeutet. Die Namen von Shakespeare und Poe weisen auf die zwei wichtigsten Referenzen Pessoas aus der englischsprachigen Literatur hin, während Almada und S√°-Carneiro die zwei für ihn bedeutendsten zeitgenössischen portugiesischen Schriftsteller waren. Namen anderer Schriftsteller und historischer Figuren kommen tendenziell in späteren Perioden, etwa ab den 20er Jahren, häufiger vor. Sie zeigen ein zunehmendes Interesse Pessoas an der Veröffentlichung eigener √úbersetzungen von einigen für ihn entscheidenden Werke der Weltliteratur. Auch die wichtigsten Beziehungen Pessoas zu zeitgenössischen portugiesischen Schriftstellern und Kritikern sind im Netzwerk deutlich zu erkennen, besonders zwischen 1913 und 1918 (vgl. Abb. 5). Es handelt sich dabei vor allem um die modernistische Generation, die sich um die Zeitschrift Orpheu versammelt hat, und ab 1928 und bis zu Pessoas Tod 1935 dann die sogenannte zweite modernistische Generation um die Zeitschrift Presen√ßa. Abstract zur Konferenz Digital Humanities im deutschsprachigen Raum 2018 dadurch hergestellt, dass die interaktive Visualisierung an die digitale Edition angebunden ist. 5 Fußnoten Abbildung 5: Teilnetzwerk 1913-1918 Fazit 1. Die digitale Edition ist in einer Beta-Version unter http://www.pessoadigital.pt [letzter Zugriff 14. Januar 2018] verfügbar; die Entwicklung wird fortlaufend über GitHub organisiert: https://github.com/cceh/pessoa [letzter Zugriff 14. Januar 2018]. Zur editorischen Herangehensweise aus digitaler Perspektive vgl. HennyKrahmer und Sep√∫lveda 2017. 2. Die dem Netzwerk zugrunde liegenden Daten sind unter https://github.com/cceh/pessoa/tree/master/app/data/ network [letzter Zugriff 14. Januar 2018] einsehbar. 3. Das Ausgangs-Layout von Bostock trägt den Titel ""Force Layout with Mousover"" (Bostock 2017). 4. http://www.pessoadigital.pt/de/network/documentation [letzter Zugriff 14. Januar 2018]. 5. Derzeit über den Menüpunkt ""Chronologie"", vgl. http:// www.pessoadigital.pt/de/index.html [letzter Zugriff 14. Januar 2018]. Die aus der Netzwerkinterpretation gewonnenen literaturhistorischen Erkenntnisse bestätigen, dass für Pessoa die Edition und Publikation des Werkes und dessen Planung nicht von der Bedeutungsebene des Werkes selbst zu unterscheiden sind. Dass bestimmte Namen insgesamt oder zu bestimmten Zeiten besonders häufig auf den Publikationsplänen auftauchen, ist gleichbedeutend mit deren Wichtigkeit für das Werk in der entsprechenden Zeitperiode. Das gilt neben dem Vorkommen einzelner Namen auch für die gemeinsamen Vorkommen mehrerer Namen, wodurch die Bedeutung bestimmter Konstellationen zu bestimmten Zeiten sowohl in Pessoas Publikationsplänen als auch in seinem Werk deutlich wird. Diese grundlegende Erkenntnis für die Interpretation von Pessoas Werk bestätigt einige Intuitionen der Kritiker über den größeren Zusammenhang mehrerer Ebenen von Pessoas Werk (z. B. der Ebene der Edition des Werkes, vgl. dazu Sep√∫lveda und Uribe 2016), sowie die Vorstellung von Pessoas Werkganzem, die einer materiellen Fragmentarität seiner Schriften gegenübersteht (vgl. dazu Martins 2003, Gusm√£o 2003, Sep√∫lveda 2013, Feij√≥ 2015). Methodisch eröffnet die Visualisierung durch das interaktive Element und den höheren Grad der Abstraktion gegenüber den edierten Dokumenten, die in der digitalen Edition studiert werden können, neue Interpretationsspielräume. Dabei ist es jedoch sehr wichtig, die Dokument-, Textund Datengrundlage sowie die methodischen Wege zum Netzwerk und zur visuellen Darstellung stets im Blick zu behalten. Für diesen Beitrag bieten wir dafür eine Dokumentation an, die direkt mit dem interaktiven Netzwerk verbunden ist. 4 Der Zusammenhang zwischen Quellen und Analyse wird auch Bibliographie Bostock, Mike (2017): ""Force Layout with Mouseover Labels"", in: Mike Bostock‚Äôs Blocks. https://bl.ocks.org/ mbostock/1212215 [letzter Zugriff 14. Januar 2018]. Cunha, Teresa Sobral (1987): ""Planos e projectos editoriais de Fernando Pessoa: uma velha quest√£o"", in: Revista da Biblioteca Nacional, S√©rie 2, Vol. 2, N. 1: 92-107. Feij√≥, Ant√≥nio M. (2015): ""Uma admira√ß√£o pastoril pelo diabo (Pessoa e Pascoaes)"", in: Pessoana. Ensaios. Lissabon: Imprensa Nacional-Casa da Moeda. Gusm√£o, Manuel (2003): ""O Fausto ‚Äî um teatro em ru√≠nas"", in: Rom√¢nica 12: 67-86. Henny-Krahmer, Ulrike / Sep√∫lveda, Pedro (2017): ""Pessoa‚Äôs editorial projects and publications: the digital edition as a multiple form of textual criticism"", in: Boot, Peter / Cappellotto, Anna / Dillen, Wout / Fischer, Franz / Kelly, Aodh√°n / Mertgens, Andreas / Sichani, Anna-Maria / Spadini, Elena / van Hulle, Dirk (eds.): Advances in Scholarly Editing. Papers presented at the DiXiT conferences in The Hague, Cologne, and Antwerp. Leiden: Sidestone Press, 125-133 https://www.sidestone.com/books/advances-indigital-scholarly-editing [letzter Zugriff 14. Januar 2018]. Martins, Fernando Cabral (2003): ""Breves notas sobre a alta defini√ß√£o"", in: Rom√¢nica. N.¬∫ 12. 157-164. Pessoa, Fernando (2012): Teoria da Heteron√≠mia. Hsg. von Fernando Cabral Martins und Richard Zenith. Lissabon: Ass√≠rio & Alvim. Rigal, Alexandre / Rodighiero, Dario / Cellard, Loup (2016): ""The Trajectories Tool: Amplifying Network Visualization Complexity"", in: Digital Humanities 2016. Abstract zur Konferenz Digital Humanities im deutschsprachigen Raum 2018 Conference Abstracts. Krak√≥w: Jagiellonian University & Pedagogical University, 328-330 http://dh2016.adho.org/ abstracts/340 [letzter Zugriff 14. Januar]. Sep√∫lveda, Pedro (2013): Os livros de Fernando Pessoa. Lissabon: √Åtica. Sep√∫lveda, Pedro / Henny-Krahmer, Ulrike (eds., 2017): Fernando Pessoa ‚Äì Digitale Edition. Projekte und Publikationen. Editorische Leitung Pedro Sep√∫lveda, technische Leitung Ulrike Henny-Krahmer. Lissabon und Köln: IELT, Neue Universität Lissabon und CCeH, Universität zu Köln http://www.pessoadigital.pt [letzter Zugriff 14. Januar 2018]. Sep√∫lveda, Pedro / Uribe, Jorge (2016): O Planeamento editorial de Fernando Pessoa. Lissabon: Imprensa Nacional-Casa da Moeda. Xanthos, Aris / Pante, Isaak / Rochat, Yannick / Grandjean, Martin (2016): ""Visualizing the Dynamics of Character Networks"", in: Digital Humanities 2016. Conference Abstracts. Krak√≥w: Jagiellonian University & Pedagogical University, 417-419 http://dh2016.adho.org/ abstracts/407 [letzter Zugriff 14. Januar 2018].",de,Abstract Konferenz Digital Humanitie deutschsprachig Raum Personenund Figurennetzwerk Fernando pessoas publikationsplän Bigalke ben Universität Köln Deutschland drach Sviatoslav Universität Köln Deutschland ständig Bedeutungsverschiebung sowohl fragmentarisch Schrift nachlasses Publikationsplän erkennen cunha martins Bedeutungsverschiebung hängen stark Wahl Autorenname ebenfalls ständig Wechsel unterliegen plänen Edition Publikation Werk hoch Bedeutung gewinnen Definition Publikationsvorhaben gehören Pessoa Zuordnung bestimmt Autorfigur fiktional Persönlichkeit sowohl bestimmt Werk konstruieren Werk Besonderheit definieren Spannung Planung Publikation Werk verstärken Pessoa verschieden insgesamt Autorennam schreiben planen schreiben Pessoa wichtig Rolle pessoas Werk Werkplän spielen Name Alberto Caeiro de Campos Ricardo Reis Abgrenzung Pseudonyme heteronyme nennen digital Edition Projekt Publikation ulrike Universität Würzburg Deutschland Pedro Universität Lissabon Portugal theisen Christian Universität Köln Deutschland Nachlass portugiesisch Dichters Fernando Pessoa finden zahlreich Liste Geplanter publikationen stehen lebzeiten tatsächlich realisiert Veröffentlichung ergeben Kontrast Ebene möglich Verwirklicht Literatur Literaturproduktion Hintergrund entstehen digital Edition Fernando Pessoa Projekt Publikation folgend vorgestellt Netzwerkvisualisierung Werkzeug Verfügung stellen Exploration Personenund Figurenkosmos pessoas Publikationsplän ermöglichen pessoas Werk Planung Publikation Notizzettel Seite notizbüchern Papier pessoas Nachlass Plan werk Handschriftlich Schreibmaschine schreiben festhalten Kooperation Institut Literatur Tradition ieln Universität Lissabon Cologne Center for ehumaniteisen Cceh Universität Köln Digital edieren dokumenten Nachlass umfassen digital Edition lebzeiten Pessoa publiziert gedichen Gegenstand vorgestellt netzwerkes ausschließlich dokument Figur Person nennen Beziehung ausgehend gemeinsam Erwähnung untersuchen Dokument Edition Tei Codiert wobei Namensvorkomm erfasst Identifikation Name stehend Person Figur zentral Index erfolgen Transkription Index bilden jeder dokument festgehalten metadaten Datengrundlage Figurenund Personennetzwerk wobei insbesondere Chronologie relevant Datierung nennen Dynamik Schrift pessoas spannungsverhältnis projiziert Werk Dichter Sinn vollkommen konzipieren tatsächlich geschrieben gering Maß publizierten verstehen Pessoa Veröffentlichung Werk extrem selektiv Dynamik Schreiben weisen Abstract Konferenz Digital Humanitie deutschsprachig Raum Netzwerkvisualisierung Person Figur Vorkomm Name pessoas Publikationsplän Hilfe interaktiv Netzwerkvisualisierung analysieren untersuchen dokument entworfen Personenund Figurenkosmos entwickeln dynamisch Netzwerkvisualisierung dh verschieden Ansatz Rigal et xanthos et ergeben Dynamik Möglichkeit Gesamtnetzwerk Person Figur dokument Hinweg Dokument bestimmt Zeiträume eingrenzen heuristisch Instrument denken Hypothesen Chronologie personenund figurenkonstellationen generieren Ansatz überprüfen Netzwerk de Network verfügbar Dokument auswerten insgesamt Name enthalten sowohl historisch Person fiktiv Figur pessoas werkweln zeigen analysieren vorkommen Name Pessoas Publikationsplän Tod früh dokument berücksichtigen Bearbeitung netzwerkdaten Hilfe xslt neriern liegen interaktiv Visualisierung Bibliothek erstellen wobei netzwerklayout Mike Bostock adaptieren Funktionalität ergänzen Erweiterunge vorliegend Anwendung vornehmen Möglichkeit Teil Netzwerk Einund Auszublende Chronologie Teilnetzwerk Verbindung einzeln Person ausgehen knoten knoten Verbindung Option Darstellung Einblende labels dicht Weite Anzeige Netzwerks Abbildung Optionen Anzeige Netzwerks Größe knoten Netzwerk zeigen häufig einzeln Name dokumenten erwähnen Ermittlung Knotengröß Formel anwenden wobei siz tatsächlich Häufigkeit vorkommens stehen fiktiv Figur historisch Person unterscheiden knoten unterschiedlich eingefärben Türkis fiktiv Dunkelbau historisch netzwerkkanter verdeutlichen dick häufig Name gemeinsam dokument vorkommen häufig gemeinsam auftreten umso dick Linie Visualisierung minimal Kantendicke Pixel gemeinsam Erwähnung pro weit Gemeinsamer Erwähnung nehmen Kantendicke Pixel abstract Konferenz Digital Humanitie deutschsprachig Raum Abbildung Teilnetzwerk Fernando Pessoa Zentralem knoten zentral funktionalitäen Netzwerkanwendung auswahloptionen Chronologie namenserwähnunge betreffen Netzwerk Gesamtdarstellung vorkommen einzeln anzeigen hinaus Anzeige Periode konkurrierend Vorschlag periodisierung Werk verschieden Einteilung Periode Auswahl anbieten Weise Entwicklung pessoas Personenund Figurenkosmos Publikationsplän darstellen kritisch untersuchen betrachten Netzwerk chronologisch anhand ausgewählt perioden innerhalb der Publikationsplan verfassen Tendenz erkennen historisch editorisch poetisch Werk maßgeblich Bedeutung beispielsweise sehen Name Heteronyme Fernando Pessoa häufig vorkommen untereinander verbinden abb Abbildung Teilnetzwerk Abbildung Gesamtnetzwerk Pessoa häufigst Name vorkommen Name gesamt Netzwerk betrachten erkennen Fernando Pessoa Name häufig vorkommen direkt folgen Name Heteronyme Alberto Caeiro Ricardo Reis de Campos etabliert wissen Bedeutung Heteronyme pessoas Werk bestätigen abb Stelle stehen William Shakespeare Edgar Allan Poe rio de mora beispielsweise interessant sehen heteronyme Mora zeitweise stark Kandidat Rolle Heteronyms gelten Fernando Pessoa verbinden gewiß Autonomie heteronymisch Universum hindeuten Name Shakespeare Poe Weisen wichtig referenz pessoas englischsprachig Literatur Almada bedeutend zeitgenössisch portugiesisch Schriftsteller Name anderer Schriftsteller historisch Figur tendenziell spät Periode häufig zeigen zunehmend Interesse pessoas Veröffentlichung entscheidend Werk Weltliteratur wichtig Beziehung pessoas zeitgenössisch portugiesisch Schriftsteller Kritiker Netzwerk deutlich erkennen abb handeln modernistisch Generation Zeitschrift orpheu versammeln pessoas Tod sogenannter modernistisch Generation Zeitschrift abstract Konferenz Digital Humanitie deutschsprachig Raum herstellen interaktiv Visualisierung digital Edition angebunden fußnot Abbildung Teilnetzwerk fazit digital Edition letzter Zugriff Januar verfügbar Entwicklung fortlaufend Github organisieren letzter Zugriff Januar editorisch Herangehensweise Digitaler Perspektive Hennykrahmer Netzwerk zugrunde liegend daten Network letzter Zugriff Januar einsehbar Bostock tragen Titel Force Layout with Mousover Bostock letzter Zugriff Januar derzeit Menüpunkt Chronologie http letzter Zugriff Januar Netzwerkinterpretation gewonnen literaturhistorisch Erkenntnis bestätigen Pessoa Edition Publikation Werk Planung bedeutungsebene Werke unterscheiden bestimmt Name insgesamt bestimmt Zeit häufig Publikationsplän auftauchen gleichbedeutend Wichtigkeit Werk entsprechend Zeitperiode gelten Vorkommen einzeln Name gemeinsam vorkommen mehrere Name wodurch Bedeutung bestimmt Konstellatione bestimmt Zeit sowohl pessoas Publikationsplän Werk deutlich grundlegend Erkenntnis Interpretation pessoas Werk bestätigen Intuition Kritiker groß Zusammenhang mehrere Ebene pessoas Werk Ebene Edition Werk urib Vorstellung pessoas Werkganzem materiell Fragmentarität Schrift gegenüberstehen martins methodisch eröffnen Visualisierung interaktiv Element hoch Grad Abstraktion ediert dokumenten digital Edition studieren interpretationsspielraum wichtig Textund Datengrundlage methodisch Weg Netzwerk visuell Darstellung stets Blick behalten Beitrag bieten Dokumentation direkt interaktiv netzwerk verbinden Zusammenhang quellen Analyse bibliographie Bostock Mike force Layout with mouseover Label Mike Bostock äôs Block Mbostock letzter Zugriff Januar cunha Teresa Sobral planos e projectos editoriais -- fernando Pessoa uma velha Revista Biblioteca Nacional rie vol uma Pastoril Pelo Diabo Pessoa e pascoaes Pessoana ensaios Lissabon Imprensa Moeda Manuel o fausto äî Teatro em Ulrike Pedro pessoa äôs editorial projects and Publications the Digital Edition As Multiple Form -- textual criticism Boot Peter Cappellotto Anna dillen Wout Fischer Franz Kelly n Mertgens Andreas Sichani Spadini Elena van Hulle Dirk eds advances Scholarly editing Paper Presented at The Dixit conferences The hague Cologne And antwerp leiden Sidestone press letzter Zugriff Januar Martin Fernando Cabral breves Nota Sobre Alta Pessoa Fernando Teoria hsg Fernando Cabral Martin Richard Zenith Lissabon alvim rigal alexandre Rodighiero Dario cellard Loup The Trajectories Tool Amplifying Network Visualization Complexity Digital Humanitie abstract Konferenz Digital Humanitie deutschsprachig Raum Conference abstracts Jagiellonian University Pedagogical University abstracts letzter Zugriff Januar Pedro os livros -- fernando Pessoa Lissabon Pedro Ulrike EDS Fernando pessoa äì digital Edition Projekt Publikation editorisch Leitung Pedro technisch Leitung Ulrike Lissabon Köln ieln Universität Lissabon Cceh Universität Köln letzter Zugriff Januar Pedro uriben Jorge o planeamento editorial -- fernando Pessoa Lissabon Imprensa Moeda xanthos Aris Pant Isaak rochat Yannick Grandjean Martin visualizing The dynamics -- character networks Digital Humanitie Conference abstracts Jagiellonian University Pedagogical University abstracts letzter Zugriff Januar,"[('pessoa', 0.40734307931156866), ('pessoas', 0.3869759253459902), ('fernando', 0.285140155518098), ('name', 0.1753786671757278), ('januar', 0.16582795489699892), ('publikationsplän', 0.16293723172462746), ('lissabon', 0.16293723172462746), ('werk', 0.14636660823645195), ('zugriff', 0.1279843701382135), ('edition', 0.12379670624169023)]"
2019,DHd2019,159_final-LAUBROCK_Jochen_Grundzüge_einer_visuellen_Stilometrie.xml,Grundzüge einer visuellen Stilometrie,"Jochen Laubrock (Universität Potsdam, Deutschland); David Dubray (Universität Potsdam, Deutschland)","Convolutional Neural Network, Maschinelles Sehen, Visuelle Stilometrie","Stilistische Analyse, Bilder","  Als Material verwenden wir zwei Sammlungen grafischer Literatur: (a) das Graphic Narrative Corpus (GNC; Dunst, Hartel and Laubrock 2009) und (b) Manga109 (Matsui et al. 2017). Das GNC ist eine kuratierte Sammlung über 200 zeitgenössischer Graphic Novels aus den Jahren 1979 bis 2017 mit einem Gesamtumfang von mehr als 50.000 Seiten. Der GNC beinhaltet Werke verschiedener Genres (z.B. Autobiographie, New Journalism, Crime, Superhelden). Manga109 besteht aus 109 Manga-Bänden (mehr als 20.000 Seiten), die zwischen 1970 und 2010 im Handel erhältlich waren und 2017 der Wissenschaft zur Verfügung gestellt wurden. Die Korpora wurden durch zufällige geschichtete Stichprobenziehung in ein Trainings- und ein Testcorpus unterteilt. Der CNN-Teil eines auf dem ImageNet-Datensatz (Deng et al. 2009) vortrainierten Xception-Netzwerk wurde benutzt, um Illustratoren in den beiden Korpora zu klassifizieren.  Zusätzlich haben wir Klassifikationen basierend auf dem Output einzelner Schichten berechnet. Insgesamt vergleichen wir also die Klassifikation unter Berücksichtigung einzelner Schichten 0, 1, ..., k vs. mit der bei Berücksichtigung aller Schichten von 0 bis k. Der Merkmalsvektor wurde im letzteren Fall durch einfache Verkettung der Signaturen gebildet. Abbildung 1 zeigt die Genauigkeit der Klassifikation als Funktion der zugrundeliegenden Merkmale. Insgesamt lassen sich die Seiten aufgrund rein visueller Analyse sehr gut ihren Urhebern zuordnen. Man erkennt am Abfall der Kurve für Merkmale aus einzelnen Schichten, dass für die Illustrator-Klassifikation die Repräsentationen mittlerer Ebenen am entscheidendsten sind. Die stilistische Signatur einer Graphic Novel basiert scheinbar eher auf Merkmalen mittlerer Komplexität wie Schraffuren, Texturen oder Schwüngen als auf höher integrierten Merkmalen wie Objektteilen oder spezifischen Motiven. Basierend auf den Merkmalsvektoren haben wir eine bildbasierte Öhnlichkeitssuche implementiert. Nach Eingabe eines Suchbildes werden beispielsweise die 10 ähnlichsten Bilder ausgegeben. Die Untersuchung der Klassifikationsfehler ist interessant, sie zeigt beispielsweise, dass unterschiedliche Werke eines Autors zusammen gruppiert werden. Verwechslungen treten eher innerhalb von als zwischen Genres auf. Selbst historische Entwicklungen lassen sich abbilden: In ""750 Years in Paris"" illustriert Vincent Mahé die Entwicklung eines Häuserblocks in Paris von 1265 bis 2015. Die Bildsuche mit einer ""frühen"" Seite liefert Bilder aus der frühen Zeit, ebenso liefert die Bildsuche mit einer ""späten"" Seite Bilder aus einer späteren Epoche. Bei der semantischen Segmentation von Sprechblasen haben wir ein hervorragendes Ergebnis erzielt. Der F1-Score auf dem Testset betrug 0.935. Auch Elemente wie ein geschwungener Hinweisstrich / Dorn und an den Rändern offene Sprechblasen konnten sehr gut segmentiert werden. Abbildung 2 zeigt ein Beispiel einer Seite, auf der alle Sprechblasen korrekt detektiert und sehr gut segmentiert wurden. Wir haben verschiedene Sammlungen grafischer Literatur mit CNNs beschrieben und den Beitrag interner CNN-Repräsentationen unterschiedlicher Schichten zur Klassifikation von Zeichenstilen untersucht. Unsere Ergebnisse zeigen, dass der Individualstil eines Zeichners eher durch Merkmale mittlerer als durch solche höherer Komplexität charakterisiert ist. Allgemein haben CNN-basierte Repräsentationen das Potenzial, eine formale Beschreibung stilistischer Merkmale abzubilden. Sie sind deshalb aussichtsreiche Kandidaten für eine quantitative Fundierung bildwissenschaftlicher Form- und Strukturanalyse.",de,Material verwenden Sammlung grafisch Literatur Graphic Narrative Corpus gnc Dunst Hartel And Laubrock b Matsui et gnc kuratiert Sammlung zeitgenössisch Graphic Novels Gesamtumfang Seite gnc beinhalten werk verschieden Genr autobiographie New Journalism Crime superhelden bestehen Seite Handel erhältlich Wissenschaft Verfügung stellen Korpora zufällig geschichtet Stichprobenziehung Testcorpus unterteilen Deng et vortrainiert benutzen illustratoren Korpora klassifizieren zusätzlich Klassifikation basierend Output einzeln Schicht berechnen insgesamt vergleichen Klassifikation Berücksichtigung einzeln Schicht k Berücksichtigung Schicht Merkmalsvektor letzter Fall einfach Verkettung Signatur bilden Abbildung zeigen Genauigkeit Klassifikation Funktion zugrundeliegend Merkmal insgesamt lassen Seite aufgrund rein visuell Analyse Urheber zuordnen erkennen Abfall Kurve merkmal einzeln Schicht repräsentation mittler Ebene entscheidendster stilistisch Signatur Graphic Novel basieren scheinbar eher merkmal mittler Komplexität Schraffur Textur schwüngen hoch integriert merkmal objektteilen spezifisch Motiv basierend Merkmalsvektore bildbasiert öhnlichkeitssuch implementieren Eingabe suchbildes beispielsweise ähnlichsten Bild ausgeben Untersuchung Klassifikationsfehler interessant zeigen beispielsweise unterschiedlich Werk Autor gruppiert verwechslungen treten eher innerhalb Genre historisch Entwicklung lassen abbilden years Paris illustrieren vincent Mahé Entwicklung häuserblocks Paris bildsuche früh Seite liefern Bild früh liefern bildsuche spät Seite Bild spät Epoche semantisch Segmentation Sprechblase hervorragend Ergebnis erzielen Testset Betrug element geschwungen hinweisstrich Dorn rändern offen Sprechblase segmentieren Abbildung zeigen Seite Sprechblase korrekt detektieren segmentieren verschieden Sammlung grafisch Literatur Cnn beschreiben Beitrag intern unterschiedlich Schicht Klassifikation zeichenstilen untersuchen Ergebnis zeigen Individualstil Zeichner eher merkmal mittler hoch Komplexität charakterisieren allgemein repräsentationen Potenzial formal Beschreibung stilistisch merkmal abzubilden aussichtsreich Kandidat quantitativ Fundierung Bildwissenschaftlicher Strukturanalyse,"[('schicht', 0.3409220787990681), ('merkmal', 0.23562109022945651), ('seite', 0.2266056909440767), ('mittler', 0.2196134155087912), ('gnc', 0.19386788787106435), ('sprechblase', 0.178807719641714), ('graphic', 0.16812236023333746), ('bildsuche', 0.1464089436725275), ('klassifikation', 0.14571129899239615), ('segmentieren', 0.13636883151962725)]"
2019,DHd2019,228_final-SPORLEDER_Caroline_Multimodales_Zusammenspiel_von_Text_und_e.xml,Multimodales Zusammenspiel von Text und erlebter Stimme 'Analyse der Lautstärkesignale in direkter Rede,"Svenja Guhr (GCDH, Universität Göttingen, Deutschland); Hanna Varachkina (GCDH, Universität Göttingen, Deutschland); Geumbi Lee (GCDH, Universität Göttingen, Deutschland); Caroline Sporleder (GCDH, Universität Göttingen, Deutschland)","Lautstärkeanalyse, Literaturanalyse, Expressionismus, Verba Dicendi","Beziehungsanalyse, Stilistische Analyse, Sprache, Multimodale Kommunikation, Ton, Text"," Autorinnen und Autoren (i.F. Autoren) nutzen die direkte Rede als Mittel, um den Figuren eine Stimme zu geben und sie in den Köpfen ihrer Leser sprechen zu lassen (Nord, 1997). Als Möglichkeiten bestehen Geräuschbeschreibungen aber auch die redeeinleitenden Verben, Verba Dicendi, die bei der Verschriftung der Figurenrede eine relevante Rolle spielen. Diese Verbgruppe beschreibt die Art und Weise, wie die Leser sich die Konversation der Figuren vorzustellen haben, d.h. ob die Romanfiguren z.B. schreiend oder flüsternd kommunizieren. Verba Dicendi können anhand ihrer multimodalen Eigenschaft das Inhaltsverständnis unterstützen. Sie beschreiben die Sprechsituation und die Realisierung der direkten Rede und dienen dem multimodalen Zusammenspiel von Text und erlebter Stimme.  Im Rahmen einer Pilotstudie haben wir die Verba Dicendi und die sie beschreibenden Adverbien näher betrachtet mit dem Ziel stichprobenartig zu untersuchen, inwiefern die Lautstärkesignale eines Textes mit literaturwissenschaftlich relevanten Kategorien korrelieren. Eine schon von Katsma (2014) geäußerte Hypothese ist, dass Lautstärke mit ""Emotionalität"" zusammenhänge. Eine literarische Strömung wie der Expressionismus, der durch eine Betonung des inneren Ausdrucks und eine Ablehnung des Rationalen charakterisiert ist, müsste sich daher durch große Schwankungen zwischen lauten und leisen Passagen auszeichnen. Um diese Hypothese zu untersuchen wurden drei Korpora (insg. 161 Texte) zusammengestellt. Das erste besteht aus 57 Prosatexten um 1900 von Autoren, die der expressionistischen Literaturströmung zugeordnet werden können (Anz, 2016: 5f.). Als Vergleichsbasis dient ein zweites Korpus desselben Zeitraums von Autoren, die verschiedenen literarischen Strömungen zugeordnet werden (Fin de Si√®cle, Exilliteratur, Naturalismus, Realismus). Das dritte Korpus, bestehend aus 14 kurzen Texten von Autoren um 1900, dient als manuell annotiertes Kontrollkorpus der Evaluierung der Ergebnisse.  Zudem wurden Adjektive und Adverbien, die die Verba Dicendi als Träger der Beschreibungsinformationen der direkten Rede umgeben, in einem Fenster von vier Tokens vor und nach dem Verb in die Betrachtung miteinbezogen. Dabei wurde herausgefunden, dass vor allem die Adverbien und Kollokationen (z.B. ""mit lauter Stimme"") für das Lautstärkeprofil relevant sind. ",de,autorinn Autor Autor nutzen direkt Rede Figur Stimme geben köpfen Leser sprechen lassen Nord Möglichkeit bestehen Geräuschbeschreibung redeeinleitend verben Verba dicendi Verschriftung Figurenrede relevant Rolle spielen Verbgruppe beschreiben Art Weise Leser Konversation Figur vorstellen romanfigur schreiend flüsternd kommunizieren Verba dicendi anhand multimodal Eigenschaft inhaltsverständnis unterstützen beschreiben Sprechsituation Realisierung direkt Rede dienen multimodal Zusammenspiel Text erlebt Stimme Rahmen Pilotstudie Verba dicendin beschreibend Adverbie nah betrachten Ziel stichprobenartig untersuchen inwiefern Lautstärkesignale Text literaturwissenschaftlich relevant Kategorie korrelieren Katsma geäußert Hypothese Lautstärke Emotionalität zusammenhängen literarisch Strömung Expressionismus Betonung innerer Ausdruck Ablehnung Rationale charakterisieren müsste Schwankung Laute leise passag auszeichnen Hypothese untersuchen Korpora Insg Text zusammenstellen bestehen Prosatext Autor expressionistisch Literaturströmung zuordnen anz Vergleichsbasis dienen Korpus zeitraums Autor verschieden literarisch Strömung zuordnen fin de cle Exilliteratur naturalismus Realismus Korpus bestehend kurz Text Autor dienen manuell annotiert Kontrollkorpus Evaluierung Ergebnis zudem adjektiv Adverbie Verba dicendi Träger Beschreibungsinformation direkt Rede umgeben Fenster Token Verb Betrachtung miteinbezogen herausfinden adverbie Kollokation lauter Stimme Lautstärkeprofil relevant,"[('verba', 0.3631658606008866), ('dicendi', 0.272374395450665), ('adverbie', 0.25369610019556293), ('stimme', 0.19823347451307388), ('autor', 0.17989845568497492), ('rede', 0.16191212631703236), ('strömung', 0.16029575920591446), ('multimodal', 0.13539136553244502), ('direkt', 0.11130307442013698), ('leser', 0.10937008891480064)]"
2019,DHd2019,260_final-VITT_Thorsten_Intervalle__Konflikte__Zyklen__Modellierung_vo.xml,"Intervalle, Konflikte, Zyklen. Modellierung von Makrogenese in der Faustedition","Thorsten Vitt (Julius-Maximilians-Universität Würzburg, Deutschland); Gerrit Brüning (Freies Deutsches Hochstift / Frankfurter Goethe-Museum); Dietmar Pravida (Freies Deutsches Hochstift / Frankfurter Goethe-Museum); Moritz Wissenbach (Julius-Maximilians-Universität Würzburg, Deutschland)","Chronologie, Datierung, Faust, Graphen, Werkgenese","Beziehungsanalyse, Modellierung, Daten, Manuskript, Forschungsprozess"," Besonders schwierig ist es, nicht bloß einzelne Objekte zu datieren, sondern eine große Menge in eine chronologische Ordnung zu bringen, wenn die Objekte genetisch voneinander abhängig sind, nur wenige absolute Daten zur Verfügung stehen und sonst nur lokale Anhaltspunkte für relative Chronologien gegeben sind (klassisches Beispiel: die antike Chronographie; Grafton 1993, Burgess/Witakowski 1999). Dies ist auch bei umfangreichen genetischen Handschriftendossiers neuzeitlicher Autoren und Werken mit komplexer Entstehungsgeschichte der Fall. Hier können einzelne Teilentwürfe in relativer zeitlicher Beziehung stehen, vereinzelt sind Datierungen verfügbar; doch die Rekonstruktion der Und so liegen die Dinge auch bei ""Faust"". Nur wenige der makrogenetischen Objekte sind genau datierbar; stattdessen gibt es eine große Menge relativer, aber strikt lokaler Chronologien für jeweils nur einige Objekte. Den bislang einzigen Versuch, Einzelaussagen zu aggregieren und alle Objekte in zeitlich-stemmatische Beziehung zueinander zu setzen, macht Fischer-Lamberg für zwei Akte des ""Faust II"". Ihre Stemmata (Fischer-Lamberg 1955: 150–166) markieren die praktische Grenze dessen, was an Einzelinformationen mit menschlichen Mitteln aggregiert werden kann. Die Rekonstruktion einer (theoretisch beliebig großen) Makrogenese verlangt nach maschineller Verarbeitung und visueller Aufbereitung vorhandener Einzelinformationen. Um dies zu ermöglichen, wurde der Informationsgehalt der verfügbaren einschlägigen Aussagen zur Datierung Eine  Eine  Die verschiedenen Aussagen werden in einem gerichteten Graphen modelliert  Aus diesem Graphen lassen sich Informationen ableiten, die sich erst aus dem Zusammenspiel der Einzelaussagen ergeben: Betrachtet man einen Zeugen Ist die Gesamtheit der Aussagen nicht widerspruchsfrei, so ergeben sich Zyklen im Graphen. Dies induziert einen Teilgraphen, in dem (vgl. Abb. 4 mit relativen Datierungen einiger Handschriften) jeder Knoten von jedem anderen erreichbar ist (der Teilgraph ist  Um den Graphen aus Abb.¬†4 zyklenfrei zu machen, ist die Entfernung von wenigstens drei Kanten notwendig (gestrichelt). Der komplette Makrogenesegraph enthält eine stark zusammenhängende Komponente mit 477 Dokumenten und 2136 Kanten 'zu umfangreich, um die Konflikte manuell zu eliminieren. Eine möglichst kleine Menge von Kanten zu entfernen, um einen zyklenfreien Graphen zu erhalten, ist ein als Entfernt man alle Konfliktkanten, so erhält man einen zyklenfreien gerichteten Graphen (DAG), der die Basis für die automatisierte Weiterverarbeitung ist. Dessen Knoten können in eine Um die aus einer Vielzahl teils widersprüchlicher Aussagen mechanisch gezogenen Schlüsse nachvollziehbar und verbesserbar zu machen, werden die Daten in einer Reihe verlinkter Darstellungen mit GraphViz (Gansner/North 2000) visualisiert. Die Grundlage bildet der Gesamtgraph mit allen Aussagen, in dem algorithmisch entfernte Aussagen (Konflikte) rotgestrichelt visualisiert werden. Zu jedem Zeugen gibt es einen Teilgraphen, der seine Nachbarn, die nächsten erreichbaren absoluten Datierungen und alle Aussagen dazwischen visualisiert. Darunter werden die Aussagen tabellarisch aufgelistet. Zu jeder (entfernten) Konfliktkante zeigt eine separate Visualisierung einen Pfad in der Gegenrichtung, mit dem die Kante in Konflikt stand, so dass der Konflikt erkennbar wird und zu den beteiligten Zeugen weiternavigiert werden kann. Anhand der Visualisierungen können die vorliegenden Reihenfolgeentscheidungen nachvollzogen, aber auch Datierungskontroversen und -lücken identifiziert undin den Quelldateien behoben werden: Die visualisierten Ergebnisse erfüllen so einen mehrfachen Zweck: Sie dienen zur systematischen Erschließung der einschlägigen Forschung, zur Klärung der genetischen Verhältnisse für den gesamten Faust und als Hilfsmittel zur Überprüfung, Vervollständigung und Verbesserung der Datenlage, d.h. zur Erweiterung des Forschungsstand. Darüber hinaus wird die ermittelte Reihenfolge in der Faustediton verwendet, um die Zeilensynopse sowie das Balkendiagramm zu sortieren. Neben der manuellen Nachbearbeitung der Daten kommen zur Verbesserung des Verfahrens alternative Heuristiken für das Minimum-Feedback-Arc-Problem in Frage (z.B. Even et al. 1998).  Eine Alternative zu der oben beschriebenen Abbildung unscharfer Aussagen auf scharf begrenzte Intervalle ist etwa die Modellierung über Fuzzy-Mengen (vgl. z.B. Barro et al. 1994). Dies erfordert jedoch auch die Neudefinition der Relationen (Schockaert/De Cock 2008) und der darauf aufbauenden Verfahren etwa zur Konfliktauflösung. Der vorgestellte Ansatz basiert auf bereits vorhandenen, mit traditionellen philologischen Mitteln gewonnenen Datierungsaussagen. In Wissenbach/Pravida/Middell (2012) wird ein Verfahren vorgestellt, mit der kodierte, textinhärente Eigenschaften von Fassungen für die regelbasierte Bildung genetischer Hypothesen genutzt werden, um auf diesem Weg generelle Hypothesen zur Arbeitsweise des Autors zu verifizieren.",de,schwierig bloß einzeln Objekt datieren Menge chronologisch Ordnung bringen Objekt genetisch voneinander abhängig absolut daten Verfügung stehen lokal Anhaltspunkt relativ Chronologien geben klassisch antik Chronographie Grafton burgess Witakowski umfangreich genetisch handschriftendossiers neuzeitlich Autor Werk komplex entstehungsgeschichen Fall einzeln teilentwürfe Relativer zeitlich Beziehung stehen vereinzeln datierungen verfügbar Rekonstruktion liegen dinge Faust makrogenetisch Objekt genau datierbar stattdessen Menge relativ strikt lokal chronologien jeweils Objekt bislang einzig Versuch einzelaussag aggregieren Objekt Beziehung Zueinander setzen Akt Faust ii Stemmata markieren praktisch Grenze einzelinformation menschlich Mittel aggregiert Rekonstruktion theoretisch beliebig Makrogenese verlangen Maschineller Verarbeitung visuell Aufbereitung vorhanden einzelinformationen ermöglichen Informationsgehalt verfügbar einschlägig Aussage Datierung verschieden Aussage gerichtet Graphen modellieren Graphen lassen Information ableiten Zusammenspiel einzelaussag ergeben betrachten Zeuge Gesamtheit Aussage widerspruchsfrei ergeben Zykl Graphen induzieren teilgraph abb relativ datierungen handschrift knoten erreichbar Teilgraph graphen Zyklenfrei Entfernung Kant notwendig stricheln komplett Makrogenesegraph enthalten stark zusammenhängend Komponente dokument Kant umfangreich Konflikt manuell eliminieren möglichst Menge Kant entfernen Zyklenfreien graphen erhalten entfernt konfliktkanen erhalten Zyklenfreien gerichtet Graph dag Basis automatisiert Weiterverarbeitung knoten Vielzahl teils widersprüchlich Aussage mechanisch gezogen schlüssen nachvollziehbar verbesserbar daten Reihe verlinkt Darstellung Graphviz Gansner North visualisiern Grundlage bilden Gesamtgraph Aussage algorithmisch entfernt Aussage Konflikt rotgestricheln visualisiern Zeugen teilgraph Nachbar nächster erreichbar absolut Datierung Aussage visualisieren Aussage tabellarisch auflisten entfernt könfliktkanen zeigen separat Visualisierung Pfad Gegenrichtung Kante Konflikt stehen Konflikt erkennbar beteiligt Zeuge weiternavigieren anhand visualisierung vorliegend Reihenfolgeentscheidung nachvollziehen datierungskontroversen identifizieren Undin Quelldateien beheben visualisiert Ergebnis erfüllen mehrfach zweck dienen systematisch Erschließung einschlägig Forschung Klärung genetisch Verhältnis gesamt Faust Hilfsmittel Überprüfung Vervollständigung Verbesserung Datenlage Erweiterung Forschungsstand hinaus ermittelt Reihenfolge Faustediton verwenden Zeilensynopse Balkendiagramm sortieren manuell Nachbearbeitung daten Verbesserung Verfahren alternativ Heuristik Frage ev et Alternative beschrieben Abbildung unscharf Aussage scharf begrenzt Intervalle Modellierung barro et erfordern Neudefinition relationen schockaert de cock aufbauend Verfahren Konfliktauflösung vorgestellt Ansatz basieren vorhanden traditionell philologisch Mittel Gewonnen datierungsaussagen Wissenbach Pravida Middell Verfahren vorstellen kodieren textinhärenen eigenschaften Fassung regelbasiert Bildung genetisch Hypothese nutzen Weg generell Hypothese Arbeitsweise Autor verifizieren,"[('aussage', 0.3072927148218585), ('graphen', 0.2273813340269167), ('genetisch', 0.20615809964723072), ('teilgraph', 0.18297430755288643), ('konflikt', 0.1775577212281312), ('objekt', 0.1652839924092798), ('kant', 0.15461857473542304), ('faust', 0.14420603152153422), ('entfernt', 0.13642880041615002), ('datierungen', 0.12198287170192429)]"
2019,DHd2019,156_final-MEYER_SICKENDIEK_Burkhard_Deep_Learning_als_Herausforderung_.xml,Deep Learning als Herausforderung für die digitale Literaturwissenschaft,"Fotis Jannidis (Julius-Maximilians-Universität Würzburg, Deutschland); Christof Schöch (Universität Trier, Trier Center for Digital Humanities, Deutschland); Jonas Kuhn
(Universität Stuttgart, Centrum für Reflektierte Textanalyse, Deutschland); Timo Baumann (Carnegie Mellon University, Pittsburgh, USA); Hussein Hussein (Freie Universität Berlin, Deutschland); Thomas Haider (Max-Planck-Institut für empirische ästhetik, Frankfurt am Main, Deutschland); Burkhard Meyer-Sickendiek (Freie Universität Berlin, Deutschland)","Deep Learning, Embedding, Prosodieerkennung, Topic Modelling","Deep Learning, Embedding, Prosodieerkennung, Topic Modelling"," Bisher konzentrierten sich die klassischen ""Digital Humanities"" eher auf die Generierung und Reflexion digitaler Ressourcen wie Textausgaben, Repositorien oder Bilddatenbanken. Dagegen gibt es nur wenige Versuche, Deep Learning in die digitalen Geisteswissenschaften einzubringen. Zumeist wurde Deep Learning in sehr großen Datenbanken von Unternehmen wie Google, YouTube, Bluefin Labors oder Echonest getestet, etwa um Social Media Signale und den Inhalt von Medien in sozialen Netzwerken zu analysieren. Gerade deshalb blieb in diesem Feld die alte Kluft zwischen traditionellen Geisteswissenschaften und Informatik bestehen. Unser Panel will einen Beitrag leisten, um diese Lücke zu schließen. Wir wollen vor allem die Probleme erörtern, die bei der rechnerischen Analyse literarischer Texte mit Techniken des tiefen Lernens entstehen, z.B.: Können maschinelle Lerntechniken durch Clustering tatsächlich verdeckte Muster in Textdaten erfassen (Graves 2012)? Wie lassen sich auf der Grundlage eines maschinell erlernten Modells Grenzfälle, Kategorisierungsfehler, Ausreißer und ähnliche Besonderheiten erkennen bzw. in den Klassifikationsprozess einbauen? Wie geht man mit dem großen Problem der ""black box"" um, wie lassen sich die in den ""hidden layers"" stattfindenden Klassifikationsprozesse nachvollziehen bzw. gar transparent machen? Und welche Tools für die manuelle (z.B. Sonic Visualizer) und automatische Annotation (z.B. PRAAT, ToBI, oder Sphinx) bzw. welche Softwares für die Modellierung (DyNet, TensorFlow, Caffe, MxNet, Keras, ConvNetJS, Gensim, Theano, und Torch) sind empfehlenswert? Fragestellung und Aufbau des Panels  Folgende Personen haben dabei eine Teilnahme an dem Panel zugesagt: Wir werden in einem ersten Teil von maximal 30 Minuten einzelne Impulsvorträge präsentieren, und dann in einem zweiten Teil von ebenfalls 30 Minuten Topic Modeling und Embedding als wichtige Themenfelder des tiefen Lernens in den Geisteswissenschaften fokussieren. In einem dritten Teil von ebenfalls 30 Minuten wollen wir dann das Panel für die Diskussionen mit dem Publikum öffnen. Mögliche Themengebiete für die Paneldiskussion im dritten und letzten Teil wären insbesondere die Verwendung tiefer Lernverfahren in den digitalen Geisteswissenschaften, etwa mit Blick auf Stilometrie, Computerstilistik, Reim- und Metrikenanalyse, Aktantenanalyse, oder Themenmodellierung. Dabei soll die Publikumsdiskussion all jenen ein Forum bieten, die sich ein tieferes Verständnis und eine praktische Schulung in deep learning sowie eine Plattform für den Austausch von Praktiken, Ergebnissen und Erfahrungen im Umfeld mit einschlägigen Tools erhoffen. Dies kann sich auch auf Kenntnisse aus den Nachbardisziplinen erstrecken, insofern diese über bereits vorhandenes Wissen hinsichtlich der Anwendung ""tiefer Lerntechniken"" etwa im Bereich des Data Mining, der Statistik oder der Verarbeitung natürlicher Sprache verfügen. Auf diese Weise erhoffen wir uns eine effektive Fokusverlagerung innerhalb der digitalen Geisteswissenschaften: von der Erstellung und Archivierung digitaler Artefakte und Repositorien hin zu echten Rechenlösungen auf der Grundlage maschinellen Lernens.",de,konzentriern klassisch Digital humanities eher Generierung Reflexion Digitaler Ressource Textausgaben Repositorien bilddatenbanken Versuch Deep Learning digital geisteswissenschaft Einzubring zumeist deep Learning datenbanken Unternehmen Google Youtube bluefin Labors echonest testen social Media signal Inhalt Medium sozial netzwerken analysieren bleiben Feld alt Kluft traditionell geisteswissenschaften Informatik bestehen Panel Beitrag leisten Lücke schließen Problem erörtern rechnerisch Analyse literarisch Text Technik tief Lernen entstehen maschinell lerntechniken Clustering tatsächlich verdeckt Muster Textdat erfassen Graves lassen Grundlage maschinell erlernet modells Grenzfälle Kategorisierungsfehler ausreiß ähnlich Besonderheit erkennen klassifikationsprozess einbauen Problem black box lassen hidd layers stattfindend Klassifikationsprozesse nachvollziehen transparent Tools Manuelle Sonic visualizer automatisch Annotation Praat Tobi Sphinx Softwares Modellierung dynen tensorflow Caffe mxnen Keras Convnetjs Gensim Theano Torch empfehlenswert Fragestellung Aufbau Panel folgend Person Teilnahme Panel zusagen maximal Minute einzeln impulsvorträge präsentieren ebenfalls Minute Topic Modeling Embedding wichtig Themenfelder tief Lernen geisteswissenschaft fokussieren ebenfalls Minute Panel Diskussion Publikum öffnen möglich Themengebiete Paneldiskussion letzter sein insbesondere Verwendung tief lernverfahren digital geisteswissenschaften Blick Stilometrie Computerstilistik Metrikenanalyse Aktantenanalyse Themenmodellierung Publikumsdiskussion all Forum bieten tiefeer Verständnis praktisch Schulung Deep Learning Plattform Austausch praktiken Ergebnis Erfahrung Umfeld einschlägig Tools erhoffen kenntnisse nachbardisziplin erstrecken insofern vorhanden Wissen hinsichtlich Anwendung tief lerntechniken Bereich Data Mining Statistik Verarbeitung natürlich Sprache verfügen Weise erhoffen effektiv Fokusverlagerung innerhalb digital geisteswissenschaften Erstellung Archivierung Digitaler Artefakt repositorien echt Rechenlösung Grundlage maschinell Lernen,"[('tief', 0.22815205448642423), ('panel', 0.2105876646976686), ('deep', 0.1743108471675428), ('minute', 0.17111404086481818), ('lerntechniken', 0.1700366987539847), ('repositorien', 0.15010310536645638), ('learning', 0.1444104570862503), ('erhoffen', 0.13844270073012907), ('geisteswissenschaften', 0.13823749531734913), ('lernen', 0.13412608631967382)]"
2019,DHd2019,195_final-DIMPEL_Friedrich_Gute_Wörter_für_Delta__Verbesserung_der_Aut.xml,Gute Wörter für Delta: Verbesserung der Autorschaftsattribution durch autorspezifische distinktive Wörter,Friedrich Michael Dimpel (Universität Erlangen); Thomas Proisl (Universität Erlangen),"Stilometrie, Autorschaftsattribution, Quantitative Verfahren","Modellierung, Stilistische Analyse, Sprache, Personen, Text","Autorschaftsattributionsverfahren sind längst etabliert (vgl. Burrows 2002, Jannidis et al. 2015). Auf DHd-Jahrestagungen wurden Techniken vorgestellt, die die Attributionsverfahren optimieren (Büttner et al. 2016, Dimpel 2017a, 2018a/b). Die Optimierung der Verfahren ist deshalb wichtig, weil ein literaturwissenschaftliches Interesse besteht, die Frage nach der Autorschaft auch bei schwierigen Bedingungen zu klären. Gute Bedingungen, bei denen in Evaluationstests über hohe Erkennungsquoten berichtet wurde, sind dann gegeben, wenn viele Texte aus der gleichen Gattung vorliegen, wenn die Texte eine ausreichende Länge aufweisen (mindestens 5.000 Wortformen), wenn die Texte chronologisch nicht zu weit auseinander liegen und wenn die Texte einen möglichst normierten Sprachstand hinsichtlich Orthographie und Standardsprache aufweisen (Schöch 2014, Eder 2013a und 2013b). All diese Bedingungen sind nicht erfüllt, wenn man mittelhochdeutsche Kleinepik untersuchen möchte 'gerade etwa mit Blick auf die Kleinepik des Strickers wäre eine stilometrische Klärung bei einigen Texten interessant. Mittelhochdeutsche Texte folgen keiner geregelten Orthographie, es liegen sowohl mehr oder weniger normalisierte als auch nicht-normalisierte digitale Texte vor, sie sind oft dialektal geprägt und viele Texte aus dem Bereich der Kleinepik übersteigen kaum eine Anzahl von 2.000 Wortformen. Anhand der ""Halben Birne"" (umstrittene Autorschaft Konrads von Würzburg; 2.469 Wortformen) wurde ein Verfahren vorgestellt, wie mit Burrows"" Delta die distinktiven Wörter ermittelt werden können, die Konrads Wortschatz von anderen Autoren unterscheiden (Dimpel 2018a). Damit konnte die Erkennungsqualität so optimiert werden, dass Delta auf die ""Halbe Birne"" angewendet werden konnte. Nun wird anhand eines weniger problematischen Korpus (Romane ca. 19. Jahrhundert Für Burrows"" Delta ermittelt man relative Worthäufigkeiten 'hier für Die guten Wörter beruhen auf den Level-2-Differenzen: Die Formal ausgedrückt werden die Level-2-Differenzen anhand von zwei Teilkorpora ermittelt: Einem Teilkorpus Wir prüfen, ob mit ""guten Wörtern"" Texte des Zielautors besser von denen anderer Autoren abgegrenzt werden können als mit dem üblichen Delta-Ansatz, der rein auf den Für die Berechnung der guten Wörter und zur Evaluation werden ein Ratekorpus und ein Vergleichskorpus verwendet. Im Vergleichskorpus befinden sich ein Autorvergleichstext (Text vom Zielautor), je nach Testreihe zwanzig oder mehr Distraktortexte von Autoren, die nicht im Ratekorpus vertreten sind, sowie in manchen Experimenten Texte von den übrigen im Ratekorpus vertretenen Autoren. Wenn der Delta-Abstand zwischen Ratetext und Autorvergleichstext der niedrigste ist, gilt dieser Text als dem Zielautor zugeordnet; ist der Abstand zu einem Distraktortext von einem anderen Autor am niedrigsten, gilt der Text als falsch erkannt. Die guten Wörter werden jeweils auf den alphabetisch ersten drei Texten des Autors für jeden Autor in vier Varianten berechnet. Ausgangspunkt sind jeweils die häufigsten 1.200 Wörter: Wir verwenden stets 400 gute Wörter, da auch kurze Texte mit 2.000 Token getestet werden sollen. Wenn eine Variante mehr als 400 gute Wörter liefert, verwenden wir die 400 Wörter mit den größten Level-2-Differenz-Mittelwerten; wenn sie weniger als 400 gute Wörter liefert, füllen wir mit MFWs auf. Wir führen drei Testreihen durch: Dadurch werden wichtige Einsatzszenarien (lange Texte, kurze Texte mit langen Vergleichstexten und kurze Texte mit kurzen Vergleichstexten) abgedeckt. Hier evaluieren wir die Methoden bei vollständigen Romanen. Einmal enthält das Vergleichskorpus Texte von allen Autoren im Ratekorpus, im zweiten Schritt enthält das Vergleichskorpus zwar einen Text des Zielautors, jedoch keinen Text der anderen Autoren im Ratekorpus. Für das erste Experiment verwenden wir ein Vergleichskorpus, das je einen Text von allen 32 Autoren enthält. Für jeden Autor ermitteln wir die guten Wörter auf Basis des Vergleichskorpus und zwei weiteren Texten des Autors (also insgesamt drei Autortexten und 31 Distraktortexten). Die Ratekorpora für jeden Autor umfassen drei Texte des Autors und drei zufällige Texte von den übrigen Autoren. Mittelwerte über alle 32 Autoren: Die klassischen Delta-Varianten (MFW400, MFW1200) haben in Bezug auf den Zielautor eine hohe Precision und einen niedrigeren Recall, während sich das Verhältnis für Texte anderer Autoren umdreht: Niedrige Erkennungsquote, dafür kaum False Positives (Texte, die fälschlicherweise dem Zielautor zugeschrieben werden). Die Gute-Wörter-Varianten führen zu deutlichen Recall-Verbesserungen beim Zielautor auf Kosten einer etwas geringeren Precision. Die Precision für Texte anderer Autoren steigt zu Lasten des Recalls. Mit den guten Wörtern werden also mehr Texte des Autors richtig erkannt (>97%), dafür gibt es minimal mehr False Positives (2,5%). Mehr häufigste Wörter funktionieren besser als weniger. Alle vier Gute-Wörter-Varianten funktionieren besser als die reinen Delta-Varianten (Verbesserungen von 6–8 Punkten bei Accuracy und F Für das zweite Experiment wird lediglich das Vergleichskorpus verändert. Es enthält jetzt jeweils einen Text des Zielautors und je einen Text von 24 zusätzlichen Autoren (die sich nicht mit den 32 getesteten Autoren überschneiden). Die wahren Autoren für drei der sechs Texte in den Ratekorpora befinden sich nicht mehr im Vergleichskorpus, was die Klassifikation der entsprechenden Texte erschwert. Tatsächlich bewegen sich alle Ergebnisse auf einem etwas niedrigeren Niveau als im ersten Experiment. Auch hier: Mittels guter Wörter steigt der Recall für Texte des Zielautors deutlich, während die Precision leicht sinkt. Für Texte anderer Autoren verhält es sich umgekehrt. Auch hier führen die guten Wörter zu besseren Accuracy- und F In dieser Testreihe wollen wir prüfen, wie gut die einzelnen Methoden für kurze Rate- und lange (vollständige) Vergleichstexte funktionieren. Dazu ziehen wir für jeden Ratetext 100 Stichproben √† 2.000 Wörter; der übrige Versuchsaufbau ist identisch zu Testreihe 1. Test 2a) Alle Autoren aus den Ratekorpora sind im Vergleichskorpus vertreten: Die Ergebnisse sind auf den 2.000-Wort-Samples spürbar schlechter. Das Gesamtbild ist jedoch dasselbe: Die guten Wörter verbessern die Klassifikation und führen zu deutlich höheren Recall-Werten für den Zielautor bei etwas niedrigerer Precision. Test 2B) Die Nicht-Zielautoren sind nicht im Vergleichskorpus vertreten: Auch hier verbessern die guten Wörter die Erkennung der Zielautortexte (Erkennungsquote >96%). Größere Einbußen gibt es bei der Erkennung von Texten, die nicht vom Zielautor stammen, so dass die guten Wörter zwar die Erkennung von Texten des Zielautors enorm verbessern (Reduktion der Fehlerrate >80%), die Ergebnisse insgesamt aber leicht schlechter sind. Nunmehr werden die guten Wörter mit zwei Ratetexten und einem Autorvergleichstext berechnet; wenn jedoch mehr als sechs Texte pro Autor vorliegen, werden entsprechend mehr Ratetexte verwendet. Da diese drei (oder mehr) Texte nicht wie oben reihum als Rate- und Autorvergleichstext verwendet werden, ist die Qualität der Wortlisten etwas schlechter. Im Evaluationstest werden nicht nur im Ratekorpus kleine Bag-of-Words mit 2.000 Wortformen verwendet, sondern auch im Vergleichskorpus, wodurch die Erkennungsquoten deutlich schlechter werden. Die Parameter für die Author-Recall-Ermittlung sind: 250 Stichproben bei 400 MFWs, im Vergleichskorpus befinden sich neben dem Autorvergleichstext 24 Texte von anderen Autoren. Die False-Positives werden hier mit je zwei Texten von allen anderen 31 Autoren als Ratetexte in 32 Tests gegen das Vergleichskorpus ermittelt, in dem neben 24 Texten von anderen Autoren reihum alle 32 Autoren mit einem Autorvergleichstext vertreten waren (Parameter: Bag-of-Words mit 2.000 Wortformen, 400 MFWs, 100 Stichproben). Auch hier übertrifft der Gewinn bei der Verbesserung der Erkennungsquote die Verschlechterung bei den False-Positives. Wenn sich der fragliche Autor im Vergleichskorpus befindet, ergibt sich sogar bei den False-Positives bei A) und B) eine leichte Verbesserung. Bei C) ist die Verschlechterung bei den False-Positives problematisch. Die Gute-Wörter-Verfahren führen zu besseren Erkennungsquoten; in geringem Umfang treten mehr False-Positives auf, doch sind die positiven Effekte größer als die negativen. Die detaillierten Zahlen, für die hier kein Platz verfügbar ist, zeigen, dass die False-Positives je nach Autor erheblich schwanken. In einem Anwendungsfall kann vorab getestet werden, ob es bei dem fraglichen Autor zu erhöhten False-Positives kommt 'um zu prüfen, ob das Verfahren bei diesem Autor gut anwendbar ist. Künftig wollen wir untersuchen, ob ein mittels Maschinellem Lernen erstelltes autorspezifisches Vokabular zu weiteren Verbesserungen führt.",de,autorschaftsattributionsverfahren längst etablieren burrows Jannidis et Technik vorstellen attributionsverfahr optimieren büttner et Dimpel -- Optimierung Verfahren wichtig literaturwissenschaftlich Interesse bestehen Frage Autorschaft schwierig Bedingung klären Bedingung evaluationstests hoch Erkennungsquot berichten geben Text gleich Gattung vorliegen Text ausreichend Länge aufweisen mindestens Wortform Text chronologisch auseinander liegen Text möglichst normiert Sprachstand hinsichtlich Orthographie standardsprach aufweisen schöch ed all Bedingung erfüllen mittelhochdeutsch Kleinepik untersuchen Blick Kleinepik Stricker stilometrisch Klärung texten interessant mittelhochdeutsch Text folgen geregelt Orthographie liegen sowohl normalisieren digital Text dialektal prägen Text Bereich Kleinepik übersteigen Anzahl Wortforme anhand halb birne umstritten Autorschaft Konrad Würzburg Wortform Verfahren vorstellen burrow delta distinktiver Wörter ermitteln Konrad Wortschatz Autor unterscheiden dimpel Erkennungsqualität optimieren Delta halb Birn anwenden anhand problematisch Korpus Roman Jahrhundert burrow Delta ermitteln relativ worthäufigkeiten gut Wörter beruhen formal ausdrücken anhand Teilkorpora ermitteln Teilkorpus prüfen gut wörtern Text Zielautor anderer Autor abgrenzen üblich rein Berechnung gut Wörter Evaluation Ratekorpus Vergleichskorpus verwenden Vergleichskorpus befinden autorvergleichstext Text Zielautor Testreihe Distraktortexte Autor Ratekorpus vertreten experiment Text übrig Ratekorpus vertreten Autor Ratetext Autorvergleichstext niedrig gelten Text Zielautor zuordnen Abstand Distraktortext Autor niedrig gelten Text falsch erkennen gut Wörter jeweils alphabetisch Text Autor Autor Variant berechnen ausgangspunkt jeweils häufig Wörter verwenden stets Wörter kurz Text Toke testen Variante Wörter liefern verwenden Wörter groß Wörter liefern füllen mfws fahren testreih wichtig Einsatzszenari Text kurz Text lang vergleichstexten kurz Text kurz vergleichstexten abdecken evaluieren Methode Vollständig Roman enthalten Vergleichskorpus texte Autor Ratekorpus Schritt enthalten Vergleichskorpus Text Zielautor Text Autor Ratekorpus Experiment verwenden Vergleichskorpus Text Autor enthalten Autor ermitteln gut Wörter Basis Vergleichskorpus Text Autor insgesamt autortext distraktortexn Ratekorpora Autor umfassen Text Autor zufällig Text übrig Autor mittelwern Autor klassisch Bezug Zielautor hoch Precision niedrig Recall Verhältnis Text anderer Autor umdrehen niedrig Erkennungsquote fals positiv Text fälschlicherweise Zielautor zuschreiben führen deutlich Zielautor Kosten gering Precision Precision Text anderer Autor steigen Last recall gut wörtern Text Autor erkennen minimal Fals positiv häufigst Wörter funktionieren funktionieren rein Verbesserung Punkt Accuracy f Experiment lediglich Vergleichskorpus verändern enthalten jeweils Text Zielautor Text zusätzlich Autor getesteter Autor überschneiden wahr Autor Text Ratekorpora befinden Vergleichskorpus Klassifikation entsprechend Text erschweren tatsächlich bewegen Ergebnis niedrig Niveau Experiment mittels Wörter steigen Recall Text Zielautor deutlich Precision sinken Text anderer Autor verhalten umgekehrt fahren gut Wörter gut f Testreihe prüfen einzeln Methode kurz vollständig vergleichstext funktionieren ziehen Ratetext Stichprobe Wörter übrig Versuchsaufbau identisch Testreihe Test Autor Ratekorpora Vergleichskorpus vertreten Ergebnis spürbar schlecht Gesamtbild gut Wörter verbessern Klassifikation führen deutlich hoch Zielautor Niedrigerer Precision Test Vergleichskorpus vertreten verbessern gut Wörter Erkennung Zielautortexte Erkennungsquote groß einbußen Erkennung Text Zielautor stammen gut Wörter Erkennung Text Zielautor enorm verbessern Reduktion fehlerrat Ergebnis insgesamt schlecht nunmehr gut Wörter ratetexten autorvergleichstext berechnen Text pro Autor vorliegen entsprechend ratetexte verwenden Text reihum autorvergleichstext verwenden Qualität wortli schlecht Evaluationstest Ratekorpus Wortforme verwenden Vergleichskorpus wodurch Erkennungsquot deutlich schlecht Parameter Stichprobe Mfws Vergleichskorpus befinden autorvergleichstext Text Autor Text Autor Ratetexte Test Vergleichskorpus ermitteln Text Autor Reihum Autor autorvergleichstext vertreten parameter Wortforme mfws Stichprobe übertreffen Gewinn Verbesserung Erkennungsquote Verschlechterung fraglich Autor Vergleichskorpus befinden ergeben sogar -- leicht Verbesserung c Verschlechterung problematisch führen besseren erkennungsquoten gering Umfang treten positiv effekte groß negativen Detailliert zahlen Platz verfügbar zeigen Autor erheblich schwanken Anwendungsfall vorab testen fraglich Autor erhöht prüfen Verfahren Autor anwendbar künftig untersuchen mittels Maschinellem lernen erstellt autorspezifisch Vokabular Verbesserung führen,"[('autor', 0.41517229907480047), ('zielautor', 0.38096341303678477), ('vergleichskorpus', 0.3402488535725004), ('wörter', 0.2584339543402708), ('text', 0.2536150203823291), ('gut', 0.1912477024703033), ('autorvergleichstext', 0.19048170651839239), ('ratekorpus', 0.16815132188187487), ('precision', 0.10290879384069983), ('vertreten', 0.09691814535391974)]"
2019,DHd2019,183_final-KREMER_Gerhard_Maschinelles_Lernen_lernen__Ein_CRETA_Hackato.xml,Maschinelles Lernen lernen: Ein CRETA-Hackatorial zur reflektierten automatischen Textanalyse,"Gerhard Kremer (Institut für maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Kerstin Jung (Institut für maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland)","maschinelle Lernverfahren, Entitätenreferenzen, Reflexion, Korpus, Evaluation, Programmiercode, Methodik, Praxiserfahrung","Programmierung, Inhaltsanalyse, Annotieren, Lehre, Organisation, Methoden","Das Ziel dieses Tutorials ist es, den Teilnehmerinnen und Teilnehmern konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten. Es werden dabei keinerlei Vorkenntnisse zu maschinellem Lernen oder Programmierkenntnisse vorausgesetzt. Es gibt keinen Grund, den Ergebnissen von maschinellen Lernverfahren im Allgemeinen und NLP-Tools im Besonderen blind zu vertrauen. Durch die konkreten Einblicke in den ""Maschinenraum"" von maschinellen Lernverfahren wird den Teilnehmenden ermöglicht, das Potenzial und die Grenzen statistischer Textanalysewerkzeuge realistischer einzuschätzen. Mittelfristig hoffen wir dadurch, den immer wieder auftretenden Frustrationen beim Einsatz automatischer Verfahren für die Textanalyse und deren teilweise wenig zufriedenstellender Ergebnis-Daten zu begegnen, aber auch die Nutzung und Interpretation der Ergebnisse von maschinellen Lernverfahren (d.h. in erster Linie von automatisch erzeugten Annotationen) zu fördern. Zu deren adäquater Nutzung, etwa in hermeneutischen Interpretationsschritten, ist der Einblick in die Funktionsweise der maschinellen Methoden unerlässlich. Insbesondere ist die Art und Herkunft der Trainingsdaten für die Qualität der maschinell produzierten Daten von Bedeutung, wie wir im Tutorial deutlich machen werden. Neben einem Python-Programm für die automatische Annotierung von Entitätenreferenzen, mit und an dem während des Tutorials gearbeitet werden wird, stellen wir ein heterogenes, manuell annotiertes Korpus sowie die Routinen zur Evaluation und zum Vergleich von Annotationen zu Verfügung. Das Korpus enthält Entitätenreferenzen, die im ""Center for Reflected Text Analytics"" (CRETA) Als empirisches Phänomen befassen wir uns mit dem Konzept der Entität und ihrer Referenz. Das Konzept steht für verschiedene linguistische und semantische Kategorien, die im Rahmen der Digital Humanities von Interesse sind. Es ist bewusst weit gefasst und damit anschlussfähig für verschiedene Forschungsfragen aus den geistes- und sozialwissenschaftlichen Disziplinen. Auf diese Weise können unterschiedliche Perspektiven auf Entitäten berücksichtigt werden. Insgesamt werden in den ausgewählten Texten fünf verschiedene Entitätenklassen betrachtet: PER (Personen/Figuren), LOC (Orte), ORG (Organisationen), EVT (Ereignisse) und WRK (Werke). Unter Entitätenreferenzen verstehen wir Ausdrücke, die auf eine Entität in der realen oder fiktiven Welt referieren. Das sind zum einen Eigennamen (Named Entities, z.B. ""Peter""), zum anderen Gattungsnamen (z.B. ""der Bauer""), sofern diese sich auf eine konkrete Instanz der Gattung beziehen. Dabei wird als Referenzausdruck immer die maximale Nominalphrase (inkl. Artikel, Attribut) annotiert. Pronominale Entitätenreferenzen werden hingegen nicht annotiert. In In den Die Grundlage für (überwachte) maschinelle Lernverfahren bilden Annotationen. Um die Annotierung von Entitätenreferenzen automatisieren zu können, bedarf es Textdaten, die die Vielfalt des Entitätenkonzepts abdecken. Bei diesem Tutorial werden wir auf Annotationen zurückgreifen, die im Rahmen von CRETA an der Universität Stuttgart entstanden sind (vgl. Blessing et al., 2017; Reiter et al., 2017a). Das Korpus enthält literarische Texte aus zwei Sprachstufen des Deutschen (Neuhochdeutsch und Mittelhochdeutsch) sowie ein sozialwissenschaftliches Teilkorpus. Der  Das Der Ablauf des Tutorials orientiert sich an sog. Zunächst diskutieren wir kurz die zugrundeliegenden Texte und deren Annotierung. Annotationsrichtlinien werden den Teilnehmerinnen und Teilnehmern im Vorfeld zur Verfügung gestellt. Im Rahmen der Einführung wird auch auf die konkrete Organisation der Annotationsarbeit eingegangen, so dass das Tutorial als Blaupause für zukünftige Tätigkeiten der Teilnehmenden in diesem und ähnlichen Arbeitsfeldern dienen kann. Die Teilnehmerinnen und Teilnehmer versuchen selbständig und unabhängig voneinander, eine Kombination aus maschinellen Lernverfahren, Merkmalsmenge und Parametersetzungen zu finden, die auf einem neuen, vom automatischen Lernverfahren ungesehenen Datensatz zu den Ergebnissen führt, die dem Goldstandard der manuellen Annotation am Öhnlichsten sind. Das bedeutet konkret, dass der Einfluss von berücksichtigten Features (z.B. Groß- und Kleinschreibung oder Wortlänge) auf die Erkennung von Entitätenreferenzen empirisch getestet werden kann. Dabei sind Intuitionen über die Daten und das annotierte Phänomen hilfreich, da simplem Durchprobieren aller möglichen Kombinationen (""brute force'"") zeitlich Grenzen gesetzt sind. Wir verzichten bewusst auf eine graphische Benutzerschnittstelle (vgl. Reiter et al., 2017b) 'stattdessen editieren die Teilnehmerinnen und Teilnehmer das (Python)-Programm direkt, nach einer Einführung und unter Anleitung. Vorkenntnisse in Python sind dabei nicht nötig: Das von uns zur Verfügung gestellte Programm ist so aufgebaut, dass auch Python-Neulinge relativ schnell die zu bearbeitenden Teile davon verstehen und damit experimentieren können. Wer bereits Erfahrung im Python-Programmieren hat, kann fortgeschrittene Funktionalitäten des Programms verwenden. Wie am Ende jedes maschinellen Lernprozesses wird auch bei uns abschließend eine Evaluation der automatisch generierten Annotationen durchgeführt. Hierfür werden den Teilnehmerinnen und Teilnehmern nach Ablauf einer begrenzten Zeit des Experimentierens und Testens (etwa 60 Minuten) die finalen, vorher unbekannten Testdaten zur Verfügung gestellt. Auf diese Daten werden die erstellten Modelle angewendet, um automatisch Annotationen zu erzeugen. Diese wiederum werden dann mit dem Goldstandard verglichen, wobei die verschiedenen Entitätenklassen sowie Teilkorpora getrennt evaluiert werden. Auch das Programm zur Evaluation stellen wir bereit. Am hier verwendeten Beispiel der automatischen Annotation von Entitätenreferenzen demonstrieren wir, welche Schritte für die Automatisierung einer Textanalyseaufgabe mittels maschinellen Lernverfahren nötig sind und wie diese konkret implementiert werden können. Die Teilnehmenden des Workshops bekommen einen zusammenhängenden Überblick von der manuellen Annotation ausgewählter Texte über die Feinjustierung der Lernverfahren bis zur Evaluation der Ergebnisse. Die vorgestellte Vorgehensweise für den gesamten Ablauf ist grundsätzlich auf ähnliche Projekte übertragbar. Das Tutorial schärft dabei das Verständnis für den Zusammenhang zwischen untersuchtem Konzept und den dafür relevanten Features, die in ein statistisches Lernverfahren einfließen. Durch Einblick in die technische Umsetzung bekommen die Teilnehmerinnen und Teilnehmer ein Verständnis für die Grenzen und Möglichkeiten der Automatisierung, das sie dazu befähigt, zum einen das Potenzial solcher Verfahren für eigene Vorhaben realistisch(er) einschätzen zu können, zum anderen aber auch Ergebnisse, die auf Basis solcher Verfahren erzielt wurden, angemessen hinterfragen und deuten zu können.  Im Vorfeld der Veranstaltung: Installationsanweisungen und Support Der Workshop wird ausgerichtet von Mitarbeitenden des ""Center for Reflected Text Analytics"" (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine ""black box"" sein, sondern auch für Nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird.  Der Interessenschwerpunkt Gerhard Kremers ist der reflektierte Einsatz von Werkzeugen der Computerlinguistik für geistes- und sozialwissenschaftliche Fragestellungen. Damit zusammenhängend gehören die Entwicklung übertragbarer Arbeitsmethoden und die angepasste, nutzerfreundliche Bedienbarkeit automatischer linguistischer Analysetools zu seinen Forschungsthemen.  Kerstin Jungs Forschungsinteressen liegen im Bereich der Nachhaltigkeit von (computer)linguistischen Ressourcen und Abläufen sowie der Verlässlichkeitsbeschreibung von automatisch erzeugten Annotationen. Dabei verfolgt sie einen aufgabenbasierten Ansatz und arbeitet an der Schnittstelle zwischen Computerlinguistik und anderen sprach- und textverarbeitenden Disziplinen. Zwischen 15 und 25. Es wird außer einem Beamer keine besondere technische Ausstattung benötigt. Es sollte sich um einen Raum handeln, in dem es möglich ist, den Teilnehmenden über die Schulter zu blicken und durch die Reihen zu gehen.",de,Ziel Tutorial teilnehmerinn Teilnehmer konkret praktisch einblicken Standardfall automatisch Textanalyse geben automatisch Erkennung Entitätenreferenze allgemein annehmen verfahrensweisen methodisch Standard maschinell lernverfahren Teilnehmerinn Teilnehmer bearbeiter lauffähig Programmiercode entscheidungsraum Verfahren ausleuchten austesten keinerlei Vorkenntnisse Maschinellem lernen Programmierkenntnisse voraussetzen Grund Ergebnis maschinell lernverfahren besonderer Blind vertrauen konkret einblick maschinenraum maschinell lernverfahren Teilnehmend ermöglichen Potenzial Grenze statistisch Textanalysewerkzeug Realistischer einschätzen mittelfristig hoffen auftretende Frustration Einsatz automatisch Verfahren Textanalyse teilweise zufriedenstellend begegnen Nutzung Interpretation Ergebnis Maschinelle lernverfahren Linie automatisch erzeugt Annotation fördern adäquat Nutzung hermeneutisch interpretationsschritten Einblick Funktionsweise maschinell Methode Unerlässlich insbesondere Art Herkunft Trainingsdat Qualität maschinell produziert daten Bedeutung Tutorial deutlich automatisch Annotierung entitätenreferenzen tutorials arbeiten stellen heterogen manuell annotiert korpus Routin Evaluation Vergleich Annotation Verfügung korpus enthalten entitätenreferenzen center for reflected Text analytics creta empirisch Phänomen befassen Konzept Entität Referenz Konzept stehen verschieden linguistisch semantisch Kategorie Rahmen Digital Humanitie Interesse bewusst fassen anschlussfähig verschieden Forschungsfrag sozialwissenschaftlich Disziplin Weise unterschiedlich Perspektive entitäten berücksichtigen insgesamt ausgewählt Text verschieden Entitätenklasse betrachten per Person Figur loc ort org organisation Evt Ereignis wrk werk Entitätenreferenze verstehen ausdrücken Entität real fiktiv Welt referieren Eigennam named entities Peter Gattungsnamen Bauer sofern konkret Instanz Gattung beziehen Referenzausdruck maximal Nominalphrase Artikel Attribut annotiert pronominale entitätenreferenzen hingegen annotiert Grundlage überwacht maschinell lernverfahren bilden Annotation Annotierung Entitätenreferenze automatisieren bedürfen textdan Vielfalt Entitätenkonzept abdecken Tutorial annotation zurückgreifen Rahmen Creta Universität Stuttgart entstehen blessing et Reiter et korpus enthalten literarisch Text Sprachstuf deutsch neuhochdeutsch mittelhochdeutsch sozialwissenschaftlich Teilkorpus Ablauf tutorials orientieren diskutieren zugrundeliegend Text Annotierung Annotationsrichtlini Teilnehmerinn Teilnehmer Vorfeld Verfügung stellen Rahmen Einführung konkret Organisation Annotationsarbeit eingehen Tutorial Blaupause zukünftig Tätigkeit Teilnehmende ähnlich Arbeitsfelder dienen Teilnehmerinn Teilnehmer versuchen selbständig unabhängig voneinander Kombination Maschinellen lernverfahren merkmalsmenge Parametersetzunge finden automatisch lernverfahren ungesehen Datensatz ergebnissen führen Goldstandard manuell Annotation öhnlichsten bedeuten konkret einfluss berücksichtigt Features Kleinschreibung wortlängen Erkennung Entitätenreferenze empirisch testen intuitioner daten annotiert Phänomen hilfreich simpl Durchprobier möglich Kombination brute Force zeitlich Grenze setzen verzichten bewussen graphisch Benutzerschnittstelle Reiter et stattdessen editieren Teilnehmerinn Teilnehmer direkt Einführung Anleitung Vorkenntnisse Python nötig Verfügung gestellt Programm aufbauen relativ schnell bearbeitenden Teil verstehen experimentieren Erfahrung fortgeschritten funktionalitäten Programm verwenden jeder maschinell Lernprozesses abschließend Evaluation automatisch generiert Annotation durchführen hierfür Teilnehmerinn Teilnehmer Ablauf begrenzt experimentierens Testen Minute Finale vorher unbekannt Testdate Verfügung stellen daten erstellt Modell anwenden automatisch annotation erzeugen wiederum Goldstandard vergleichen wobei verschieden Entitätenklasse Teilkorpora trennen evaluieren Programm Evaluation stellen bereit verwendet automatisch Annotation Entitätenreferenze demonstrieren Schritt Automatisierung Textanalyseaufgabe mittels maschinell lernverfahren nötig konkret implementieren Teilnehmende Workshop bekommen zusammenhängend Überblick manuell Annotation ausgewählter Text Feinjustierung lernverfahren Evaluation Ergebnis vorgestellt vorgehensweise gesamt Ablauf grundsätzlich ähnlich Projekt übertragbar Tutorial schärfen Verständnis Zusammenhang untersucht Konzept relevant Feature statistisch lernverfahren einfließen Einblick technisch Umsetzung bekommen Teilnehmerinn Teilnehmer Verständnis Grenze Möglichkeit Automatisierung befähigen Potenzial Verfahren Vorhaben realistisch einschätzen Ergebnis Basis Verfahren erzielen angemessen Hinterfrag deuten Vorfeld Veranstaltung Installationsanweisunge Support Workshop ausrichten Mitarbeitende center for reflected Text analytics Creta Universität Stuttgart Creta verbinden Literaturwissenschaft Linguistik Philosophie Sozialwissenschaft Maschineller Sprachverarbeitung Visualisierung Hauptaufgabe Creta Entwicklung reflektiert Methode Textanalyse wobei methode Gesamtpaket konzeptuell Rahmen annehmen technisch Implementierung Interpretationsanleitung verstehen Methode black box transparent reflektiert Einsatz Hinblick sozialwissenschaftlich Fragestellung Interessenschwerpunkt Gerhard Kremer reflektiert Einsatz Werkzeug Computerlinguistik sozialwissenschaftlich Fragestellung zusammenhängend gehören Entwicklung übertragbar arbeitsmethoden angepas nutzerfreundlich Bedienbarkeit automatisch linguistisch Analysetools Forschungsthem kerstin Jung Forschungsinteresse liegen Bereich Nachhaltigkeit Computer linguistisch Ressource abläufen Verlässlichkeitsbeschreibung automatisch erzeugt Annotation verfolgen aufgabenbasiert Ansatz arbeiten Schnittstelle Computerlinguistik Textverarbeitende disziplinen Beamer besonderer technisch Ausstattung benötigen Raum handeln Teilnehmend Schulter blicken Reihe,"[('lernverfahren', 0.2886863741839128), ('teilnehmerinn', 0.26084623434904625), ('teilnehmer', 0.2261943477184239), ('entitätenreferenze', 0.19248222088510875), ('tutorial', 0.1809796878065532), ('automatisch', 0.174241023729503), ('maschinell', 0.17362313777620006), ('creta', 0.17205763000022856), ('annotation', 0.15163913181670222), ('sozialwissenschaftlich', 0.1410162488443137)]"
2019,DHd2019,191_final-GIUS_Evelyn_Korpuserstellung_als_literaturwissenschaftliche_.xml,Korpuserstellung als literaturwissenschaftliche Aufgabe,"Evelyn Gius (Universität Hamburg, Deutschland); Katharina Krüger (Universität Hamburg, Deutschland); Carla Sökefeld (Universität Hamburg, Deutschland)","Korpora, Korpuserstellung, Dubletten","Sammlung, Bereinigung, Literatur, Methoden, Forschungsprozess, Text","Die Praxis der Zusammenstellung von Primärtexten zu einem Korpus ist gewissermaßen literaturwissenschaftliches Alltagsgeschäft, trotzdem wird sie selten problematisiert. Die beiden Standardfälle der nicht-digitalen Korpusanalyse erscheinen auch bezüglich der Korpuszusammenstellung unproblematisch: (1) Die Forschungsfrage erfordert eine bestimmte Textbasis (etwa bei einer Untersuchung zu Krankheitsdarstellungen bei Thomas Mann), (2) Die Korpuserstellung basiert auf der Kanonizität der Texte. Zumindest im zweiten Fall ist das geeignete Vorgehen allerdings nicht selbstverständlich, denn es müsste begründet werden, nach welchen Kriterien Werke tatsächlich exemplarisch und repräsentativ sind. Dies wird jedoch kaum thematisiert (vgl. Gius, in Vorbereitung). Bei der konkreten Erstellung eines Ein Grund für die geringe Auseinandersetzung mit dem Thema kann sein, dass die Praxeologie der Zusammenstellung literarischer digitaler Korpora verhältnismäßig neu ist und sich bislang keine literaturwissenschaftlichen Routinen etablieren konnten, die das Zusammenstellen des Untersuchungsobjektes betreffen. Die digitale literaturwissenschaftliche Korpuserstellung stellt also ein ungelöstes methodologisches Grundproblem dar. Wir erläutern im Folgenden die Problematik der literaturwissenschaftlichen Korpuserstellung exemplarisch, um eine Diskussion darüber anzustoßen sowie mögliche Lösungsansätze vorzustellen. Im Rahmen unseres Forschungsprojektes zu genderspezifischer Darstellung von Krankheit in literarischen Texten im Forschungsverbund hermA (""Automatische Modellierung hermeneutischer Prozesse"") Da die Entwicklung von Strategien zur Bildung themenspezifischer Subkorpora ebenfalls Ziel des Forschungsvorhabens ist, gab es für die Textauswahl zunächst keine inhaltlichen Einschränkungen. Kriterien zur Textauswahl waren deshalb nur das Datum der Erstveröffentlichung, Textsprache, das Genre und Textlänge; Grundlage für ihre Ermittlung bildeten die Kolimo-Metadaten. Metadaten  Kinder- und Jugendbuch (im Kernkorpus)  Ohne spezifisches Genre Metadaten Kategorisierung der Angaben in den Metadaten Recherche  Ins Korpus aufgenommen wurden Texte mit Erstveröffentlichung zwischen 1870 und 1920. Die Auswahl wurde anhand der vorliegenden Metadaten getroffen und bei Bedarf weiter ergänzt. Je nach Repositorium unterschied sich die zusätzlich nötige Recherchearbeit stark: elf DTA-Texte, 7.602 Gutenberg-Texte, und 27.300 TextGrid-Texte hatten keine Datumsangabe. Da wir uns auf Phänomene konzentrieren, die in der deutschsprachigen zeitgenössischen Literatur verhandelt wurden, lag der Fokus auf standarddeutschen Texten. Übersetzungen wurden in ein Sonderkorpus aufgenommen, zu weniger als 90% deutschsprachige Texte aussortiert. Das nach diesen Kriterien manuell erstellte Kernkorpus umfasst nur noch etwa 2.700 Werke.  Die dargestellten Schwierigkeiten bei der Entscheidung über die Aufnahme eines Textes in das Kernkorpus basierten auf unvollständigen, widersprüchlichen oder nicht ohne weiteres aufeinander abbildbaren Metadaten und konnten anhand der dargelegten Setzungen vergleichsweise einfach gelöst werden. Weitaus schwieriger gestaltete sich hingegen die Identifikation von Dubletten und die Konzipierung einer geeigneten Problembehandlung –¬†ein leider typisches Problem bei der Aggregation eines Korpus aus verschiedenen Quellen. So zeigte sich bei einer ersten manuellen Durchsicht der Liste und der stichprobenartigen Überprüfung der Volltexte, dass Dubletten nicht eindeutig anhand von Metadaten identifizierbar sind. Die naheliegende Lösung, ein Volltextvergleich, müsste jedoch bei 2.700 Texten über sieben Millionen mögliche Paare vergleichen und würde auch bei der Nutzung von Cluster Computing Jahrzehnte dauern. Deshalb mussten Heuristiken entwickelt werden, um das Verfahren abzukürzen. Entsprechend haben wir einen Workflow entworfen, um mit möglichst hoher Treffgenauigkeit Dubletten zu identifizieren. Ziel war, alle echten Dubletten zu finden und das Korpus entsprechend zu bereinigen, ohne Texte vorschnell zu streichen. Dafür wurden folgende, größtenteils automatische Prüfungen durchgeführt: Bei den verbleibenden Textpaaren gehen wir von echten Dubletten aus. Für diese erfolgt eine Repositorien-Priorisierung nach der Qualität der Repositorien (1. DTA, 2. TextGrid, 3. Gutenberg).  Die Digitalisierung fördert die literaturwissenschaftliche Korpuserstellung in neuem Umfang und macht dadurch die Textzusammenstellung als literaturwissenschaftliches Problem offensichtlich, das methodologisch kaum beleuchtet ist. Oft dominiert die Frage, welche Texte überhaupt ins Korpus können, also aus welchen bereits digitalisierten oder noch digitalisierbaren Texten ausgewählt werden kann, die literaturwissenschaftlichen Überlegungen zur Textauswahl. Da die Menge digitalisiert vorliegender Texte stetig steigt, haben Korpora außerdem immer häufiger eine Größe, die von einzelnen Forscher/innen nicht mehr überschaubar ist. Deshalb muss sich die literaturwissenschaftliche Begründung der Relevanz der Texte in einem Korpus einer gewissen Größe im Zweifelsfall auf Aspekte, die als Informationen in den Metadaten der Texte vorliegen 'wie Zeit, Gattung/Genre oder Autor/innen 'beschränken. Sowohl die Menge der zur Verfügung stehenden Texte als auch die Qualität der Primär- und Metadaten sind noch steigerungsfähig. Hier ist die Forschungsgemeinschaft genauso gefordert wie Bibliotheken und Archive. Im Sinne einer wissenschaftlichen Qualitätssicherung ist es daher umso wichtiger, im Hinblick auf das Da das literaturwissenschaftliche Wissen über ein Korpus mit steigender Korpusgröße notwendigerweise ab- und damit die Gefahr unerwünschter Korrelationen zunimmt, sollte das Korpus außerdem mit Informationen angereichert werden, die für die Interpretation von Analyseergebnissen genutzt werden können (Epochenzugehörigkeit, thematische Zuordnung etc). Dadurch können entdeckte Korrelationen auf einer größeren Basis an Texteigenschaften 'also: besser 'interpretiert werden. Da das manuelle Ergänzen von Informationen sehr arbeitsaufwändig ist, sollten dafür (halb-)automatische Verfahren entwickelt bzw. genutzt werden. Die Anforderungen an ein Korpus variieren je nach Kontext und Forschungsfrage des Projekts, deshalb müssen allgemeine Qualitätskriterien für die Korpuserstellung eine gewisse Offenheit aufweisen. Grundsätzlich gilt aber: Für die Korpuserstellung muss frühzeitig eine Strategie zur Priorisierung von Problemen entwickelt 'und dokumentiert! 'werden. Dabei geht es darum, sich wie in diesem Beitrag skizziert eine Übersicht über konzeptuelle und technische Aspekte zu verschaffen und anschließend einfach zu lösende Probleme und konzeptuell wichtige Entscheidungen in eine geeignete Reihenfolge zu bringen 'den Workflow für die Korpuserstellung.",de,Praxis Zusammenstellung Primärtext Korpus gewissermaßen literaturwissenschaftlich alltagsgeschäft selten problematisieren Standardfäll Korpusanalyse erscheinen bezüglich Korpuszusammenstellung unproblematisch Forschungsfrage erfordern bestimmt Textbasis Untersuchung Krankheitsdarstellung Thomas Mann Korpuserstellung basieren Kanonizität Text zumindest Fall geeignet vorgehen selbstverständlich müsste begründen kriterien Werk tatsächlich exemplarisch repräsentativ thematisieren Gius Vorbereitung konkret Erstellung Grund gering Auseinandersetzung Thema Praxeologie Zusammenstellung literarisch Digitaler korpora verhältnismäßig neu bislang literaturwissenschaftlich Routin etablieren zusammensteller untersuchungsobjektes betreffen digital literaturwissenschaftlich Korpuserstellung stellen ungelöst methodologisch grundproblem dar erläutern folgend Problematik literaturwissenschaftlich Korpuserstellung exemplarisch Diskussion anstoßen möglich Lösungsansätze vorstellen Rahmen unser forschungsprojektes genderspezifisch Darstellung Krankheit literarisch Text Forschungsverbund Herma automatisch Modellierung hermeneutisch prozesse Entwicklung Strategie Bildung themenspezifisch Subkorpora ebenfalls Ziel Forschungsvorhaben Textauswahl inhaltlich Einschränkung kriterien Textauswahl Datum Erstveröffentlichung Textsprache Genre Textlänge Grundlage Ermittlung bilden Metadat Jugendbuch Kernkorpus spezifisch Genre metadat Kategorisierung Angabe metadaten Recherche Korpus aufnehmen Text Erstveröffentlichung Auswahl anhand vorliegend metadaten treffen Bedarf ergänzen Repositorium unterscheiden zusätzlich nötig Recherchearbeit stark Datumsangabe phänomen konzentrieren deutschsprachig zeitgenössisch Literatur verhandeln liegen Fokus standarddeutsch Text übersetzungen Sonderkorpus aufnehmen deutschsprachig Text aussortieren kriterien manuell erstellt Kernkorpus umfassen Werk dargestellt Schwierigkeit Entscheidung Aufnahme Text Kernkorpus basieren unvollständig widersprüchlich aufeinander abbildbaren metadaten anhand dargelegt setzungen vergleichsweise einfach lösen weitaus schwierig gestalten hingegen Identifikation dubletten Konzipierung geeignet Problembehandlung typisch Problem Aggregation korpus verschieden quellen zeigen manuell Durchsicht Liste stichprobenartig Überprüfung Volltexte dubletter eindeutig anhand metadaten identifizierbar naheliegend Lösung Volltextvergleich müsster Text Million möglich paar Vergleich Nutzung Cluster Computing Jahrzehnt dauern Heuristike entwickeln Verfahren abzukürzen entsprechend Workflow entwerfen möglichst hoch Treffgenauigkeit dubletten identifizieren Ziel echt dubletten finden Korpus entsprechend bereinigen Text vorschnell streichen folgend größtenteils automatisch Prüfung durchführen verbleibend Textpaare echt dubletten erfolgen Qualität Repositorie Dta Textgrid Gutenberg Digitalisierung fördern literaturwissenschaftlich Korpuserstellung neu Umfang Textzusammenstellung literaturwissenschaftlich Problem offensichtlich Methodologisch beleuchten dominieren Frage Text Korpus Digitalisiert digitalisierbaren Text auswählen literaturwissenschaftlich Überlegung Textauswahl Menge digitalisieren vorliegend Text stetig steigen Korpora häufig Größe einzeln Forscher innen überschaubar literaturwissenschaftlich Begründung Relevanz Text Korpus gewiß Größe Zweifelsfall Aspekt Information metadaten Text vorliegen Gattung Genre Autor innen beschränken sowohl Menge Verfügung stehend Text Qualität metadaten steigerungsfähig Forschungsgemeinschaft genauso fordern Bibliothek Archiv Sinn wissenschaftlich Qualitätssicherung umso wichtig Hinblick literaturwissenschaftlich wissen korpus Steigender Korpusgröße notwendigerweise Gefahr unerwünscht Korrelation zunehmen korpus Information anreichern Interpretation analyseergebnissen nutzen Epochenzugehörigkeit thematisch Zuordnung etc entdeckt Korrelation groß Basis Texteigenschaft interpretieren Manuelle ergänzen Information arbeitsaufwändig Verfahren entwickeln nutzen Anforderung Korpus variieren Kontext Forschungsfrage Projekt allgemein qualitätskriterien Korpuserstellung gewiß Offenheit aufweisen grundsätzlich gelten Korpuserstellung frühzeitig Strategie Priorisierung Problem entwickeln dokumentieren Beitrag skizzieren Übersicht konzeptuell technisch Aspekt verschaffen anschließend einfach lösend Problem konzeptuell wichtig Entscheidung geeignet Reihenfolge bringen Workflow Korpuserstellung,"[('korpuserstellung', 0.32995225700158587), ('dubletten', 0.23923225922314892), ('literaturwissenschaftlich', 0.2095440932062345), ('metadaten', 0.17553401907436514), ('korpus', 0.17100590327089712), ('text', 0.1672248222109132), ('kernkorpus', 0.15161862659786454), ('textauswahl', 0.14608593458643113), ('erstveröffentlichung', 0.11141335643070757), ('kriterien', 0.09933991002427633)]"
2019,DHd2019,151_final-WILLAND_Marcus_Ein_neues_Format_für_die_Digital_Humanities__.xml,Ein neues Format für die Digital Humanities: Shared Tasks. Zur Annotation narrativer Ebenen,"Marcus Willand (Universität Heidelberg, Universität Stuttgart); Evelyn Gius (Universität Hamburg); Nils Reiter (Universität Stuttgart)","Annotation, Shared Task, Evaluation","Annotieren, Theoretisierung, Bewertung, Projektmanagement, Methoden, Text","Dieses Paper führt unsere letztjährige Präsentation Bei einem ST bewerben sich Teams mit einem Vorschlag für die Lösung eines durch die Organisatoren ausgeschriebenen Problems, den Task. STs sind kompetitive Verfahren, weil die Lösungsvorschläge vergleichend evaluiert und gemäß einer definierten Metrik in eine Rangfolge gebracht werden. Vor allem in der Sprachverarbeitung (NLP, natural language processing) sind diese Arbeitszusammenhänge weit verbreitet und ein wesentlicher Antrieb für die Fortschritte bei wichtigen Aufgaben, etwa des syntaktischen Parsings. Wir haben uns für ein zweiphasiges Verfahren entschieden. Die erste Phase '""SANTA"" genannt: Systematic Analysis of Narrative Texts through Annotation 'widmet sich der Erstellung von Annotationsrichtlinien für das Phänomen narrativer Ebenen. Die acht Teams divergieren hinsichtlich ihrer: Der Workshop selbst war konzeptionell offen angelegt und sollte den Teilnehmer/innen wie auch uns Organisator/innen ermöglichen, den geplanten Ablauf in Reaktion auf die Arbeitsergebnisse zu verändern. Dies war realisierbar, da bis auf wenige Kurzvorträge (z.B. stellte jedes Team zu Beginn in 5 Minuten die zentralen Aspekte seiner Richtlinien vor) hauptsächlich in kooperativen Formaten wie Gruppenarbeiten, Feedback-Runden und Plenumsdiskussionen gearbeitet wurde. Am Der Diese Kriterien wurden zuerst während des Workshops im Plenum reflektiert und anschließend per online-Fragebogen live in die Evaluation überführt. Jede Dimension sollte auf nachvollziehbare Weise potentielle Guideline-Stärken vergleichbar machen und so für eine ausgewogene Beurteilung durch die Workshopteilnehmer/innen sorgen. Die drei Dimensionen sind 'um in der Tagungssprache zu bleiben 'Die erste Dimension beurteilt anhand von vier Fragen die Qualität der Guidelines hinsichtlich ihrer Die zweite Dimension evaluiert die Die dritte Dimension bewertet anhand von vier Fragen, wie der auf Basis einer Richtlinie annotierte Text das T Jede der Fragen wurde von den Teams in einer Feedback-Runde erläutert und anhand einer vierstufigen Likert-Skala online beurteilt. Die dergestalt relativ differenziert abgefragten drei Evaluationsdimensionen lassen sich 'stark abstrahiert 'auch verstehen als prozedurale Vergewisserungskriterien für gute Guidelines zur Annotation literaturwissenschaftlicher (oder allgemein: geisteswissenschaftlicher) Konzepte auf Texten unterschiedlicher Genres. Sie können prozessorientiert abgebildet werden: Am Die drei Dimensionen erwiesen sich damit als praktikables Instrument einer differenzierten Bewertung der Guidelines. Das Experiment ""Shared Task für die DH"" ist also geglückt. Die drei Bewertungsdimensionen stehen allerdings auch weiterhin für eine der großen methodischen Herausforderungen im",de,Paper führen letztjährig Präsentation st bewerben Team Vorschlag Lösung Organisator ausgeschrieben Problem Task sts kompetitiv Verfahren lösungsvorschläge vergleichend evaluieren gemäß definiert Metrik Rangfolge bringen Sprachverarbeitung nlp natural language Processing Arbeitszusammenhänge verbreiten wesentlich Antrieb Fortschritt wichtig Aufgabe syntaktisch Parsing zweiphasig Verfahren entscheiden Phase Santa nennen Systematic Analysis of Narrative Texts through Annotation widmen Erstellung Annotationsrichtlinien phänom narrativ Ebene Team Divergier hinsichtlich Workshop konzeptionell anlegen Teilnehmer innen Organisator innen ermöglichen geplant Ablauf Reaktion arbeitsergebnisse verändern realisierbar kurzvorträge stellen jeder Team Beginn Minute zentral Aspekt Richtlinie hauptsächlich kooperativ Format gruppenarbeit Plenumsdiskussion arbeiten kriterien Workshop Plenum reflektieren anschließend per live Evaluation überführen Dimension nachvollziehbar Weise potentiell vergleichbar ausgewogen Beurteilung Workshopteilnehmer innen Sorge Dimension tagungssprach bleiben Dimension beurteilen anhand Frage Qualität guidelin Hinsichtlich Dimension evaluieren Dimension bewerten anhand Frage Basis Richtlinie annotiert Text t Frage Team erläutern anhand vierstufig Online beurteilen Dergestalt relativ differenziert abgefragten evaluationsdimensionen lassen stark abstrahieren verstehen prozedural Vergewisserungskriterien guidelines Annotation literaturwissenschaftlich allgemein geisteswissenschaftlicher Konzept Text unterschiedlich Genre prozessorientiert abbilden dimension erweisen praktikabl Instrument differenziert Bewertung guidelines Experiment shared Task dh glücken Bewertungsdimensione stehen weiterhin methodisch Herausforderung,"[('team', 0.29731712991952597), ('dimension', 0.28989465349689353), ('organisator', 0.19418841818624022), ('guidelines', 0.15304411602152324), ('beurteilen', 0.14865856495976298), ('innen', 0.1475407818537703), ('richtlinie', 0.12390255318407503), ('task', 0.11257700994611068), ('differenziert', 0.10524834049126015), ('workshop', 0.09926038154569702)]"
2019,DHd2019,204_final-TRAUTMANN_Marjam_DER_STURM__Digitale_Quellenedition_zur_Gesc.xml,DER STURM. Digitale Quellenedition zur Geschichte der internationalen Avantgarde. Drei Forschungsansätze.,"Anne Katrin Lorenz (Akademie der Wissenschaften und der Literatur, Mainz); Lea Müller-Dannhausen (Johannes Gutenberg-Universität Mainz); Marjam Trautmann (Akademie der Wissenschaften und der Literatur | Mainz)","Briefe, Digitale Edition, Modellierung, Netzwerkanalyse, Simultaneitätsstudie","Transkription, Annotieren, Netzwerkanalyse, Daten, benannte Entitäten (named entities), Forschung","Digitale Editionen gehören zu den ""prominentesten Themen"" (Sahle 2017: 237) in den Digital Humanities. Sie leisten Grundlagenarbeit für die geisteswissenschaftliche Forschung und bilden ein eigenes Forschungsfeld in den Digital Humanities. Hier verortet sich auch das digitale Editions- und Forschungsprojekt ""DER STURM. Digitale Quellenedition zur Geschichte der internationalen Avantgarde"" (https://sturm-edition.de). Gegenstand ist das 1910 gegründete Berliner Kunstunternehmen ""Der Sturm"" um den Publizisten, Komponisten und Kritiker Herwarth Walden, der zahlreichen Künstlerinnen und Künstlern unterschiedlicher Kunstgattungen eine Plattform bot. Das Unternehmen umfasste die Zeitschrift ""Der Sturm"", die ""Sturm""-Galerie, die ""Sturm""-Bühne sowie den ""Sturm""-Verlag. Neben den Veröffentlichungen des ""Sturm"" selbst bezeugen die überlieferten Briefe an das Ehepaar Walden den internationalen Einfluss des Unternehmens. Unser Editionsprojekt, in dem bereits digital verfügbares Material aus dem ""Sturm""-Kontext transkribiert, standardkonform nach XML/TEI P5 aufbereitet und mit Normdaten versehen wird, führt diese Quellen erstmals zentral zusammen und setzt sie mittels digitaler Methoden in Relation zueinander. Integraler Teil des Editionsprojektes ist die Forschung mit den STURM-Quellen. Auf dem Poster stellen wir neben der Quellenedition die folgenden drei Forschungsansätze vor, die sich explizit mit den im Projekt edierten Materialien beschäftigen. Ein Ansatz arbeitet mit den in der digitalen Quellenedition DER STURM zusammengeführten Quellen und modelliert deren Verknüpfungen und Beziehungen untereinander. Die Grundlage bildet hierbei das CIDOC Conceptual Reference Model als Domain-Ontologie im Bereich Cultural Heritage (Doerr 2009), die im Bereich der musealen Sammlungen und der bildenden Kunst weit verbreitet ist. Ergänzt wird diese Ontologie durch fachspezifische Vokabulare wie den Getty Art & Architecture Thesaurus (AAT). Die semantischen Verknüpfungen zwischen den einzelnen Quellen und Quellen-Typen beziehen sich auf die in den bereits edierten Quellen annotierten Entitäten (Personen, Orte und Werke) sowie auf weitere Entitäten, die noch in den Editionsprozess aufgenommen werden: auf Körperschaften, Ereignisse und Themen. In der digitalen Quellenedition des STURM sind für diese Entitäten bereits URIs vorhanden bzw. vorgesehen. Durch Anreicherung der Entitäten mit Normdaten werden Verknüpfungen mit weiteren Ressourcen ermöglicht. Die semantische Modellierung der STURM-Domäne 'bestehend aus den Quellen der Edition und dem, was sie bezeugen 'dient der Weiterentwicklung der digitalen Quellenedition DER STURM sowie perspektivisch der Weiternutzung der dabei gewonnenen Daten, denn sie bildet die Grundlage für eine Verfügbarmachung der Daten und ihrer Zusammenhänge in Form von Linked Open Data. In dieser Form können die Daten beim Ermitteln und Zeigen von Zusammenhängen helfen und somit die Basis bilden für weitere Forschungen in einzelnen Fachwissenschaften, insbesondere in der Kunst- und in der Literaturwissenschaft. Ein weiterer Ansatz im Projekt beschäftigt sich mit der historischen Netzwerkforschung (Düring et al. 2016). In der bisherigen ""Sturm""-Forschung steht aufgrund der Komplexität und Dezentralität der Quellen eine dezidiert ""kunsthistorische Beschäftigung mit dem Das Netzwerk ist multimodal, bestehen die einzelnen zueinander in Relation stehenden Entitäten doch aus im Kontext des ""Sturm"" aufkommenden Personen, multimedialen Werken, Briefdaten und einigem mehr. Diese Daten gilt es in der Visualisierung des erhobenen Netzwerkes anschaulich zu machen (Baillot 2018: 357). Darüber hinaus offenbart die computergestützte Historische Netzwerkforschung auch in der Analyse komplexer Netzwerke ihre Stärken. Durch eine rein klassische Untersuchung des facettenreichen ""Sturm""-Netzwerkes, so die These, würden Darstellung und Analyse unübersichtlicher und damit fehleranfälliger werden. Die kritische Untersuchung der Quellen selbst 'und damit einhergehend die Interpretation des erhobenen Gesamtnetzwerkes ""Sturm"" 'bildet einen weiteren wichtigen Schritt in der Gesamtanalyse des Kunstunternehmens hinsichtlich eines möglichen ""gesamtgesellschaftlichen Spiegels"". Ziele der historischen Netzwerkstudie sind das Ausmachen und die Analyse von Ein Beispiel für die fachspezifische Nutzung digitaler Editionen gibt die Studie zu avantgardistischen Simultaneitätskonzepten im ""Sturm"". Als ein die Avantgarde bestimmendes Strukturprinzip kommt Simultaneität in verschiedenen Kunstrichtungen vor, die innerhalb ihrer Programmatik mitunter konkurrierende Modelle entwickeln. F. M. Marinettis ""Technisches Manifest der futuristischen Literatur"", die kubistischen ""Fenster""-Bilder Robert Delaunays oder die Simultangedichte der Dadaisten ähneln sich im grundlegenden Bestreben, nach dem avantgardistischen Primat von ""Transgression und Diffundierung"" (Asholt / Fähnders 2000: 17) beim Rezipienten gleichzeitig unterschiedliche Wirklichkeitsansichten und -ebenen zu erzeugen. Im medialen Komplex des ""Sturm"", mit seinen vielfältigen Publikations- und Distributionswegen sowie seinen dezidiert antimimetischen Gestaltungsprinzipien, finden diese bildkünstlerischen wie literarischen Werke trotz Gattungs- und Stilunterschieden gleichermaßen eine adäquate Präsentationsform. Eingebettet in den spezifischen historischen Kontext des ""Sturm""-Netzwerks werden sie von Para- und Metatexten begleitet, die ihre Rezeption lenken und kommentieren 'und die nicht zuletzt in den privaten Korrespondenzen an Walden vorbereitet und verhandelt werden. Die webbasierte Zusammenführung der verschiedenen Quellen des ""Sturm"" erlaubt es nun, wechselseitige Bezüge zwischen den unterschiedlichen Textsorten offenzulegen und so diskursiv etablierte sprachlich-rhetorische Muster zu rekonstruieren. Mit Hilfe korpuslinguistisch informierter diskursanalytischer Verfahren wird der Frage nachgegangen, inwiefern solche multimodalen Diskurspraktiken die Bedeutung von Gleichzeitigkeit im historischen ""Sturm""-Netzwerk unterschiedlich konstituieren und auf diese Zeitkonstruktion als typisch avantgardistischen Diskurs verweisen. Um die entsprechenden ""Diskursfragmente"" (Jäger 2012: 98ff.) im Netzwerkkontext situieren und den einzelnen Künstlern und Kunstrichtungen zuordnen zu können, sollen zusätzlich netzwerkanalytische Zugänge berücksichtigt werden. Schließlich zielt die Auswertung der im Projekt erarbeiteten Daten darauf ab, den Begriff der Simultaneität im Problemfeld von Scheitern (Habermas) und Überleben (Luhmann) der Avantgarde zu konturieren.",de,digital editionen gehören prominentest Thema sahlen Digital Humanitie leisten Grundlagenarbeit geisteswissenschaftlich Forschung bilden Forschungsfeld Digital Humanitie verorten digital forschungsprojekt Sturm digital Quellenedition Geschichte international Avantgarde Gegenstand gegründet Berliner Kunstunternehme Sturm Publizist Komponist Kritiker Herwarth walden zahlreich künstlerinn Künstler unterschiedlich Kunstgattung Plattform bieten Unternehmen umfas Zeitschrift Sturm Veröffentlichung Sturm bezeugen überlieferten Brief Ehepaar walden international einfluss Unternehmen Editionsprojekt digital verfügbar Material transkribieren Standardkonform xml tei aufbereiten normdat versehen führen quellen erstmals zentral setzen mittels Digitaler Methode Relation zueinander integral editionsprojektes Forschung Poster stellen Quellenedition folgend Forschungsansätz explizit Projekt ediert Materialien beschäftigen Ansatz arbeiten digital Quellenedition Sturm zusammengeführt quellen modellieren Verknüpfung Beziehung untereinander Grundlage bilden hierbei Cidoc Conceptual Reference Model Bereich Cultural Heritage doerr Bereich museal Sammlung bildend Kunst verbreiten ergänzen Ontologie fachspezifisch Vokabular Getty Art Architecture Thesaurus aat semantisch Verknüpfung einzeln quellen beziehen ediert quellen annotierten Entität Person Ort werk entitäen Editionsprozess aufnehmen Körperschaft Ereignis Thema digital Quellenedition Sturm entität Uris vorhanden vorsehen Anreicherung Entität normdaten Verknüpfung Ressource ermöglichen semantisch Modellierung bestehend quellen Edition bezeugen dienen Weiterentwicklung digital Quellenedition Sturm perspektivisch Weiternutzung gewonnen daten bilden Grundlage Verfügbarmachung daten zusammenhänge Form Linked open data Form daten ermitteln zeigen Zusammenhängen helfen somit Basis bilden Forschung einzeln fachwissenschafen insbesondere Literaturwissenschaft weit Ansatz Projekt beschäftigen historisch Netzwerkforschung düring et bisherig stehen aufgrund Komplexität Dezentralität quellen dezidiert kunsthistorisch Beschäftigung Netzwerk Multimodal bestehen einzeln Zueinander Relation stehend Entität Kontext Sturm aufkommend Person multimedialen Werk Briefdat einiger daten gelten Visualisierung erhoben netzwerkes anschaulich Baillot hinaus offenbaren computergestützt historisch Netzwerkforschung Analyse Komplexer netzwerke Stärke rein klassisch Untersuchung facettenreich These Darstellung Analyse unübersichtlich fehleranfällig kritisch Untersuchung quellen einhergehend Interpretation erhoben Gesamtnetzwerke Sturm bilden wichtig Schritt Gesamtanalyse kunstunternehmens hinsichtlich möglich gesamtgesellschaftlich Spiegel Ziel historisch Netzwerkstudie ausmachen Analyse fachspezifisch Nutzung digitaler editionen Studie avantgardistischen simultaneitätskonzepen Sturm Avantgarde bestimmend Strukturprinzip Simultaneität verschieden Kunstrichtung innerhalb Programmatik mitunter konkurrierend Modell entwickeln Marinettis technisch Manifest futuristisch Literatur kubistisch Robert Delaunays Simultangedicht Dadaist ähneln grundlegend bestreben avantgardistisch Primat Transgression diffundierung asholt fähnders rezipient gleichzeitig unterschiedlich Wirklichkeitsansicht erzeugen medial Komplex Sturm vielfältig Distributionsweg dezidiert antimimetisch gestaltungsprinzipien finden Bildkünstlerisch literarisch Werk trotz Stilunterschiede gleichermaßen adäquat Präsentationsform einbetten spezifisch historisch Kontext Metatext begleiten Rezeption lenken kommentieren zuletzt privat Korrespondenze walden vorbereiten verhandeln webbasierter Zusammenführung verschieden quellen Sturm erlauben wechselseitig bezüge unterschiedlich textsorten Offenzuleg diskursiv etabliert Muster rekonstruieren Hilfe korpuslinguistisch informiert diskursanalytisch Verfahren Frage nachgehen inwiefern multimodal Diskurspraktik Bedeutung Gleichzeitigkeit historisch unterschiedlich konstituieren Zeitkonstruktion typisch avantgardistisch Diskurs verweisen entsprechend diskursfragmente Jäger netzwerkkontext situieren einzeln Künstler kunstrichtung Zuordne zusätzlich netzwerkanalytisch zugänge berücksichtigen schließlich zielen Auswertung Projekt erarbeitet daten Begriff Simultaneität Problemfeld scheitern Habermas überleben Luhmann Avantgarde konturieren,"[('sturm', 0.5410367388523061), ('quellenedition', 0.24202933245048486), ('quellen', 0.22728640413583848), ('avantgarde', 0.1452175994702909), ('walden', 0.1452175994702909), ('entität', 0.11364320206791924), ('digital', 0.10962382243295724), ('bilden', 0.09828730541620864), ('avantgardistisch', 0.09681173298019394), ('simultaneität', 0.09681173298019394)]"
2019,DHd2019,271_final-STAECKER_Thomas__Eine_digitale_Edition_kann_man_nicht_sehen_.xml,"""Eine digitale Edition kann man nicht sehen"" - Gedanken zu Struktur und Persistenz digitaler Editionen","Thomas Staecker (ULB Darmstadt, Deutschland)","Digitale Edition, Textwissenschaft, Markup, Persistenz","Umwandlung, Strukturanalyse, Modellierung, Annotieren, Schreiben, Text","Digitale Editionen haben nach einer Phase des Ausprobierens und Entwickelns nunmehr eine Reife erreicht, dass sie in vielen Disziplinen nicht mehr als exotischer Sonder-, sondern als Regelfall angesehen werden, was sich in Publikationen wie ¬†(Apollon et. al 2014; Driscoll/Pierazzo 2016) oder Förderbedingungen (DFG 2015) spiegelt. Trotz dieser greifbaren Fortschritte stehen digitale Editionen nach wie vor in der Kritik. Genannt wird immer wieder die fehlende Stabilität und ungelöste Frage der Langzeitarchivierung und - verfügbarkeit. Doch dieses Gefühl des Mangels, so die These des Beitrags, resultiert nicht aus noch nicht geklärten methodischen oder technischen Fragen, sondern beruht auf einer Fehleinschätzung der Natur digitaler Editionen, die in Analogie zum Druckmedium meist nur von ihrer Oberfläche her beurteilt werden. Mit einem Perspektivwechsel, der die Eigentümlichkeiten digitaler Editionen und die zugrundeliegende strukturellen und algorithmischen Komponenten ernst nimmt, ist indes vergleichbare Stabilität möglich, zumindest wenn man sich über das Dokumentenmodell und über die Form seiner technischen Realisierung verständigt. Die Entwicklung und Nutzung der Markupsprache XML war von Anbeginn an begleitet von Kritik über die Unzulänglichkeit des hierarchischen OHCO Modell (DeRose et al. 1990) für die Repräsentation von Text. Trotz verschiedener Vorschläge konnte bis heute keine abschließende, alle spezifischen Kodierungsprobleme klärende Lösung gefunden werden. Nun hat das der Popularität von XML im Allgemeinen und der in diesem Feld maßgeblichen TEI im Besonderen nicht geschadet. Nach wie vor erfreut sich XML/TEI großer Beliebtheit, auch wenn in jüngerer Zeit der Unterschied von TEI und XML betont wird (Cummings 2017). Das ist umso erstaunlicher, als es an alternativen Ansätzen nicht gemangelt hat (DeRoses 2004; Speerberg-McQueen 2007). Von MECS, GODDAG, TexMECS über LMNL bis zuletzt Text as a Graph (TAGML) entstanden Markup-Konzepte, die für sich in Anspruch nehmen und nahmen, XML und seine Beschränkungen zu überwinden. Gerade mit dem neuesten Konzept des Was aber von Anfang an bei der Diskussion um Overlap und die Unzulänglichkeiten von XML vernachlässigt wurde, ist, dass die Limitationen des Textmodells nicht unbedingt mit Limitationen der Serialisierung und der digitalen Technik in eins gesetzt werden können. So ist es zwar richtig, dass SGML und in der Folge XML mit dem OHCO Modell im Kopf entwickelt wurden, doch haben bereits (Renear et al.1993) in ihrer Revision darauf abgehoben, dass im pragmatischen Sinne deskriptives Markup auch unabhängig von der OHCO These verwendet werden kann. Eben weil XML vor allem eine Syntax ist, war es letztlich immer wieder möglich, für ""konforme"" Lösungen zu sorgen, wie die Vorschläge der TEI zu nicht-hierarchischen Strukturen (TEI Guidelines: Chap. 20) verdeutlichen, aber auch die Beiträge von Renear zum Konzept des ""trojanischen Markup"" (Renear 2004) und zur XMLisierung von LMNL in CLIX (Renear 2004) oder xLMNL (Piez 2012). Der Grund lag auch darin, dass XML nicht nur in gut etablierte Strukturen der X-Familie eingebettet ist (XSD, XSLT, XQuery etc.), sondern auch allgemeiner die zentrale Document Object Modell (DOM)-Schnittstelle mit allen wichtigen Webelementen wie HTML bzw. XHTML (HTML 5), CSS oder Javascript teilt. Trotz aller Experimente ist heute in der Praxis kaum strittig, dass die TEI und das in ihr entwickelte Dokumentenmodel und mit Abstrichen auch ihre Serialisierung in XML das Mittel der Wahl für digitale Editionen ist. Allerdings bleibt das Modell unvollständig, wenn man nicht auch die anderen Komponenten der Edition in die Überlegungen einbezieht. Wie das analoge Buch über die Rolle zum Kodex und über die Handschrift zum Druck gefunden hat, so muss auch das digitale Buch zu einer stabilen Struktur und Form finden, um nicht nur in der Wissenschaft, sondern auch in Gedächtniseinrichtungen wie Bibliotheken langfristig gesichert, reproduziert und als wissenschaftlich referenzierbares Objekt über Schnittstellen zur Verfügung gestellt und genutzt werden zu können. Dabei sind die Besonderheiten digitaler Dokumente bzw. ihre spezifische Dynamik bzw. Potentialität zu beachten (PDF z.B. erfüllt diese Kriterien nicht). So ist es für das Verständnis der digitalen Edition wichtig, in die Debatte um die Zu- oder Unzulänglichkeit bestimmter Markupsprachen auch das eine Edition verwirklichende Ensemble von Dateien und Funktionen einzubeziehen, deren logisches Zusammenspiel zu bestimmen und nicht nur nach dem, wie die TEI es nennt, ""abstract model"" und dessen Serialisierungen zu unterscheiden, sondern auch Regelstrukturen, Präsentationsmodelle und differenzierte Metadatenformate als zum Verständnis notwendige Aspekte zu berücksichtigen. Editionen treten uns typischerweise als eine Kombination von Text mit Markup, Schemadatei, Stylesheets, Transformations- und sonstigen Skripten entgegen. Konkret handelt es sich um eine Reihe von Dateien (oder Datenströmen), wie z.B. .xml,.xslt,.xsd,.css oder .html, die zusammen ein funktionales Ganzes bilden, das als solches nicht nur die Stelle des physischen Dokumentes einnimmt, sondern auch die Grundlage der Langzeitarchivierung bildet. Dieser ganzheitlich betrachteten digitalen Edition ist eigentümlich, dass sie erst durch eine konkrete, meist nutzergesteuerte algorithmische Verarbeitungsanweisung nach dem klassischen EVA-Prinzip im Viewport oder Empfängersystem ""verwirklicht"" wird, während ihre Persistenz in den in die Edition hineinkodierten und in ihren Darstellungsfunktionen niedergelegten Möglichkeiten, keineswegs aber in der sichtbaren Oberfläche liegt. Letztere reduziert sich zu einem Ausschnitt, der nur bedingt das gesamte Potential der Edition aufzeigen kann. Wenn diese kombinatorisch vollständig beschreibbaren Möglichkeiten der Präsentation, die zutreffend mit dem Begriff der Schnittstelle verbunden werden (Boot/Zundert 2011; Zundert 2018), den Kernbegriff der digitalen Edition konstituieren, resultieren daraus eine Reihe von praktischen und theoretischen Konsequenzen. Ein erster wichtiger Schritt liegt in der Erkenntnis der Superiorität der Kodierungsgrundlage über die erzeugte angezeigte Oberfläche (Turska/Cummings/Rahtz 2016): ""Data is the important Long-term Outcome"". Das heißt aber nicht, dass die Oberfläche gleichgültig wäre. Sie darf nur nicht, weil sichtbar, als das einzig wichtige, ja nicht einmal als für die Edition maßgebliche Layer begriffen werden. Die Oberfläche, Visualisierung, die Ausgabe, die Schnittstelle, allgemein das algorithmische Erzeugnis, können über die primäre, immer aber reduzierte und mit Blick auf die kombinatorischen Möglichkeiten ausschnitthafte Darstellungsfunktion hinaus ihrerseits eigenständige Produkte bzw ""Interpretationen"" sein, vgl. (Zundert 2018). Die Edition selbst sind sie aber nicht, denn eine digitale Edition kann man, streng genommen, nicht sehen. Entsprechend sind zum einen eine Reihe von Text- bzw. Dokumentelemente wie das Layout als eigenständiges bedeutungstragendes oder zumindest -beinflussendes Phänomen als digitale ""Textästhethek"" und als ein Ergebnis einer Funktion mit vielfältigen Parametern neu zu interpretieren (Stäcker 2019), zum anderen verändern sich Nutzungsszenarien etwa bei der Archivierung und Zitierbarkeit von Editionen, denn wenn die Oberfläche eine von mehreren Möglichkeiten ist, kann sie nicht ohne weitere Vorkehrung Gegenstand des Zitierziels sein. ¬†Auf anderer Ebene bedeutet es, dass der/die Autor/Autorin oder, vermutlich genauer, das Autorenteam ein genaues Verständnis auch der technischen Dimension des digitalen Textes haben muss, um seinen nicht nur natürlichen, sondern auch maschinellen ""Leser"" zu erreichen, oder aber, dass die Autorintention die Schaffung von Möglichkeiten der digitalen Hermeneutik und Analyse einschließen muss. Eine wesentlich Dimension der digitalen Edition ist ferner ihre Verankerung im ""Netz"". Daraus ergeben sich generell Anforderungen an ihre ""Hypertextualität"", ihre ""Schnittstellen"" ¬†(Zundert 2018; Stäcker 2019) und Fähigkeit, sich in das ""semantic web"" zu integrieren (Ciotti/Tomasi 2016). Dazu zählt auch die unmittelbare Integration der genutzten ""Forschungsdaten"", etwa der digitalen Faksimiles, die im Rahmen der Es besteht die Hoffnung, dass mit dem Blickwechsel von dem zweidimensionalen sichtbaren Ergebnis auf die unsichtbare Potentialität der Edition sich das eher Proteushafte der Oberfläche der digitalen Edition auflöst und auch für die schon lange gärende Frage nach deren Persistenz und Nachhaltigkeit ein zufriedenstellender Ansatz gerade in ihrer, mit dem Motto der Tagung gesprochen: Multimodalität, gefunden werden kann. Der Beitrag möchte diesen Gedanken anhand von Beispielen weiter ausführen, um einen tragfähigen Begriff von einer persistenten digitale Edition als einem funktionalen und organischen Ensemble von exakt definierbaren Komponenten zu entwickeln.",de,digital editionen Phase Ausprobieren Entwickeln nunmehr Reife erreichen disziplinen exotisch Regelfall ansehen Publikation et -- driscoll Pierazzo förderbedingung dfg spiegeln trotz greifbar Fortschritt stehen digital editionen Kritik nennen fehlend Stabilität ungelö Frage Langzeitarchivierung Verfügbarkeit Gefühl Mangel These Beitrag resultieren geklärt methodisch technisch Frage beruhen Fehleinschätzung Natur digitaler editionen Analogie Druckmedium meist Oberfläche beurteilen Perspektivwechsel eigentümlichkeit Digitaler editionen zugrundeliegend strukturell algorithmisch Komponent ernst nehmen indes vergleichbar Stabilität zumindest Dokumentenmodell Form technisch Realisierung verständigen Entwicklung Nutzung markupsprach xml Anbeginn begleiten Kritik Unzulänglichkeit hierarchisch Ohco Modell deros et Repräsentation Text trotz verschieden vorschläge Abschließende spezifisch kodierungsprobleme klärend Lösung finden Popularität xml Feld maßgeblich Tei besonderer schaden erfreut xml tei Beliebtheit jüngerer Unterschied Tei xml betonen cummings umso erstaunlich alternativ Ansatz mangeln deroses Mecs goddag Texmecs lmnl zuletzt Text as Graph Tagml entstehen Anspruch nehmen nehmen xml Beschränkunge überwinden neu Konzept Anfang Diskussion Overlap Unzulänglichkeit xml vernachlässigen limitationen textmodells unbedingt limitationer Serialisierung digital Technik einer setzen Sgml Folge xml Ohco Modell Kopf entwickeln renear et Revision abgehoben pragmatisch Sinn deskriptiv markup unabhängig Ohco These verwenden xml Syntax letztlich konform Lösung sorgen Vorschlag tei Struktur tei guidelines chap verdeutlichen beitrag Renear Konzept trojanisch Markup renear xmlisierung lmnl Clix Renear xlmnl Piez Grund liegen xml etabliert Struktur einbetten xsd xslt xquery allgemein zentral Document Object Modell wichtig webelementen html xhtml Html Css Javascript teilen trotz experiment Praxis strittig tei entwickelt Dokumentenmodel abstrich Serialisierung xml Wahl digital editionen bleiben Modell unvollständig Komponente Edition Überlegung einbeziehen analog Buch Rolle Kodex Handschrift Druck finden digital Buch stabil Struktur Form finden Wissenschaft gedächtniseinrichtung Bibliothek langfristig sichern reproduzieren wissenschaftlich referenzierbar Objekt Schnittstelle Verfügung stellen nutzen Besonderheit Digitaler Dokument spezifisch Dynamik Potentialität beachten pdf erfüllen kriterien Verständnis digital Edition wichtig Debatte Unzulänglichkeit bestimmt markupsprach Edition verwirklichend Ensemble Datei Funktion einbeziehen logisch Zusammenspiel bestimmen tei nennen abstract Model serialisierungen unterscheiden regelstrukturen präsentationsmodell differenziert metadatenformate Verständnis notwendig Aspekt berücksichtigen editionen treten typischerweise Kombination Text Markup Schemadatei Stylesheet Sonstig Skript entgegen konkret handeln Reihe datei datenströmen funktional bilden Stelle physisch dokumentes einnehmen Grundlage Langzeitarchivierung bilden ganzheitlich betrachtet digital Edition eigentümlich konkret meist nutzergesteuert algorithmisch Verarbeitungsanweisung klassisch Viewport empfängersyst verwirklichen Persistenz Edition hineinkodieren darstellungsfunktion Niedergelegt Möglichkeit keineswegs sichtbar Oberfläche liegen letzterer reduzieren Ausschnitt bedingt gesamt Potential Edition aufzeigen kombinatorisch vollständig beschreibbar Möglichkeit Präsentation zutreffend Begriff Schnittstelle verbinden Boot zunderen zunderen Kernbegriff digital Edition konstituieren resultieren Reihe praktisch theoretisch Konsequenz wichtig Schritt liegen Erkenntnis Superiorität Kodierungsgrundlage erzeugt angezeigt oberfläch turska Cumming Rahtz data -- -- important Outcome oberfläch gleichgültig sichtbar einzig wichtig Edition maßgeblich Layer begreifen oberfläch Visualisierung Ausgabe Schnittstell allgemein algorithmisch Erzeugnis primär reduzieren Blick kombinatorisch Möglichkeit ausschnitthaft Darstellungsfunktion hinaus ihrerseits eigenständig Produkt Bzw Interpretation zunderen Edition digital Edition streng nehmen sehen entsprechend Reihe dokumentelemenen Layout eigenständig bedeutungstragend zumindest phänomen digital Textästhethek Ergebnis Funktion vielfältig Parameter neu interpretieren Stäcker verändern Nutzungsszenarie Archivierung Zitierbarkeit editionen oberfläch mehrere Möglichkeit Vorkehrung Gegenstand Zitierziel anderer Ebene bedeuten Autor Autorin vermutlich genau Autorenteam genau Verständnis technisch Dimension digital Text natürlichen maschinell Leser erreichen Autorintention Schaffung Möglichkeit digital Hermeneutik Analyse einschließen wesentlich Dimension digital Edition ferner Verankerung Netz ergeben generell anforderungen Hypertextualität schnittstellen Stäcker Fähigkeit Semantic web integrieren ciotti tomasi zählen unmittelbar Integration genutzt forschungsdat digital faksimile Rahmen bestehen Hoffnung Blickwechsel zweidimensional sichtbar Ergebnis unsichtbar Potentialität Edition eher Proteushaft Oberfläche digital Edition auflösen gärend Frage Persistenz Nachhaltigkeit zufriedenstellend Ansatz Motto Tagung sprechen Multimodalität finden Beitrag Gedanke anhand Beispiel ausführen tragfähig Begriff persistenten digital Edition funktional organisch ensembel exakt definierbaren Komponent entwickeln,"[('edition', 0.3140218389595275), ('xml', 0.24517550401812738), ('editionen', 0.2079620467751068), ('digital', 0.17828661331696732), ('renear', 0.1771308924623463), ('tei', 0.16978314212384404), ('oberfläch', 0.1563656387695362), ('zunderen', 0.13284816934675972), ('ohco', 0.13284816934675972), ('unzulänglichkeit', 0.11226054015626782)]"
2019,DHd2019,181_final-KONLE_Leonard_Makroanalytische_Untersuchung_von_Heftromanen.xml,Makroanalytische Untersuchung von Heftromanen,"Fotis Jannidis (Universität Würzburg, Deutschland); Leonard Konle (Universität Würzburg, Deutschland); Peter Leinen (Deutsche Nationalbibliothek, Frankfurt a.M.)","Gattungen, Komplexität, Schemaliteratur, Hochliteratur","Inhaltsanalyse, Strukturanalyse, Stilistische Analyse, Kollaboration, Literatur, Text","Heftromane, früher als ""Romane der Unterschicht"" (Nusser 1981) abgewertet, sind aufgrund eines weniger wertungsfreudigen Umgangs mit Populärliteratur (Hügel 2007, Kelleter 2012) in den letzten 10-15 Jahren wieder Gegenstand der Literaturforschung geworden (z.B. Nast 2017, Stockinger 2018). ""Heftromane"" wurden immer definiert durch das eigene Publikationsformat (zumeist rd. 64 Seiten), eigene Formen der Distribution über den Zeitschriftenmarkt und nicht über den Buchhandel, und auch die Soziographie der Heftromanleser weicht deutlich von der der sonstigen Literatur ab. Im Folgenden berichten wir über erste Ergebnisse einer Auswertung von 9.000 deutschsprachigen Heftromanen aus den Jahren 2009-2017. Möglich wurde die Forschung durch eine Kooperation zwischen der Würzburger Arbeitsgruppe zur literarischen Textanalyse und der Deutschen Nationalbibliothek (DNB), die die Daten vorhält. Ziel dieser ersten, noch weitgehenden explorativen Studie, sind die Antworten auf zwei Fragen: Wie unterscheiden sich die Gattungen der Heftromane untereinander und wie unterscheiden sich die Heftromane von Hochliteratur? Im ersten Schritt gehen wir der Frage nach, wie gut sich die Gattungen klassifizieren lassen und welche Texteigenschaften dabei eine Rolle spielen. Im zweiten Schritt werden die Gattungen inhaltlich erfasst. Zuletzt geht es um die angeblich einfachere Sprache der Heftromane. Die Analyse stützt sich auf die digitalen Texte, die an die Deutsche Nationalbibliothek abgeliefert wurden. Die Deutsche Nationalbibliothek (DNB) sammelt, archiviert, verzeichnet im gesetzlichen Auftrag die ab 1913 in Deutschland veröffentlichten Medienwerke sowie die im Ausland veröffentlichten deutschsprachigen Medienwerke, Übersetzungen deutschsprachiger Medienwerke in andere Sprachen und fremdsprachige Medienwerke über Deutschland und stellt diese der Öffentlichkeit zur Verfügung. Seit der Gesetzesnovelle von 2006 gehört auch das Sammeln von Medienwerken, die online publiziert werden, ausdrücklich zu den Aufgaben der DNB. Der Bestand der DNB umfasst derzeit etwa 5 Millionen digitale Objekte, darunter ca. 900.000 E-Books, ca. 1,5 Millionen E-Journal Ausgaben und ca. 2 Millionen E-Paper Ausgaben. Neben dem umfangreichen physischen Bestand steht den Nutzerinnen und Nutzern der DNB damit ein wachsender Fundus von ""born digital"" Objekten zur Verfügung. Die Anforderungen an die Informationsversorgung haben sich durch den digitalen Wandel insgesamt stark verändert. Die Einführung neuer Forschungsmethoden wie z.B. automatisierte Daten- und Textanalysen großer digitaler Bestände gehen mit der Notwendigkeit veränderter Formen der Bereitstellung von Beständen einher.  In der Vorverarbeitung wurden, soweit das aufgrund der selbst innerhalb eines Verlags sehr heterogenen Ausgangslage automatisch möglich war, Werbung, Leseproben usw. entfernt. Problematische Texte wurden nicht für die Analyse verwendet. Die unausgewogene Verteilung in Abbildung 1 kommt durch die Neupublikation älterer Hefte zustande. Die Verteilung über die Gattungen (Abbildung 2) ist sehr unausgewogen. Abb. 3 zeigt, dass manche der Gattungen von einzelnen Serien dominiert werden. Außerdem haben wir ein Vergleichskorpus ""Hochliteratur"" mit 500 Romanen von Autoren erstellt, die einen literarischen Preis gewonnen haben oder dafür vorgeschlagen wurden. Wir nehmen das Verhältnis von Types zu Tokens in einem Text als Maß für die Variabilität der Sprache und als Größe des Wortschatzes. Wie die Boxplots in Abbildung 13 zeigen, unterscheidet sich Hochliteratur (""hlit"") in diesem Punkt keineswegs grundlegend vom Heftroman. t-Test.  Auch bei der durchschnittlichen Länge der Worte, häufig verwendet für Maße der Leseschwierigkeit von Texten, ist die Varianz innerhalb der Heftromane größer der Unterschied zwischen den Heftromanen und der Hochliteratur (siehe Abbildung 14).   ",de,heftromane Roman Unterschicht nusser abwerten aufgrund wertungsfreudig umgangs Populärliteratur Hügel Kelleter letzter Gegenstand Literaturforschung nast stocking heftromane definieren Publikationsformat zumeist rd Seite Form Distribution Zeitschriftenmarkt Buchhandel Soziographie Heftromanleser weichen deutlich sonstig Literatur folgend berichten Ergebnis Auswertung Deutschsprachig heftromanen Forschung Kooperation Würzburger Arbeitsgruppe literarisch Textanalyse deutsch Nationalbibliothek dnb daten vorhälen Ziel weitgehend explorativ Studie Antwort Frage unterscheiden Gattung Heftromane untereinander unterscheiden Heftromane Hochliteratur Schritt Frage gattung klassifizieren lassen texteigenschafen Rolle spielen Schritt Gattung inhaltlich erfassen zuletzt angeblich einfacher Sprache Heftromane Analyse stützen digital Text deutsch Nationalbibliothek abliefern deutsch Nationalbibliothek dnb sammeln archivieren verzeichnen gesetzlich Auftrag Deutschland veröffentlicht Medienwerk Ausland veröffentlicht deutschsprachigen Medienwerk übersetzungen deutschsprachig Medienwerk Sprache fremdsprachig Medienwerk Deutschland stellen Öffentlichkeit Verfügung Gesetzesnovelle gehören Sammeln Medienwerke Online publizieren ausdrücklich Aufgabe dnb Bestand Dnb umfassen derzeit Million digital Objekt Million ausgaben Million ausgaben umfangreich physisch Bestand stehen nutzerinnen Nutzer Dnb wachsend Fundus born digital objekten Verfügung Anforderung informationsversorgung digital Wandel insgesamt stark verändern Einführung neu forschungsmethod automatisiert textanalyse Digitaler Beständ Notwendigkeit verändert Form Bereitstellung Beständ einher Vorverarbeitung soweit aufgrund innerhalb Verlag heterogen Ausgangslage automatisch Werbung leseproben entfernt problematisch Text Analyse verwenden unausgewogen Verteilung Abbildung Neupublikation alt heft zustande Verteilung gattung Abbildung unausgewogen abb zeigen Gattung einzeln Serie dominieren Vergleichskorpus Hochliteratur Romane Autor erstellen literarisch Preis gewinnen vorschlagen nehmen Verhältnis Types Token Text Maß Variabilität Sprache Größe Wortschatz Boxplot Abbildung zeigen unterscheiden Hochliteratur Hlit Punkt keineswegs grundlegend heftroman durchschnittlich Länge Wort häufig verwenden Maß Leseschwierigkeit Text Varianz innerhalb Heftromane groß Unterschied heftroman Hochliteratur sehen Abbildung,"[('heftromane', 0.37057086407505263), ('dnb', 0.2956068985672906), ('medienwerk', 0.27985495360184404), ('hochliteratur', 0.24704724271670175), ('gattung', 0.16829917403560285), ('nationalbibliothek', 0.16541983188910442), ('ausgaben', 0.13992747680092202), ('unausgewogen', 0.13033183649976726), ('million', 0.12642058827351557), ('heftroman', 0.12352362135835088)]"
2019,DHd2019,222_final-BURGHARDT_Manuel__The_Bard_meets_the_Doctor____Computergest_.xml,"""The Bard meets the Doctor"" 'Computergestützte Identifikation intertextueller Shakespearebezüge in der Science Fiction-Serie Dr. Who","Manuel Burghardt (Universität Leipzig, Deutschland); Selina Meyer (Universität Regensburg, Deutschland); Stephanie Schmidtbauer (Universität Regensburg, Deutschland); Johannes Molz (Ludwig-Maximilians-Universität München, Deutschland)","Text Reuse, Intertextualität, Shakespeare","Entdeckung, Inhaltsanalyse, Kontextsetzung, Methoden, Text","In der jüngeren Literatur- und Kulturtheorie geht man davon aus, dass alle literarischen Texte immer auch durch eine reiche Tradition, ja ein ganzes Ökosystem, bestehender Literatur beeinflusst sind (Allen, 2000, S. 1). Dieser Einfluss, der sich im Text sowohl in impliziten als auch in expliziten Querverbindungen durch Zitate offenbart, wird gemeinhin als Intertextualität bezeichnet Wenngleich Film in erster Linie ein visuelles Medium ist, so bietet sich über die Dialoge zusätzlich ein verbaler Analysezugang (vgl. Kozloff, 2000), Klarer (1998, S. 54) spricht in dem Zusammengang gar von Film als semi-textuelles Genre. Die den Filmen zugrunde liegenden Skripte lassen sich als Dramen lesen, da sie ausschließlich aus Figurennamen, Sprechakten und Bühnenanweisungen bestehen. Für die quantitative Analyse von Intertextualitätsphänomenen bei Filmen liegt ein großer Vorteil in der Verfügbarkeit vollständig transkribierter Dialoge die als Untertitel Vor diesem Hintergrund exploriert der vorliegende Beitrag intertextuelle Bezüge auf das Werk Shakespeares in der britischen Fernsehserie Dr. Who und setzt dabei auf computergestützte Analysemethoden, die bislang vor allem im Bereich klassischer Altertumswissenschaften und historischer Sprachen eingesetzt wurden. Dr. Who eignet sich dabei in besonderer Weise, da die TV-Serie fester Bestandteil der britischen Kultur ist, was automatisch eine größere Nähe zum Werk ihres Landsmannes Shakespeare bedeutet. Andererseits gibt es einige Folgen bei Dr. Who, bei denen bereits im Titel explizite Shakespeare-Bezüge deutlich werden Im Rahmen unserer Fallstudie werden systematisch Referenzen aus zehn der bekanntesten Shakespeare-Stücke ( Für die Identifikation von textuellen Übereinstimmungen und Textähnlichkeiten ( Weiterhin wurde das Shakespeare-Korpus in jeweils überlappende 9-Gramme ( Gleichzeitig sind allerdings sehr kurze Referenzen, bspw. die Nennung einer Figur wie ""Othello"" oder ""Macbeth"", nicht ausgeschlossen, gehen aber bei diesem Ansatz verloren. Wir ergänzen den Insgesamt wurden mit der  Bei genauerer Überprüfung der Treffer wird schnell klar, dass Figurennamen aus Shakespeares Historiendramen häufig auch in der ursprünglichen Bedeutung der zugrundliegenden historischen Figur zitiert werden (bspw.  Auffällig niedrig scheint im Vergleich zu den   Gleichzeitig erhalten wir mit dem local aligment-Ansatz auch viele falsch-positive Treffer, die zwar längere, in beiden Texten vorkommende Sequenzen beschreiben, aber keine genuinen Shakespeare-Zitate sind, sondern eher hochfrequente Idiome, bspw.:  Der zweigleisige Ansatz einer",de,jung Kulturtheorie literarisch Text reich Tradition ökosystem bestehend Literatur beeinflussen einfluss Text sowohl impliziten explizit querverbindungen Zitat offenbaren gemeinhin Intertextualität bezeichnen wenngleich Film Linie visuell Medium bieten Dialoge zusätzlich verbal Analysezugang Kozloff klar sprechen Zusammengang Film Genre Film zugrunde liegend Skript lassen Dram lesen ausschließlich figurennam Sprechakt Bühnenanweisung bestehen quantitativ Analyse intertextualitätsphänomenen Film liegen Vorteil Verfügbarkeit vollständig transkribiert Dialoge untertitel Hintergrund explorieren vorliegend Beitrag intertextuell bezug Werk Shakespeare britisch fernsehserie -- who setzen computergestützt analysemethoden bislang Bereich klassisch Altertumswissenschaft historisch Sprache einsetzen dr Who eignen besonderer Weise fest Bestandteil britisch Kultur automatisch groß Nähe Werk landsmann Shakespeare bedeuten andererseits Folge dr who Titel explizit deutlich Rahmen fallstudie systematisch referenzen bekannt Identifikation Textuell übereinstimmungen Textähnlichkeite weiterhin jeweils überlappend gleichzeitig kurz Referenz Nennung Figur othello macbeth ausschließen Ansatz verlieren ergänzen insgesamt genauer Überprüfung Treffer schnell klar Figurenname Shakespeare Historiendramen häufig ursprünglich Bedeutung zugrundliegend historisch Figur zitieren auffällig niedrig scheinen Vergleich gleichzeitig erhalten Local treff lang Text vorkommend Sequenz beschreiben genuin eher hochfrequent idiome zweigleisig Ansatz,"[('film', 0.29958503321493374), ('who', 0.28755317986887585), ('shakespeare', 0.2246887749112003), ('britisch', 0.1917021199125839), ('dialoge', 0.18168807184511265), ('dr', 0.16757406380952972), ('klar', 0.11521799163530971), ('explizit', 0.1082327597334237), ('gleichzeitig', 0.10425020097908133), ('kozloff', 0.1029080639740834)]"
2019,DHd2019,134_final-BRUNNER_Annelen_Das_Redewiedergabe_Korpus.xml,Das Redewiedergabe-Korpus.   Eine neue Ressource,"Annelen Brunner (Institut für Deutsche Sprache, Deutschland); Lukas Weimer (Universität Würzburg, Deutschland); Ngoc Duyen Tanja Tu (Institut für Deutsche Sprache, Deutschland); Stefan Engelberg (Institut für Deutsche Sprache, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland)","Redewiedergabe, Annotation, Linguistik, Literatur, Korpus, Historisch, Zeitschriften, Zeitungen","Annotieren, Veröffentlichung, Daten, Literatur, Forschung, Text","In diesem Beitrag Redewiedergabe ist sowohl für die Linguistik als auch die Literaturwissenschaft ein interessanter Untersuchungsgegenstand. Die Repräsentation der Figurenstimme in Erzähltexten hat in der Narratologie viel Aufmerksamkeit erfahren und wurde in zahlreichen Kategoriesystemen abgebildet (vgl. z.B. Genette 2010; Mart√≠nez / Scheffel 2016). In der Linguistik besteht ein Interesse an sprachlichen Formen der Redewiedergabe, sowie an Redeeinleitungsverben (vgl. z.B. Hauser 2008, Engelberg 2015). Detaillierte, manuell annotierte Korpora mit diesem Themenschwerpunkt sind bislang vor allem für das Deutsche sehr rar. Ein Vorbild mit detaillierter, literaturwissenschaftlich motivierter Annotation mehrere Redewiedergabetypen für das Englische ist das Korpus von Semino/Short 2004. Das ebenfalls manuell annotierte DROC-Korpus hat seinen Schwerpunkt auf Figurenreferenzen in Romanen, enthält in diesem Kontext allerdings auch Annotationen direkter Wiedergabe mit Sprecherzuordnung (Krug et al. 2018b). Unser Korpus ist eine direkte Weiterentwicklung des aus 13 Erzähltexten bestehenden Korpus aus Brunner 2015, unterscheidet sich jedoch von diesem vor allem in folgenden Aspekten: Es enthält neben fiktionalen auch nicht-fiktionale Texte, die Annotationen sind durch Mehrfachannotation wesentlich verlässlicher und es ist deutlich umfangreicher (für das Beta-Release ca. 350.000 Tokens vs. 57.000 Tokens in Brunner 2015). Das RW-Korpus umfasst Textmaterial aus dem Zeitraum 1840-1920. Es beruht auf den folgenden drei Textquellen, aus denen jeweils nur die Texte ausgewählt wurden, die in den Untersuchungszeitraum passen: Bei der Korpuszusammenstellung sollte eine möglichst große Diversität der enthaltenen Texte erzielt werden. Um dies zu erreichen, setzt sich das Korpus aus Textausschnitten (Samples) zusammen. Diese haben mindestens 500 Wörter für fiktionale Texte bzw. 200 Wörter für nicht-fiktionale Texte 'mit dieser großzügigeren Grenze war es möglich, auch kurze, abgeschlossene Artikel aufzunehmen, die für Zeitungen/Zeitschriften typisch sind. Die Samples wurden mit folgenden Besonderheiten randomisiert aus dem vorhandenen Textmaterial gezogen: Bei den Texten der Digitalen Bibliothek wurde erzwungen, dass jeder vertretene Autor innerhalb einer Dekade gleichermaßen berücksichtigt wird. Entsprechend wurde beim MKHZ erzwungen, dass alle in einer Dekade vertretenen unterschiedlichen Zeitungen/Zeitschriften gleichermaßen berücksichtigt werden. Damit wurde verhindert, dass Autoren bzw. Zeitungen/Zeitschriften mit wenig Material beim Sampling-Prozess vollkommen herausfallen. Das Beta-Release enthält Texte von etwa 140 unterscheidbaren Autoren und aus 20 unterschiedlichen Zeitungen/Zeitschriften. Die Quelltexte wurden größtenteils in ihrem Ursprungszustand belassen, mit zwei Ausnahmen: Da für die Zeitschrift ""Die Grenzboten"" nur automatische OCR-Erkennung durchgeführt wurde, wurden die Samples aus dieser Textquelle manuell nachkorrigiert. In den Texten aus den beiden anderen Quellen wurden häufige Sonderzeichen, wie das Schaft-S, durch moderne Öquivalente ersetzt, jedoch weisen die Texte dennoch in unterschiedlichem Maße altertümliche Schreibungen und z.T. auch Sonderzeichen auf. Insgesamt ist festzuhalten, dass die Textformen im RW-Korpus sehr divers sind, so sind z.B. Texte im Dialekt enthalten, sowie Zeitungsausschnitte, die reine Listen sind. Wir haben uns bewusst dagegen entschieden, solch ungewöhnliches Material herauszufiltern, um eine realistische Repräsentation des Textmaterials aus den untersuchten Dekaden zu erhalten. Beim RW-Korpus wurde eine Ausgewogenheit in der zeitlichen Dimension (Textmaterial pro Dekade) sowie zwischen fiktionalen und nicht-fiktionalen Texten angestrebt. Entgegen ursprünglicher Annahmen stellte es sich als nicht sinnvoll heraus, die Trennung fiktional - nicht-fiktional rein aufgrund der Textquelle zu treffen: Es liegt in der Natur der Textsorte Zeitung/Zeitschrift, dass dort auch fiktionale Texte abgedruckt werden (im Feuilleton, als Fortsetzungsromane u.Ö.). Somit wurde das Kriterium ""fiktional"" für jedes Sample individuell festgelegt. Unsere Definition für ""Fiktion"" ist dabei angelehnt an Gabriel 2007: ""Ein erfundener (""fingierter"") einzelner Sachverhalt oder eine Zusammenfügung solcher Sachverhalte zu einer erfundenen Geschichte"" (Gabriel 2007: 594). Bei der Identifizierung wurde besonderer Wert auf paratextuelle Merkmale (z.B. Untertitel, Rubriken u.Ö.) gelegt. Von den Samples aus dem MKHZ und den ""Grenzboten"" wurden auf diese Weise ca. 12% als fiktional eingestuft. Die folgende Tabelle zeigt die wichtigsten Metadaten des RW-Korpus, welche nach dem Sampling und der Textkorrektur vergeben werden. Aufgrund der Diversität der in Zeitungen/Zeitschriften vertretenen Texte wurde für jedes Sample eine nähere Klassifikation des Texttyps vorgenommen, so dass auch dessen Einfluss auf die Verteilung der Redewiedergabetypen untersucht werden kann. Die folgende Abbildung gibt einen ersten Eindruck, welch deutliche Abweichungen hier erkennbar sind. Gezeigt werden nur die Texttypen, für beim Korpusstand vom 25.09.2018 mehr als 10 Samples vorlagen. Die Y-Achse zeigt Prozent der Tokens im Text. Wir unterscheiden die vier Typen direkte, indirekte, frei indirekte (""erlebte"") und erzählte Wiedergabe, sowie die drei Medien Rede ( Außerdem annotieren wir die Rahmenformel, die eine direkte oder eine indirekte Wiedergabe einleiten kann. In den Rahmenformeln sowie den Instanzen von erzählter Wiedergabe wird das zentrale Wort markiert, das auf die Sprech-/Gedanken-/Schreibhandlung verweist (z.B. Während die Unterscheidung der drei Medien nur in Ausnahmefällen problematisch ist, bedürfen die vier Typen genauerer Definitionen. Die direkte Wiedergabe ( Die indirekte Wiedergabe ( Die freie indirekte Wiedergabe ( Die erzählte Wiedergabe ( Ein Sonderfall sind uneingeleitete Konjunktivsätze, die zur Wiedergabe verwendet werden. Diese werden als Mischform zwischen indirekter und frei indirekter Wiedergabe markiert. Darüber hinaus gibt es zusätzliche Attribute, die Besonderheiten bei der Wiedergabe markieren und in der folgenden Tabelle dargestellt werden: Die detaillierten Annotationsrichtlinien können unter http://redewiedergabe.de/richtlinien/richtlinien.html eingesehen werden. Die genaue Identifizierung und Klassifizierung der Redewiedergaben auf der Grundlage des detaillierten Annotationssystems ist eine schwierige Aufgabe. Jedes Sample des RW-Korpus durchläuft darum einen mehrschrittigen Prozess. Zunächst wird es von zwei Annotatoren unabhängig voneinander annotiert. Danach vergleicht ein weiterer Experte die Annotationen und erstellt, falls notwendig, eine Konsens-Annotation, die dann ins finale Korpus aufgenommen wird. Jedes Sample wird also von drei Personen bearbeitet, um größtmögliche Konsistenz zu gewährleisten. Die Annotatoren arbeiten mit dem eclipse-basierten Annotationswerkzeug ATHEN (entwickelt von Markus Krug im Projekt Kallimachos, www.kallimachos.de), für das im Projekt eine spezielle Oberfläche für die Redewiedergabe-Annotation implementiert wurde (für eine detaillierte Beschreibung vgl. auch Krug et al. 2018a). Das Werkzeug ist frei verfügbar unter der Adresse Das Beta-Release wird in einem standardisierten und dokumentierten Textformat im Langzeitarchiv des Instituts für Deutsche Sprache zur freien Nutzung zur Verfügung gestellt ( Nutzungsszenarien für das Korpus sind vielfältig: Aus NLP-Perspektive kann es als Test- und Trainingsmaterial für automatische Redewiedergabeerkenner verwendet werden. Aus linguistischer Perspektive bieten sich Korpusstudien zu sprachlichen Eigenheiten der Redewiedergabe an, wie z.B. die laufenden Studien zu Redewiedergabeeinleitern von Tu. Aus literaturwissenschaftlicher Perspektive erlaubt das Korpus z.B. Untersuchungen zu der Häufigkeit und Form von Wiedergaben in Erzähltexten in ihrer Relation zur Figurencharakterisierung.",de,Beitrag Redewiedergabe sowohl Linguistik Literaturwissenschaft interessant Untersuchungsgegenstand Repräsentation Figurenstimme Erzähltext Narratologie Aufmerksamkeit erfahren zahlreich Kategoriesystem abbilden Genette scheffel Linguistik bestehen Interesse sprachlich Form Redewiedergabe Redeeinleitungsverbe Hauser Engelberg detailliert manuell annotiert Korpora Themenschwerpunkt bislang deutsch rar Vorbild Detaillierter literaturwissenschaftlich motiviert Annotation mehrere Redewiedergabetype englisch Korpus Semino Short ebenfalls manuell annotiert Schwerpunkt Figurenreferenze romanen enthalten Kontext annotation direkt Wiedergabe Sprecherzuordnung krug et Korpus direkt Weiterentwicklung Erzähltext bestehend Korpus Brunner unterscheiden folgend Aspekt enthalten fiktionalen Text Annotation Mehrfachannotation wesentlich verlässlich deutlich umfangreich Token Token Brunner umfassen textmaterial Zeitraum beruhen folgend Textquelle jeweils Text auswählen Untersuchungszeitraum passen Korpuszusammenstellung möglichst diversität enthalten Text erzielen erreichen setzen korpus Textausschnitt Samples mindestens Wörter fiktional Text Wörter Text großzügigeren Grenze kurz abgeschlossen Artikel aufnehmen Zeitung zeitschriften typisch Samples folgend Besonderheit randomisieren vorhanden Textmaterial ziehen Text digital Bibliothek erzwingen vertreten Autor innerhalb Dekade gleichermaßen berücksichtigen entsprechend Mkhz erzwingen dekade vertreten unterschiedlich Zeitung Zeitschrift gleichermaßen berücksichtigen verhindern Autor Zeitung Zeitschrift Material vollkommen herausfallen enthalten Text unterscheidbaren Autor unterschiedlich Zeitung Zeitschrift Quelltexte größtenteils Ursprungszustand belassen Ausnahme Zeitschrift grenzbot automatisch durchführen Samples Textquelle manuell nachkorrigiert Text quellen häufig sonderzeichen modern öquivalente ersetzen weisen Text dennoch unterschiedlich Maß altertümlich Schreibunge sonderzeichen insgesamt festhalten Textform divers Text Dialekt enthalten Zeitungsausschnitte rein liste bewussen entscheiden solch ungewöhnlich Material herausfiltern realistisch Repräsentation Textmaterial untersuchten Dekad erhalten Ausgewogenheit zeitlich Dimension textmaterial pro Dekade fiktional texten anstreben entgegen ursprünglich annahmen stellen sinnvoll heraus Trennung fiktional rein aufgrund Textquelle treffen liegen Natur Textsorte Zeitung Zeitschrift fiktional Text abdrucken Feuilleton fortsetzungsroman somit kriterium fiktional jeder Sample individuell festlegen Definition Fiktion anlehnen gabriel erfunden fingiert einzeln Sachverhalt Zusammenfügung sachverhalen erfunden Geschichte gabriel Identifizierung besonderer Wert paratextuell merkmal untertitel rubrik legen Samples Mkhz grenzboten Weise fiktional einstufen folgend Tabelle zeigen wichtig Metadat Sampling Textkorrektur vergeben aufgrund diversität Zeitung Zeitschrift vertreten Text jeder Sample näh Klassifikation Texttyp vornehmen einfluss Verteilung Redewiedergabetype untersuchen folgend Abbildung Eindruck welcher deutlich abweichungen erkennbar zeigen Texttype Korpusstand samples vorliegen zeigen Prozent Token Text unterscheiden Type direken indirekt frei indirekt erleben erzählt Wiedergabe Medium Rede annotieren Rahmenformel direkt indirekt Wiedergabe einleiten Rahmenformel Instanz erzählt Wiedergabe zentral Wort markieren verweisen Unterscheidung Medium Ausnahmefälle problematisch bedürfen Type genauer Definition direkt Wiedergabe indirekt Wiedergabe frei indirekt Wiedergabe erzählt Wiedergabe Sonderfall uneingeleitet Konjunktivsätz Wiedergabe verwenden Mischform indirekt frei indirekt Wiedergabe markieren hinaus zusätzlich Attribut Besonderheit Wiedergabe markieren folgend Tabelle darstellen Detailliert Annotationsrichtlinie eingesehen genau Identifizierung Klassifizierung Redewiedergab Grundlage Detailliert Annotationssystem schwierig Aufgabe jeder Sample durchlaufen mehrschrittigen prozess Annotator unabhängig voneinander annotiert vergleichen weit Experte annotatio erstellen falls notwendig final Korpus aufnehmen jeder Sample Person bearbeiten größtmöglich Konsistenz gewährleisten Annotator arbeiten Annotationswerkzeug Athen entwickeln Markus Krug Projekt kallimachos Projekt speziell oberfläch implementieren detailliert Beschreibung krug et Werkzeug frei verfügbar Adresse standardisiert dokumentiert Textformat Langzeitarchiv Institut deutsch Sprache frei Nutzung Verfügung stellen Nutzungsszenarie korpus vielfältig Trainingsmaterial automatisch Redewiedergabeerkenner verwenden linguistisch Perspektive bieten korpusstudien sprachlich Eigenheit Redewiedergabe laufend Studie redewiedergabeeinleitern tu literaturwissenschaftlich Perspektive erlauben korpus Untersuchung Häufigkeit Form Wiedergabe Erzähltext Relation Figurencharakterisierung,"[('wiedergabe', 0.43154409288583434), ('zeitung', 0.23135208657182535), ('indirekt', 0.21829266704435174), ('samples', 0.19279340547652113), ('zeitschrift', 0.18367649824313848), ('fiktional', 0.16274773919811245), ('textmaterial', 0.1542347243812169), ('dekade', 0.1275027006604827), ('textquelle', 0.1275027006604827), ('sample', 0.1247386668824867)]"
2019,DHd2019,250_final-VAUTH_Michael_Netzwerkanalyse_narrativer_Informationsvermitt.xml,Netzwerkanalyse narrativer Informationsvermittlung in Dramen,"Michael Vauth (Technische Universität Hamburg, Deutschland)","Netzwerkanalyse, Narrativität, Informationsvermittlung, Dramenanalyse","Annotieren, Netzwerkanalyse, Visualisierung, Literatur","In diesem Beitrag wird ein Verfahren vorgestellt, das Netzwerkvisualisierungen dramatischer Texte für eine spezifische Form der kommunikativen Interaktion zwischen Figuren fokussiert. Es wird gezeigt, inwiefern gewichtete, gerichtete und dynamische Figurennetzwerke narrative Informationsvermittlung in der Figurenrede visualisieren können und auf diesem Weg dramennarratologische Analysen bzw. Annotationen ausgewertet werden. Im Gegensatz zu literaturwissenschaftlichen Netzwerkanalysen, die um die automatisierte Analyse Darüber hinaus werden mit Rückgriff auf die ermittelten Netzwerkdaten Deutungspotenziale exemplarisch an Kleists Ausgangspunkt der vorgestellten Netzwerke ist eine Typologie narrativer Figurenrede bzw. von Binnenerzählungen, die zur Annotation der Dramen Heinrich von Kleists genutzt wurden. Tabelle 1: Vorkommen narrativer Figurenrede in Kleists Dramen Diese manuellen Annotationen sind die Grundlage dafür, dass unterschiedliche Formen der Informationsvermittlung netzwerkgraphisch visualisiert werden können. Unter Rückgriff auf die Annotationen der narrativen Figurenrede und die TEI-Annotationen von Sprecherfiguren und Szenen- sowie Aktwechseln im TextGrid-Korpus wurden chronologisierte Sender-Adressaten-Kanten erstellt. Dazu wurden die vier Sprecherfiguren, die auf eine narrative Öußerung folgen oder ihr vorangehen, als Adressaten berücksichtigt, sofern keine Akt- oder Szenenwechsel zwischen narrativer Figurenrede und potenziellem Adressaten liegt und es sich um unterschiedliche Figuren handelt. Die Anzahl der erzeugten Kanten ist somit deutlich höher als die Anzahl der narrativen Öußerungen (siehe exemplarisch in Tabelle 2 die Kanten 4 und 5 sowie 8-10). Tabelle 2: Auszug aus der Kantenliste zu DFS Das Kantengewicht ( Die Figurennetzwerke, die auf dieser Grundlage erstellt werden, illustrieren, welche Figuren sich zu welchem Zeitpunkt des Dramenverlaufs narrativ äußern, welche Figuren narrative Informationen bekommen und wie häufig Figuren an narrativem Informationsaustausch beteiligt sind. Abbildung 1 zeigt dieses Potential der Netzwerkvisualisierung exemplarisch für Die Größe der Knotenbeschriftung repräsentiert hier die betweenness centrality Tabelle 3: Betweenness centrality, (weighted) in- und outdegree in DFS Schon anhand dieses Beispiels und der netzwerkmetrischen Daten in Tabelle 3 lassen sich einige Vorzüge einer netzwerkgraphischen Annotationsauswertung zeigen: Zudem können unterschiedliche Formen der narrativen Figurenrede netzwerkgraphisch miteinander verglichen werden. Die Abbildungen 3 und 4 zeigen dies exemplarisch. In Abbildung 3 werden Figurenerzählungen visualisiert, in denen sich Figuren in Übereinstimmung mit der fiktionalen Wirklichkeit äußern. Abbildung 4 zeigt narrative Öußerungen, bei denen das Gegenteil der Fall ist. Es handelt sich also um narrative Falschaussagen. Der Vergleich ist in diesem Fall aufgrund der vorhandenen Parallelen Wie Solange die automatische Annotation narrativer Figurenrede nicht möglich ist, setzt das vorgestellte Verfahren einen relativ großen Annotationsaufwand voraus. Es ermöglicht somit keinen umfassenden Vergleich von Dramen, was unter anderem zur Einordnung der vorgestellten quantitativen Netzwerkanalysen wünschenswert wäre. In diesem Beitrag wurde jedoch exemplarisch gezeigt, inwiefern netzwerkgraphische Visualisierungen für die Auswertung narratologischer Annotationen einen analytischen Mehrwert haben können. Die formalen Annotationen können und sollen durch inhaltsbezogene Annotationen angereichert werden. Auf dieser Grundlage könnte netzwerkgraphisch der Informationsaustausch über bestimmte Themen oder Figuren visualisiert werden. Tabelle 4: Figuren mit höchster betweenness centrality in Kleist",de,Beitrag Verfahren vorstellen netzwerkvisualisierung dramatisch Text spezifisch Form kommunikativ Interaktion Figur fokussieren zeigen inwiefern gewichtet gerichtet dynamisch Figurennetzwerk narrativ Informationsvermittlung Figurenrede visualisieren Weg dramennarratologisch Analyse annotation auswerten Gegensatz literaturwissenschaftlich Netzwerkanalyse automatisiert Analyse hinaus Rückgriff ermittelt Netzwerkdat deutungspotenzial exemplarisch Kleist Ausgangspunkt vorgestellter Netzwerk Typologie narrativ Figurenred binnenerzählungen Annotation Dram heinrich Kleist nutzen Tabell vorkommen narrativ Figurenrede Kleist dramen manuell Annotation Grundlage unterschiedlich Form Informationsvermittlung netzwerkgraphisch visualisieren Rückgriff annotatio narrativ Figurenred Sprecherfigure Aktwechsel Chronologisierte erstellen Sprecherfigure narrativ Öußerung folgen vorangehen Adressat berücksichtigen sofern Szenenwechsel narrativ Figurenrede Potenziellem Adressat liegen unterschiedlich Figur handeln Anzahl erzeugt Kant somit deutlich hoch Anzahl narrativ öußerungen sehen exemplarisch Tabelle Kant Tabell Auszug Kantenliste dfs Kantengewicht Figurennetzwerk Grundlage erstellen illustrieren Figur Zeitpunkt Dramenverlauf narrativ äußern Figur narrativ Information bekommen häufig figuren narrativ informationsaustausch beteiligen Abbildung zeigen Potential Netzwerkvisualisierung exemplarisch Größe Knotenbeschriftung repräsentieren Betweenness centrality Tabelle betweenness centrality weighted Outdegree Dfs anhand Beispiel netzwerkmetrisch daten Tabelle lassen vorzug netzwerkgraphisch Annotationsauswertung zeigen zudem unterschiedlich Form narrativ Figurenrede netzwerkgraphisch miteinander vergleichen abbildungen zeigen exemplarisch Abbildung figurenerzählungen visualisieren Figur übereinstimmung fiktional Wirklichkeit äußern Abbildung zeigen narrativ öußerungen Gegenteil Fall handeln narrativ Falschaussag Vergleich Fall aufgrund vorhanden Parallel solange automatisch Annotation narrativ Figurenrede setzen vorgestellt Verfahren relativ Annotationsaufwand voraus ermöglichen somit umfassend Vergleich Dram Einordnung vorgestellter quantitativ netzwerkanalysen wünschenswert Beitrag exemplarisch zeigen inwiefern netzwerkgraphisch visualisierungen Auswertung narratologischer annotation analytisch mehrweren Formale annotationen inhaltsbezogen annotation angereichert Grundlage netzwerkgraphisch informationsaustausch bestimmt Thema Figur visualisieren Tabell Figur hoch Betweenness Centrality Kleist,"[('narrativ', 0.46135733142006735), ('netzwerkgraphisch', 0.3163661150395015), ('kleist', 0.21387080385743024), ('figurenrede', 0.21224706091413467), ('betweenness', 0.17680263114323455), ('figur', 0.17552153301081844), ('centrality', 0.16756689578709402), ('exemplarisch', 0.1522050052535494), ('visualisieren', 0.14012728441583186), ('vorgestellter', 0.12654644601580062)]"
2019,DHd2019,142_final-HORSTMANN_Jan_Texte_digital_annotieren_und_analysieren_mit_C.xml,Texte digital annotieren und analysieren mit CATMA 6.0,"Jan Horstmann (Universität Hamburg, Deutschland); Jan Christoph Meister (Universität Hamburg, Deutschland); Marco Petris (Universität Hamburg, Deutschland); Mareike Schumacher (Universität Hamburg, Deutschland)","Annotation, Analyse, Visualisierung","Strukturanalyse, Annotieren, Einführung, Visualisierung, Methoden, Text","In diesem hands-on Workshop werden wir die Möglichkeiten der für Geisteswissenschaftler*innen entwickelten Annotations- und Analyseplattform CATMA 6.0 praktisch erkunden. Es werden keinerlei technische Vorkenntnisse vorausgesetzt. Inhaltlich werden wir uns vor allem den theoretischen und praktischen Aspekten der digitalen Annotation von (literarischen) Texten, als auch der Analyse und Visualisierung dieser Texte und der erstellten Annotationen widmen. CATMA (Computer Assisted Text Markup and Analysis; CATMA unterstützt Von linguistischen Textanalysetools unterscheidet sich CATMA insbesondere durch seinen Zudem bietet CATMA auch die Möglichkeit, bereits annotierte Texte zu verarbeiten (z.B. durch den Upload von XML-Dateien) und die in anderen Tools erstellten Annotationen anzuzeigen, mit zu analysieren und damit wissenschaftlich nachzunutzen. Außerdem lassen sich in CATMA auch automatische (z.B. POS für deutschsprachige Texte) und halb-automatische Annotationen generieren. Die Annotation von Texten gehört seit Jahrhunderten zu den textwissenschaftlichen Kernpraktiken (vgl. Moulin 2010). Genauer lassen sich hier Freitextkommentare, taxonomiebasierte Annotation und Textauszeichnung unterscheiden, wobei die Übergänge häufig fließend sind (vgl. Jacke 2018, ¬ß 9). Während CATMA 6.0 auch eine Funktion für Freitextkommentare bietet, ist die taxonomiebasierte Annotation das eigentliche Kerngeschäft des Tools 'wobei die Taxonomie prinzipiell undogmatisch erstellt werden kann und die Form von sog. Tagsets annimmt, denen für kollaborative Annotationsprojekte wahlweise eine Annotations-Guideline beigegeben werden kann (vgl. auch Bögel et al). Im Workshop werden wir den Unterschied von Neben der Annotation sind die Analyse und Visualisierung der Text- und Annotationsdaten das andere wichtige Standbein von CATMA. Hier wird Neben diesen grundlegenden Funktionen, die alle per Klick ausgeführt werden können, bietet CATMA den sog. Im Analysebereich können außerdem halbautomatische Annotationen erstellt werden, d.h. man annotiert wiederkehrende Wörter oder Wortgruppen auf einmal mit einem bestimmten Tag, statt dies manuell und wiederholt im Annotationsmodul zu tun. Der Wechsel zwischen der Arbeit im Annotations- und Analyse- und Visualisierungs-Modul ist ein iterativer Prozess, der die klassisch-zirkuläre hermeneutische Interpretationsarbeit in der Literaturwissenschaft widerspiegelt (vgl. Gius, in Vorbereitung). Im Workshop werden wir uns in einer Mischung aus Präsentations- und Hands-on-Phasen der textanalytischen Arbeit in CATMA 6.0 nähern. Nach einer generellen Einführung in das Tool werden die Teilnehmer*innen anhand eines vorgegebenen Beispieltextes den gesamten Workflow von der individuellen taxonomiebasierten Textannotation, über die Analyse hin zur Visualisierung und Interpretation der Text- und Annotationsdaten kennenlernen und praktisch erproben können. Die Teilnehmer*innen sollen ausgehend vom digitalen Text in die Lage versetzt werden, Annotationen manuell und automatisch unterstützt zu erstellen und in Annotation Collections zu speichern, Tagsets/Taxonomien zu entwickeln und den Text alleine und in Kombination mit den Annotationen zu analysieren und zu visualisieren. Für Diskussionen und individuelle Rückfragen (theoretischer, praktischer und technischer Art) auf jedem Niveau und in Bezug auf die Projekte der Teilnehmer*innen wird ausreichend Möglichkeit bestehen. Im Workshop werden wir den Arbeitsablauf der digitalen Texterforschung praktisch kennenlernen: Dr. Jan Horstmann Universität Hamburg, Institut für Germanistik, Überseering 35, Postfach #15, 22297 Hamburg Jan Horstmann ist Postdoc und koordiniert das DFG-Projekt forTEXT, in dem neben der Dissemination von digitalen Routinen, Ressourcen und Tools in die klassischeren Fachwissenschaften auch die Weiterentwicklung von CATMA eine wesentliche Rolle spielt. Als Literaturwissenschaftler interessiert er sich vor allem für die neuen Perspektiven und Erkenntnispotentiale, die DH-Methoden auf literarische Artefakte bereithalten können, und forscht in diesem Sinne unter anderem zu Entsagung und Ironie bei Goethe. Prof. Dr. Jan Christoph Meister Universität Hamburg, Institut für Germanistik, Überseering 35, Postfach #15, 22297 Hamburg Jan Christoph Meister ist Professor für Digital Humanities mit dem Schwerpunkt Literaturwissenschaft. Als ursprünglicher Erfinder von CATMA hat er etliche Forschungsprojekte zur Annotation und Visualisierung textueller Daten und der Entwicklung und Verbesserung von DH-Tools geleitet. Marco Petris, Dipl. Inform. Universität Hamburg, Institut für Germanistik, Überseering 35, Postfach #15, 22297 Hamburg Marco Petris ist Informatiker mit starker Affinität zu geisteswissenschaftlichen Fragestellungen. Er ist von Anfang an an der Entwicklung von CATMA beteiligt und beschäftigt sich mit allen Aspekten der DH-Toolentwicklung, des Tool-Designs und der Implementierung. Mareike Schumacher, M.A. Universität Hamburg, Institut für Germanistik, Überseering 35, Postfach #15, 22297 Hamburg Mareike Schumacher promoviert als digitale Literaturwissenschaftlerin über Orte und narratologische Ortskategorien in literarischen Texten, beschäftigt sich besonders mit den Methoden des distant reading (u.a. Named Entity Recognition oder Stilometrie) und ist im forTEXT-Projekt u.a. für die Dissemination in den (sozialen) Medien zuständig. Bis zu 30 Personen. Teilnehmer*innen bringen ihren eigenen Laptop mit, der mit dem Internet verbunden ist (Achtung: Touch-Devices werden derzeit noch nicht unterstützt). Am Workshop können bis zu 30 Personen teilnehmen. Neben einer stabilen Internetverbindung werden ein Beamer und eine Leinwand benötigt.",de,workshop Möglichkeit entwickelt analyseplattform catma praktisch erkunden keinerlei technisch Vorkenntnisse voraussetzen inhaltlich theoretisch praktisch Aspekt digital Annotation literarisch texten Analyse Visualisierung Text erstellt Annotation widmen Catma Computer Assisted Text Markup And Analysis Catma unterstützen linguistischen Textanalysetools unterscheiden Catma insbesondere zudem bieten Catma Möglichkeit annotiert Text verarbeiten upload Tools erstellt annotation anzeigen analysieren wissenschaftlich nachnutzen lassen Catma automatisch pos deutschsprachig Text annotation generieren Annotation Text gehören Jahrhundert textwissenschaftlich Kernpraktike Moulin genau lassen Freitextkommentar taxonomiebasieren Annotation Textauszeichnung unterscheiden wobei Übergänge häufig fließend Jacke catma Funktion Freitextkommentar bieten taxonomiebasiert Annotation eigentlich Kerngeschäft Tool wobei Taxonomie prinzipiell undogmatisch erstellen Form Tagset annehmen kollaborativ Annotationsprojekt wahlweise beigegeben Bögel et al Workshop Unterschied Annotation Analyse Visualisierung annotationsdan wichtig Standbein Catma grundlegend Funktion per Klick ausführen bieten Catma analysebereich halbautomatisch Annotation erstellen annotieren Wiederkehrende Wörter Wortgruppe bestimmt manuell wiederholt Annotationsmodul Wechsel Arbeit iterativ Prozess hermeneutisch Interpretationsarbeit Literaturwissenschaft widerspiegeln Gius Vorbereitung Workshop Mischung textanalytisch Arbeit catma nähern generell Einführung Tool anhand vorgegeben beispieltext gesamt Workflow individuell Taxonomiebasierte Textannotation Analyse Visualisierung Interpretation annotationsdat Kennenlern praktisch erproben ausgehend digital Text Lage versetzen annotation manuell automatisch unterstützen erstellen Annotation Collection speichern Tagset Taxonomien entwickeln Text alleine Kombination Annotation analysieren visualisieren Diskussion individuell Rückfrage theoretisch praktisch technisch Art Niveau Bezug Projekt ausreichend Möglichkeit bestehen Workshop Arbeitsablauf digital Texterforschung praktisch kennenlernen dr Jan Horstmann Universität Hamburg Institut Germanistik überseering Postfach Hamburg Jan Horstmann Postdoc koordinieren fortext Dissemination digital Routine Ressource Tools klassischer fachwissenschafen Weiterentwicklung catma wesentlich Rolle spielen Literaturwissenschaftler interessieren Perspektive erkenntnispotentialen literarisch artefakt Bereithalt forschen Sinn Entsagung Ironie Goethe Prof dr Jan Christoph Meister Universität Hamburg Institut Germanistik überseering Postfach Hamburg Jan Christoph Meister Professor Digital Humanitie Schwerpunkt Literaturwissenschaft ursprünglich Erfinder Catma etlicher forschungsprojekte Annotation Visualisierung textuell daten Entwicklung Verbesserung leiten Marco Petris dipl Inform Universität Hamburg Institut Germanistik überseering Postfach Hamburg Marco Petris informatiker stark Affinität geisteswissenschaftlich Fragestellung Anfang Entwicklung Catma beteiligen beschäftigen Aspekt Implementierung Mareike Schumacher Universität Hamburg Institut Germanistik überseering Postfach Hamburg Mareike Schumacher promovieren digital Literaturwissenschaftlerin ort narratologisch Ortskategorien literarisch Text beschäftigen Methode distant Reading named entity Recognition Stilometrie Dissemination sozial Medium zuständig Person bringen Laptop Internet verbinden Achtung derzeit unterstützen Workshop Person teilnehmen stabil Internetverbindung Beamer Leinwand benötigen,"[('catma', 0.4328281519415395), ('hamburg', 0.3040451846976219), ('annotation', 0.22735290285792847), ('postfach', 0.21812179940133963), ('überseering', 0.21812179940133963), ('jan', 0.158748556709337), ('germanistik', 0.158748556709337), ('institut', 0.14379067726530417), ('praktisch', 0.14337994635418405), ('workshop', 0.13936756137566275)]"
2019,DHd2019,193_final-USLU_Tolga_text2ddc_meets_Literature___Ein_Verfahren_für_die.xml,text2ddc meets Literature - Ein Verfahren für die Analyse und Visualisierung thematischer Makrostrukturen,"Alexander Mehler (Goethe University of Frankfurt, Deutschland); Tolga Uslu (Goethe University of Frankfurt, Deutschland); Rüdiger Gleim (Goethe University of Frankfurt, Deutschland); Daniel Baumartz (Goethe University of Frankfurt, Deutschland)","topic, visualization, literature","Inhaltsanalyse, Visualisierung, Literatur","In diesem Poster geht es um die thematische Analyse und Visualisierung literarischer Werke mithilfe automatisierter Klassifikationsalgorithmen. Hierfür wird ein bereits entwickelter Algorithmus namens text2ddc (Uslu et. al. 2018) verwendet, um die Themenverteilungen literarischer Werke zu identifizieren. Darüber hinaus thematisiert der Beitrag, wie diese Verteilungen von Themen und deren Abhängigkeiten untereinander visualisiert werden können. Bei text2ddc handelt es sich um einen Klassifikator auf Basis neuronaler Netze, der Texte einer bestimmten Anzahl von Sprachen nach der Dewey-Dezimalklassifikation (DDC) kategorisiert. Die DDC ist ein internationaler Standard für die Themenklassifikation im Bereich von (digitalen) Bibliotheken. Um text2ddc zu trainieren, wurde die Wikipedia verwendet. Da viele Artikel der Wikipedia mit der Gemeinsamen Normdatei (GND) verlinkt sind und die GND Informationen zu den entsprechenden DDC-Kategorien hinterlegt, war es möglich, ein vergleichsweise großes und zugleich breites DDC-orientiertes Trainingskorpora für das Deutsche aufzubauen. Am Beispiel dieses Korpus erreicht unser Algorithmus einen F-Score von 87,4%. Da die Artikel der Wikipedia auch über Sprachgrenzen hinweg untereinander verlinkt sind, war es zudem möglich, text2ddc für über 40 Sprachen zu trainieren. text2ddc wurde auf Korpora verschiedener Genres angewandt, um deren Themenverteilungen zu analysieren. Zum einen betrifft dies die Wikipedia selbst, aber auch Korpora basierend auf StadtWikis, anhand derer bestimmt wurde, welche Themen dominant sind und wie diese zusammenhängen. Ein drittes Beispiel betrifft literarische Texte bzw. historische Texte der Wissenschaft. Abbildung 1 zeigt etwa die Themenverteilung von Der zugrundeliegende Algorithmus basiert auf folgendem Prozedere: Zunächst wird der Inputtext in Sektionen untergliedert, wofür die jeweilige logische Dokumentstruktur ausgewertet wird. Anschließend werden verschiedene NLP-Methoden angewendet, um Informationen über Lemmata und Das Poster untersucht anhand der Werke einer Reihe von deutschsprachigen Autoren (u.a. Karl Marx, Sigmund Freud, Franz Kafka, Friedrich Nietzsche, Thomas Mann und Martin Heidegger) die Möglichkeiten und Grenzen von Themenkarten zur Erfassung makrostruktureller Themenzusammenhänge von Texten, wie sie unser Algorithmus erfasst. Auf diese Weise soll eine Alternative zu den in den DH omnipräsenten",de,Poster thematisch Analyse Visualisierung literarisch werk Mithilfe Automatisierter klassifikationsalgorithmen hierfür entwickelt Algorithmus namens Uslu et verwenden Themenverteilung literarisch Werk identifizieren hinaus thematisieren Beitrag verteilungen Them Abhängigkeit untereinander visualisiern handeln Klassifikator Basis neuronaler Netz Text bestimmt Anzahl Sprache ddc kategorisieren ddc international Standard Themenklassifikation Bereich digital bibliotheken trainieren wikipedia verwenden Artikel wikipedia gemeinsam Normdatei gnd verlinkt Gnd Information entsprechend hinterlegt vergleichsweise breit Trainingskorpora deutsch aufbauen Korpus erreichen Algorithmus Artikel wikipedia sprachgrenz Hinweg untereinander verlinkt zudem Sprache trainieren Korpora verschieden Genr anwenden Themenverteilung analysieren betreffen wikipedia Korpora basierend Stadtwikis anhand Derer bestimmen Thema dominant zusammenhängen betreffen literarisch Text historisch Text Wissenschaft Abbildung zeigen Themenverteilung zugrundeliegend Algorithmus basieren folgend Prozedere inputtext Sektion untergliederen wofür jeweilig logisch Dokumentstruktur auswerten anschließend verschieden anwenden Information Lemmata Poster untersuchen anhand Werk Reihe deutschsprachig Autor Karl Marx Sigmund freud Franz Kafka friedrich nietzschen Thomas Mann Martin Heidegger Möglichkeit Grenze Themenkart Erfassung makrostrukturell Themenzusammenhänge Text Algorithmus erfassen Weise Alternative dh omnipräsent,"[('themenverteilung', 0.3109402565587613), ('wikipedia', 0.30173549571546493), ('algorithmus', 0.2781425377630124), ('ddc', 0.18299225377003686), ('verlinkt', 0.18299225377003686), ('gnd', 0.15086774785773246), ('artikel', 0.1390712688815062), ('untereinander', 0.13438975256509555), ('werk', 0.11760749890574465), ('trainieren', 0.11736819144262496)]"
2019,DHd2019,149_final-KRUG_Markus_Detecting_Character_References_in_Literary_Novel.xml,Detecting Character References in Literary Novels using a Two Stage Contextual Deep Learning approach,"Markus Krug (Chair of Applied Computer Science and Artificial Intelligence, Universität Würzburg, Deutschland); Sebastian Kempf (Chair of Applied Computer Science and Artificial Intelligence, Universität Würzburg, Deutschland); Schmidt David (Chair of Applied Computer Science and Artificial Intelligence, Universität Würzburg, Deutschland); Weimer Lukas (Chair of Literary Computing, Universität Würzburg, Deutschland); Puppe Frank (Chair of Applied Computer Science and Artificial Intelligence, Universität Würzburg, Deutschland)","Character Reference detection, Deep Learning, BiLSTM-CRF","Datenerkennung, Programmierung, Inhaltsanalyse, Daten, Metadaten","In recent years analyzing constellations of fictional entities in literary fiction has seen a lot of interest (Elson et al. 2010, Agarwal et al. 2012). Those constellations are often visualized as networks of entities apparent in the document. Even though the pipelines used for preprocessing vary drastically they all share some common steps. In order to draw a network, one needs to define nodes and edges. An obvious choice for the nodes are the fictional entities. Those entities appear either as pronouns, nominal references or names. The detection of those character references (CR) was used for a multitude of different applications in automatic digital humanities processing, most notably genre detection (Hettinger et al. 2015) and coreference resolution (e.g. Lee et al. 2013). This section only mentions the most dominant work in terms of Named Entity Recognition techniques as well as those with comparable results in terms of domain. The standard approach is to segment the input document into sentences and apply a sequence classifier on these sentences. The most robust classifier applied to this task was a Conditional Random Field (CRF) (Lafferty et al. 2001). Until the success of deep learning approaches, the classifier published by Stanford (Finkel et al. 2005) and its adaptation to German (Faruqui et al. 2010) were the dominant approaches. Lately the combination of a Bi-LSTM with word-embeddings (e.g. Mikolov et al. 2013) which automatically derived features for a CRF classifier in a deep learning approach surpassed manually created features (Huang et al. 2015). Most recently, Riedl and Padó (2018) included pretraining into the Bi-LSTM-CRF architecture and achieved state of the art results. A maximum entropy classifier with cluster features derived by word2vec and manually crafted features yielded an F1-score of 90% (Jannidis et al. 2017), serving as the only comparable work for German literary data. However, all of these approaches classify each sentence and every token inside on their own so that subsequent sentences do not benefit from previously detected names. This does not only overcomplicate the detection, it can also introduce inconsistent results (e.g. ""Effi"" is detected as a name in sentence 10 and 27, but was not detected in sentence 25). Introducing no dependencies between each individual reference seems a wasted opportunity, especially in novels, because the same character reappears multiple times. This paper experimented with network architectures to leverage this shortcoming. The idea of this approach is not new and can even be dated back to the Brills Tagger (Brill 1992) - the classification by two separate classifiers each with its individual perspective on the problem. The corpus DROC (Krug et al. 2018) provides the data used in this paper. It contains about 393.000 tokens from 90 different samples taken from German novels. Each sample comprises at least one chapter. In total, this corpus contains about 53.000 manually annotated character references. 100-dimensional Word2Vec word embeddings trained on 1700 novels of project Gutenberg The method used in this paper follows the intuition that, especially in literary fiction, entities appear many times throughout the text. Because each document introduces its own fictional world, each word (meaning the set of all appearances of a token with the same string) has a dominant meaning. However, not every instance of a word can be easily detected. While some might be surrounded by verbs of communication (""sagen"", ""antworten"", ...), others might only be surrounded by stop words, which are not beneficial for classification. Therefore, this work introduces two passes through the text. The first pass tries to assign the dominant meaning to a word and is assumed to produce a high recall but a mediocre precision. The purpose of the second pass is to disambiguate individual instances which have been classified as a character reference but could have multiple senses. Furthermore, while the first pass might detect ""Effi"" and ""Briest"" as references, there is no information about whether the string ""Effi Briest"" is a single reference or two distinct references. This is solved in the second pass, which is trained for a sequential prediction and is supposed to detect the exact span of a reference. The architectures of both neural networks can be depicted as follows: An instance fed into the first network consists of a list of tuples, each comprising the span of the token, encoded by a word embedding, as well as a left context and a right context. Our previous work determined a context of the previous two and the next two tokens (also encoded by a pre-trained word embedding) as best performing for the determination of character references. The last input vector was derived by a Bi-LSTM character encoding of the target word. The tuples were arranged in order of appearance in the original text and encoded by a Bi-LSTM, feeding an additional tupel at each time step. The Bi-LSTM subsequently generates a condensed representation of those tuples into a vector of 256 units. The intuition is that this vector contains the most informative parts of all contexts for a given target word. The network is trained using log-loss and predicts whether the target word is a reference or not. The network was trained for 15 epoches on 58 documents (longer training did not necessarily result in a better classification accuracy) and applied to a separate set spanning 14 documents. This second set is then used to train the second network, using Bi-LSTM character embeddings with a subsequent Bi-LSTM. However, the network is only applied to tokens that had been classified as a character reference in the first pass. This follows the intuition that it can now be decided if the current instance is of a different semantic category, which can be detected by analyzing its context. The input of this network is the snippet around the target word with a context size of two. The second task of ¬†this network, detecting the exact bounds of a reference, is done by predicting labels in an I-O-B setting. It is noteworthy that words that were not detected in the first pass can not be recovered. The second network is trained with 25 epochs and finally tested on 18 test documents. We compared the architecture described in Section 4 (denoted 2-stage) with the state of the art architecture (Bi-LSTM-CRF) similar to Riedl and Padó (2018). A Bi-LSTM-CRF using character embeddings in Tensorflow Table 1: Results of the two systems applied to DROC. The numbers were derived in a 5-fold scenario and are noted in %. Evaluation is done on token and on entity level using Precision, Recall and F1. Even though the 2-stage approach seems intuitive at first, it can not compete with the results obtained by the state of the art architecture. Surprisingly, the architecture failed to provide a high recall (this is already apparent after the first pass, where the recall is similar to the state of the art system, however no exact borders can be predicted). A possible explanation for this result is the high amount of about 50% of tokens with only a single appearance in the text. Since only two context tokens to the left and the right are used, the architecture has a shortcoming compared to the Bi-LSTM-CRF, which encodes the entire sentence. The architecture does especially fail to recognize references that contain the token ""von"" (such as ""Baron von Instetten)"". While being competitive in terms of the precision, further work has to be done to increase the recall for this approach. This paper presents a 2-stage contextual approach to detect character references using deep learning. The results show that while the precision yields competitive results, the recall is still much lower. Possible approaches for this shortcoming might be changing the loss function - currently a false negative and a false positive yield the same penalty - and combining both models. The state of the art model can then be used for words that only appear a single time in the text and the 2-stage approach for words appearing more than once. This could retain the high quality while still generating a consistent labeling by making use of the dependencies between individual appearances of a word.",en,recent year analyze constellation fictional entity literary fiction see lot interest elson et al agarwal et al constellation visualize network entity apparent document pipeline preprocesse vary drastically share common step order draw network need define node edge obvious choice node fictional entity entity appear pronoun nominal reference name detection character reference cr multitude different application automatic digital humanity processing notably genre detection hettinger et al coreference resolution lee et al section mention dominant work term name entity recognition technique comparable result term domain standard approach segment input document sentence apply sequence classifier sentence robust classifier apply task conditional random field crf lafferty et al success deep learning approach classifier publish stanford finkel et al adaptation german faruqui et al dominant approach lately combination bi lstm word embedding mikolov et al automatically derive feature crf classifier deep learning approach surpass manually create feature huang et al recently riedl padó include pretraine bi lstm crf architecture achieve state art result maximum entropy classifier cluster feature derive manually craft feature yield score jannidis et al serve comparable work german literary datum approach classify sentence token inside subsequent sentence benefit previously detect name overcomplicate detection introduce inconsistent result effi detect sentence detect sentence introduce dependency individual reference wasted opportunity especially novel character reappear multiple time paper experiment network architecture leverage shortcoming idea approach new date brill tagger brill classification separate classifier individual perspective problem corpus droc krug et al provide datum paper contain token different sample take german novel sample comprise chapter total corpus contain manually annotate character reference dimensional word embedding train novel project gutenberg method paper follow intuition especially literary fiction entity appear time text document introduce fictional world word mean set appearance token string dominant meaning instance word easily detect surround verb communication sagen antworten surround stop word beneficial classification work introduce pass text pass try assign dominant meaning word assume produce high recall mediocre precision purpose second pass disambiguate individual instance classify character reference multiple sense furthermore pass detect effi bri reference information string effi briest single reference distinct reference solve second pass train sequential prediction suppose detect exact span reference architecture neural network depict follow instance feed network consist list tuple comprise span token encode word embed left context right context previous work determine context previous token encode pre train word embed well perform determination character reference input vector derive bi lstm character encoding target word tuple arrange order appearance original text encode bi lstm feed additional tupel time step bi lstm subsequently generate condense representation tuple vector unit intuition vector contain informative part context give target word network train log loss predict target word reference network train epoch document long training necessarily result well classification accuracy apply separate set span document second set train second network bi lstm character embedding subsequent bi lstm network apply token classify character reference pass follow intuition decide current instance different semantic category detect analyze context input network snippet target word context size second task network detect exact bound reference predict label o b setting noteworthy word detect pass recover second network train epoch finally test test document compare architecture describe section denote stage state art architecture bi lstm crf similar riedl padó bi lstm crf character embedding tensorflow table result system apply droc number derive fold scenario note evaluation token entity level precision recall stage approach intuitive compete result obtain state art architecture surprisingly architecture fail provide high recall apparent pass recall similar state art system exact border predict possible explanation result high token single appearance text context token left right architecture shortcoming compare bi lstm crf encode entire sentence architecture especially fail recognize reference contain token von baron von instetten competitive term precision work increase recall approach paper present stage contextual approach detect character reference deep learning result precision yield competitive result recall low possible approach shortcoming change loss function currently false negative false positive yield penalty combine model state art model word appear single time text stage approach word appear retain high quality generate consistent labeling make use dependency individual appearance word,"[('bi', 0.2565949199777989), ('reference', 0.2495282805145894), ('lstm', 0.23899871506149373), ('word', 0.23517505004026543), ('network', 0.19962262441167156), ('detect', 0.19643309799908382), ('pass', 0.1734640948969289), ('al', 0.17233512630669068), ('architecture', 0.16807428686862325), ('approach', 0.1677852719221814)]"
2019,DHd2019,239_final-KRAUTTER_Benjamin_Klassifikation_von_Titelfiguren_in_deutsch.xml,"Klassifikation von Titelfiguren in deutschsprachigen Dramen und Evaluation am Beispiel von Lessings ""Emilia Galotti""","Benjamin Krautter (Universität Stuttgart, Deutschland); Janis Pagel (Universität Stuttgart, Deutschland)","Dramen, Klassifikation, maschinelles Lernen, Titelfiguren, Literatur","Inhaltsanalyse, Strukturanalyse, Netzwerkanalyse, Literatur","In seiner Studie zu Gotthold Ephraim Lessings bürgerlichem Trauerspiel Zur Einteilung und Abstufung des Dramenpersonals mithilfe quantitativ erfassbarer Kriterien führt Manfred Pfister in den späten 1970er Jahren die Terminologie der ""quantitative[n] Dominanzrelationen"" (Pfister¬†2001¬†[1977]:¬†226) ein. Er benennt hierfür zwei Kriterien, die Haupt- von Nebenfiguren unterscheiden sollen: die ""Dauer der Bühnenpräsenz einer Figur"" (ebd.) und den Anteil der Figurenrede am Haupttext. Laut Pfister fehle es allerdings an einer differenzierten Handlungsgrammatik, die auch funktionale Relationen 'etwa die aktiven Handlungsschritte von Figuren 'operationalisieren könne (vgl. ebd.:¬†227). Er wirbt letztlich für einen nicht weiter explizierten multidimensionalen Ansatz, der die quantitative Einteilung des Personals zuverlässiger und feingliedriger machen soll. An diese Idee der quantitativen und zugleich multidimensionalen Einteilung dramatischer Figuren versuchen wir im Folgenden mittels digitaler Analysetechniken anzuschließen (vgl.¬†Fischer¬†u.a.¬†2018). Dazu fassen wir das Problem der Figureneinteilung als Klassifikationsaufgabe. Zielsetzung ist es, titelgebende Dramenfiguren mit maschinellen Lernverfahren automatisch auszuzeichnen. Dadurch sind wir in der Lage, die genauen Einflussfaktoren zu prüfen und die Ergebnisse transparent zu evaluieren. Nach unserem Dafürhalten sind es zumindest drei Gründe, die titelgebende Dramenfiguren zu einer geeigneten Zielkategorie der Klassifikation machen. Den möglichen alternativen Konzepten mangelt es erstens an einer konsensfähigen Definition und Differenzierung. Das gilt insbesondere für die Begriffe ""HeldIn"" und ""ProtagonistIn"", die teilweise synonym verwendet werden (vgl. etwa Plett¬†2002:¬†21f., Jannidis¬†2004:¬†90,¬†104f.). Überdies ist gerade das Heldenkonzept stark von literaturgeschichtlichen Entwicklungen geprägt und somit historisch variabel (vgl. etwa Alt¬†1994:¬†167f., Platz-Waury¬†2007¬†[1997]:¬†591, Martus¬†2011:¬†15). Die intersubjektive Annotation der Figurenkategorien bereitet zweitens Schwierigkeiten, vor allem dann, will man ProtagonistInnen oder HeldInnen mit Blick auf ihre Bedeutung für die Handlung bzw. den zentralen Konflikt des Dramas bestimmen. In der aktuellen Forschung folgen mehrere Studien der von Wladimir Propp (1986¬†[1928]) am russischen Volksmärchen eingeführten Figurentypologisierung, um literarische Figuren auf formaler oder automatischer Basis (sub-)klassifizieren zu können (etwa Declerck¬†/¬†Koleva¬†/¬†Krieger¬†2012 oder Finlayson¬†2017). Moretti (2011 und 2013) nutzt indessen Netzwerkdarstellungen von Shakespeares Das untersuchte Korpus umfasst 38 Dramen mit mindestens einer Titelfigur, deren Veröffentlichung sich von der Mitte des 18. bis ins frühe 20. Jahrhundert erstreckt.  Um dem Vorsatz eines multidimensionalen Modells gerecht zu werden, kombinieren wir als Features zählbasierte Metriken ( Die Anhand Lessings Gleich vier Titelfiguren benennt die Klassifikation für Lessings bürgerliches Trauerspiel Warum also wird Emilia trotzdem als Titelfigur erkannt? Die Featureanalyse in Für den Leser ist es dagegen wohl eher Emilias durchgängige passive Präsenz in den Dialogen und Monologen anderer Figuren, die sie als Titelfigur kennzeichnet. Exemplarisch dafür steht bereits der erste Akt. Ausgelöst durch eine Bittschrift verliert sich der Prinz in unruhigen Gedanken an Emilia Galotti: ""Ich kann doch nicht mehr arbeiten. 'Ich war so ruhig, bild"" ich mir ein, so ruhig 'Auf einmal muß eine arme Bruneschi, Emilia heißen: 'weg ist meine Ruhe, und alles! –"" (Lessing 2000 [1772]:¬†293). Mit wechselnden Gesprächspartnern 'Maler Conti, Marinelli und Camillo Rota 'wird Emilia immer wieder zum Mittelpunkt der folgenden Dialoge. Das gilt insbesondere für die sechste Szene, als Marinelli die anstehende Vermählung Emilias mit dem Grafen Appiani preisgibt, woraufhin der eifersüchtige Prinz seinem Kammerherren Marinelli die völlige Handlungsfreiheit in dieser Angelegenheit zugesteht (vgl. ebd.,¬†300–305). Diese passive Präsenz Emilias lässt sich über weite Teile des Dramas nachvollziehen. In 16 Szenen wird in der Figurenrede mit ihrem Namen auf sie referiert, obwohl sie selbst zu diesem Zeitpunkt nicht aktiv am Bühnengeschehen beteiligt ist. Verglichen mit anderen Dramenfiguren ist dieser Wert sehr hoch. Marinellis Name wird beispielsweise nur in fünf Szenen aufgerufen, auf Emilias Mutter Claudia wird ohne ihre Anwesenheit auf der Bühne gar nicht namentlich referiert, wie Wir konnten zeigen, dass unser multidimensionales Modell sinnvolle Ergebnisse für die Klassifikation titelgebender Figuren liefert (MCC¬†0.66). Titelfiguren werden sehr zuverlässig erkannt ( Arnim, L. A. von: Marino Caboga Brentano, C.: Ponce de Leon Büchner, G.: Dantons Tod Büchner, G.: Leonce und Lena Büchner, G.: Woyzeck Goethe, J. W.: Götz von Berlichingen mit der eisernen Hand Goethe, J. W.: Iphigenie auf Tauris Goethe, J. W.: Torquato Tasso Gottsched, J. Ch.: Der sterbende Cato Grabbe, Ch. D.: Don Juan und Faust Grabbe, Ch. D.: Hannibal Grabbe, Ch. D.: Herzog Theodor von Gothland Grabbe, Ch. D.: Napoleon oder Die hundert Tage Gutzkow, K.: Richard Savage, Sohn einer Mutter Gutzkow, K.: Uriel Acosta Hofmannsthal, H. von: Elektra Hofmannsthal, H. von: Ödipus und die Sphinx Kotzebue, A. von: Die beiden Klingsberg Laube, H.: Monaldeschi Laube, H.: Struensee Lessing, G. E.: Emilia Galotti Lessing, G. E.: Miss Sara Sampson Pfeil, J. G. B.: Lucie Woodvil Romantik Iffland, A. W.: Figaro in Deutschland Schiller, F.: Die Jungfrau von Orléans Schiller, F.: Die Piccolomini Schiller, F.: Die Verschwörung des Fiesko zu Genua Schiller, F.: Maria Stuart Schiller, F.: Wallensteins Tod Schiller, F.: Wilhelm Tell Schlaf, J.: Meister Oelze Schlegel, A. W.: Alarkos Schlegel, A. W.: Ion Schlegel, J. E.: Canut Schnitzler, A.: Anatol Schnitzler, A.: Professor Bernhardi Tieck, L.: Der gestiefelte Kater Tieck, L.: Prinz Zerbino Tieck, L.: Ritter Blaubart Uhland, L.: Ludwig der Bayer Wieland, Ch. M.: Klementina von Porretta Wieland, Ch. M.: Lady Johanna Gray",de,Studie Gotthold Ephraim lessings bürgerlich Trauerspiel Einteilung Abstufung dramenpersonals Mithilfe quantitativ erfassbar kriterien führen Manfred Pfister spät Terminologie quantitativ n dominanzrelationer benennen hierfür kriterien nebenfiguren unterscheiden Dauer Bühnenpräsenz Figur Anteil Figurenrede Haupttext laut pfister fehlen differenziert Handlungsgrammatik funktional Relation aktiv Handlungsschritte Figur operationalisieren können werben letztlich expliziert multidimensional Ansatz quantitativ Einteilung Personal zuverlässig feingliedrig Idee quantitativ multidimensional Einteilung dramatisch Figur versuchen folgend mittels Digitaler analysetechniken anzuschließen fassen Problem Figureneinteilung Klassifikationsaufgabe Zielsetzung titelgebenden Dramenfigur maschinell lernverfahren automatisch auszuzeichnen Lage Genau einflussfaktoren prüfen Ergebnis Transparent evaluieren unser Dafürhalten zumindest gründe Titelgebend dramenfiguren geeignet Zielkategorie klassifikation möglich alternativ Konzept mangeln erstens konsensfähig Definition Differenzierung gelten insbesondere begriffe Heldin Protagonistin teilweise synonym verwenden überdies Heldenkonzept stark literaturgeschichtlich Entwicklung prägen somit historisch variabel intersubjektiv Annotation Figurenkategorie bereiten zweitens schwierigkeiten protagonistinnen heldinnen Blick Bedeutung Handlung zentral Konflikt Drama bestimmen aktuell Forschung folgen mehrere Studie Wladimir Propp russisch volksmärchen eingeführt Figurentypologisierung literarisch Figur formal automatisch Basis moretti nutzen indessen netzwerkdarstellungen Shakespeare untersucht korpus umfassen Dram mindestens Titelfigur Veröffentlichung Mitte Frühe Jahrhundert erstrecken Vorsatz multidimensional Modell gerecht kombinieren features zählbasierter metriken anhand Lessings Titelfigure benennen Klassifikation Lessings bürgerlich Trauerspiel Emilia Titelfigur erkennen featureanalyse Leser eher emilias durchgängig passiv Präsenz Dialog Monologe anderer Figur Titelfigur kennzeichnen exemplarisch stehen Akt auslösen Bittschrift verlieren Prinz unruhig Gedanke Emilia Galotti arbeiten ruhig Bild ruhig arm bruneschi Emilia heißen weg Ruhe lessing wechselnd Gesprächspartner Maler Conti Marinelli Camillo rota Emilia Mittelpunkt folgend Dialoge gelten insbesondere Szene marinelli anstehend Vermählung emilias graf Appiani preisgiben woraufhin eifersüchtig Prinz kammerherren marinelli völlig Handlungsfreiheit Angelegenheit zugestehen passiv Präsenz emilias lässt weit Teil Drama nachvollziehen Szene Figurenrede Name referieren obwohl Zeitpunkt aktiv bühnengeschehe beteiligen vergleichen dramenfigur Wert Marinellis Name beispielsweise Szene aufrufen emilias Mutter Claudia Anwesenheit Bühne namentlich referieren zeigen multidimensional Modell sinnvoll Ergebnis Klassifikation titelgebend Figur liefern titelfiguren zuverlässig erkennen Arnim Marino Caboga Brentano Ponce -- Leon Büchner dantons Tod Büchner Leonce Lena Büchner Woyzeck Goethe Götz Berliching eisern Hand Goethe Iphigenie Tauris Goethe Torquato Tasso gottsched ch sterbend Cato Grabbe ch Don juan Faust Grabbe ch hannibal grabbe ch Herzog Theodor Gothland Grabbe ch napoleon hundert Gutzkow Richard Savage Sohn Mutter Gutzkow uriel acosta Hofmannsthal elektra Hofmannsthal ödipus Sphinx Kotzebue Klingsberg lauben Monaldeschi lauben Struensee Lessing Emilia Galotti Lessing Miss Sara Sampson Pfeil Lucie woodvil Romantik Iffland Figaro Deutschland Schiller Jungfrau orléans Schiller Piccolomini Schiller Verschwörung fiesko genua Schiller Maria Stuart Schiller wallensteins Tod Schiller Wilhelm Tell schlaf Meister Oelze Schlegel Alarkos Schlegel ion Schlegel Canut Schnitzler Anatol Schnitzler Professor Bernhardi Tieck Gestiefelte kat Tieck Prinz zerbino Tieck Ritter Blaubart Uhland Ludwig Bayer Wieland ch Klementina Porretta Wieland ch Lady Johanna Gray,"[('ch', 0.32839280918480984), ('schiller', 0.20486018611709564), ('emilia', 0.1795693518862212), ('emilias', 0.17478457470719017), ('grabbe', 0.17478457470719017), ('multidimensional', 0.1478934374560823), ('tieck', 0.13108843103039264), ('marinelli', 0.13108843103039264), ('titelfigur', 0.13108843103039264), ('büchner', 0.13108843103039264)]"
2019,DHd2019,139_final-TU_Ngoc_Duyen_Tanja_Automatic_recognition_of_direct_speech_w.xml,Automatic recognition of direct speech without quotation marks. A rule-based approach,"Ngoc Duyen Tanja Tu (Institut für Deutsche Sprache, Deutschland); Markus Krug (Universität Würzburg, Deutschland); Annelen Brunner (Institut für Deutsche Sprache, Deutschland)","direkte Rede, regelbasiertes Verfahren, automatische Erkennung","Datenerkennung, Programmierung, Annotieren","Many texts incorporate multiple voices: the voice of the narrator and those of the characters or people who are quoted by the narrator. Separating these quotations from the surrounding text is relevant for many applications: In the field of literary studies it is a requirement for studies concerning character representation like sentiment analysis (e.g. Blessing et al. 2016; Schmidt / Burghardt / Dennerlein 2018) or character networks (e.g. Rydberg-Cox 2011; Dimpel 2018). For non-fictional texts, recognizing quotations is relevant for question answering and similar tasks. As those applications rely on processing a lot of textual material, having a way to automatically detect instances of direct speech (DS) is crucial. As long as a specific pattern of quotation marks is used consistently this is a trivial task. Unfortunately, this is not necessarily the case: First, there is an astounding number of ways to encode quotation marks and incorrect or inconsistent usage is very common. Mistakes like missing closing quotation marks happen easily and can throw off a parser relying solely on those markers. (Brunner 2015: 180-182) The problem is worse for older texts, where the typographic rules are even less standardized and additional errors can occur in the digitization process. Finally, in literature it is not uncommon that authors deliberately choose not to use any markers for stylistic reasons. Those types of texts are especially common in the Digital Humanities and it is useful to have a tool that is not dependent on the use of quotation marks. In addition to that, the tool presented in this talk is rule-based and thus requires no training material. The detection of DS is usually a pre-processing step for another task so it rarely gets much focus in the respective papers. There are many applications for English that could be cited here, but we will deliberately focus on applications for German. Pouliquen / Steinberger / Best 2007 develop a tool for automatic quotation detection and speaker attribution in newspaper articles for several languages. They look for the proper name of a public character, followed by a verb associated with speech representation, followed by quotation marks. Due to this strict pattern, their recall is relatively low (76%) but they identified 81.7% correct quotations. The tool GutenTag, developed by Brooke et al. 2015, implements several NLP techniques to use on the texts of Gutenberg corpus. The GutenTag DS recognition relies solely on quotation marks. Before processing the text, the tool checks which types of quotation marks (single or double quotes) are used in it. In her study about the automatic recognition of speech representation in German literary texts, Brunner 2015 implements two strategies for DS detection: Her rule-based approach uses quotation marks as well as pattern matching to identify frames (proper name - speech verb - colon/comma). This leads to some success in texts with unmarked DS. On a corpus of 13 German narrative texts an f-score of 0.84 for the category Jannidis et al. 2018 implemented a recognizer for DS based on deep learning, specifically trained to work without quotation marks. It is trained on a corpus of 300 German fictional texts in which the quotation marks were removed. This recognizer achieves an accuracy of 0.84 in sentence-wise evaluation and 0.90 in token-wise evaluation on the corpus that is called ""Gutenberg"" in our evaluation. To our knowledge, there is at the moment no recognizer for DS in German texts that is rule-based and does not use quotation marks. We evaluated our algorithm on four different and distinct data sets. 1) Gutenberg 2) DROC_red 3) RW 3a) RW_fict: 222 fictional text samples 3b) RW_nonfict: 206 non-fictional text samples The algorithm tries to detect whether a given token or a given sentence is part of a DS. Currently, it cannot reliably determine the exact borders of individual DS instances. The technique is purely rule-based and does not rely on machine learning or training data of any sort. The following pre-processing steps were performed: a) tokenization with the OpenNLP Tokenizer The algorithm tries to solve the problem in the following steps:  If meaningful paragraphs are present in the text, those paragraphs usually tend to represent either narrative sections or dialogue sections. As it is unlikely that a narrative section contains DS at all, this knowledge can be used to adapt the recognizer rules. However if no such paragraphs are available, the algorithm starts by reconstructing surrogate sections, using the concept of ""coherence"" between narrative perspective and tempus. It is determined whether the sentence contains first/second or third person pronouns and which tense is used. If consecutive sentences agree on both, they belong to the same segment. If the narrative perspective is in third person and the dominant tense is past, the segment is categorized as ""narrative"". Inside those sections a penalty is introduced, so that more than a single weak indicator has to be found to assume DS. The resulting sentences are the final result for an evaluation based on sentence borders.  For evaluation, two performance metrics are applied: 1) Sentence level accuracy ‚Üí a true positive is achieved by correctly predicting whether DS is contained in the sentence 2) Token level accuracy ‚Üí each token is evaluated individually. The results on the different corpora are depicted in the table below. We report micro accuracy values to not favor documents which are much shorter than others. Both accuracy metrics show very similar results on three corpora. An interesting fact is that the corpus which was used to create the rules (DROC_red) shows the worst results of all three fictional datasets. Gutenberg contains rather schematic narratives which appear to be easier compared to the other data sets. A more fine grained analysis shows that the best document for DROC_red yields a token level accuracy of 99.7% and the worst document an accuracy of 21.5%. The largest gap can be found in the RW_fict dataset with 0% accuracy for the worst and 100% accuracy for the best document. This shows that while in average the results are promising, there are still phenomena that need to be addressed separately. For the only non-fictional corpus, RW_nonfict, the scores drop by a large margin. This is because the algorithm finds anchors in sections without DS and propagates those incorrectly to the surrounding section. Those incorrect detected anchors are sentence written in first person or rhetorical questions, which are mistaken as DS. This resulted in about 1800 false positives while only 89 sentences of DS were not detected. We proposed a rule-based approach to detect DS without the help of any quotation markers. Our approach creates coherent sections which segment the documents. Specialized rules detect DS on the sentence level inside those segments. The annotation is then expanded from those anchor sentences. Post-processing removes frame sub-sentences to get an exact span for the utterance. Our evaluation shows that the results appear stable throughout different datasets in the fictional domain and are comparable to the results achieved in related work. The tool even achieves a higher score compared to Jannidis et al. 2018 on the sentence level. The current algorithm still has issues with non-fictional texts and some types of fictional texts (especially romantic letters and reports written in first person singular) which suggests that it should be extended to detect the type of document in advance in order to classify in a more robust approach across different domains.",en,text incorporate multiple voice voice narrator character people quote narrator separate quotation surround text relevant application field literary study requirement study concern character representation like sentiment analysis blessing et al schmidt burghardt dennerlein character network rydberg cox dimpel non fictional text recognize quotation relevant question answer similar task application rely process lot textual material have way automatically detect instance direct speech ds crucial long specific pattern quotation mark consistently trivial task unfortunately necessarily case astounding number way encode quotation mark incorrect inconsistent usage common mistake like miss closing quotation mark happen easily throw parser rely solely marker brunner problem bad old text typographic rule standardized additional error occur digitization process finally literature uncommon author deliberately choose use marker stylistic reason type text especially common digital humanity useful tool dependent use quotation mark addition tool present talk rule base require training material detection ds usually pre processing step task rarely get focus respective paper application english cite deliberately focus application german pouliquen steinberger good develop tool automatic quotation detection speaker attribution newspaper article language look proper public character follow verb associate speech representation follow quotation mark strict pattern recall relatively low identify correct quotation tool gutentag develop brooke et al implement nlp technique use text gutenberg corpus gutentag ds recognition rely solely quotation mark process text tool check type quotation mark single double quote study automatic recognition speech representation german literary text brunner implement strategy ds detection rule base approach use quotation mark pattern match identify frame proper speech verb colon comma lead success text unmarked ds corpus german narrative text f score category jannidis et al implement recognizer ds base deep learning specifically train work quotation mark train corpus german fictional text quotation mark remove recognizer achieve accuracy sentence wise evaluation token wise evaluation corpus call gutenberg evaluation knowledge moment recognizer ds german text rule base use quotation mark evaluate algorithm different distinct data set gutenberg rw fictional text sample non fictional text sample algorithm try detect give token give sentence ds currently reliably determine exact border individual ds instance technique purely rule base rely machine learning training datum sort follow pre processing step perform tokenization opennlp tokenizer algorithm try solve problem following step meaningful paragraph present text paragraph usually tend represent narrative section dialogue section unlikely narrative section contain ds knowledge adapt recognizer rule paragraph available algorithm start reconstruct surrogate section concept coherence narrative perspective tempus determined sentence contain second person pronoun tense consecutive sentence agree belong segment narrative perspective person dominant tense past segment categorize narrative inside section penalty introduce single weak indicator find assume ds result sentence final result evaluation base sentence border evaluation performance metric apply sentence level accuracy üí true positive achieve correctly predict ds contain sentence token level accuracy üí token evaluate individually result different corpora depict table report micro accuracy value favor document short accuracy metric similar result corpus interesting fact corpus create rule show bad result fictional dataset gutenberg contain schematic narrative appear easy compare data set fine grain analysis show good document yield token level accuracy bad document accuracy large gap find dataset accuracy bad accuracy good document show average result promise phenomenon need address separately non fictional corpus score drop large margin algorithm find anchor section ds propagate incorrectly surround section incorrect detect anchor sentence write person rhetorical question mistaken ds result false positive sentence ds detect propose rule base approach detect ds help quotation marker approach create coherent section segment document specialized rule detect ds sentence level inside segment annotation expand anchor sentence post processing remove frame sub sentence exact span utterance evaluation show result appear stable different dataset fictional domain comparable result achieve related work tool achieve high score compare jannidis et al sentence level current algorithm issue non fictional text type fictional text especially romantic letter report write person singular suggest extend detect type document advance order classify robust approach different domain,"[('ds', 0.42913885958275466), ('quotation', 0.3761979425445952), ('sentence', 0.2572098942320104), ('mark', 0.2125730572063241), ('fictional', 0.20055728569855694), ('rule', 0.16939764915007455), ('accuracy', 0.16168696602162175), ('section', 0.14697708241829163), ('result', 0.14068626471867948), ('detect', 0.1352737636767517)]"
2019,DHd2019,150_final-FISCHER_Frank_Programmable_Corpora___Die_digitale_Literaturw.xml,Programmable Corpora 'Die digitale Literaturwissenschaft zwischen Forschung und Infrastruktur am Beispiel von DraCor,"Frank Fischer (Higher School of Economics, Moskau, Russland); Ingo Börner (Universität Wien); Mathias Göbel (WU Wien); Angelika Hechtl (Niedersächsische Staats- und Universitätsbibliothek Göttingen); Christopher Kittel (Karl-Franzens-Universität Graz); Carsten Milling (Berlin); Peer Trilcke (Universität Potsdam)","Literaturwissenschaft, Infrastruktur, Dramenforschung","Infrastruktur, Literatur","Obwohl sich infrastrukturell einiges getan hat, sieht ein typischer Operationsmodus der digitalen Literaturwissenschaft immer noch so aus, dass eine bestimmte Forschungsmethode auf ein oft nur ephemeres Korpus angewandt wird. Im besten Fall ist das Ergebnis Doch seit kurzem gibt es Anzeichen, dass sich dies ändert. Einige Digital-Humanities-Projekte stellen Schnittstellen zu stabilen Korpora zur Verfügung, über die man mannigfaltige Zugriffsmöglichkeiten bekommt und reproduzierbar arbeiten kann. Eines dieser Projekte ist DraCor, eine offene Plattform zur Dramenforschung, die in diesem Vortrag vorgestellt werden soll (zugänglich unter Öhnlich wie die COST Action zu europäischen Romanen (Schöch et al. 2018), versucht das DraCor-Projekt als Basis für eine digitale Komparatistik einen Stamm an multilingualen Dramenkorpora aufzubauen, die in basalem TEI kodiert sind. Ein selbst betriebenes russischsprachiges  ( Die Vorteile von frei auf GitHub gehosteten Korpora liegen auf der Hand. Unabhängig von den letztlich durch die Plattform zur Verfügung gestellten Schnittstellen können die Korpora alternativ durch Klonen oder andere Downloadmethoden, etwa über den SVN-Wrapper von GitHub, direkt bezogen und individuell weiterverarbeitet werden. Ein offen zugängliches GitHub-Repositorium heißt auch, dass Pull Requests zur Fehlerkorrektur und Forks für Erweiterungen möglich und erwünscht sind. DraCor als Plattform setzt auf die eXist-Datenbank, um die TEI-Dateien zu verarbeiten und Funktionen zur Beforschung der Korpora zur Verfügung zu stellen. Das Frontend wurde mit ReactJS gebaut, ist responsiv und einfach erweiterbar. Der Schwerpunkt liegt aber nicht auf der GUI, sondern auf der API (vgl. generell zur Unterscheidung zwischen beiden Schnittstellenansätzen Bleier/Klug 2018). Um dem Ideal und der Möglichkeit nahe zu kommen, auf einfache Weise ""alle Methoden auf alle Texte"" anwenden zu können (Frank/Ivanovic 2018), braucht es mehr als offene Korpora. Der zitierte Text von Frank/Ivanovic macht sich hinsichtlich dessen für SPARQL-Endpunkte stark; auch DraCor bietet einen solchen an, besitzt darüber hinaus aber eine reiche API, die über Swagger dokumentiert und erläutert wird  ( Ein einfaches Use-Case-Szenario sieht dann so aus, dass man etwa im RStudio mit zwei, drei Zeilen Code einen Blick in ein Korpus werfen kann, etwa über die zeitliche Entwicklung der Anzahl der Charaktere im russischen Drama zwischen 1740 und 1940, die in der Metadatentabelle festgehalten sind  (  Die Möglichkeiten beschränken sich aber nicht darauf, vorgefertigte API-Funktionen zu benutzen. Neue Forschungsideen zeitigen immer auch neue Bedarfe an einfach bezieh- und reproduzierbaren Daten und Metriken; die API kann dementsprechend erweitert werden. Dies wird dadurch erleichtert, dass über Apache Ant die gesamte Entwicklungsumgebung auf dem eigenen System nachgebaut werden kann. Durch bereits implementierte Funktionen können neben Struktur- und Metadaten etwa auch Volltexte ohne Markup bezogen werden (auch Untermengen von Volltexten wie Regieanweisungen), etwa wenn Methoden wie die Stilometrie oder das Topic Modeling der Endzweck sind, also Methoden, die nach dem ""bag of words""-Prinzip arbeiten, für das kein Markup vonnöten ist. Insgesamt wird durch den Aufbau und die Dokumentation offener APIs die bisher oft aufwendige Reproduzierbarkeit von Forschungsergebnissen erheblich erleichtert. Ein Beispiel für die vielseitigen Nutzungsmöglichkeiten der DraCor-API ist die Shiny App, die Ivan Pozdniakov aufgesetzt hat  ( Das Markup oder andere Formalisierungen literarischer Texte sind nicht selbsterklärend. Zwar gibt es einige Standards, aber die jeweilige Operationalisierungslösung hängt von der Forschungsfrage ab. Allein das Extrahieren von Figurennetzwerkdaten ist auf viele Arten und Weisen möglich, was dazu führt, dass etwa alle von verschiedenen Forschungsgruppen extrahierten Netzwerke aus Shakespeares ""Hamlet"" zu leicht verschiedenen Ergebnissen kommen. Selbst für Dramen ist dies also schon ein nicht-trivialer Akt, von Romanen dann ganz zu schweigen (beispielhaft seien Grayson et al. 2016 genannt, die verschiedene Extraktionsmethoden für Romane durchtesten und die Ergebnisse vergleichen). Um diese Erkenntnis schon in der Lehre zu fördern, wurde das Tool ""Easy Linavis""  ( Neben einem Ansatz zur Gamifizierung des TEI-Korrekturvorgangs (Göbel/Meiners 2016) haben wir für Lehrzwecke auch ein Dramenquartett entwickelt, um spielerisch das Verständnis von Netzwerkwerten zu trainieren (Fischer at al. 2018). Die aufgezählten, um die Plattform herumgruppierten didaktischen Mittel sind integraler Bestandteil des ganzen Projekts, da sie auf dessen Daten und Operationalisierungen aufsetzen. Wichtig dabei war die Erkenntnis, dass Daten mehrere Gestalten annehmen und für Forschung und Lehre gleichermaßen von Bedeutung sein können. Im TEI-Code sind PND- bzw. Wikidata-Identifier sowohl für Autor*innen als auch für die Werke hinterlegt. Auf diese Weise lassen sich verschiedene Realien, die außerhalb der eigenen Korpusarbeit liegen, hinzufügen. Eine automatisch erstellte Autor*innengalerie hat dabei noch eher illustrativen Charakter (de la Iglesia/Fischer 2016). Darüber hinaus kann man aber zum Beispiel feststellen, ob es nicht einen unbewussten regionalen Bias im Korpus gibt. Dafür lässt man sich über die Wikidata-Identifier die Verteilung der Geburts- und Sterbeorte der Autor*innen auf einer Karte anzeigen. So konnte dann für das deutschsprachige Korpus GerDraCor ausgeschlossen werden, dass es einen solchen Bias gibt, da sich die Orte relativ gleichmäßig über die (historisch) deutschsprachigen Gebiete verteilen (Göbel/Fischer 2015). Ebenso lässt sich über die Wikidata-ID der Stücke herausfinden, wo diese uraufgeführt worden sind (Beispiel-Query: Projekte wie DraCor versuchen nichts anderes als den digitalen Literaturwissenschaften eine verlässliche und ausbaufähige Infrastruktur zu geben, damit sie sich stärker auf eigentliche Forschungsfragen konzentrieren und reproduzierbare Ergebnisse hervorbringen können. Eine wichtige Folgerung für uns war, dass wir die Weiterentwicklung unserer seit vier Jahren entwickelten all-in-one Python-Skriptsammlung In Anlehnung an das Projekt ""ProgrammableWeb"" 'das eine Datenbank von offenen APIs unterhält und dessen Slogan lautet: ""APIs, Mashups and the Web as Platform"" (zugänglich unter Programmable Corpora erleichtern es, Forschungsfragen auf viele Arten und Weisen um Korpora herum programmieren zu können. Es steht zu erwarten, dass sich infrastrukturelle Anstrengungen dieser Art für die gesamte Community auszahlen mit Effekten, wie sie John Womersley in seiner Präsentation auf der ICRI2018 in Wien aufgezählt hat: a) dramatically increase scientific reach; b) address research questions of long duration requiring pooled effort; c) promote collaboration, interdisciplinarity, interaction. Der Anschlussmöglichkeiten sind viele, egal ob man gar nicht programmieren möchte, sondern nur eine GEXF-Datei für Gephi benötigt, ob ein Korpus über seine Verbindungen zur Linked Open Data Cloud beforscht oder einfach aus R oder Python heraus bestimmte Daten bezogen werden sollen, ohne dass man sich mit dem Korpus und dessen Maintenance und Reproduzierbarkeit selbst kümmern muss (all dies bleibt natürlich aber eine Option). Programmable Corpora erleichtern die Entscheidung, auf welcher Ebene der eigene Forschungsprozess einsetzt.",de,obwohl infrastrukturell tun sehen typisch Operationsmodus digital Literaturwissenschaft bestimmt Forschungsmethode ephemer Korpus anwenden Fall Ergebnis kurz Anzeichen ändern stellen Schnittstelle stabil Korpora Verfügung mannigfaltig zugriffsmöglichkeit bekommen reproduzierbar arbeiten Projekt Dracor offen Plattform Dramenforschung Vortrag vorstellen zugänglich öhnlich cost Action europäisch Roman schöch et versuchen Basis digital Komparatistik Stamm multilingual Dramenkorpora aufbauen Basalem tei kodieren betrieben russischsprachig Vorteil frei Github gehostet Korpora liegen Hand unabhängig letztlich Plattform Verfügung gestellt Schnittstelle Korpora alternativ klon downloadmethoden Github direkt bezog individuell weiterverarbeiten zugänglich pull Requests Fehlerkorrektur Fork Erweiterunge erwünscht dracor Plattform setzen verarbeiten Funktion Beforschung Korpora Verfügung stellen Frontend Reactjs bauen responsiv einfach erweiterbar Schwerpunkt liegen Gui Api generell Unterscheidung schnittstellenansätz Bleier klug Ideal Möglichkeit nahe einfach Weise Methode Text anwende Frank Ivanovic brauchen offen Korpora zitierte Text Frank Ivanovic hinsichtlich stark dracor bieten besitzen hinaus reich Api swagg dokumentieren erläutern einfach sehen Rstudio zeil Code Blick Korpus werfen zeitlich Entwicklung Anzahl Charaktere russisch Drama Metadatentabelle festhalten Möglichkeit beschränken vorgefertigen benutzen Forschungside zeitigen Bedarfe einfach reproduzierbar daten metriken Api erweitern erleichtern apach ant gesamt Entwicklungsumgebung System nachbauen implementiert Funktion Metadat volltext Markup beziehen untermengen volltexte Regieanweisung Methode Stilometrie Topic Modeling Endzweck Methode bag of arbeiten Markup vonnöten insgesamt Aufbau Dokumentation Offener apis aufwendig Reproduzierbarkeit Forschungsergebnisse erheblich erleichtern vielseitig Nutzungsmöglichkeit Shiny app Ivan Pozdniakov aufsetzen Markup formalisierung literarisch Text selbsterklärend Standard jeweilig Operationalisierungslösung hängen Forschungsfrage extrahieren Figurennetzwerkdat Art weise führen verschieden forschungsgruppen extrahiert netzwerke Shakespeare Hamlet verschieden Ergebnis dramen Akt romanen schweigen Beispielhaft Grayson et nennen verschieden extraktionsmethoden roman durchtesten Ergebnis vergleichen Erkenntnis Lehre fördern tool easy linavis Ansatz Gamifizierung Göbel Meiners Lehrzwecke Dramenquartett entwickeln spielerisch Verständnis netzwerkwerten Trainier Fischer at Aufgezählt Plattform herumgruppiert didaktisch integral Bestandteil Projekt daten Operationalisierung aufsetzen wichtig Erkenntnis daten mehrere Gestalt annehmen Forschung Lehre gleichermaßen Bedeutung sowohl Werk hinterlegt Weise lassen verschieden Realie außerhalb Korpusarbeit liegen hinzufügen automatisch erstellt eher illustrativ Charakter -- -- iglesia Fischer hinaus feststellen unbewusst regional Bia Korpus lässen Verteilung Sterbeort Karte anzeigen deutschsprachig Korpus Gerdracor ausschließen bia Ort relativ gleichmäßig historisch deutschsprachigen Gebiet verteilen göbel Fischer lässen Stück herausfinden uraufgeführt Projekt dracor versuchen anderer digital Literaturwissenschaften verlässlich ausbaufähig Infrastruktur geben stark eigentlich forschungsfrag konzentrieren reproduzierbar Ergebnis hervorbringen wichtig Folgerung Weiterentwicklung entwickeln Anlehnung Projekt Programmableweb datenbank offen apis unterhalten Slogan lauten apis Mashups and The web as Platform zugänglich programmabel Corpora erleichtern forschungsfragen Art Weise Korpora herum programmieren stehen erwarten infrastrukturelle Anstrengung Art gesamt Community auszahlen Effekt John Womersley Präsentation Wien aufzählen dramatically Increase Scientific Reach b address research questions of Long Duration requiring Pooled effort c promot Collaboration Interdisciplinarity Interaction Anschlussmöglichkeit egal programmieren gephi benötigen Korpus Verbindung Linked Open Data Cloud Beforscht einfach r Python heraus bestimmt daten beziehen Korpus Maintenance Reproduzierbarkeit kümmern all bleiben Option Programmable Corpora erleichtern Entscheidung Ebene Forschungsprozess einsetzen,"[('dracor', 0.18085547873761984), ('erleichtern', 0.16331344398904632), ('reproduzierbar', 0.1603050233923013), ('korpora', 0.15234233458618016), ('plattform', 0.14419989542156753), ('apis', 0.1401286758392612), ('api', 0.12832626911921569), ('einfach', 0.1278587730299208), ('korpus', 0.10935495309153165), ('göbel', 0.10687001559486752)]"
2019,DHd2019,212_final-DUNST_Alexander_Multimodale_Stilometrie__Herausforderungen_u.xml,Multimodale Stilometrie: Herausforderungen und Potenzial kombinatorischer Bild- und Textanalysen am Beispiel Comics,"Alexander Dunst (Universität Paderborn, Deutschland); Rita Hartel (Universität Paderborn, Deutschland)","Multimodal, Comics, Stilometrie","Annotieren, Stilistische Analyse, Bilder, Literatur, Multimedia, Multimodale Kommunikation","Stilometrische Untersuchungen blicken auf eine lange Tradition in der Literaturwissenschaft zurück (Holmes). Im Gegensatz dazu befinden sich quantitative Untersuchungen des Stils visueller Kunst und multimodaler Medien in einem frühen Experimentierstadium, das von explorativen Untersuchungen, Methodenentwicklung und -Adaption geprägt ist. Auch hier sind Fortschritte erkennbar, etwa in der digitalen Kunstgeschichte, der Filmwissenschaft und in der Comicforschung (Manovich, Douglas & Zepel; Qui, Taeb & Hughes; Baxter, Khitrova & Tsivian; Cutting et al.; Dunst & Hartel 2018a). Dabei konzentriert sich die stilistische Klassifikation bisher entweder auf visuelle oder sprachliche Kanäle. Beispielhaft zu nennen sind die Analyse der historischen Entwicklung von Filmfarben bei Barbara Flückiger oder Ben Schmidts thematische Untersuchungen populärer TV-Serien (Flückiger; Schmidt). Während dieser monomodale Fokus bei visueller Kunst der oftmaligen Dominanz der Bildebene geschuldet ist, so sind dafür bei multimodalen Medien wie Film, Fernsehen, Computerspielen oder Comics andere Gründe ausschlaggebend. Mit dem Topic Modeling oder der Textstilometrie stehen Methoden zur Verfügung, die auf sprachlichen Daten basieren und in der digitalen Literaturwissenschaft laufend weiterentwickelt werden. Visuelle Stilometrie, obwohl weit weniger ausgereift, kann auf die erwähnten Arbeiten aus der Kunstgeschichte und empirischen Filmwissenschaft zurückgreifen. Auch technische Hürden tragen zu monomodalen Analysen bei: je nach Medium ist die digitale Erschließung und Analyse von Informationskanälen mit erheblichen Schwierigkeiten verbunden, etwa die Spracherkennung von Filmdialogen, die automatische Erkennung von handschriftlichen Texten in Comics oder die computergestützte Verarbeitung großer Mengen an Bildmaterial. Die Kombination unterschiedlicher Informationskanäle in visuellen Medien führt jedoch unweigerlich zur Frage, inwieweit Stil durch die Analyse einzelner Modi erfasst werden kann. Auch thematische Untersuchungen setzen sich dem Vorwurf aus, komplexe Medien auf zu schmaler Datenbasis zu interpretieren, wenn Filmanalysen alleine auf Untertiteln oder Drehbüchern basieren. Im Gegenzug verbindet sich mit der Einbeziehung mehrerer Informationskanäle in stilometretische Untersuchungen die Hoffnung, multimodale Medien vollständiger beschreiben und einzelne Autoren, Genres oder Epochen genauer voneinander unterscheiden zu können. Im Folgenden werden erste Untersuchungen vorgestellt, die auf Basis eines Corpus an englischsprachigen Comicbüchern 'so genannten ""Graphic Novels"" 'visuelle und Textstilometrie kombinieren (Dunst, Hartel & Laubrock). Wie bereits dokumentiert, haben wir auf Basis kunsthistorischer und filmwissenschaftlicher Vorarbeiten eine visuelle Stilometrie für Comicbücher entwickelt, die zwischen einzelnen Genres, Autoren und Publikationsformen unterscheiden und diese auf repräsentativer Datenbasis stilistisch beschreiben kann (Dunst & Hartel 2018b). Die relativ geringe Datenbasis, die mit der Analyse eines kulturellen Nischenproduktes einhergeht, bedeutete jedoch, dass nicht in allen Fällen signifikante Ergebnisse erzielt werden konnten. Insbesondere die Entwicklung einzelner Gattungen innerhalb eines Mediums lässt sich im historischen Verlauf nicht signifikant belegen. Auch nationale Traditionen konnten nicht immer stilistisch voneinander unterschieden werden, selbst wo sich diese für den Betrachter deutlich voneinander unterscheiden. Abbildung 1 zeigt, dass die von uns verwendeten visuellen Maße den japanischen Manga-Autor Osamu Tezuka stilistisch nicht von anglo-amerikanischen Werken abgrenzen konnten (Dunst & Hartel 2018a). In beiden Fällen 'also sowohl bei Autor- als auch bei Genreunterscheidungen 'liegt es nahe, dass die kombinatorische Analyse von visueller und Textstilistik die Wahrscheinlichkeit erfolgreicher Unterscheidungen erhöhen würde. Allerdings stellen diese zusätzlichen Dimensionen eine multimodale Stilometrie vor methodische Herausforderungen. Wird eine große Zahl an Maßen für die Klassifikation herangezogen, so erschwert dies die qualitative Interpretation der Ergebnisse. Zwar lässt sich mit Hilfe einer Principal Component Analysis (PCA) darstellen, ob sich Genres oder Autoren signifikant unterscheiden. Je mehr Dimensionen in die PCA einfließen, desto schwerer fällt allerdings die Aussage, auf welchen Maßen diese Unterschiede fußen. Hinzu kommen wie erwähnt technische Hürden: abgesehen von den Angeboten bekannter Softwaregiganten führen automatische Spracherkennungssysteme noch zu relativ schlechten Resultaten. Öhnlich liegt der Fall bei Comics, deren Texte auf Handschriften basieren. Erst seit kurzem können diese Texte mit Hilfe automatischer Erkennungssysteme, die auf ""Deep Learning"" basieren, zugänglich gemacht werden. Dennoch liegen Fehlerraten weit über jenen, die die Basis der meisten literaturwissenschaftlichen Analysen bilden. Der nächste Abschnitt stellt eine Methode vor, die dennoch die Verwendung einfacher Textmaße für die multimodale Stilometrie ermöglicht. Die Analysen basieren auf dem ersten repräsentativen Corpus englischsprachiger Comicbücher, von uns ""Graphic Narrative Corpus"" (GNC) genannt (Dunst, Hartel & Laubrock). Wie in früheren Arbeiten beschrieben (Hartel & Dunst), nutzen wir die Bag Error Rate (BER) für eine Abschätzung, ob die Qualität der erkannten Texte ausreichend gut ist, um diese für die Textanalysen heranzuziehen. Hierzu haben wir als Gold Standard rund 10% der Seiten einer Graphic Novel manuell annotiert. Wir betrachten in unseren Analysen die Multi-Menge (oft als ""Bag"" bezeichnet) aller Wörter, also die ungeordnete Menge aller Wörter, wobei Wörter 'im Gegensatz zur herkömmlichen Menge 'in dieser Multimenge auch mehrfach vorkommen. Wir berechnen also für jedes in einem der beiden Texte enthaltenen Wörter die Differenzen der Häufigkeiten freq BER := (Œ£ Frühere Analysen haben gezeigt, dass wir den Text als geeignet für die Analyse erachten können, wenn für eine Graphic Novel die BER kleiner als 40 ist. Auf den erkannten Texten betrachten wir die Textähnlichkeit basierend auf einer euklidischen Vektordistanz der Dokument-Vektoren der Term-Dokument-Matrix, die jeweils die für jedes Dokument D die relative Vorkommenshäufigkeit tf(D,t) der 2000 häufigsten Wörter t enthält. Bzgl. der visuellen Maße betrachten wir die mittlere Helligkeit jeder Seite, die Entropie und die Anzahl der Flächen als Maß für die visuelle Unruhe eines Bildes, sowie den Color Layout Descriptor und den Edge Histogram Descriptor des Standards MPEG7 (Mart√≠nez, Koenen & Pereira). Diese haben sich in früheren Arbeiten als vielversprechende Maße herausgestellt (Dunst & Hartel 2018a). Wann immer eine Dimensionsreduktion notwendig ist, führen wir diese mithilfe einer PCA durch. Um z.B. die textuellen und visuellen Maße kombiniert zu betrachten, haben wir 'um einem Ungleichgewicht der 2000 Dimensionen für die 2000 häufigsten Wörter im Vergleich zu den ca. 40 visuellen Maßen entgegenzuwirken - zunächst via PCA die textuellen Dimensionen auf 40 reduziert. Anschließend haben wir die Dimensionen der textuellen PCA und die Dimensionen der visuellen Maße mit Hilfe einer weiteren PCA kombiniert. Zur Untersuchung signifikanter Zusammenhänge nutzen wir die ANOVA (ANalysis Of VAriance), die untersucht, ob die Varianz zwischen den Kategorien größer ist als die Varianz innerhalb der Kategorien. Abbildung 2 stellt die Ergebnisse der multimodalen Stilometrie im Vergleich mit rein visuellen oder Textmaßen dar. Dabei zeigt sich, dass die Kombination von Bild- und Textanalyse nicht immer zum Erfolg führt. Zwar ergeben sich aus der Analyse beider Informationskanäle statistisch deutlichere Signifikanzen bei der Autor-Identifikation und Genreunterscheidung. Der Effekt ist allerdings gering. Im Fall der Klassifikation nach Originalsprache des Werks sowie unterschiedlicher Publikationsformen 'etwa als Einzelband oder als fortgesetzte Serie'führt die Hinzunahme der Textanalyse derzeit nicht zu signifikanten Ergebnissen. Bei der Analyse unterschiedlicher Formen von Autorschaft (Einzelautor*innen, Zusammenarbeit von einer Autor*in und einer Illustrator*in und größeren Autor*innen-Teams) liegt das Resultat der Textanalyse weit über der Signifikanzgrenze. Die deutlich signifikanten Ergebnisse der visuellen Stilometrie setzen sich allerdings auch in der Kombination beider Kanäle durch. Insgesamt lässt sich sagen, dass trotz der gleichen Anzahl der verwendeten visuellen und Textmaße erstere derzeit aussagekräftiger erscheinen. Weiter erscheint es sinnvoll, die Analyse beider Informationskanäle immer auch einzeln zu betrachten und diese nicht immer sofort zu kombinieren. Wie bereits kurz erwähnt, zeigt sich für eine Klassifikation der entscheidende Einfluss der verwendeten Textmaße. Abbildung 3 stellt alle von uns untersuchten Werke in Streudiagrammen dar und unterscheiden diese farblich nach Originalsprache. Im Fall japanischer Manga handelt es sich hier außerdem um eine eigenständige Nationaltradition. Obwohl uns japanische und französischsprachige Werke in englischer √úbersetzung vorliegen, führt nur die Textanalyse zu einer klaren Unterscheidung. Dies steht im klaren Gegensatz zu den Ergebnissen in Abbildung 1. Zwei potenzielle Ursachen können für dieses, auf den ersten Blick kontraintuitive, Ergebnis angeführt werden. Erstens erscheint es möglich, dass diese Unterscheidung eine Folge des √úbersetzungsprozesses sind. Wahrscheinlicher ist, dass sich typische Merkmale der Texte von Manga auch in √úbersetzung erhalten 'in diesem Fall die Frequenz einzelner Wörter. Wir haben erste Untersuchungen vorgestellt, die am Beispiel von Comicbüchern visuelle und Textmaße für eine multimodale Stilometrie kombinieren. Dabei handelt es sich, insbesondere in letzterem Fall, um sehr einfache Maße, die dem derzeitigen Stand der automatischen Texterkennung für Comicschriften geschuldet sind, und insgesamt um erste Pilotversuche. Wie sich zeigte, führt die Kombination der Text- und Bildebene in der Analyse bisher nicht immer zu besseren Ergebnissen. Allerdings ist dies trotz der geringen Anzahl der untersuchten Werke sowohl bei der Gattungsunterscheidung als auch bei der Autoridentifikation der Fall, für die in der Literaturwissenschaft seit längerem ähnliche Maße herangezogen werden. Insgesamt erscheint es sinnvoll, vor einer Kombination die Analyse der visuellen und textlichen Informationskanäle immer auch einzeln zu betrachten. Einen alternativen Zugang zu dem hier gewählten bietet die stilistische Klassifikation mit Hilfe neuronaler Netzwerke (für Comics: Laubrock & Dubray). Obwohl hier potenziell bessere Ergebnisse erzielt werden können, präferieren wir aus mehreren Gründen einen niederschwelligen Ansatz: trotz der Zuhilfenahme der PCA in den hier abgebildeten Darstellungen versprechen wir uns von der Verwendung einzelner Maße eine bessere qualitative Interpretation. Zweitens ist dieser Zugang weniger datenhungrig und daher der geringen Anzahl an Werken in unserem Corpus angemessen. In einem nächsten Schritt wollen wir die Ergebnisse der Texterkennung verbessern. Dies wird es ermöglichen, zusätzliche Textmaße und Werke für unsere Analyse heranzuziehen und unsere Ergebnisse zu verbessern.",de,stilometrisch Untersuchung blicken Tradition Literaturwissenschaft Holmes Gegensatz befinden quantitativ Untersuchung Stil visuell Kunst Multimodaler Medium früh Experimentierstadium explorativ Untersuchung Methodenentwicklung prägen Fortschritt erkennbar digital Kunstgeschicht Filmwissenschaft Comicforschung Manovich Douglas zepel Qui taeb hughes Baxter Khitrova Tsivian cutting et Dunst Hartel konzentrieren stilistisch Klassifikation visuell sprachlich kanal beispielhaft nennen Analyse historisch Entwicklung Filmfarbe Barbara Flückiger ben Schmidt thematisch Untersuchung Populärer Flückiger Schmidt monomodal Fokus visuell Kunst oftmalig Dominanz Bildeben schulden multimodal Medium Film Fernsehen computerspielen Comic gründe ausschlaggebend Topic Modeling Textstilometrie stehen Methode Verfügung sprachlich daten basieren digital Literaturwissenschaft laufend weiterentwickeln visuell Stilometrie obwohl ausgereifen erwähnt arbeiten kunstgeschicht empirisch Filmwissenschaft zurückgreifen technisch Hürde tragen Monomodale Analyse Medium digital Erschließung Analyse Informationskanäl erheblich Schwierigkeit verbinden Spracherkennung Filmdialog automatisch Erkennung handschriftlich Text Comic computergestützt Verarbeitung Menge Bildmaterial Kombination unterschiedlich informationskanäle visuell Medium führen unweigerlich Frage inwieweit Stil Analyse Einzelner modi erfassen thematisch Untersuchung setzen Vorwurf komplex Medium schmal datenbasis interpretieren Filmanalyse alleine untertiteln Drehbücher basieren Gegenzug verbinden Einbeziehung mehrere informationskanäle Stilometretische Untersuchung Hoffnung multimodal Medium vollständig beschreiben einzeln Autor genr epochen genau voneinander unterscheiden folgend Untersuchung vorstellen Basis Corpus englischsprachig Comicbüchern genannt Graphic novels visuell Textstilometrie kombinieren Dunst Hartel Laubrock dokumentieren Basis kunsthistorisch filmwissenschaftlich vorarbeien visuell Stilometrie Comicbücher entwickeln einzeln Genre Autor Publikationsforme unterscheiden repräsentativ datenbasis stilistisch beschreiben Dunst Hartel relativ gering datenbasis Analyse kulturell nischenproduktes einhergehen bedeuten Fall signifikant Ergebnis erzielen insbesondere Entwicklung einzeln Gattung innerhalb Medium lässen historisch Verlauf signifikant belegen national Tradition stilistisch voneinander unterscheiden Betrachter deutlich voneinander unterscheiden Abbildung zeigen verwendet visuell Maß japanisch Osamu Tezuka Stilistisch Werk abgrenzen Dunst Hartel Fall sowohl Genreunterscheidunge liegen nahe kombinatorisch Analyse Visueller Textstilistik Wahrscheinlichkeit erfolgreich Unterscheidung erhöhen stellen zusätzlich dimensionen multimodal Stilometrie methodische Herausforderung Zahl maßen Klassifikation heranziehen erschweren qualitativ Interpretation Ergebnis lässen Hilfe Principal Component Analysis Pca darstellen genr Autor signifikant unterscheiden dimension Pca einfließen desto schwer fallen Aussage maßen Unterschied fußen hinzu erwähnen technisch Hürde absehen Angebot bekannt Softwaregigant fahren automatisch Spracherkennungssystem relativ schlecht Resultat öhnlich liegen Fall Comic Text Handschrift basieren kurz Text Hilfe automatisch erkennungssysteme deep learning basieren zugänglich dennoch liegen fehlerraen Basis meister literaturwissenschaftlich Analyse bilden nächster Abschnitt stellen Methode dennoch Verwendung einfach Textmaße multimodal Stilometrie ermöglichen analysen basieren repräsentativ Corpus englischsprachig comicbüch graphcr Narrative Corpus gnc nennen Dunst Hartel Laubrock früh arbeit beschreiben Hartel Dunst nutzen Bag Error Rate ber Abschätzung Qualität erkannt Text ausreichend textanalyse heranziehen hierzu Gold Standard Seite Graphic novel manuell annotieren betrachten unser Analyse bag bezeichnen Wörter ungeordnet Menge Wörter wobei wört Gegensatz herkömmlich Menge Multimenge mehrfach vorkommen berechnen jeder Text enthalten Wörter Differenz häufigkeit Freq ber früh Analyse zeigen Text geeignet Analyse erachten Graphic novel ber erkannt Text betrachten Textähnlichkeit basierend euklidisch Vektordistanz jeweils jeder Dokument d relativ Vorkommenshäufigkeit tf d t häufig Wörter t enthalten visuell Maß betrachten mittlerer Helligkeit Seite Entropie Anzahl Fläche Maß visuell Unruhe Bild Color Layout Descriptor Edge Histogram Descriptor Standard Koene Pereira früh arbeiten vielversprechend Maße Herausgestellt Dunst Hartel Dimensionsreduktion notwendig fahren Mithilfe Pca textuell visuell Maß kombinieren betrachten Ungleichgewicht dimensionen häufig Wörter Vergleich visuell maeß entgegenwirken via Pca textuell dimensionen reduzieren anschließend dimension textuell Pca Dimension visuell Maß Hilfe Pca kombinieren Untersuchung signifikant zusammenhang nutzen Anova Analysis -- variance untersuchen Varianz Kategorie groß Varianz innerhalb kategorien Abbildung stellen Ergebnis multimodal Stilometrie Vergleich rein visuellen textmaßen dar zeigen Kombination Textanalyse Erfolg führen ergeben Analyse beide informationskanäle statistisch deutlicherer Signifikanze Genreunterscheidung Effekt gering Fall Klassifikation Originalsprach werks unterschiedlich Publikationsform einzelband fortgesetzt Hinzunahme Textanalyse derzeit signifikanten Ergebnis Analyse unterschiedlich Form Autorschaft Zusammenarbeit groß liegen Resultat Textanalyse Signifikanzgrenze deutlich signifikant Ergebnis visuell Stilometrie setzen Kombination beide kanal insgesamt lässen sagen trotz gleich Anzahl verwendet Visuell Textmaße erstern derzeit aussagekräftig erscheinen erscheinen sinnvoll Analyse beide informationskanäle einzeln betrachten sofort kombinieren erwähnen zeigen Klassifikation entscheidend Einfluss verwendet Textmaße Abbildung stellen untersucht Werk Streudiagrammen dar unterscheiden farblich Originalsprach Fall Japanischer Manga handeln eigenständig Nationaltradition obwohl japanisch französischsprachig Werk englisch vorliegen führen Textanalyse klar Unterscheidung stehen klar Gegensatz Ergebnis Abbildung Potenzielle ursachen Blick kontraintuitive Ergebnis anführen erstens erscheinen Unterscheidung Folge wahrscheinlich typisch merkmale Text Manga erhalten Fall Frequenz einzeln Wörter Untersuchung vorstellen Comicbücher visuell Textmaße multimodal Stilometrie kombinieren handeln insbesondere letzter Fall einfach Maß derzeitig stehen automatisch Texterkennung Comicschrift schulden insgesamt pilotversuch zeigen führen Kombination Bildeben Analyse gut Ergebnis trotz gering Anzahl untersucht Werk sowohl Gattungsunterscheidung Autoridentifikation Fall Literaturwissenschaft lang ähnlich Maß heranziehen insgesamt erscheinen sinnvoll Kombination Analyse visuell textlich informationskanäle einzeln betrachten alternativ Zugang Gewählt bieten stilistisch Klassifikation Hilfe neuronaler netzwerk Comic Laubrock Dubray obwohl potenziell gut Ergebnis erzielen präferieren mehrere Grund niederschwellig Ansatz trotz Zuhilfenahme Pca abgebildet Darstellung versprechen Verwendung einzeln Maß gut qualitativ Interpretation zweitens Zugang datenhungrig gering Anzahl Werk unser Corpus angemessen nächster Schritt Ergebnis Texterkennung verbessern ermöglichen zusätzlich Textmaße Werk Analyse heranziehen Ergebnis verbessern,"[('visuell', 0.3130591307018322), ('hartel', 0.2051118625915954), ('dunst', 0.19762715712317946), ('pca', 0.19129892881846494), ('textmaße', 0.17337688240146573), ('informationskanäle', 0.17337688240146573), ('multimodal', 0.15512735357615454), ('stilometrie', 0.15487315891747885), ('analyse', 0.1496803299196632), ('maß', 0.14587823524511143)]"
2019,DHd2019,115_final-CALVO_TELLO_Jose_Gattungserkennung_über_500_Jahre.xml,Gattungserkennung über 500 Jahre,"José Calvo Tello (Universität Würzburg, Deutschland)","Gattung, Maschinelles Lernen, Korpora, Spanisch","Datenerkennung, Programmierung, Inhaltsanalyse, Stilistische Analyse, Literatur, Text","Wie gut lassen sich Gattungen und Untergattungen durch Maschinelles Lernen über eine längere Periode erkennen? Obwohl eine Reihe von Artikeln die Frage hauptsächlich für Englisch (Kessler, Numberg, und Schütze 1997; Petrenz und Webber 2011; Underwood 2014) und Deutsch (Hettinger et al. 2016) beantwortet hat, befasst sich wenig Forschung mit diesem Thema aus einer diachronischen Perspektive oder wird auf spanischen Texte angewendet (Henny-Krahmer 2018). Welche Gattungen sind leichter zu erkennen, welche komplizierter? Welche Algorithmen, Transformationen und Anzahl der lexikalischen Einheiten funktionieren am besten? Zur Beantwortung ob verschiedene Gattungen durch Maschinelles Lernen erkannt werden können, wurde das umfangreichste historische Korpus des Spanischen analysiert, CORDE. Dieses Korpus wurde von der Real Academia Espa√±ola kompiliert (Rojo S√°nchez 2010; S√°nchez S√°nchez und Dom√≠nguez Cintas 2007) und ist ein standard-Tool in der Hispanistik über das online Such-Interface (Kabatek und Pusch 2011). Für die Analyse wurden die Frequenzen der Tokens und die Metadaten jedes Texts an Forscher weitergegeben. Das Korpus beinhaltet ca. 300 Millionen Tokens (34.000 Texte) und die Texte sind mit expliziten Metadaten über Jahrhunderte, Länder und Gattungen markiert. Die Daten der mittelalterlichen Sektion des Korpus präsentieren mehrere Probleme (Rodr√≠guez Molina und Octavio de Toledo y Huerta 2017), wie beispielsweise ausgeprägte Unausgewogenheit der Anzahl der Texten im Vergleich zu anderen Jahrhunderten oder schwankende philologische Qualität. Deswegen wurden für diese Analyse nur die Texte der letzten 500 Jahre des Korpus selektiert, die länger als 100 Tokens sind. Somit beinhaltet das analysierte Korpus über 22.000 Texte (über 244 Millionen Tokens). Die Metadaten unterscheiden: Eine komplette Liste der Gattungen ist auf den Abbildungen zu finden. Die Klassifikation wurde binarisiert durchgeführt, d. h. jeder Text könnte zu jeder Gattung gehören oder nicht. Verschiedene Parameter wurden evaluiert: Das Korpus wurde für jede Gattung undersampled: die gleiche Anzahl an positiven wie an negativen Fällen wurden für jede Gattung gesamplet. Die Evaluation wurde mit Hilfe von Cross-Validation (10 folds) durchgeführt und der Mittelwert der F1 Scores berechnet. Der Code wird als Python Notebook über GitHub zugänglich sein. Die höchsten F1 Scores der Kombinationen von Parametern für jede Gattung lagen zwischen 0,9 und 1,0 mit einem Mittelwert der verschiedenen Gattungen von 0,96 (Standardabweichung von 0,03). Diese sehr hohen Ergebnisse ähneln sich denen von Underwood (2014), der an einem sehr großen Datensatz forschte. Die häufigsten Parameter bei den besten Ergebnissen waren Logistic Regression (16 Fälle von 27), binäre Häufigkeit (16, was nicht zu erwarten war) und 6.000 MFW (9). Auf den nächsten Boxplots sind die 10 besten Kombinationen zu sehen. Jeder Punkt entspricht dem Mittelwert der F1 Scores der Cross-Validation der 10 besten Kombinationen von Parametern. Diese sind nach Gattung differenziert aufgelistet:  Folgende Gattungen wurden am besten erkannt: Theater (Vers und Prosa), Romane, lyrischer Vers, und Fachtexte über Naturwissenschaften und Kunst. Lyrische Prosa zeigt heftige Schwankungen, außerdem wurden die niedrigsten Ergebnisse von folgenden Gattungen erreicht: Autobiografie, narrativer Vers, Essay, lyrische Prosa, Prosa sowie Fachtexte über Gesellschaft, Geschichte und Geisteswissenschaften. Ein interessanter Aspekt ist, welche die allgemeinen Tendenzen der Parameter und dessen Kombinationen sind. Dafür eignet sich ein Facet Grid Scatter Plot mit den Algorithmen als Spalten und den Transformationen als Reihen (einzelne Punkte entsprechen den Mittelwert der F1 Scores pro Gattung):  Hinsichtlich der Transformation (Reihen) zeigen die relative und die logarithmierte Häufigkeit niedrigere Ergebnisse als TF-IDF, z-scores und die binäre Häufigkeit. Bei den Algorithmen (Spalten) ist KNN merklich schlechter als die anderen drei. Zuletzt ist noch zu erkennen, dass die Qualität der Ergebnisse bis zu einer Anzahl von 2.000 Tokens zunimmt, und mit Schwankungen bis 6.000 stabil bleibt. Ein interessanter Aspekt ist die Tatsache, dass spezifische Kombinationen (SVC + TF-IDF, binäre + Logistic Regression, relative + Random Forest) von Vorteil im Vergleich zu anderen sind.",de,lassen gattung untergattung Maschinelles lernen lang Periode erkennen obwohl Reihe Artikel Frage hauptsächlich Englisch Kessler numberg schützen Petrenz webber underwood deutsch hettinger et beantworten befassen Forschung Thema diachronisch Perspektive spanisch Text anwenden Gattung leicht erkennen kompliziert algorithm Transformatione Anzahl lexikalisch einheit Funktioniere Beantwortung verschieden Gattung Maschinelles lernen erkennen umfangreich historisch Korpus spanisch analysieren Corde korpus Real academia kompilieren rojo Nchez Nchez Nchez cintas Hispanistik Online kabatek pusch Analyse Frequenz Token Metadat jeder texts Forscher weitergeben korpus beinhalten Million Token Text Text explizit metadaten jahrhunderte Land gattung markieren daten mittelalterlich Sektion Korpus präsentieren mehrere Problem Molina octavio de Toledo y Huerta beispielsweise ausgeprägt Unausgewogenheit Anzahl Text Vergleich jahrhundert schwankend philologisch Qualität Analyse Text letzter Korpus selektieren lang Token somit beinhalten analysiert Korpus Text Million Token metadaten unterscheiden komplett Liste gattung Abbildung finden Klassifikation binarisiert durchführen Text Gattung gehören verschieden Parameter evaluieren korpus Gattung undersampled gleich Anzahl Positiv negativ Fall Gattung Gesamplet Evaluation Hilfe Fold durchführen Mittelwert Scores berechnen Code python Notebook Github zugänglich hoch scores Kombination Parameter Gattung liegen Mittelwert verschieden Gattung Standardabweichung hoch Ergebnis ähneln Underwood Datensatz forschen häufigsten parameter Ergebnis Logistic Regression Fall binär Häufigkeit erwarten mfw nächster Boxplot Kombinatione sehen Punkt entsprechen Mittelwert Scores Kombination Parameter Gattung differenziert auflisten folgend gattungen erkennen Theater vers Prosa roman lyrisch Ver fachtexte naturwissenschaft Kunst lyrisch Prosa zeigen heftig Schwankung niedrig Ergebnis folgend Gattung erreichen Autobiografie narrativ Ver Essay lyrisch Prosa Prosa Fachtexte Gesellschaft Geschichte geisteswissenschaften interessant Aspekt Tendenz parameter kombination eignen Facet Grid Scatter plot Algorithm spalten Transformation Reihe einzeln Punkt entsprechen Mittelwert Scores pro Gattung hinsichtlich Transformation Reihe zeigen relativ Logarithmierte Häufigkeit niedrig Ergebnis binär Häufigkeit algorithm spalten Knn merklich schlecht zuletzt erkennen Qualität Ergebnis Anzahl Token zunehmen Schwankung stabil bleiben interessant Aspekt Tatsache spezifisch Kombination svc binär Logistic Regression relativ random Forest Vorteil Vergleich,"[('gattung', 0.3994996223848696), ('scores', 0.20136656063441366), ('mittelwert', 0.18595366951998976), ('nchez', 0.17848544837150318), ('token', 0.17493079077274495), ('parameter', 0.1632513075209118), ('prosa', 0.16302378449486363), ('lyrisch', 0.14669723041125524), ('korpus', 0.1420498995685961), ('erkennen', 0.1403586045524503)]"
2020,DHd2020,191_final-ULRICH_Mona_Science_Data_Center_für_Literatur.xml,Science Data Center für Literatur,Mona Ulrich (Deutsches Literaturarchiv Marbach); Jan Hess (Deutsches Literaturarchiv Marbach); Roland Kamzelak (Deutsches Literaturarchiv Marbach); Heinz Werner Kramski (Deutsches Literaturarchiv Marbach); Kerstin Jung (Institut für maschinelle Sprachverarbeitung der Universität Stuttgart); Jonas Kuhn (Institut für maschinelle Sprachverarbeitung der Universität Stuttgart); Claus-Michael Schlesinger (Institut für Literaturwissenschaft / Digital Humanities der Universität Stuttgart); Gabriel Viehhauser (Institut für Literaturwissenschaft / Digital Humanities der Universität Stuttgart); Björn Schembera (Höchstleistungsrechenzentrum Stuttgart); Thomas Bönisch (Höchstleistungsrechenzentrum Stuttgart); Andreas Kaminski (Höchstleistungsrechenzentrum Stuttgart),"Digitale Literatur, Literatur im Netz, Repositories, Plattform, Forschungsdaten, Datenlebenszyklus","Sammlung, Annotieren, Archivierung, Link, Literatur, Forschung"," Das jüngst ins Leben gerufene interdisziplinäre Für die Archivierung, Analyse und Vermittlung von Digitaler Literatur wird eine Forschungsplattform entwickelt. Da eine solche Plattform nur in der interdisziplinären Zusammenarbeit zu bewerkstelligen ist, sind im Projekt Partner mit unterschiedlichen Expertisen in den einzelnen Teilbereichen vereint, nämlich das Deutsche Literaturarchiv Marbach, das Höchstleistungsrechenzentrum Stuttgart, sowie das Institut für Maschinelle Sprachverarbeitung und die Abteilung Digital Humanities der Universität Stuttgart. Die born-digital Bestände des Deutschen Literaturarchivs bestehen zum einen aus digitalen Nachlässen und zum anderen aus archivierten netzliterarischen Werken. Der umfangreichste digitale Nachlass am Deutschen Literatuarchiv ist von Friedrich Kittler und umfasst 1,5 Millionen Dateien. Zur deutschsprachigen Netzliteratur können weitaus weniger Objekte gezählt werden. Netzliteratur ist durch Verlinkungen und Multimedialität geprägt. Das erschwert die Definition von Objektgrenzen und führt zu nichtlinearen Objektstrukturen, die in der Rezeption nichtlineare Handlungen ermöglichen Zum einem scheinen sich diese Texte also zur Anwendung computergestützter und computerlinguistischer Methoden besonders anzubieten, da sie genuin in elektronischer Form vorliegen. Zum anderen bringt gerade diese Form für ihre Archivierung und Bereitstellung eine Reihe von besonderen Anforderungen mit sich. Digitale Nachlässe sind aufgrund großer Mengen an Daten ohne computergestützte Methoden kaum erschließbar und zugänglich zu machen. Um auf diese wachsende Herausforderung in Archiven und Bibliotheken einzugehen, soll der Einsatz von Methoden der Digital Humanities für die inhaltliche Erschließung textbasierter born-digital Bestände erprobt werden. Wenn digitale Nachlässe bereits obsolete Dateiformate enthalten, sind diese nicht ohne vorherige Formatmigration für aktuelle computergestützte Analysen zugänglich. Auch literarische Webseiten sind von den hochfrequenten Erneuerungszyklen digitaler Technik betroffen. Weiterentwicklungen der Betriebssysteme, der Browser, des HTML-Standards und gängiger Webtechnologien können zu fehlerhafter Darstellung oder fehlenden Funktionen einer Webseite führen. Um ein Werk der Netzliteratur dokumentieren zu können, sind daher neue Formen der Modellierung von Texten, die über eine bloß lineare Form hinausgehen, gefragt. Diese und weitere Bestände sollen mit modernen digitalen Methoden erschlossen, erforscht und vermittelt werden können. Im Zentrum stehen daher der Aufbau verteilter langzeitverfügbarer Repositories für Digitale Literatur inklusive Forschungsdaten und die Entwicklung der SDC4Lit-Forschungsplattform. Die Repositories werden vom Projekt und seinen Kooperationspartnern regelmäßig erweitert und bilden den zentralen Speicher für das Harvesting von Netzliteratur und weiteren Formen elektronischer Literatur im künftigen Betrieb des SDC. Die Forschungsplattform bietet die Möglichkeit zum computergestützten Arbeiten mit den Beständen der Repositories. Bereits entwickelte oder in der Entwicklung befindliche Ansätze zur Archivierung und Bereitstellung von WARC-Archiven (Lin et al. 2017), Textkorpora (Fischer et al. 2019) und Analysefunktionen (Hinrichs et al. 2010) sowie strukturierte Reflexionen eigener Strategien (Kramski, von Bülow 2011) weisen auf eine modulare und integrierte Lösung bei der Bereitstellung von Daten und Services. Die entsprechend geplante modulare Architektur der bereitgestellten Services ermöglicht eine nachhaltige Integration von Repositories und Analysemethoden sowie die Möglichkeit zur späteren bedarfsorientierten Einbindung von Korpora und Analysewerkzeugen. Für die Entwicklung des Repositories und der Forschungsplattform ist der Kontakt zu an der Herstellung, Verbreitung, Erforschung und Vermittlung von elektronischer Literatur beteiligten Communities ein entscheidendes Element. Diese Beteiligung wird über einen mehrköpfigen Beirat und Outreach-Maßnahmen wie Workshops, Seminare und die Arbeit mit Fokusgruppen erreicht. Eine wichtige Aufgabe des Projekts ist in diesem Zusammenhang die Modellierung von Formen digitaler Literatur, die zunächst beispielorientiert im Umgang mit einem bereits vorhandenen Corpus digitaler Literatur erfolgt. Neben digitalen Objekten und entsprechenden Metadaten wird auch ein Repository der anfallenden Forschungsdaten nachvollziehbar und nachhaltig gespeichert. Zu den Forschungsdaten zählen erstens die bei der Arbeit des SDC anfallenden Forschungsdaten, insbesondere solche, die für das Anbieten von Diensten auf der Plattform notwendig sind, etwa mittels Machine Learning errechnete Datenmodelle für an das Corpus angepasste computerlinguistische Analysewerkzeuge (Eigennamenerkenner, Parser, Topic Models etc.). Zweitens soll das Repository die Möglichkeit bieten, die von Nutzer*innen der Forschungsplattform generierten Forschungsdaten strukturiert zu speichern und für die weitere Forschung zur Verfügung zu stellen, etwa Annotationen oder ergänzte Metadaten zu einzelnen Objekten oder zu Objektklassen. Die Sammlung, Bereitstellung, Erforschung und Vermittlung von Literatur im medialen Wandel ist eine Aufgabe, die Forschung und Archive gleichermaßen betrifft. SDC4Lit verfolgt deshalb das Ziel, diese Aufgabe und die entsprechenden Unteraufgaben interdisziplinär zu bearbeiten.",de,jüngst Leben gerufen interdisziplinäre Archivierung Analyse Vermittlung Digitaler Literatur Forschungsplattform entwickeln Plattform interdisziplinär Zusammenarbeit bewerkstelligen Projekt Partner unterschiedlich Expertise einzeln teilbereich vereinen nämlich deutsch Literaturarchiv Marbach Höchstleistungsrechenzentrum Stuttgart Institut maschinell Sprachverarbeitung Abteilung Digital Humanities Universität Stuttgart Bestände deutsch literaturarchivs bestehen digital Nachläss archiviert netzliterarisch Werk umfangreich digital Nachlass deutsch Literatuarchiv friedrich Kittler umfassen Million dateien deutschsprachig Netzliteratur weitaus Objekt zählen Netzliteratur verlinkung Multimedialität prägen erschweren Definition Objektgrenzen führen nichtlinearen objektstrukturen Rezeption nichtlinear Handlung ermöglichen scheinen Text Anwendung Computergestützter computerlinguistisch Methode anbieten genuin elektronisch Form vorliegen bringen Form Archivierung Bereitstellung Reihe besonderer Anforderung digital nachlässe aufgrund menge daten computergestützt Methode erschließbar zugänglich wachsend Herausforderung archiv bibliothek eingehen Einsatz Methode Digital Humanitie inhaltlich Erschließung textbasiert Bestände erproben digital nachläß obsolet Dateiformat enthalten vorherig Formatmigration aktuell computergestützt Analyse zugänglich literarisch Webseit hochfrequent erneuerungszykl Digitaler Technik betreffen weiterentwicklungen Betriebssysteme Browser gängig webtechnologien fehlerhaft Darstellung fehlend Funktion Webseite führen Werk Netzliteratur dokumentieren Form Modellierung Text bloß linear Form hinausgehen fragen Beständ modern digital Methode erschließen erforschen vermitteln Zentrum stehen Aufbau verteilt langzeitverfügbar repositories digital Literatur inklusive Forschungsdat Entwicklung Repositories Projekt Kooperationspartner regelmäßig erweitern bilden zentral Speicher Harvesting Netzliteratur Form elektronisch Literatur künftig Betrieb sdc Forschungsplattform bieten Möglichkeit computergestützt arbeiten Beständ Repositories entwickelt Entwicklung befindlich Ansatz Archivierung Bereitstellung lin et Textkorpora Fischer et Analysefunktion Hinrichs et strukturiert Reflexion strategien Kramski Bülow weisen modular integriert Lösung Bereitstellung daten services entsprechend geplant modular Architektur bereitgestellt Service ermöglichen nachhaltig Integration repositories analysemethoden Möglichkeit spät bedarfsorientiert Einbindung Korpora analysewerkzeugen Entwicklung Repositori Forschungsplattform Kontakt Herstellung Verbreitung Erforschung Vermittlung elektronisch Literatur beteiligt Communities entscheidend Element Beteiligung Mehrköpfigen Beirat Workshops seminar Arbeit Fokusgruppe erreichen wichtig Aufgabe Projekt Zusammenhang Modellierung Form Digitaler Literatur beispielorientiern Umgang vorhanden Corpus Digitaler Literatur erfolgen digital objekt entsprechend metadaten Repository anfallend Forschungsdat nachvollziehbar nachhaltig speichern forschungsdat zählen erstens Arbeit sdc anfallend Forschungsdat insbesondere Anbiet Dienst Plattform notwendig mittels Machine Learning errechnet Datenmodelle Corpus angepas computerlinguistisch Analysewerkzeuge eigennamenerkenn Parser Topic Model zweitens Repository Möglichkeit bieten forschungsplattform generiert Forschungsdat strukturiern speichern Forschung Verfügung stellen annotation ergänzen Metadat einzeln Objekt objektklassen Sammlung Bereitstellung Erforschung Vermittlung Literatur medial Wandel Aufgabe Forschung Archiv gleichermaßen betreffen verfolgen Ziel Aufgabe entsprechend Unteraufgab interdisziplinär bearbeiten,"[('repositories', 0.2467930167225378), ('forschungsplattform', 0.22986898527819502), ('netzliteratur', 0.22986898527819502), ('forschungsdat', 0.21500667447451624), ('literatur', 0.1633537004483959), ('vermittlung', 0.1564104204463169), ('bereitstellung', 0.1528245532762497), ('elektronisch', 0.14587720821882974), ('digital', 0.13972683377344464), ('archivierung', 0.1380098482384209)]"
2020,DHd2020,227_final-KRAUTTER_Benjamin_Ein_Schritt_zurück__Distinktive_Eigenschaf.xml,Ein Schritt zurück: Distinktive Eigenschaften im deutschsprachigen Drama,"Benjamin Krautter (Universität Stuttgart, Deutschland)","Topic Modeling, Netzwerkanalyse, Klassifikation, Operationalisierung, Methodologie","Strukturanalyse, Theoretisierung, Netzwerkanalyse, Literatur, Text"," Ein mögliches Anwendungsszenario zeigt Ziel dieses Beitrags ist es jedoch, einen Schritt hinter solche makroanalytischen Befunde zurück zu treten. In einem vorgelagerten Arbeitsschritt möchte ich erörtern, welche quantitativ erfassbaren Merkmale dramatischer Texte überhaupt geeignet sind, um eine literarhistorische Einordnung und Unterscheidung der Dramen vorzunehmen. Anders formuliert sollen also die Kriterien ermittelt werden, die mit Blick auf die Entstehungszeit der Dramen unterscheidungstragend sind. Zu diesem Zweck dient im vorliegenden Fall eine einfach gehaltene Klassifikationsaufgabe. Ließen sich die dramatischen Texte erfolgreich ihrem Veröffentlichungszeitraum zuweisen, könnte daraus auf die Kriterien rückgeschlossen werden, die den entscheidenden Beitrag zu dieser Klassifikation leisten. Daran anschließend wäre eine Rückübersetzung der ermittelten Merkmale denkbar 'analog zur Operationalisierung –, die eine Interpretation anleiten könnten. Die folgenden Untersuchungen konzentrieren sich auf 443 deutschsprachige Dramen zwischen 1730 und 1930 aus dem Die historische Verortung der dramatischen Texte fasse ich als basal gehaltene Klassifikationsaufgabe. Ziel der Klassifikation ist es, mittels maschineller Lernverfahren näherungsweise den Veröffentlichungszeitraum der Dramen zu bestimmen, um daran anschließend die Einflussfaktoren identifizieren und untersuchen zu können. Dazu greife ich auf Metadaten zurück, die Angaben zur Erstaufführung und zur Erstpublikation umfassen. Diese Metadaten werden genutzt, um jedes Drama einer von vier heuristisch gesetzten Zeitspannen zuzuordnen, die jeweils circa 50 Jahre umfassen: 1730–1785 (93 Dramen), 1786–1832 (116 Dramen), 1833–1881 (105 Dramen) und 1882–1930 (129 Dramen). Die dadurch entstehenden Zeiträume dienen als Zielpunkt der Klassifikation und orientieren sich an wichtigen literaturgeschichtlichen Zäsuren: Aufklärung, Goethezeit, Realismus und literarische Moderne (vgl. etwa Brenner¬†2011). Basis der Netzwerk- und Zentralitätsmetriken sind Netzwerkgraphen, die auf Präsenz- bzw. Adjazenzmatrizen fußen. Knoten und Kanten repräsentieren hierbei Dramenfiguren und deren Interaktion, wobei Interaktion als das gemeinsame Sprechen innerhalb einer Szene operationalisiert ist (vgl. Trilcke 2013:¬†238f.). Eine Kante zwischen zwei Knoten wird also genau dann instanziiert, wenn die beiden fraglichen Figuren innerhalb derselben Dramenszene sprechen. Das bedeutet auch, dass verschiedene poetologische Vorstellungen von Akt und Aufzug sowie Szene und Auftritt, die im Verlauf der Dramengeschichte einem Wandel unterliegen, einen Eingang in die Graphen findet (vgl. etwa Vogel¬†2012). Für die Klassifikation nutze ich die folgenden acht Maße:  Das maschinelle Klassifikationsverfahren selbst nutzt den Algorithmus  Die in Auf Basis dieser Daten lassen sich nun analog zu  Auch der in Die größte Schwierigkeit bei der Interpretation dieser Daten bleibt jedoch nach wie vor bestehen und ist den hier gezeigten Analysen vorgelagert. Es ist die Operationalisierung der Fragestellung, die zumeist mit einem großen konzeptuellen Aufwand verbunden ist (vgl. Gius 2019:¬†2f.; Reiter / Willand 2018). Denn unklar bleibt, wie sich Zentralitätsmetriken in einem Figurennetzwerk oder Wahrscheinlichkeitsverteilungen von Worthäufigkeiten zu literaturwissenschaftlichen Kategorien verhalten. Die bemessenen Werte müssten sich konzeptionell so rückübersetzen lassen, dass sie auch mit Blick auf spezifisch literaturwissenschaftliche Fragestellungen interpretiert werden können. Dass etwa die Handlung literarischer Texte nicht einfach durch ein Figurennetzwerk abzubilden ist, muss auch Moretti erkennen, weshalb er die Netzwerktheorie letztlich nurmehr als Vorstufe, als ""beginning of the beginning"" (Moretti 2011:¬†2) zu einer quantifizierbaren Handlung einordnet. Das vorgestellte multidimensionale Modell liefert sinnvolle Ergebnisse und kann den recht weit gefassten Veröffentlichungszeitraum deutschsprachiger Dramen mit angemessener Genauigkeit (F",de,möglich Anwendungsszenario zeigen Ziel Beitrag Schritt makroanalytisch Befund treten vorgelagert arbeitsschritt erörtern quantitativ erfassbar merkmal dramatisch Text eignen literarhistorisch Einordnung Unterscheidung Dram vornehmen formulieren kriterien ermitteln Blick Entstehungszeit Dram unterscheidungstragend zweck dienen vorliegend Fall einfach gehalten Klassifikationsaufgabe lassen dramatisch Text erfolgreich Veröffentlichungszeitraum zuweisen kriterien rückgeschlossen entscheidend Beitrag klassifikation leisten anschließend Rückübersetzung ermittelt merkmal denkbar analog Operationalisierung Interpretation anleiten können folgend Untersuchung konzentrieren deutschsprachig Dram historisch Verortung dramatisch Text fassen basal gehalten Klassifikationsaufgabe Ziel Klassifikation mittels Maschineller lernverfahren näherungsweise Veröffentlichungszeitraum Dram bestimmen anschließend einflussfaktor identifizier untersuchen greifen metadaten Angabe Erstaufführung Erstpublikation umfassen metadaten nutzen jeder Drama heuristisch gesetzt zeitspann Zuzuordn jeweils circa umfassen dramen dramen Dram dramen entstehend zeiträume dienen Zielpunkt Klassifikation orientieren wichtig Literaturgeschichtliche Zäsur Aufklärung Goethezeit Realismus literarisch modern Basis zentralitätsmetriken netzwerkgraphen adjazenzmatrizen fußen knoten Kant repräsentieren hierbei dramenfiguren Interaktion wobei Interaktion gemeinsam sprechen innerhalb Szene operationalisieren Trilcke Kante knoten genau instanziieren fraglich Figur innerhalb dramenszen sprechen bedeuten verschieden poetologisch Vorstellung Akt Aufzug Szene auftreten Verlauf dramengeschichte Wandel unterliegen Eingang graphen finden Klassifikation nutzen folgend Maß maschinell Klassifikationsverfahren nutzen Algorithmus Basis daten lassen analog groß Schwierigkeit Interpretation daten bleiben bestehen gezeigt Analyse vorgelageren Operationalisierung Fragestellung zumeist konzeptuell Aufwand verbinden Gius Reiter Willand unklar bleiben zentralitätsmetriken Figurennetzwerk wahrscheinlichkeitsverteilungen Worthäufigkeit literaturwissenschaftlich Kategorie verhalten bemessen wert Müsst konzeptionell rückübersetzen lassen Blick spezifisch literaturwissenschaftlich Fragestellung interpretieren Handlung literarisch Text einfach Figurennetzwerk abzubilden moretti erkennen weshalb Netzwerktheorie letztlich nurmehr Vorstufe beginning -- -- beginning moretti quantifizierbar Handlung einordnen vorgestellt multidimensional Modell liefern sinnvoll Ergebnis gefasst Veröffentlichungszeitraum deutschsprachig dramen angemessen Genauigkeit f,"[('veröffentlichungszeitraum', 0.24626779385710393), ('dram', 0.21797798721380443), ('dramen', 0.20333241776097877), ('gehalten', 0.16417852923806928), ('zentralitätsmetriken', 0.16417852923806928), ('klassifikation', 0.16339621174679012), ('dramatisch', 0.1463931368469273), ('klassifikationsaufgabe', 0.1387355991802894), ('beginning', 0.12568486206179552), ('figurennetzwerk', 0.1224143438365714)]"
2020,DHd2020,167_final-GIUS_Evelyn_Korpusbereinigung_für_größere_Textmengen.xml,Korpusbereinigung für größere Textmengen. Eine (kurze) Problematisierung und ein Lösungsansatz für Duplikate,"Benedikt Adelmann (Universität Hamburg, Deutschland); Evelyn Gius (Technische Universität Darmstadt, Deutschland)","Korpuserstellung, Dubletten, Bereinigung","Umwandlung, Sammlung, Bereinigung, Text"," Mit der Verfügbarkeit digitaler Texte wurde die Praxis der Korpuserstellung jedoch erweitert und es wurde offensichtlich, dass sie nicht ohne weiteres in die bestehende literaturwissenschaftliche Disziplinarmatrix (Kuhn 1970) integriert werden kann. Viele der digital verfügbaren Texte sind nämlich weder kanonisch noch repräsentativ, die Qualität einzelner Texte ist aus philologischer Sicht oft fragwürdig und ein Korpus enthält trotz der Vielzahl der verfügbaren digitalen Texte selten die gesamte Population relevanter Texte, sondern nur eine Teilmenge. Über die philologisch einwandfreie Auswahl von Texten hinaus birgt die Kuration von Korpora weitere Herausforderungen. Der vorgestellte Ansatz wurde für ein Korpus entwickelt, das in einem Forschungsprojekt zur geschlechtsspezifischen Darstellung von Krankheit in literarischen Texten im Rahmen der Forschungskooperation hermA Im daraus resultierenden Korpus von mehr als 2.500 Texten mussten Artefakte behandelt werden, die durch unterschiedliche Digitalisierungsstrategien verursacht wurden. Nicht nur die Erhaltung von Sonderzeichen wie das recht häufige lange s (≈ø) zwischen oder innerhalb der Repositorien war inkonsistent, sondern auch die Verwendung von Bindestrichen (Wortverbindung, Worttrennung an Zeilenumbruch, andere Bindestriche, Gedankenstriche) oder die Kodierung von Zeilenumbrüchen und Absätzen. Diese Probleme konnten mit einer relativ einfachen Heuristik angegangen werden. Ein schwierigeres Problem ist die Frage der Duplikate: Insbesondere bei der Zusammenstellung eines Korpus aus verschiedenen Quellen kann es vorkommen, dass der gleiche Text mehrfach vorhanden ist. In der Regel ist es nicht erwünscht, mehr als eine Instanz desselben Textes im Korpus zu haben, da die Überrepräsentation einzelner Werke bei statistischen Analysen zu verzerrten Ergebnissen führen kann. Daher sollte die Identifizierung von Duplikaten ein wesentlicher Bestandteil der Korpuserstellung sein. Dabei gibt es zwei Probleme: Erstens wächst die Anzahl der ungeordneten Werkpaare, die alle potenziell Duplikate sein könnten, quadratisch mit der Anzahl der Werke. In unserem Korpus mit gut 2.500 Texten müssten deshalb 3,1 Millionen Werkpaare überprüft werden. Zweitens ist auch für jedes einzelne Textpaar die Feststellung, ob es sich um Duplikate handelt, aufgrund von Metadaten- und Textinkonsistenzen eine nicht-triviale Aufgabe. Ansätze zur Wir haben mit zwei Methoden zur automatischen Duplikatidentifizierung experimentiert. Beide sind Heuristiken für die Suche nach Werkpaaren, die Duplikate sind; sie lösen aber nicht das daran anschließende Problem, zu entscheiden, welche von mehreren Instanzen tatsächlich in das Korpus aufgenommen werden sollen. Für die Evaluation wurden alle Duplikatkandidaten, die von mindestens einer der beiden Methoden gefunden wurden, manuell auf ihre Richtigkeit überprüft. Wir berichten über den Prozentsatz der automatisch als Duplikate identifizierten Paare, die tatsächlich Duplikate sind (Precision), und den Prozentsatz der tatsächlichen Duplikate, die automatisch identifiziert werden (Recall). Allerdings lässt sich der Recall nur exakt bestimmen, wenn alle 3,1 Millionen Werkpaare manuell untersucht werden. Als Annäherung verwenden wir daher stattdessen den Prozentsatz der bei der manuellen Prüfung identifizierten tatsächlichen Duplikate (Gesamtzahl: 355), die ebenfalls automatisch gefunden werden. Die erste Methode basiert auf Metadaten und ist deshalb schnell genug, um alle ungeordneten Werkpaare im Korpus zu testen. Für jedes Werkpaar werden Autor*innen- und Titelinformationen verglichen. Was die Autor*innen-Informationen betrifft, so wird die sogenannte Edit-Distanz (Levenshtein 1965) der vollständigen Namen der Autor*innen berechnet; die Edit-Distanz ist die kleinste Anzahl von Zeicheneinfügungen, Zeichenlöschungen und Zeichenersetzungen (""Edits""), mit der der erste Autor*innen-Name in den zweiten umgewandelt werden kann. Für die Titelinformationen haben wir das Maß leicht modifiziert: Wir verwenden die kleinste Anzahl von Edits, die einen der Titel in einen Zwei Texte werden als Duplikate betrachtet, wenn der Abstand sowohl beim Autor*innen-Namen als auch beim Titel höchstens so hoch wie der Schwellenwert ist. Bei einem Schwellenwert von 2 wurden 672 Duplikatpaare mit einer Precision von 51,9 % und einem Recall von 98,3 % identifiziert. Unter den sechs nicht identifizierten Duplikaten finden sich beispielsweise zwei Werke von Karl May mit abweichender Behandlung von Sammelband-/Einzelwerktitel (""Ardistan und Dschinnistan. 1. Band"" vs. ""Der Mir von Dschinnistan""; ""Satan und Ischariot III"" vs. ""Im Todesthale"") und ein Fall, in dem bei einem der beiden Werke fälschlich der Name des Autors als Titel eingetragen war (Ferdinand von Saar, ""Vae victis!""). Das zweite Verfahren berechnet die Edit-Distanzen von Volltexten. Da dies eine zeitaufwändige Operation ist, beschränken wir uns auf den Vergleich von Texten, bei denen der Autor*innen-Name eine Edit-Distanz von maximal zwei hat. Manuelle Überprüfungen zeigten, dass diese Schwelle alle Rechtschreibfehler und Varianten in unseren Daten erfasst, verschiedene Autor*innen mit ähnlichen Namen jedoch ausnimmt. Für Volltexte verwenden wir wieder Teilzeichenketten-Edit-Distanzen, da ein Text mehr oder weniger vollständig in einem anderen enthalten sein kann (z. B. bei Anthologien). Zugunsten der Rechenzeiten verwenden wir wortbezogene Distanzen mit Insertionskosten gleich Deletionskosten gleich Substitutionskosten gleich eins. Zwei Texte gelten als Duplikate, wenn die Teilzeichenketten-Edit-Distanzen für beide Richtungen (1 als Teilzeichenkette von 2 oder umgekehrt), dividiert durch die Länge des jeweils als Teilzeichenkette einzubettenden Texts, unter 15 % liegt. Auf diese Weise können wir 307¬†Duplikate mit einer Precision von 98 % und einem Recall von 84,8 % bestimmen. 678¬†Paare wurden so durch mindestens ein Verfahren als Duplikate identifiziert (Precision: 52,4 %), Die Digitalisierung hat die Arbeit mit literarischen Korpora erheblich gefördert. Die schiere Menge an Texten in einem Korpus muss sowohl technisch als auch konzeptionell unterstützt werden. Für die Erreichung dieser Ziele ist es umso wichtiger, Qualitätskriterien für die Zusammenstellung von Korpora im Hinblick auf die verfügbaren Daten, d. h. für Texte aus heterogenen Quellen unterschiedlicher Qualität, zu entwickeln und umzusetzen. Zusätzlich zu diesen noch zu entwickelnden Kriterien für die wissenschaftliche Qualitätssicherung können einige pragmatische Entscheidungen die Qualität eines Korpus und seiner Texte in Fällen mit geringer Daten- und insbesondere Metadatenqualität erheblich verbessern. Der vorgestellte Ansatz zum Umgang mit Duplikaten kann zusammen mit den genannten Vorverarbeitungsschritten ein wichtiger erster Schritt in diesem Prozess sein.",de,Verfügbarkeit digital Text Praxis Korpuserstellung erweitern offensichtlich bestehend Literaturwissenschaftliche Disziplinarmatrix kuhn integrieren Digital verfügbar Text nämlich weder kanonisch repräsentativ Qualität einzeln Text philologisch Sicht fragwürdig Korpus enthalten trotz Vielzahl verfügbar digital Text selten gesamt Population relevant Text Teilmenge philologisch einwandfrei Auswahl Text hinaus bergen Kuration Korpora Herausforderung vorgestellt Ansatz Korpus entwickeln Forschungsprojekt geschlechtsspezifisch Darstellung Krankheit literarisch Text Rahmen Forschungskooperation Herma resultierend Korpus Text artefakte behandeln unterschiedlich Digitalisierungsstrategien verursachen Erhaltung sonderzeichen häufig s innerhalb Repositorie inkonsistent Verwendung Bindestrich Wortverbindung Worttrennung zeilenumbruch bindestrich Gedankenstrich Kodierung zeilenumbrüchen Absätz Problem relativ einfach Heuristik angehen schwierigeres Problem Frage Duplikate insbesondere Zusammenstellung korpus verschieden quellen vorkommen gleich Text mehrfach vorhanden Regel erwünscht Instanz Text Korpus überrepräsentation einzeln Werk statistisch Analyse verzerrt Ergebnis führen Identifizierung duplikaten wesentlich Bestandteil Korpuserstellung Problem erstens wachsen Anzahl ungeordnet Werkpaar Potenziell Duplikate können quadratisch Anzahl Werk unser Korpus text Müsst Million Werkpaar überprüfen zweitens jeder einzeln Textpaar Feststellung Duplikate handeln aufgrund Textinkonsistenz Aufgabe Ansatz Methode automatisch Duplikatidentifizierung experimentieren Heuristike Suche Werkpaaren Duplikate lösen anschließend Problem entscheiden mehrere Instanz tatsächlich Korpus aufnehmen Evaluation duplikatkandidaen mindestens Methode finden manuell Richtigkeit überprüfen berichten Prozentsatz automatisch duplikat Identifiziert Paar tatsächlich duplikat Precision Prozentsatz tatsächlich Duplikate automatisch identifizieren Recall lässen Recall exakt bestimmen Million Werkpaar manuell untersuchen Annäherung verwenden stattdessen Prozentsatz manuell Prüfung Identifiziert tatsächlich Duplikate Gesamtzahl ebenfalls automatisch finden Methode basieren metadaten schnell ungeordnet Werkpaar Korpus testen jeder Werkpaar Titelinformation vergleichen betreffen sogenannter Levenshtein vollständig Name berechnen klein Anzahl zeicheneinfügungen zeichenlöschung zeichenersetzung edits umwandeln Titelinformation Maß modifizieren verwenden klein Anzahl edits Titel Text Duplikate betrachten Abstand sowohl Titel höchstens schwellenwern schwellenwert duplikatpaare Precision Recall identifizieren identifizierten duplikaten finden beispielsweise Werk Karl May abweichend Behandlung Ardistan dschinnistan Band dschinnistan satan ischariot iii todesthal Fall Werk fälschlich Name Autor Titel eintragen Ferdinand saar vae victis Verfahren berechnen Volltext zeitaufwändig Operation beschränken Vergleich Text maximal Manuell überprüfungen zeigen Schwelle Rechtschreibfehler varianen unser daten erfasst verschieden ähnlich Name ausnimmt Volltext verwenden Text vollständig enthalten Anthologien zugunsten Rechenzeite verwenden Wortbezogene distanzen insertionskosten deletionskoster substitutionskosen einer Text gelten Duplikate Richtung Teilzeichenkette umgekehrt dividieren Länge jeweils Teilzeichenkette einzubettenden texts liegen Weise Precision Recall bestimmen mindestens Verfahren Duplikate identifizieren Precision Digitalisierung Arbeit literarisch Korpora erheblich fördern schier Menge Text Korpus sowohl technisch konzeptionell unterstützen Erreichung Ziel umso wichtig qualitätskriterien Zusammenstellung Korpora Hinblick verfügbar daten Text heterogen quellen unterschiedlich Qualität entwickeln umsetzen zusätzlich entwickelnd kriterien wissenschaftlich Qualitätssicherung pragmatisch Entscheidung Qualität korpus Text Fall gering insbesondere Metadatenqualität erheblich verbessern vorgestellt Ansatz Umgang duplikaten genannt vorverarbeitungsschritten wichtig Schritt Prozess,"[('duplikate', 0.4242761720066596), ('werkpaar', 0.2670109821137204), ('text', 0.19197495828215214), ('korpus', 0.16965550763792958), ('duplikaten', 0.16020658926823222), ('prozentsatz', 0.1492202923690183), ('recall', 0.14081559958071782), ('precision', 0.13848399129359354), ('bindestrich', 0.10680439284548815), ('titelinformation', 0.10680439284548815)]"
2020,DHd2020,192_final-KONLE_Lenard_Information_Leakage_in_Sub_Genre_classification.xml,Confounding variables in Sub-Genre classification: instructive problems,Fotis Jannidis (Universität Würzburg); Leonard Konle (Universität Würzburg); Peter Leinen (Deutsche Nationalbibliothek),"Gattung, Predictive Modeling, Bias","Inhaltsanalyse, Modellierung, Stilistische Analyse, Text"," Most research on genre classification has been looking into what you could call ""high level classes"" like newspaper genres (news, editorials etc.; e.g. Frank and Bouckaert, 2006) or web genres (blog, personal website etc.; e.g. Eissen and Stein, 2004). Under this perspective all texts we are looking at belong to one genre: the novel. The subgenres are types of love stories like the doctor novel (""Arztroman"") or the country novel (""Heimatroman"") and types of adventure novels, mainly distinguished by the setting: the war novel (""Kriegsroman"") or the science fiction novel. These novels are cheap (""dime novels"") and published in a booklet format and are usually distributed via magazine kiosks and not book shops (Stockinger 2018). From the very beginning it was clear to us, that they don""t contain a random collection of each genre. On the contrary, the crime novels for example are just a small and very specific subsection of crime novels in general. But nevertheless we assumed that genre is the main aspect to group novels - for publishers and readers. Our dataset consists of 11,600 dime novels from 12 different genres (see Fig.1). The genre label come from the four publishers who divide the market among themselves. (Bastei, Martin Kelter, Pabel Moewig and Cora). The corpus has been documented in previous studies such as Jannidis et. al. (2019a) and Jannidis et. al (2019b).   To understand the first phenomenon better, we plotted the distribution of the authors across the genres¬† (see Fig. 2): Many authors write exclusively within a genre. The greatest overlap can be found in the genres In order to gain an insight into the influence of genre and publisher on the text form, we use Ivis (Szubert 2019) for unsupervised dimensionality reduction. The coloring of the data points according to publisher (figure 3) and genre (figure 4) shows the strong influence of these variables on the texts. It is also clear that Cora Verlag allows less variance among genres and thus becomes the most discriminatory factor. Figure 5 shows a detail of the previous plot, but focuses on microstructures. Theses structures indicate, that on this level genre and publisher are not enough to explain the distribution and that something else 'author or series 'comes into play.  We created a restricted setup with a clear separation of authors, series and publishers between training and test data (i.e. authors which were in the training data, were not included in the test data etc.), and tested the subgenres in an one-vs-rest scheme. Figure 6 shows the results of this setup with at least 30 different combinations of test and training data per genre and a sample size of 200 novels split in half for training and test data.  Though now we control for confounding variables, it is less clear, what it implies for the genre model. It is not unusual in genre theory to conceptualize genre in an ideal way as independent of other factors like authorship, time, publisher etc. which corresponds to the ""strict"" version of splitting train and test data.¬† But at the same time, these factors may be so intertwined with the genre features, that it is difficult, if not impossible to separate them at all (Hempfer 2010). Under this perspective our attempt to construct a ""clean"" and strict model of genre, independent of publishers etc. is a misguided attempt. Looking back we now see that we started our research with some assumptions which seem to be unfounded for this part of the literary market which is dominated by four publishers: We assumed that the genre labels have the same function as in the rest of the literary market. But the small number of publishers seems to create a different situation. We assume now, that at least in some instances combinations of genre names with publisher names (loves stories from Cora vs. love stories from Bastei-Lübbe) describe the clusters best. To start to evaluate this hypothesis, we trained the corpus on label combinations: 1) Genre and Publisher, e.g. ""Cora-Love"", 2) Genre and Series. Figure 6 shows, that in many, but not all cases these combinations achieve very good results, which indicates that a clear-cut set of features corresponds these combinations. In some genres the same is true for series, for example doctoral or horror, while in others the series have no clear feature set (erotic, love).  Following up the indications for confounding variables we uncovered the complicated situation of genre in this subfield of the literary market. We succeeded to explore some of its substructures which haven""t been described yet in literary studies, though it has been always one of its topics that this kind of literature is a commodity (Nusser 1973, Nusser 1991, Nutz 1999, Stockinger 2018). It is quite astonishing that almost every genre behaves differently, but this may be the result of a decades-old competition between this small number of publishers.¬† Probably the different structures correspond to different strategies of each publisher. Bastei-Lübbe for example seems to follow a strategy where each series has a distinct profile, while Cora is focussing more on the publisher name as brand (Fig. 7 and Fig. 4) - though the clustering may also be influenced by the fact that Cora translates many novels from English. It would be an interesting follow-up-project, to find out, whether the readers of these genres know about these structures and how this knowledge directs their choices. Last but not least, we think that the strategies to control for known and unknown confounding variables in text classification, especially if it is done to understand existing structures and not so much to predict really new data, needs to be explored in more detail. We like to thank Reviewer 2 for providing detailed and very informative feedback especially on the relation between data leakage and confounding variables as well as on the evaluation of dimension reduction techniques.",en,research genre classification look high level class like newspaper genre news editorial etc frank bouckaert web genre blog personal website etc eissen stein perspective text look belong genre novel subgenre type love story like doctor novel arztroman country novel heimatroman type adventure novel mainly distinguish setting war novel kriegsroman science fiction novel novel cheap dime novel publish booklet format usually distribute magazine kiosk book shop stockinger beginning clear contain random collection genre contrary crime novel example small specific subsection crime novel general assume genre main aspect group novel publisher reader dataset consist dime novel different genre genre label come publisher divide market bastei martin kelter pabel moewig cora corpus document previous study jannidis et al jannidis et al understand phenomenon well plot distribution author fig author write exclusively genre great overlap find genre order gain insight influence genre publisher text form use ivis szubert unsupervised dimensionality reduction coloring data point accord publisher figure genre figure show strong influence variable text clear cora verlag allow variance genre discriminatory factor figure show detail previous plot focus microstructure theses structure indicate level genre publisher explain distribution author series come play create restrict setup clear separation author series publisher training test datum author training datum include test datum etc test subgenre vs rest scheme figure show result setup different combination test training datum genre sample size novel split half training test datum control confound variable clear imply genre model unusual genre theory conceptualize genre ideal way independent factor like authorship time publisher etc correspond strict version splitting train test time factor intertwine genre feature difficult impossible separate hempfer perspective attempt construct clean strict model genre independent publisher etc misguided attempt look start research assumption unfounded literary market dominate publisher assume genre label function rest literary market small number publisher create different situation assume instance combination genre name publisher name love story cora love story bastei lübbe describe cluster well start evaluate hypothesis train corpus label combination genre publisher cora love genre series figure show case combination achieve good result indicate clear cut set feature correspond combination genre true series example doctoral horror series clear feature set erotic love follow indication confound variable uncover complicated situation genre subfield literary market succeed explore substructure describe literary study topic kind literature commodity nusser nusser nutz stockinger astonishing genre behave differently result decade old competition small number probably different structure correspond different strategy publisher bastei lübbe example follow strategy series distinct profile cora focusse publisher brand fig fig clustering influence fact cora translate novel english interesting follow project find reader genre know structure knowledge direct choice think strategy control known unknown confound variable text classification especially understand exist structure predict new datum need explore detail like thank reviewer provide detailed informative feedback especially relation datum leakage confound variable evaluation dimension reduction technique,"[('genre', 0.4322218447688069), ('publisher', 0.4149612628293379), ('novel', 0.2842608015209231), ('cora', 0.16855058798274), ('series', 0.15545713430120922), ('clear', 0.14616718107137586), ('love', 0.13445395036556326), ('variable', 0.13445395036556326), ('market', 0.12728932992940276), ('combination', 0.12180598422614655)]"
2020,DHd2020,148_final-DU_Keli_Der_Spielraum_zwischen__zu_wenig__und__zu_viel_.xml,"Der Spielraum zwischen ""zu wenig"" und ""zu viel""","Keli Du (Universität Würzburg, Deutschland)","Topic Modeling, Evaluation, Topics-Anzahl","Entdeckung, Inhaltsanalyse, Methoden, Text","Als eine quantitative textanalytische Methode wurde Topic Modeling  Außerdem, wenn Topic Modeling für Forschung in Digital Humanities eingesetzt wird, interagieren die Benutzer normalerweise direkt mit Topics. Deshalb sind die standardmäßigen internen Evaluationsmethoden  Das Korpus der Untersuchung besteht aus 2000 deutschen Zeitungsartikeln zwischen 2001 und 2014  Vor der Topic Modeling basierten Dokument-Klassifikation wurde Bag-of-Words (BoW) basierte Klassifikation zuerst durchgeführt, um eine Baseline der Klassifikation zu definieren. Die Tests erfolgten auch als 10-fache Kreuzvalidierung mit linearer SVM ",de,quantitativ textanalytisch Methode Topic Modeling Topic Modeling Forschung Digital Humanitie einsetzen interagieren Benutzer normalerweise direkt Topics standardmäßig intern evaluationsmethoden Korpus Untersuchung bestehen deutsch Zeitungsartikel Topic Modeling basiert Bow basiert Klassifikation durchführen baselin Klassifikation definieren Test erfolgn Kreuzvalidierung Linearer svm,"[('modeling', 0.36969312667908344), ('basiert', 0.34613947450239263), ('topic', 0.3420899298299018), ('erfolgn', 0.2125659686008172), ('klassifikation', 0.21155308295700695), ('linearer', 0.19798908476361388), ('bow', 0.19798908476361388), ('zeitungsartikel', 0.19798908476361388), ('standardmäßig', 0.19798908476361388), ('evaluationsmethoden', 0.19798908476361388)]"
2020,DHd2020,248_final-PIELSTR_M_Steffen_Metadaten_basierte_Visualisierungen_im_Sti.xml,"Metadaten-basierte Visualisierungen im Stilometrie-Paket ""Stylo""","Steffen Pielström (Julius-Maximilians-Universität Würzburg, Deutschland); Maciej Eder (Pedagogical University of Kraków, Polen)","Stilometrie, Software, R, stylo","Stilistische Analyse, Visualisierung, Literatur, Text","Dabei ist Ein Aspekt, der immer wieder zu Nachfragen von Usern geführt hat, ist der Umgang mit Metadaten in der durch die Community wohl am häufigsten genutzte Die Informationen über die Gruppenzugehörigkeit eines Textes entnimmt Bislang war die systematische Benennung der Textdateien der einzige Weg, solche Information zur Gruppenzugehörigkeit an die Funktion zu übermitteln. Von Nutzerweite wurde immer wieder der Wunsch nach zusätzlichen Möglichkeiten geäußert, Metadaten zur Gruppenzugehörigkeit der Texte an die Funktion zu übergeben. In den neueren Die Funktion akzeptiert sowohl Faktor als auch einen Vektor von Strings als Gruppierungsvariable. Die andere Möglichkeit ist, die Information zur Gruppenzugehörigkeit der Texte in einer CSV-Datei zu hinterlegen und dem Parameter den Dataipfad als String zu übergeben. Die betreffende CSV-Datei enthält eine Spalte mit der Überschrift ""filename"", die alle Dateinamen des Corpus in alphabetischer Reihenfolge enthält, und mindestens eine weitere Spalte mit Gruppenlabels. Um die Spalte mit der gewünschten Gruppierungsvariable auszuwählen wird der Titel der gewünschten Spalte an den Funktionsparamter Der Default-Wert ist ""author"". Wenn dem Paramter Dieser zusätzliche Parameter in der",de,Aspekt Nachfrag usern führen Umgang metadaten Community häufig genutzen Information Gruppenzugehörigkeit Text entnehmen bislang systematisch Benennung Textdateien einzig Weg Information Gruppenzugehörigkeit Funktion übermitteln Nutzerweite Wunsch zusätzlich Möglichkeit äußern metadaen Gruppenzugehörigkeit Text Funktion übergeben neu Funktion akzeptieren sowohl Faktor Vektor String Gruppierungsvariable Möglichkeit Information Gruppenzugehörigkeit Text hinterlegen Parameter Dataipfad String übergeben betreffend enthalten Spalte Überschrift Filenam dateinamen Corpus alphabetisch Reihenfolge enthalten mindestens spalen gruppenlabels Spalt gewünscht Gruppierungsvariable auswählen Titel gewünscht spalen Funktionsparamter Author Paramter zusätzlich Parameter,"[('gruppenzugehörigkeit', 0.5020318044935698), ('gruppierungsvariable', 0.23380228299988864), ('gewünscht', 0.22158902577919107), ('übergeben', 0.21211568749949283), ('string', 0.20437540653229486), ('spalen', 0.20437540653229486), ('funktion', 0.18114713414595274), ('information', 0.1327120903571915), ('parameter', 0.12830803435021101), ('nutzerweite', 0.12550795112339244)]"
2020,DHd2020,113_final-WEIMER_Lukas_Stilometrische_Untersuchung_von_Figurenreden_in.xml,Stilometrische Untersuchung von Figurenreden in realistischen Erzähltexten,"Lukas Weimer (Julius-Maximilians-Universität Würzburg, Deutschland)","Korpus, Annotation, Figurenrede, Stilometrie","Sammlung, Annotieren, Stilistische Analyse, Literatur","Das Poster stellt ein Korpus deutschsprachiger Erzählungen des 19. Jahrhunderts vor, in dem Figurenreden und ihre jeweiligen Sprecher annotiert und extrahiert wurden. Sie dienen als Basis für stilistische Auswertungen mit dem etablierten Abstandsmaß Delta. Es stellt sich die Frage, ob sich der Autorenstil in den jeweiligen Figurenreden niederschlägt, sich also Figuren desselben Autors zusammengruppieren, oder ob Figurentypen dominanter sind, sich gleiche Figurentypen also werkübergreifend stilistisch ähneln. Erste Ergebnisse hiervon werden als Grafiken präsentiert. Stilometrische Verfahren gehen v.a. auf John Burrows zurück. Sein entwickeltes Abstandsmaß Das Korpus setzt sich aus acht realistischen Erzähltexten zwischen 1848 und 1871 zusammen, da dieser Zeitraum allgemein als Kernzeit des Realismus anerkannt ist (Aust 2006, Plumpe 2007). Um Vergleiche zu ermöglichen, enthält das Korpus zusätzlich drei Erzähltexte von vor 1848. Die Korpusauswahl beruht auf einem mehrschrittigen Prozess: Mit der Längenbegrenzung von 8.000-20.000 Wörtern wurde darauf geachtet, dass die Erzählungen einerseits lang genug sind, um stilometrische Verfahren anwenden zu können und andererseits kurz genug, um die manuelle Annotation in einem angemessenen zeitlichen Rahmen durchzuführen. Außerdem wurde darauf geachtet, sowohl kanonisierte als auch gänzlich unbekannte Texte zu integrieren, weibliche Autoren ins Korpus aufzunehmen und die Erstpublikationsorgane zu variieren. Wie in der damaligen Zeit üblich, wurde ein Großteil der Erzählungen in Zeitschriften, Almanachen oder Taschenbüchern veröffentlicht. Diese waren auf ganz verschiedene Leserschichten ausgerichtet, so dass eine Variation hier alle Stilniveaus erfassen sollte. Die Korpustexte sind die folgenden elf Erzählungen: Da einige der Texte noch nicht erschlossen waren, wurden sie vor der Annotation OCR-korrigiert. Für die Annotation wurde der im Zuge des Redewiedergabe-Projekts (Brunner et al. 2018) entstandene STWR-View des Annotationstools ATHEN (Krug et al. 2018) verwendet. Bei der Annotation wurden sämtliche direkten Figurenreden manuell annotiert und ihrem jeweiligen Sprecher zugeordnet (zur automatischen Zuordnung von Sprechern: Krug et al. 2016). So konnte die gesamte direkte Redemenge einzelner Figuren extrahiert werden. In direkte Reden einer Figur A eingelagerte Reden einer Figur B wurden dabei nur der Figur B als zugehörig annotiert. Auf diese Weise wurde sichergestellt, dass Figuren ausschließlich ihre eigenen Reden zugeordnet wurden (diese Problematik ist besonders relevant bei Binnenerzählungen). Zusätzlich wurden ausschließlich Figuren in die Auswertung integriert, deren gesamte Redemenge 200 Wörter übersteigt, um stilometrische Verfahren wirksam anwenden zu können. Diese Grenze ist für stilometrische Verfahren noch immer vergleichsweise niedrig. Eder (2015) hat evaluiert, dass korpusabhängig mindestens 2500-5000 Wortformen nötig sind, damit Auswertungen mit Delta zu guten Ergebnissen führen. Aufgrund des Korpus dieser Studie kann dieser Mindestwert allerdings nicht eingehalten werden. Die folgenden Grafiken zeigen den Output des R-package Auswertung mit 100 häufigsten Wörtern: Auswertung mit 1000 häufigsten Wörtern: In beiden Auswertungen ist zu erkennen, dass sich häufig Figuren desselben Autors zueinander gliedern. Besonders beim Mundartdichter Gotthelf ( Im weiteren Verlauf der Arbeit müssen die Maße verfeinert und sollen andere Abstandsmaße getestet, Variablen geändert und Ergebnisse evaluiert werden. Die Problematik der Kürze der Texte könnte durch eine Optimierung des Verfahrens verringert werden. So könnten eine Kombination aus Wortform-",de,Poster stellen Korpus deutschsprachig Erzählung Jahrhundert figurenred jeweilig Sprecher annotiert extrahiern dienen Basis stilistisch Auswertung etabliert Abstandsmaß Delta stellen Frage Autorenstil jeweilig figurenred niederschlägen figur Autor zusammengruppieren Figurentype dominant gleich Figurentype werkübergreifend stilistisch ähneln Ergebnis hiervon grafiken präsentieren stilometrisch Verfahren John burreoswn entwickelt abstandsmaß korpus setzen realistisch Erzähltext Zeitraum allgemein Kernzeit Realismus anerkennen Aust Plumpe Vergleich ermöglichen enthalten Korpus zusätzlich Erzähltext Korpusauswahl beruhen Mehrschrittigen prozess Längenbegrenzung wörtern achten erzählungen einerseits stilometrisch Verfahren anwenden andererseits Manuelle Annotation angemessen zeitlich Rahmen durchführen achten sowohl kanonisiert gänzlich unbekannt Text integrieren weiblich Autor Korpus aufnehmen erstpublikationsorgane variieren damalig üblich Großteil Erzählung Zeitschrift almanachen Taschenbücher veröffentlichen verschieden leserschichten ausrichten Variation Stilniveau erfassen Korpustext folgend erzählungen Text erschließen Annotation Annotation Zug Brunner et entstanden Annotationstool athen krug et verwenden Annotation sämtlicher direkt figurenred manuell annotieren jeweilig Sprecher zuordnen automatisch Zuordnung Sprecher krug et gesamt direkt Redemenge einzeln Figur extrahieren direkt Rede Figur eingelagert reden Figur b Figur b zugehörig annotiert Weise sicherstellen Figur ausschließlich Rede zuordnen Problematik relevant Binnenerzählung zusätzlich ausschließlich Figur Auswertung integrieren gesamt Redemenge Wörter übersteigen stilometrisch Verfahren wirksam anwenden Grenze stilometrisch Verfahren vergleichsweise niedrig Eder evaluieren korpusabhängig mindestens Wortforme nötig Auswertung Delta gut Ergebnis führen aufgrund Korpus Studie mindestweren einhalten folgend grafiken zeigen Output Auswertung häufig wörtern Auswertung häufig wörtern Auswertung erkennen häufig figur Autor Zueinander gliedern Mundartdichter gotthelf Verlauf Arbeit Maß verfeinern Abstandsmaße testen variablen ändern Ergebnis evaluieren Problematik Kürze Text Optimierung verfahren verringern können Kombination,"[('figur', 0.25715071701260667), ('auswertung', 0.23224770248166757), ('stilometrisch', 0.2103419028693273), ('figurenred', 0.19177849855872028), ('sprecher', 0.16959602276856778), ('redemenge', 0.16222408825793586), ('grafiken', 0.16222408825793586), ('abstandsmaß', 0.15109943972789333), ('verfahren', 0.1429464406192214), ('figurentype', 0.13208172745346924)]"
2020,DHd2020,116_final-KREMER_Gerhard_Maschinelles_Lernen_lernen__Ein_CRETA_Hackato.xml,Maschinelles Lernen lernen: Ein CRETA-Hackatorial zur reflektierten automatischen Textanalyse,"Gerhard Kremer (Universität Stuttgart, Deutschland); Kerstin Jung (Universität Stuttgart, Deutschland)","Entitäten, Entitätenreferenzen, maschinelle Lernverfahren, Evaluation, Python, Programmiercode, Annotationen","Programmierung, Annotieren, Bewertung, Software, Text, Werkzeuge","Das Ziel dieses Tutorials ist es, den Teilnehmenden konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten. Es werden dabei keinerlei Vorkenntnisse zu maschinellem Lernen oder Programmierkenntnisse vorausgesetzt. Es gibt keinen Grund, den Ergebnissen von maschinellen Lernverfahren im Allgemeinen und NLP-Tools im Besonderen blind zu vertrauen. Durch die konkreten Einblicke in den ""Maschinenraum"" von maschinellen Lernverfahren wird den Teilnehmenden ermöglicht, das Potenzial und die Grenzen statistischer Textanalysewerkzeuge realistischer einzuschätzen. Mittelfristig hoffen wir dadurch, den immer wieder auftretenden Frustrationen beim Einsatz automatischer Verfahren für die Textanalyse und deren teilweise wenig zufriedenstellender Ergebnis-Daten zu begegnen, aber auch die Nutzung und Interpretation der Ergebnisse von maschinellen Lernverfahren (d.h. in erster Linie von automatisch erzeugten Annotationen) zu fördern. Zu deren adäquater Nutzung, etwa in hermeneutischen Interpretationsschritten, ist der Einblick in die Funktionsweise der maschinellen Methoden unerlässlich. Insbesondere ist die Art und Herkunft der Trainingsdaten für die Qualität der maschinell produzierten Daten von Bedeutung, wie wir im Tutorial deutlich machen werden. Neben einem Python-Programm für die automatische Annotierung von Entitätenreferenzen, mit und an dem während des Tutorials gearbeitet werden wird, stellen wir ein heterogenes, manuell annotiertes Korpus sowie die Routinen zur Evaluation und zum Vergleich von Annotationen zu Verfügung. Das Korpus enthält Entitätenreferenzen, die im ""Center for Reflected Text Analytics"" (CRETA) Als empirisches Phänomen befassen wir uns mit dem Konzept der Entität und ihrer Referenz. Das Konzept steht für verschiedene linguistische und semantische Kategorien, die im Rahmen der Digital Humanities von Interesse sind. Es ist bewusst weit gefasst und damit anschlussfähig für verschiedene Forschungsfragen aus den geistes- und sozialwissenschaftlichen Disziplinen. Auf diese Weise können unterschiedliche Perspektiven auf Entitäten berücksichtigt werden. Insgesamt werden in den ausgewählten Texten fünf verschiedene Entitätenklassen betrachtet: PER (Personen/Figuren), LOC (Orte), ORG (Organisationen), EVT (Ereignisse) und WRK (Werke). Unter Entitätenreferenzen verstehen wir Ausdrücke, die auf eine Entität in der realen oder fiktiven Welt referieren. Das sind zum einen Eigennamen (Named Entities, z.B. ""Peter""), zum anderen Gattungsnamen (z.B. ""der Bauer""), sofern diese sich auf eine konkrete Instanz der Gattung beziehen. Dabei wird als Referenzausdruck immer die maximale Nominalphrase (inkl. Artikel, Attribut) annotiert. Pronominale Entitätenreferenzen werden hingegen nicht annotiert. In In den Die Grundlage für (überwachte) maschinelle Lernverfahren bilden Annotationen. Um die Annotierung von Entitätenreferenzen automatisieren zu können, bedarf es Textdaten, die die Vielfalt des Entitätenkonzepts abdecken. Bei diesem Tutorial werden wir auf Annotationen zurückgreifen, die im Rahmen von CRETA an der Universität Stuttgart entstanden sind (vgl. Blessing et al., 2017; Reiter et al., 2017a). Das Korpus enthält literarische Texte aus zwei Sprachstufen des Deutschen (Neuhochdeutsch und Mittelhochdeutsch) sowie ein sozialwissenschaftliches Teilkorpus. Der  Das Der Ablauf des Tutorials orientiert sich an sog. Zunächst diskutieren wir kurz die zugrundeliegenden Texte und deren Annotierung. Annotationsrichtlinien werden den Teilnehmerinnen und Teilnehmern im Vorfeld zur Verfügung gestellt. Im Rahmen der Einführung wird auch auf die konkrete Organisation der Annotationsarbeit eingegangen, so dass das Tutorial als Blaupause für zukünftige Tätigkeiten der Teilnehmenden in diesem und ähnlichen Arbeitsfeldern dienen kann. Die Teilnehmerinnen und Teilnehmer versuchen selbständig und unabhängig voneinander, eine Kombination aus maschinellen Lernverfahren, Merkmalsmenge und Parametersetzungen zu finden, die auf einem neuen, vom automatischen Lernverfahren ungesehenen Datensatz zu den Ergebnissen führt, die dem Goldstandard der manuellen Annotation am Öhnlichsten sind. Das bedeutet konkret, dass der Einfluss von berücksichtigten Features (z.B. Groß- und Kleinschreibung oder Wortlänge) auf die Erkennung von Entitätenreferenzen empirisch getestet werden kann. Dabei sind Intuitionen über die Daten und das annotierte Phänomen hilfreich, da simplem Durchprobieren aller möglichen Kombinationen (""brute force"") zeitlich Grenzen gesetzt sind.Zusätzlich werden bei jedem Testlauf Informationen über die Entscheidungen protokolliert, um die Erklärbarkeit der Ergebnisse zu unterstützen. Wir verzichten bewusst auf eine graphische Benutzerschnittstelle (vgl. Reiter et al., 2017b) 'stattdessen editieren die Teilnehmerinnen und Teilnehmer das (Python)-Programm direkt, nach einer Einführung und unter Anleitung. Vorkenntnisse in Python sind dabei nicht nötig: Das von uns zur Verfügung gestellte Programm ist so aufgebaut, dass auch Python-Neulinge relativ schnell die zu bearbeitenden Teile davon verstehen und damit experimentieren können. Wer bereits Erfahrung im Python-Programmieren hat, kann fortgeschrittene Funktionalitäten des Programms verwenden. Wie am Ende jedes maschinellen Lernprozesses wird auch bei uns abschließend eine Evaluation der automatisch generierten Annotationen durchgeführt. Hierfür werden den Teilnehmerinnen und Teilnehmern nach Ablauf einer begrenzten Zeit des Experimentierens und Testens (etwa 60 Minuten) die finalen, vorher unbekannten Testdaten zur Verfügung gestellt. Auf diese Daten werden die erstellten Modelle angewendet, um automatisch Annotationen zu erzeugen. Diese wiederum werden dann mit dem Goldstandard verglichen, wobei die verschiedenen Entitätenklassen sowie Teilkorpora getrennt evaluiert werden. Auch das Programm zur Evaluation stellen wir bereit. Am hier verwendeten Beispiel der automatischen Annotation von Entitätenreferenzen demonstrieren wir, welche Schritte für die Automatisierung einer Textanalyseaufgabe mittels maschinellen Lernverfahren nötig sind und wie diese konkret implementiert werden können. Die Teilnehmenden des Workshops bekommen einen zusammenhängenden Überblick von der manuellen Annotation ausgewählter Texte über die Feinjustierung der Lernverfahren bis zur Evaluation der Ergebnisse. Die vorgestellte Vorgehensweise für den gesamten Ablauf ist grundsätzlich auf ähnliche Projekte übertragbar. Das Tutorial schärft dabei das Verständnis für den Zusammenhang zwischen untersuchtem Konzept und den dafür relevanten Features, die in ein statistisches Lernverfahren einfließen. Durch Einblick in die technische Umsetzung bekommen die Teilnehmerinnen und Teilnehmer ein Verständnis für die Grenzen und Möglichkeiten der Automatisierung, das sie dazu befähigt, zum einen das Potenzial solcher Verfahren für eigene Vorhaben realistisch(er) einschätzen zu können, zum anderen aber auch Ergebnisse, die auf Basis solcher Verfahren erzielt wurden, angemessen hinterfragen und deuten zu können. Neben diesem CRETA-Hackatorial befindet sich noch ein weiterer Workshop des Stuttgarter DH-Zentrums CRETA in Begutachtung. Auch wenn es eine gewisse Schnittmenge zwischen den Workshops gibt (Textgrundlagen, Anwendungsfälle), ist die jeweilige Zielsetzung grundsätzlich verschieden: Während es beim hier vorgestellten CRETA-Hackatorial um Verfahren des Maschinellen Lernens geht, konzentriert sich der parallel ausgearbeitete CRETA-Workshop auf den grundlegenderen Schritt der Operationalisierung 'es geht also darum, Ansätze aufzuzeigen, wie ein Untersuchungsvorhaben oder theoretisches Konzept überhaupt für die computergestützte Analyse ""vor- bzw. aufbereitet"" werden kann. Beide Workshops ergänzen einander sinnvoll, was die Teilnahme an beiden oder an nur einem der Workshops möglich macht. Im Vorfeld der Veranstaltung: Installationsanweisungen und Support Der Workshop wird ausgerichtet von Mitarbeitenden des ""Center for Reflected Text Analytics"" (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine ""black box"" sein, sondern auch für Nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird. Der Interessenschwerpunkt Gerhard Kremers ist der reflektierte Einsatz von Werkzeugen der Computerlinguistik für geistes- und sozialwissenschaftliche Fragestellungen. Damit zusammenhängend gehören die Entwicklung übertragbarer Arbeitsmethoden und die angepasste, nutzerfreundliche Bedienbarkeit automatischer linguistischer Analysetools zu seinen Forschungsthemen. Kerstin Jungs Forschungsinteressen liegen im Bereich der Nachhaltigkeit von (computer)linguistischen Ressourcen und Abläufen sowie der Verlässlichkeitsbeschreibung von automatisch erzeugten Annotationen. Dabei verfolgt sie einen aufgabenbasierten Ansatz und arbeitet an der Schnittstelle zwischen Computerlinguistik und anderen sprach- und textverarbeitenden Disziplinen. Zwischen 15 und 25. Es wird außer einem Beamer und ausreichend Stromanschlüssen für die Laptops der Teilnehmenden keine besondere technische Ausstattung benötigt. Es sollte sich um einen Raum handeln, in dem genügend Platz ist, durch die Reihen zu gehen und den Teilnehmenden über die Schulter zu blicken.",de,Ziel Tutorial teilnehmend konkret praktisch einblicke Standardfall automatisch Textanalyse geben automatisch Erkennung Entitätenreferenze allgemein annehmen verfahrensweisen methodisch Standard maschinell lernverfahren Teilnehmerinn Teilnehmer bearbeiter lauffähig Programmiercode entscheidungsraum Verfahren ausleuchten austesten keinerlei Vorkenntnisse Maschinellem lernen Programmierkenntnisse voraussetzen Grund Ergebnis maschinell lernverfahren besonderer Blind vertrauen konkret einblick maschinenraum maschinell lernverfahren Teilnehmend ermöglichen Potenzial Grenze statistisch Textanalysewerkzeug Realistischer einschätzen mittelfristig hoffen auftretende Frustration Einsatz automatisch Verfahren Textanalyse teilweise zufriedenstellend begegnen Nutzung Interpretation Ergebnis Maschinelle lernverfahren Linie automatisch erzeugt Annotation fördern adäquat Nutzung hermeneutisch interpretationsschritten Einblick Funktionsweise maschinell Methode Unerlässlich insbesondere Art Herkunft Trainingsdat Qualität maschinell produziert daten Bedeutung Tutorial deutlich automatisch Annotierung entitätenreferenzen tutorials arbeiten stellen heterogen manuell annotiert korpus Routin Evaluation Vergleich Annotation Verfügung korpus enthalten entitätenreferenzen center for reflected Text analytics creta empirisch Phänomen befassen Konzept Entität Referenz Konzept stehen verschieden linguistisch semantisch Kategorie Rahmen Digital Humanitie Interesse bewusst fassen anschlussfähig verschieden Forschungsfrag sozialwissenschaftlich Disziplin Weise unterschiedlich Perspektive entitäten berücksichtigen insgesamt ausgewählt Text verschieden Entitätenklasse betrachten per Person Figur loc ort org organisation Evt Ereignis wrk werk Entitätenreferenze verstehen ausdrücken Entität real fiktiv Welt referieren Eigennam named entities Peter Gattungsnamen Bauer sofern konkret Instanz Gattung beziehen Referenzausdruck maximal Nominalphrase Artikel Attribut annotiert pronominale entitätenreferenzen hingegen annotiert Grundlage überwacht maschinell lernverfahren bilden Annotation Annotierung Entitätenreferenze automatisieren bedürfen textdan Vielfalt Entitätenkonzept abdecken Tutorial annotation zurückgreifen Rahmen Creta Universität Stuttgart entstehen blessing et Reiter et korpus enthalten literarisch Text Sprachstuf deutsch neuhochdeutsch mittelhochdeutsch sozialwissenschaftlich Teilkorpus Ablauf tutorials orientieren diskutieren zugrundeliegend Text Annotierung Annotationsrichtlini Teilnehmerinn Teilnehmer Vorfeld Verfügung stellen Rahmen Einführung konkret Organisation Annotationsarbeit eingehen Tutorial Blaupause zukünftig Tätigkeit Teilnehmende ähnlich Arbeitsfelder dienen Teilnehmerinn Teilnehmer versuchen selbständig unabhängig voneinander Kombination Maschinellen lernverfahren merkmalsmenge Parametersetzunge finden automatisch lernverfahren ungesehen Datensatz ergebnissen führen Goldstandard manuell Annotation öhnlichsten bedeuten konkret einfluss berücksichtigt Features Kleinschreibung wortlängen Erkennung Entitätenreferenze empirisch testen intuitioner daten annotiert Phänomen hilfreich simpl Durchprobier möglich Kombination brute force zeitlich Grenze setzen testlauf Information Entscheidung protokollieren Erklärbarkeit Ergebnis unterstützen verzichten bewussen graphisch Benutzerschnittstelle Reiter et stattdessen editieren Teilnehmerinn Teilnehmer direkt Einführung Anleitung Vorkenntnisse Python nötig Verfügung gestellt Programm aufbauen relativ schnell bearbeitenden Teil verstehen experimentieren Erfahrung fortgeschritten funktionalitäten Programm verwenden jeder maschinell Lernprozesses abschließend Evaluation automatisch generiert Annotation durchführen hierfür Teilnehmerinn Teilnehmer Ablauf begrenzt experimentierens Testen Minute Finale vorher unbekannt Testdate Verfügung stellen daten erstellt Modell anwenden automatisch annotation erzeugen wiederum Goldstandard vergleichen wobei verschieden Entitätenklasse Teilkorpora trennen evaluieren Programm Evaluation stellen bereit verwendet automatisch Annotation Entitätenreferenze demonstrieren Schritt Automatisierung Textanalyseaufgabe mittels maschinell lernverfahren nötig konkret implementieren Teilnehmende Workshop bekommen zusammenhängend Überblick manuell Annotation ausgewählter Text Feinjustierung lernverfahren Evaluation Ergebnis vorgestellt vorgehensweise gesamt Ablauf grundsätzlich ähnlich Projekt übertragbar Tutorial schärfen Verständnis Zusammenhang untersucht Konzept relevant Feature statistisch lernverfahren einfließen Einblick technisch Umsetzung bekommen Teilnehmerinn Teilnehmer Verständnis Grenze Möglichkeit Automatisierung befähigen Potenzial Verfahren Vorhaben realistisch einschätzen Ergebnis Basis Verfahren erzielen angemessen Hinterfrag deuten befinden weit Workshop Stuttgarter Creta Begutachtung gewiß Schnittmenge Workshops Textgrundlage anwendungsfäll jeweilig Zielsetzung grundsätzlich verschieden vorgestellt Verfahren maschinell Lernen konzentrieren parallel ausgearbeitet grundlegender Schritt Operationalisierung Ansatz aufzuzeigen untersuchungsvorhab theoretisch Konzept computergestützt Analyse aufbereitet Workshop ergänzen sinnvoll Teilnahme Workshops Vorfeld Veranstaltung Installationsanweisunge Support Workshop ausrichten Mitarbeitende center for reflected Text analytics Creta Universität Stuttgart Creta verbinden Literaturwissenschaft Linguistik Philosophie Sozialwissenschaft Maschineller Sprachverarbeitung Visualisierung Hauptaufgabe Creta Entwicklung reflektiert Methode Textanalyse wobei methode Gesamtpaket konzeptuell Rahmen annehmen technisch Implementierung Interpretationsanleitung verstehen Methode black box transparent reflektiert Einsatz Hinblick sozialwissenschaftlich Fragestellung Interessenschwerpunkt Gerhard Kremer reflektiert Einsatz Werkzeug Computerlinguistik sozialwissenschaftlich Fragestellung zusammenhängend gehören Entwicklung übertragbar arbeitsmethoden angepas nutzerfreundlich Bedienbarkeit automatisch linguistisch Analysetools Forschungstheme kerstin jungs Forschungsinteresse liegen Bereich Nachhaltigkeit Computer linguistisch Ressource abläufen Verlässlichkeitsbeschreibung automatisch erzeugt Annotation verfolgen aufgabenbasiert Ansatz arbeiten Schnittstelle Computerlinguistik Textverarbeitende disziplinen Beamer ausreichend Stromanschlüsse laptops Teilnehmende besonderer technisch Ausstattung benötigen Raum handeln genügend Platz Reihe Teilnehmend Schulter blicken,"[('lernverfahren', 0.2791762401657727), ('teilnehmerinn', 0.2162170561327592), ('creta', 0.19966748636925813), ('maschinell', 0.18889144600455526), ('teilnehmer', 0.1874938930960501), ('entitätenreferenze', 0.1861413198921147), ('tutorial', 0.1750177123220262), ('automatisch', 0.1685010386269507), ('annotation', 0.14664371604743376), ('sozialwissenschaftlich', 0.1363707804565662)]"
2020,DHd2020,163_final-HORSTMANN_Jan_Annotieren__Analysieren__Visualisieren___Einf_.xml,"Annotieren, Analysieren, Visualisieren 'Einführung in CATMA 6","Jan Horstmann (Universität Hamburg, Deutschland); Jan Christoph Meister (Universität Hamburg, Deutschland); Marco Petris (Universität Hamburg, Deutschland); Mareike Schumacher (Universität Hamburg, Deutschland); Marie Flüh (Universität Hamburg, Deutschland)","kollaboratives Arbeiten, Interpretation, Hermeneutik","Inhaltsanalyse, Strukturanalyse, Annotieren, Organisation, Visualisierung, Literatur","Der ohne technische Vorkenntnisse besuchbare hands-on Workshop bildet eine Einführung in die Möglichkeiten der für Geisteswissenschaftler√Ønnen entwickelten, webbasierten Annotations- und Analyseplattform CATMA, deren sechste Version im Oktober 2019 veröffentlicht wurde. Im Zentrum stehen theoretische und praktische Aspekte der digitalen Annotation von (literarischen) Texten sowie die Analyse und Visualisierung dieser Texte und der erstellten Annotationen. CATMA ( CATMA unterstützt... Von linguistischen Textanalysetools unterscheidet sich CATMA insbesondere durch seinen ""undogmatischen"" Ansatz: Das System schreibt mit seiner hermeneutischen Annotation (vgl. Piez 2010) weder definierte Annotationsschemata oder -regeln vor, noch erzwingt es die Verwendung von starren Ja-/Nein- oder Richtig-/Falsch-Taxonomien. Wenn eine Textstelle mehrere Interpretationen zulässt (wie es in literarischen Texten häufig der Fall ist), ist es in CATMA (durch die Nutzung von Standoff-Markup) daher möglich, mehrere und sogar widersprechende Annotationen zu vergeben und so der Bedeutungsvielfalt der Texte Rechnung zu tragen. Mit der Die seit Jahrhunderten zu den textwissenschaftlichen Kernpraktiken gehörende Annotation (vgl. Moulin 2010) lässt sich in sog. Highlights, Freitextkommentare sowie taxonomiebasierte Annotation und Textauszeichnung aufteilen, wobei die Übergänge häufig fließend sind (vgl. Jacke 2018, ¬ß 9). Während CATMA 6 auch die Möglichkeit für Highlights und Freitextkommentare bietet, ist die taxonomiebasierte Annotation das eigentliche Kerngeschäft des Tools 'wobei die Taxonomie prinzipiell undogmatisch erstellt werden kann und die Form von sog. Tagsets annimmt, denen für kollaborative Annotationsprojekte wahlweise eine Annotations-Guideline beigegeben werden kann (vgl. auch Bögel et al). Im Workshop werden wir den Unterschied von Neben der Annotation sind die Analyse und Visualisierung der Text- und Annotationsdaten das andere wichtige Standbein von CATMA. Hier wird Neben diesen grundlegenden Funktionen, die alle per Klick ausgeführt werden können, bietet CATMA die sog. Im Analysebereich können außerdem halbautomatische Annotationen erstellt werden, d.¬†h. man annotiert wiederkehrende Wörter oder Wortgruppen auf einmal mit einem bestimmten Tag, statt dies manuell und wiederholt im Annotationsmodul zu tun. Der Wechsel zwischen der Arbeit im Annotations- und Analyse- und Visualisierungs-Modul ist ein iterativer Prozess, der die klassisch-zirkuläre hermeneutische Interpretationsarbeit in der Literaturwissenschaft widerspiegelt (vgl. Gius, im Erscheinen). Im Workshop werden wir uns in abwechselnden Präsentations- und Hands-on-Phasen der textanalytischen Arbeit in CATMA 6 nähern. Nach einer generellen Einführung in das Tool werden die Teilnehmer√Ønnen anhand eines vorgegebenen Beispieltextes den gesamten Workflow von der individuellen taxonomiebasierten Textannotation, über die Analyse hin zur Visualisierung und Interpretation der Text- und Annotationsdaten kennenlernen und praktisch erproben können. Die Teilnehmer√Ønnen sollen ausgehend vom digitalen Text in die Lage versetzt werden, Annotationen manuell und automatisch unterstützt zu erstellen und in Annotation Collections zu speichern, Tagsets/Taxonomien zu entwickeln und den Text alleine und in Kombination mit den Annotationen zu analysieren und zu visualisieren. Für kritische Reflektionen, Diskussionen sowie individuelle Rückfragen (theoretischer, praktischer und technischer Art) auf jedem Niveau und in Bezug auf die Projekte der Teilnehmer√Ønnen wird ausreichend Möglichkeit bestehen. Im Workshop werden wir anhand von Kafkas Erzählung Jan Horstmann ist Postdoc und koordiniert das DFG-Projekt forTEXT ( Jan Christoph Meister ist Professor für Digital Humanities mit dem   Schwerpunkt Literaturwissenschaft. Als ursprünglicher Erfinder von   CATMA hat er etliche Forschungsprojekte zur Annotation und   Visualisierung textueller Daten und der Entwicklung und Verbesserung   von DH-Tools geleitet. Marco Petris ist Informatiker mit starker Affinität zu   geisteswissenschaftlichen Fragestellungen. Er ist von Anfang an an   der Entwicklung von CATMA beteiligt und beschäftigt sich mit allen   Aspekten der DH-Toolentwicklung, des Tool-Designs und der   Implementierung. Mareike Schumacher promoviert als digitale Literaturwissenschaftlerin über Orte und narratologische Ortskategorien in literarischen Texten, beschäftigt sich besonders mit den Methoden des Marie ist Master of Education und interessiert sich besonders für Christoph Martin Wieland und seine Zeitgenossen. Außerdem liegt ihr Forschungsschwerpunkt auf der Wertung von Literatur. Sie studierte in Kiel und Hamburg und ist nun wissenschaftliche Mitarbeiterin an der Universität Hamburg. Bis zu 30 Personen. Teilnehmer√Ønnen bringen ihren eigenen Laptop mit, der mit dem Internet verbunden ist (",de,technisch Vorkenntnisse besuchbar Workshop bilden Einführung Möglichkeit entwickelt webbasiert Analyseplattform Catma Version Oktober veröffentlichen Zentrum stehen theoretisch praktisch Aspekt digital Annotation literarisch Text Analyse Visualisierung Text erstellt Annotation Catma Catma unterstützen linguistisch Textanalysetools unterscheiden Catma insbesondere undogmatisch Ansatz System schreiben hermeneutisch Annotation Piez weder definiert Annotationsschemata erzwingen Verwendung Starr Textstelle mehrere Interpretation zulässt literarisch Text häufig Fall Catma Nutzung mehrere sogar widersprechend Annotation vergeben Bedeutungsvielfalt Text Rechnung tragen Jahrhundert textwissenschaftlich Kernpraktik gehörend Annotation Moulin lässen highlights Freitextkommentar taxonomiebasiert Annotation Textauszeichnung aufteilen wobei Übergänge häufig fließend Jacke Catma Möglichkeit Highlights Freitextkommentar bieten taxonomiebasiert Annotation eigentlich Kerngeschäft Tool wobei Taxonomie prinzipiell undogmatisch erstellen Form Tagset annehmen kollaborativ Annotationsprojekt wahlweise beigegeben Bögel et al Workshop Unterschied Annotation Analyse Visualisierung annotationsdan wichtig Standbein Catma grundlegend Funktion per Klick ausführen bieten Catma analysebereich halbautomatisch Annotation erstellen annotieren Wiederkehrende Wörter Wortgruppe bestimmt manuell wiederholt Annotationsmodul Wechsel Arbeit iterativ Prozess hermeneutisch Interpretationsarbeit Literaturwissenschaft widerspiegeln Gius erscheinen Workshop abwechselnd textanalytisch Arbeit Catma nähern generell Einführung Tool anhand vorgegeben beispieltext gesamt Workflow individuell Taxonomiebasierte Textannotation Analyse Visualisierung Interpretation annotationsdat Kennenlern praktisch erproben ausgehend digital Text Lage versetzen annotation manuell automatisch unterstützen erstellen Annotation Collection speichern Tagset Taxonomien entwickeln Text alleine Kombination Annotation analysieren visualisieren kritisch reflektion Diskussion individuell Rückfrage theoretisch praktisch technisch Art Niveau Bezug Projekt ausreichend Möglichkeit bestehen Workshop anhand Kafka Erzählung Jan Horstmann Postdoc koordinieren Fortext Jan Christoph Meister Professor Digital Humanitie schwerpunkt Literaturwissenschaft ursprünglich Erfinder Catma etlicher forschungsprojekte Annotation Visualisierung textuell daten Entwicklung Verbesserung leiten Marco Petris informatiker stark Affinität geisteswissenschaftlich Fragestellung Anfang Entwicklung Catma beteiligen beschäftigen Aspekt Implementierung Mareike Schumacher promovieren digital Literaturwissenschaftlerin ort narratologisch Ortskategorien literarisch Text beschäftigen Methode Marie Master -- Education interessieren Christoph Martin Wieland Zeitgenosse liegen Forschungsschwerpunkt Wertung Literatur studieren Kiel Hamburg wissenschaftlich Mitarbeiterin Universität Hamburg Person bringen Laptop Internet verbinden,"[('catma', 0.4697200235694457), ('annotation', 0.2915914224595953), ('workshop', 0.14299667153719747), ('highlights', 0.13987603623567527), ('freitextkommentar', 0.13028392351303675), ('taxonomiebasiert', 0.12347821122840877), ('undogmatisch', 0.11819929066352312), ('visualisierung', 0.11804533643504789), ('praktisch', 0.11033515380902649), ('christoph', 0.11023934649775288)]"
2020,DHd2020,136_final-PAGEL_Janis_Vom_Phänomen_zur_Analyse___ein_CRETA_Workshop_zu.xml,Vom Phänomen zur Analyse 'ein CRETA-Workshop zur reflektierten Operationalisierung in den DH,"Nora Ketschik (Universität Stuttgart); Benjamin Krautter (Universität Stuttgart); Sandra Murr (Universität Stuttgart); Janis Pagel (Universität Stuttgart); Nils Reiter (Universität zu Köln, Universität Stuttgart)","Operationalisierung, reflektiert, Entitäten, Erzählebenen, Wertherness, CRETA","Modellierung, Annotieren, Theoretisierung, Visualisierung, Literatur, Text","Der Workshop adressiert eine der großen Herausforderungen für Arbeiten in den Digital Humanities 'die Operationalisierung geisteswissenschaftlicher Konzepte und Fragestellungen für computergestützte Methoden (vgl. Jannidis 2010, 109–132; Moretti 2013; Flanders, Jannidis 2015; Jacke 2014, 118–139). Während Geisteswissenschaftler vor allem mit komplexen, häufig textübergreifenden Phänomenen arbeiten und als relevant erachtete Kontexte der behandelten Themen heranziehen, ist die computergestützte Arbeit an identifizierbare Phänomene auf der Textoberfläche gebunden. Die hieraus erwachsende Diskrepanz zwischen Erwartungen und Ergebnissen gilt es über eine adäquate Operationalisierung, Als Anwendungsfälle stellen wir drei unterschiedliche literatur- und sozialwissenschaftliche Phänomene vor, zu denen wir im Rahmen des Stuttgarter ""Center for Reflected Text Analytics"" (CRETA) Zum einen befassen wir uns mit dem Konzept der Entität und ihrer Referenz in literatur- und sozialwissenschaftlichen Texten (vgl. Reiter u.a. 2017, 19–22; Blessing u.a. 2017). Als Entitätenreferenzen gelten alle Ausdrücke, die auf eine Entität der realen oder fiktiven Welt referieren. Dazu zählen Personen/Figuren, Orte, Organisationen sowie Ereignisse, so dass das Konzept der Entität bewusst weit gefasst und für verschiedene Forschungsfragen anschlussfähig ist. Auf Entitäten kann auf verschiedene Weise referiert werden, u.a. über Eigen- und Gattungsnamen (z.B. ""Angela Merkel"", ""die Kanzlerin""). Um Entitäten in einem Text zu extrahieren, müssen folglich die Entitätenreferenzen annotiert und kookkurrente Ausdrücke aufgelöst werden. Die Herausforderungen bestehen vor allem in der Festlegung der Referenzausdrücke (welche Ausdrücke werden berücksichtigt?), in der Abgrenzung von Entitätenreferenzen gegenüber Generika sowie im Umgang mit Verschachtelungen, Metonymien und textspezifischen Besonderheiten. Am Beispiel zweier Textsorten (mhd. Artusroman und Bundestagsdebatten) stellen wir das Phänomen und Möglichkeiten der Umsetzung vor. Des Weiteren beschäftigen wir uns mit der Annotation von Erzählebenen. Als dritten Anwendungsfall stellen wir die sog. ""Wertherness"" vor, womit eine Sammlung von Texteigenschaften gemeint ist, die Texte als ""Wertheriaden"" identifizieren können. Die Veröffentlichung von Goethes ""Die Leiden des jungen Werthers"" 1774 zog eine Reihe an literarischen Adaptationen nach sich, die sich durch verschiedene Bezugnahmen auf den Originaltext als sog. Wertheriaden ausweisen. Die Referenzen können dabei sowohl formaler (z.B. Briefroman, Dreiecksbeziehung) als auch inhaltlicher (z.B. Rolle der Natur, Verhältnis Subjekt-Gesellschaft) Art sein. Für eine computergestützte Analyse solcher Referenztexte müssen einerseits die einzelnen formalen und semantischen Kategorien operationalisiert und in den Texten identifiziert werden, andererseits ist zu untersuchen, welche Kriterien in bekannten Wertheriaden in Kombination miteinander auftreten. Im Workshop stellen wir zwei Ansätze zur Operationalisierung vor, die sich 'in verschiedenen Phasen des Forschungsprozesses 'sehr gut gegenseitig ergänzen. Der erste Ansatz besteht dabei in der Schärfung von Als zweiten Ansatz stellen wir die Idee vor, Zielphänomene In einem Theorieteil führen wir in die Problematik der Operationalisierung von geisteswissenschaftlichen Phänomenen für die computergestützte Analyse ein. Anhand der drei oben genannten Beispiele aus der CRETA-Praxis thematisieren wir die Problematik und stellen die Ansätze der Operationalisierung im Detail vor. Je nach Interesse kann anschließend einer dieser Anwendungsfälle ausgewählt und bearbeitet werden. Im praktischen Teil des Workshops haben die Teilnehmenden die Möglichkeit, beide Operationalisierungsansätze an ihrem gewählten Anwendungsfall zu erproben. Hierfür befassen sie sich zunächst mit dem Phänomen, indem sie es anhand eines Textauszugs manuell annotieren und parallel stichpunktartig die Richtlinien schärfen. In einer ersten Diskussionsrunde werden die verschiedenen Ergebnisse gesammelt und diskutiert. Zur Erprobung des zweiten Ansatzes stellen wir für jeden Anwendungsfall einen Operationalisierungs-""Baukasten"" vor. Dieser besteht aus einer Sammlung von Python-Skripten in einem Jupyter-Notebook Ziel unseres Workshops ist es, die Teilnehmenden für die Wichtigkeit der Operationalisierung in den Digital Humanities zu sensibilisieren und ihnen Lösungsangebote vorzustellen. Durch die interdisziplinäre Ausrichtung von DH-Arbeiten kommt der Operationalisierung eine Schlüsselposition zu, indem diese eine Brücke zwischen geisteswissenschaftlichem Phänomen und computergestützter Umsetzung schlägt. Mit den gewählten Anwendungsfällen wollen wir den Teilnehmenden ein ""Repertoire"" für die Operationalisierung verschiedener Aufgabentypen mitgeben. Wir zeigen zum einen, dass die Annotation eines Phänomens als Methode seiner Operationalisierung dienen kann (vgl. Gius, Jacke 2017, 233–254); zum anderen führen wir für textbasierte Phänomene eine approximative Operationalisierung ein (vgl. Reiter/Willand, 2018). Beide Verfahrensweisen sind auf andere Anwendungsfälle übertragbar. Gleichzeitig möchten wir deutlich machen, dass es für jedes Untersuchungsvorhaben nicht nur eine, sondern verschiedene Wege der Operationalisierung gibt. Die Spielräume, die bei der Operationalisierung geisteswissenschaftlicher Fragestellungen entstehen, machen es notwendig, Entscheidungen reflektiert zu treffen, sie offenzulegen und ihren Einfluss auf die Ergebnisse als Voraussetzung für eine angemessene Interpretation zu bedenken.  Neben diesem Workshop zur Operationalisierung wird noch ein weiterer Workshop des Stuttgarter DH-Zentrums CRETA während der diesjährigen DHd-Konferenz stattfinden (Gerhard Kremer, Kerstin Jung: ""Maschinelles Lernen lernen: Ein CRETA-Hackatorial zur reflektierten automatischen Textanalyse""). Auch wenn es eine gewisse Schnittmenge zwischen den Workshops gibt (Textgrundlagen, Anwendungsfälle), ist die jeweilige Zielsetzung grundsätzlich verschieden: Während es beim CRETA-Hackatorial um Verfahren des Maschinellen Lernens geht, konzentriert sich der hier vorgestellte Workshop auf den grundsätzlicheren Schritt der Operationalisierung. Es geht also darum, Ansätze aufzuzeigen, wie ein Untersuchungsvorhaben oder theoretisches Konzept überhaupt für die computergestützte Analyse ""vor- bzw. aufbereitet"" werden kann. Beide Workshops ergänzen einander sinnvoll, was die Teilnahme an beiden oder an nur einem der Workshops möglich macht. (insgesamt 3 Stunden + 30 Min. Pause) - Kaffeepause (30 Min.) - Zwischen 15 und 25. Abgesehen von Beamer und ausreichend Steckdosen ist keine besondere technische Ausstattung erforderlich. Die Teilnehmenden arbeiten im praktischen Teil an ihrem eigenen PC. Informationen zu eventuellen Vorab-Installationen werden rechtzeitig mitgeteilt. Der Workshop wird von Mitarbeitenden des ""Center for Reflected Text Analytics"" (CRETA) der Universität Stuttgart veranstaltet, die bereits erfahrene Workshop-Leiter/-innen im DH-Bereich sind (DHd 2017, DH 2017,DHd 2018, ESU 2018,DHd 2019, HCH 2019). Das BMBF-geförderte eHumanities-Zentrum CRETA ist auf die interdisziplinäre Zusammenarbeit von Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung ausgerichtet. Die übergreifende Zielsetzung besteht in der Erarbeitung systematischer und transparenter Workflows, in denen die Entwicklung komputationeller Modelle und Methoden kritisch reflektiert und adäquat auf die unterschiedlichen geistes- und sozialwissenschaftlichen Forschungsfragen angepasst wird. Nora Ketschik ist Promotionsstudentin in der Abteilung für Germanistische Mediävistik. Im Rahmen von CRETA führt sie Netzwerkanalysen zu ausgewählten mittelhochdeutschen Romanen durch und setzt sich dabei kritisch mit der Verwendung computergestützter Methoden für literaturwissenschaftliche Analysezwecke auseinander. Benjamin Krautter ist Promotionsstudent in der Abteilung für Neuere Deutsche Literatur II und Mitarbeiter im Projekt QuaDramA - Quantitative Drama Analytics. Dort arbeitet er an der Operationalisierung Aristotelischer Kategorien für die quantitative Dramenanalyse. Er beschäftigt sich zudem mit der Integration quantitativer Methoden in literaturwissenschaftliche Fragestellungen ( Sandra Murr ist Promotionsstudentin in der Abteilung für Neuere Deutsche Literatur I. In CRETA arbeitet sie an der digitalen Analyse des ""Wertheriaden-Korpus"", Texte, die in der Folge von Goethes ""Werther"" seit 1774 erschienen sind. Mittels computergestützter Verfahren wird sich mit der Frage auseinandergesetzt, anhand welcher charakteristischer Kriterien eine ""Wertheriade"" als solche definiert wird und wie sich entsprechende strukturelle und inhaltliche Kriterien operationalisieren, in den Texten automatisch identifizieren und reflektiert vergleichen lassen. Janis Pagel ist Promotionsstudent am Institut für Maschinelle Sprachverarbeitung und Mitarbeiter im QuaDramA-Projekt. Er forscht zu Anwendungen von computerlinguistischen Methoden auf literaturwissenschaftliche Fragestellungen und innerhalb von CRETA hauptsächlich zu Koreferenzresolution für literarische Texte. Nils Reiter hat Computerlinguistik/Informatik an der Universität des Saarlandes studiert, wurde 2013 an der Uni Heidelberg promoviert und ist seit 2014 Post-Doc am Institut für Maschinelle Sprachverarbeitung. Seit seiner Promotion ist er im Bereich Digital Humanities unterwegs, mit einem besonderen Interesse an Fragen der Operationalisierung, und zwar sowohl im Hinblick auf Automatisierung wie auch auf manuelle Annotation. Er arbeitet dabei auch an praktischen Fragen der Kooperation zwischen Geistes- und Computerwissenschaftler*innen, und organisiert einen shared task zur Erkennung von Erzählebenen. Derzeit ist er Vertretungsprofessor für Sprachliche Informationsverarbeitung/Digital Humanities an der Universität zu Köln.",de,Workshop adressieren Herausforderung Arbeit Digital Humanitie Operationalisierung geisteswissenschaftlich Konzept Fragestellung computergestützt Methode Jannidis moretti Flanders Jannidis jack geisteswissenschaftl komplex häufig textübergreifend Phänomen arbeiten relevant erachtet Kontexte behandelten Thema heranziehen computergestützt Arbeit identifizierbar Phänomen Textoberfläche binden Hieraus erwachsend Diskrepanz Erwartung Ergebnis gelten adäquat Operationalisierung anwendungsfäll stellen unterschiedlich sozialwissenschaftlich Phänomen Rahmen Stuttgarter center for reflected Text analytics Creta befassen Konzept Entität Referenz sozialwissenschaftlich Text Reiter blessing Entitätenreferenze gelten Ausdrück Entität real fiktiv Welt referieren zählen Person Figur Ort Organisation Ereignis Konzept Entität bewussen fassen verschieden Forschungsfrag anschlussfähig entität verschieden Weise referieren gattungsnamen Angela Merkel Kanzlerin Entität Text extrahieren folglich Entitätenreferenze annotiert kookkurrent Ausdrück auflösen Herausforderung bestehen Festlegung Referenzausdrücke ausdrücke berücksichtigen Abgrenzung Entitätenreferenze generika Umgang Verschachtelung Metonymi textspezifisch Besonderheit zwei Textsort mhd artusroman bundestagsdebatten stellen Phänomen Möglichkeit Umsetzung beschäftigen Annotation Erzähleben Anwendungsfall stellen wertherness womit Sammlung Texteigenschaft meinen Text wertheriad identifizieren Veröffentlichung goeth leid jung Werther ziehen Reihe literarisch adaptation verschieden Bezugnahme Originaltext wertheriad ausweisen Referenz sowohl formal Briefroman Dreiecksbeziehung inhaltlich Rolle Natur Verhältnis Art computergestützt Analyse Referenztexte einerseits einzeln formal semantisch Kategori operationalisieren Text identifizieren andererseits untersuchen kriterien bekannt wertheriaden Kombination miteinander auftreten Workshop stellen Ansatz Operationalisierung verschieden Phase Forschungsprozesse gegenseitig ergänzen Ansatz bestehen Schärfung Ansatz stellen Idee zielphänomen Theorieteil fahren Problematik Operationalisierung geisteswissenschaftlich Phänomen computergestützt Analyse anhand genannt Beispiel thematisieren Problematik stellen Ansatz Operationalisierung Detail Interesse anschließend anwendungsfäll auswählen bearbeiten praktisch Workshop Teilnehmende Möglichkeit operationalisierungsansätzen gewählt Anwendungsfall erproben hierfür befassen Phänomen anhand Textauszug manuell annotieren parallel stichpunktartig Richtlinie schärfen diskussionsrunde verschieden Ergebnis sammeln diskutieren Erprobung ansatz stellen anwendungsfall bestehen Sammlung Ziel unser Workshop Teilnehmend Wichtigkeit Operationalisierung Digital Humanitie sensibilisieren lösungsangebot vorstellen interdisziplinär Ausrichtung Operationalisierung Schlüsselposition Brücke geisteswissenschaftlich Phänomen computergestützter Umsetzung schlagen gewählt anwendungsfällen Teilnehmend repertoire Operationalisierung verschieden Aufgabentype mitgeben zeigen Annotation Phänomen Methode Operationalisierung dienen Gius Jacke führen textbasiert Phänomen approximativ Operationalisierung Reiter Willand Verfahrensweise anwendungsfäll übertragbar gleichzeitig möchten deutlich jeder untersuchungsvorhaben verschieden Weg Operationalisierung Spielraum Operationalisierung geisteswissenschaftlich Fragestellung entstehen notwendig Entscheidung reflektieren treffen Offenzuleg einfluss Ergebnis Voraussetzung angemessen Interpretation bedenken Workshop Operationalisierung weit Workshop Stuttgarter Creta diesjährig stattfinden Gerhard Kremer kerstin jung maschinelles lernen lernen reflektiert automatisch Textanalyse gewiß Schnittmenge Workshops Textgrundlage anwendungsfäll jeweilig Zielsetzung grundsätzlich verschieden Verfahren maschinell Lernen konzentrieren vorgestellt Workshop grundsätzlicher Schritt Operationalisierung Ansatz aufzuzeigen untersuchungsvorhab theoretisch Konzept computergestützt Analyse aufbereitet Workshop ergänzen sinnvoll Teilnahme Workshops insgesamt Stunde Pause Kaffeepause absehen Beamer ausreichend steckdosen besonderer technisch Ausstattung erforderlich Teilnehmende arbeiten praktisch Pc Information eventuellen rechtzeitig mitteilen Workshop Mitarbeitende center for reflected Text analytics Creta Universität Stuttgart veranstalten erfahren dhd dh esu hch Creta interdisziplinär Zusammenarbeit Literaturwissenschaft Linguistik Philosophie Sozialwissenschaft Maschineller Sprachverarbeitung Visualisierung ausrichten übergreifend Zielsetzung bestehen Erarbeitung systematisch Transparenter workflows Entwicklung komputationell Modell Methode kritisch reflektieren adäquat unterschiedlich sozialwissenschaftlich Forschungsfrag angepasst Nora Ketschik Promotionsstudentin Abteilung germanistisch Mediävistik Rahmen Creta führen netzwerkanalysen ausgewählt mittelhochdeutsch Roman setzen kritisch Verwendung Computergestützter Methode literaturwissenschaftlich Analysezweck auseinander Benjamin Krautter Promotionsstudent Abteilung neu deutsch Literatur ii Mitarbeiter Projekt Quadrama quantitativ Drama Analytics arbeiten Operationalisierung aristotelisch kategorien quantitativ Dramenanalyse beschäftigen zudem Integration quantitativ Methode literaturwissenschaftlich Fragestellung Sandra Murr Promotionsstudentin Abteilung neu deutsch Literatur Creta arbeiten digital Analyse Text Folge goeth werth erscheinen mittels Computergestützter Verfahren Frage auseinandergesetzen anhand charakteristisch kriterien Wertheriade definieren entsprechend strukturell inhaltlich kriterien operationalisieren text automatisch identifizier reflektieren vergleichen lassen janis Pagel Promotionsstudent Institut maschinell Sprachverarbeitung Mitarbeiter forschen anwendungen computerlinguistisch Methode literaturwissenschaftlich Fragestellung innerhalb Creta hauptsächlich Koreferenzresolution literarisch Text nils Reiter Computerlinguistik Informatik Universität Saarlande studieren Uni Heidelberg promovieren Institut maschinell Sprachverarbeitung Promotion Bereich digital Humanitie unterwegs besonderer Interesse Frage Operationalisierung sowohl Hinblick Automatisierung Manuelle Annotation arbeiten praktisch Frage Kooperation organisieren shared Task Erkennung Erzähleben derzeit Vertretungsprofessor sprachlich informationsverarbeitung Digital Humanitie Universität Köln,"[('operationalisierung', 0.3976344763839075), ('creta', 0.23115888780227883), ('workshop', 0.20873581313386902), ('phänomen', 0.17946482571257782), ('anwendungsfäll', 0.14303922451521459), ('entität', 0.1331549509520246), ('entitätenreferenze', 0.11082825504816994), ('abteilung', 0.10727941838641093), ('computergestützt', 0.10655904568666404), ('sozialwissenschaftlich', 0.10149369069638897)]"
2020,DHd2020,147_final-REITER_Nils_Intertextualität_in_literarischen_Texten_und_dar.xml,Intertextualität in literarischen Texten und darüber hinaus,Julia Nanette; Ben Sulzbacher; Nils Reiter; Axel Pichler,"Intertextualität, Literatur, Philosophie, Modellierung, Annotation","Intertextualität, Literatur, Philosophie, Modellierung, Annotation","Die Analyse der Formen und Funktionen von Intertextualität ist ein Forschungsbereich, dessen heuristischer Anspruch seit dem erstmaligen Auftreten des Terminus ""Intertextualität"" in Julia Kristevas Aufsatz Diese divergierenden Tendenzen innerhalb der Intertextualitätsforschung können nicht zuletzt auf die Tatsache zurückgeführt werden, dass sich literarische Intertextualität selbst 'wie bereits das Wort ""An Gerade dieses Changieren zwischen Regelhaftigkeit und Dynamik bereitete bisherigen Untersuchungen literarischer Intertextualität mit den Mitteln analoger Textarbeit stets große Probleme: Klassische Textanalysen und abstrakte Modelle erweisen sich gleichermaßen als defizitär, indem für eine nachvollziehbare Erfassung der bestehenden Vielfalt intertextueller Relationen gerade das Ineinandergreifen von Modellierung und Interpretation entscheidend ist (vgl. Nantke/Schlupkothen 2018, 2019). Die formale Modellierung bietet hier gesteigerte Möglichkeiten der systematischen Erfassung und der induktiven Kategorienbildung sowie der unmittelbaren Visualisierung. Auf diese Weise können Modelle entstehen, welche flexibel genug sind, um unterschiedlichste Formen von Intertextualität adäquat zu erfassen, und dabei gleichzeitig eine formale Strenge aufweisen, die einer maschinellen Abfrage sowie der Kombination mit (teil-)automatisiert erzeugten Analyseergebnissen offensteht. Bislang finden sich Beispiele für den Einsatz computergestützter Verfahren zur Intertextualitätsdetektion vor allem im Bereich der Konkret soll im Panel anhand verschiedener Beispiele aufgezeigt und diskutiert werden, wie und wo sich digitale Ansätze zur Erfassung und Modellierung intertextueller Beziehungen zwischen den Polen ""Formalisierung"" und ""interpretative Freiheit"" verorten lassen. Dabei verstehen wir eine intertextuelle Referenz als eine von einer Leserin/einem Leser wahrgenommene ""Wiederholung"" aus einem anderen Text, wobei die Wiederholung im Regelfall nicht (nur) die Textoberfläche betrifft, sondern Ideen, Gedanken, Formulierungen, Syntax- oder Plotstrukturen. Im Rahmen des Panels werden Verbindungsmöglichkeiten von quantitativen und qualitativen Verfahren zur Erschließung von Intertextualität evaluiert. Ebenfalls wird dabei erörtert, wie im Zuge der Modellierung interpretative Spielräume immer wieder zur Herausforderung für die Formalisierungsbestrebungen werden und wie derartige Situationen positiv gewendet spezifische Funktionsweisen von Intertextualität sichtbar machen können.  Für die systematisierende Erfassung intertextueller Relationen wurde im Projekt Mithilfe der Daraus abgeleitet erfolgt eine Modellierung der relevanten Kategorien in drei ""Bäumen"", welche die Phänomene nach den Ebenen Die Struktur der   Intertextuelle Referenzen spielen neben der Literatur auch in der Philosophie eine große Rolle. So werden etwa in der Zeitschrift Zunächst stellen wir ein Kategoriensystem vor, dass in einem Bottom-Up-Verfahren etabliert wurde. Dazu wurden die Nietzsche-Nachweise als existierende Annotationen aufgefasst und eine ""Meta-Annotation"" zugefügt, die die Art der Referenz charakterisiert (z.B. ""semantisch äquivalente Paraphrase"" oder ""syntaktische Öhnlichkeit""). Mit den üblichen Methoden aus der reflektierenden Annotationspraxis (Übereinstimmung) können Definitionen für diese Charakterisierungen geschärft werden, so dass ein robuster Überblick über verschiedene Arten der Referenzen vorliegt. Im Gegensatz zu Ansätzen, die vollständig ""from scratch"" annotieren, bewahrt der Rückgriff auf existierende Referenzen davor, eine subjektiv motivierte Teilmenge an Referenzen in Betracht zu ziehen. Anknüpfungspunkte und Gemeinsamkeiten mit den im ersten Beitrag vorgestellten Kategorien zu eruieren ist eines der Ziele des Panels. Anstelle eines neuen Modells implementiert WordWeb/IDEM Es ist geplant, dass das Panel der folgenden Struktur folgt: Frau Prof. Dr. Evelyn Gius, TU Darmstadt, hat zugesagt, die Moderation des Panels zu übernehmen.",de,Analyse Form Funktion Intertextualität forschungsbereich heuristisch Anspruch erstmalig Auftret Terminus Intertextualität Julia Kristevas Aufsatz divergierend Tendenz innerhalb Intertextualitätsforschung zuletzt Tatsache zurückführen literarisch Intertextualität Wort Changieren Regelhaftigkeit Dynamik bereitet bisherig Untersuchung literarisch Intertextualität Mittel Analoger Textarbeit stets Problem klassisch textanalyse abstrakt Modell erweisen gleichermaßen defizitär nachvollziehbar Erfassung bestehend Vielfalt intertextuell Relation Ineinandergreifen Modellierung Interpretation entscheidend nantke schlupkeothen formal Modellierung bieten gesteigert Möglichkeit systematisch Erfassung induktiv Kategorienbildung unmittelbar Visualisierung Weise Modell entstehen flexibel unterschiedlich Form Intertextualität Adäquat erfassen gleichzeitig formal Strenge aufweisen maschinell Abfrage Kombination erzeugt analyseergebnissen offenstehen bislang finden Beispiel Einsatz Computergestützter Verfahren Intertextualitätsdetektion Bereich konkret Panel anhand verschieden Beispiel aufzeigen diskutieren digital Ansatz Erfassung Modellierung intertextuell Beziehung Pole Formalisierung interpretativ Freiheit verorten lassen verstehen intertextuell Referenz Leserin Leser wahrgenommen Wiederholung Text wobei Wiederholung Regelfall Textoberfläche betreffen Idee Gedanke formulierungen plotstrukturen Rahmen Panel Verbindungsmöglichkeit quantitativen qualitativ Verfahren Erschließung Intertextualität evaluieren ebenfalls erörtern Zug Modellierung interpretativ Spielraum Herausforderung formalisierungsbestrebung derartig Situation positiv wenden spezifisch Funktionsweise Intertextualität sichtbar systematisierend Erfassung Intertextueller Relation Projekt Mithilfe ableiten erfolgen Modellierung relevant Kategorie Bäume Phänomen Ebene Struktur intertextuell Referenz spielen Literatur Philosophie Rolle Zeitschrift stellen Kategoriensystem etablieren existierend annotatio auffassen zufügen Art Referenz charakterisieren semantisch äquivalent Paraphrase syntaktisch Öhnlichkeit üblich Methode reflektierend Annotationspraxis Übereinstimmung Definition charakterisierung schärfen robust Überblick verschieden Art Referenz vorliegen Gegensatz ansätzen vollständig from Scratch annotieren bewahren Rückgriff existierend Referenz subjektiv motiviert Teilmenge Referenz Betracht ziehen anknüpfungspunkte Gemeinsamkeit Beitrag vorgestellt Kategorie eruieren Ziel Panel anstelle Modell implementieren Wordweb idem planen Panel folgend Struktur folgen Frau Prof dr Evelyn Gius Tu Darmstadt zusagen Moderation Panel übernehmen,"[('intertextualität', 0.3740789803491716), ('referenz', 0.266706646853052), ('panel', 0.2374031494006391), ('intertextuell', 0.21827287846371032), ('modellierung', 0.18007137607339338), ('erfassung', 0.18001208696573048), ('wiederholung', 0.12958590092841038), ('interpretativ', 0.11739586796446908), ('existierend', 0.09784601228994315), ('modell', 0.09399794074734333)]"
2020,DHd2020,156_final-HORSTMANN_Jan_Routinen__Ressourcen_und_Tools_der_digitalen_T.xml,"Routinen, Ressourcen und Tools der digitalen Texterforschung. Ein einfacher Einstieg","Jan Horstmann (Universität Hamburg, Deutschland); Marie Flüh (Universität Hamburg, Deutschland); Marco Petris (Universität Hamburg, Deutschland)","digitale Methoden, Dissemination, Lernen und Lehren","Annotieren, Einführung, Lehre, Literatur, Forschungsprozess, Werkzeuge","Die Anwendung computergestützter Verfahren in den Geistes- und Kulturwissenschaften prägt seit geraumer Zeit die Entwicklung unterschiedlicher Fachdisziplinen (vgl. Thaller 2012). Neue Methoden bahnen sich ihren Weg in den Methodenkanon ganz unterschiedlicher Domänen (vgl. Sahle 2015). Wie aber kann man Lehrenden 'mit den unterschiedlichen Ansprüchen universitär Dozierender oder Lehrender an Schulen 'einen möglichst niedrigschwelligen, aber dennoch wissenschaftlich seriösen Zugang zu dem Repertoire digitaler Methoden der Texterforschung eröffnen, das zum Spektrum der Digital Humanities zählt? Wie kann man sowohl Begeisterung wie kritische Kompetenz im konkreten Umgang mit Verfahren der digitalen Textanalyse so vermitteln, dass die Alltagspraxis des Lehrens und Forschens davon profitiert? Man muss nicht immer gleich einen theoretischen ""Paradigmenwechsel"" ausrufen, sondern kann das ""neue"" Feld besser zunächst im ""hands-on""-Modus erschließbar machen. Durch einen niedrigschwelligen Disseminationsansatz entsteht die Möglichkeit, dass alte Fragen und neue Methoden sinnvoll aufeinander bezogen werden können (vgl. etwa Horstmann / Kleymann 2019). Das im November 2017 an der Universität Hamburg gestartete DFG-Projekt forTEXT ( In der Rubrik Routinen stellen wir einführende Einträge zu digitalen Ausgewählte und etablierte deutschsprachige Für jede vorgestellte Methode stellen wir mindestens ein Tool vor, das für die praktische Umsetzung dieser Methode eingesetzt werden kann. Die Tools werden bedarfsgerecht hinsichtlich ihrer Funktionalität, Anwendungsfreundlichkeit, Nutzerbetreuung, Datensicherheit, Nutzungsbedingungen und des Grads ihrer Etablierung im wissenschaftlichen Diskurs befragt. Die Tooleinträge folgen 'wie auch die einzelnen Beitragsformate in den Kategorien Routinen und Ressourcen 'einem wiedererkennbaren Schema, in dem konkrete Fragen aus Nutzer√Ønnenperspektive gestellt und beantwortet werden. Mit der Entwicklung der sechsten Version von CATMA ( Das Projekt wird durch umfangreiche Maßnahmen der nicht-digitalen Dissemination seiner Inhalte begleitet. Einerseits bieten die Projektmitarbeiter√Ønnen bedarfsgerechte Workshops und Vorträge für Forschungsgruppen oder Veranstaltungsreihen an Universitäten und auf Konferenzen an. Darüber hinaus werden schulinterne Workshops durchgeführt, die auf die z.¬†T. sehr unterschiedliche technische Infrastruktur vor Ort eingehen und sich in der inhaltlichen Ausrichtung ebenfalls eng an der spezifischen Bedarfslage der Teilnehmer√Ønnen orientieren. Die umfangreiche Social-Media-Strategie von forTEXT (vgl. Horstmann / Schumacher 2019) ist ein essentieller Teil des gesamten Dissiminationsprogramms: Auf Twitter, Youtube, Facebook und Pinterest treten wir in unterschiedlichen Modi mit diverse Zielgruppen in Kontakt und führen diese in die digitale Arbeit mit Texten ein. So tritt forTEXT nicht nur an neue Nutzer√Ønnengruppen heran, sondern integriert sich auch selbst im fachwissenschaftlichen/DH-Diskurs. Im Januar 2020 wird ein digitales Empfehlungssystem implementiert, das im Frage-Antwort-Schema die Projekte der Nutzer√Ønnen so klassifiziert, dass die automatische Generierung individualisierter Empfehlungen von Routinen, Ressourcen und Tools zur Bearbeitung der jeweiligen Fragestellung möglich sein wird. Das Empfehlungssystem wird somit dafür sorgen, dass die einzelnen Bereiche von forTEXT einerseits zusammengefasst, andererseits aber auch bedarfsorientiert und effektiv durch sie navigiert werden kann. Das System macht damit insbesondere Nutzer√Ønnen ohne vorherige DH-Erfahrung den Einstieg in digitale Methoden zur Unterstützung ihrer Projekte individuell möglich.",de,Anwendung Computergestützter Verfahren Kulturwissenschaft prägen geraum Entwicklung unterschiedlich Fachdisziplin thall Methode Bahn Weg Methodenkanon unterschiedlich Domäne sahlen lehrend unterschiedlich ansprüch Universitär Dozierender Lehrender Schule möglichst niedrigschwellig dennoch wissenschaftlich seriösen Zugang Repertoire Digitaler Methode Texterforschung eröffnen Spektrum Digital Humanitie zählen sowohl Begeisterung kritisch Kompetenz konkret Umgang Verfahren digital Textanalyse vermitteln Alltagspraxis Lehren forschens profitieren theoretisch Paradigmenwechsel ausrufen Feld erschließbar niedrigschwellig Disseminationsansatz entstehen Möglichkeit alt Frage Methode sinnvoll aufeinander beziehen Horstmann Kleymann November Universität Hamburg gestartet Fortext Rubrik routin stellen einführend einträge digital ausgewählt etablieren deutschsprachig vorgestellt Methode stellen mindestens Tool praktisch Umsetzung Methode einsetzen Tools bedarfsgerecht hinsichtlich Funktionalität Anwendungsfreundlichkeit Nutzerbetreuung Datensicherheit nutzungsbedingung grads Etablierung wissenschaftlich Diskurs befragen tooleinträge folgen einzeln Beitragsformat kategori Routin Ressource Wiedererkennbaren Schema konkret Frage stellen beantworten Entwicklung Version Catma Projekt umfangreich Maßnahme Dissemination Inhalt begleiten einerseits bieten bedarfsgerechen Workshops vortrag Forschungsgruppe veranstaltungsreihen Universität Konferenz hinaus Schulintern Workshops durchführen unterschiedlich technisch infrastruktur Ort eingehen inhaltlich Ausrichtung ebenfalls eng spezifisch Bedarfslage orientieren umfangreich Fortext Horstmann Schumacher essentiell gesamt Dissiminationsprogramm Twitter Youtube Facebook pinterest treten unterschiedlich Modi diverser Zielgruppe Kontakt führen digital Arbeit Text treten fortext heran integrieren fachwissenschaftlich Januar digital Empfehlungssystem implementieren Projekt klassifizieren automatisch Generierung individualisiert Empfehlung Routine Ressource Tools Bearbeitung jeweilig Fragestellung Empfehlungssystem somit sorgen einzeln Bereich Fortext einerseits zusammengefasst andererseits bedarfsorientiert effektiv navigieren System insbesondere vorherig Einstieg digital Methode Unterstützung Projekt individuell,"[('fortext', 0.27101692421701423), ('empfehlungssystem', 0.16487215573031064), ('methode', 0.16036270363087884), ('niedrigschwellig', 0.1562596391174333), ('workshops', 0.14957925223627924), ('horstmann', 0.13950607422078812), ('routin', 0.13950607422078812), ('digital', 0.1336243151417658), ('unterschiedlich', 0.126173682416837), ('treten', 0.09800372020293581)]"
2020,DHd2020,174_final-WEIMER_Lukas_Redewiedergabe_in_Heftromanen_und_Hochliteratur.xml,Redewiedergabe in Heftromanen und Hochliteratur,"Annelen Brunner (Leibniz-Institut für Deutsche Sprache, Mannheim); Fotis Jannidis (Julius-Maximilians-Universität Würzburg, Deutschland); Ngoc Duyen Tanja Tu (Leibniz-Institut für Deutsche Sprache, Mannheim); Lukas Weimer (Julius-Maximilians-Universität Würzburg, Deutschland)","Redewiedergabe, Heftromane, Genre, Automatische Erkennung","Programmierung, Annotieren, Stilistische Analyse, Sprache, Literatur, Text","Die Art und Weise, wie die Rede und Gedanken einer Figur im Erzähltext eingebunden werden, ist einer der traditionellen Aspekte der Narratologie (vgl. z.B. Genette 2010; Mart√≠nez/Scheffel 2016). Die vorgestellte Studie untersucht die Anteile unterschiedlicher Redewiedergabeformen im Vergleich zwischen zwei Literaturtypen von gegensätzlichen Enden des Spektrums: Hochliteratur 'definiert als Werke, die auf der Auswahlliste von Literaturpreisen standen 'und Heftromanen, massenproduzierten Erzählwerken, die zumeist über den Zeitschriftenhandel vertrieben werden und früher abwertend als ""Romane der Unterschicht"" (Nusser 1981) bezeichnet wurden. Unsere These ist, dass sich diese Literaturtypen hinsichtlich ihrer Erzählweise unterscheiden, und sich dies in den verwendeten Wiedergabeformen niederschlägt. Der Fokus der Untersuchung liegt auf der Dichotomie zwischen direkter und nicht-direkter Wiedergabe, die schon in der klassischen Rhetorik aufgemacht wurde (vgl. McHale 2014). Die Studie geht von manuell annotierten Daten aus und evaluiert daran die Validität automatischer Annotationswerkzeuge, die im Anschluss eingesetzt werden, um die Menge des betrachteten Materials beträchtlich zu erweitern. Zur Kontrastierung von Heftromanen und Hochliteratur mit quantitativen Methoden liegen bereits Studien vor, welche sich mit Fragen der sprachlichen und thematischen Komplexität beschäftigen (Jannidis/Konle/Leinen 2019a/2019b). Das verwendete Annotationssystem sowie die Erkenner wurden im Rahmen des Redewiedergabe-Projekts entwickelt (Brunner et al. 2019a/2019b). Für die Voruntersuchung wurden aus 22 Hochliteratur-Texten und 22 Heftromanen zufällige Textausschnitte von ca. 1000 Tokens gezogen. Da Heftromane typischerweise in Reihen mit unterschiedlichem Fokus erscheinen, betrachten wir auch das Verhalten dieser unterschiedlichen Heftroman-Genres. Die Heftroman-Ausschnitte wurden darum je zur Hälfte aus den Genres Liebesroman und Horrorroman gewählt. Die Texte wurden von zwei Erstannotatoren unabhängig voneinander bearbeitet. Eine dritte Person erstellte dann auf dieser Grundlage eine Konsensannotation, indem sie die beiden Annotationen verglich, Unstimmigkeiten bereinigte und wenn nötig offensichtliche Fehler korrigierte. Das Annotationssystem (Brunner et al. 2019a) erfasst sowohl die Wiedergabe von Rede als auch von Gedanken und Geschriebenem. Es umfasst vier Haupttypen von Wiedergabe: Frei-indirekte Wiedergabe ist im Folgenden ausgeschlossen, da sich die automatische Erkennung für diese Form als noch zu unzuverlässig erwiesen hat. Die Formen indirekte und erzählte Wiedergabe sind zu ""nicht-direkt"" zusammengefasst. Diese Form umfasst damit sowohl die klassische indirekte Wiedergabe mit Rahmenformel und abhängiger Proposition als auch strukturell abweichende und häufig stärker zusammenfassende. Sie steht im Gegensatz zur direkten Wiedergabe insofern, als die Rede, Gedanken oder schriftliche Öußerung einer Figur in den Erzählertext integriert anstatt in einem Zitat klar davon abgesetzt wird. Die automatischen Erkenner beruhen auf DeepLearning. Um eine bessere Einschätzung der Erkennungswerte zu geben, ein paar Worte zur Vorkommenshäufigkeit der Redewiedergabetypen: Im den konsensannotierten Testdaten liegt der durchschnittliche Anteil von direkter Wiedergabe knapp unter 30% der Tokens (mit starken Schwankungen), von nicht-direkter bei ca. 15%. Tabelle 1 zeigt die Übereinstimmungswerte zwischen den Erstannotatoren, um einen Eindruck zu vermitteln, wie verlässlich eine von Menschen durchgeführte Annotation wäre. Tabelle 2 zeigt nun für die Formen direkt und nicht-direkt die Übereinstimmungsquoten der automatischen Methoden im Vergleich zur Konsensannotation. Wenn man als Baseline einen Erkenner annimmt, der jedes Token mit 50% Wahrscheinlichkeit als Teil von Wiedergabe klassifiziert, käme man für die Testdaten (alle Samples) für direkt auf einen F1-Score von 0,36 (Precision: 0,28; Recall: 0,50), für nicht-direkt auf einen F1-Score von 0,23 (Precision: 0,17; Recall: 0,50), wobei die Einzelscores für Heftromane vs. Hochliteratur bei direkt gleich wären, bei nicht-direkt etwas besser für Hochliteratur (F1: 0,25). Bei direkter Wiedergabe sind die Erkennungsraten der automatischen Methoden vor allem bei den Heftromanen gut, es gibt jedoch Schwankungen zwischen den Textausschnitten. Probleme treten insbesondere bei Ich-Perspektive in Kombination mit unmarkierter Wiedergabe auf, was in Hochliteratur häufiger vorkommt. Dennoch sind die mit dem maschinellen Erkenner erzielten Ergebnisse 'gerade für solche Fälle 'deutlich stabiler als eine Identifikation von direkter Wiedergabe anhand von Anführungszeichen gewesen wäre. Insgesamt neigt der Erkenner dazu, den Anteil von direkter Wiedergabe eher zu über- als zu unterschätzen. Der durchschnittliche absolute Fehler bei der Abschätzung der Anteile liegt im Schnitt bei ca. 10%. Für die nicht-direkte Wiedergabe ist zu betonen, dass die Übereinstimmungsquote auch zwischen Menschen deutlich schlechter ist (vgl. Tabelle 1). Ursache ist, dass durch die stärkere Integration in den Erzähltext sowohl die genaue Abgrenzung als auch die Entscheidung, was als Wiedergabe zu werten ist, schwieriger sind. Die automatischen Methoden erreichen bei den Heftromanen fast gleiche Verlässlichkeit, während die Hochliteratur-Abschnitte sich wiederum als etwas schwieriger erweisen. Da die Anteile von nicht-direkt geringer sind und weniger Schwankungen unterliegen als die Anteile von direkt, ist auch der durchschnittliche absolute Fehler deutlich geringer (ca. 3%), wobei der Anteil von nicht-direkt eher unterschätzt wird. Da für die Erzählweise eines Textes auch das Zusammenspiel der beiden Wiedergabetypen von Interesse ist, visualisieren wir die Textausschnitte der unterschiedlichen Untersuchungsgruppen in einem Scatterplot (Abb. 1). In dieser Darstellung auf Basis der manuellen Konsens-Annotation lässt sich ein Trend der Horrorroman-Textausschnitte erkennen, mit niedrigen Werte sowohl in direkt als auch in nicht-direkt zusammenzuclustern, während Hochliteratur und Liebesroman stark gestreut erscheinen. Mit einem Permutationstest (p=0,01) (Koplenig 2019) lassen sich im Vergleich Heftroman vs. Hochliteratur allerdings auf keiner der beiden Dimensionen signifikante Unterschiede nachweisen. Bei dem Vergleich mit Genres sind lediglich die Abweichungen zwischen Hochliteratur und Horrorroman im Anteil nicht-direkter Wiedergabe signifikant. Legt man die automatisch annotierten Daten zugrunde, verschwindet auch diese Signifikanz. Im nächsten Schritt erweitern wir unser Untersuchungsmaterial stark. Das Korpus wurde aus Volltexten zusammengestellt, wobei diesmal die Unterschiede zwischen Hochliteratur und den einzelnen Genres in den Fokus gerückt wurden: 50 Hochliteratur-Texte wurden mit jeweils 50 Heftromanen aus vier unterschiedlichen Reihen kontrastiert, die unterschiedliche Genres repräsentieren (vgl. Tab. 3). Ein Ziel war, eine größtmögliche Diversität von Autoren zu erreichen, um zu verhindern, dass das Autorensignal die Gruppenzugehörigkeiten überlagert, die uns eigentlich interessieren. Problematisch war dies bei Horrorromanen, wo ein Autor die Reihe extrem dominiert und Krimis, bei denen so gut wie keine Autoreninformationen verfügbar waren. Es ist allerdings bekannt, dass die Reihe ""Jerry Cotton"" von über 100 unterschiedlichen Autoren verfasst wurde (vgl. Karr 2019). Die Texte wurden mit den automatischen Erkennern komplett annotiert. Da die Variation der Textlängen insbesondere in der Gruppe Hochliteratur stark ist, wurden die Texte in 1000-Token-Abschnitte zerlegt, für diese die Anteile von direkter und nicht-direkter Wiedergabe berechnet und die Ergebnisse anschließend für jeden Text gemittelt (analog zur standardisierten Type-Token-Ratio). Anders als bei den Testdaten zeigen sich bei der Auswertung nun klare Unterschiede in beiden Dimensionen: Der Anteil direkter Wiedergabe ist bei Hochliteratur geringer, während der Anteil nicht-direkter Wiedergabe höher ist. Die Signifikanz dieser Unterschiede, wie auch viele Unterschiede zwischen den Genres, lassen sich mit dem Permutationstest mit p=0.01 bestätigen (vgl. Abbildung 2 und 3). Bei der Betrachtung beider Dimensionen in Relation (Abb. 4) fällt sofort auf, dass die Hochliteratur-Texte eine deutliche Streuung aufweisen, während nicht nur die einzelnen Genres, sondern auch die Heftromane als Gruppe zusammenclustern. Angesichts der Tatsache, dass die Heftroman-Genres bewusst reglementierte Reihen sind, während die Hochliteratur-Gruppe nur dadurch definiert ist, dass die enthaltenen Werke als literarisch hochwertig eingeschätzt wurden, ist dieser Befund nicht erstaunlich. Es ist jedoch durchaus bemerkenswert, dass sich der Unterschied zwischen konventionalisiertem und individualistischem Erzählen auf der Dimension der Redewiedergabetypen so deutlich quantitativ nachweisen lässt. Die Hochliteratur-Texte sind zudem die einzige Gruppe, in der ein ‚ÄöÜbergewicht"" an nicht-direkter im Gegensatz zu direkter Wiedergabe auftritt. Die Autoren sind also in der Art und Weise, wie sie Figurenstimmen in den Text einbinden, sowohl individualistischer als auch eher bereit, nicht das direkte Zitat zu wählen. Innerhalb der Gruppe der Heftromane man kann für die Genres Liebesroman, Horrorroman und Krimi einen nahezu linearen Anstieg der beiden Wiedergabeformen in Relation zueinander beobachten, wobei der Anteil direkter Wiedergabe stets höher ist. Es differenziert sich recht klar das Horror-Genre mit einem insgesamt geringeren Wiedergabeanteil, während die ‚Äökommunikativeren"" Genres Liebesroman und Krimi sich stark überlagern. Für diese beiden Genres lassen sich auch keine signifikanten Unterschiede nachweisen. Science-Fiction nimmt eine Zwischenstellung ein: Die Texte sind diverser und streuen ähnlich wie Hochliteratur, wenn auch nicht so extrem. Es ist das einzige Heftroman-Genre, für das sich auf keiner der beiden Dimensionen signifikante Unterschiede zu Hochliteratur nachweisen lassen. Dies passt zu Beobachtungen von Jannidis/Konle/Leinen (2019a), dass Science-Fiction unter den Heftroman-Genres eine Sonderstellung einnimmt und auch bei unterschiedlichen Komplexitätsmaßen wie standardisierter Type-Token-Ratio und Wortlänge höher abschneidet als die anderen Genres. Warum zeigen sich diese interessanten Muster erst in den Volltextdaten und nicht in der Voruntersuchung? Die Erklärung ist, dass die Schwankungen in den Anteilen von Wiedergabe innerhalb eines Erzähltextes so stark sind, dass sie die beobachteten Trends überlagern. Abb. 5 zeigt einen Datenpunkt für jeden der 1000 Token-Abschnitte aus dem Untersuchungskorpus. Zwar werden in der Gesamtheit dieser Datenpunkte die gleichen Trends sichtbar wie in Abb. 4, doch wenn man 'wie bei der Testauswertung 'nur wenige zufällig gezogene 1000-Token-Abschnitte betrachtet, ist es unwahrscheinlich, dass sie erkennbar wären. Die Ausweitung auf mehr Material, die durch die Anwendung automatischer Methoden möglich wurde, führt hier also zu einem Erkenntnisgewinn, der sonst nur mit extremem Annotationsaufwand möglich gewesen wäre. Da die schlechteren Erkennungsraten des Direkte-Wiedergabe-Erkenners für Texte in der Ich-Perspektive bekannt sind, wurde für einen großen Teil der Texte die Erzählperspektive ermittelt. Die Durchmischung ist sowohl bei den Hochliteraturtexten als auch bei den Heftromanen gegeben und Texte beider Perspektiven platzieren sich an unterschiedlichen Stellen. Einzig der Bereich mit sehr niedrigem Anteil von direkter Wiedergabe (<17%) ist ausschließlich durch Texte in Er-Perspektive besetzt. Der Einfluss der Erzählperspektive ist ein Faktor, der in weiteren Untersuchungen genauer betrachtet werden sollte. Mit den vorhandenen Werkzeugen ist es denkbar, die Studien auf noch mehr Textmaterial auszuweiten und dabei auch weitere Genres von Heftromanen zu untersuchen. Zudem lässt sich die Methodik leicht auf Fragestellungen zu den Anteilen von Redewiedergabe in anderen Textgruppen übertragen, z.B. zwischen fiktionalem und nicht-fiktionalem Material oder im diachronen Vergleich. Wir arbeiten zudem im Redewiedergabe-Projekt weiter daran, unsere automatischen Erkenner zu verbessern, insbesondere auch den für freie-indirekte Wiedergabe (zum aktuellen Stand vgl. Brunner et al. 2019c). Die im Redewiedergabe-Projekt entwickelten Erkenner werden nach Abschluss des Projekts im Frühjahr 2020 der Forschungsgemeinschaft zur Verfügung gestellt werden, ebenso wie große Teile des verwendeten manuell annotierten Trainingsmaterials.",de,Art Weise Rede Gedanke Figur Erzähltext einbinden traditionell Aspekt Narratologie Genette scheffel vorgestellt Studie untersuchen Anteil unterschiedlich Redewiedergabeform Vergleich Literaturtype Gegensätzlich Ende spektrums Hochliteratur definieren Werk Auswahllist Literaturpreise stehen heftromanen massenproduziert erzählwerken zumeist Zeitschriftenhandel vertreiben abwertend Roman Unterschicht nusser bezeichnen These Literaturtype hinsichtlich Erzählweise unterscheiden verwendet wiedergabeformen niederschlagen Fokus Untersuchung liegen Dichotomie direkt Wiedergabe klassisch Rhetorik aufmachen mchal Studie manuell annotiert daten evaluieren Validität automatisch Annotationswerkzeuge Anschluss einsetzen Menge betrachtet Material beträchtlich erweitern Kontrastierung heftroman Hochliteratur quantitativ Methode liegen Studie Frage sprachlich thematisch Komplexität beschäftigen Jannidis Konle Leinen verwendet Annotationssystem Erkenner Rahmen entwickeln Brunner et Voruntersuchung heftroman zufällig Textausschnitt Token ziehen Heftromane typischerweise Reihe unterschiedlich Fokus erscheinen betrachten verhalten unterschiedlich Hälfte Genre liebesroman horrorroman wählen Text Erstannotator unabhängig voneinander bearbeiten Person erstellen Grundlage Konsensannotation annotation Verglich unstimmigkeiten bereinigen nötig offensichtlich Fehler korrigieren annotationssyst Brunner et erfassen sowohl Wiedergabe Rede Gedanke geschrieben umfassen haupttypen Wiedergabe Wiedergabe folgend ausschließen automatisch Erkennung Form unzuverlässig erweisen Form indirekt erzählt Wiedergabe zusammengefasst Form umfassen sowohl klassisch indirekt Wiedergabe Rahmenformel abhängig Proposition strukturell abweichend häufig stark zusammenfassende stehen Gegensatz direkt Wiedergabe insofern Rede Gedanke schriftlich Öußerung Figur Erzählertext integrieren anstatt Zitat klar absetzen automatisch erkenner beruhen Deeplearning gut Einschätzung Erkennungswerte geben paar Wort Vorkommenshäufigkeit redewiedergabetype Konsensannotierte testdaten liegen durchschnittlich Anteil direkt Wiedergabe knapp Token stark Schwankung Tabell zeigen Übereinstimmungswerte Erstannotator Eindruck vermitteln verlässlich Mensch durchgeführt Annotation Tabell zeigen Form direkt übereinstimmungsquoten automatisch Methode Vergleich Konsensannotation baselin Erkenner annehmen jeder token Wahrscheinlichkeit Wiedergabe klassifizieren kommen Testdat Samples direkt Precision recall Precision recall wobei einzelscores heftromane Hochliteratur direkt sein Hochliteratur direkt Wiedergabe erkennungsrater automatisch Methode heftroman Schwankung Textausschnitt Problem treten insbesondere Kombination unmarkiert Wiedergabe Hochliteratur häufig vorkommen dennoch maschinell erkenner erzielt Ergebnis Fall deutlich stabil Identifikation direkt Wiedergabe anhand anführungszeich insgesamt neigen Erkenner Anteil direkt Wiedergabe eher unterschätzen durchschnittlich absolut Fehler Abschätzung Anteil liegen Schnitt Wiedergabe betonen übereinstimmungsquote Mensch deutlich schlecht Tabelle ursache stark Integration Erzähltext sowohl genau Abgrenzung Entscheidung Wiedergabe werten schwierig automatisch Methode erreichen heftroman fast gleich Verlässlichkeit wiederum schwierig erweisen Anteil gering Schwankung unterliegen Anteil direkt durchschnittlich absolut Fehler deutlich gering wobei Anteil eher unterschätzen Erzählweise Text Zusammenspiel Wiedergabetype Interesse visualisieren Textausschnitt unterschiedlich Untersuchungsgrupp Scatterplot abb Darstellung Basis manuell lässen Trend erkennen niedrig Wert sowohl direkt zusammenzuclustern Hochliteratur liebesroman stark streuen erscheinen Permutationstest koplenig lassen Vergleich heftroman Hochliteratur dimension signifikant Unterschied nachweisen Vergleich genre lediglich Abweichung Hochliteratur Horrorroman Anteil Wiedergabe signifikant legen automatisch annotiert daten zugrunde verschwinden signifikanz nächster Schritt erweitern untersuchungsmaterial stark korpus Volltext zusammenstellen wobei diesmal Unterschied Hochliteratur einzeln Genre Fokus rücken jeweils heftroman unterschiedlich Reihe kontrastieren unterschiedlich genre repräsentieren tab Ziel größtmöglich Diversität Autor erreichen verhindern Autorensignal gruppenzugehörigkeiten überlageren eigentlich interessieren problematisch Horrorroman Autor Reihe extrem dominieren Krimis Autoreninformation verfügbar Reihe Jerry Cotton unterschiedlich Autor verfassen Karr Text automatisch erkennern komplett annotiert Variation textläng insbesondere Gruppe Hochliteratur stark Text zerlegt Anteil direkt Wiedergabe berechnen Ergebnis anschließend Text mitteln analog standardisiert Testdate zeigen Auswertung klar Unterschied dimensionen Anteil direkt Wiedergabe Hochliteratur gering Anteil Wiedergabe hoch Signifikanz Unterschied Unterschied Genre lassen Permutationstest bestätigen Abbildung Betrachtung beide dimension Relation abb fallen sofort deutlich Streuung aufweisen einzeln Genr Heftromane Gruppe zusammenclustern angesichts Tatsache bewussen reglementierte Reihe definieren enthalten Werk literarisch hochwertig einschätzen Befund erstaunlich bemerkenswert Unterschied konventionalisiert individualistischem erzählen Dimension Redewiedergabetype deutlich quantitativ nachweisen lässt zudem einzig Gruppe äöüberwichen Gegensatz direkt Wiedergabe auftreten Autor Art Weise figurenstimmen Text einbinden sowohl individualistisch eher bereit direkt Zitat wählen innerhalb Gruppe Heftromane genr Liebesroman horrorroman Krimi nahezu linear Anstieg wiedergabeformen Relation zueinander beobachten wobei Anteil direkt Wiedergabe stets hoch differenzieren klar insgesamt gering wiedergabeanteil äökommunikativeren genr Liebesroman krimi stark überlagern genre lassen Signifikant Unterschied nachweisen nehmen Zwischenstellung Text diverser streuen ähnlich Hochliteratur extrem einzig dimension signifikant Unterschied Hochliteratur nachweisen lassen passen Beobachtung Jannidis Konle leinen Sonderstellung einnehmen unterschiedlich Komplexitätsmaße Standardisierter wortlängen hoch abschneiden Genre zeigen interessant Muster volltextdat Voruntersuchung Erklärung Schwankung Anteil Wiedergabe innerhalb erzähltextes stark beobachtet Trend überlagern abb zeigen datenpunken Untersuchungskorpus Gesamtheit Datenpunkt gleich Trend sichtbar abb Testauswertung zufällig gezogen betrachten unwahrscheinlich erkennbar sein Ausweitung Material Anwendung automatisch Methode führen Erkenntnisgewinn extremem Annotationsaufwand Schlechtere Erkennungsrat Text Text Erzählperspektiv ermitteln Durchmischung sowohl Hochliteraturtext heftroman geben Text beide Perspektive platzieren unterschiedlich Stelle einzig Bereich niedrig Anteil direkt Wiedergabe ausschließlich Text besetzen einfluss Erzählperspektive Faktor Untersuchung genau betrachten vorhanden Werkzeug denkbar Studie textmaterial ausweiten Genr Heftroman untersuchen zudem lässen Methodik Fragestellung Anteil Redewiedergabe Textgruppe übertragen fiktional Material diachron Vergleich arbeiten zudem automatisch Erkenner verbessern insbesondere Wiedergabe aktuell Stand Brunner et entwickelt Erkenner Abschluss Projekt Frühjahr Forschungsgemeinschaft Verfügung stellen Teil verwendet manuell annotiert Trainingsmaterial,"[('wiedergabe', 0.5452860250359235), ('hochliteratur', 0.3308340316391137), ('anteil', 0.22965047084940884), ('heftroman', 0.20359017331637766), ('direkt', 0.20026710287993782), ('erkenner', 0.18795996894043543), ('horrorroman', 0.11531340702430433), ('unterschied', 0.10430012872955383), ('heftromane', 0.10179508665818883), ('liebesroman', 0.10179508665818883)]"
2020,DHd2020,247_final-SCHMIDT_Thomas_Der_Einsatz_von_Distant_Reading_auf_einem_Kor.xml,Der Einsatz von Distant Reading auf einem Korpus deutschsprachiger Songtexte,"Thomas Schmidt (Universität Regensburg, Deutschland); Marlene Bauer (Universität Regensburg, Deutschland); Florian Habler (Universität Regensburg, Deutschland); Hannes Heuberger (Universität Regensburg, Deutschland); Florian Pilsl (Universität Regensburg, Deutschland); Christian Wolff (Universität Regensburg, Deutschland)","Songtexte, Lyrics, Distant Reading, Sentiment Analysis, Topic Modeling","Programmierung, Inhaltsanalyse, Sprache, Text","Die Idee des Distant Reading (Moretti, 2002) ist davon geprägt, durch den Einsatz von Methoden der computergestützten Textanalyse und Textvisualisierung große Mengen an Literatur zu explorieren, um Einsichten zu gewinnen, die mit herkömmlichen Methoden nicht möglich sind. Der Einsatz von Distant Reading wird dabei mittlerweile auch außerhalb der Literaturwissenschaften untersucht wie z.B. in den Religionswissenschaften (Pfahler et al., 2018). Im folgenden Beitrag wird ein Projekt vorgestellt, in dem der Einsatz und Nutzen von Distant Reading in ersten Analysen auf einer größeren Menge deutschsprachiger Songtexte exploriert wird. Ziel des Projekts ist es, mittels Distant Reading Unterschiede in gängigen Genres populärer Musik herauszukristallisieren. Im Bereich des Text Mining wird die Analyse von Songtexten vor allem im Kontext von Retrieval- und Recommender-Aufgaben betrieben. Ziel ist meist die automatische Klassifikation und Vorhersage verschiedener Kategorien, z.B. dem Genre (Fell & Sporleder, 2014; De Sousa et al., 2016). Außerhalb dieses Arbeitsgebiets findet man in Bereichen der Kultur- und Literaturwissenschaften sowie der Psychologie Studien mit Songtexten als Untersuchungsgegenstand (Cole, 1971; Kuhn, 1999). Forschungsinteressen umfassen dabei Analysen spezifischen Musikern ( Als Plattform für die Akquise der Songtexte wurde Für jeden gewählten Interpreten wurden über ein Skript alle Songtexte mit Metadaten von LyricWiki akquiriert. Die Akquise des Korpus wurde mittels eines frei verfügbaren angepassten ruby-Skripts durchgeführt Abbildung 1 illustriert Eckdaten zum Gesamtkorpus und den Künstlern. In der Spalte ""Bekannte Vertreter"" werden einige Künstler beispielhaft angegeben. Abbildung 2 zeigt die Songverteilung im zeitlichen Verlauf und Genre-Kontext auf. Im Bereich des Preprocessing wurden Stoppwörter entfernt und alle Wörter zu Normalisierungszwecken in Kleinschreibung gebracht. Für die allgemeine Textanalyse und das Topic Modeling wurden alle Analysen mittels Die Repetition von besonders bedeutenden Wörtern ist ein gängiges Stilmittel bei der Gestaltung von Songtexten. Aus diesem Grund betrachten wir die Analyse der häufigsten Wörter von Songtexten als besonders aufschlussreich. Die folgenden Bilder (Abbildung 3-6) illustrieren die 10 häufigsten Wörter (Most Frequently Used Words; MFWs) der einzelnen Genres. Man erkennt, dass es drei Wörter gibt, die in allen vier Genres gleichmäßig stark vertreten sind: ""Welt"", ""Leben"" und ""Zeit"". Diese Konzepte sind demnach konsistenter Inhalt deutschsprachiger Liedtexte unabhängig vom Genre. Die größte Differenzierung zeigen die Genres Rap, in dem Terme der Umgangs- und Jugendsprache enthalten sind, aber auch thematische Schwerpunkte deutlich werden (""Geld"") sowie das Genre Schlager, das vor allem von emotionalen Termen wie ""Liebe"", ""Herz"" oder ""Glück"" dominiert wird.  Man erkennt, dass für alle 4 Genres insbesondere Varianten von Liebe einen erheblichen Beitrag zur positivien Polarität leisten. Rap grenzt sich deutlich mit für das Genre typischen Themen ab, ausgedrückt durch Wörter wie ""reich"" und mit Slang (""hart"", ""alter""). Alle Genres weisen insgesamt auf eine negative Polarität hin. Entgegen der naiven Intuition sind die Genres ""Rap"" und ""Rock"" dabei noch am positivsten (gemessen an den normalisierten Werten) bewertet. Erste Analysen machen jedoch auch Probleme der lexikonbasierten Sentiment-Analyse deutlich. Die Wörter ""wein"" (weinen) und ""feuer"" (das Feuer) sind in SentiWS als negativ markiert, haben aber in unseren Texten oft eher positive Konnotationen. Bei dem Wort ""wein"" dann, wenn dieses durch die Normalisierung von ""der Wein"" hergeleitet wird. In zukünftigen Arbeiten wollen wir mit einem domänenspezifischen Lexikon arbeiten, das für die jeweilige Anwendungsdomäne optimiert ist. Topic Modeling ist eine Methode, um den Anteil verschiedener Themen in Dokumenten zu analysieren. Ein Thema ist dabei ein selbst definiertes Label für eine Liste von Wörtern, die besonders häufig zusammen auftreten. Als Algorithmus wurde Latent Dirichlet Allocation (LDA) gewählt (Blei et al., 2003). Das Topic Modeling wurde separat für die einzelnen Genres durchgeführt, um Unterschiede und Gemeinsamkeiten zu untersuchen. Wir sind momentan noch am Anfang der Analyse der einzelnen Topics, aber neben Differenzen werden auch Topics gefunden, die ähnliche Konzepte widerspiegeln. Folgende Visualisierungen geben die Wortlisten wider, die wir jeweils als das Topic ""Liebe"" in den einzelnen Genres benannt haben. Die Wortgröße gibt die Häufigkeit des Wortes im jeweiligen Sub-Korpus wider (Abbildung 8). Auffällig ist, dass insbesondere bei Rap familiäre Begriffe wie ""Mama"", ""Vater"" oder auch ""Bruder"" Bestandteil des Topics sind, was traditionellerweise ein häufiger Schwerpunkt im Rap-Genre ist. In unseren zukünftigen Arbeiten wollen wir insbesondere das Korpus systematisch vergrößern und verbessern. Momentane Probleme sind z.B. die Ungleichverteilung in der Menge bezüglich der Genres aber auch ein Fokus auf eher aktuelle Künstler. Wenngleich wir schon erste Eigenheiten der Genres feststellen konnten, wollen wir Methoden wie Sentiment Analysis und Topic Modeling noch weiter explorieren, indem wir beispielsweise die Varianz der Sentiments untersuchen. Des Weiteren wollen wir unsere Arbeit aber auch auf andere Textanalyse-Möglichkeiten wie Kollokationsprofile von Keywords, Named Entity Recognition und Stilometrie ausweiten. Durch die Zusammenarbeit mit Musik- und Literaturwissenschaftlern wollen wir in Zukunft auch explorieren, welche weiteren Forschungsfragen mit Hilfe größerer Korpora und Distant Reading-Methoden beantwortet werden können.",de,Idee distant Reading moretti prägen Einsatz Methode computergestützt Textanalyse Textvisualisierung Menge Literatur explorieren Einsicht gewinnen herkömmlich Methode Einsatz distant Reading mittlerweile außerhalb Literaturwissenschaften untersuchen Religionswissenschafte Pfahler et folgend Beitrag Projekt vorstellen Einsatz nutzen distant Reading Analyse groß Menge deutschsprachig Songtext explorieren Ziel Projekt mittels distant Reading Unterschied gängig Genre populär Musik herauszukristallisieren Bereich Text Mining Analyse Songtext Kontext betreiben Ziel meist automatisch Klassifikation Vorhersage verschieden kategorien Genre Fell Sporleder de sousa et außerhalb Arbeitsgebiet finden Bereich Literaturwissenschaften Psychologie Studie Songtext Untersuchungsgegenstand Cole Kuhn Forschungsinteresse umfassen analys spezifisch Musiker Plattform Akquise Songtext gewählt interpreter Skript Songtext metadaten Lyricwiki akquirieren Akquise Korpus mittels frei verfügbar angepasst durchführen Abbildung illustrieren eckdaen Gesamtkorpus Künstler spalt bekannt Vertreter Künstler beispielhaft angeben Abbildung zeigen Songverteilung zeitlich Verlauf Bereich Preprocessing Stoppwörter entfernt Wörter normalisierungszwecken Kleinschreibung bringen allgemein Textanalyse Topic Modeling analyse mittels Repetition bedeutend wörtern gängig Stilmittel Gestaltung Songtext Grund betrachten Analyse häufig Wörter Songtexte aufschlussreich folgend Bild Abbildung illustrieren häufig Wörter Most Frequently used words mfws einzeln Genre erkennen Wörter genre gleichmäßig stark vertreten Welt leben Konzept demnach konsistent Inhalt deutschsprachig Liedtexte unabhängig Genre groß Differenzierung zeigen genr Rap Terme Jugendsprache enthalten thematisch Schwerpunkt deutlich Geld Genre Schlager emotional Term lieb Herz Glück dominieren erkennen genre insbesondere Variant Liebe erheblich Beitrag Positivien Polarität leisten Rap grenzen deutlich Genre typisch Thema ausdrücken Wörter reich Slang hart alt genr weisen insgesamt negativ Polarität entgegen naiv Intuition genr rap Rock positiv messen Normalisiert werten bewerten Analyse Problem lexikonbasiert deutlich Wörter Wein wein Feuer Feuer Sentiws negativ markieren unser Text eher positiv Konnotation Wort Wein Normalisierung Wein hergeleiten zukünftigen arbeiten domänenspezifisch Lexikon arbeiten jeweilig Anwendungsdoman optimieren Topic Modeling Methode Anteil verschieden Thema dokumenten analysieren Thema definiert Label Liste wörtern häufig auftreten Algorithmus latent dirichlet Allocation lda wählen blei et Topic Modeling Separat einzeln Genr durchführen Unterschied Gemeinsamkeit untersuchen momentan Anfang Analyse einzeln Topics Differenz Topic finden ähnlich Konzept widerspiegeln folgend Visualisierung geben Wortliste wider jeweils Topic Liebe einzeln Genre benennen Wortgröße Häufigkeit Wort jeweilig wider abbildung auffällig insbesondere Rap familiär begriffe Mama Vater Bruder Bestandteil Topic traditionellerweise häufig schwerpunken unser zukünftigen arbeiten insbesondere Korpus systematisch vergrößern verbessern momentan Problem Ungleichverteilung Menge bezüglich Genre Fokus eher aktuell Künstler wenngleich Eigenheit Genre feststellen Method Sentiment Analysis Topic Modeling explorieren beispielsweise Varianz Sentiment untersuchen Arbeit Kollokationsprofile Keywords named entity Recognition Stilometrie ausweiten Zusammenarbeit literaturwissenschaftler Zukunft explori Forschungsfrag Hilfe groß Korpora distant beantworten,"[('songtext', 0.30878011517873866), ('genre', 0.29482202168009947), ('rap', 0.22100930242696884), ('wein', 0.22100930242696884), ('topic', 0.20747888322097208), ('distant', 0.166396471142975), ('wörter', 0.14992596668770233), ('künstler', 0.1400694329775683), ('genr', 0.13887917974993436), ('modeling', 0.12812590302703664)]"
2020,DHd2020,246_final-SCHOLGER_Martina_Das_Theater_mit_dem_Theater__Thementransfer.xml,Das Theater mit dem Theater: Thementransfer in den Spectators,"Alexandra Fuchs (Universität Graz, Österreich); Bernhard Geiger (Know-Center Graz, Österreich); Elisabeth Hobisch (Universität Graz, Österreich); Philipp Koncar (Technische Universität Graz, Österreich); Jacqueline More (Universität Graz, Österreich); Sanja Saric (Universität Graz, Österreich); Martina Scholger (Universität Graz, Österreich)","Zeitschriften, Aufklärung, TEI, Topic Modelling, Netzwerkanalyse, 18. Jahrhundert","Transkription, Inhaltsanalyse, Annotieren, Netzwerkanalyse, Text, Visualisierung","Die journalistische Gattung der ""Spectators"" des 18. Jahrhunderts stellt ein wichtiges Kulturerbe aus der Zeit der Aufklärung dar. Die Zeitschriften entsprachen dem demokratischen Ideal, kulturelle und moralische Fragen in nicht-akademischen Kreisen zu verbreiten und Werte der Aufklärung wie Weltoffenheit, Toleranz, intellektuelle Kritik und soziale Verantwortung zu popularisieren. Ausgehend von den englischen Modellzeitschriften Anhand der Untersuchung von populären Themen mit maschinellen Methoden präsentiert der Beitrag zentrale Linien des Transfers von Diskursen innerhalb des Genres der Spectators und reflektiert damit den Zeitgeist des 18. Jahrhunderts. Mit Hilfe einer Kombination aus Als Fallbeispiel kann das Thema Theater angeführt werden, das in zahlreichen Zeitschriften unterschiedlicher Länder unabhängig voneinander diskutiert wurde. Während in Frankreich das neoklassizistische Theater im 18. Jahrhundert bereits vollends etabliert und somit als Thema in den Spectators weniger relevant war, erlebten Italien und Spanien eine bewegte nationale Theaterdiskussion. Seit dem 16. Jahrhundert hatte sich das italienische Theater stetig weiterentwickelt. Im 18. Jahrhundert wurde es jedoch als Repräsentationsmedium für ein modernes Italien auserkoren und erlebte eine massive Veränderung. Unabhängig davon wurde auch in Spanien der politische Streit zwischen Progressisten und Traditionalisten über das Thema des Theaters ausgetragen: Spanische Intellektuelle versuchten zunehmend durch kulturelle Reformen den intellektuellen Anschluss an Europa zu schaffen und lieferten sich mit Traditionalisten einen regelrechten Streit über eine radikale Reform des Theaters nach neoklassizistisch französischem Vorbild. (Vgl. z.B. Guinard 1973, 133-138; Ertler 2003, 120-124) Die Basis der Untersuchungen bildet das Korpus von etwa 4000 Texten, das im Darüber hinaus werden die (manuellen und maschinellen) Annotationen verwendet, um das zeitliche Auftreten ausgewählter Themen innerhalb einer Sprachgemeinschaft und sprachenübergreifend zu analysieren. Als Referenz für diese Untersuchungen dient das Zeitschriftennetzwerk der Spectators: Dieses stellt die Abhängigkeit von Zeitschriften unterschiedlicher Sprachgemeinschaften hinsichtlich Übersetzung, Adaption und Imitation dar. ",de,journalistisch Gattung spectators Jahrhundert stellen wichtig Kulturerbe Aufklärung dar Zeitschrift entsprechen demokratisch Ideal kulturell moralisch Frage Kreise verbreiten Wert Aufklärung Weltoffenheit Toleranz intellektuell Kritik sozial Verantwortung popularisieren ausgehend englisch Modellzeitschrift anhand Untersuchung populär Thema maschinell Methode präsentieren Beitrag zentral linien Transfer diskurse innerhalb genr Spectator reflektieren Zeitgeist Jahrhundert Hilfe Kombination Fallbeispiel Thema Theater anführen zahlreich Zeitschrift unterschiedlich Land unabhängig voneinander diskutieren Frankreich neoklassizistisch Theater Jahrhundert vollends etablieren somit Thema spectators relevant erleben Italien Spanien bewegt national Theaterdiskussion Jahrhundert italienisch Theater stetig weiterentwickeln Jahrhundert Repräsentationsmedium modern Italien auserkoren erleben massiv Veränderung unabhängig Spanien politisch Streit Progressist Traditionalist Thema Theater austragen spanisch Intellektuell versuchen zunehmend kulturell Reform intellektuell Anschluss Europa schaffen liefern Traditionalist regelrecht Streit radikal Reform Theater neoklassizistisch französisch Vorbild Guinard Ertler Basis Untersuchung bilden korpus Text hinaus Manuelle Maschinelle annotationen verwenden zeitlich auftret ausgewählter Thema innerhalb Sprachgemeinschaft sprachenübergreifend analysieren Referenz Untersuchung dienen Zeitschriftennetzwerk Spectator stellen Abhängigkeit Zeitschrift unterschiedlich Sprachgemeinschaft hinsichtlich Übersetzung Adaption Imitation dar,"[('theater', 0.3147967437014291), ('intellektuell', 0.2471982618467713), ('thema', 0.2182172241464353), ('zeitschrift', 0.178053134845785), ('traditionalist', 0.17693210387038527), ('spectator', 0.17693210387038527), ('neoklassizistisch', 0.17693210387038527), ('streit', 0.17693210387038527), ('reform', 0.17693210387038527), ('spectators', 0.17693210387038527)]"
2020,DHd2020,261_final-DENNERLEIN_Katrin_Datamodelling_Drama_and__Musical_theater.xml,Datamodelling Drama and (Musical)theater,Birk Weiberg; Klaus Illmayer; Katrin Bicher; Gesa zur Nieden; Katrin Dennerlein,"Ontologies, Metadata, Databases, Conversion, Discovering, Gathering","Ontologies, Metadata, Databases, Conversion, Discovering, Gathering","Die Katalogisierung von Sammlungs- und Bibliotheksbeständen und zahlreiche Datenbankprojekte aus der Dramen-, Theater- und Musikforschung haben in den letzten Jahrzehnten eine bisher kaum berücksichtigte Fülle von Material zum (Musik-)theater in gedruckter und handschriftlicher Form zu Tage gefördert und recherchierbar gemacht. Die ebenfalls rasch voranschreitende Bild- und Metadatendigitalisierung macht dieses Material der Forschung einfach zugänglich. Ein Portal, das die vielen Einzelprojekte zu Aufführungsdaten, Texten, Noten und Werken gemeinsam recherchierbar machen würde, so dass Strukturen und Zusammenhänge wie Werk- und Aufführungsserien, Gattungszusammenhänge oder Popularität und Wirkung sichtbar werden, gibt es derzeit jedoch nicht. Stattdessen wurden in den  letzten Jahren inhaltlich und technisch sehr heterogene Datenbanken zur Erfassung von Dramentexten, Libretti, Noten, Aufführungen und Theatertruppen angelegt. Um diesen Teil des kulturellen Erbes strukturiert zugänglich zu machen, wäre es nötig Material aus Bibliothekskatalogen, Archivbeständen, Findbüchern und bereits bestehenden Datenbankprojekten zu verknüpfen. Grundlage dafür wären eine umfassende Ontologie des (Musik-)Theaterbereichs, das die Relationen der Objekte und Metadaten abbildete und die Abfrage der Zusammenhänge möglich machte. Zu erfassen wären einerseits die Materialien wie handschriftliche Soufflierbücher, Noten, Theaterzettel, Ariendrucke, Kupferstiche, Videomaterial, Zeitungsberichte usw., aber auch die sie verknüpfenden Praktiken des Produzierens, Bearbeitens, Übersetzens, Aufführens, Kombinierens, Druckens und Distribuierens. Einen zentralen Ansatzpunkt scheint  Swiss Performing Arts Datamodel zu bieten.   In der Aufführungsdatenbank ""Theadok"",  An der Staats- und Universitätsbibliothek Dresden (SLUB) soll der RISM zu einem zentralen Nachweisinstrument für Musikdrucke des 16. bis 18. Jahrhunderts ausgebaut werden.   ",de,Katalogisierung bibliotheksbeständ zahlreich datenbankprojekte Musikforschung letzter Jahrzehnt berücksichtigt Fülle Material gedruckt handschriftlich Form fördern recherchierbar ebenfalls rasch voranschreitend Metadatendigitalisierung Material Forschung einfach zugänglich Portal einzelprojekte aufführungsdaten text not Werk gemeinsam recherchierbar Struktur Zusammenhäng aufführungsserien gattungszusammenhäng Popularität Wirkung sichtbar derzeit stattdessen letzter inhaltlich technisch heterogen datenbanken Erfassung dramentext Libretti not aufführung Theatertruppe anlegen kulturell erbes strukturiern zugänglich nötig Material Bibliothekskatalog Archivbeständ findbüchern bestehend Datenbankprojekt verknüpfen Grundlage sein umfassend Ontologie Relation Objekt Metadat abbildet Abfrage zusammenhänge erfassen sein einerseits Materialium handschriftlich Soufflierbücher non Theaterzettel ariendrucken kupferstich videomaterial zeitungsberichter verknüpfend Praktik produzierens Bearbeiten übersetzens aufführens Kombinierens druckens distribuierens zentral ansatzpunkt scheinen swiss Performing arts Datamodel bieten Aufführungsdatenbank Theadok Universitätsbibliothek Dresden slub Rism zentral Nachweisinstrument Musikdrucke Jahrhundert ausbauen,"[('not', 0.19951941093227188), ('recherchierbar', 0.19098958926587314), ('handschriftlich', 0.1840202175039136), ('material', 0.18186733363392196), ('sein', 0.1343541972064596), ('zugänglich', 0.11448158636770063), ('übersetzens', 0.11300772855244531), ('nachweisinstrument', 0.11300772855244531), ('aufführungsserien', 0.11300772855244531), ('aufführungsdatenbank', 0.11300772855244531)]"
2020,DHd2020,239_final-KUHN_Jonas_Textanalyse_mit_kombinierten_Methoden___ein_konze.xml,Textanalyse mit kombinierten Methoden 'ein konzeptioneller Rahmen für reflektierte Arbeitspraktiken,"Jonas Kuhn (Universität Stuttgart, Deutschland); Axel Pichler (Universität Stuttgart, Deutschland); Nils Reiter (Universität Stuttgart, Deutschland); Gabriel Viehhauser (Universität Stuttgart, Deutschland)","Arbeitspraktiken, Methoden, Reflexion","Modellierung, Kontextsetzung, Theoretisierung, Methoden, Forschungsprozess, Text","Die Zahl und Intensität der Aktivitäten im interdisziplinären Spektrum der Dieser Beitrag fokussiert auf denjenigen Teilbereich der DH, der sich zum Ziel setzt, adaptierbare datenorientierte Computermodelle methodisch adäquat in kombiniert komputationell/geisteswissenschaftliche Arbeitspraktiken zu integrieren. Methodologisch zielt diese Teildisziplin also darauf ab, Forschungsfragen aus einem geisteswissenschaftlichen Kontext mit kombinierten Methoden (bzw. mit ""mixed methods"") adäquat bearbeiten zu können. Es erscheint uns daher an der Zeit, intensiver über einen geeigneten konzeptionellen Rahmen für Arbeiten aus einem der DH-Teilbereiche zu diskutieren, in denen kombinierte Methoden zum Einsatz kommen 'einen Rahmen, der eine gleichermaßen adäquate Reflexion für alle einfließenden Vorannahmen ermöglicht und zudem einfach genug darstellbar ist, dass sich ein methodisch adäquater Workflow mit vertretbarem Aufwand und ohne Brüche konstruieren lässt. In diesem Beitrag stellen wir Kernpunkte eines generalisierten arbeitspraktischen Vorgehensmodells dar, das wir aus den Erfahrungen des Stuttgarter  Als konkrete Illustration des Vorgehens mögen Arbeiten aus dem QuaDramA-Projekt dienen (Krautter/Pagel 2019, Krautter et al. 2018): ein Korpus von deutschsprachigen Dramen wird mit kombinierten Methoden erschlossen; ein exemplarischer Analyseschritt dabei liegt in der Klassifikation von Figuren nach bestimmten Typen. Einige Figurentypen sind bereits literaturwissenschaftlich etabliert (zärtlicher Vater) oder lassen sich relativ treffsicher aus der Figurentafel (Tochter) oder den Metadaten (Titelfigur) extrahieren. Andere Typen wie z.B. die Protagonistin bzw. der Protagonist entziehen sich einer aus unmittelbar verfügbaren Texteigenschaften oder Metadaten ableitbaren Zuweisung, sind jedoch von Bedeutung für literaturhistorische Betrachtungen (etwa für die Frage, inwieweit Emilia Galotti als Titelfigur aus G. E. Lessings bürgerlichem Trauerspiel (1772) den Status einer Protagonistin hat). Eine Annäherung an derartige Kategorien mit kombinierten Methoden kann ausgehend von klaren Fällen eine vorläufige Operationalisierung ansetzen, darauf aufbauend datenbasierte Computermodelle erzeugen und die Modellvorhersagen auf dem Gesamtkorpus in den Prozess einer Verfeinerung der Operationalisierung einfließen lassen. Als Ausgangspunkt skizziert Abbildung 1 ein DH-Vorgehen, das sich bei nicht-trivialen Modellierungsaufgaben etabliert hat 'in Anlehnung an ausgeprägte methodische Konventionen in der Korpus- und Computerlinguistik (vgl. u.a. Hovy/Lavid 2010, Kuhn/Reiter 2015, Stefanowitsch 2018, Kuhn 2019): Die Analysekategorie, die im Rahmen einer geisteswissenschaftlichen Gesamtfragestellung angewendet werden soll, wird konzeptuell operationalisiert 'gängiger Weise in Form von präzisen Annotationsrichtlinien. Der erste zentrale Aspekt für eine effektive Praxis der Methodenkombination liegt in der Das bislang geschilderte Vorgehen fokussiert ausschließlich auf die technische Optimierung der Vorhersagemodelle für fixierte Referenzdaten. Ein effektiver konzeptioneller Rahmen für die Methodenkombination muss daneben Prozessen Raum geben, die eine sukzessive Verfeinerung der Analysekategorien vornehmen, um einem geisteswissenschaftlichen Fragenkomplex gerecht zu werden. Hier sind mehrere Aspekte zu unterscheiden: Letzteres ist allerdings bei einer komputationell/geisteswissenschaftlichen Methodenkombination der Fall. Die Gütekriterien, die zur Revision einer geisteswissenschaftlichen Analysekategorie führen, können grundsätzlich anderen methodischen Prinzipien und Vorannahmen folgen. Eine probehalber vorgenommene Operationalisierung eines vielschichtigen Konzepts in der Textanalyse (z.B. Protagonisten in Dramen) mag sich zum Beispiel als unergiebig erweisen, obgleich die komputationelle Umsetzung entsprechend den Referenzdaten eine hohe Vorhersagequalität ermöglicht. Abbildung 2 zeigt entsprechend eine stärker ausdifferenzierte Skizze des konzeptionellen Rahmens. Der vierte Aspekt ist hier bereits angedeutet: Wie Abbildung 2 suggeriert findet die Übersetzung aus der geisteswissenschaftlichen in die Sphäre der datenbasierten Computermodellierung sinnvollerweise für einzelne Inferenzschritte separat statt (obgleich wie in Fn. 5 angedeutet die Computerarchitektur für einen Schritt selbst technisch komplex sein kann). Es wird deutlich, dass bei der Bearbeitung von nicht-trivialen Fragestellungen rasch ein vielschichtiges Geflecht von Komponenten mit unterschiedlichem Status entsteht. Ein Hauptziel der hier vorgeschlagenen Konzeption liegt darin, das Augenmerk auf genau jene Statusunterschiede zu lenken, die für ein methodisch reflektiertes Vorgehen zu relevanten Vorannahmen relevant sind. Strukturell sind die Elemente unserer Konzeption trotz der darstellbaren Komplexität vergleichsweise einfach 'sie konzentrieren sich auf die Aufgabe der methodenübergreifenden Übersetzung mittels der referenzdatengestützten Operationalisierung und komputationellen Modellierung. Für die konkrete arbeitspraktischen Projektroutine sollte also eine verhältnismäßig übersichtliche Sicht auf die relevanten Komponenten möglich sein. Die abschließende Abbildung 3 demonstriert jedoch, dass der konzeptionelle Rahmen bei Bedarf eine Schnittstelle zu einer sehr differenziert ausgearbeiteten wissenschaftstheoretischen Konzeption wie der von Danneberg/Albrecht 2017 bietet. (Aus Platzgründen können wir in diesem Abstract nicht auf Details eingehen.) Ein reflektiertes Vorgehen kann also auch auf fundamentalere Fragen etwa zur Problematisierung von divergierenden Wissensansprüchen über Disziplingrenzen hinweg eingehen.",de,Zahl Intensität Aktivität interdisziplinären Spektrum Beitrag fokussieren derjenige Teilbereich dh Ziel setzen adaptierbar datenorientiert Computermodelle methodisch Adäquat Kombiniert Komputationell geisteswissenschaftlich Arbeitspraktik integrieren Methodologisch zielen Teildisziplin forschungsfragen geisteswissenschaftlich Kontext Kombiniert Methode mixed Methods adäquat bearbeiten erscheinen intensiv geeignet konzeptionell Rahmen Arbeit diskutieren kombiniert Methode Einsatz Rahmen gleichermaßen adäquat Reflexion einfließend vorannahmen ermöglichen zudem einfach darstellbar methodisch adäquat workflow vertretbar Aufwand Brüch konstrui lässn Beitrag stellen kernpunkte generalisiert arbeitspraktischen vorgehensmodells dar Erfahrung Stuttgarter konkret Illustration Vorgehen arbeiten dienen Krautter Pagel krautt Et Korpus Deutschsprachig dramen kombiniert Methode erschließen exemplarisch Analyseschritt liegen Klassifikation Figur bestimmt Type Figurentype literaturwissenschaftlich etablieren zärtlich Vater lassen relativ treffsicher Figurentafel Tochter Metadat Titelfigur extrahieren Typ Protagonistin Protagonist entziehen unmittelbar verfügbar texteigenschafen metadaten ableitbaren Zuweisung Bedeutung literaturhistorisch Betrachtung Frage inwieweit emilian Galotti Titelfigur Lessings bürgerlich Trauerspiel Status Protagonistin Annäherung derartig kategorien Kombiniert Methode ausgehend klar Fall vorläufig Operationalisierung ansetzen aufbauend datenbasiert Computermodelle erzeugen Modellvorhersag Gesamtkorpus Prozess Verfeinerung Operationalisierung einfließen lassen Ausgangspunkt skizzieren Abbildung Modellierungsaufgaben etablieren Anlehnung ausgeprägt methodisch Konvention Computerlinguistik hovy Lavid Kuhn Reiter stefanowitsch kuhn Analysekategorie Rahmen geisteswissenschaftlich Gesamtfragestellung anwenden konzeptuell operationalisieren gängig Weise Form präzise Annotationsrichtlini zentral Aspekt effektiv Praxis Methodenkombination liegen bislang geschildert vorgehen fokussieren ausschließlich technisch Optimierung Vorhersagemodelle Fixiert referenzdaen effektiv konzeptionell Rahmen Methodenkombination prozessen Raum geben sukzessiv Verfeinerung Analysekategorie vornehmen geisteswissenschaftlich Fragenkomplex gerecht mehrere Aspekt unterscheiden letzterer Komputationell geisteswissenschaftlich Methodenkombination Fall Gütekriterie Revision geisteswissenschaftlich Analysekategorie führen grundsätzlich methodisch prinzipien vorannahmen folgen probehalber vorgenommen Operationalisierung vielschichtig Konzept Textanalyse Protagonist Dram unergiebig erweisen obgleich komputationell Umsetzung entsprechend Referenzdat hoch Vorhersagequalität ermöglichen Abbildung zeigen entsprechend stark ausdifferenziert Skizze konzeptionell Rahmen Aspekt andeuten Abbildung suggerieren finden Übersetzung Geisteswissenschaftlich Sphäre datenbasiert Computermodellierung sinnvollerweise einzeln Inferenzschritt Separat obgleich fn angedeuten Computerarchitektur Schritt technisch komplex deutlich Bearbeitung Fragestellung rasch vielschichtig Geflecht Komponent unterschiedlich Status entstehen Hauptziel vorgeschlagen Konzeption liegen Augenmerk genau Statusunterschiede lenken methodisch reflektiert vorgehen relevant vorannahmen relevant strukturell elemente Konzeption trotz darstellbar Komplexität vergleichsweise einfach konzentrieren Aufgabe methodenübergreifend Übersetzung mittels referenzdatengestützt Operationalisierung komputationell Modellierung konkret arbeitspraktisch projektroutine verhältnismäßig übersichtlich Sicht relevant Komponente abschließend Abbildung demonstrieren konzeptionell Rahmen Bedarf Schnittstelle differenziert ausgearbeitet wissenschaftstheoretisch Konzeption Danneberg Albrecht bieten Platzgründ Abstract Detail eingehen reflektiert vorgehen fundamentaler Frage Problematisierung divergierend wissensansprüch disziplingrenz Hinweg eingehen,"[('kombiniert', 0.2817943662709398), ('geisteswissenschaftlich', 0.21852180566959303), ('komputationell', 0.21365929636220646), ('methodenkombination', 0.1690766197625639), ('vorannahmen', 0.14779625953907682), ('konzeptionell', 0.14777734133255563), ('adäquat', 0.14777734133255563), ('methodisch', 0.14555387345092774), ('analysekategorie', 0.1430636687020637), ('vorgehen', 0.1340039310067861)]"
2020,DHd2020,173_final-WILLAND_Marcus_Passive_Präsenz_tragischer_Hauptfiguren_im_Dr.xml,Passive Präsenz tragischer Hauptfiguren im Drama,"Marcus Willand (Universität Heidelberg, Deutschland); Benjamin Krautter (Universität Heidelberg, Deutschland, Universität Stuttgart); Janis Pagel (Universität Stuttgart); Nils Reiter (Universität Stuttgart, Universität zu Köln)","Digitale Dramenanalyse, Operationalisierung, Korpusanalyse, CLS","Inhaltsanalyse, Strukturanalyse, Beziehungsanalyse, Modellierung, Netzwerkanalyse","Dramen entwerfen einen fiktiven sozialen Raum (Bourdieu 1985), dessen Bewohner sich ständig In unserem Beitrag möchten wir den Zusammenhang zwischen aktiver und passiver Figurenpräsenz in dramatischen Texten untersuchen, indem wir quantitative und qualitative Analysen kombinieren. In einem ersten Schritt entwickeln wir eine Operationalisierung für eine computergestützte Analyse aktiver und passiver Präsenz und werden in einem zweiten Schritt die aus den Analysen resultierenden Ergebnisse mit besonderem Fokus auf Hauptfiguren diskutieren. Seit einigen Jahren gilt die Netzwerkanalyse als eine der zentralen Forschungsgebiete innerhalb der digitalen Dramenanalyse. Typischerweise modellieren Netzwerke auf Basis von Konfigurationsmatrizen (vgl. Marcus 1973, ins. S. 308ff. und Pfister 2001, S. 235–240) aber nur die aktive (szenische) Präsenz von Figuren (Moretti 2011; Trilcke u.a. 2015; Piper u.a. 2017), obwohl Ko-Präsenz-Relationen nur einen eingeschränkten Aussagewert bezüglich der ""soziale Welt"" eines Dramas zulassen. Denn sie beruhen lediglich auf Informationen über die Anzahl an Szenen, in denen Figuren gemeinsam auftreten. Aktive Figuren wurden aber natürlich auch anders beforscht. Karsdorp u.a. (2015) stellen einen Ansatz zur automatischen Bestimmung von Liebesbeziehungen vor, Willand und Reiter (2017) verwenden semantische Wörterbücher, um Figurenrede und Geschlecht in einen Zusammenhang zu stellen. Nalisnick und Baird (2013) analysieren das Die aktive Präsenz von Figuren lässt sich unterschiedlich operationalisieren, etwa indem der Anteil der Rede einer Figur an der Gesamtrede einer Szene oder eines Akts gemessen wird. Abb. 1 zeigt dies für die fünf Akte von In den Akten 1 und 4 ist Emilia überhaupt nicht aktiv präsent. In den Akten 2, 3 und 5 ist sie es, aber der Anteil ihrer Rede vergleichsweise gering. Wieso aber ist sie titelgebende Protagonistin dieses Stücks, wenn sie doch kaum handelt? Deutlich wird das, betrachtet man ihre passive Präsenz: Die Punkte in Abb. 2 repräsentieren die Erwähnungen des Namens ""Emilia"" in der Rede anderer Figuren (y-Achse) im Verlauf des Stücks (x-Achse). Sie zeigen, dass Emilia während des gesamten Stücks von allen Figuren wiederholt erwähnt wird. Diese Zusammenfassend liefern diese Analysen Argumente für die Interpretationshypothese, dass Emilia den dramatischen Konflikt nicht selbst aktiv löst 'was sie zur positiven Hauptfigur machen würde –, sondern lediglich auslöst. So wird sie zum passiv-tragischen Gegenstand der Figurenhandlung. Für jede Figur definieren wir die Gleich in mehrfacher Hinsicht handelt es sich bei der Methode um eine Heuristik: Einerseits erfasst die Analyse von Figurennamen längst nicht alle Erwähnungen einer Figur. Wir gehen aber davon aus, dass der Figurenname mindestens einmal in jeder Szene genannt wird, in der auf eine Figur referiert wird. Andererseits können Szenen sehr unterschiedlich lang ausfallen, was für die hier durchgeführten Analysen der passiven Präsenz unberücksichtigt bleibt. Anders formuliert: Jede Szene ist bei dieser Form der Analyse gleich gewichtet. Unterschiedliche poetologische Funktionalisierungen von Szenen, wie sie im Verlauf der Dramengeschichte zu beobachten sind, u.a. anhand des Abrückens von der Sowohl die aktive als auch die passive Präsenz wird der besseren Vergleichbarkeit halber über die Zahl der Szenen normalisiert. Dafür wird die Menge an aktiven Auftritten sowie passiven Erwähnungen einer Figur durch die Gesamtzahl der Szenen im Drama geteilt, sodass der Gesamtwert der Figurenaktivität immer zwischen 0 (spricht nie/wird nie erwähnt) und 1 (spricht in jeder Szene/wird immer erwähnt) liegt. Somit ergibt sich für die Berechnung: Die automatische Erkennung von Hauptfiguren in dramatischen Texten ist bisher nur in Ansätzen versucht worden (Krautter & Pagel 2019; Fischer u.a. 2018), sie würde auf dem Gebiet der digitalen Dramenanalyse aber die Grundlage für erkenntnisversprechende Anschlussfragen schaffen. Diese Operationalisierung erlaubt es uns, die figuren- und gattungsspezifische Verteilung der Resultate zu vergleichen und so bisher ungesehene Aspekte von Hauptfiguren zu identifizieren. In diesem Beitrag stellen wir das Genre Unser Korpus enthält deutschsprachige dramatische Texte aus der Zeit zwischen 1750 und 1800 (Fischer u.a. 2019). Es ist in zwei Teilkorpora aufgeteilt, die auf sehr unterschiedlichen Poetiken beruhen: Sechs Stücke des Abb. 4 visualisiert die präsentische Auswertung des BT-Korpus. Jeder Punkt stellt dabei eine Figur dar. In den Stücken treten fast so viele weibliche wie männliche Figuren auf, wobei die sowohl aktiv als auch passiv präsentesten Figuren überraschenderweise weiblich sind. Zudem werden keine Extremwerte erreicht: Alle aktiven Präsenzwerte liegen unter 0,7, alle passiven unter 0,5. Eine Gesamtpräsenz von 1 ist bei keiner Figur zu beobachten. Basierend auf den Präsenzwerten kann ein Schwellenwert etabliert werden, der ungefähr bei 0,4 liegt. Dieser Schwellenwert (gestrichelte Linie in Abb. 4) ergibt sich hier nicht vollständig induktiv aus den Daten, sondern wird theoriegeleitet gesetzt. An dieser Stelle greifen die formale, quantitative und die qualitative Analyse ineinander. Die Figurenverteilung in Abb. 5 (SD-Korpus) unterscheidet sich von derjenigen in Abb. 4 deutlich. Der Schwellenwert liegt hier mit 0,6 viel höher. Um als Hauptfigur zu gelten, muss eine Figur im SD also eine höhere Gesamtpräsenz aufweisen als im BT. Dies ist eine der zentralen Erkenntnisse dieses Forschungsbeitrags. Die Figuration (vgl. hierzu Elias 2002) dramatischer Hauptfiguren scheint somit textgruppenspezifisch und durch die Ermittlung des Präsenzwertes analysierbar zu sein. Darüber hinaus ist das Geschlecht ein relevanter Faktor im Gattungsvergleich. Im SD treten insgesamt weniger weibliche Figuren auf und nur 3 von 11 Hauptfiguren sind weiblich. Zudem sind die weiblichen Figuren eher passiv präsent, während die männlichen überwiegend aktiv präsent sind. Auch die insgesamt aktivsten Figuren sind jeweils männlich. Nur eine einzige Figur erreicht den maximalen Präsenzwert von 1, nämlich Guelfo in Klingers Nicht zuletzt ist die Diese Ergebnisse sind ein gewichtiger Hinweis auf grundlegend divergierende Bauprinzipien dramatischer Texte, die sich offenbar nicht nur durch Handlungen und Themen unterscheiden, sondern auch durch die spezifische Präsenzgestaltung von Hauptfiguren. Da diese Unterschiede durch lineares Lesen jedoch kaum identifiziert werden können, möchte dieser Forschungsbeitrag als Argument für die Erweiterung der qualitativ-interpretierenden Dramenanalyse durch quantitative Methoden verstanden werden. Die aktive Präsenz einer Figur lässt sich auch anhand anderer Einheiten skalieren, etwa anhand der Akte oder der Gesamtzahl der in einem Drama gesprochenen Repliken. Es wäre ebenfalls möglich, den Wert der aktiven Präsenz als die Zahl der gesprochenen Tokens (i.d.R. Wörter und Satzzeichen) und den Wert der passiven Präsenz als die Zahl namentlicher Nennungen aufzufassen. Dadurch könnten einige zuvor beschriebene Problematiken ausgeräumt werden, etwa die der differierenden Szenenlängen. Figuren, die nur kurze Passagen sprechen oder punktuell erwähnt werden, hätten dann vermutlich kleinere Aktivitätswerte, als es bei der szenisch gebundenen Präsenzberechnung der Fall ist. Die Rede- und Erwähnungsverteilung dürfte als näher an der vom Zuschauer bzw. Leser wahrgenommenen Realität des fiktiven sozialen Raums liegen. Hierbei stellen sich allerdings auch neue Herausforderungen. So kommt der Koreferenz von Figuren ein deutlich größeres Gewicht zu. Da wir zuverlässige Koreferenzen momentan nur für einzelne Stücke manuell annotiert vorliegen haben und somit auf die namentlichen Erwähnungen beschränkt sind, ergeben sich unter Umständen stark fehlerbehaftete Werte: Wenn etwa Figuren, die nur selten namentlich Erwähnung finden, überproportional stark auf andere Weise referenziert werden. Ist die namentliche Erwähnung hingegen an einzelne Szenen gebunden, hat diese Fehlerquelle geringeren Einfluss auf die Werte. Um die Auswirkungen der unterschiedlichen Operationalisierungen zumindest einer ersten Exploration zu unterziehen, nehmen wir die Präsenzanalyse von Klingers Abb. 7 zeigt die Präsenzanalyse für Verglichen mit den Präsenzwerten in Abb. 6 ergeben sich erhebliche. Aufgrund der wenigen namentlichen Erwähnungen sinkt vor allem die passive Präsenz von Ferdinando von fast 0,8 auf etwa 0,1. Ferdinando wird also konsistent in vielen Szenen erwähnt, die Zahl der Erwähnung bleibt aber insgesamt vernachlässigbar, vergleicht man seinen Wert mit Guelfo. Wir gehen jedoch davon aus, dass die Fehleranfälligkeit der Koreferenzheuristik hier insgesamt ungenauere Ergebnisse liefert. Der Beitrag stellt eine Methode vor, die ein erweitertes Präsenzkonzept operationalisiert, das neben der aktiven Präsenz dramatischer Figuren auch die passive Präsenz umfasst. Die passive Präsenz operationalisieren wir als Zahl der Szenen, in der eine Figur namentlich erwähnt wird, ohne selbst aktiv auf der Bühne zu stehen. Die Ergebnisse unserer Korpusanalysen lassen auf unterschiedliche Bauprinzipien dramatischer Texte schließen, die an die spezifische Präsenz von Hauptfiguren gebunden sind. Für die Zukunft erscheint es fruchtbar, die hier eruierten Erkenntnisse im Lichte poetologischer Setzungen und Funktionalisierungen 'etwa der Vorbildfunktion Shakespeares 'zu untersuchen.  ",de,Dram entwerfen fiktiv sozial Raum bourdieu Bewohner ständig unser Beitrag möchten Zusammenhang aktiv passiv Figurenpräsenz dramatisch Text untersuchen quantitativ qualitativ Analyse kombinieren Schritt entwickeln Operationalisierung computergestützt Analyse aktiv passiv Präsenz Schritt analys resultierend Ergebnis besonder Fokus Hauptfigur diskutieren gelten Netzwerkanalyse zentral forschungsgebiete innerhalb digital Dramenanalyse typischerweise Modelliere netzwerke Basis Konfigurationsmatrize Marcus Pfister aktiv szenisch Präsenz Figur moretti trilcken Piper obwohl Eingeschränkt aussageweren bezüglich sozial Welt Drama zulassen beruhen lediglich Information Anzahl Szene Figur gemeinsam auftreten aktiv Figur beforschen karsdorp stellen Ansatz automatisch Bestimmung Liebesbeziehung Willand Reiter verwend semantisch Wörterbücher Figurenred Geschlecht Zusammenhang stellen Nalisnick baird analysieren aktiv Präsenz Figur lässt unterschiedlich operationalisieren Anteil Rede Figur Gesamtrede Szene Akt messen abb zeigen Akt akte Emilia aktiv präsent akte Anteil Rede vergleichsweise gering wieso titelgebend Protagonistin Stück handeln deutlich betrachten passiv Präsenz Punkt abb repräsentieren Erwähnunge namens emilia Rede anderer Figur Verlauf Stück zeigen Emilia gesamt Stück Figur wiederholt erwähnen zusammenfassend liefern analyse Argument Interpretationshypothese Emilia dramatisch Konflikt aktiv lösen positiv Hauptfigur lediglich auslösen Gegenstand Figurenhandlung Figur definieren mehrfacher Hinsicht handeln Methode Heuristik einerseits erfassen Analyse Figurennam längst Erwähnung Figur Figurenname mindestens Szene nennen Figur referieren andererseits Szene unterschiedlich ausfallen durchgeführt Analyse passiv Präsenz unberücksichtigt bleiben formulieren Szene Form Analyse wichten unterschiedlich poetologisch Funktionalisierunge Szene Verlauf Dramengeschichte beobachten anhand Abrücken sowohl Aktiv passiv Präsenz gut Vergleichbarkeit halber Zahl Szene normalisieren Menge aktiv Auftritt passiv Erwähnung Figur Gesamtzahl Szene Drama teilen sodass Gesamtwert Figurenaktivität sprechen erwähnen sprechen Szene erwähnen liegen somit ergeben Berechnung automatisch Erkennung Hauptfigure dramatisch Text Ansatz versuchen Krautter Pagel Fischer Gebiet digital Dramenanalyse Grundlage erkenntnisversprechend anschlussfrag schaffen Operationalisierung erlauben gattungsspezifisch Verteilung Resultate vergleichen ungesehen Aspekt hauptfiguren identifizieren Beitrag stellen Genre Korpus enthalten deutschsprachig dramatisch Text Fischer Teilkorpora aufteilen unterschiedlich Poetik beruhen Stück abb visualisiern präsentisch Auswertung Punkt stellen Figur dar Stück treten fast weiblich männlich Figur wobei sowohl aktiv passiv präsentest Figur überraschenderweise weiblich zudem Extremwert erreichen aktiv Präsenzwert liegen Passive Gesamtpräsenz Figur beobachten basierend Präsenzwert schwellenwern etablieren ungefähr liegen schwellenwern gestrichelt Linie abb ergeben vollständig induktiv daten theoriegeleitet setzen Stelle greifen formal quantitativ qualitativ Analyse ineinander Figurenverteilung abb unterscheiden abb deutlich Schwellenwert liegen hoch Hauptfigur gelten Figur Sd hoch Gesamtpräsenz aufweisen bt zentral erkenntnisse forschungsbeitrags Figuration hierzu elias dramatisch hauptfiguren scheinen somit textgruppenspezifisch Ermittlung präsenzwertes Analysierbar hinaus Geschlecht relevant Faktor Gattungsvergleich Sd treten insgesamt weiblich Figur hauptfiguren weiblich zudem weiblich Figur eher passiv präsent männlich überwiegend aktiv präsent insgesamt aktivst Figur jeweils männlich einzig Figur erreichen Maximale präsenzweren nämlich Guelfo Klingers zuletzt Ergebnis gewichtig Hinweis grundlegend divergierend Bauprinzipie dramatisch Text offenbar handlung Thema unterscheiden spezifisch Präsenzgestaltung hauptfiguren Unterschied linear lesen identifizieren Forschungsbeitrag Argument Erweiterung Dramenanalyse quantitativ Methode verstehen aktiv Präsenz Figur lässen anhand anderer Einheit skalieren anhand Akt Gesamtzahl Drama gesprochen Replike ebenfalls Wert aktiv Präsenz Zahl gesprochen Token Wörter satzzeich Wert passiv Präsenz Zahl namentlich nennung Aufzufasse können zuvor beschrieben Problematik ausräumen differierend Szenenlänge Figur kurz Passag sprechen punktuell erwähnen vermutlich klein Aktivitätswert szenisch gebunden Präsenzberechnung Fall Erwähnungsverteilung dürfen nah Zuschauer Leser wahrgenommen Realität fiktiv sozial Raum liegen hierbei stellen Herausforderung Koreferenz Figur deutlich größer wichen zuverlässig koreferenzen momentan einzeln Stück manuell annotiert vorliegen somit namentlich Erwähnunge beschränken ergeben umständ stark fehlerbehaftet Wert Figur selten namentlich Erwähnung finden überproportional stark Weise referenzieren namentlich Erwähnung hingegen einzeln Szene binden Fehlerquelle geringeren einfluss Wert Auswirkung unterschiedlich Operationalisierung zumindest Exploration unterziehen nehmen Präsenzanalyse Klingers abb zeigen Präsenzanalyse vergleichen Präsenzwert abb ergeben erheblich aufgrund weniger namentlich Erwähnunge sinken passiv Präsenz Ferdinando fast Ferdinando Konsistent Szene erwähnen Zahl Erwähnung bleiben insgesamt vernachlässigbar vergleichen Wert Guelfo Fehleranfälligkeit Koreferenzheuristik insgesamt ungenauer Ergebnis liefern Beitrag stellen Methode erweitert Präsenzkonzept operationalisieren aktiv Präsenz dramatisch Figur passiv Präsenz umfassen passiv Präsenz operationalisieren Zahl Szene Figur namentlich erwähnen aktiv Bühne stehen Ergebnis Korpusanalyse lassen unterschiedlich Bauprinzipie dramatisch Text schließen spezifisch Präsenz hauptfiguren binden Zukunft erscheinen fruchtbar eruiert erkenntnis licht poetologisch Setzung Funktionalisierunge Vorbildfunktion shakespearer untersuchen,"[('präsenz', 0.34535856410320015), ('aktiv', 0.33425354790768974), ('figur', 0.33201777628691326), ('passiv', 0.32675961950892835), ('szene', 0.2394512434226303), ('namentlich', 0.16337980975446417), ('dramatisch', 0.15324230159126892), ('hauptfiguren', 0.1311816185126864), ('stück', 0.12971152231217747), ('erwähnung', 0.1269810458615663)]"
2020,DHd2020,153_final-WIEDMER_Nathalie_Romeo__Freund_des_Mercutio__Semi_Automatisc.xml,"Romeo, Freund des Mercutio: Semi-Automatische Extraktion von Beziehungen zwischen dramatischen Figuren","Nathalie Wiedmer (Universität Stuttgart, Deutschland); Janis Pagel (Universität Stuttgart, Deutschland); Nils Reiter (Universität Stuttgart, Deutschland, Universität Köln, Deutschland)","Figurenbeziehungen, Figurenverzeichnis, Dramen, Dramenanalyse, GerDraCor","Beziehungsanalyse, Annotieren, Literatur, Text","In diesem Beitrag stellen wir eine Methode vor, um Informationen über Figurenrelationen in dramatischen Texten, die innerhalb der  Das Verfahren 'und dessen Implementierung in einem Python-Skript 'ist auch für in Zukunft digitalisierte Dramen anwendbar, und wird von uns als quelloffene Software zur Verfügung gestellt. Es ist vergleichsweise einfach auf neue Sprachstufen oder Genres anpassbar und liefert 'auch bei nicht-perfekten Ergebnissen 'eine gute Vorlage. Eine Evaluation des Verfahrens erfolgt auf ungesehenen Testdaten. Außerdem veröffentlichen wir einen Datensatz mit extrahierten Figurenrelationen aus deutschsprachigen Dramen, die manuell validiert und korrigiert wurden. Diese Daten werden zur einfachen und breiten Nutzung im TEI-Format in das GerDraCor Unsere Methode unterscheidet zwischen sieben Kategorien von Figurenrelationen (Tabelle 1). Ausschlaggebend für die Zuordnung zu einer der Kategorien sind Signalwörter wie ""Vater"", ""Kammerdiener"", ""Geschwister"" etc. Diese Signalwörter werden in einer kontextfreien Grammatik der entsprechenden Kategorie zugeordnet. Kontextfreie Grammatiken bezeichnen in der Informatik eine Sammlung aller syntaktisch korrekten Programme einer Programmiersprache (Böckenhauer und Hromkoviƒç 2013, 177). Die formalisierte Art, in der die Grammatik alle Regeln einer Programmiersprache enthält, erlaubt es, automatisierte Syntaxanalysen von Programmen durchzuführen (Böckenhauer und Hromkoviƒç 2013, 177). Die Regeln werden mit Hilfe zweier Alphabete beschrieben: Das Terminalalphabet enthält alle Wörter einer Sprache, wohingegen das Nichtterminalalphabet Variablen enthält, die vorgeben, auf welche Art und Weise die Wörter kombiniert werden können (Böckenhauer und Hromkoviƒç 2013, 178). Wir nutzen eine solche Grammatik, um drei verschiedene Zeilenarten im Figurenverzeichnis zu unterscheiden, bei denen es sich um Nichtterminale handelt. Alle in den Sätzen vorkommenden Tokens sind Terminale, deren Kombination und Anzahl Aufschluss darüber gibt, um was für eine Art von Zeile es sich jeweils handelt. Auf diese Weise können auch zeilenübergreifende Relationen erkannt werden. Zu Beginn des Programmablaufs werden die in GerDraCor vorhandenen Figuren-IDs zusammen mit dem Figurenverzeichnis ausgelesen und gespeichert. Da wir die Beziehungen zwischen den Figuren ausschließlich anhand der Angaben im Figurenverzeichnis konstruieren, muss der Dramentext nicht extra eingelesen werden. Daraus ergibt sich die Beschränkung, dass jegliche Beziehungen, die nicht im Figureverzeichnis explizit gemacht werden, vom Programm auch nicht erkannt werden können. Es geht demnach ausschließlich darum, das Personenverzeichnis maschinenlesbar und -interpretierbar zu machen. So ignoriert das Programm beispielsweise auch alle Zeilen, die eine Gruppe von Figuren als Kollektiv einführt, da diese als ""Nummern oder als anonyme Angehörige von Untergruppen"" (Schlaffer 1972, 11) meistens keine eigenen Namen haben und auch keine explizit gemachten Beziehungen. Anschließend werden alle Tokens jeder Zeile des Figurenverzeichnisses daraufhin untersucht, ob es sich dabei um Figurennennungen oder Signalwörter handelt und die Grammatik einem Parser übergeben, der die Zeilen des Figurenverzeichnisses in Baumstrukturen überführt (Abbildung¬†2).  Aus den erstellten Baumstrukturen werden einzelne Informationen ausgelesen, die grundlegend für die Erkennung der Figurenrelationen sind. Zuerst wird überprüft, wie viele IDs sich in einer Zeile befinden. Die erste oder einzige wird zur Erstellung späterer Relationen abgespeichert. Befindet sich in einer Zeile zusätzlich zu einer ID noch ein Signalwort für eine Figurenrelation, bezieht sich die Zeile in der Regel auf die vorangegangene, wie beispielsweise in <castItem corresp=""#saladin"">Sultan Saladin.</castItem> <castItem corresp=""#sittah"">Sittah, seine Schwester.</castItem> Die zweite Zeile enthält neben dem Namen noch das Signalwort ""Schwester"", das auf die Beziehungsart siblings hinweist, eine ungerichtete Relation. Da keine zweite Figurenbezeichnung in der Zeile vorkommt, entnimmt das Programm als zweiten Part für die Geschwisterbeziehung den Namen bzw. die daraus abgeleitete ID saladin aus der vorherigen Zeile: <relation name=""siblings"" mutual=""#sittah #saladin"" /> Wenn die beiden benötigten IDs für das Erstellen der Figurenrelation feststehen, wird die Art der Relation durch das Auslesen des Signalworts aus der Baumstruktur festgestellt. Danach werden daraus die Zeilen mit den Figurenrelationen erstellt und diese anschließend in die jeweilige TEI-Version des Textes geschrieben. Befindet sich in einer Zeile eine zweite Figuren-ID, bezieht sich die Zeile nicht auf eine vorangegangene, sondern stellt selbst den zweite Bezugspunkt der Relation. Das ist beispielsweise bei der Figur ""Camillo Rota"" in <castItem corresp=""#camillo_rota"">Camillo Rota, einer von des Prinzen Räten.</castItem> Die erste erkannte ID ist camillo_rota, die zweite der_prinz, abgeleitet aus ""des Prinzen"". Die IDs werden in gerichtete Relationen mit aktivem und passivem Part überführt: <relation name=""associated_with"" active=""#camillo_rota"" passive=""#der_prinz"" /> Das Programm arbeitet dabei ausschließlich mit den IDs. Dafür ist es nicht nötig, dass Figurennamen explizit als Namen oder Adelstitel als Titel erkannt werden. Es geht ausschließlich darum aus den einzelnen Wörtern einer Zeile im Figurenverzeichnis Namen bzw. Namensteile und Titelangaben herauszufiltern, die den IDs entsprechen, um die Zeilen einer oder mehreren Figuren zuordnen zu können. Um auch IDs zu erkennen, die sich geringfügig von den Namensnennungen im Figurenverzeichnis unterscheiden, überprüft das Programm pro Wort eine Reihe an Varianten. So trennt es beispielsweise vom oben gennannten Wort ""Prinzen"" das Suffix ab und überprüft, ob ein Artikel Teil der ID ist. So kann ""des Prinzen"" der ID ""der_prinz"" zugeordnet werden. In manchen Fällen funktioniert diese Abwandlung aber nicht so reibungslos. In Um die Methode zu evaluieren, wurden die automatisch erzeugten Relationen manuell nachkorrigiert und so ein Goldstandart erzeugt. Im Schnitt bearbeiteten die Korrektoren 12 Texte pro Stunde. Beim Abgleich der automatisch erzeugten Ergebnisse mit dem Goldstandart lag der Macro-Average-Recall Wert bei 0,3 (Standardabweichung: 0,3) und der Wert von Macro-Average-Precision bei 0,55 (Standardabweichung: 0,4), was einen Macro-Average-F-Score von 0,49 (Standardabweichung: 0,25) ergibt. GerDraCor ist ein deutsches Dramenkorpus, das nach TEI-P5 Standarts kodiert ist und im Dezember 2019 474 Dramen enthält, die im Zeitraum von 1730 bis 1940 veröffentlicht wurden (Fischer u.¬†a. 2019). Es ist Teil des größeren DraCor (Fischer u.¬†a. 2019), das als Im Rahmen der manuellen Nachkorrektur wurden außerdem interessante Fälle identifiziert. So wird etwa eine Gruppe von Figuren in dem oben abgebildeten Figurenverzeichnis von Schillers Wir stellen im folgenden zwei Analysen vor, in denen von den automatisch extrahierten Relationen Gebrauch gemacht wird, sowohl eine Einzeltext- als auch eine Korpusanalyse. Diese illustrieren Möglichkeiten, die Relationen in der Textanalyse zu berücksichtigen. Im ersten Beispiel betrachten wir Shakespeares  Auch wenn Abbildung¬†3 eine gewisse Symmetrie suggeriert, ist diese keineswegs gegeben wenn wir die Redeanteile nach Familien aufschlüsseln, wie es aus den Annotationen ebenfalls direkt möglich ist. Abbildung¬†4 zeigt die aggregierten Redeanteile der Figuren, wobei Figuren, die durch Verwandtschaft oder Arbeitsverhältnis zu einer der Familien gehören, zusammengefasst wurden (mit Ausnahme von Mercutio und Paris, die beide mit dem Prinzen verwandt sind). Es zeigt sich, dass Angehörige der Familie Capulet etwas weniger als doppelt so viele Wörter äußern als Angehörige der Familie Montague. Betrachtet man das annotierte Gesamtkorpus stellt man fest, dass die Relationen ungleich verteilt sind. Während Ehen/Verlobungen, Elternschaft und sonstige Assoziationen relativ häufig vorkommen, spielen Geliebte, sonstige Verwandtschaften, Freundschaften und Geschwister eine vergleichsweise kleine Rolle. In Abbildung¬†6 sehen wir die Anzahl der Relationen bestimmter Typen ins Verhältnis gesetzt zur Großgattung (Komödie/Tragödie). Dabei wurden die Angaben auf den Titeln der Dramen übernommen und leicht vereinheitlicht (z.B. Bürgerliches Trauerspiel Eine Verteilung der genannten Relationen nach Autor zeichnet jedoch ein anderes Bild (Abbildung 7). Bestimmte Autoren, vor allem Ludwig Anzengruber (1839-1889) und Johann Nestroy (1801-1862), haben klare Tendenzen dazu, mehr Relationen im Figurenverzeichnis zu nennen. Beide verfassen tendenziell Possen und Komödien. Mit den von uns bereitgestellten maschinenlesbaren Informationen ermöglichen wir Analysen dramatischer Figuren, die die als bekannt vorausgesetzten Informationen im Figurenverzeichnis mit berücksichtigen können. Neben den oben skizzierten Analysen können die Informationen auch in inhaltliche Analysen einfließen und etwa die soziale Nähe mit der Bühnennähe korrelieren o.ä. Kontextfreie Grammatiken haben sich hier 'trotz der bekannten Schwächen im Bezug auf natürliche Sprache 'als effizienter Formalismus herausgestellt, um die Figurenverzeichnisse maschinenlesbar zu machen. Wir halten dieses Verfahren für geeignet, um auch in anderen Kontexten mit semi-strukturierten Textdaten zu arbeiten, wo aufgrund der begrenzten Menge ein maschinelles Lernverfahren nur bedingt zum Einsatz kommen kann.",de,Beitrag stellen Methode Information figurenrelationen dramatisch Text innerhalb Verfahren Implementierung Zukunft digitalisiert Dram anwendbar quelloffen Software Verfügung stellen vergleichsweise einfach Sprachstuf Genre anpassbar liefern Ergebnis Vorlage evaluation verfahren erfolgen ungesehenen Testdat veröffentlichen Datensatz extrahiert figurenrelationen Deutschsprachig dramen manuell validiern korrigieren daten einfach breit Nutzung Gerdracor Methode unterscheiden kategorien figurenrelation tabell ausschlaggebend Zuordnung Kategori signalwört Vater kammerdien geschwister signalwört kontextfrei Grammatik entsprechend Kategorie zuordnen kontextfrei Grammatike bezeichnen Informatik Sammlung syntaktisch korrekt Programm Programmiersprache Böckenhauer Hromkoviƒç formalisierte Art Grammatik Regel Programmiersprache enthalten erlauben automatisiert Syntaxanalyse programm Durchzuführ Böckenhauer Hromkoviƒç Regel Hilfe zwei alphabete beschreiben Terminalalphabet enthalten Wörter Sprache wohingegen Nichtterminalalphabet variablen enthalten vorgeben Art Weise Wörter kombinieren Böckenhauer Hromkoviƒç nutzen Grammatik verschieden Zeilenart Figurenverzeichnis unterscheiden nichtterminal handeln sätzen vorkommend Token terminal Kombination Anzahl aufschluss Art Zeile jeweils handeln Weise zeilenübergreifend Relation erkennen Beginn Programmablauf gerdracor vorhanden Figurenverzeichnis ausgelesen speichern Beziehung Figur ausschließlich anhand Angabe Figurenverzeichnis konstruieren Dramentext extra eingelesen ergeben Beschränkung jeglicher Beziehung Figureverzeichnis explizit Programm erkennen demnach ausschließlich Personenverzeichnis maschinenlesbar ignorieren Programm beispielsweise zeil Gruppe Figur kollektiv einführen Nummer anonym angehöriger untergruppen schlaffer meistens Name Explizit gemacht Beziehung anschließend Token Zeile figurenverzeichniss daraufhin untersuchen Figurennennung signalwört handeln Grammatik Parser übergeben zeile figurenverzeichniss baumstrukturen überführen erstellt baumstrukturer einzeln Information ausgelesen grundlegend Erkennung figurenrelation überprüfen ids Zeile befinden einzig Erstellung spät Relation abgespeicheren befinden Zeile zusätzlich id Signalwort Figurenrelation beziehen Zeile Regel vorangegangen beispielsweise castit castit zeile enthalten Name Signalwort Schwester Beziehungsart siblings hinweisen ungerichtet Relation Figurenbezeichnung Zeile vorkommen entnehmen Programm Part Geschwisterbeziehung Name abgeleiten id Saladin vorherig zeile Relation Saladin benötigten ids Erstellen Figurenrelation feststehen Art Relation auslesen Signalwort Baumstruktur feststellen Zeile figurenrelationen erstellen anschließend jeweilig Text schreiben befinden Zeile beziehen Zeile vorangegangen stellen Bezugspunkt Relation beispielsweise Figur camillo rota castit Rota prinz erkennen id ableiten Prinz ids gerichtet relationen aktiv passiv Part überführen Relation Programm arbeiten ausschließlich ids nötig Figurenname Explizit Name Adelstitel Titel erkennen ausschließlich einzeln wörtern Zeile Figurenverzeichnis Name Namensteil Titelangabe Herauszufilter ids entsprechen zeile mehrere Figur Zuordn ids erkennen geringfügig Namensnennung Figurenverzeichnis unterscheiden überprüfen Programm pro Wort Reihe Variant trennen beispielsweise gennannt Wort Prinz suffix überprüfen Artikel id Prinz id zuordnen Fall funktionieren Abwandlung reibungslos Methode evaluieren automatisch erzeugt relationen manuell nachkorrigieren Goldstandart erzeugen Schnitt bearbeiten korrektor Text pro Stunde abgleich automatisch erzeugt Ergebnis Goldstandart liegen wert Standardabweichung Wert Standardabweichung Standardabweichung ergeben Gerdracor deutsch dramenkorpus standarts kodieren Dezember Dram enthalten Zeitraum veröffentlichen Fischer groß Dracor Fischer Rahmen manuell Nachkorrektur interessant Fall identifizieren Gruppe Figur abgebildet Figurenverzeichnis Schiller stellen folgend Analyse automatisch extrahiert relation Gebrauch sowohl Korpusanalyse illustrieren möglichkeiten relationen Textanalyse berücksichtigen betrachten shakespearer gewiß Symmetrie suggerieren keineswegs geben Redeanteile Familie aufschlüsseln annotation ebenfalls direkt zeigen aggregiert Redeanteil Figur wobei figuren Verwandtschaft Arbeitsverhältnis Familie gehören zusammengefasst Ausnahme Mercutio Paris Prinz verwandt zeigen angehörige Familie capulet doppelt Wörter äußern Angehörige Familie Montague betrachten annotiert Gesamtkorpus stellen fest relationen ungleich verteilen Ehe verlobungen Elternschaft sonstig Assoziation relativ häufig vorkommen spielen gelieben sonstig Verwandtschaft freundschafn geschwister vergleichsweise Rolle sehen Anzahl Relation bestimmt Typ Verhältnis setzen Großgattung Komödie Tragödie Angabe Titel Dram übernehmen vereinheitlichen bürgerlich Trauerspiel Verteilung genannt Relation Autor zeichnen anderer Bild Abbildung bestimmt Autor Ludwig anzengruber Johann Nestroy klar Tendenz relationen Figurenverzeichnis nennen verfassen tendenziell Posse Komödie bereitgestellt maschinenlesbaren Information ermöglichen analysen dramatisch Figur vorausgesetzt Information Figurenverzeichnis berücksichtigen skizziert analysen Information inhaltlich Analyse einfließen sozial Nähe bühnennäh Korrelier kontextfrei Grammatike trotz bekannt Schwäche Bezug natürlich Sprache effizient Formalismus herausstellen Figurenverzeichnisse maschinenlesbar halten Verfahren geeignet Kontext textdan arbeiten aufgrund begrenzt Menge maschinelles lernverfahren bedingt Einsatz,"[('zeile', 0.41434982794027186), ('figurenverzeichnis', 0.3004248140251329), ('relation', 0.1937437781327895), ('ids', 0.1834528498469723), ('programm', 0.17635816756238193), ('prinz', 0.16575354704588371), ('id', 0.14798208140723998), ('familie', 0.13260283763670697), ('figurenrelation', 0.13260283763670697), ('relationen', 0.12832353306329314)]"
2020,DHd2020,168_final-GIUS_Evelyn_Computationelle_Textanalyse_als_fünfdimensionale.xml,Computationelle Textanalyse als fünfdimensionales Problem,"Evelyn Gius (Technische Universität Darmstadt, Deutschland)","Komplexität, Phänomene, Erkenntnisinteresse","Modellierung, Theoretisierung, Bewertung, Text","In diesem Beitrag wird ein Modell vorgestellt, das zu einer Einschätzung der Komplexität von Forschungsansätzen dient, die sich Texten mit computationellen Analysen nähern. Das Modell wurde vor dem Hintergrund der (literaturwissenschaftlichen) Analyse von literarischen Texten entwickelt, es ist jedoch 'ggf. mit leichten Anpassungen 'für Textanalysen generell geeignet. Die Komplexität von Digital Humanities-Projekten ist bestimmt von der Aushandlung von Vorannahmen, Methoden, der Passung zum Gegenstand, der konkreten interdisziplinären Zusammenarbeit, die fachlich, persönlich und oft auch karrierestrategisch eine große Herausforderung für die Beteiligten sein kann, bis hin zur Darstellung von Ergebnissen für eine oder mehrere Forschungscommunities. Neben Fragen der Projektplanung und -steuerung, wissenschaftspolitischen und wissenschaftskommunikativen Aspekten geht es auch um Fragen, die das eigentliche Forschungsgeschehen betreffen. Dieses wird aktuell in Bezug auf seine Relevanz und Ausrichtung diskutiert: Eine harsche Kritik von Nan Z. Da (2019a) an den Verfahren der DH initiierte eine mit dem etwas überzogenen Begriff ""Digital Humanities War"" bezeichnete Auseinandersetzung. Diese Auseinandersetzungen gehen zum Großteil an den eigentlichen Forschungszugängen vorbei. Dabei wäre es aus Sicht der Digital Humanities Ausgangspunkt des Modells sind die drei Aspekte, die für jede computergestützte Textanalyse wesentlich sind: Die Phänomene, denen das Interesse gilt, die Texte, die untersucht werden, und die Art, wie Erkenntnis erzeugt wird. Eine Einschätzung der Phänomene, die in einer computationellen Textanalyse untersucht werden, kann anhand der Phänomenbeschreibung stattfinden. Für diese kann man fragen: Wird das Phänomen als einfach, nicht weiter unterteilt, oder als aus mehreren Phänomenen zusammengesetzt betrachtet? Dabei geht es wohlgemerkt nicht um eine allgemein gültige Definition des entsprechenden Phänomens, sondern um die von den Forscher*innen genutzte Beschreibung. Beschreibungen für dasselbe Phänomen können in unterschiedlichen Forschungsprojekten entsprechend unterschiedlich ausfallen. In Bezug auf ein aktuelles Forschungsprojekt zu Gender und Krankheit in literarischen Prosatexten Neben der Bestimmung der Teile, aus denen eine Phänomenbeschreibung zusammengesetzt ist, geht es auch um die Frage, welches Wissen zur Bestimmung des Phänomens herangezogen werden muss. Dies kann zum einen Wissen sein, das der Text vermittelt. Aber es kann auch weiteres Wissen nötig werden, wie etwa spezielles Domänenwissen, zusätzliches (innerfiktionales oder außerfiktionales) Weltwissen u.ä. Die Kernfrage ist entsprechend: Braucht man über das Textwissen hinausgehendes weiteres Wissen, um ein Phänomen zu identifizieren? Auch hier gilt: Die Einstufung der Komplexität gilt für den betrachteten Anwendungsfall, andere Fälle haben ggf. für dieselben Phänomene andere Komplexitätsgrade. Im Projekt Gender und Krankheit wurde etwa mit Koreferenz-Auflösung experimentiert, die überwiegend auf Textphänomenen basiert. Das Krankheitskonzept wiederum wurde unter Rückgriff auf Wissen für zeitgenössische Krankheiten und Krankheitsbezeichnungen bearbeitet (etwa ""Phthise"" als Bezeichnung für Tuberkulose). Abbildung 1 stellt beispielhaft die beiden Dimensionen der Komplexität einiger Phänomene dar, die im Projekt Gender und Krankheit eine Rolle spielen. Oft wird vorschnell angenommen, dass für die textorientierten Digital Humanities die nun wesentlich größere Menge an untersuchten Texten distinktiv ist. Dabei ist die Frage, ob es sich um 'vermeintliche 'Big Data handelt oder nicht, aus Sicht der computationellen Textanalyse nur insofern interessant, als damit die Frage zusammenhängt, ob man die Texte, die man analysiert, kennt bzw. kennen kann oder nicht. In Bezug auf die Komplexität der genutzten Texte relevanter ist hingegen die umfassendere Frage: Wie viele (wie) verschiedene Texte werden analysiert? Dabei fällt unter Heterogenität von Texten die Anzahl der Texte selbst, aber auch die Anzahl von verschiedenen Texteigenschaften, die für die Fragestellung relevant sind bzw. sein könnten. Im Fall literarischer Texte sind das typischerweise Eigenschaften wie Gattung, Genre, Epoche, Autorgender, Erscheinungsort etc. Die Textheterogenität reicht von einem Text bis zu sehr vielen, sehr heterogenen Texten reicht In der Komplexitätsdimension des Analysemodus geht es darum, wer die Erkenntnisse produziert. Hier sind die beiden Möglichkeiten recht offensichtlich: Auf der einen Seite steht (menschliches) Lesen, auf der anderen Seite maschinelles Erschließen. Die Hauptfrage ist also: Wird die Textbasis durch Menschen oder durch Computer erschlossen? Dabei wird für alle Zugänge als gegeben vorausgesetzt, dass der Computer genutzt wird. Während das Lesen in Annotationen von Textstellen oder zumindest in die Ergänzung der Texte um Metainformationen resultiert, wird beim maschinellen Erschließen im Normalfall Textmining betrieben. Beide Textzugangsarten können weiter differenziert werden nach der Interpretationstheorie (etwa in text-, leser- oder autororientierte Zugänge) bzw. dem angewendeten maschinellen Verfahren (etwa in regelbasierte und Lernverfahren). In konkreten Forschungsprojekten kommen fast immer beide Modi vor. So werden im Projekt Gender und Krankheit manuelle Annotationen von Textpassagen und halb-automatische Verfahren zur Wortfeldgenerierung für die weitere Verarbeitung oder die Methodenentwicklung mit automatischen Verfahren zur Figurenerkennung, Segmentierung und Sentimentanalyse kombiniert. Da die Zwischenschritte in der Analyse zumeist manuell überprüft und teilweise ergänzt werden, handelt es sich hier um ein Verfahren zwischen Lesen und automatischem Erschließen und damit um eine eher geringe Komplexität. Schließlich geht es bei der Betrachtung von computationellen Textanalysen auch darum, wie der Computer eingesetzt wird, um Erkenntnisse zu generieren. Wenn man von der literaturwissenschaftlichen Praxis der Textanalyse ausgeht, ist die komplexeste Aufgabe jene, die Textbasis insgesamt im Hinblick auf die gewählte Fragestellung zu interpretieren. Interpretation ist jedoch bislang nicht der Fokus computationeller Zugänge zu literarischen Texten. Trotzdem lohnt es sich, Interpretation als ein Extrem der Dimension der Erkenntnis zu denken. In Anlehnung an die literaturwissenschaftliche Praxis kann man die Komplexitätsdimension des Erkenntnisbeitrags computationeller Analysen als von der Analyse des Textes für ein erstes Textverständnis bis hin zur Interpretation der Textbasis als Ganzes ausgedehnt sehen. Unabhängig von der Frage, welche Systematik man für die Tätigkeiten verwendet, die mit Textverstehen befasst sind, ist die zentrale Frage in der letzten Komplexitätsdimension: Wie weit geht der Erkenntnisbeitrag der computationellen Methode? Es geht also um die Frage nach der Neuheit des computationell Erforschten. Grob kann man die Komplexitätsstufen des Erkenntnisbeitrags wie folgt erfassen: Werden in einer deduktiven bzw. einfachen Textanalyse aufgrund von bestehenden Hypothesen bzw. Regeln (also bestehenden Analysekategorien und -verfahren) durchgeführt, werden aus der Betrachtung von Texten neue Analysekategorien oder auch Taxonomien entwickelt oder handelt es sich um Hypothesen über größere Zusammenhänge in den Texten, also um ihre Interpretation? Bei der Auseinandersetzung mit der Komplexitätsdimension des Erkenntnisbeitrags ist zu beachten, dass in einer typischen literaturwissenschaftlichen Textanalyse meist alle Modi vorliegen und fließend ineinander übergehen. Für die Komplexitätseinschätzung ist relevant, welche Modi davon computationell unterstützt werden sollen. Im Fall des Projekts zu Gender und Krankheit soll etwa deduktiv die Veränderung der Figurenkonstellation anhand der Figurennennungen analysiert werden. Ein induktives Verfahren liegt vor, wenn Genderkategorien durch Clustering von Figurenrede herausgearbeitet werden (die dann wieder deduktiv in der Analyse genutzt werden). Und schließlich liegt ein abduktiver Zugang vor, wenn durch eine Gesamtbetrachtung ein neues Element entdeckt würde, das Figurenkrankheit beeinflusst. Wie bereits dargelegt, betrifft die Bestimmung der Komplexität in den fünf Dimensionen primär die normativen Setzungen durch die Forscher*innen. Ausschlaggebend ist weniger, wie Texte, Phänomene und Erkenntnis an sich modelliert werden Für die Betrachtung und Kritik eines Zugangs sollten alle fünf Dimensionen berücksichtigt werden. Damit vermeidet man auch vorschnelle Kritik, die sich auf eine einfache Modellierung einer Dimension beschränkt und den Zugang insgesamt als unterkomplex betrachtet, obwohl er in einer oder mehreren anderen Dimensionen Erhebliches leistet. Darüber hinaus eignet sich das Modell als Instrument für den Entwurf eines Zugangs. Es kann in allen Phasen computationeller Textanalyse genutzt werden 'vom Design des Forschungszugangs zu Beginn der Forschungsarbeit über die wiederholten Bestandsaufnahme oder Nachjustierung im Projektverlauf bis hin zur Einordnung der erzielten Ergebnisse am Ende und der Reflektion des gesamten Prozesses. Abschließend seien noch einmal die fünf Dimensionen mit ihren Kernfragen dargestellt:",de,Beitrag Modell vorstellen Einschätzung Komplexität Forschungsansätz dienen Text computationell Analyse nähern Modell Hintergrund literaturwissenschaftlich Analyse literarisch Text entwickeln leicht Anpassung textanalyse generell geeignet Komplexität Digital bestimmt Aushandlung vorannahmen Methode Passung Gegenstand konkret interdisziplinären Zusammenarbeit fachlich persönlich karrierestrategisch Herausforderung beteiligter Darstellung Ergebnis mehrere forschungscommunities Frage Projektplanung wissenschaftspolitisch wissenschaftskommunikativ Aspekt Frage eigentlich Forschungsgeschehen betreffen aktuell Bezug Relevanz Ausrichtung diskutieren harsch Kritik nan Verfahren dh initiiert überzogen Begriff Digital Humanitie bezeichnet Auseinandersetzung Auseinandersetzung Großteil eigentlich Forschungszugäng vorbei Sicht Digital Humanitie Ausgangspunkt Modell Aspekt computergestützt Textanalyse wesentlich phänomen Interesse gelten Text untersuchen Art Erkenntnis erzeugen Einschätzung Phänomen computationell Textanalyse untersuchen anhand Phänomenbeschreibung stattfinden fragen Phänomen einfach unterteilen mehrere Phänomen zusammengesetzt betrachten wohlgemerkt allgemein gültig Definition entsprechend Phänomen genutzt Beschreibung Beschreibunge Phänome unterschiedlich Forschungsprojekt entsprechend unterschiedlich ausfallen Bezug aktuell Forschungsprojekt gend Krankheit literarisch Prosatext Bestimmung Teil Phänomenbeschreibung zusammensetzen Frage wissen Bestimmung Phänomen heranziehen wissen Text vermitteln wissen nötig Spezielles domänenwissen zusätzlich innerfiktional außerfiktionale weltwissen Kernfrage entsprechend brauchen Textwissen hinausgehend wissen Phänomen identifizieren gelten Einstufung Komplexität gelten betrachtet Anwendungsfall Fall phänomen Komplexitätsgrade Projekt gend Krankheit experimentieren überwiegend Textphänomenen basieren Krankheitskonzept wiederum Rückgriff Wissen zeitgenössisch Krankheit Krankheitsbezeichnung bearbeiten phthise Bezeichnung tuberkulos Abbildung stellen beispielhaft Dimension Komplexität Phänomen dar Projekt gend Krankheit Rolle spielen vorschnell annehmen textorientiert Digital Humanitie wesentlich groß Menge untersucht text distinktiv Frage vermeintlich big data handeln Sicht computationell Textanalyse insofern interessant Frage zusammenhängen Text analysieren kennen kennen Bezug Komplexität genutzt Text Relevanter hingegen umfassender Frage verschieden Text analysieren fallen Heterogenität Text Anzahl Text Anzahl verschieden Texteigenschaft Fragestellung relevant können Fall literarisch Text typischerweise eigenschaften Gattung Genre Epoche autorgend Erscheinungsort Textheterogenität reichen Text heterogen Text reichen Komplexitätsdimension Analysemodus erkenntnis produzieren Möglichkeit offensichtlich Seite stehen menschlich lesen Seite Maschinelles erschließen Hauptfrage Textbasis Mensch Computer erschließen zugäng gegeben voraussetzen Computer nutzen Lesen Annotation Textstell zumindest Ergänzung Text Metainformation resultieren maschinell Erschließen Normalfall Textmining betreiben textzugangsarten differenzieren Interpretationstheorie autororientiert zugäng angewendeten maschinell Verfahren Regelbasierte lernverfahren konkret Forschungsprojekt fast modi Projekt gend Krankheit Manuell Annotation Textpassag Verfahren Wortfeldgenerierung Verarbeitung Methodenentwicklung automatisch Verfahren Figurenerkennung Segmentierung Sentimentanalyse kombinieren Zwischenschritte Analyse zumeist manuell überprüfen teilweise ergänzen handeln Verfahren Lese automatisch erschließen eher gering Komplexität schließlich Betrachtung computationell textanalyse Computer einsetzen erkenntnis generieren literaturwissenschaftlich Praxis Textanalyse ausgehen komplexe Aufgabe Textbasis insgesamt Hinblick gewählt Fragestellung interpretieren Interpretation bislang Fokus Computationeller Zugänge literarisch Text lohnen Interpretation extrem Dimension Erkenntnis denken Anlehnung literaturwissenschaftlich Praxis Komplexitätsdimension erkenntnisbeitrags Computationeller analysen Analyse Text Textverständnis Interpretation Textbasis ausdehnen sehen unabhängig Frage systematik Tätigkeit verwenden Textversteh befassen zentral Frage letzter Komplexitätsdimension Erkenntnisbeitrag computationell Methode Frage Neuheit Computationell erforschten grob Komplexitätsstufe erkenntnisbeitrags folgen erfassen deduktiv einfach Textanalyse aufgrund bestehend Hypothese Regel bestehend Analysekategori durchführen Betrachtung Text Analysekategorie Taxonomien entwickeln handeln Hypothesen groß zusammenhänge Text Interpretation Auseinandersetzung Komplexitätsdimension erkenntnisbeitrag beachten typisch literaturwissenschaftlich Textanalyse meist modi vorliegen fließend ineinander übergehen Komplexitätseinschätzung relevant modi computationell unterstützen Fall Projekt Gender Krankheit deduktiv Veränderung Figurenkonstellation anhand Figurennennung analysieren induktiv Verfahren liegen Genderkategorien Clustering Figurenrede herausarbeiten deduktiv Analyse nutzen schließlich liegen abduktiv Zugang Gesamtbetrachtung neu Element entdecken Figurenkrankheit beeinflussen darlegen betreffen Bestimmung Komplexität dimension primär normativ Setzung ausschlaggebend text phänomen Erkenntnis modellieren Betrachtung Kritik Zugang Dimension berücksichtigen vermeiden vorschnell Kritik einfach Modellierung Dimension beschränken Zugang insgesamt unterkomplex betrachten obwohl mehrere dimension erheblich leisten hinaus eignen Modell Instrument Entwurf Zugang phas Computationeller Textanalyse nutzen Design Forschungszugang Beginn Forschungsarbeit wiederholt Bestandsaufnahme Nachjustierung Projektverlauf Einordnung erzielt Ergebnis Reflektion gesamt prozeß abschließend dimensionen Kernfrage darstellen,"[('krankheit', 0.2459981518443215), ('computationell', 0.22658997092119737), ('phänomen', 0.20411201543417148), ('textanalyse', 0.1933247856852166), ('komplexität', 0.19081701315501004), ('komplexitätsdimension', 0.18577769610238132), ('text', 0.17623823683333445), ('gend', 0.16399876789621431), ('dimension', 0.13866934326484753), ('frage', 0.1315556728942508)]"
2020,DHd2020,141_final-FISCHER_Frank_Besuch_im__Marstheater____Eine_Netzwerkmodelli.xml,"Besuch im ""Marstheater"" 'Eine Netzwerkmodellierung von Karl Kraus' Riesendrama ""Die letzten Tage der Menschheit""","Frank Fischer (Higher School of Economics, Moskau, Russland); Anna Busch (Universität Potsdam); Angelika Hechtl (WU Wien); Peer Trilcke (Universität Potsdam); Andreas Vogel (Hamburg)","Karl Kraus, Drama, TEI, Netzwerkanalyse","Beziehungsanalyse, Modellierung, Veröffentlichung, Visualisierung","Karl Kraus' Endzeitdrama ""Die letzten Tage der Menschheit"", 1919 zum ersten Mal vollständig erschienen (Buchausgabe 1922), ist in vielerlei Hinsicht inkommensurabel. Der schiere Umfang sprengt alle Gattungsnormen (638 Seiten in der ""Volk und Welt""-Ausgabe von 1978). Die fünf Akte plus Vorspiel und Epilog sind in 220 Szenen unterteilt, es gibt je nach Zählweise um die 1.000 sprechende Figuren bzw. Instanzen (zum Vergleich: als nächstgrößtes deutschsprachiges Drama gilt Grabbes ""Napoleon oder Die hundert Tage"" von 1831 mit 259 Figuren). Die Zählweise ist nicht nur deshalb kontingent, weil es zahlreiche Rufe aus der Menge gibt, die sich nicht quantifizieren lassen (wozu vor allem auch das undurchsichtige Stimmengewirr im Epilog gehört), sondern auch, weil es konkrete Gruppierungen wie die ""Fünfzig Drückeberger"" (III/26) oder ""Die zwölfhundert Pferde"" (V/55) gibt, die man theoretisch quantifizieren könnte, auch wenn dies nicht unmittelbar sinnvoll erscheint. Insgesamt spricht man tatsächlich besser von Sprecherinstanzen, die von historischen Personen über namenlose Zwischenrufer und allegorische Figuren (etwa den ""Hyänen, die Menschengesichter tragen"") bis hin zur ""Stimme Gottes"" reichen. Es ist nicht nur auf das Thema des Stücks bezogen 'die Apokalypse des Ersten Weltkriegs –, sondern auch auf die Form, wenn Kraus im Vorwort schreibt: ""Die Aufführung des Dramas, dessen Umfang nach irdischem Zeitmaß etwa zehn Abende umfassen würde, ist einem Marstheater zugedacht. Theatergänger dieser Welt vermöchten ihm nicht standzuhalten."" (Kraus 1978, S.¬†5) Die Handlung der Tragödie ist ""unmöglich, zerklüftet, heldenlos"" (ebd.) und erschwert jede Absicht, das Stück darzustellen, zumal vollständig. Dies betrifft sowohl Inszenierungen auf der Bühne oder als Hörspiel (obwohl es schon Kompletteinspielungen gibt) als auch digitale Modellierungen der Figurenbeziehungen. Es ist Konsens innerhalb des Forschungszweigs der Netzwerkanalyse dramatischer Texte, dass sich eine Einzelanalyse der verhältnismäßig übersichtlichen Figurennetzwerke selten lohnt. Das Augenmerk liegt daher normalerweise auf der Untersuchung struktureller Entwicklungen hunderter oder tausender Stücke über verschiedene historische Zeiträume (Algee-Hewitt 2017, Trilcke/Fischer 2018). ""Die letzten Tage der Menschheit"" gehören hier zu den Ausnahmen. Ziel dieses Projekts ist es, das Stück als soziales Netzwerk zu visualisieren, basierend auf Kookkurrenzen von Sprecherinstanzen in den einzelnen Szenen. Voraussetzung dafür ist eine brauchbare Formalisierung des Gesamttextes. Dieser ist einerseits bereits digitalisiert, in annehmbarer Qualität innerhalb des Projekts Gutenberg-DE (obwohl es in dieser Version kaum eine Seite ohne zumindest kleinere OCR-Fehler gibt). Andererseits gibt es noch keine digitale Fassung in einem Format, das die wissenschaftliche Auswertung ermöglicht. Am Beginn dieses Projekts stand daher die Herstellung einer TEI-Version des Dramas, die vor Konferenzbeginn veröffentlicht wurde und damit der wissenschaftlichen Community zum ersten Mal eine Version des Textes zur Verfügung stellt, die auf die FAIR-Prinzipien setzt (findable, accessible, interoperable, reusable). Neben einem Qualitätssprung hinsichtlich der Textbasis im Vergleich zur Gutenberg-DE-Version stand dabei die Auszeichnung der Sprecher-IDs im Mittelpunkt. Da, wie bereits angedeutet, diese Auszeichnung kontingent ist, also je nach Formalisierungsentscheidung anders aussehen kann, wird dieser Prozess offengelegt. So werden etwa die Vielzahl an Stimmen aus Menschenmengen oder die Unzahl ausrufender Zeitungsverkäufer nachvollziehbar individualisiert, speziell die Massenszenen in Wien, etwa die Geschehnisse an der Sirk-Ecke, die das Vorspiel und jeden der fünf Akte eröffnen. Ergebnis ist ein visualisiertes Netzwerk, das auf einem Poster im A0-Format einen Blick ins Kraus'sche ""Marstheater"" erlaubt, auf die schiere Masse der Auftritte und Stimmen, aus der doch eine Struktur hervorscheint, wie sie bisher im Kontext der Kraus-Forschung noch nicht visualisiert worden ist. So werden viele ""innere Symmetrien"" sichtbar (Matala de Mazza 2018), die das Stück strukturieren, wiederkehrende Konstellationen wie etwa die vier Offiziere am Beginn jedes Aktes oder die Szenen in der Schulklasse (I/9 und V/23). Deutlich wird im Netzwerkgraph auch die Diskrepanz zwischen Front und Heimat, zwei Welten für sich, wobei Kraus den Fokus auf die entlarvende Sprache von nicht direkt am Krieg beteiligten Personen legt: ""Wenn nicht Krieg wär, möcht man rein glauben, es is Friede."" (Kraus 1978, S.¬†95) Da der Text nunmehr als Volltext-TEI-Dokument vorliegt, lässt sich auch der Word Space in das Netzwerk hineinmodellieren, d.¬†h., die Anzahl der Wörter pro Sprecherinstanz. Auf diese Weise scheinen deutlich die (quantitativ gesehen) Hauptfiguren dieses ""heldenlosen"" Dramas auf (etwa der ""Nörgler"" und der ""Optimist"" sowie der ""Patriot"" und der ""Abonnent""), die oft über dutzende Seiten als Zweierkonstellationen auftreten, die aber darüber hinaus, wie der Graph verdeutlicht, auch anderweitig vernetzt sind. Um auch komparatistische Aspekte abzudecken, werden auf dem Poster vergleichend einige Netzwerkmetriken präsentiert, um die Gigantomanie des Dramas mit Zahlen zu verdeutlichen. Zur Gewährleistung der Nachnutzbarkeit und Nachhaltigkeit der Modellierung wurde das Stück auch dem German Drama Corpus hinzugefügt (",de,Karl Kraus Endzeitdrama letzter Menschheit Mal vollständig erscheinen Buchausgabe vielerlei Hinsicht inkommensurabel schier Umfang sprengen Gattungsnorm Seite Volk akte plus vorspiel Epilog Szene unterteilt zählweise sprechend Figur instanzen Vergleich nächstgrößt deutschsprachig Drama gelten grabbes Napoleon hundert Figur zählweise kontingent zahlreich ruf Menge quantifizieren lassen wozu undurchsichtig Stimmengewirr Epilog gehören konkret Gruppierung fünfzig drückeberg iii zwölfhunderen Pferd v theoretisch quantifizieren unmittelbar sinnvoll erscheinen insgesamt sprechen tatsächlich Sprecherinstanze historisch Person namenlos Zwischenrufer allegorisch Figur Hyän Menschengesichter tragen Stimme Gott reichen Thema Stück beziehen Apokalypse Weltkrieg Form Kraus Vorwort schreiben Aufführung Drama Umfang irdisch Zeitmaß Abend umfassen Marstheater zugedenken Theatergänger Welt vermöchen standzuhalen Kraus Handlung Tragödie unmöglich zerklüften heldenlos erschweren Absicht Stück darstellen zumal vollständig betreffen sowohl Inszenierung Bühne hörspiel obwohl kompletteinspielungen digital Modellierunge Figurenbeziehunge Konsens innerhalb Forschungszweig Netzwerkanalyse dramatisch Text Einzelanalyse verhältnismäßig übersichtlich Figurennetzwerk selten lohnen Augenmerk liegen normalerweise Untersuchung strukturell Entwicklung Hunderter Tausender Stück verschieden historisch zeiträume trilcken Fischer letzter Menschheit gehören Ausnahme Ziel Projekt Stück sozial netzwerk visualisieren basierend Kookkurrenzen Sprecherinstanze einzeln Szene Voraussetzung brauchbar Formalisierung Gesamttext einerseits digitalisieren annehmbar Qualität innerhalb Projekt obwohl Version Seite zumindest klein andererseits digital Fassung Format wissenschaftlich Auswertung ermöglichen Beginn Projekt stehen Herstellung Drama Konferenzbeginn veröffentlichen wissenschaftlich Community Mal Version Text Verfügung stellen setzen findabel accessibel interoperabel Reusable Qualitätssprung hinsichtlich Textbasis Vergleich stehen Auszeichnung Mittelpunkt andeuten Auszeichnung kontingent Formalisierungsentscheidung aussehen Prozess offengelegen Vielzahl Stimme Menschenmengen Unzahl ausrufend Zeitungsverkäufer nachvollziehbar individualisieren speziell Massenszenen Wien Geschehniss vorspiel Akt eröffnen Ergebnis visualisiert Netzwerk Poster Blick Marstheater erlauben schier Masse Auftritt Stimme Struktur hervorscheinen Kontext visualisiern innerer Symmetrie sichtbar matala de mazza Stück strukturieren Wiederkehrend konstellationen Offizier Beginn jeder akter Szene Schulklasse i v deutlich Netzwerkgraph Diskrepanz Front Heimat Welt wobei Kraus Fokus entlarvend Sprache direkt Krieg beteiligt Person legen Krieg sin möcht rein glauben -- frieden Kraus Text nunmehr vorliegen lässn Word Space Netzwerk hineinmodellieren Anzahl Wörter pro Sprecherinstanz Weise scheinen deutlich quantitativ sehen hauptfiguren heldenlosen Dramas nörgl optimisen Patriot Abonnent dutzend Seite zweierkonstellationen auftreten hinaus Graph verdeutlichen anderweitig vernetzt komparatistisch Aspekt abdecken Poster vergleichend Netzwerkmetrik präsentieren Gigantomanie Drama Zahl verdeutlichen Gewährleistung Nachnutzbarkeit Nachhaltigkeit Modellierung Stück German Drama Corpus hinzufügen,"[('kraus', 0.2748967175243331), ('stück', 0.25070067959606585), ('drama', 0.17241113022610668), ('stimme', 0.13598296478220714), ('zählweise', 0.1245611281179792), ('marstheater', 0.1245611281179792), ('vorspiel', 0.1245611281179792), ('sprecherinstanze', 0.11601924765066624), ('schier', 0.11601924765066624), ('kontingent', 0.11601924765066624)]"
2020,DHd2020,280_final-HERRMANN_J__Berenike____hungere_schon_nach_dem_nächsten_Band.xml,... hungere schon nach dem nächsten Band. Eine Untersuchung von Metaphern für Leseerfahrungen in Web 2.0 Literaturrezensionen,"Berenike Herrmann (Universität Basel, Schweiz); Thomas Messerli (Universität Basel, Schweiz)","Metaphern, Social Reading, Web 2.0, Leser, Literaturkritik, Metaphernidentifikation","Modellierung, Annotieren, Stilistische Analyse, Sprache, Methoden, Text","Kaum ein geisteswissenschaftlicher Forschungsgegenstand hat eine so intensive Diskussion erfahren wie die Metapher (Eggs, 2000). Doch existieren nur wenige Ansätze zu ihrer Formalisierung innerhalb der Digital Humanities. Unser Beitrag stellt einen einfachen Ansatz der Metaphernanalyse auf grösseren Datenmengen vor, um auch in nicht-annotierten Texten metaphorische Mappings zu finden. Mit diesem Ansatz analysieren wir konzeptuelle Strukturen des Leseerlebens von Laien-RenzensentInnen. Mittels einer Verschränkung korpusbasierter und korpusgetriebener Methoden (Tognini-Bonelli, 2001) untersuchen wir ein Korpus von Laienrezensionen (ca. 1,3 Mio Beiträge) explorativ auf den Metapherngebrauch mit der Zieldomäne ""Leseerleben"". Metaphern werden mit der Kognitiven Theorie der Metapher (KTM) als Denk- bzw. Erfahrungsfiguren (Lakoff & Johnson, 1980, S. 4) gefasst. Ausgehend von Befunden zu Laienbuchrezensionen im Englischen (Stockwell, 2009; Nuttall & Harrison, 2018) und zu feuilletonistischen Rezensionen (Köhler, 1999) operieren wir auf der Sprachoberfläche und inferieren von dort konzeptuelle Mappings zwischen Ziel- und Quelldomänen (Herrmann, im Druck; Shutova, 2017; Steen et al., 2010), wobei besonderes Augenmerk auf das Mapping LESEN IST NAHRUNGSAUFNAHME gelegt wird. Ausgangspunkt ist der Befund Nuttall und Harrisons (2018), dass Nahrungsmetaphern in englischsprachigen Das LoBo-Korpus (extrahiert von der Social Reading-Plattform ""Lovelybooks"") beinhaltet ca. 1,3 Mio. deutschsprachige Laienrezensionen von 54.000 NutzerInnen, die sich auf jeweils ein Buch beziehen. Die Bücher sind kategorisiert nach 15 Genres, die der Plattform selbst entnommen sind. Das Korpus ist PoS-annotiert (Tree-Tagger), lemmatisiert und in CWB ( Angesichts der Herausforderungen einer reliablen automatischen Metapherndetektion (Veale, Shutova, & Klebanov, 2016) wählen wir bewusst eine korpusstilistische Herangehensweise (Deignan & Semino, 2010). Wir verschränken als induktiven Schritt A Kookurrenzanalyse und manuelle Identifikation mit einem deduktiven Schritt B (regelbasierte Suche nach spezifischen Quelldomänen-Indikatoren). Ziel ist eine möglichst hohe Vollständigkeit und Genauigkeit der Identifikation potenzieller Metapherntypen, wobei eine formale Evaluation der Methode im gegenwärtigen Stadium mangels Goldstandard jedoch nicht möglich ist. Um in Schritt A die Metaphern zu finden, die sich auf Leseerleben beziehen, müssen zunächst Objekte des Leseerlebens (OdL) identifiziert werden. OdL sind Referenten des Leseerlebens ( Ergänzend zur Korpusanalyse annotieren wir eine Stichprobe auf metaphorischen Sprachgebrauch (Herrmann, Woll, & Dorst, 2019). Unsere erste Fallstudie untersuchte insgesamt 18 randomisiert ausgewählte Rezensionen zu sechs Büchern (je drei pro Buch). Dieses Subkorpus enthält zu gleichen Teilen Rezensionen von ""anspruchsvollen Bestsellern"" und Fantasy-Romanen. Ziel war es, die Sequenz metaphorischer Ausdrücke sowie die lexikalische und konzeptuelle Variation abzuschätzen. In Schritt B untersuchten wir ausgehend von der Annahme eines systematischen Mappings LESEN IST NAHRUNGSAUFNAHME (ausgehend von entsprechenden Befunden durch Nuttall & Harrison, 2018, Köhler, 1999) die je hundert häufigsten Lemmata der drei ""Inhaltswortklassen"" Substantiv, Adjektiv und Verb auf mögliche Indikatoren. Innerhalb eines Fensters von zehn Wörtern um die in Schritt A festgelegten OdL (Ausdrücke, die Zieldomäne LESEN indizieren, z.B. Lemma (semantisches Feld: Essen & Trinken) Die Ergebnisse der Schritte 1 und 2 zeigen eine grosse Vielfalt metaphorischer Ausdrücke auf, die sich auf verschiedene Objekte des Leseerlebens beziehen. Aufschlussreich ist dabei nicht die absolute Häufigkeit der Metaphernkandidaten im Korpus 'zumal keine zuverlässigen Vergleichsdaten zur Verfügung stehen –, wohl aber die quantitative Analyse der relativen Verteilung auf Rezensionen verschiedener Ratings und Genres. Das Auftreten der hier untersuchten stark wirkungsbezogenen Metaphorik gibt etwa Aufschlüsse über Rezensionsmuster, die sich je nach quantitativer Bewertung und je nach literarischer Gattung unterscheiden. Die qualitative Untersuchung ermöglicht dagegen eine erste Typologie von Mappings, die wir im Folgenden mit Beispielen für konventionelle und kreative Metaphern illustrieren. Viele Ausdrücke sind erwartete, stark konventionalisierte Metaphern, wie etwa Es finden sich darüber hinaus aber auch viele Beispiele, die einen kreativen Umgang mit Metaphern illustrieren: Unsere Resultate zeigen bislang fünf verschiedene Typen von LESEN IST NAHRUNGSAUFNAHME auf. Lesen wird etwa (A) als eine Form von Nahrungsaufnahme konzeptualisiert, bei der Lesende als ""Essende"" und literarische Werke und deren Bestandteile als ""verzehrbar"" positioniert werden. Weiter wird (B) Schreiben als ""Kochen"" und ""Bewirten"" dargestellt, wobei Autoren als Köche, Lesende als Gäste und Lektüre als bekocht/bewirtet erscheinen. Darüber hinaus findet sich aber auch (C) ein anderes Mapping, das zwar auf die gleiche Quelldomäne zurückgreift, aber dem Objekt des Leseerlebens selbst als Agnes konstruiert. Andere Mappings (D) beziehen sich dagegen direkt auf die Objekte des Leseerlebens, werden sprachlich aber als Vergleich realisiert, der nach Steen et al. (2010) stärker intentional markiert ist. Schliesslich findet sich eine Reihe (E) von Mappings, die zwar auf die Quelldomäne NAHRUNGSAUFNAHME und Zieldomäne LESEN rekurrieren, sich dabei aber nicht auf Vorgänge der Nahrungsaufnahme, sondern auf das Embodiment von Emotionen zu beziehen scheinen. Unsere Studie leistet einerseits einen methodischen Beitrag zur Metaphernidentifikation mit einfachen korpusstilistischen Mitteln und gibt andererseits Aufschluss über die Produktivität der Quelldomäne NAHRUNGSAUFNAHME für die Konzeptualisierung von Leseerleben. Schliesslich zeigt unsere manuelle Annotation weitere Mappings auf, die den Umgang mit Objekten des Leseerlebens nicht als Nahrungsaufnahme konzeptualisieren, sondern als ""Reisen"" und ""Bewegung"", oder auch auch als ""Interaktion mit externen Kräften"". Bücher und Geschichten werden einerseits als ""Behälter"", andererseits wie ""Personen"" mit Qualitäten und Intentionen konzeptualisiert, ja als ""Freunde"" der Lesenden, wobei ""gegenseitige Kompatibilität"" als axiologischer Wert - situiert zwischen den inhaltlichen und (hedonistisch sowie praktisch) wirkungsbezogenen Werten nach Heydebrand und Winko (1996) - erscheint. Folgestudien sollen die Verbesserung der automatisierten Detektion leisten, unter anderem durch die Einbindung von semantischen Informationen aus GermaNet. So sollen durch systematische Untersuchung der häufigen konzeptuellen Metaphern und ihre Korrelation mit der Lovelybooks-Sterne-Wertung weitere Rückschlüsse auf zugrundeliegende Wertmassstäbe bei der Bewertung von online-Laienrezensionen ermöglicht werden.",de,geisteswissenschaftlich Forschungsgegenstand intensiv Diskussion erfahren Metapher Eggs existieren Ansatz Formalisierung innerhalb Digital Humanitie Beitrag stellen einfach Ansatz Metaphernanalyse grösseren datenmenger text metaphorisch Mapping finden Ansatz analysieren konzeptuell Struktur Leseerleben mittels Verschränkung korpusbasiert Korpusgetriebener Method untersuchen Korpus Laienrezensionen mio beiträge explorativ Metapherngebrauch Zieldomäne leseerleben Metapher kognitiv Theorie Metapher ktm Erfahrungsfigure lakoff Johnson fassen ausgehend befinden laienbuchrezensionen englisch stockwell Nuttall Harrison feuilletonistisch rezension Köhler operieren sprachoberfläch inferieren konzeptuell Mapping quelldomänen Herrmann Druck shutova steen et wobei besonderer Augenmerk Mapping lesen Nahrungsaufnahme legen ausgangspunkt Befund Nuttall Harrison Nahrungsmetapher englischsprachigen extrahiern social lovelybooks beinhalten Mio deutschsprachig Laienrezensionen nutzerinnen jeweils Buch beziehen büch kategorisiert Genres Plattform entnehmen korpus lemmatisieren cwb angesichts Herausforderung reliabl automatisch Metapherndetektion veal shutova Klebanov wählen bewussen korpusstilistisch Herangehensweise Deignan Semino verschränken induktiv Schritt Kookurrenzanalyse Manuelle Identifikation deduktiv Schritt b regelbasiert Suche spezifisch Ziel möglichst hoch Vollständigkeit Genauigkeit Identifikation potenziell Metapherntype wobei formal Evaluation Methode gegenwärtig Stadium mangels Goldstandard Schritt Metapher finden Leseerleben beziehen Objekt Leseerleben odl identifizieren odl Referent Leseerleben ergänzend Korpusanalyse annotieren Stichprobe metaphorisch Sprachgebrauch Herrmann Woll dorst Fallstudie untersuchen insgesamt Randomisiert ausgewählt Rezension büchern pro Buch subkorpus enthalten gleich Teil Rezension anspruchsvoll bestsellern Ziel Sequenz metaphorisch Ausdrück lexikalisch konzeptuell Variation abschätzen Schritt b untersuchen ausgehend Annahme systematisch Mapping lesen Nahrungsaufnahme ausgehend entsprechend Befund Nuttall Harrison Köhler hundert häufigsten Lemmata Inhaltswortklasse substantiv adjektiv verb möglich indikatoren innerhalb Fenster wörtern Schritt festgelegt Odl ausdrücke zieldomäne lesen indizieren Lemma semantisch Feld essen trinken Ergebnis Schritt zeigen Vielfalt metaphorisch Ausdrück verschieden Objekt Leseerleben beziehen aufschlussreich absolut Häufigkeit metaphernkandidater Korpus zumal Zuverlässige Vergleichsdat Verfügung stehen quantitativ Analyse relativ Verteilung rezension verschieden Ratings genre auftreten untersucht stark wirkungsbezogen Metaphorik aufschlüsse Rezensionsmuster quantitativ Bewertung literarisch Gattung unterscheiden qualitativ Untersuchung ermöglichen Typologie Mapping folgend Beispiel konventionell kreativ metaphern illustrieren Ausdrück erwartet stark konventionalisiert Metapher finden hinaus Beispiel kreativ Umgang Metapher illustrieren Resultat zeigen bislang verschieden Typ lesen Nahrungsaufnahme lesen Form Nahrungsaufnahme konzeptualisieren lesend essend literarisch Werk Bestandteil verzehrbar positionieren b schreiben kochen bewirt darstellen wobei Autor köch lesend gäste lektüre Bekocht bewirten erscheinen hinaus finden c anderer Mapping gleich quelldomäne zurückgreift Objekt leseerlebens agnes konstruieren Mapping d beziehen direkt Objekt Leseerleben sprachlich Vergleich realisieren Steen et stark intentional markieren schliesslich finden Reihe e Mapping quelldomäne Nahrungsaufnahme zieldomäne lesen rekurrieren vorgänge Nahrungsaufnahme Embodiment Emotion beziehen scheinen Studie leisten einerseits methodisch Beitrag Metaphernidentifikation einfach korpusstilistisch Mittel andererseits aufschluss Produktivität quelldomäne Nahrungsaufnahme Konzeptualisierung Leseerlebe Schliesslich zeigen Manuelle Annotation mappings Umgang objekten Leseerleben Nahrungsaufnahme konzeptualisieren Reise Bewegung Interaktion extern Kräft büch schichten einerseits Behälter andererseits Person qualität Intention konzeptualisieren Freund lesend wobei gegenseitig Kompatibilität axiologisch Wert situieren inhaltlich hedonistisch praktisch wirkungsbezogen werten Heydebrand Winko erscheinen folgestudien Verbesserung automatisiert Detektion leisten Einbindung semantisch Information rmanen systematisch Untersuchung häufig konzeptuell Metapher Korrelation Rückschlüsse Zugrundeliegend wertmassstäbe Bewertung ermöglichen,"[('leseerleben', 0.3477004567295928), ('nahrungsaufnahme', 0.3477004567295928), ('mapping', 0.2661778257223477), ('metapher', 0.22684502673577858), ('lesen', 0.15922072318015595), ('rezension', 0.15346957061298513), ('metaphorisch', 0.1370151462622888), ('nuttall', 0.1303876712735973), ('quelldomäne', 0.1303876712735973), ('odl', 0.1303876712735973)]"
2020,DHd2020,157_final-HALL_Mark_Die_Kanonfrage_2_0.xml,Die Kanonfrage 2.0,Corinna Dziudzia (KU Eichstätt - Ingolstadt); Mark Hall (Martin-Luther-Universität Halle-Wittenberg),"kanon, literaturgeschichte, portal","Theoretisierung, Community-Bildung, Crowdsourcing, Webentwicklung, Personen, Text","Maßgeblich leitete die amerikanische Literaturwissenschaft in den 1970er Jahren eine kritische Revision mit der Frage ein, wer eigentlich anhand welcher Kriterien entscheidet, welches literarische Werk zum Kanon gehört (vgl. Ziolkowski 2009).  Diese Kritik findet in einem anhaltenden Prozess des ""Entdeckens"" und ""Sichtbarmachens"" nichtkanonisierter Autor_innen und Werke Niederschlag (vgl. u.a. Brinker-Gabler 1978; Hilger 2015) und fördert ein breites Spektrum heterogenen Literatur- und Kunstschaffens zu Tage. Die im Rahmen der Kanondebatte im Wesentlichen formulierte Kritik am zu ""weißen"" und zu ""männlichen"" Kanon als ""Machtinstrument"" (Winko 1996, 500) wird auf der theoretischen Ebene mit dem Nachdenken über diskursive Machtpraktiken ebenso reflektiert wie mit der Konzeption des Archivbegriffs (vgl. Foucault 1990; Derrida 2009) und für den digitalen Raum erweitert. Abigail De Kosnik spricht entsprechend von einer potentiellen Verschmelzung von Kanon und Repertoire: [‚Ä¶] digital archives potentially redefine what A. Assmann calls a ""active memory"" and ""passive memory"", in the sense that these become highly individualized: all materials contained in an online database are equally available to the user 'no materials are any more ""hidden"" or ""stored away"" than any other materials, all materials that are indexed can be retrieved from the database 'and so users of an Internet Archive may ""activate"" whichever of the materials they wish, constructing their own personal canons based on the materials that they use. [‚Ä¶] Digital archives erect no physical barriers between categories of information, so conceivably any piece of information, any archived data, can enter into one person""s repertoire and canon; thus, there are as many possible canons as there are archive users, and no possibility for a single canon, achieved by a consensus of cultural archive users, that would be distinct from the culture""s archive. (De Kosnik 2016, 66) So wie durch die frühe Kanondebatte die grundlegende Frage gestellt worden ist, wessen Werke eigentlich publiziert, rezensiert und damit potentiell kanonisiert werden können, muss diese Frage heute aktualisiert werden: Wessen Werke werden wie digitalisiert, um im Sinne De Kosniks überhaupt derart aktiviert werden zu können? Die Kanonkritik verweist zudem auf grundlegende Reflexionen der Wissenschaftsgeschichte. Wesentlich ist hierfür etwa die Wiederentdeckung Ludwig Flecks und sein durch Thomas Kuhn verstärktes Betonen, wie sehr das jeweils tradierte Wissen Ausweis von Selektionsprozessen ist, das konkreten Rahmenbedingungen genauso wie Irrtümern und Denkzwängen unterliegt (Fleck 1980, 31; Kuhn 1996): Insofern die vorherige Generation das tradierenswerte Wissen in Lehrbüchern für die nachfolgende Generation auswählt, erscheint die jeweils angebotene bzw. fehlende Wissensrepräsentation aufschlussreich. Denn ungeachtet der seit mittlerweile Jahrzehnten geäußerten Kritik an der Homogenität des Kanons ist noch in jüngerer Zeit, etwa durch die Frauenforschung, festgestellt worden, wie überraschend wenig, teilweise gar nicht, schreibende Frauen in Form ihrer Namen und Werke aktuell in deutschen Leselisten, Literaturgeschichten und Lehrbüchern repräsentiert sind (vgl. Sylvester-Habenicht 2009). Die Stabilität des kleinen Kernkanons (vgl. Braam & Hagstedt 2017, 83), in dem sich immer noch vor allem männliche Autoren präsent zeigen, scheint sowohl durch die Debatte als auch die große Zahl an Entdeckungen vergessener schreibender Frauen (vgl. u.a. die Forschungsarbeiten Brinker-Gablers oder Becker-Cantarinos) wenig veränderbar, das Wissen um die Existenz der Texte ist nach wie vor marginal und auf kleine Expert_innenkreise beschränkt. In der digitalen Welt erweist sich Google Books zwar als umfangreiches Archiv der Texte auch von Autorinnen, stellt allerdings die Werke vorrangig als Scans zur Verfügung, weswegen diese für die automatisierte DH-Analyse weniger nutzbar sind, aber zumindest für die Lektüre verwendet werden können (vorausgesetzt, sie werden gefunden). Das wird ergänzt durch (allerdings nicht textverlässliche) Primärliteratur-Archive wie das Es gibt allerdings digitale Archivprojekte, welche das erklärte Ziel haben, die Arbeiten von Frauen sichtbarer zu machen, unter anderem das ""Women Writers Project"" (Connell et al. 2017), ""Orlando: Women""s Writing in the British Isles from the Beginnings to the Present"" (Booth 2017) oder ""DaSind - Die Datenbank Schriftstellerinnen in Deutschland, Österreich, Schweiz 1945-2008"" (Schulz 2008). Die ersten zwei sind jedoch kostenpflichtig und zielen nur auf englischsprachige Texte, und das dritte Projekt ist seit über einem Jahr nicht mehr online verfügbar (Stand: Dezember 2019). Digitale literaturwissenschaftliche Forschung der deutschen Literatur scheint entsprechend vorrangig mit jenen Digitalisaten unternommen zu werden, die prominent zur Verfügung stehen, leicht zugänglich sind und in entsprechend nutzbaren Formaten vorliegen, daher überrascht es nicht, dass sich die Digital Humanities tendenziell eines recht kleinen und männlichen Kanons deutscher Literatur bedienen (Hall, 2019). Um diese Schieflage zunächst aufzuzeigen und dann potentiell zu korrigieren, wurde das Das Projekt verfolgt vier Ziele: 1. Die Kanonfrage vor dem Hintergrund der Digitalisierung neu zu stellen 2. Lücken in der Digitalisierung von Werken außerhalb des Kanons aufzuzeigen 3. Den Zugang zu vorhandenen Digitalisaten zu vereinfachen, um die Schwelle zur Nutzung dieser Werke zu reduzieren 4. Autor_innen und ihre Werke als bisher eher marginalisiertes Wissen auch für die nicht-wissenschaftliche Öffentlichkeit zugänglich zu machen Um diese Ziele erreichen zu können, wird im Rahmen des Projekts ein Online-Portal entwickelt, zusammen mit den notwendigen Werkzeugen, die darin enthaltenen Daten zu verwalten. Eine Grundidee in der technischen Umsetzung ist es, nicht noch ein weiteres Archiv für Digitalisate bereitzustellen, sondern die in den verschiedenen existierenden Archiven vorhandenen Digitalisate mit allgemeinen Informationen über tendenziell vergessene Autor_innen zusammenzuführen. Der These folgend, dass Vieles ""unter der Oberfläche"" schlummert, wenngleich nicht immer in optimalen Formaten, geht es dem Projekt primär um das Sichtbarmachen dessen, was da ist und seien es zunächst nur die Namen von Autorinnen. Es geht dem Projekt nicht um die Digitalisierung oder Archivierung von Werken, die noch nicht vorliegen, vielmehr um das Aufzeigen von potentiell systematischen Leerstellen. Den Ausgangspunkt für das Vorhaben bildet eine erste Liste an Namen von Autor_innen, die von den Projektmitgliedern entwickelt wurde. Langfristig ist das Projekt so angelegt, dass aus der DH-Community Namen hinzugefügt werden können und das Portal so langsam wächst. Basierend auf den Namen werden Werke sowie weitere relevante Informationen in verschiedenen Archiven identifiziert. Zur Zeit werden dazu vor allem vier Quellarchive genutzt: Die Basierend auf den derart identifizierten Daten, wird dann das Online-Portal generiert. So vorhanden, wird dann im Rahmen des zweiten Projektziels für alle Autor_innen eine Liste der bekannten Werke geführt 'unabhängig davon, ob und in welcher Form diese digitalisiert sind. Daraus generiert das Portal eine Reihe von Statistiken, welche einen Überblick darüber geben, in welchem Grad die Werke einzelner Autor_innen, bzw. der Gesamtbestand, digital bereits aufgearbeitet sind. Diese Statistiken sind auch über das Suchsystem zugänglich, es ist also möglich, zum Beispiel nach Autor_innen zu suchen, welche während eines gewissen Zeitraums an einem bestimmten Ort gewirkt haben. Potentiell können darüber Verbindungen von Autor¬≠_innen in Form von Netzwerken erkennbar werden. Dies unterstützt Geisteswissenschaftler_innen nicht zuletzt in der Identifikation potentiell interessanter Forschungsfragen. Parallel dazu werden für alle Daten und Metadaten maschinenlesbare Versionen bereitgestellt. Dies unterstützt das dritte Projektziel, da die Daten des Portals nahtlos in digitale Arbeitsabläufe der DH-Forschung integriert werden können. Um die nicht-wissenschaftliche Öffentlichkeit anzusprechen (Johnson 2008), bietet das Portal eine Reihe von Funktionalitäten an. Es ist bekannt, dass es Nicht-Experten schwer fällt, erfolgreich zu suchen (Geser 2004; Wilson & Elsweiler 2010) und sie eine Präferenz für Browsing haben (Walsh et al. 2018). Daher entwickelt das Projekt eine Reihe von browsing-basierten Schnittstellen. Unter anderem ""ein Werk/eine Autorin des Tages"", welches entweder zufällig oder basierend auf Lebensdaten der Autorin ausgewählt und den Nutzern als Impuls vorgeschlagen wird. Auch sollen die Werke, basierend auf ihren Themen, automatisch gruppiert werden, damit Benutzer_innen durch die daraus entstehende Themenstruktur stöbern können. Zusätzlich wird das Portal eine Lesefunktion für jene Textdokumente anbieten, welche in den Quellarchiven in maschinenlesbarer Form vorhanden sind. Ziel ist es dabei nicht, eine Schnittstelle zur wissenschaftlichen Arbeit mit den Texten zu bieten, sondern eine Komponente, mit denen Texte wie ein gedrucktes Buch gelesen werden können. Die gewählte Lösung eines (weiteren) Portals birgt natürlich die Frage, wie macht man es sichtbar und warum sollten Nutzer_innen es nutzen? Die Sichtbarkeit innerhalb der DH-Community soll über Beiträge in Konferenzen und Zeitschriften erreicht werden. In einem ersten Pilotversuch wurde das Portal in der universitären Lehre zum Gegenstand eines Seminars mit Studierenden der germanistischen Literaturwissenschaft gemacht, darüber stellt sich idealerweise perspektivisch ein Multiplikatoreneffekt ein. Der Zugriff auf Nutzer_innen aus der nicht-wissenschaftlichen Öffentlichkeit ist natürlich wesentlich schwieriger. Es ist aber so, dass soziale Medien von den unterrepräsentierten Gruppen oft stark genutzt werden und wir sehen das als die primäre Methode, um eine breitere Sichtbarkeit des Projekts zu erreichen (McLean and Maalsen 2013). Die Problematik des Bias im Kanon kann natürlich nicht vom Projekt direkt gelöst werden. Unser Ziel ist es vielmehr, einen ersten Schritt zu unternehmen, um die Sensitivität für die Kanonfrage in den DH zu unterstützen und einen kritischen Diskurs zur Frage, welche Texte in welcher Form eigentlich digitalisiert werden, bzw. darüber hinaus, woran digitale literaturwissenschaftliche Forschung erfolgt, bzw. erfolgen kann, zu fördern. Durch eine breitere Aufstellung der für DH-Forschung genutzten Daten wäre entsprechend zu hoffen, dass sich der mutmaßlich bisher unbewußte Bias der Daten reduziert. Momentan wird digitale Forschung tendenziell an einem verengten und homogenen Kanon deutscher Literatur betrieben, während literarische Werke jenseits einer ""männlichen"" Auswahl, teils durch fehlende Digitalisate, teils durch mangelnde Sichtbarkeit, kaum berücksichtigt werden. Damit werden die Bemühungen der einzelnen Fachdisziplinen um Heterogenität und Diversität konterkariert, denn es betrifft nicht nur das Schreiben von Autorinnen, sondern ein breiteres Spektrum an unterrepräsentierten Gruppen. Langfristig ist das Ziel des Projekts, sich selbst unnotwendig zu machen, insofern der Bias in den digitalen Quellarchive behoben ist, aber bis dahin will das Projekt die Leerstellen sichtbarer und greifbarer machen.",de,maßgeblich leiten amerikanisch Literaturwissenschaft kritisch Revision Frage eigentlich anhand kriterien entscheiden literarisch Werk Kanon gehören Ziolkowski Kritik finden anhaltend Prozess entdeckens sichtbarmachens nichtkanonisierter werk Niederschlag hilg fördern breit spektrum heterogen Kunstschaffens Rahmen Kanondebatte wesentlich formuliert Kritik weiß männlich Kanon Machtinstrument Winko theoretisch Ebene Nachdenken diskursiv Machtpraktik reflektieren Konzeption archivbegriffs Foucault derrida digital Raum erweitern Abigail de Kosnik sprechen entsprechend potentiell Verschmelzung Kanon Repertoire Digital archiv potentially Redefine What Assmann Call activ memory and passiv memory The sense That These Become Highly individualized all Material Contained Online database are equally available to -- user no materials are any More hidd or stored Away than any other Material all Material thaen are indexed can be Retrieved from -- database and users -- Internet archiv may activat whichever -- -- materials They Wish Constructing Their own Personal Canon based -- -- materials thaen They use Digital archiv erect no physical barrier between categories -- Information conceivably any piece -- Information any Archived detaen can Enter Into One Person -- repertoire and Canon thus theren are as many Possible Canon As theren are archive users And no possibility for Single Canon Achieved by consensus -- cultural archive users thaen Would be distinct from -- culture -- archiven de Kosnik Frühe Kanondebatte grundlegend Frage stellen Werk eigentlich publizieren rezensieren potentiell kanonisieren Frage aktualisieren Werk digitalisieren Sinn de Kosnik derart aktivieren Kanonkritik verweisen zudem grundlegend Reflexion Wissenschaftsgeschichte wesentlich hierfür Wiederentdeckung Ludwig flecks Thomas Kuhn verstärkt betonen jeweils Tradiert wissen Ausweis Selektionsprozessen konkret Rahmenbedingung genauso irrtümern denkzwängen unterliegen Fleck kuhn insofern vorherig Generation tradierenswert wissen Lehrbüchern nachfolgend Generation auswählen erscheinen jeweils angeboten fehlend Wissensrepräsentation aufschlussreich ungeachtet mittlerweile Jahrzehnt geäußert Kritik Homogenität Kanon jung Frauenforschung feststellen überraschend teilweise schreibend Frau Form Name Werk aktuell deutsch Leseliste literaturgeschichten lehrbüchern repräsentieren Stabilität kernkanons Braam Hagstedt männlich Autor präsent zeigen scheinen sowohl Debatte Zahl entdeckung Vergessener schreibend Frau forschungsarbeit veränderbar wissen Existenz Text marginal beschränken digital Welt erweisen Google Books umfangreich Archiv Text autorinnen stellen Werk vorrangig scan Verfügung weswegen automatisiert nutzbar zumindest lektüre verwenden voraussetzen finden ergänzen textverlässlich digital Archivprojekt erklärt Ziel arbeiten Frau sichtbarer wom writers Project connell et orlando women -- Writing The British isles from The Beginning to -- Present Booth dasind Datenbank Schriftstellerinne Deutschland Österreich Schweiz Schulz kostenpflichtig Ziel englischsprachig Text Projekt Online verfügbar stehen Dezember digital literaturwissenschaftlich Forschung deutsch Literatur scheinen entsprechend vorrangig Digitalisat unternehmen Prominent Verfügung stehen zugänglich entsprechend nutzbaren Format vorliegen überraschen Digital humaniteisen tendenziell männlich Kanon deutsch Literatur bedienen Hall Schieflage Aufzuzeige potentiell korrigieren Projekt verfolgen ziel Kanonfrage Hintergrund Digitalisierung neu stellen Lücke Digitalisierung Werk außerhalb Kanon aufzuzeigen Zugang vorhanden Digitalisat vereinfachen Schwelle Nutzung Werk reduzieren Werk eher marginalisiert wissen Öffentlichkeit zugänglich Ziel erreichen Rahmen Projekt entwickeln notwendig Werkzeug enthalten daten verwalten Grundidee technisch Umsetzung Archiv digitalisat bereitstellen verschieden existierend archiv vorhanden Digitalisate Information tendenziell vergessen Zusammenzuführ These folgend viele Oberfläche schlummern wenngleich optimal Format Projekt primär sichtbarmach Name autorinnen Projekt Digitalisierung Archivierung Werk vorliegen vielmehr aufzeigen potentiell systematisch leerstellen Ausgangspunkt Vorhaben bilden Liste Name Projektmitglieder entwickeln langfristig Projekt anlegen Name hinzufügen Portal langsam wachsen basierend Name werk relevant Information verschieden archiven identifizieren quellarchiv nutzen basierend derart identifiziert daten neriern vorhanden Rahmen Projektziel Liste bekannt Werk führen unabhängig Form digitalisieren generieren Portal Reihe Statistik Überblick geben Grad Werk einzeln Gesamtbestand Digital aufarbeiten Statistik Suchsystem zugänglich suchen gewiß zeitraums bestimmt Ort wirken potentiell Verbindung Form netzwerken erkennbar unterstützen zuletzt Identifikation potentiell interessant forschungsfragen parallel daten metadat maschinenlesbar Version bereitstellen unterstützen Projektziel daten Portal nahtlos digital Arbeitsabläufe integrieren Öffentlichkeit anzusprechen Johnson bieten Portal Reihe funktionalitäten schwer fallen erfolgreich suchen Geser Wilson elsweil Präferenz Browsing Walsh et entwickeln Projekt Reihe Schnittstelle Werk Autorin Tag zufällig basierend Lebensdat Autorin auswählen Nutzer impuls vorschlagen werk basierend Thema automatisch gruppieren entstehend Themenstruktur stöbern zusätzlich Portal Lesefunktion textdokument anbieten quellarchiven Maschinenlesbarer Form vorhanden Ziel Schnittstelle wissenschaftlich Arbeit Text bieten Komponente Text gedruckt Buch lesen gewählt Lösung Portal bergen Frage sichtbar nutzen Sichtbarkeit innerhalb beitrag konferenz Zeitschrift erreichen Pilotversuch Portal universitär Lehre Gegenstand Seminar Studierend germanistisch Literaturwissenschaft stellen idealerweise perspektivisch Multiplikatoreneffekt Zugriff Öffentlichkeit wesentlich schwierig sozial Medium unterrepräsentiert Gruppe stark nutzen sehen primär Methode breit Sichtbarkeit Projekt erreichen mclean and Maalsen Problematik Bia Kanon Projekt direkt lösen Ziel vielmehr Schritt unternehmen Sensitivität Kanonfrage dh unterstützen kritisch Diskurs Frage Text Form eigentlich digitalisieren hinaus woran digital literaturwissenschaftlich Forschung erfolgen erfolgen fördern breit Aufstellung genutzt daten entsprechend hoffen mutmaßlich unbewußt Bias daten reduzieren momentan digital Forschung tendenziell verengt homogen Kanon deutsch Literatur betreiben literarisch Werk jenseits männlich Auswahl teils fehlend digitalisat teils mangelnd Sichtbarkeit berücksichtigen Bemühung einzeln Fachdisziplin Heterogenität Diversität konterkarieren betreffen Schreiben Autorinn breiter Spektrum unterrepräsentiert Gruppe langfristig Ziel Projekt unnotwendig insofern Bias digital quellarchiv behoben Projekt leerstell Sichtbarer Greifbarer,"[('werk', 0.22225103522368286), ('kanon', 0.2138289232525), ('portal', 0.20931058263690921), ('are', 0.1836266675060384), ('archiv', 0.1505939374738898), ('canon', 0.13682745578521813), ('any', 0.13682745578521813), ('potentiell', 0.1364511709637236), ('projekt', 0.12480673945253332), ('digitalisat', 0.11577613647169047)]"
2020,DHd2020,243_final-GUHR_Svenja_Doctoral_Consortium__Svenja_Guhr.xml,Raise your voice! - Über den Zusammenhang zwischen Lautstärkemerkmalen in literarischen Prosatexten und der Emanzipation der Frau von 1848 bis 1920,"Svenja Guhr (TU Darmstadt, Deutschland)","(Audio-)Narratologie, Deutsche Prosaliteratur, Digitale Philologie, Emanzipation der Frau, Lautstärke, Verba Dicendi","(Audio-)Narratologie, Deutsche Prosaliteratur, Digitale Philologie, Emanzipation der Frau, Lautstärke, Verba Dicendi","Mein im Rahmen des Die Untersuchungen bauen auf der Hypothese auf, dass in der Mitte des 19. Jahrhunderts Frauenfiguren in literarischen Prosatexten prozentual weniger, kürzere und leisere Redebeiträge zugeschrieben werden als männlichen Figuren, was sich jedoch mit der ansteigenden Emanzipation der Frau verändert. Das Projekt zielt darauf herauszufinden, ob sich Frauen- und Männerfiguren im Verlauf der betrachteten Zeitperiode in ihrer Anzahl an Redebeiträgen und ihrer Lautstärke annähern. Die steigende Lautstärke zeichnet sich dabei durch eine ""lautere"" Beschreibung von Redebeiträgen aus, die u.a. durch als ""lauter"" wahrnehmbare redeeinleitende Verben gekennzeichnet sind. Lautstärke wird dabei als ein narratologisches Element betrachtet, das in der Literaturwissenschaft bisher nur wenig Aufmerksamkeit erhalten hat. Der Umgang mit Erzählformen und Diskursen als ein wichtiges Kriterium der Narratologie fand seine Erweiterung durch das neue Forschungsfeld der Audionarratologie. Diese widmet sich u.a. der Relation zwischen Narrativen und den in der Lesevorstellung bei der stillen Lektüre erlebten Geräuschen, Tönen und Dynamik (Mildorf / Kinzel 2016; Kuzmiƒçov√° 2013). Literarische Texte beinhalten neben Beschreibungen von natürlichen und industriellen Geräuschen (z.B. Natur- und Maschinengeräusche) auch Wiedergaben von Figurenrede. Insbesondere in Prosa ordnen Autoren und Autorinnen (i. F. generisches Femininum) ihren Figuren durch beschreibende Einleitungen von Redebeiträgen Stimmen zu, die in den Gedanken von Rezipientinnen wahrzunehmen sind. Ein neuer Ansatz der Figurenanalyse findet in der Betrachtung von Tönen, Lautstärke und Stimmvolumen in literarischen Prosatexten Anwendung. Vor allem die redeeinleitenden Verben, die direkte wie indirekte Redebeiträge einleiten und somit die Art und Weise der Figurenrede beschreiben, ermöglichen den Rezipientinnen die Wahrnehmung von Figurenstimmen und -lautstärke, z.B. ob eine Figur schreit oder flüstert. In Anlehnung an Hunt (2017), die in ihrer Studie ein Korpus aus anglophoner Kinder- und Jungendliteratur auf stereotypische Rollendarstellungen untersuchte, die sie anhand der ""gendered nature"" von redeeinleitenden Verben herausstellte, wird auch in meinem Forschungsprojekt eine genderdifferenzierte Betrachtung dieser Verbgruppe vorgenommen. In Hunts Ausführungen stützt sie sich auf Caldas-Coulthards (1992) Unterscheidung von redeeinleitenden Verben in neutrale (z.B. Ziel der Lautstärkeuntersuchung ist es herauszufinden, ob es einen Zusammenhang zwischen der beschriebenen Sprechweise weiblicher Prosafiguren und der ansteigenden Emanzipation der Frau in der deutschsprachigen Gesellschaft ab der zweiten Hälfte des 19. Jahrhunderts bis zum Erhalt des deutschen Frauenwahlrechts 1919 gibt (erweitert um das Jahr 1920, damit die Auswirkungen der Einführung des Frauenwahlrechtes mit aufgenommen werden). Weitere Unterhypothesen beschäftigen sich mit dem Zusammenhang zwischen der beschriebenen Lautstärke einer Frauenfigur mit ihrem Bildungsstand sowie ihrer gesellschaftlichen Stellung. Weiterhin wird untersucht, ob Frauenfiguren in der Öffentlichkeit leiser beschrieben werden als im privaten Raum (vgl. Howe 2000). Darüber hinaus soll herausgestellt werden, ob Frauenfiguren in Anwesenheit von Männerfiguren leiser dargestellt werden als in der alleinigen Gesellschaft von Frauenfiguren, wobei untersucht wird, ob die plötzliche Anwesenheit auch nur einer Männerfigur in einem weiblichen Beisammensein die Art und Weise, in der Frauenfiguren miteinander sprechen, beeinflusst und ob diese Verhaltensänderung von der Beziehung der anwesenden männlichen Figur zu den Frauenfiguren (z.B. Vater, Bruder, Cousin, Fremder, Arbeitgeber, etc.) abhängt. Die Studie basiert auf der Analyse eines deutschsprachigen Korpus bestehend aus literarischen Prosatexten (Ziel: ca. 500). Das betrachtete thematische Korpus (vgl. Baker 2007: 26, Gür-SÃßeker 2014: 585) befindet sich aktuell (Stand: Januar 2020) noch in der Erstellungsphase, wobei auch auf existierende und teilweise bereits annotierte Korpora wie z.B. auf das Redewiedergabekorpus der Kooperation zwischen dem Leibniz-Institut für Deutsche Sprache, Mannheim und der Universität Würzburg (Brunner et al.) zurückgegriffen werden wird. Die strömungsübergreifend deutschsprachigen Prosatexte werden nach den folgenden Kriterien ausgewählt: deutschsprachig, Zeit der Publikation zwischen 1848 und 1920, Für einen ersten Analyseansatz wurde ein Probekorpus erstellt, das 80 deutschsprachige Prosatexte umfasst und in zwei vergleichbare Subkorpora unterteilt wurde (2x 40 Prosatexte). Bei der Erstellung wurden die Texte nach den zuvor genannten Kriterien ausgewählt, wobei der Fokus auf die zwei Zeiträume 1865-75 und 1885-95 gelegt wurde, in denen jeweils eine überregional bedeutende Frauenrechtsaktion stattfand (Richards 2004). Das Probekorpus diente als Grundlage zur Entwicklung einer regelbasierten Methode, mit deren Hilfe redeeinleitende Verben sowie die sie umgebenden Adjektive, Adverbien (z.B. Im Laufe des Dissertationsvorhabens sollen die Untersuchungen zur Lautstärkewertzuweisung zu redeeinleitenden Verben in einem umfangreicheren und wissenschaftlich fundierten Verfahren wiederholt werden. Zudem werden methodische Herausforderungen wie die Auflösung von Koreferenzen und Anaphern sowie die Erkennung von (unregelmäßigen) Redebeiträgen, Szenengrenzen und Figurenkonstellationen einen großen Bestandteil meiner Forschung einnehmen.",de,Rahmen Untersuchung bauen Hypothese Mitte Jahrhundert frauenfiguren literarisch Prosatext prozentual kürz leis redebeiträge zuschreiben männlich Figur ansteigend Emanzipation Frau verändern Projekt zielen herausfinden Männerfigur Verlauf betrachtet Zeitperiode Anzahl redebeiträgen Lautstärke annähern steigend Lautstärke zeichnen laut Beschreibung Redebeiträg lauter wahrnehmbare redeeinleitend verben kennzeichnen Lautstärke narratologisch Element betrachten Literaturwissenschaft Aufmerksamkeit erhalten Umgang Erzählforme diskursen wichtig Kriterium Narratologie finden Erweiterung Forschungsfeld Audionarratologie widmen Relation narrativ Lesevorstellung still lektüre erlebt Geräusch Tön Dynamik mildorf kinzel literarisch Text beinhalen Beschreibung natürlich industriell geräuschen maschinengeräusch wiedergaben Figurenrede insbesondere Prosa ordn Autor autorinn generisch Femininum Figur beschreibend Einleitung redebeiträg Stimme Gedanke rezipientinnen Wahrzunehm neu Ansatz Figurenanalyse finden Betrachtung Tön Lautstärke stimmvolumen literarisch Prosatext Anwendung Redeeinleitend verben direkt indirekt redebeiträge einleiten somit Art Weise Figurenrede beschreiben ermöglichen Rezipientinn Wahrnehmung Figurenstimm Figur Schreit flüstern Anlehnung Hunt Studie korpus anglophon jungendliteratur stereotypisch Rollendarstellunge untersuchen anhand gendered Nature redeeinleitend verben herausstellen Forschungsprojekt genderdifferenziert Betrachtung Verbgruppe vornehmen Hunt ausführungen stützen Unterscheidung redeeinleitend Verbe Neutrale Ziel Lautstärkeuntersuchung herausfinden Zusammenhang beschrieben Sprechweise weiblich Prosafigur ansteigend Emanzipation Frau deutschsprachig Gesellschaft Hälfte Jahrhundert Erhalt deutsch Frauenwahlrecht erweitern Auswirkung Einführung frauenwahlrechtes aufnehmen Unterhypothesen beschäftigen Zusammenhang beschrieben Lautstärke Frauenfigur Bildungsstand gesellschaftlich Stellung weiterhin untersuchen frauenfiguren Öffentlichkeit leise beschreiben privat Raum how hinaus herausstellen Frauenfigure Anwesenheit männerfiguren Leiser darstellen alleinig Gesellschaft Frauenfigur wobei untersuchen plötzlich Anwesenheit Männerfigur weiblich Beisammensein Art Weise Frauenfigur miteinander sprechen beeinflussen Verhaltensänderung Beziehung anwesend männlich Figur Frauenfigur Vater Bruder Cousin Fremder Arbeitgeber abhängen Studie basieren Analyse deutschsprachig Korpus bestehend literarisch Prosatext Ziel betrachtet thematisch Korpus Baker befinden aktuell stehen Januar Erstellungsphase wobei existierend teilweise annotiert Korpora Redewiedergabekorpus Kooperation deutsch Sprache Mannheim Universität Würzburg Brunner et zurückgegriffen strömungsübergreifend deutschsprachig Prosatext folgend kriterien auswählen deutschsprachig Publikation Analyseansatz Probekorpus erstellen deutschsprachig Prosatext umfassen vergleichbar Subkorpora unterteilt Prosatext Erstellung Text zuvor genannt kriterien auswählen wobei Fokus Zeiträum legen jeweils überregional bedeutend Frauenrechtsaktion stattfinden richards Probekorpus dienen Grundlage Entwicklung regelbasierten Methode Hilfe redeeinleitend verben umgebend adjektiv Adverbie Lauf Dissertationsvorhaben Untersuchung Lautstärkewertzuweisung redeeinleitend Verbe umfangreicher wissenschaftlich fundiert Verfahren wiederholen zudem methodisch Herausforderung Auflösung koreferenzen anaphern Erkennung unregelmäßig redebeiträg Szenengrenzen figurenkonstellationen Bestandteil Forschung einnehmen,"[('redeeinleitend', 0.3230297208428229), ('lautstärke', 0.25512953445411557), ('prosatext', 0.22862653145584336), ('frauenfigur', 0.21535314722854856), ('redebeiträg', 0.17340634101063684), ('verben', 0.16454598585023258), ('probekorpus', 0.11560422734042457), ('frauenfiguren', 0.11560422734042457), ('verbe', 0.11560422734042457), ('hunt', 0.11560422734042457)]"
2020,DHd2020,233_final-HORSTMANN_Jan_Netzwerkanalyse_spielerisch_vermitteln_mit_Dra.xml,"Netzwerkanalyse spielerisch vermitteln mit DraCor und forTEXT: Zur nicht-digitalen Dissemination einer digitalen Methode in Form des Kartenspiels ""Dramenquartett""","Jan Horstmann (Universität Hamburg, Deutschland); Marie Flüh (Universität Hamburg, Deutschland); Mareike Schumacher (Universität Hamburg, Deutschland); Frank Fischer (Higher School of Economics Moskau, Russland); Peer Trilcke (Universität Postdam, Deutschland); Jan Christoph Meister (Universität Hamburg, Deutschland)","niedrigschwellige Dissemination, Kartenspiel, Lerntypen","Netzwerkanalyse, Einführung, Lehre, Visualisierung, Interaktion, Literatur","Mit ELTeC (European Literary Text Collection; Das DFG-Projekt Die Videos vermitteln die Methode über eine Text-Bild-Audio-Kombination: Das Methodenvideo bietet eine Fallstudie zum Figurennetzwerk von Angesprochen werden hier theoretische, strukturelle und emotionale autodidaktische Vermittlungsmuster (zur Bedeutung von Emotionen für autodidaktisches Lernen vgl. Mega u.a. 2014). Auf der Tonebene ist ein erklärender Duktus vorherrschend. Die ,selfmade""-Anmutung der Videos vermittelt, dass die autodidaktische Erarbeitung der Inhalte Betrachter√Ønnen und Ersteller√Ønnen des Videos miteinander verbindet (vgl. Horstmann & Schumacher 2019). Die Tutorial-Reihe schließlich funktioniert ähnlich wie die Lerneinheit als Schritt-für-Schritt-Anleitung und bietet die Möglichkeit, die Arbeit mit Gephi als Screencast zu erlernen. Das Tutorial-Video zur Nutzung des DraCor-Tools ezlinavis verknüpft die praktische Erstellung von Netzwerken mit der Nutzung der Ressource TextGrid Repository (vgl. Horstmann 2018) und den Methoden Named Entity Recognition (vgl. Schumacher 2018a) und Annotation in CATMA (vgl. Jacke 2018 und Schumacher 2019a). Die Dissemination einer digitalen Methode wie der Netzwerkanalyse durch ein nicht-digitales Kartenspiel bietet Möglichkeiten, die die bisher genannten digitalen Medien nicht abdecken konnten. Die Spieler√Ønnen werden in einer nicht-digitalen Umgebung mit den funktional reduzierten Ergebnissen einer digitalen Analyse konfrontiert, können diese visuell und haptisch erfahren und spielerisch explorieren. Der empfohlene Spielmodus ist ,Supertrumpf"" Die im Folgenden vorgestellte, reflektierte und erprobte Pipeline geht von einer ersten theoretischen Annäherung durch forTEXT-Tutorials aus, auf die eine spielerische Vertiefung der spezifischen Objektkonstitution qua Netzwerkanalyse und der entsprechenden Metriken mittels des Dramenquartetts folgt. Anschließende Arbeitsphasen könnten, wie in 3. skizziert, z.¬†B. die formalisierte Erstellung, Gestaltung und Analyse von Dramennetzwerken mittels ezlinavis und Gephi oder die konkrete Bearbeitung von literarhistorischen Forschungsfragen mittels DraCor umfassen. Der quantifizierende Zugriff auf Dramentexte kann als ""radikale ,Anästhetisierung"" der Objekte"" (Trilcke, im Erscheinen) beschrieben werden. Auf die qua Formalisierung erfolgende Anästhetisierung, bei der die ursprüngliche ästhetische Dimension des literarischen Kunstwerks zunächst ausgesetzt wird, folgt jedoch eine reästhetisierende Transformation im Zuge der Diagrammatisierung (vgl. ebd.). Ein entscheidender Vorteil digitaler Diagramme ist die Möglichkeit der Interaktion (vgl. Horstmann, im Erscheinen): Netzwerke lassen sich je nach Wahl des Layoutalgorithmus unterschiedlich darstellen, ein semantischer Zoom ermöglicht überdies, zusätzliche Informationen des Ausgangsmaterials zu visualisieren. Dramennetzwerke in einer festgelegten (und damit nicht mehr veränderbaren) Form als Spielkarte zu drucken, bedeutet daher in erster Linie eine funktionale Reduktion. Gerade diese funktionale Reduktion eröffnet jedoch didaktische Spielräume: Das Wissen, dass die abgedruckten Netzwerke ebenfalls in digitaler Form vorhanden und dort sogar manipulierbar sind, wird im Laufe des Spielprozesses die Neugier auf diese Funktionsvielfalt steigern, sodass der Übergang in die ,digitale Arbeit"" fließend stattfinden kann und nicht mehr als etwas kategorial anderes empfunden wird. Die Interaktion zwischen Benutzer√Ønnen und Netzwerken als konzeptioneller Bestandteil digitaler Netzwerkdarstellungen wird übertragen auf die Interaktion zwischen den Spieler√Ønnen, wodurch nicht zuletzt die von Jenkins (2006, 2) sog. Das Kartenspiel entfaltet seinen didaktischen Mehrwert auch, weil es situational gerahmt ist: Es wird in kollektiven Unterrichtsphasen eingesetzt, die darauf abzielen, sich einem abstrakten Unterrichtsgegenstand auf spielerische Weise anzunähern. Da Menschen in ihrer Rolle als Im Fokus steht der Versuch, nicht nur kumulatives bzw. assimilatives Lernen zu initiieren, wodurch v.¬†a. begrenztes, anwendungsorientiertes Wissen oder thematisch, anwendungsorientiertes Wissen produziert werden würde (vgl. Illeris 2010). Die 'von der konkreten Kenntnis des Spielprinzips ,Supertrumpf"" unabhängige 'spielerische Aktivierung unterschiedlicher Sinneskanäle und die damit einhergehende Diskussion über Fachinhalte zielt auf die Einleitung akkommodativer und transformativer Lernprozesse und darauf, über Fachwissen in relevanten Kontexten frei verfügen zu können. Das im Wintersemester 2019/2020 an der Universität Hamburg durchgeführte Seminar ""Digitale Literaturwissenschaft und pädagogische Praxis"" hat unterschiedliche Standardverfahren und Werkzeuge erprobt, die gegenwärtig in der digitalen Literaturwissenschaft eingesetzt werden. Dieses Feld wird zunehmend auch für Lehrer√Ønnen insbesondere im gymnasialen Bereich relevant: Bereits die heutige Schüler√Ønnengeneration zählt zu den Um den Effekt des Dramenquartetts auf den Lernerfolg der Studierenden zu untersuchen, wurde eigens ein Testverfahren entwickelt, das die Wissensstände vor und nach dem Einsatz des Quartetts mess- und v.¬†a. vergleichbar macht. Das Verfahren setzt sich aus fünf aufeinander aufbauenden Phasen zusammen: (1) Vorbereitend befasst sich ein Teil der Lerngruppe mit schriftlichen forTEXT-Lernmaterialien zur digitalen Netzwerkanalyse, während der andere Teil die Video-Fallstudien und -Tutorials konsultiert. (2) Ausgangspunkt der Erhebung stellt folglich ein gruppenspezifisch relativ homogener Wissensstand dar, der grundlegende Kenntnisse über die Methode der digitalen Netzwerkanalyse beinhaltet. Um die Wissensstände beider Gruppen vor dem Einsatz des Quartetts zu erfassen, wurde eine Umfrage entworfen und zu Beginn des Seminars in Einzelarbeit mit dem Audience Response System ARSnova durchgeführt. Die Umfragen adressieren mit jeweils neun Fragen drei Anforderungsbereiche (I: Reproduktionsleistung, II: Reorganisation- und Transferleistung, III: Reflexion und Problemlösung). Den Anforderungsbereichen entsprechend beinhalten sie Single-Choice-, Multiple-Choice- sowie Freitextfragen. (3) Nach der ersten Quizphase wurde die gesamte Testgruppe in Kleingruppen eingeteilt, die im Supertrumpf-Modus das Dramenquartett spielen. (4) Eine zweite Umfrage erfasst den Wissensstand beider Gruppen, nachdem sie das Dramenquartett gespielt haben. (5) Die Auswertung des ersten Testdurchlaufs, der mit 11 Teilnehmenden durchgeführt wurde, verweist auf einen lernförderlichen Effekt des Dramenquartetts. Im Rahmen der ersten Quizrunde wurden 43% der Fragen, nach der zweiten Umfrage 52% der Fragen richtig beantwortet. Darüber hinaus verweist ein erster Blick auf die Freitextantworten darauf, dass der spielerische Zugang die intrinsische Motivation, sich über den Seminarkontext hinaus mit digitaler Netzwerkanalyse auseinanderzusetzen, steigert. Das erarbeitete Verfahren zur vergleichenden Lernstandserhebung hat sich bewährt und wird in einem weiteren Seminar eingesetzt, um den Einfluss einer spielerischen Wissensvermittlung auf Kompetenz- und Wissensstand zu untersuchen. Das Projekt lotet das didaktische Potenzial von Gamification-Ansätzen in den DH konzeptionell und praktisch aus, indem es das DraCor-Kartenspiel mit Tools und Tutorials in einer didaktischen ,Pipeline"" verbindet und damit in die Disseminationsstrategie von forTEXT integriert. Der damit entwickelte Prototyp eines Konzepts, das auch fachdidaktisch Weiterentwicklungspotenzial birgt, ermöglicht diverse Adaptionen und Transformationen: in Hinblick auf die Netzwerkanalyse literarischer Texte, in Hinblick auf andere Methoden der Digital Humanities sowie in Hinblick auf das didaktische Szenario einer Verzahnung von analogen und digitalen Ansätzen. So ließen sich auf der Grundlage der Netzwerkdaten aus anderen DH-Projekten, etwa zu Romanen, andere generische Karten-Sets entwerfen, wobei auch die 'durch ezlinavis in Kombination mit Gephi ermöglichte 'kollaborative Erstellung eigener Sets denkbar ist. Diese selbstständige Erstellung von Karten-Sets würde nicht zuletzt auch den haptischen Lerntyp ansprechen. Eine Weiterentwicklung der didaktischen Engführung von Analogem und Digitalem ließe sich über eine Verzahnung des Kartenspiels mit der digital-interaktiven Repräsentation der einzelnen Dramen auf DraCor vornehmen (z.¬†B. über QR-Codes). Unter didaktischen Gesichtspunkten bietet sich des Weiteren die Möglichkeit, kreativ-produktionsorientierte Elemente in die skizzierte Pipeline einzubauen, etwa indem die Lernenden Netzwerke ,erfinden"", die sie zunächst händisch zeichnen und dann 'den Schritt in den digitalen Raum machend 'mittels ezlinavis formal erfassen müssen. Der im Projekt durchgeführte Testlauf soll in diesem Sinne zu einer weiteren Diskussion über didaktische Potenziale sowohl von Gamification-Ansätzen als auch der Verzahnung von analogen und digitalen Lehrmitteln anregen und damit grundsätzlich der Reflexion über didaktische Szenarien dienen, die den spielerischen, kreativen Übergang zwischen lebensweltlich vertrauten Situationen und der Abstraktion digitaler Forschungsprozesse gestalten.",de,Eltec european Literary Text Collection Videos vermitteln Methode Methodenvideo bieten Fallstudie Figurennetzwerk ansprechen theoretisch strukturell emotional autodidaktisch Vermittlungsmuster Bedeutung Emotion autodidaktisch lernen Mega Tonebene erklärend Duktus vorherrschend Videos vermitteln autodidaktisch Erarbeitung inhalt Video miteinander verbinden Horstmann Schumacher schließlich funktionieren ähnlich Lerneinheit bieten Möglichkeit Arbeit gephi Screencast erlernen Nutzung ezlinavis verknüpfen praktisch Erstellung netzwerken Nutzung Ressource Textgrid Repository Horstmann Methode Named entity Recognition schumach Annotation Catma Jacke schumach Dissemination digital Methode Netzwerkanalyse Kartenspiel bieten Möglichkeit genannt digital Medium abdecken Umgebung Funktional reduziert Ergebnis digital Analyse konfrontieren Visuell haptisch erfahren spielerisch explorieren empfohlener Spielmodus Supertrumpf folgend vorgestelln reflektieren erproben Pipeline theoretisch Annäherung spielerisch Vertiefung spezifisch Objektkonstitution qua Netzwerkanalyse entsprechend metriken mittels Dramenquartett folgen anschließend arbeitsphasen können skizzieren formalisiert Erstellung Gestaltung Analyse dramennetzwerken mittels ezlinavis gephi konkret Bearbeitung literarhistorisch Forschungsfrag mittels Dracor umfassen quantifizierend Zugriff Dramentexte radikal Anästhetisierung Objekt trilcken erschein beschreiben qua Formalisierung erfolgend Anästhetisierung ursprünglich ästhetisch Dimension literarisch kunstwerks aussetzen folgen reästhetisierend Transformation Zug Diagrammatisierung entscheidend Vorteil Digitaler Diagramm Möglichkeit Interaktion Horstmann erscheinen netzwerke lassen Wahl Layoutalgorithmus unterschiedlich darstellen semantisch Zoom ermöglichen überdies zusätzlich Information Ausgangsmaterial visualisieren Dramennetzwerk festgelegt veränderbaren Form Spielkarte drucken bedeuten Linie funktional Reduktion funktional Reduktion eröffnen didaktisch Spielraum wissen abgedruckt netzwerken ebenfalls Digitaler Form vorhanden sogar manipulierbar Lauf spielprozesses Neugier Funktionsvielfalt steigern sodass Übergang digital Arbeit fließend stattfinden kategorial anderer empfinden Interaktion netzwerken konzeptionell Bestandteil Digitaler netzwerkdarstellungen übertragen Interaktion wodurch zuletzt Jenkins Kartenspiel entfalten didaktisch mehrweren Situational rahmen Kollektive Unterrichtsphase einsetzen abzielen abstrakt Unterrichtsgegenstand spielerisch Weise annähern Mensch Rolle Fokus stehen Versuch kumulativ assimilativ lernen initiieren wodurch begrenzt anwendungsorientierter wissen thematisch anwendungsorientierter wissen produzieren Illeris konkret Kenntnis Spielprinzip Supertrumpf unabhängig spielerisch Aktivierung unterschiedlich sinneskanäle einhergehend Diskussion Fachinhalte zielen Einleitung akkommodativ Transformativer lernprozesse Fachwisse relevant Kontext frei verfügen Wintersemester Universität Hamburg durchgeführte Seminar digital Literaturwissenschaft pädagogisch Praxis unterschiedlich Standardverfahr Werkzeug erproben gegenwärtig digital Literaturwissenschaft einsetzen Feld zunehmend insbesondere gymnasial Bereich relevant heutig zählen Effekt Dramenquartett lernerfolg studierende untersuchen eigens Testverfahren entwickeln Wissensstände Einsatz quartetts vergleichbar Verfahren setzen aufeinander aufbauend Phase vorbereitend befassen Lerngruppe schriftlich digital Netzwerkanalyse konsultieren Ausgangspunkt Erhebung stellen folglich gruppenspezifisch relativ homogener Wissensstand dar grundlegend Kenntnisse Methode digital Netzwerkanalyse beinhalten wissensstände beide Gruppe Einsatz quartetts erfassen Umfrage entwerfen Beginn seminars Einzelarbeit audience response System arsnova durchführen Umfrage adressieren jeweils Frage Anforderungsbereich i Reproduktionsleistung ii Transferleistung iii Reflexion Problemlösung anforderungsbereich entsprechend beinhalen freitextfragen Quizphase gesamt Testgruppe Kleingruppen eingeteilen Dramenquartett spielen Umfrage erfassen Wissensstand beide Gruppe Dramenquartett spielen Auswertung Testdurchlauf Teilnehmende durchführen verweisen lernförderlich Effekt Dramenquartett Rahmen quizrunde Frage Umfrage Frage beantworten hinaus verweisen Blick Freitextantwort spielerisch Zugang intrinsisch Motivation Seminarkontext hinaus Digitaler Netzwerkanalyse auseinandersetzen steigern erarbeitet Verfahren vergleichend Lernstandserhebung bewähren Seminar einsetzen einfluss spielerisch Wissensvermittlung Wissensstand untersuchen Projekt loten didaktisch Potenzial dh konzeptionell praktisch Tools tutorials didaktisch Pipeline verbinden Disseminationsstrategie fortext integrieren entwickelt Prototyp Konzept fachdidaktisch weiterentwicklungspotenzial bergen ermöglichen diverser adaptionen Transformation Hinblick Netzwerkanalyse literarisch Text Hinblick Methode Digital Humanitie Hinblick didaktisch Szenario Verzahnung Analog digital Ansatz lassen Grundlage netzwerkdaten romanen generisch entwerfen wobei ezlinavis Kombination gephi ermöglichen kollaborativ Erstellung Set denkbar selbstständig Erstellung zuletzt haptisch lerntyp ansprechen Weiterentwicklung didaktisch Engführung analog digitalem lassen Verzahnung Kartenspiel Repräsentation einzeln Dram Dracor vornehmen didaktisch Gesichtspunkt bieten Möglichkeit elemenen Skizzierte Pipeline einbauen lernend netzwerk erfinden händisch zeichnen Schritt digital Raum machend mittels Ezlinavis formal erfassen Projekt durchgeführt testlauf Sinn Diskussion didaktisch Potenziale sowohl Verzahnung Analog digital lehrmitteln anregen grundsätzlich Reflexion didaktisch Szenarie dienen spielerisch kreativ Übergang lebensweltlich vertraut Situation Abstraktion digitaler forschungsprozesse gestalten,"[('didaktisch', 0.28722196610651846), ('spielerisch', 0.25937852778820336), ('dramenquartett', 0.1935447718519379), ('ezlinavis', 0.17539791191635679), ('netzwerkanalyse', 0.16786981411591725), ('umfrage', 0.16336985773570784), ('digital', 0.13240674423187843), ('kartenspiel', 0.1315484339372676), ('autodidaktisch', 0.1315484339372676), ('wissensstand', 0.12252739330178089)]"
2020,DHd2020,114_final-MEYER_SICKENDIEK_Burkhard_Requirements_on_the_Punctuation_Re.xml,Requirements on the Punctuation Reconstruction for the Translation of Post-modern Poetry,"Burkhard Meyer-Sickendiek (Freie Universität Berlin, Deutschland); Timo Baumann (Universität Hamburg, Deutschland); Hussein Hussein (Freie Universität Berlin, Deutschland)","Postmodern Poetry, Translation, Punctuation Reconstruction","Übersetzung, Modellierung, Stilistische Analyse, Literatur, Ton, Text","Punctuation is an important and cohesive device in all kinds of written discourse. Standard marks used to separate words, phrases, clauses and sentences for the purpose of cohesion. Already [2][5][1] pointed out that through punctuation marks, one can signal different information structures in written language. Regarding the translation of texts, we use such marks to identify the ends of sentences, closely related sentences or clauses, etc. This is why missing punctuation burdens the translations and forces the translator to go over the text several times to understand its meaning [10]. Understanding the uses and functions of punctuation marks, therefore, is extremely important for translators, as their purpose is to clarify the meaning of a particular construction within a text. On the other hand, modern poetry often disregarded such punctuations. Ever since Italian Futurism around 1900 spoke of the ""parole in libert√†"", i.e. the liberation of words from grammatical and syntactic limitations, modern poetry has hardly used punctuation. This lack of punctuation makes analysis, but also translation, more difficult. The only way to reconstruct this punctuation is by listening to the poems, i.e. by subsequently identifying sentence boundaries. However, this lack of punctuation can be found very often in modern and post-modern poetry, so the challenge is to recognize the phrase boundaries. We contribute in the paper an application towards the problem of identifying left-out punctuation in post-modern poetry, by proving that only a very simple type of punctuation - the semicolon - is needed to improve machine translation. This simple punctuation refers to phrase boundaries, the so-called ""grammetrical units"", which Donald Wesling defined in his study ""The Scissors of Meter"" [11]. Such units must be identified in order to improve machine translation. The need for adding left-out punctuation becomes in case of creating machine translations obvious with regards to the poem ""bitte verlassen sie diesen raum"" (english: please leave this room) written by the German poet Nicolai Kobus [6] (Text A):  bitte verlassen sie diesen raum so wie sie ihn vorfinden möchten danke möchten sie diesen raum vorfinden wie sie ihn verlassen haben bitte räumen sie alles so vorgefundene als wären sie verlassen worden danke sie möchten doch nicht daß man sie so verlassen im raum vor findet bitte seien sie für einen so verlassen vorgefundenen raum dankbar [...]  please leave this room in the state in which you would like to find it thank you would you like to find this room in the state in which you have left it please clear out everything thus found as though you had been left thank you you would not like somebody to find you left abandoned in the room now would you please be grateful for a room a space found in such an abandoned state (...)  In the human translation or the target poem, made by Hales, there is just a little difference. This difference is caused by the missing punctuation. And it can basically be explained by the fact that Hales has chosen a different line arrangement. In terms of content, however, her translation is reproduced correctly. Since there is no specific translation system trained with poem data with/without punctuation (small amounts of training data), we used a Google machine translation (GMT) system [3]. When we compare this (human) translation with the GMT system, we recognize the difficulty of recognizing the sentence boundaries within the poem without punctuation (Text C):  please leave this room as they would like to find him Thank you for wanting this room find out how to leave him please have everything clear found as if they were Thank you you do not want that one So leave them in the room please find one for you leave found space (...)  Obviously, this machine translation (MT) becomes much better if we add the full punctuation marks to the source text, when listening to the audio of the poem (Text D):  please leave this room as you would like him to find Thank you. Do you want this room find how they leave him to have? Please clear everything up found as if they were been left. thank you Do not want that one So leave them in the room please, please be for one leave found space grateful. (...)  Punctuation is an essential aspect of poetry translations, as it is for discourse analysis in general [8]. Punctuation ""gives a semantic indication of the relationship between sentences and clauses, which may vary according to languages"", as well as to translations [4]. The philological scholar of our project annotated the punctuation information manually by using text and audio information in the 120 poems, focusing on the intonation of poets reading their poems. In order to clarify the question which type of punctuation has to be added, we inserted two kinds of punctuation in the source text. In a first step, we focused on six different punctuation marks: full stop (.), comma (,), semicolon (;), colon (:), exclamation mark (!), and question mark (?). In a second step, we simplified this insertion by reducing these six marks to a single semicolon. The human reference translations are compared with the automatic translation of GMT system without/with consideration of punctuation information. The experiment consists of three tasks based on the GMT system: The translation enhancement should be observable from improved translation quality scores. The results are calculated by bilingual evaluation understudy (BLEU) [9] score, which used for evaluating the quality of text by translation. The BLEU score of tasks 1, 2, and 3 are 0.256, 0.275, and 0.280, respectively. The results indicate that we need just one type of punctuation - semicolon - to improve the scoring for automatic translations of post-modern poetry. Every generic translation system is trained with data in which segments are defined by end points. It is astonishing that even the addition of a semicolon to segmental boundaries is sufficient to improve machine translation. This also explains the central problem: machine translation does not fail because of mixing up questions and statements, but because of mixing up segmental units and enjambements. In our future work, we plan to train a specific system on translating unpunctuated poetry in order to compare the results with manual translations. The fact that we add punctuation signs on the basis of oral representations of the poems is acceptable when it comes to audio poems, in which the oral representation is an essential part of the poem as a piece of art, closely connected to the written form.",en,punctuation important cohesive device kind write discourse standard mark separate word phrase clause sentence purpose cohesion point punctuation mark signal different information structure write language translation text use mark identify end sentence closely relate sentence clause etc miss punctuation burden translation force translator text time understand meaning understand use function punctuation mark extremely important translator purpose clarify meaning particular construction text hand modern poetry disregard punctuation italian futurism speak parole liberation word grammatical syntactic limitation modern poetry hardly punctuation lack punctuation make analysis translation difficult way reconstruct punctuation listen poem subsequently identify sentence boundary lack punctuation find modern post modern poetry challenge recognize phrase boundary contribute paper application problem identify left punctuation post modern poetry prove simple type punctuation semicolon need improve machine translation simple punctuation refer phrase boundary call grammetrical unit donald wesle define study scissor meter unit identify order improve machine translation need add left punctuation case create machine translation obvious regard poem bitte verlassen sie diesen raum english leave room write german poet nicolai kobus text bitte verlassen sie diesen raum wie sie ihn vorfinden möchten danke möchten sie diesen raum vorfinden wie sie ihn verlassen haben bitte räumen sie alle vorgefundene als wären sie verlassen worden danke sie möchten doch nicht daß man sie verlassen m raum vor findet bitte seien sie für einen verlassen vorgefundenen raum dankbar leave room state like find thank like find room state leave clear find leave thank like somebody find leave abandon room grateful room space find abandon state human translation target poem hale little difference difference cause missing punctuation basically explain fact hale choose different line arrangement term content translation reproduce correctly specific translation system train poem datum punctuation small amount training datum google machine translation gmt system compare human translation gmt system recognize difficulty recognize sentence boundary poem punctuation text c leave room like find thank want room find leave clear find thank want leave room find leave find space obviously machine translation mt well add punctuation mark source text listen audio poem text d leave room like find thank want room find leave clear find leave thank want leave room leave find space grateful punctuation essential aspect poetry translation discourse analysis general punctuation give semantic indication relationship sentence clause vary accord language translation philological scholar project annotate punctuation information manually text audio information poem focus intonation poet read poem order clarify question type punctuation add insert kind punctuation source text step focus different punctuation mark stop comma semicolon colon exclamation mark question mark second step simplify insertion reduce mark single semicolon human reference translation compare automatic translation gmt system consideration punctuation information experiment consist task base gmt system translation enhancement observable improved translation quality score result calculate bilingual evaluation understudy bleu score evaluate quality text translation bleu score task respectively result indicate need type punctuation semicolon improve scoring automatic translation post modern poetry generic translation system train datum segment define end point astonishing addition semicolon segmental boundary sufficient improve machine translation explain central problem machine translation fail mix question statement mix segmental unit enjambement future work plan train specific system translate unpunctuated poetry order compare result manual translation fact add punctuation sign basis oral representation poem acceptable come audio poem oral representation essential poem piece art closely connect write form,"[('punctuation', 0.5348972257556572), ('translation', 0.41723409712481085), ('leave', 0.25425648905536435), ('room', 0.22630267243508576), ('poem', 0.22630267243508576), ('sie', 0.2057297022137143), ('find', 0.20343230224643227), ('mark', 0.14174446822169695), ('poetry', 0.1172525876031986), ('thank', 0.11497297253581411)]"
2020,DHd2020,176_final-HODEL_Tobias_Maschinelles_Lernen_in_den_Geisteswissenschafte.xml,Maschinelles Lernen in den Geisteswissenschaften. Systemische und epistemologische Konsequenzen einer neuen Technologie,,"Maschinelles Lernen, Methodendiskussion, Epistemologie","Maschinelles Lernen, Methodendiskussion, Epistemologie","Seit einigen Jahren machen maschinelles Lernen und Überlegungen zu den Konsequenzen der dadurch entstehenden Artificial Intelligence Schlagzeilen. Von Spracherkennung über selbstfahrende Autos bis hin zu komplexen Spielen, maschinelles Lernen macht Computer in einzelnen Handlungsfeldern leistungsfähiger als Menschen. In der Theorie werden drei Formen ( Ein Ansatz, das sogenannte Ebenso werden andere unüberwachte und überwachte Verfahren des maschinellen Lernens eingesetzt, um Strukturen in großen Datenmengen zu finden und die Zusammenhänge zwischen den Daten und ihnen zugeordneten Kategorien zu erkennen (z.B. Verfahren zur Dimensionalitätsreduktion, Clustering, Klassifikation, Die Technologien, die auf die 1980er Jahre zurückgehen, wurden lange nur testweise eingesetzt, weil die Leistungsfähigkeit der Computersysteme nicht ausreichend war Das Panel hat zum Ziel, die Entwicklung und Anwendung des maschinellen Lernens mit einer Reflexion zu verbinden, die die Konsequenzen des Einsatzes aufzeigt. Dabei soll weder der häufig mit euphorischen Erwartungen verbundene Nutzen, noch unberechtigte Fundamentalabwehr befeuert werden. Vielmehr ist die differenzierte Beurteilung aus unterschiedlichen Blickwinkeln das Ziel. Im Panel zentral gesetzt werden epistemologische Fragen, die gerade aufgrund der imitierenden Natur des maschinellen Lernens entscheidend sind für die Aufbereitung von Trainingsmaterial oder die Implementierung in Entscheidungsprozesse. Gleichzeitig ähneln die Prozesse, die die Algorithmen übernehmen Vorgehensweisen geisteswissenschaftlicher Verstehensprozesse, die unter dem Begriff der ""Hermeneutik"" versammelt werden. Maschinelles Lernen hat entsprechend das Potential, als Methode unsere Zugänge und den Blick auf unser Material fundamental zu erweitern. Im Rahmen des Panels werden vier Protagonist*innen ihre Perspektive auf die Konsequenzen der Nutzung des maschinellen Lernens werfen:  Der Einsatz des maschinellen Lernens erfordert insbesondere bei der Erstellung neuer Algorithmen Fertigkeiten aus den Computerwissenschaften. Genau dieser Aufgabe stellt sich Sofia Ares Oliveira täglich, wenn sie als Ingenieurin selbständig neuronale Netze für dhlab der Eidgenössisch Technischen Hochschule in Lausanne (EPFL) erstellt. Im Rahmen des Panels wird sie verantwortlich sein für eine kurze Einführung in maschinelles Lernen. Aufgrund jahrelanger Beschäftigung mit der visuellen Analyse digitalisierter Dokumente, ist Ares Oliveira Spezialistin für den Aufbau und die Umsetzung neuronaler Netze zur semantischen Aufbereitung von Dokumenten (Segmentierung und Annotation). ""DH segment"" Die zwei Teilbeiträge von Sofia Ares Oliveira werden auf Englisch vorgetragen.  Anhand von Beispielen aus der jüngeren Forschung in den Computational Literary Studies (u.a. Underwood 2019 und So 2019) möchte der Beitrag aufzeigen, dass Verfahren des überwachten  In dem Beitrag werden verschiedene Möglichkeiten vorgestellt, maschinelle Lernverfahren für die Erforschung historischer Gattungen anhand des Textstils einzusetzen, insbesondere Clustering, Klassifikation und Topic Modeling  Im Rahmen von Projekt READ wurde mit der Einführung von maschinellen Lernverfahren die Erkennung von Handschriften und alten Drucken markant verbessert. Da die neuronalen Netze auf Trainingsmaterial basieren (also Die Panelisten werden kurz und thesenhaft ihre Perspektive auf die Technologie darlegen, dabei sollen sie u.a. zu drei Komplexen Stellung nehmen: Wo wird der Einsatz der Technologie in den Geisteswissenschaften neue Erkenntnisse bringen, welche Dokumente/Materialien/Daten eignen sich nicht für die Behandlung mit Fragen nach Erkenntnismöglichkeiten werden in diversen geisteswissenschaftlichen Disziplinen seit Jahrzehnten diskutiert. Die Nutzung von Algorithmen des maschinellen Lernens erfordern jedoch klare Aussagen zur untersuchten Materie, unabhängig davon, ob es sich um Neben der Angst vor dem Kontrollverlust und etwaigem Rückgang von Arbeitsplätzen oder der Überwachung von Menschenmassen, sind es nicht zuletzt Skandale zur Verletzung der Privatsphäre, die in den vergangenen Monaten zum Ruf nach der Regelung des Einsatzes der Technologie führten Im wissenschaftlichen Bereich sind es zurzeit vor allem die angewandte Informatik und Mathematik sowie die Computerlinguistik, die maschinelles Lernen in ihre Forschungen integrieren. In den Digital Humanities spielt die Technologie bislang von wenigen Zentren abgesehen eine untergeordnete Rolle. In absehbarer Zeit dürfte sie ein wichtiger Teil der Disziplin werden 'nicht nur im Recherche –, sondern auch im Auswertungs- und Schreibprozess. Insbesondere im Umgang mit digitalisierten Dokumenten, großen Datenmengen und Bildquellen können neuronale Netze ein wichtiges Mittel sein, um Daten zu finden, zu sortieren und auszuwerten. Die digitalen Geisteswissenschaften umfassen mit ihrem Methodenapparat sowohl komplexe Softwareentwicklung, als auch die Anwendung statistischer Modelle und das Erklären mit hermeneutischen Verfahren. Daher ist die Disziplin prädestiniert in den Diskussionen dieser gesellschaftsverändernden Technologie eine Vorreiterrolle einzunehmen.",de,Maschinelle lernen Überlegung Konsequenz entstehend artificial intelligence schlagzeilen Spracherkennung selbstfahrend Auto komplexen spielen maschinelles lernen Computer einzeln Handlungsfelder leistungsfähig Mensch Theorie Form Ansatz sogenannter unüberwacht überwacht Verfahren maschinell lernen einsetzen Struktur datenmenger finden zusammenhang daten zugeordnet Kategorie erkennen Verfahren Dimensionalitätsreduktion Clustering Klassifikation Technologie zurückgehen testweise einsetzen Leistungsfähigkeit computersystem ausreichend Panel Ziel Entwicklung Anwendung maschinell Lernen Reflexion verbinden Konsequenz einsatzes aufzeigt weder häufig euphorisch Erwartung verbunden nutzen unberechtigt Fundamentalabwehr befeuern vielmehr differenziert Beurteilung unterschiedlich blickwinkeln Ziel Panel zentral setzen epistemologisch Frage aufgrund imitierend Natur maschinell Lernen entscheidend Aufbereitung Trainingsmaterial Implementierung Entscheidungsprozesse gleichzeitig ähneln prozesse algorithm übernehmen vorgehensweisen geisteswissenschaftlicher verstehensprozesse Begriff Hermeneutik versammeln Maschinelles lernen entsprechend Potential Methode zugäng Blick Material Fundamental erweitern Rahmen Panel Perspektive Konsequenz Nutzung maschinell lernens werfen Einsatz maschinell lernens erfordern insbesondere Erstellung neu Algorithm Fertigkeit Computerwissenschaften genau Aufgabe stellen Sofia ares oliveira täglich Ingenieurin selbständig neuronal Netz Dhlab eidgenössisch technisch Hochschule Lausanne epfl erstellen Rahmen Panel verantwortlich kurz Einführung Maschinelles lernen aufgrund jahrelang Beschäftigung visuell Analyse digitalisiert dokumenen ares oliveira Spezialistin Aufbau Umsetzung neuronaler Netz semantisch Aufbereitung dokument Segmentierung Annotation dh Segment teilbeitrag Sofia ares oliveira englisch vortragen anhand Beispiel jung Forschung Computational literary Studies Underwood Beitrag aufzeigen Verfahren überwacht Beitrag verschieden Möglichkeit vorstellen maschinell lernverfahren Erforschung historisch Gattung anhand Textstil einsetzen insbesondere Clustering Klassifikation Topic Modeling Rahmen Projekt Read Einführung Maschinelle lernverfahren Erkennung handschrift alt drucken markant verbessern neuronal Netz Trainingsmaterial basieren panelister Thesenhaft Perspektive Technologie darlegen komplex Stellung nehmen Einsatz Technologie geisteswissenschaften erkenntnis bringen dokument Materialien daten eignen Behandlung Frage Erkenntnismöglichkeit diverser geisteswissenschaftlich disziplinen Jahrzehnt diskutieren Nutzung Algorithm maschinell Lernen erfordern klar Aussage untersucht Materie unabhängig Angst Kontrollverlust etwaig Rückgang Arbeitsplatz Überwachung Menschenmasse zuletzt Skandal Verletzung Privatsphäre Monat Ruf Regelung einsatzes Technologie führen wissenschaftlich Bereich Zurzeit angewandt Informatik Mathematik Computerlinguistik Maschinelles lernen Forschung integrieren Digital Humanitie spielen Technologie bislang weniger zentren absehen untergeordnet Rolle absehbar dürfen wichtig Disziplin Recherche schreibprozess insbesondere Umgang digitalisierter dokument datenmenger Bildquelle neuronal Netz wichtig daten finden sorti auszuwerten digital geisteswissenschaften umfassen Methodenapparat sowohl komplex Softwareentwicklung Anwendung statistisch Modell erklären hermeneutisch Verfahren Disziplin prädestinieren Diskussion gesellschaftsverändernd Technologie Vorreiterrolle einnehmen,"[('lernen', 0.29104594486183927), ('technologie', 0.2625878006949402), ('maschinell', 0.1975887208792467), ('maschinelles', 0.1883069363843034), ('ares', 0.18448496115845567), ('oliveira', 0.18448496115845567), ('netz', 0.17505853379662678), ('panel', 0.1523210672632863), ('neuronal', 0.14539661048519487), ('konsequenz', 0.12857899632981792)]"
2020,DHd2020,187_final-BURGHARDT_Manuel__The_Vectorian____Eine_parametrisierbare_Su.xml,"""The Vectorian"" 'Eine parametrisierbare Suchmaschine für intertextuelle Referenzen","Manuel Burghardt (Computational Humanities Group, Universität Leipzig); Bernhard Liebl (Computational Humanities Group, Universität Leipzig)","intertextuality, text reuse, information retrieval, NLP, word embeddings","Entdeckung, Programmierung, Inhaltsanalyse, Annotieren, Bewertung, Text","Shakespeare ist überall. Über alle zeitlichen und medialen Grenzen hinweg finden sich intertextuelle Bezüge auf die Werke von Shakespeare (vgl. Garber, 2005; Maxwell & Rumbold, 2018), der damit nicht nur der meistzitierte und meistgespielte Autor aller Zeiten, sondern auch der meistuntersuchte Autor der Welt ist (Taylor, 2016). Doch wenngleich in zahllosen Studien diverse Einzelaspekte von Shakespeares Werk aus Perspektive der Intertextualitätsforschung gründlich mittels By the Eine weitere methodische Einschränkung machen wir, indem wir Phänomene wie strukturelle Öhnlichkeit (Versmaß, Figurenkonstellation) und stilistische Öhnlichkeit Abb. 1 zeigt die Systemarchitektur der besagten Suchmaschine, die fortan als ""The Vectorian"" Kern des Abb. 2 zeigt das Frontend des Der Neben den beiden Der Die am besten bewerteten Ergebnisse sind zunächst viele Varianten nach dem Schema ""under the X tree"", bspw. ""under the Beim Parameter der POST-STSS, ein Parameter der unterschiedliche POS unterschiedlich stark gewichtet, ist in Kombination mit dem Im aktuellen Stadium dient der",de,Shakespeare überall zeitlich medial Grenze hinweg finden intertextuell bezüge Werk Shakespeare Garber maxwell Rumbold meistzitiert meistgespieln Autor Zeit meistuntersucht Autor Welt taylor wenngleich zahllos Studie diverser Einzelaspekt Shakespeare Werk Perspektive Intertextualitätsforschung gründlich mittels by the methodische Einschränkung phänomen strukturell Öhnlichkeit versmaß Figurenkonstellation stilistisch Öhnlichkeit abb zeigen Systemarchitektur besagt Suchmaschin fortan The vectorian Kern abb zeigen Frontend bewertet Ergebnis Variant Schema Under The x tree Under The Parameter Parameter unterschiedlich pos unterschiedlich stark wichten Kombination aktuell Stadium dienen,"[('shakespeare', 0.31313220329698377), ('the', 0.2959631951343761), ('under', 0.24238029760517418), ('öhnlichkeit', 0.19243208465045183), ('parameter', 0.14661499070413364), ('meistzitiert', 0.143415392344219), ('versmaß', 0.143415392344219), ('rumbold', 0.143415392344219), ('maxwell', 0.143415392344219), ('vectorian', 0.143415392344219)]"
2020,DHd2020,179_final-PRELL_Martin_Altbausanierung_mit_Niveau___die_Digitalisierun.xml,Altbausanierung mit Niveau 'die Digitalisierung gedruckter Editionen,Frederike Neuber (); Thorsten Schaßen (); Dominik Kasper (); Martina Gödel (); Thomas Stäcker (),"(Retro)Digitalisierung, Digitale Editionen, Druckeditionen, Best Practices","(Retro)Digitalisierung, Digitale Editionen, Druckeditionen, Best Practices","Während das Buch immer noch den höchsten Stellenwert in der geisteswissenschaftlichen Forschung im Allgemeinen besitzt, sind Editionen, die als Buch erscheinen, seit Jahren rückläufig (Eggert 2009). Bestehende Druckeditionen wirken mittlerweile neben ihren digitalen Nachfolgerinnen wie Relikte aus einer anderen Zeit. Ihr wissenschaftlicher Wert bleibt weitestgehend in den Grenzen des Buches verhaftet, während der digitale Editionskosmos wächst und perspektivisch zu einem dichten Wissensnetz wird. Um Druckeditionen besser verfügbar zu machen, sie mit anderen Editionen zu vernetzen, oder einen neuen Blick auf die Quellen zu ermöglichen, häufen sich in den letzten Jahren Unternehmungen zur Digitalisierung von Druckeditionen. Die mit der Digitalisierung von Editionen verbundenen, generalisierbaren Anforderungen und Implikationen sind, trotz ihrer unmittelbaren Relevanz für den Bereich der Digitalen Editionen, bisher noch nicht systematisch und projektübergreifend untersucht worden. Da bis dato zudem kaum auf die zahlreichen Erfahrungen bestehender Digitalisierungsprojekte zurückgegriffen werden kann, existiert sowohl bei laufenden als auch neuen Projekten stets die Gefahr, dass die organisatorischen, konzeptionellen und technischen Herausforderungen unterschätzt oder gar nicht erst erkannt werden. So entpuppen sich bspw. Projekte, die zunächst mit geringem Aufwand umsetzbar scheinen, nicht selten als Mammutaufgaben, die in Bezug auf Komplexität und Ressourcenbedarf die Anforderungen vergleichbarer Aus wissenschaftstheoretischer Perspektive stellt sich die Frage, welchen Stellenwert digitalisierte Editionen im Kosmos digitaler Editionstypen einnehmen können, wenn sie, wie Sahle formuliert, gar keine digitalen Editionen sind (Sahle 2013: 58ff.). In diesem Spannungsfeld gilt zu diskutieren, wie gedruckte editorische Leistungen der Vergangenheit unter den neuen medialen Bedingungen methodisch angemessen transformiert und für die Zukunft gesichert werden können. Das Panel richtet sich als Forum für den Erfahrungsaustausch und die Diskussion über theoretische und praktische Implikationen bei der Digitalisierung von Editionen sowohl an SoftwareentwicklerInnen aus den digitalen Geisteswissenschaften als auch an FachwissenschaftlerInnen. Vier Fragefelder sollen aus der Perspektive verschiedener Akteure im Panel diskutiert werden: Das Panel beginnt mit einer Einleitung durch die Moderatoren, der kurze Statements der Beitragenden mit Schwerpunkt auf bestimmte Fragefelder folgen und die mit einer These oder Fragestellung enden. Sie dienen als Problemaufriss und zur Identifizierung unterschiedlicher Positionierungen im Kontext der (Retro)Digitalisierung, über die im Anschluss debattiert wird. Es folgt eine Diskussion im Plenum. Darauf aufbauend werden die Beitragenden (sowie weitere Interessierte) im Nachgang der DHd2020 die Arbeit an einem Leitfaden aufnehmen, der sowohl technisch-praktische als auch methodische Fragen der Digitalisierung von Druckeditionen berücksichtigt und als Ausgangspunkt für einen weiterführenden Diskurs dient. Der Entwurf des Leitfadens soll online vorab veröffentlicht werden. Die diskutierte und finalisierte Fassung (in englischer und deutscher Sprache) wird dauerhaft zugänglich gemacht werden.  Im editionswissenschaftlichen Diskurs unterscheidet man im Spektrum der digitalen Editionstypen meist zwischen "" In der Umsetzung der Retrodigitalisierung können vor allem zwei paradigmatische Schwierigkeiten ausgemacht werden: Zum einen wird der mit dieser Transformation verbundene Aufwand unterschätzt. Zum anderen wird der gedruckten Vorlage allzu häufig ein sakrosankter Status zugeschrieben. Damit verbunden sind zahlreiche Fragen, die einer Klärung im jeweiligen Projektkontext bedürfen. Häufig unklar ist bspw. ob und wenn ja, in welcher Form in den Text eingegriffen werden darf; sei dies aus Gründen der Fehlerkorrektur oder der Angleichung an den aktuellen Forschungsstand. Zentraler Diskussionspunkt wird im Statement die Frage nach dem Einfluss des Layouts der Druckedition auf die digitale Präsentation sein. Ebenso wird in die Debatte der Aspekt eingebracht, dass die Retrodigitalisierung häufig als rein technischer Prozess ohne philologischen Anspruch und wissenschaftlichen Mehrwert bewertet wird (Ball et. al 2016; Sahle 2012) und die beteiligten digital affinen WissenschaftlerInnen zum Dienstleister marginalisiert werden. Dies wird auch durch Missverständnisse bedingt, die mit dem Eingang neuer Terminologie in das Editionsprojekt aufgrund der  Auf dem Weg vom Druck zur digitalisierten oder gar digitalen Edition können unterschiedliche Welches Vorgehen Anwendung findet, wird unter anderem dadurch bestimmt, welche Erwartungen und Mentalitäten das Projekt prägen. Unterschiedliche  Ein Projekt Projekten, die in der Planungs- oder  Bibliotheken bewahren gedruckte Editionen. Mit der Durchsetzung des digitalen Paradigmas werden diese selbst Gegenstand editorischer Prozesse. Nicht nur die Edition, sondern auch der digitale Transformationsprozess stellt eine neue erschließende Dimension dar: Frederike Neuber ist wissenschaftliche Mitarbeiterin bei der TELOTA-Initiative der Berlin-Brandenburgischen Akademie der Wissenschaften. Sie ist Mitherausgeberin von ""Jean Paul - Sämtliche Briefe digital"" und im Institut für Dokumentologie und Editorik u. a. als Torsten Schaßan ist wissenschaftlicher Mitarbeiter an der Herzog August Bibliothek Wolfenbüttel. Er betreut dort den Bereich Digitale Editionen. An der HAB wurden mehrere Retrodigitalisierungsvorhaben umgesetzt, darunter die Briefe der Fruchtbringenden Gesellschaft und ""Controversia et Confessio"". Dominik Kasper ist wissenschaftlicher Mitarbeiter an der Akademie der Wissenschaften und der Literatur Mainz. Erfahrungen mit Retrodigitalisierung konnte er in den Projekten ""Deutsche Inschriften Online"" und ""PROPYLÖEN 'Goethes Biographica"" (Leiter der Frankfurter Arbeitsstelle) sammeln. Martina Gödel ist seit 2011 freiberuflich unter dem Namen Thomas Stäcker ( Max Grüntgens (Moderation) ist wissenschaftlicher Mitarbeiter an der Akademie der Wissenschaften und der Literatur Mainz. Erfahrungen mit Retrodigitalisierung konnte er in den Projekten ""Deutsche Inschriften Online"" (Leiter der Mainzer Arbeitsstelle) und ""PROPYLÖEN 'Goethes Biographica"" sammeln. Martin Prell (Moderation) ist DH-Koordinator der PROPYLÖEN-Edition (Goethe- und Schiller-Archiv Weimar) und des ""Editionenportal Thüringen"" (Universität Jena). Er gibt unter anderem die Briefe Erdmuthe Benignas von Reuß-Ebersdorf heraus.",de,Buch hoch stellenwert geisteswissenschaftlich Forschung besitzen editionen Buch erscheinen rückläufig eggern bestehend Druckeditione wirken mittlerweile digital Nachfolgerinn Relikt wissenschaftlich Wert bleiben weitestgehend Grenze Buch verhaften digital editionskosmos wachsen perspektivisch dicht Wissensnetz druckeditioner verfügbar editionen vernetzen Blick quellen ermöglichen häufen letzter Unternehmung Digitalisierung druckeditionen Digitalisierung edition verbunden Generalisierbare anforderungen implikation trotz unmittelbar Relevanz Bereich digital editionen systematisch projektübergreifend untersuchen dato zudem zahlreich erfahrung bestehend digitalisierungsprojekte zurückgegriffen existieren sowohl laufend Projekt stets Gefahr organisatorische konzeptionell technisch Herausforderung unterschätzen erkennen entpuppen projeken gering Aufwand umsetzbar scheinen selten mammutaufgaben Bezug Komplexität ressourcenbedarf anforderung vergleichbar wissenschaftstheoretisch Perspektive stellen Frage stellenwern Digitalisiert editionen Kosmos Digitaler editionstype einnehmen sahl formulieren digital editionen sahl Spannungsfeld gelten diskutieren gedruckt editorisch Leistung Vergangenheit medial Bedingung methodisch angemessen transformieren Zukunft sichern Panel richten Forum erfahrungsaustausch Diskussion theoretisch praktisch Implikation Digitalisierung Edition sowohl Softwareentwicklerinn digital geisteswissenschaften fachwissenschaftlerinnen fragefeld Perspektive verschieden Akteur Panel diskutieren Panel beginnen Einleitung Moderator kurz Statements Beitragende Schwerpunkt bestimmt fragefelder Folge These Fragestellung enden dienen Problemaufriss Identifizierung unterschiedlich Positionierung Kontext Retro Digitalisierung Anschluss debattieren folgen Diskussion Plenum aufbauend Beitragende Interessiert Nachgang Arbeit leitfad aufnehmen sowohl methodisch Frage Digitalisierung Druckedition berücksichtigen Ausgangspunkt weiterführend Diskurs dienen Entwurf Leitfaden Online vorab veröffentlichen diskutiert finalisiert Fassung englisch deutsch Sprache dauerhaft zugänglich editionswissenschaftlich Diskurs unterscheiden Spektrum digital Editionstype meist Umsetzung Retrodigitalisierung paradigmatisch Schwierigkeit ausmachen Transformation verbunden Aufwand unterschätzen gedruckt Vorlage allzu häufig Sakrosankter Status zuschreiben verbinden zahlreich Frage Klärung jeweilig Projektkontext bedürfen häufig unklar Form Text eingreifen Grund Fehlerkorrektur Angleichung aktuell Forschungsstand Zentraler diskussionspunken Statement Frage einfluss layouts Druckedition digital Präsentation Debatte Aspekt einbringen Retrodigitalisierung häufig rein technisch Prozess philologisch Anspruch wissenschaftlich mehrweren bewerten Ball et -- sahl beteiligt Digital affin wissenschaftlerinnen Dienstleister marginalisieren Missverständnisse bedingt Eingang neu Terminologie Editionsprojekt aufgrund Weg Druck digitalisiert digital Edition unterschiedlich vorgeh Anwendung finden bestimmen Erwartung Mentalität Projekt prägen unterschiedlich Projekt projeken bibliothek bewahr gedruckte editionen Durchsetzung digital Paradigmas Gegenstand editorisch Prozesse Edition digital Transformationsprozess stellen erschließend Dimension dar frederike Neuber wissenschaftlich Mitarbeiterin Akademie Wissenschaft Mitherausgeberin Jean Paul sämtlicher Brief Digital Institut Dokumentologie Editorik torst Schaßan wissenschaftlich Mitarbeiter Herzog August Bibliothek wolfenbüttel betreuen Bereich digital editionen hab mehrere Retrodigitalisierungsvorhabe umsetzen Brief fruchtbringend Gesellschaft Controversia et confessio Dominik Kasper wissenschaftlich Mitarbeiter Akademie Wissenschaft Literatur Mainz Erfahrung Retrodigitalisierung projekt deutsch inschrift online propylöen goeth Biographica Leiter Frankfurter Arbeitsstelle sammeln Martina Gödel freiberuflich Name Thomas Stäcker max Grüntgens Moderation wissenschaftlich Mitarbeiter Akademie Wissenschaft Literatur Mainz Erfahrung Retrodigitalisierung projekt deutsch inschrift online Leiter mainzer Arbeitsstelle Propylöen goeth Biographica sammeln Martin Prell Moderation Weimar Editionenportal Thüringen Universität Jena Brief erdmuthe Benigna heraus,"[('editionen', 0.25363244630108317), ('retrodigitalisierung', 0.18255199840194056), ('digitalisierung', 0.18116603307220228), ('digital', 0.17667000948430275), ('wissenschaftlich', 0.17209274652773932), ('sahl', 0.16202286282783646), ('mitarbeiter', 0.12403461804585306), ('brief', 0.12080704177116833), ('akademie', 0.11073038031196943), ('edition', 0.10942398720254434)]"
2020,DHd2020,226_final-SCHUMACHER_Mareike_m_w_Figurengender_zwischen_Stereotypisier.xml,m*w Figurengender zwischen Stereotypisierung und literarischen und theoretischen Spielräumen. Genderstereotype und -bewertungen in der Literatur des 19. Jahrhunderts,"Mareike Schumacher (University of Hamburg, Deutschland); Marie Flüh (University of Hamburg, Deutschland)","Gendertheorie, Bewertungsanalyse, digitale Annotation","Inhaltsanalyse, Modellierung, Annotieren, Kontextsetzung, Theoretisierung, Visualisierung","Während in den Digital Humanities bereits erste korpusbasierte Analysen von Figurengender in der Literatur vorgelegt wurden (Underwood 2019: 111 ff), wird in den Kulturwissenschaften zu diesem Thema selten korpusbasiert gearbeitet. Stattdessen sind Theorien zur Genderthematik häufig philosophisch-soziologisch (z.¬†B. bei Beauvoir oder Bourdieu), diskurstheoretisch (z.¬†B. Foucault) oder dekonstruktivistisch (z.¬†B. Butler) Die leitende Fragestellung des m*w-Projektes ist: Wie werden Genderrollen in der Literatur des 19. Jahrhunderts dargestellt und bewertet? Um uns dieser Fragestellung zu nähern, erstellen wir mithilfe einer Auswahl theoretischer Ansätze ein Modell. Dieses nutzen wir, um im überwachten Machine-Learning-Verfahren der Named Entity Recognition (NER) ein Tool darauf zu trainieren, Figuren und ihre Genderzuschreibungen automatisch zu erkennen. Die Ergebnisse des NER-Verfahrens nutzen wir um unser Modell weiter zu schärfen. Im abschließenden Close Reading der Novellen werden schließlich Genderbeschreibungen und -bewertungen analysiert und mit dem Modell abgeglichen. Grundsätzlich nehmen wir sowohl Modell als auch Korpus als variable bzw. dynamische Größen wahr, die sich für den Praxistest der Operationalisierung theoriebasierter Modelle zur Erforschung literarischer Genderrollen eignen.  Davon ausgehend, dass sich sowohl in der Theorie als auch in Erzähltexten Spielräume als Zwischenräume auftun, die dadurch sichtbar werden, dass sie sich von den sie umgebenden normierten Räumen unterscheiden Um entscheiden zu können, ob Eigenschaften und Handlungen von Figuren stereotypen Rollenbildern zugeschrieben werden können oder nicht, haben wir zunächst möglichst viele Rollenbilder in unser Modell integriert. Jede dieser Rollen kann sowohl im Sein (Gender-Identität) als auch im Handeln (Gender-Performanz) von Figuren verankert sein. Die sechs in Abb. 1 abgebildeten Oberkategorien von Eigenschaften sind ebenfalls der Theorie entnommen. Allerdings wurden hier die Beschreibungen stärker kondensiert, um die zahlreichen genannten Einzeleigenschaften für die digitale Annotation besser handhabbar zu machen. Alle drei eingesetzten Methoden - NER, Annotation von Stereotypen und Emotionsanalyse haben sich im ersten Proof of Concept als fruchtbar für die digitale Erforschung von Figurengender erwiesen. Besonders eklatante Zwischenergebnisse fassen wir im Folgenden zusammen. Im Laufe des NER-Trainingsprozesses erwies sich die Kategorie ""divers"" nicht als dem Korpus angemessen, weshalb wir die NER-Kategorien auf ""männlich"", ""weiblich"", ""genderneutral"" Um Hinweise auf eine mögliche Verzerrung durch die Zusammensetzung des Trainingskorpus zu bekommen, haben wir einen dritten Testtext hinzu genommen   (  Anschließend wurden die NER-Kategorien mit Unterkategorien versehen, die den Benennungen der Genderrollen des Modells entsprechen (Abb. 5). Sofern es keine adäquate, in der Theorie erwähnte Rolle oder Eigenschaft gab, wurden zusätzliche Annotationskategorien erstellt und den Oberkategorien ""unsortierte Rollen"" und ""unsortierte Eigenschaften"" zugewiesen. Die digitale Annotation des ersten Beispieltextes zeigt, dass stereotype Beschreibungen vor allem zu Beginn der Erzählung häufig und somit besonders für die Etablierung der Figuren von Bedeutung sind. Stereotype Eigenschaften sind hier zwar divers, für männliche und weibliche Figuren gibt es aber jeweils einige wenige, die quantitativ herausstechen. Für weibliche Figuren ist das vor allem Öußerlichkeit/Schönheit für männliche sind es Körperlichkeit/Trinkfestigkeit und Wirksamkeit/Herrschaft. Unsortierte Rollen und Eigenschaften sind zumeist in einem binären Rollensystem verankert und dennoch werden in dieser Novelle zum Teil Stereotype aufgebrochen. Dies wird hauptsächlich durch die Zuschreibung einzelner Eigenschaften zu einer Figur eines Genders erreicht, die in der theoretischen Literatur eher dem Stereotyp des anderen zugeschrieben werden (ausführlichere Auswertung des ersten Beispieltextes in Schumacher 2020 [3]).  Die Emotionen werden in den allermeisten Fällen verbal (407 Annotationen) von den Figuren ausgedrückt. Über die Veränderung des körperlichen Zustands (19 Annotationen), und nonverbal (107 Annotationen) werden Emotionen vergleichsweise selten repräsentiert. Die Auswertung der Properties ergibt genderspezifische Emotionsinformationen (s. Abb. 7 und 8). Weibliche Figuren treten ängstlicher auf als männliche. Männliche Figuren reagieren häufiger zornig als weibliche. Sie empfinden außerdem häufiger Ekel 'hier meistens im Sinne von Abneigung 'als weibliche Figuren. Diese leiden häufiger unter gedrückter Stimmung und empfinden deutlich häufiger negative Basisemotionen als männliche Figuren. Diese treten im Schnitt fröhlicher auf als die weiblichen Figuren. Die männlichen Figuren zeigen häufiger positive Basisemotionen als die weiblichen Figuren und auch die positive Basisemotion LIEBE überwiegt seitens der männlichen Figuren. Scham 'als Unterkategorie der Problemfälle 'ist seitens der weiblichen Figuren deutlich stärker ausgeprägt (für eine ausführlichere Auswertung des ersten Beispieltextes vgl. Flüh 2020).  ",de,Digital Humanitie Korpusbasiert analysen Figurengender Literatur vorlegen underwood ff Kulturwissenschaft Thema selten korpusbasieren arbeiten stattdessen Theorie Genderthematik häufig Beauvoir bourdieu diskurstheoretisch Foucault dekonstruktivistisch Butler leitend Fragestellung genderrollen Literatur Jahrhundert darstellen bewerten Fragestellung nähern erstellen Mithilfe Auswahl theoretisch Ansatz Modell nutzen überwacht named entity Recognition ner Tool trainieren Figur genderzuschreibungen automatisch erkennen Ergebnis nutzen Modell schärfen abschließend Close Reading Novell schließlich genderbeschreibung analysieren Modell abgeglichen grundsätzlich nehmen sowohl Modell Korpus variabel dynamisch Größe Praxistest Operationalisierung theoriebasiert Modell Erforschung literarisch genderrollen eignen ausgehend sowohl Theorie erzähltext spielräume zwischenräume auftun sichtbar umgebend normiert Räume unterscheiden entscheiden eigenschaften Handlung figuren Stereotype rollenbildern zuschreiben möglichst Rollenbilder Modell integrieren Rolle sowohl Handeln Figur verankern abb abgebildeten Oberkategorien Eigenschaft ebenfalls Theorie entnehmen Beschreibung stark kondensieren zahlreich genannt einzeleigenschaften digital Annotation handhabbar eingesetzt Methode ner Annotation Stereotype Emotionsanalyse proof of Concept fruchtbar digital Erforschung figurengend erweisen eklatant zwischenergebnisse fassen folgend Lauf erweisen Kategorie divers Korpus angemessen weshalb männlich weiblich genderneutral Hinweis möglich Verzerrung Zusammensetzung Trainingskorpus bekommen Testtext hinzu nehmen anschließend unterkategorien versehen Benennung genderrollen Modell entsprechen abb sofern adäquat Theorie erwähnt Rolle Eigenschaft zusätzlich Annotationskategorie erstellen oberkategorien unsortiert rollen unsortiert eigenschaft zugewiesen digital Annotation Beispieltext zeigen Stereotype beschreibungen Beginn Erzählung häufig somit Etablierung Figur Bedeutung Stereotype eigenschaften divers männlich weiblich Figur jeweils quantitativ herausstechen weiblich Figur Öußerlichkeit Schönheit männliche Körperlichkeit Trinkfestigkeit Wirksamkeit Herrschaft unsortiert Rolle eigenschaften zumeist binären rollensyst verankern dennoch Novelle Stereotype aufbrechen hauptsächlich Zuschreibung einzeln eigenschaften Figur Gender erreichen theoretisch Literatur eher Stereotyp zuschreiben ausführlich Auswertung Beispieltext Schumacher Emotion allermeister Fall verbal Annotation Figur ausdrücken Veränderung Körperlichen zustands Annotation nonverbal annotation Emotion vergleichsweise selten repräsentieren Auswertung Properties ergeben genderspezifisch Emotionsinformation abb weiblich Figur treten ängstlich Männliche männlich Figur reagieren häufig zornig weibliche empfinden häufig Ekel meistens Sinn Abneigung weiblich Figur leiden häufig gedrückt Stimmung empfinden deutlich häufig negativ Basisemotione männlich Figur treten Schnitt Fröhlicher weiblich Figur männlich Figur zeigen häufig positiv Basisemotion weiblich Figur positiv Basisemotion lieben überwiegen seitens männlich Figur Scham Unterkategorie problemfäll seitens weiblich Figur deutlich stark ausgeprägt ausführlich Auswertung beispieltext Flüh,"[('figur', 0.3628967748236127), ('weiblich', 0.28353127759849867), ('stereotype', 0.24182003364941035), ('männlich', 0.2157886305796567), ('unsortiert', 0.17170066385023902), ('modell', 0.1637154404622947), ('genderrollen', 0.15992615145679093), ('häufig', 0.14269968939911606), ('beispieltext', 0.13979748957003368), ('theorie', 0.1267516052550142)]"
2020,DHd2020,143_final-HORSTMANN_Jan_Interpretationsspielräume__Undogmatisches_Anno.xml,Interpretationsspielräume. Undogmatisches Annotieren literarischer Texte in CATMA 6,"Jan Horstmann (Universität Hamburg, Deutschland); Janina Jacke (Universität Hamburg, Deutschland)","Annotation, Interpretation, Kollaboration","Inhaltsanalyse, Annotieren, Theoretisierung, Literatur, Metadaten, Forschungsprozess","Werden Da DH-Tools möglichst an disziplinspezifische geisteswissenschaftliche Theorien, Methoden und Praktiken rückgebunden werden sollen (vgl. Sahle 2015), sollten digitale Zugänge zur Literaturerforschung diese Spielräume berücksichtigen. Undogmatisches Annotieren mit CATMA 6 ( Wie CATMA 6 geisteswissenschaftlichen Anforderungen (und damit den genannten Spielräumen) gerecht zu werden sucht, soll anhand von vier Funktionskomplexen demonstriert werden: unterschiedlichen Annotationsmodi, Mehrfachannotation, Metaannotation und kollaborativem Annotieren. Dieser Beitrag kann somit auch als exemplarische Umsetzung der Forderung verstanden werden, einen Brückenschlag zwischen DH- und traditionell-geisteswissenschaftlichen Methoden zu schaffen. Die Interpretation literarischer Texte wird gemeinhin als eine Kernaufgabe literaturwissenschaftlichen Arbeitens betrachtet. Regeln der Textinterpretationen oder Gütekriterien für Interpretationshypothesen sind aufgrund der Theorie- und Methodenvielfalt nicht eindeutig festgelegt. Zwei Überzeugungen scheinen jedoch über unterschiedliche Ausrichtungen hinweg in der literaturwissenschaftlichen Forschungsgemeinschaft (relativ) allgemein anerkannt: (a) Trotz der Pluralität zulässiger Interpretationen gibt es auch Interpretationen, die einem Text Angesichts dieser Sachlage lässt sich leicht erkennen, warum die Methode der Annotation im Zusammenhang mit Interpretationen fruchtbar angewandt werden kann: Der Prozess des Annotierens geht mit textnahem Arbeiten einher, und Annotationen werden grundsätzlich bestimmten Textstellen zugewiesen. Damit ist Annotation besonders geeignet für kleinschrittige textdeskriptive bzw. -analytische Vorhaben, die dann eine Grundlage für Interpretationen liefern können. Interpretationen selbst werden dann 'auch in DH-Projekten 'häufig in Form zusammenhängender Texte erstellt, innerhalb derer auf textanalytische Ergebnisse (Annotationen) Bezug genommen wird. Hier kann Annotation demnach als Werkzeug von Interpretation gelten. Es kann allerdings durchaus sinnvoll sein, auch Interpretationshypothesen selbst im Prozess des Annotierens zu entwickeln und als Annotationen festzuhalten. Denn zum einen verschwimmt sogar bei gemeinhin als deskriptiv geltenden Operationen wie der narratologischen Analyse otf die Grenze zu (inhaltsspezifizierender) Interpretation. Wie groß die Spielräume im Rahmen von Annotation sein müssen 'sowohl hinsichtlich des Grads der Formalisiertheit der Annotation (vgl. 2.2) als auch der Einigkeit unter verschiedenen Annotator√Ønnen –, hängt entsprechend davon ab, ob es sich um deskriptiv-analytische Annotationen handelt, die als Vorarbeit für Interpretationen fungieren, oder um genuin interpretative Annotationen (vgl. 3).  Viele Annotationstools (bisher auch CATMA) ermöglichen ausschließlich die Annotation mithilfe von Tagsets, also hierarchisch gegliederten Kategorien. Dafür müssen Forschende allerdings schon ein formalisiertes Kategoriensystem haben, mit dem sie den Text untersuchen wollen. Annotation sollte abern auch zur noch unstrukturierten Textexploration nutzbar sein. Zudem sollte auch Interpretation selbst mithilfe von Annotation ermöglicht werden, was aber meist nicht (allein) unter Nutzung von Kategorien umsetzbar ist. In CATMA 6 werden deshalb folgende Annotationsmodi implementiert: (1) (2) (3) Damit ein strukturiertes Annotieren mit Tagsets nicht nur im Rahmen heuristischer Text Neben freiem Generieren und iterativem Überarbeiten von Tagsets ist eine Bedingung für die Nutzung tagsetbasierten Annotierens als interpretationsunterstützender Methode die Möglichkeit der (diversen oder sogar widersprüchlichen) Mehrfachannotation derselben Textstelle. Dies trägt zum einen dem Umstand Rechnung, dass ein Text aus unterschiedlichen Perspektiven untersucht werden kann: Beispielsweise kann eine Textpassage zugleich intermediale Bezüge enthalten und Passagen literarischer Texte sind zudem häufig interpretationsoffen, weshalb unterschiedliche, teilweise auch widersprüchliche Interpretationen gleichermaßen gültig sein können. So mögen etwa (inkompatible) Thesen darüber, wer/was durch eine im Text auftretende Figur verkörpert werden soll, plausibel sein. Da bei der Interpretation von Literatur die Spielräume nicht grenzenlos sind und nach diversen Regeln gespielt werden muss (vgl. bspw. Jannidis 2003), benötigt eine Annotationsumgebung, die taxonomiegestütztes Interpretieren ermöglicht, auch Optionen zur Einordnung, Erläuterung und Aushandlung von Interpretationen. Diese Rolle erfüllen in CATMA 6 Metaannotationen, die wiederum taxonomiebasiert (als Annotationskategorien lassen sich mit Properties versehen, denen pro Annotation feste oder ad hoc vergebbare Values zugeordnet werden können, um Annotationen genauer zu qualifizieren. Die gleiche Funktion erfüllen freitextbasierte Metaannotationen, die erstmalig in CATMA 6 nutzbar sind. Ob Metaannotationen als freie Kommentare oder auf Taxonomiebasis zur Anwendung kommen, kann vom Grad der theoretischen Ausarbeitung der genutzten Interpretationsheuristik abhängen oder eine Frage des Anwendungskontextes bzw. der persönlich präferierten Arbeitsweise sein. In technischer Hinsicht sind Annotationen gemäß dem Web Annotation Data Model Während Metaannotationen eingesetzt werden können, um einem Tagset Analysekategorien auf einer horizontalen Gliederungsebene hinzuzufügen, Kollaboratives Annotieren ist in der Linguistik eine etablierte Methode, um Annotationsentscheidungen abzusichern (vgl. Wissler et al. 2014). In der Literaturwissenschaft ist es noch wenig etabliert (vgl. Röcke 2016); auch ist ein behutsameres Vorgehen angebracht, wenn es darum geht, Annotations- bzw. Interpretationsentscheidungen abzusichern. Als fruchtbar hat sich ein iteratives Vorgehen erwiesen, bei dem Forscher√Ønnen diskrepant annotierte Passagen diskutieren, um Gründe für unterschiedliche Entscheidungen herauszustellen (vgl. Gius & Jacke 2017). Durch eine gründliche Metaannotation kann dieser Workflow verschlankt werden. Je nach Grund kann abgewogen werden, ob es sich um eine legitime Uneinigkeit handelt. So können Interpretationsspielräume bei kollaborativem Annotieren zugleich gewahrt und sinnvoll eingegrenzt werden. Kollaboration wird in CATMA ermöglicht durch die mit GitLab per API verknüpfte projektzentrierte Systemarchitektur (vgl. Fig.¬†2). Eine GitLab-Group Das neue Rollen- und Rechtesystem erlaubt somit eine differenzierte Festlegung von Spielräumen auch bei der Konzeption eines kollaborativen Annotationsprojekts. Um einem Projekt weitere Mitglieder hinzuzufügen bietet CATMA 6 zwei Möglichkeiten: (1) das manuelle Hinzufügen einzelner CATMA-Nutzer√Ønnen durch Eingabe des CATMA-Usernames. Diese Funktion bietet sich für den asynchronen Arbeitsmodus etwa in Forschungsprojekten an. (2) Die Die Verknüpfung mit GitLab bietet zudem Versionierungs- und damit einhergehende Konfliktlösungsfunktionen. Denn während CATMA beispielsweise inhaltlich widersprüchliche Mehrfachannotationen erlaubt, stellt das zeilenbasiert arbeitende Versionierungssystem Git einen Konflikt fest, wenn Nutzer√Ønnen inkompatible Önderungen in ihren Projekten vorgenommen haben, die dieselbe Zeile des zugrundeliegenden Codes betreffen. Nehmen wir beispielsweise an, in einem kollaborativen Forschungsprojekt wurde dieselbe Metaannotation von Nutzer√Ønnen 1 und 2 je unterschiedlich geändert. Sobald Nutzer√Øn 2 die Arbeit mit dem Team synchronisiert, meldet CATMA einen Konflikt, anstatt eigenmächtig einer Version den Vorzug zu geben (siehe Fig. 3). Dabei werden eigene und fremde Version nebeneinandergestellt und Nutzer√Øn 2 kann sich informiert für eine Version entscheiden, ohne tiefergehende Kenntnisse über die zugrunde liegenden technischen GitLab-Prozesse haben zu müssen. Diese Funktionalität unterstützt den 'im kollaborativen Modus noch stärker im Vordergrund stehenden 'diskursiven Aushandlungsprozess von Annotation und Interpretation und reagiert somit auf die Forderung nach flexiblen disziplinspezifischen Arbeitsabläufen. Das Schaubild (vgl. Fig. 4) verdeutlicht die identifizierten literaturwissenschaftlichen Spielräume (linke Spalte) und die daraus erwachsenden generellen Anforderungen an digitale Arbeitsumgebungen (mittlere Spalte). Wie CATMA 6 diese Anforderungen konkret umsetzt, findet sich in der rechten Spalte.",de,möglichst disziplinspezifisch geisteswissenschaftlich Theorie Methode praktik Rückgebund sahlen digital zugänge Literaturerforschung Spielraum berücksichtigen undogmatisch Annotiere Catma catma geisteswissenschaftlichen Anforderung genannt Spielräume gerecht suchen anhand Funktionskomplexe demonstrieren unterschiedlich Annotationsmodi Mehrfachannotation Metaannotation kollaborativ Annotiere Beitrag somit exemplarisch Umsetzung Forderung verstehen Brückenschlag Methode schaffen Interpretation literarisch Text gemeinhin Kernaufgabe literaturwissenschaftlich Arbeiten betrachten Regel Textinterpretation Gütekriterie interpretationshypothese aufgrund Methodenvielfalt eindeutig festlegen überzeugungen scheinen unterschiedlich Ausrichtung hinweg literaturwissenschaftlich Forschungsgemeinschaft relativ allgemein anerkennen trotz Pluralität zulässig Interpretation Interpretation Text angesichts Sachlage lässen erkennen Methode Annotation Zusammenhang Interpretation fruchtbar anwenden Prozess Annotieren textnah arbeiten einher annotatio grundsätzlich bestimmt Textstell zuweisen Annotation geeignet kleinschrittig textdeskriptiv vorhaben Grundlage Interpretation liefern interpretationen häufig Form zusammenhängend Text erstellen innerhalb der textanalytisch ergebnisse annotation Bezug nehmen Annotation demnach Werkzeug Interpretation gelten sinnvoll interpretationshypothesen Prozess Annotieren entwickeln annotation festhalten verschwimmen sogar gemeinhin deskriptiv geltend operationen narratologisch Analyse otf Grenze inhaltsspezifizierend Interpretation Spielraum Rahmen Annotation sowohl hinsichtlich Grad formalisiertheit Annotation Einigkeit verschieden hängen entsprechend annotationen handeln Vorarbeit Interpretation fungieren genuin interpretativ annotation annotationstool catma ermöglichen ausschließlich Annotation Mithilfe Tagset hierarchisch gegliedert Kategorie forschend formalisiert Kategoriensystem Text untersuchen Annotation abern unstrukturiert Textexploration nutzbar zudem Interpretation Mithilfe Annotation ermöglichen meist Nutzung Kategorie umsetzbar Catma folgend Annotationsmodi implementieren strukturiert annotieren Tagset Rahmen heuristisch Text frei generieren iterativ überarbeiten Tagset Bedingung Nutzung Tagsetbasiert Annotieren interpretationsunterstützend Methode Möglichkeit diverser sogar widersprüchlich Mehrfachannotation Textstelle tragen Umstand Rechnung Text unterschiedlich Perspektive untersuchen beispielsweise Textpassage intermedial bezüge enthalten passag literarisch Text zudem häufig interpretationsoffen weshalb unterschiedlich teilweise widersprüchlich Interpretation gleichermaßen gültig Inkompatible thesen Text auftretend Figur verkörpern plausibel Interpretation Literatur Spielraum grenzenlos diverser Regel spielen Jannidis benötigen Annotationsumgebung taxonomiegestütztes interpretieren ermöglichen Optionen Einordnung Erläuterung Aushandlung Interpretation Rolle erfüllen Catma metaannotationen wiederum taxonomiebasieren Annotationskategorie lassen properties versehen pro Annotation feste ad Hoc vergebbar Values zuordnen Annotation genau qualifizieren gleich Funktion erfüllen Freitextbasierte metaannotationen erstmalig Catma nutzbar metaannotationen frei Kommentar Taxonomiebasis Anwendung Grad theoretisch Ausarbeitung genutzt Interpretationsheuristik abhängen Frage anwendungskontext persönlich Präferiert arbeitsweise technisch Hinsicht Annotation gemäß Web Annotation Data Model Metaannotatione einsetzen Tagset Analysekategorien horizontal Gliederungsebene Hinzuzufüge kollaborativer annotieren Linguistik etabliert Methode annotationsentscheidung Abzusicher Wissler Et Literaturwissenschaft etablieren Röcke behutsamer vorgehen angebrachen Interpretationsentscheidung absichern fruchtbar iterativ vorgehen erweisen diskrepant annotiert Passag diskutieren gründe unterschiedlich Entscheidung herauszustell Gius Jacke gründlich Metaannotation Workflow verschlanken Grund abgewogen legitim Uneinigkeit handeln interpretationsspielraum kollaborativ annotiere wahren sinnvoll eingrenzen Kollaboration Catma ermöglichen Gitlab per Api verknüpft projektzentriert Systemarchitektur Rechtesystem erlauben somit differenziert Festlegung Spielräum Konzeption kollaborativ Annotationsprojekt Projekt Mitglied hinzufügen bieten Catma Möglichkeit Manuelle Hinzufügen einzeln Eingabe Funktion bieten asynchron Arbeitsmodus Forschungsprojekt Verknüpfung Gitlab bieten zudem einhergehend Konfliktlösungsfunktion Catma beispielsweise inhaltlich widersprüchlich Mehrfachannotation erlauben stellen zeilenbasiert arbeitend versionierungssystem git Konflikt fest inkompatibel önderungen Projekt vornehmen Zeile zugrundeliegend Codes betreffen nehmen beispielsweise kollaborativ Forschungsprojekt Metaannotation unterschiedlich ändern sobald Arbeit Team synchronisieren melden Catma Konflikt anstatt eigenmächtig Version Vorzug geben sehen fig fremd Version nebeneinandergestellt informieren Version entscheiden tiefergehend Kenntnisse zugrunde liegend technisch Funktionalität unterstützen kollaborativ Modus stark vordergrund stehend diskursiv Aushandlungsprozess Annotation Interpretation reagieren somit Forderung Flexible disziplinspezifischen arbeitsabläufen Schaubild fig verdeutlichen identifiziert literaturwissenschaftlich Spielraum linker spalen erwachsend Generell Anforderung digital Arbeitsumgebunge mittlerer spalen catma Anforderung konkret umsetzen finden spalen,"[('catma', 0.3252188732424817), ('interpretation', 0.2826201258935578), ('annotation', 0.2484780565864676), ('spielraum', 0.1482779562539174), ('kollaborativ', 0.14782676056476443), ('metaannotationen', 0.1452684299654899), ('metaannotation', 0.1452684299654899), ('tagset', 0.13237332243654845), ('annotiere', 0.128238448577964), ('mehrfachannotation', 0.128238448577964)]"
2022,DHd2022,FISCHER_Frank_Dramatische_Metadaten___Die_Datenbank_deutschs.xml,Dramatische Metadaten   Die Datenbank deutschsprachiger Einakter 1740–1850,"Dîlan Canan Çakir (Universität Stuttgart); Frank Fischer (Higher School of Economics, Moskau)","Einakter, Drama, Literatur, Datenbank, API","Archivierung, Literatur, Metadaten"," Der Einakter galt zu jener Zeit auch als Einübungsform und hat viele Erstlingswerke hervorgebracht, beispielhaft seien die später mit umfangreicheren Werken äußerst erfolgreich gewordenen Dramatiker Gotthold Ephraim Lessing oder Adolph Müllner genannt. Die Datenbank macht auch den Anteil von Autorinnen an der Dramenproduktion sichtbarer (dank der Verknüpfung der Autor*innen mit ihren Wikidata-Einträgen können die Informationen zum Geschlecht automatisch bezogen werden; eine Statistik dazu befindet sich auf unserer Website). Zwar sind nur knapp 5% der erfassten Einakterautor*innen weiblich, allerdings treten so neben bekannteren Vertreterinnen wie Luise Adelgunde Victorie Gottsched und produktiven Theaterautorinnen wie Johanna Franul von Weißenthurn oder Charlotte Birch-Pfeiffer auch unbekanntere Verfasserinnen von Einaktern zutage.  Der Hinweis auf Frankreich ist auch deshalb wichtig, weil ein knappes Drittel der Einakter nachweislich aus Übersetzungen oder Adaptionen besteht, und zwar in großer Mehrzahl (über 90%) aus dem Französischen; andere Sprachen spielen kaum eine Rolle. Die Vorlagen für diese Übersetzungen oder Bearbeitungen haben wir ebenfalls in der Datenbank erfasst, inklusive Metadaten zu Werken und Autor*innen. Über Wikidata werden dann auch die Geokoordinaten bezogen und mit der JavaScript-Bibliothek Leaflet auf eine Weltkarte gemappt, die ebenfalls auf der Website zu finden ist. Dadurch wird auch ein geodatenbasierter Zugang zum Korpus möglich. Auf besagter Karte wird deutlich, dass sich die Handlung deutschsprachiger Einakter für den von uns beobachteten Zeitraum vor allem in den Grenzen des Alten Reichs entfaltet. Neben Berlin und Wien ist allerdings Paris die mit Abstand häufigste Ortsangabe, was freilich an der Vielzahl von Übersetzungen liegt. Einakter, die in der Neuen Welt, im Nahen, Mittleren oder Fernen Osten spielen, bilden die absolute Ausnahme. Durch unseren exhaustiven Ansatz bei der Korpuszusammenstellung lassen sich diese Ausnahmen aber zum ersten Mal systematisch erfassen.  Abb. 2 zeigt die Anzahl von Szenen pro Einakter in chronologischer Verteilung. Der Durchschnitt liegt bei 14 Szenen. Für die Mehrzahl der Stücke ist die Szenenanzahl zwischen 7 und 20 angesiedelt, wie man im dunklen Innern der Datenwolke erkennen kann. Als Vergleich seien die 131 zwischen 1740 und 1850 publizierten fünfaktigen deutschsprachigen Dramen des GerDraCor-Korpus (vgl. Fischer et al. 2019) herangezogen, deren durchschnittliche Aktlänge bei knapp unter 7 Szenen liegt. Die Spieldauer für die Einakter beträgt typischerweise zwischen 15 Minuten und einer Stunde (teils ist diese explizit mit angegeben). Die Figurenanzahl der Einakter liegt durchschnittlich bei 7, wobei die Mehrzahl der Stücke zwischen 5 und 7 Personen bzw. Sprechinstanzen im Personenverzeichnis auflistet (Abb. 3). Um wieder mit GerDraCor zu vergleichen: Dort liegt die durchschnittliche Anzahl der Figuren für die 131 Fünfakter des Zeitraum 1740–1850 bei 29. Das Verhältnis zwischen weiblichen und männlichen Figuren liegt für dieselben Fünfakter bei 23:100; hingegen in der Einakter-Datenbank (momentan 2305 Stücke) bei 46:100. Der höhere Anteil weiblicher Figuren in einaktigen Stücken lässt sich unter anderem mit bestimmten Handlungsschwerpunkten erklären (viele einaktige Eheanbahnungskomödien bei nur wenigen Tragödien und historischen Dramen).  Die hier präsentierten Ergebnisse können jeweils auf dem aktuellen Stand der Datenbank überprüft werden. Vorgehaltene und errechnete Daten werden über leicht zugängliche Endpunkte exponiert, die unsere Daten im CSV- und JSON-Format anbieten. Die Daten können entweder für die Nutzung in Tabellenkalkulationen wie Microsoft Excel oder LibreOffice Calc heruntergeladen oder direkt über eine Programmiersprache bezogen werden. Beispiele für die Verwendung in R gibt es auf der Website des Projekts.",de,Einakter gelten Einübungsform Erstlingswerke hervorbringen beispielhaft umfangreicheren Werk äußerst erfolgreich geworden Dramatiker Gotthold Ephraim Lessing adolph Müllner nennen Datenbank Anteil autorinnen Dramenproduktion sichtbar Verknüpfung Information Geschlecht automatisch beziehen Statistik befinden Website knapp erfasst weiblich treten bekannt vertreterinnen luise adelgund Victorie gottsched produktiv Theaterautorinn Johanna Franul weißenthurn Charlotte unbekannter Verfasserinn Einaktern zutage Hinweis Frankreich wichtig knapp Drittel Einakter nachweislich übersetzung Adaption bestehen Mehrzahl französisch Sprache spielen Rolle Vorlag übersetzung bearbeitung ebenfalls Datenbank erfasst inklusive metadaten werken Wikidata Geokoordinat beziehen Leaflet Weltkarte mappen ebenfalls Website finden geodatenbasiert Zugang Korpus besagt Karte deutlich Handlung deutschsprachig einakt beobachtet Zeitraum Grenze alt Reich entfalten Berlin Wien Paris Abstand häufigst Ortsangabe freilich Vielzahl übersetzungen liegen einakt Welt nah mittlerer fern Osten spielen bilden absolut Ausnahme unser exhaustiv Ansatz Korpuszusammenstellung lassen Ausnahme Mal systematisch erfassen abb zeigen Anzahl Szene pro Einakter chronologisch Verteilung Durchschnitt liegen Szene Mehrzahl Stück Szenenanzahl ansiedeln dunkel innern Datenwolke erkennen Vergleich publiziert Fünfaktig deutschsprachig Dram Fischer et heranziehen durchschnittlich aktlänge knapp Szene liegen Spieldauer Einakter betragen typischerweise Minute Stunde teils explizit angeben Figurenanzahl Einakter liegen durchschnittlich wobei Mehrzahl Stück Person sprechinstanzen Personenverzeichnis auflistet abb Gerdracor vergleichen liegen durchschnittlich Anzahl Figur Fünfakter Zeitraum Verhältnis weiblich männlich Figur liegen Fünfakter hingegen momentan Stück hoch Anteil weiblich Figur einaktig Stück lässen bestimmt Handlungsschwerpunkte erklären einaktig Eheanbahnungskomödi weniger Tragödie historisch dramen präsentiert Ergebnis jeweils aktuell Stand Datenbank überprüfen vorgehalten errechnet daten zugänglich Endpunkt exponieren daten anbieten daten Nutzung Tabellenkalkulation Microsoft Excel Libreoffice Calc heruntergeladen direkt Programmiersprache beziehen Beispiel Verwendung r Website Projekt,"[('einakter', 0.36560611608211396), ('stück', 0.19622537282406902), ('mehrzahl', 0.1936474199994725), ('website', 0.15611631214779004), ('datenbank', 0.14991848769332763), ('knapp', 0.14716902961805176), ('einaktig', 0.1462424464328456), ('fünfakter', 0.1462424464328456), ('liegen', 0.14137755988199172), ('einakt', 0.1362137519633028)]"
2022,DHd2022,REBORA_Simone_Towards_a_Computational_Study_of_German_Book_R.xml,Towards a Computational Study of German Book Reviews   A Comparison between Emotion Dictionaries and Transfer Learning in Sentiment Analysis,"Simone Rebora (University of Bielefeld, Germany); Thomas Messerli (University of Basel, Switzerland); Berenike Herrmann (University of Bielefeld, Germany)","Book reviews, Machine learning, Sentiment analysis","Inhaltsanalyse, Annotieren"," The dictionary-based and TL approaches were evaluated on two manually annotated datasets, working with two annotators: in the first dataset (~21,000 sentences), the annotation task was that of identifying evaluative language (vs. descriptive language); in the second dataset (~13,500 sentences), the task focused on the distinction between positive and negative sentiment. These two classification tasks form the basis for a large-scale analysis of the LOBO corpus, which segments reviews into evaluative and descriptive passages, to describe differences in evaluation practices across genres (e.g., romance, science fiction) and ratings (1-5 stars).   The evaluation procedure was repeated on Task 2 (positive vs. negative sentiment). Again, inter-annotator agreement was strong for manual annotation of the Gold Standard (Cohen""s Kappa = 0.79). Annotation percentages are shown by Fig. 2 (where the ""other"" category indicates both mixed feelings and the absence of evaluation).  Our results highlight the higher efficiency of TL-methods (see Table 4) and of dictionaries based on vector space models (like SentiArt and AffectiveNorms). They show that computational methods can reliably identify sentiment of book reviews in German. In order to fruitfully use similar methodology to identify types of engagement by reviewers with literature beyond the descriptive/evaluative and positive/negative dichotomies, a useful next step will be to attempt the design of TL-tasks for the identification of more fine-grained evaluative practices. These include the construction of and orientation to particular evaluative scales (e.g. reading pleasure, literary quality) and particular subjects of evaluation (e.g. novels, authors, characters).",en,dictionary base tl approach evaluate manually annotated dataset work annotator dataset sentence annotation task identify evaluative language descriptive language second dataset sentence task focus distinction positive negative sentiment classification task form basis large scale analysis lobo corpus segment review evaluative descriptive passage describe difference evaluation practice genre romance science fiction rating star evaluation procedure repeat task positive negative sentiment inter annotator agreement strong manual annotation gold standard kappa annotation percentage show fig category indicate mixed feeling absence evaluation result highlight high efficiency tl method table dictionary base vector space model like sentiart affectivenorm computational method reliably identify sentiment book review german order fruitfully use similar methodology identify type engagement reviewer literature descriptive evaluative positive negative dichotomy useful step attempt design tl task identification fine grain evaluative practice include construction orientation particular evaluative scale read pleasure literary quality particular subject evaluation novel author character,"[('evaluative', 0.4038928485868283), ('descriptive', 0.24233570915209696), ('tl', 0.24233570915209696), ('task', 0.2341491302993897), ('negative', 0.21392642148117277), ('positive', 0.2047806736241651), ('identify', 0.18068968556179166), ('dataset', 0.18068968556179166), ('sentiment', 0.15006440208836863), ('dictionary', 0.14261761432078185)]"
2022,DHd2022,ROEDER_Torsten_Aufbau_eines_Referenzkorpus__Erste_Sätze_in_d.xml,"Aufbau eines Referenzkorpus ""Erste Sätze in der deutschsprachigen Literatur""","Anna Busch (Theodor-Fontane-Archiv, Universität Potsdam); Torsten Roeder (Bergische Universität Wuppertal, Germany)","Korpuserstellung, Segmentierung, deutschsprachiche Literatur","Sammlung, Inhaltsanalyse, Annotieren, Stilistische Analyse, Visualisierung, Literatur","  Der sämtlichen bisherigen Studien zu ersten Sätzen ""mangelnden Gesamtsicht"" (Alt 2020: 246) zu begegnen, ist Anliegen des Korpus ""Erste Sätze in der deutschsprachigen Literatur"". Dazu wird ein Datenkorpus erstellt, publiziert und anschließend in einer Verzahnung von quantitativen und textanalytischen Herangehensweisen eine erste Auswertung unternommen. Als Ausgangsmaterial dienen mehrere Volltextkorpora (Deutsches Textarchiv, Zeno, u.a.), aus denen Texte nach Gattungen extrahiert wurden. Es ist deutlich, dass die vorhandenen Volltextangebote zwar unterschiedlich reichhaltige Strukturinformationen über das jeweilige Dokument bieten, aber die automatische Abgrenzung geschlossener Texteinheiten oft nicht trivial und ohne Einzelprüfung nicht zuverlässig möglich ist (z.B. bei Sammelbänden, Texten mit mehreren Kapiteln, Texte in mehreren Bänden). Dies bildet allerdings die Voraussetzung für das Extrahieren der ersten Sätze. Hinzu kommt, dass der Beginn des ""poetischen Texts"" durch z.B. vorangestellte Vorworte, Widmungstexte oder Einleitungen automatisiert nicht immer eindeutig zu lokalisieren ist. Ferner ist die Abgrenzung von ""ersten Sätzen"" ein semantisches Problem. Sätze lassen sich als grammatisch-analytische Einheiten begreifen, die durch bestimmte Satzzeichen voneinander abgetrennt werden, was der maschinellen Verarbeitung entgegenkommt. Jedoch unterscheiden und verändern sich die zur Abgrenzung eines Satzes verwendeten Zeichen erheblich (man betrachte allein die Entwicklungen zwischen dem 17. und 18. Jahrhundert). Die absolute Trennschärfe mancher Satzzeichen steht zudem kontextabhängig infrage, weshalb Sätze teils auch als Sinneinheiten zu begreifen sind, in denen Satzzeichen eine strukturierende, aber nicht unterbrechende Funktion innewohnt (vgl. Abb. 2a/b). Sollte man also eher von einem fließenden ""Beginn"" oder ""Anfang"" sprechen? Bei der Bestimmung der ""ersten Sätze"" spielen somit Unschärfebereiche hinein, die sich wiederum auf Korpuskonsistenz und -vergleichbarkeit auswirken können. Das derzeitig erstellte Korpus ist vollständig mitsamt Metadaten und Quellenangaben inkl. Positionsangaben in TEI codiert. Gattungsabhängig bewegt sich die Anzahl der Satzanfänge zwischen 100 und 1000 Einträgen. Mithilfe der manuell und automatisch erstellten Annotationen lässt sich das Korpus nach verschiedenen Parametern analysieren und visualisieren, beispielsweise nach Veröffentlichungsdatum, Textgattung, Geschlecht von Verfasserin oder Verfasser, Personen-, Orts- oder Zeitbezüge im Text (vgl. Abb. 1c) oder Länge des Gesamttexts. Außerdem wird dokumentiert, welchen Auswahlkriterien die jeweiligen Datenquellen unterlagen und wie dies im Hinblick auf die Ausgewogenheit des Korpus bei der Auswertung berücksichtigt werden sollte (vgl. Hug/Boenig 2021). Zur Dissemination des Korpus wurde 2021 das Twitter-Projekt ""@satzomat"" gelauncht, das täglich zwei erste Sätze sendet (vgl. Abbildungen 1–3). Ziel ist es, eine ""Typologie des ersten Satzes"" mithilfe computerphilologischer Auswertungsverfahren zu erstellen sowie zu fragen, inwieweit Gattungen im Verlaufe der Geschichte bestimmte Typen von ersten Sätzen determinierten (z.B. Landschaftsbild, Rahmenhandlung) und ob sich weitere Korrelationen mithilfe der Metadaten und Annotationen feststellen lassen.   ",de,sämtlichen bisherig Studie Satz mangelnd Gesamtsicht alt begegnen Anliegen Korpus Sätz deutschsprachig Literatur Datenkorpus erstellen publizieren anschließend Verzahnung quantitativen textanalytisch herangehenswei Auswertung unternehmen ausgangsmaterial dienen mehrere Volltextkorpora deutsch Textarchiv Zeno Text Gattung extrahiern deutlich vorhanden Volltextangebote unterschiedlich reichhaltig Strukturinformation jeweilig Dokument bieten automatisch Abgrenzung geschlossen Texteinheit trivial Einzelprüfung zuverlässig sammelbänden Text mehrere Kapitel Text mehrere bänden bilden Voraussetzung extrahieren Sätz hinzu Beginn poetisch texts vorangestellt vorwort Widmungstext Einleitunge automatisieren eindeutig lokalisieren ferner Abgrenzung Satz semantisch Problem Sätz lassen einheiten begreifen bestimmt Satzzeich voneinander abtrennen maschinell Verarbeitung entgegenkommen unterscheiden verändern Abgrenzung satzes verwendet Zeichen erheblich betrachten Entwicklung Jahrhundert absolut trennschärfen Satzzeich stehen zudem kontextabhängig Infrage weshalb Sätz teils sinneinheiten begreifen satzzeich strukturierend unterbrechend Funktion innewohnen abb b eher fließend Beginn Anfang sprechen Bestimmung Sätz spielen somit unschärfebereichen hinein wiederum Korpuskonsistenz auswirken derzeitig erstellt Korpus vollständig mitsamt metadaten quellenangaben Positionsangabe Tei Codiert gattungsabhängig bewegen Anzahl Satzanfäng einträgen Mithilfe manuell automatisch erstellt Annotation lässt korpus verschieden Parameter analysieren visualisieren beispielsweise Veröffentlichungsdatum Textgattung Geschlecht Verfasserin verfass zeitbezüge Text abb Länge Gesamttext dokumentieren Auswahlkriterie jeweilig datenquellen Unterlage Hinblick Ausgewogenheit korpus Auswertung berücksichtigen Hug Boenig Dissemination Korpus launchen täglich Sätz senden abbildungen Ziel Typologie Satz Mithilfe Computerphilologischer auswertungsverfahren erstellen fragen inwieweit Gattung verlaufe Geschichte bestimmt Typ Sätze determinieren Landschaftsbild Rahmenhandlung korrelation Mithilfe metadaten annotation feststellen lassen,"[('sätz', 0.33062062851831725), ('satzzeich', 0.22871256915291208), ('abgrenzung', 0.18438543017044223), ('satz', 0.14669244215882185), ('korpus', 0.13718310000521305), ('mithilfe', 0.12169185057664897), ('begreifen', 0.11804354699192858), ('beginn', 0.10403400209755737), ('mehrere', 0.09179787101371863), ('erstellt', 0.09172928944379936)]"
2022,DHd2022,CHARVAT_Vera_Maria_Computational_Literary_Studies_Data_Lands.xml,Computational Literary Studies Data Landscape Review,"Ingo Börner (Universität Potsdam); Vera Maria Charvat (Österreichische Akademie der Wissenschaften, Austrian Centre for Digital Humanities and Cultural Heritage (ACDH-CH)); Matej None (Österreichische Akademie der Wissenschaften, Austrian Centre for Digital Humanities and Cultural Heritage (ACDH-CH)); Micha≈Ç Mrugalski (Humboldt-Universität zu Berlin); Carolin Odebrecht (Humboldt-Universität zu Berlin)","Computational Literary Studies, Metadaten, Linked Open Data, Data Landscape, Research Discovery, FAIR principles","Modellierung, Community-Bildung, Organisation, Infrastruktur, Metadaten, Forschungsergebnis"," Das übergeordnete Ziel von ""Computational Literary Studies Infrastructure"" Um die Auffindbarkeit und den forschungsorientierten Zugang zu literarischen Daten für die CLS-Community zu ermöglichen, ist eine Inventarisierung der CLS-Datenlandschaft erforderlich, die forschungsrelevante Kriterien für die Datenauswahl sowie deren Erfassung und Beschreibung anwendet. Mit dieser Inventarisierung, die wir in Form einer Dabei stellen wir uns unter anderem folgende Fragen: Welche Beschreibungsmerkmale sind für die Daten als Kollektion im Sinne einer eigenen epistemischen Einheit wesentlich? Welche Beschreibungsmerkmale sind in Bezug auf die literarischen Vorlagen und deren Aufbereitungen wichtig? Wie kann nach Kollektionen oder einzelnen Datensätzen im Sinne der Die Ergebnisse unserer Aufbauend auf der Review werden die Ergebnisse in Form eines stetig wachsenden, interaktiven Online-Katalogs literarischer Corpora für die CLS-Community bereitgestellt. Dieser wird eine umfassende Übersicht über die verfügbaren Ressourcen inklusive ausführlicher beschreibender Metadaten liefern und die üblichen Abfrage- und Erschließungsmöglichkeiten mittels verschiedener Such- und Filtermechanismen bieten. Konzeptueller Ausgangspunkt für die strukturierte Sammlung der Informationen ist das Metamodell für Korpusmetadaten (MKM; Odebrecht 2018) 'ein, generisches erweiterbares Beschreibungsmodell, für die zentralen Entitäten Während das Modell selbst abstrakt definiert ist, erarbeiten wir eine kongruente/entsprechende Ontologie im OWL-Format (OWL, 2012), welche eine Repräsentation der Daten in RDF (Resource Description Framework) Ebenso ist zu berücksichtigen, dass dieser Katalog Teil von einem komplexen Gefüge an Ressourcen, Providern und Disseminationskanälen bzw. Aggregatoren ist. Die Position des Katalogs in diesem Gefüge und seine Beziehung zu verwandten Aggregatoren wie CLARIN VLO (Virtual Language Observatory) Die",de,übergeordnet Ziel Computational literary studies infrastructure Auffindbarkeit forschungsorientiert Zugang literarisch daten ermöglichen Inventarisierung erforderlich Forschungsrelevant kriterien datenauswahl Erfassung Beschreibung anwenden Inventarisierung Form stellen folgend Frage beschreibungsmerkmal daten Kollektion Sinn epistemisch Einheit wesentlich beschreibungsmerkmal Bezug literarisch Vorlag Aufbereitung wichtig Kollektion einzeln datensätzen Sinn Ergebnis Aufbauend Review Ergebnis Form stetig wachsend interaktiv literarisch Corpora bereitstellen umfassend Übersicht verfügbar Ressource inklusive ausführlich beschreibend metadaten liefern üblich Erschließungsmöglichkeit mittels verschieden filtermechanismen bieten konzeptuell Ausgangspunkt strukturiert Sammlung Information metamodell Korpusmetadaten mkm odebrechen generisch erweiterbareer beschreibungsmodell zentral Entität Modell abstrakt definieren erarbeiten kongruent entsprechend Ontologie owl Repräsentation daten Rdf resource description Framework berücksichtigen Katalog komplex Gefüge Ressource Providern disseminationskanäl aggregatoren Position Katalog gefüge Beziehung verwandt Aggregator Clarin vlo virtual language Observatory,"[('beschreibungsmerkmal', 0.2467630592351105), ('inventarisierung', 0.2467630592351105), ('kollektion', 0.22984108214978463), ('katalog', 0.22984108214978463), ('gefüge', 0.21783474833575536), ('ressource', 0.1249906835578594), ('erweiterbareer', 0.12338152961755525), ('forschungsrelevant', 0.12338152961755525), ('providern', 0.12338152961755525), ('datenauswahl', 0.12338152961755525)]"
2022,DHd2022,BROTTRAGER_Judith_Doctoral_Consortium__Judith_Brottrager.xml,Relating the Unread   Modellierungen der Literaturgeschichte,"Judith Brottrager (TU Darmstadt, Germany)","Distant Reading, Kanonisierung, Digitale Literaturwissenschaft","Distant Reading, Kanonisierung, Digitale Literaturwissenschaft"," Mein Dissertationsprojekt folgt dieser Tradition, indem kanonisierte und nicht-kanonisierte literarische Werke auf unterschiedlichen Ebenen mit Rückgriffen auf literaturhistorische Daten miteinander in Beziehung gesetzt werden. Die für die Untersuchungen erstellten Korpora und Datensätze umfassen etwa 1.200 englisch- und deutschsprachige Texte von 1688 bis 1914, wodurch diachrone und synchrone Vergleiche von Kanonisierungsprozessen und -mustern über Sprachgrenzen hinweg ermöglicht werden. Durch diese Datenvielfalt sollen einerseits etablierte literaturhistorische Narrative untersucht und quantitativ überprüft werden und andererseits literaturwissenschaftliche Kategorien wie Kanonisierung und Wertung so operationalisiert werden, dass sie mit computationellen Methoden zur Bestimmung von Textähnlichkeiten gewinnbringend kombiniert und diese Öhnlichkeit schließlich als Netzwerkmodelle dargestellt werden können. Der Korpusaufbau folgt einem systematisch angepassten Ansatz von Algee-Hewitt und McGurl, der darauf abzielt, von einem vorgefundenen zu einem maßgeschneiderten Korpus zu gelangen, indem Bestenlisten, Bestsellerlisten und von Expert*innen kuratierte Literaturlisten kombiniert werden, um ein repräsentatives Korpus für die englischsprachige Literatur des 20. Jahrhunderts zu erstellen (Algee-Hewitt/McGurl 2015). Durch die Kombination dieser Listen decken Algee-Hewitt und McGurl drei Ebenen der literarischen Produktion ab: den normativ-exklusiven Kanon, populäre Texte und von Expert*innen für Postkoloniale und Feministische Literaturwissenschaft vorgeschlagene Werke. Für die Umsetzung für die Zeitspanne von 1688-1914 wurde dieser Ansatz systematisch adaptiert, indem narrative Literaturgeschichten, Anthologien und (spezialisierte) Sekundärtexte, die diese Ebenen abdecken, identifiziert und als bibliografische Quellen für die Korpuserstellung genutzt wurden. Der Workflow umfasst Webscraping, X-Technologien/Transformationen und Retro-Digitalisierungen. Analog zur Korpuserstellung wurden Daten zur Kontextualisierung der jeweiligen Korpustexte, aber auch der gesamten literarischen Produktion gesammelt. Durch diese Daten können die Korpora mit den von Algee-Hewitt et al. als ""the published"" Die gesammelten Daten werden neben diesen Metadatenanalysen auch für die Operationalisierungen der literaturwissenschaftlichen Konzepte der Kanonisierung und Wertung eingesetzt. Aufbauend auf die theoretischen Grundlagen von Heydebrand und Winko werden Kanonisierung und Wertung als Scores implementiert, die ausdrücken, wie hoch die Wahrscheinlichkeit ist, dass ein bestimmter Text sehr kanonisiert ist beziehungsweise zur Entstehungszeit sehr gut rezipiert wurde (Heydebrand/Winko 1996). Ein besonderes Augenmerk liegt hierbei auf der Differenzierung der Konzepte und der Einbindung der Rezeptionsebene über Marker für Publikumsinteresse (wie Einträge in Leihbibliothekskatalogen und Zweitauflagen innerhalb einer Generation) und sprachliche Werturteile, die durch Sentiment Analysen vergleichbar werden. Unter Verwendung der generierten Scores soll schließlich untersucht werden, ob Kanonisierung und Wertung mit textintrinsischen Merkmalen in Verbindung gebracht werden können. Stilometrische Berechnungen, Topic Modeling und Word Embeddings sowie wortartenbasierte Ansätze sollen dabei als alleinstehende Analysen der Textähnlichkeiten durchgeführt werden. Als zusätzliche Analysen- und Visualisierungsmethode dienen Netzwerkmodelle, die anhand der Ergebnisse der Textähnlichkeitsberechnungen erstellt werden, zur Exploration von Öhnlichkeitsstrukturen. Besonders auf dieser Ebene soll der Bezug zur literaturwissenschaftlichen Forschung durch die Identifikation von dichten stilistischen Öhnlichkeitsgruppen und durch aus den Modellen abgeleitete Einzeltextanalysen hergestellt werden.",de,Dissertationsprojekt folgen Tradition kanonisiert literarisch Werk unterschiedlich Ebene Rückgriffen literaturhistorisch daten miteinander Beziehung setzen Untersuchung erstellt Korpora datensätz umfassen deutschsprachig Text wodurch Diachrone Synchrone vergleiche Kanonisierungsprozesse Sprachgrenz Hinweg ermöglichen Datenvielfalt einerseits etabliert literaturhistorisch Narrative untersuchen quantitativ überprüfen andererseits literaturwissenschaftlich Kategorie Kanonisierung Wertung operationalisiern computationell Methode Bestimmung Textähnlichkeite gewinnbringend kombinieren Öhnlichkeit schließlich Netzwerkmodelle darstellen Korpusaufbau folgen systematisch angepasst Ansatz Mcgurl abzielen vorgefundenen maßgeschneidert Korpus gelangen bestenlist bestsellerlist kuratiert Literaturliste kombinieren repräsentativ korpus englischsprachig Literatur Jahrhundert erstellen mcgurl Kombination liste decken mcgurl Ebene literarisch Produktion Kanon populär Text postkolonial feministisch Literaturwissenschaft vorgeschlagen Werk Umsetzung Zeitspanne Ansatz systematisch adaptieren narrative literaturgeschichten anthologien spezialisiert sekundärtexte eben abdecken identifizieren bibliografisch quellen Korpuserstellung nutzen Workflow Umfasst Webscraping Transformation analog Korpuserstellung daten Kontextualisierung Jeweilige Korpustext gesamt literarisch Produktion sammeln daten Korpora et the published gesammelt daten Metadatenanalyse Operationalisierung literaturwissenschaftlich Konzept Kanonisierung Wertung einsetzen aufbauend theoretisch Grundlag Heydebrand Winko Kanonisierung Wertung scores implementieren ausdrücken Wahrscheinlichkeit bestimmt Text kanonisieren beziehungsweise Entstehungszeit rezipieren Heydebrand Winko besonderer Augenmerk liegen hierbei Differenzierung Konzept Einbindung Rezeptionsebene Marker Publikumsinteresse einträge leihbibliothekskatalogen zweitauflagen innerhalb Generation sprachlich werturteil Sentiment Analyse vergleichbar Verwendung generiert Scores schließlich untersuchen Kanonisierung Wertung textintrinsisch Merkmale Verbindung bringen stilometrisch Berechnung Topic Modeling Word embeddings wortartenbasiert Ansatz alleinstehend Analyse Textähnlichkeite durchführen zusätzlich visualisierungsmethode dienen netzwerkmodellen anhand Ergebnis Textähnlichkeitsberechnung erstellen Exploration öhnlichkeitsstrukturen Ebene Bezug literaturwissenschaftlich Forschung Identifikation dicht stilistisch öhnlichkeitsgruppen modellen abgeleitet einzeltextanalysen herstellen,"[('wertung', 0.28267615489204756), ('kanonisierung', 0.28267615489204756), ('mcgurl', 0.2508874198568489), ('heydebrand', 0.15578840759608806), ('textähnlichkeite', 0.1476504036828636), ('korpuserstellung', 0.13181988830417596), ('scores', 0.13181988830417596), ('winko', 0.12471065906590782), ('produktion', 0.12471065906590782), ('ebene', 0.10874335607058368)]"
2022,DHd2022,SCHUMACHER_Mareike__Wie_Wölkchen_im_Morgenlicht____zur_autom.xml,"""Wie Wölkchen im Morgenlicht"" Zur automatisierten Metaphern-Erkennung und der Datenbank literarischer Raummetaphern laRa","Mareike Schumacher (Technische Universität Darmstadt, Germany)","Metaphern, Textanalyse, Methodenvergleich","Kontextsetzung, Literatur, Metadaten, Methoden, benannte Entitäten (named entities), Text"," Zwei grundlegende Probleme bei der Betrachtung von Metaphern im Vergleich zu nicht-metaphorischen Ausdrücken sind Uneigentlichkeit und Variabilität. Metaphern werden grundsätzlich aus drei Größen konstruiert: dem sprachlichen Ausdruck, dem, was der sprachliche Ausdruck im Wortsinne bezeichnet und etwas Öhnlichem (vgl. Wenz 1997: 32). Dabei unterscheidet sich die Grammatik metaphorischer Ausdrücke meist in nichts von wörtlich gemeinten Phrasen (vgl. Thaller 2021: 91). Metaphern sind dynamische Konstrukte, die eine Entwicklung durchlaufen von Einführung, über Etablierung zur Konventionalisierung und schließlich bis hin zum Übergang in den eigentlichen Sprachgebrauch (vgl. Thaller 2021), d.h. Ausdrücke können die einmal aufgebaute Metaphorik auch wieder verlieren. Metaphern sind aber nicht nur ein interessantes sprachliches Phänomen, sondern können auch als prägende Ausdrücke menschlichen Handelns fungieren (vgl. Blumenberg 1971: 213 und Wenz 1997: 33). Sie sind eine wesentliche Basis menschlichen Denkens.  Im Rahmen meiner Dissertation Named Entity Recognition (NER) ist ursprünglich eine computerlinguistische Methode zur automatischen Erkennung und Klassifizierung klar benannter Einheiten (vgl. Schumacher 2018: ¬ß1). Die am häufigsten in Named-Entity-Recognition-Tools implementierten Kategorien sind Personen, Orte und Organisationen. Für die literaturwissenschaftliche Nutzung von NER bedarf es allerdings einer Domänenadaption, bei der sowohl die implementierten Kategorien als auch die Traningsdaten angepasst werden müssen. Die Methode wurde bereits erfolgreich für die Erkennung literarischer Figuren adaptiert (vgl. Jannidis et al. 2015) und auch eine Unterklassifizierung nach Genderzuweisungen ist möglich und für literaturwissenschaftliche Forschung gewinnbringend (vgl. Schumacher und Flüh 2020). Die Kategorie des Ortes ist für literarische Texte nahezu ebenso relevant wie die der Person bzw. Figur. Statt eines komplexen Raumkonzeptes, das in mehrere Unterkategorien aufgeteilt wird, werden bei der linguistischen Nutzung von NER-Tools lediglich Ortsnamen erkannt. Um ein NER-Tool so zu adaptieren, dass es Raumreferenzen erkennen und in eine von sieben Kategorien literarischen Raumes einsortieren kann, wurde ein Machine-Learning-Training durchgeführt. Im Folgenden werden diejenigen Ausschnitte des Trainings vorgestellt, die zeigen, inwiefern die automatisierte Erkennung von Raummetaphern dabei gescheitert ist. Die Wahl des Tools fiel auf den in den Digital Humanities gut etablierten Stanford Named Entity Recognizer (Finkel, Grenager und Manning 2005), in dem kontextsensitive Conditional-Random-Fields-Algorithmen (zu CRF-Algorithmen vgl. Sutton und McCallum 2010) implementiert sind (Manning et al. 2014). Das Trainingskorpus besteht aus Ausschnitten aus 80 Romanen aus vier Jahrhunderten (18–21). Aus jedem Jahrhundert wurden 20 Erzähltexte integriert, sodass das Trainingskorpus einen gleichmäßigen Aufbau aufweist Die Testergebnisse können wie in Abb. 1 visualisiert werden: Der Vergleich mit der insgesamt am besten erkannten Kategorie ""Ort"" zeigt, dass sich für die Testergebnisse der Kategorie ""Metapher"" keine Regelmäßigkeiten ergeben. Für einzelne Testtexte zeigen jahrhundertspezifische Trainingsdaten oder Trainingsdaten aus zwei Jahrhunderten den größten Trainingserfolg. Für andere ist ein Trainingskorpus aus einem ganz anderen zeitlichen Kontext passender. Eine schrittweise Ausweitung des alle vier Jahrhunderte umfassenden Trainingsmaterials zeigt keinerlei regelmäßigen Zuwachs der Erkennungsgenauigkeit. Einen viel typischeren Trainingsverlauf zeigt die Ortskategorie. Hier ist hauptsächlich die Größe des Trainingskorpus ausschlaggebend. Je mehr Trainingsdaten eingesetzt werden, desto höher die Erkennungsquote. Auch der zeitliche Kontext ist hier nur bei wenigen Testtexten bedeutend. Insgesamt zeigt die Kummulierung der Trainingsdaten eine zwar langsame aber kontinuierliche Steigerung. Das Training der Metaphernerkennung gleicht hingegen einem Glücksspiel: Mal führt das Hinzufügen neuer Trainingsdaten zu einer Verbesserung des Classifiers, mal wird dadurch alles buchstäblich zurück auf Null gesetzt. Als alternatives Recherche-Tool und um besser zu verstehen, warum die Automatisierung von Metaphernerkennung so problematisch ist, wurde die relationale Graphdatenbank literarischer Raummetaphern Durch die Anreicherung mit Propertys kann die Datenbank auf vielfältige Weise durchsucht werden. Wenn sowohl das Wortmaterial als auch die Bedeutung einer Metapher extrem ähnlich waren, wie z.B. der Fall bei ""in schlechte Hände geraten"" und ""in schlechten Händen sein"", wurden die Phrasen mit einem zweiten Typ von Relation untereinander verbunden (Relationstyp Das Netzwerk der Raummetaphern in Abb. 3 zeigt, dass es nur wenige zentrale Metaphern gibt, d.h. Metaphern, die sowohl viele Varianten aufweisen als auch in vielen Texten vorkommen. Viele Texte bilden mit ihren Raummetaphern eigene Cluster, die vielfach nur über Variationen mit anderen Metaphern, also nur indirekt mit anderen Texten verknüpft sind. Manche Text-Metaphern-Cluster sind gar nicht mit dem Hauptnetzwerk verbunden. Noch deutlicher wird das Gefüge, wenn eine Raummetapher einzeln betrachtet wird. Die Wahl fiel auf die Ein-Wort-Metapher ""Weg"", die keine der zentralsten Metaphern ist, sondern einen mittleren Vernetzungsgrad aufweist. Abb. 4 zeigt diese Raummetapher mit ihren Varianten und den Erzähltexten, in denen sie vorkommen: ",de,grundlegend Problem Betrachtung Metapher Vergleich Ausdrücken Uneigentlichkeit Variabilität Metapher grundsätzlich Größ konstruieren sprachlich Ausdruck sprachlich Ausdruck Wortsinn bezeichnen öhnlich Wenz unterscheiden Grammatik metaphorisch Ausdrück meist wörtlich gemeint Phras Thaller Metapher dynamisch Konstrukt Entwicklung durchlaufen Einführung Etablierung Konventionalisierung schließlich Übergang eigentlich Sprachgebrauch Thaller ausdrücke aufgebaut Metaphorik verlieren Metapher interessant sprachlich Phänomen prägend Ausdrück menschlich handelns Fungieren Blumenberg Wenz wesentlich Basis menschlich Denken Rahmen Dissertation named entity Recognition ner ursprünglich computerlinguistisch Methode automatisch Erkennung Klassifizierung klar benannt Einheit Schumacher häufig implementiert Kategori Person Ort Organisation literaturwissenschaftlich Nutzung ner bedürfen Domänenadaption sowohl implementiert Kategorie traningsdaten angepasst Methode erfolgreich Erkennung literarisch Figur adaptieren Jannidis et Unterklassifizierung genderzuweisungen literaturwissenschaftlich Forschung gewinnbringend schumach Flüh Kategorie Orte literarisch Text nahezu relevant Person Figur komplex Raumkonzept mehrere Unterkategori aufteilen linguistisch Nutzung lediglich ortsnamen erkennen adaptieren raumreferenzen erkennen kategori literarisch Raumes einsortieren durchführen folgend Ausschnitt Training vorstellen zeigen inwiefern automatisiert Erkennung Raummetapher scheitern Wahl Tools fallen Digital Humanitie etabliert Stanford Named entity recognizer Finkel grenage Manning kontextsensitiv Sutton mccallum implementieren Manning et Trainingskorpus bestehen Ausschnitt Romane jahrhundert Jahrhundert erzähltext integrieren sodass Trainingskorpus gleichmäßig Aufbau Aufweist testergebnisse abb visualisiern Vergleich insgesamt erkannt Kategorie Ort zeigen Testergebnisse Kategorie metapher Regelmäßigkeit ergeben einzeln Testtext zeigen jahrhundertspezifisch Trainingsdat treiningsdaten Jahrhundert groß Trainingserfolg Trainingskorpus zeitlich Kontext Passender schrittweise Ausweitung jahrhunderte umfassend Trainingsmaterials zeigen keinerlei regelmäßig Zuwachs Erkennungsgenauigkeit typischeren Trainingsverlauf zeigen Ortskategorie hauptsächlich Größe Trainingskorpus ausschlaggebend Trainingsdat einsetzen desto hoch Erkennungsquote zeitlich Kontext weniger Testtexte bedeutend insgesamt zeigen Kummulierung Trainingsdat langsam kontinuierlich Steigerung Training Metaphernerkennung gleichen hingegen Glücksspiel mal führen Hinzufügen neu Trainingsdaten Verbesserung Classifier mal buchstäblich null setzen alternativ verstehen Automatisierung Metaphernerkennung problematisch relational Graphdatenbank literarisch Raummetapher Anreicherung Propertys Datenbank vielfältig Weise durchsuchen sowohl Wortmaterial Bedeutung Metapher extrem ähnlich Fall schlecht Hand geraten schlecht Hände Phrase Typ Relation untereinander verbinden relationstyp Netzwerk Raummetapher abb zeigen zentral metaphern metaphern sowohl Variant aufweisen Text vorkommen Text bilden Raummetapher Cluster vielfach Variation Metapher indirekt Text verknüpfen Hauptnetzwerk verbinden deutlich gefüge Raummetapher einzeln betrachten Wahl fallen Weg zentralsten metaphern mittlerer Vernetzungsgrad Aufweist abb zeigen Raummetapher Variant erzähltexten vorkommen,"[('raummetapher', 0.3670495914455292), ('metapher', 0.31929159241636823), ('trainingskorpus', 0.19923282096935646), ('metaphern', 0.15508375313873887), ('trainingsdat', 0.13061057166723997), ('zeigen', 0.12932032102558247), ('testergebnisse', 0.1223498638151764), ('wenz', 0.1223498638151764), ('thaller', 0.1223498638151764), ('metaphernerkennung', 0.11395962259231768)]"
2022,DHd2022,TRILCKE_Peer_Poesie_als_Fehler__Ein__Tool_Misuse__Experiment.xml,"Poesie als Fehler   Ein ""Tool Misuse""-Experiment zur Prozessierung von Lyrik","Henny Sluyter-Gäthje (Universität Potsdam, Germany); Peer Trilcke (Universität Potsdam, Germany)","Lyrik, NLP, Fehler","Programmierung, Annotieren, Bereinigung, Stilistische Analyse, Literatur, Text","Analysen der Computational Literary Studies (CLS) vorverarbeiten ihre Untersuchungsgegenstände typischerweise mit Tools des Natural Language Processing (NLP). Dabei weichen literarische Texte aufgrund ihrer historischen und/oder ästhetischen Eigenart teils eklatant von den Daten ab, auf deren Grundlage die Der Das folgende Experiment geht von dieser basalen Überlegung aus. Gegenstand des Experiments ist die Lyrik, der regelmäßig eine ""Tendenz zu erhöhter Devianz"" (Müller-Zettelmann 2000: 100) attestiert wird, die sich in Form gattungsspezifischer ""Störungen"" (Zymner 2019: 29f.) ausdrückt. Wir entwickeln eine Pipeline, die gezielt ""Fehler"" von NLP-Tools provoziert und diese ""Fehler"" regelbasiert typologisiert. Damit möchten wir für die CLS auch exemplarisch den Ansatz des Fehler von Tools wären idealerweise über Daten mit Gold-Standard-Annotationen zu ermitteln. Solche Daten liegen für unser Szenario nicht vor. Deshalb implementieren wir als Workaround eine Pipeline, die folgende Idee umsetzt: Die Verarbeitung einer Zeichenkette durch ein NLP-Tool (Tokenisierung, Lemmatisierung, POS-Tagging) sollte eine Zeichenkette ergeben, die in einem Wörterbuch zu finden ist: Die Ausgabe, die aus der Eingabe ""gehst"" resultiert, sollte sich als Lemma ""gehen"" der Wortart ""Verb"" in einem Wörterbuch finden. Da wir nicht die spezifischen Sprachverarbeitungsprobleme eines individuellen NLP-Tools in den Blick nehmen wollen, lassen wir unser Lyrikkorpus von mehreren Tools prozessieren und betrachten jene Types als potenzielle Fehler, deren von den Tools ausgegebene Lemmata (inkl. POS-Tag) wir nicht in einem Wörterbuch finden. Prozessiert wird ein deutsches kanonbasiertes Korpus mit ""prototypischer"" Lyrik, das Texte von 12 Autor:innen umfasst, die in der statistisch begründeten Anthologie von Braam (2019) am häufigsten mit Gedichten vertreten sind. Ins Korpus aufgenommen wurden sämtliche im TextGridRepository Unsere ""Fehler-Pipeline"" präferiert Die in die Pipeline eingespeisten Textdateien (txt-Format) werden tokenisiert, POS-tagged und lemmatisiert (Abb. 1). Um potenzielle Fehler als taggerspezifische Fehler einzuordnen, verwenden wir vier NLP-Tools. Diese sind frei verfügbar, stellen Bei der Verb-Rekonstruktion werden abgetrennte Verbpartikel regelbasiert (mithilfe von POS-Tag und Abstandsmaßen) an das dazugehörige Lemma des Verbs angefügt. Die darauffolgende Fehlerbetrachtung ist auf dem Vergleich der Lemmata mit Wörterbüchern gestützt. Da wir erwarten, dass Fehler, die ihren Ursprung in der Domänenspezifität haben, sich hauptsächlich auf Inhaltswörter, d.i. Nomen (NOUN), Verben (VERB) und Adjektive (ADJ), beschränken, werden der Wörterbuchvergleich und die darauffolgenden Schritte nur für diese Wortarten durchgeführt. Die Lemmata werden unter Berücksichtigung der Wortart in der lexikalisch-semantischen Ressource GermaNet Für jeden Type erzeugen wir Listen von Lemmata pro Tool. Für die 5.144 Gedichttexte ergibt das eine Typezahl von 70.422 (Tab. 1, Spalte ""all""), die je nach Tagger aufgrund verschiedener Tokenisierung und POS-Tagging zu unterschiedlichen Tokenzahlen führt. Um zu verhindern, dass Fehler toolspezifisch sind, werten wir einen Type nur dann als potenziellen Fehler, wenn mindestens zwei Tools den Type lemmatisiert haben und kein Lemma aus den Listen im Wörterbuch gefunden wurde (Tab. 01, Spalte ""pFail""). Diese potenziellen Fehler werden regelbasiert in Fehlertypen eingeteilt. Auf dem pFail-Set führen wir eine regelbasierte Typologisierung durch. Die Typen postulieren wir ausgehend von manuellen Inspektionen des pFail-Set. Für jeden Typen wird eine Regel formuliert. Die Regeln werden daraufhin in einer spezifischen Reihenfolge auf das pFail-Set angewendet. Mehrfachtypisierungen sind nicht möglich; die Reihenfolge der Regelanwendung hat mithin Konsequenzen für die Menge an jeweils identifizierten Vorkommnissen. Die Reihenfolge lautet: CONTRACT, ELISION_APO, PUNC, SHORT, COMP_DASH, COMP, PART_ADJ, ELISION_SIMPLE, ORTH_UPPER, ORTH_SZ, PREFIXED, EPITHESIS, ELISION_END. Die Typendefinitionen führt Tab. 2 auf. 53,33 % der Types im pFail-Set für Lyrik und 59,88 % der Types im pFail-Set für Prosa werden identifiziert (vgl. Tab. 3). Die identifizierten Typen können zu Gruppen zusammengefasst werden: PUNC und SHORT sind überwiegend unterhalb der Wortebene anzusiedelnde Zeichen, meist Rauschen, das bei Lyrik und Prosa in vergleichbarem Umfang auftaucht. ORTH_SZ dokumentiert den ebenfalls bei Lyrik und Prosa vergleichbar ausgeprägten Effekt der Die 10 weiteren Typen lassen sich zu drei Gruppen zusammenführen. COMP_DASH, COMP, PART_ADJ, PREFIXED versammeln Zu resümieren, dass die spezifisch lyrische ""Störung"" für die NLP-Tools insbesondere aus der Darin zeigen sich zwei Felder für Anschlussforschungen: Erstens wäre zu erproben, ob sich bessere Pipelines für die automatisierte NLP-Tool-Fehleridentifikation ohne Annotationsdaten konzipieren lassen, dafür wäre es hilfreich, die Pipeline auf einem kleinen Set an Gold-Standard-Annotationen zu evaluieren; zweitens könnte auf der Grundlage unserer Pipeline gegen die Baseline von 53,33 % das regelbasierte Typologisierungsverfahren optimiert werden. Die manuelle Annotation einer kleinen Sammlung von Gedichten mit Informationen zum Abweichungscharakter jedes einzelnen Wortes würde es ermöglichen, unsere Annahme, dass unser Verständnis der Devianz durch die Nutzbarmachung des Problems der Domänenadaption von NLP-Tools operationalisiert werden kann, zu prüfen. So könnte sichergestellt werden, dass wir durch die Fehlertypisierung der NLP-Tools tatsächlich etwas über die Spezifik des Literarischen erfahren. In jedem Fall haben wir mit dem vorliegenden",de,analysen computational literary Studies cls vorverarbeien untersuchungsgegenstände typischerweise Tools natural language Processing nlp weichen literarisch Text aufgrund historisch ästhetisch Eigenart teils Eklatant daten Grundlage folgend Experiment basal Überlegung Gegenstand Experiment Lyrik regelmäßig Tendenz erhöht devianz attestieren Form gattungsspezifisch Störunge zymn ausdrücken entwickeln pipeline gezielt Fehler provozieren Fehler Regelbasiert typologisiern möchten Cls exemplarisch Ansatz Fehler Tools sein idealerweise daten ermitteln daten liegen Szenario implementieren Workaround pipeline folgend Idee umsetzen Verarbeitung Zeichenkette Tokenisierung Lemmatisierung zeichenkette ergeben Wörterbuch finden Ausgabe Eingabe gehst resultieren Lemma Wortart Verb Wörterbuch finden spezifisch Sprachverarbeitungsproblem individuell Blick nehmen lassen Lyrikkorpus mehrere Tools prozessieren betrachten Types potenziell Fehler Tool ausgegeben Lemmata Wörterbuch finden prozessieren deutsch kanonbasiert Korpus prototypisch Lyrik Text Autor innen umfassen statistisch begründet Anthologie Braam häufig Gedicht vertreten Korpus aufnehmen sämtlicher Textgridrepository präferieren Pipeline eingespeist Textdateien tokenisieren lemmatisieren abb potenziell Fehler taggerspezifisch Fehler einordnen verwenden frei verfügbar stellen abgetrennt Verbpartikel regelbasieren Mithilfe abstandsmaßen dazugehörig Lemma Verb anfügen darauffolgend Fehlerbetrachtung Vergleich Lemmata wörterbüchern stützen erwarten Fehler Ursprung Domänenspezifität hauptsächlich inhaltswörter nomen Noun verben Verb adjektiv adj beschränken wörterbuchvergleich darauffolgend Schritt Wortart durchführen Lemmata Berücksichtigung Wortart Ressource rmanen Type erzeugen Liste Lemmata pro Tool Gedichttexte ergeben Typezahl tab spalen all Tagger aufgrund verschieden Tokenisierung unterschiedlich Tokenzahl führen verhindern fehl toolspezifisch werten Type potenziell Fehler mindestens Tools Type lemmatisieren Lemma Liste Wörterbuch finden tab spalen Pfail potenziell Fehler regelbasieren Fehlertype eingeteilen fahren regelbasiert Typologisierung Typ postulieren ausgehend manuell inspektionen Typ Regel formulieren Regel daraufhin spezifisch Reihenfolge anwenden Mehrfachtypisierunge Reihenfolge Regelanwendung mithin Konsequenz Menge jeweils Identifiziert vorkommnissen Reihenfolge lauten Contract punc Short Comp prefixed Epithesis Typendefinitione führen tab Types Lyrik Types Prosa identifizieren tab Identifiziert typen Gruppe zusammengefassen punc Short überwiegend unterhalb Wortebene anzusiedelnd Zeichen meist rausch Lyrik Prosa vergleichbar Umfang auftauchen dokumentieren ebenfalls Lyrik Prosa vergleichbar ausgeprägt Effekt Type lassen Gruppe zusammenführen Comp prefixed versammeln resümieren spezifisch lyrisch Störung insbesondere zeigen feld anschlussforschung erstens erproben gut Pipelines automatisiert annotationsdat konzipieren lassen hilfreich Pipeline set evaluieren zweitens Grundlage Pipeline baselin regelbasiert Typologisierungsverfahr optimieren Manuelle Annotation Sammlung Gedicht Information abweichungscharakter jeder einzeln Wort ermöglichen Annahme Verständnis Devianz Nutzbarmachung Problem Domänenadaption operationalisieren prüfen sicherstellen Fehlertypisierung tatsächlich Spezifik literarisch erfahren Fall vorliegend,"[('fehler', 0.33259697885902656), ('lyrik', 0.21251093509481192), ('pipeline', 0.1912128425519296), ('wörterbuch', 0.1700087480758495), ('tab', 0.1700087480758495), ('lemmata', 0.16227024879882304), ('potenziell', 0.1432787916002277), ('types', 0.13923357769499772), ('type', 0.13218485730875323), ('verb', 0.12750656105688712)]"
2022,DHd2022,RATH_Brigitte_Muster_von__you__und__thou___Modellierung_der_.xml,"Muster von ""you"" und ""thou"" Modellierung der Anrede im englischen Sonett","Brigitte Rath (Universität Innsbruck, Austria)","Machine Learning Prediction Modelle, Lyrik, Anrede","Annotieren, Stilistische Analyse, Metadaten, Personen, Text","Bekanntermaßen unterscheidet das Early Modern English zwei Reihen von Pronomina der zweiten Person Singular: V-Formen (you, your etc.) und T-Formen (thou, thy etc.) Die T-Formen gehen im Zuge des Sprachwandels zum Modern English im Verlauf des 17. Jahrhunderts verloren (vgl. z.B. Lass 1999: 153), bleiben jedoch in der Lyrik erhalten. So beginnt etwa ein berühmtes, 1850 von Elizabeth Barrett Browning veröffentlichtes Sonett mit diesem Vers: ""How do I love thee? Let me count the ways."" Diese wie selbstverständliche Verwendung der T-Formen in der englischsprachigen Lyrik auch weit nach dem Wandel zum Modern English ist bisher nicht systematisch untersucht. Dieses Projekt sucht daher mit Hilfe digitaler Methoden Antworten auf folgende zwei Forschungsfragen: (1) Ist die Verwendung von T-Formen und V-Formen in Sonetten nach dem Sprachwandel synonym? (2) Falls nein: Welche Muster lassen sich beschreiben? Die Hypothese zur Frage (1) lautet, dass die Verwendung von T-Formen und V-Formen sich als nicht synonym erweisen wird, weil es gerade in der Lyrik eine gesteigerte Sensibilität für die Anrede gibt, und so zu erwarten steht, dass die linguistisch gebotenen Möglichkeiten für Differenzierung voll ausgeschöpft werden. Diese Erwartung widerspricht dem in Gedichtkommentaren häufig anzutreffenden und üblicherweise nicht weiter belegten Hinweis, ""thou"" sei einfach eine in Gedichten anzutreffende Version von ""you"". Hypothesen für Faktoren, die eine Rolle bei der Frage (2) interessierenden Musterbildung spielen könnten, werden vor allem aus der historischen Soziolinguistik gewonnen: So kommen neben potentiellen individuellen Vorlieben von Autor:innen sowie der Entstehungszeit als plausible Faktoren nominale Anredeformen in Frage, weil Studien aus der historischen Soziolinguistik nahelegen, dass bestimmte Anreden (z.B. ""terms of endearment"") mit der Verwendung von T-Formen verbunden sind (vgl. Nevala 2004: 2146; Mazzon 2010), sowie die jeweilige Kategorie des:der Angesprochenen, weil Studien einen Zusammenhang zwischen bestimmten Kategorien von Angesprochenen wie etwa Kinder, Tiere oder Geister und der Verwendung von T-Formen zeigen (vgl. Yang 1991: 258; Carter/McRae 2002: 120-121). Basis für diese Untersuchung ist ein selbsterstelltes Korpus von (bisher) 1.611 englischsprachigen, auf den britischen Inseln zwischen 1530 und 1910 publizierten Sonetten, für das die Gedichttexte manuell in TEI-5 konformem XML transkribiert und mit Metadaten (Autor:in, Titel, Entstehungsjahr, Publikationsjahr) und Annotationen zu nominalen Anredeformen, Kategorie der Adressat:innen (Gott, Mensch, Tier, Naturphänomen etc.), intertextuellen Verweisen und Reimschemata angereichert werden. Mit diesem Korpus wurde eine Reihe von Experimenten mit Machine Learning Prediction Modellen gemacht. Mit fünf verschiedenen Machine Learning Prediction Modellen (Naive Bayes, Support Vector Machine, Decision Tree, Random Forest und XGBoost) wurde jeweils der k-fold cross validation approach (vgl. z.B. Han, Pei, Kamber 2011) durchgeführt. Die jeweiligen Trefferquoten wurden mit drei Baseline-Modellen (ZeroR sowie zwei Modellen, die alle Sonette jeweils einer Klasse zuordnen, hier: AlwaysT und AlwaysV) auf der Basis üblicher Standardwerte für Machine Learning verglichen: Precision, Recall, FMeasure, Accuracy und Area Under the ROC Curve (AUC). (vgl. z.B. Han, Pei, Kamber 2011; Mohri, Rostamizadeh, Talwalkar 2012) Es zeigt sich, dass Machine Learning Modelle, insbesondere XGBoost, bessere Ergebnisse als die Baseline-Modelle für die Vorhersage liefern können. Da dieses Modell einen hohen Anteil an Fällen korrekt zuordnet, folgt, dass das Modell Regeln in der Verteilung von T- und V-Formen erkennt, dass T- und V-Formen im Sonett also nicht austauschbar sind. Auf der Basis dieser Experimente kann so die erste Frage, ""Ist die Verwendung von T-Formen und V-Formen in Sonetten nach dem Sprachwandel synonym?"" tentativ mit nein beantwortet werden. Für Hinweise auf mögliche Faktoren, die die Musterbildung beeinflussen, wurden mit den Machine Learning Prediction Modellen Ablation-Experimente durchgeführt: Input-Faktoren wurden individuell entfernt und die jeweilige Performanz des Modells erneut gemessen. Sinkt die Vorhersagekraft des Modells durch das Entfernen eines Faktors, so spielt dieser Faktor für die Vorhersage dieses Modells eine Rolle, was als ein erstes Indiz dafür gewertet werden kann, dass der entsprechende Faktor zur Musterbildung auch jenseits des Modells beitragen könnte. Es zeigt sich, dass dabei die Verwendung des Pronomens ""ye"", das bisher bei der Entwicklung von V- und T-Formen kaum beachtet wird, eine wichtige Rolle spielt; als weiterer möglicher Faktor erweist sich die Kategorie der Angesprochenen. Das Projekt bietet für die historische Linguistik einen Beitrag zur präziseren Beschreibung der Sprachentwicklung. Für die Literaturwissenschaft erlaubt diese erstmalige systematische Beschreibung der Verteilung von T-Formen und V-Formen in englischsprachigen Sonetten bessere Gedichtinterpretationen, weil sie erstens überhaupt ein Augenmerk auf die verwendeten Pronomen der Anrede legt und zweitens die im Einzeltext gewählten Formen nun vor dem Hintergrund eines Musters gelesen werden können. Das Projekt trägt so zur aktuellen Forschungsdiskussion zur Anrede in der Lyrik bei. (vgl. z.B. Culler 1981, Culler 2015, Hedley 2009, Keniston 2006, Pollard 2012, Waters 2012) Dieses Projekt wurde vom Vizerektorat Forschung der Universität Innsbruck mit Mitteln aus der Aktion D. Swarovski und vom Forschungszentrum Digital Humanities der Universität Innsbruck durch Mittel aus dem DI4DH Programm unterstützt und so erst ermöglicht. Die Korpuserstellung übernahmen mit einem ebenso scharfen Blick fürs Detail wie für das Gesamtprojekt Marina Höfler, Serena Obkircher und Teresa Wolf. Die Machine Learning Prediction Models wurden mit großer Umsicht von Ario Santoso und Mingzi Kong konzipiert, implementiert und trainiert.",de,bekanntermaßen unterscheiden early modern english Reihe Pronomina Person Singular you your Thou thy Zug Sprachwandel Modern english Verlauf Jahrhundert verlieren Lass bleiben Lyrik erhalten beginnen berühmt Elizabeth Barrett Browning veröffentlicht Sonett Ver how do i love Thee let -- count -- ways selbstverständlich Verwendung englischsprachig Lyrik Wandel Modern english systematisch untersuchen Projekt suchen Hilfe digitaler Methode Antwort folgend Forschungsfrag Verwendung Sonette Sprachwandel Synonym falls Muster lassen beschreiben Hypothese Frage lauten Verwendung synonym erweisen Lyrik gesteigert Sensibilität Anrede erwarten stehen linguistisch geboten Möglichkeit Differenzierung voll ausschöpfen Erwartung widersprechen gedichtkommentaren häufig anzutreffend üblicherweise belegt Hinweis thou einfach gedicht anzutreffend Version you Hypothesen Faktor Rolle Frage interessierend Musterbildung spielen können historisch Soziolinguistik gewinnen potentiell individuell vorlieben Autor innen Entstehungszeit plausibel Faktor Nominale anredeformen Frage Studie historisch Soziolinguistik nahelegen bestimmt Anred Terms -- Endearment Verwendung verbinden nevala Mazzon jeweilig Kategorie angesprochen Studie Zusammenhang bestimmt kategorien Angesprochen Kind Tier Geist Verwendung zeigen Yang Carter mcrae Basis Untersuchung selbsterstellt Korpus englischsprachigen britisch inseln publiziert soneten Gedichttexte manuell konformem xml transkribieren Metadat Autor Titel Entstehungsjahr Publikationsjahr annotatio Nominale anredeformen Kategorie Adressat innen Gott Mensch Tier naturphänomen intertextuell verweisen Reimschemata angereicheren Korpus Reihe experimenten Machine Learning Prediction modellen verschieden Machine Learning Prediction modellen naiv Bayes Support vector Machine Decision Tree random Forest xgboost jeweils Cross Validation approach han pei kamber durchführen jeweilig Trefferquot Zeror Modell Sonette jeweils Klasse zuordnen Alwayst Alwaysv Basis üblich Standardwerte Machine Learning vergleichen Precision Recall fmeasuren accuracy area under The Roc Curve auc han pei kamber mohri Rostamizadeh Talwalkar zeigen machin Learning Modell insbesondere xgboost gut Ergebnis Vorhersage liefern Modell hoch Anteil Fall korrekt zuordnen folgen Modell regeln Verteilung erkennen Sonett austauschbar Basis Experimente Frage Verwendung Sonette Sprachwandel Synonym tentativ beantworten Hinweis möglich Faktor Musterbildung beeinflussen Machine Learning Prediction modellen durchführen individuell entfernt jeweilig performanz Modell erneut messen sinken Vorhersagekraft Modell entfernen Faktor spielen Faktor Vorhersage Modell Rolle Indiz werten entsprechend Faktor Musterbildung jenseits Modell beitragen zeigen Verwendung Pronomen ye Entwicklung beachten wichtig Rolle spielen weit möglich Faktor erweisen Kategorie Angesprochen Projekt bieten historisch Linguistik Beitrag präziser Beschreibung Sprachentwicklung Literaturwissenschaft erlauben erstmalig systematisch Beschreibung Verteilung englischsprachig sonetten gut Gedichtinterpretation erstens Augenmerk verwendet Pronome Anrede legen zweitens Einzeltext gewählt Form Hintergrund Muster lesen Projekt tragen aktuell Forschungsdiskussion Anrede Lyrik Culler culler Hedley Keniston Pollard waters Projekt Vizerektorat Forschung Universität innsbruck Mittel Aktion Swarovski Forschungszentrum Digital Humanitie Universität Innsbruck Programm unterstützen ermöglichen Korpuserstellung übernehmen scharf Blick für Detail Gesamtprojekt Marina Höfler Serena Obkircher Teresa Wolf Machine Learning Prediction models Umsicht Ario Santoso Mingzi Kong konzipieren implementieren trainieren,"[('faktor', 0.2028793720473492), ('machine', 0.18434604186726208), ('learning', 0.17783329032590603), ('modell', 0.17113071423915605), ('prediction', 0.17048433672458527), ('verwendung', 0.16103865115889462), ('musterbildung', 0.15704291551502386), ('anrede', 0.15704291551502386), ('sonette', 0.15704291551502386), ('lyrik', 0.15612520951975237)]"
2022,DHd2022,SCHMIDT_David_Adapting_Coreference_Algorithms_to_German_Fair.xml,Adapting Coreference Algorithms to German Fairy Tales,"David Schmidt (Universität Würzburg, Germany); Markus Krug (Universität Würzburg, Germany); Frank Puppe (Universität Würzburg, Germany)","Koreferenzauflösung, Märchen, Domänenadaption","Inhaltsanalyse, Literatur, benannte Entitäten (named entities), Text","Coreference Resolution has been posing an ongoing challenge to researchers for more than 50 years. It is the task of grouping mentions (concrete or abstract references, represented as textual spans) into clusters representing entities. The approaches for solving this problem have been manifold and range from rule-based approaches (Lee et al., 2013) over classical machine learning approaches (Rahman and Ng, 2009) to modern approaches based on Deep Learning (Lee et al., 2017; Joshi et al., 2020). Coreference Resolution can act as a ""glue"" between information that is extracted on a local level (usually sentences) in order to obtain representations for an entire document or a collection of documents. This enables many interesting downstream applications such as the creation of character networks (Elson et al., 2010; Krug, 2020) or tracking of events involving central objects in textual media (such as the dagger in Emilia Galotti) (Hatzel and Biemann, 2021). The transfer of existing approaches to new domains or types of text usually comes with a drop in performance. In this work, we examine the performance of a rule-based and an end-to-end Deep Learning algorithm and their adaptability to the domain of German fairy tales. These experiments should provide insight into: a) the drop experienced from one kind of texts to another b) the reliability of state-of-the-art Deep Learning approaches compared to rule-based approaches for a change of texts and c) Capabilities for the adaptation to mitigate this natural drop in performance. This helps to estimate the required amount of manual work that is to be expected when transferring to a new kind of text, especially when the new type features a low number of annotated documents. For this we use fragments of German novels provided from the DROC corpus (Krug et al., 2018) and as target domain we make use of annotated fairy tales by the Brothers Grimm. In the next section we present our data, followed by the coreference algorithms as well as the methods for the domain adaptation in more detail. We conclude the paper by presenting and discussing the results of our experiments and potential follow up work. There are several recent works that evaluate the performance of coreference resolution models when applied to a different domain than they have been trained on. Srivastava et al. (2018) examine the performance of several coreference resolution systems (rule-based, statistical and projection-based) on English and German out-of-domain data and find that the rule-based system is the best choice for their use cases. Han et al. (2021) train a coreference resolution model based on c2f (Lee et al., 2018) and SpanBERT (Joshi et al., 2020) on two different corpora, Ontonotes (Hovy et al., 2006) and their new corpus FantasyCoref. They then evaluate both models on FantasyCoref and find that the model trained on the same domain outperforms the other one. (Toshniwal et al., 2021) examine the generalization capabilities of coreference models by evaluating the performance of longdoc (Toshniwal et al., 2020) on out-of-domain data using several English datasets and find that models which have been trained on several datasets jointly perform better than those trained on a single dataset. The data sets for our experiments were the DROC corpus (Krug et al., 2018), comprising 90 fragments of German novels, and 46 tales from the seventh edition of the Children""s and Household Tales by the Brothers Grimm Table 1 shows some statistics about the mentions in the documents of DROC and the fairy tales. One can see that a document in DROC is on average about twice as long as a document of the fairy tales. Names are used a lot less often in fairy tales, while the usage of noun phrases increases and that of pronouns is comparable. There is also an important difference regarding the entities that are referred to by the annotated mentions: In DROC, only human characters are annotated. In the fairy tales, animals and legendary beings (like giants) are also annotated because they are important (and sometimes the only) characters (e.g. the Wolf in A notable difference of both corpora to a lot of other corpora like OntoNotes (Hovy et al., 2006) and LitBank (Bamman et al., 2020) is how the mentions are annotated: OntoNotes annotates the maximal extent of a span (e.g. ""[eine kleine süße Dirne]"") while DROC and the fairy tales only annotate the heads (""eine kleine süße [Dirne]""). For the experiments, DROC was split (a fix split) into a training set and a test set in a ratio of 80% to 20%: 72 documents for training and 18 for evaluation. The fairy tales were evaluated via five-fold cross validation. In order to assess the capabilities of domain adaptation from German novels to German fairy tales, we made use of a rule-based coreference resolution system and a model based on neural networks. We briefly present both methods followed by the way of adaptation. The rule-based approach we use is an adaptation of the sieves algorithm by (Lee et al., 2013) to German (Krug et al., 2015). It partitions its rules into so-called sieves, which are ordered by the precision of their rules and applied one after the other to a document. This enables the rules to make use of the decisions of previously applied rules. Most rules use string matching to resolve names and noun phrases. Among the first sieves is also one that uses information about direct speeches to resolve all first person pronouns to the speaker and all second person pronouns to the addressee, and another that resolves relative and reflexive pronouns based on dependency parse trees. All other pronouns are resolved at the end since they do not possess much helpful information and can only be resolved unreliably (compared to a lot of names and noun phrases). As Deep Learning architecture, we decided to use c2f (Lee et al., 2018) The adaptation of both approaches was done as follows: Rule-based approach: Most rules in the sieves algorithm previously skipped family relation words and did not try to resolve them to an antecedent. In fairy tales, family relation words are most often unique (e.g. there is only one character called mother and one called father), so family relation words now are resolved to an antecedent if they are preceded by a definite article. Reflexive pronouns are resolved with the help of a dependency parse tree, which was not possible for several reflexive pronouns in the fairy tales. These are now resolved together with most other pronouns (lacking information about gender and number, hardly any antecedent can be ruled out, so they are usually resolved to the first that is checked). In addition to that, there were a few small changes that were done as the result of an error analysis on the fairy tales but are not motivated by the domain (they would probably also slightly improve the results on DROC). Deep Learning approach: We trained and evaluated three variants of the c2f algorithm: c2f trained on DROC (c2f D) for 75000 steps, c2f trained on the fairy tales (c2f FT) for about 50000 steps and c2f pre-trained on DROC for 75000 steps and fine-tuned on the fairy tales for an additional 20000 steps (c2f D+FT) Table 2 displays the results of the sieves algorithm (old and adapted version) and c2f (trained on DROC, the fairy tales or both) on DROC (first two rows) and the fairy tales. As metrics we use MUC (Vilain et al., 1995), B¬≥ (Bagga and Baldwin, 1998), CEAF The results show multiple interesting aspects: We have shown that domain adaptation of both, a rule-based system and a Deep Learning based system, yields substantial improvements to coreference resolution on a target domain (in our case fairy tales). The evaluation also opens possibilities for further combination of the results of the rule-based system and the Deep Learning based system, which we leave for further work.",en,coreference resolution pose ongoing challenge researcher year task group mention concrete abstract reference represent textual span cluster represent entity approach solve problem manifold range rule base approach lee et al classical machine learn approach rahman ng modern approach base deep learning lee et al joshi et al coreference resolution act glue information extract local level usually sentence order obtain representation entire document collection document enable interesting downstream application creation character network elson et al krug tracking event involve central object textual medium dagger emilia galotti hatzel biemann transfer exist approach new domain type text usually come drop performance work examine performance rule base end end deep learning algorithm adaptability domain german fairy tale experiment provide insight drop experience kind text b reliability state art deep learning approach compare rule base approach change text c capability adaptation mitigate natural drop performance help estimate require manual work expect transfer new kind text especially new type feature low number annotated document use fragment german novel provide droc corpus krug et al target domain use annotated fairy tale brother grimm section present datum follow coreference algorithm method domain adaptation detail conclude paper present discuss result experiment potential follow work recent work evaluate performance coreference resolution model apply different domain train srivastava et al examine performance coreference resolution system rule base statistical projection base english german domain datum find rule base system good choice use case han et al train coreference resolution model base lee et al spanbert joshi et al different corpora ontonote hovy et al new corpus fantasycoref evaluate model fantasycoref find model train domain outperform toshniwal et al examine generalization capability coreference model evaluate performance longdoc toshniwal et al domain datum english dataset find model train dataset jointly perform well train single dataset datum set experiment droc corpus krug et al comprise fragment german novel tale seventh edition household tale brother grimm table show statistic mention document droc fairy tale document droc average twice long document fairy tale name lot fairy tale usage noun phrase increase pronoun comparable important difference entity refer annotated mention droc human character annotate fairy tale animal legendary being like giant annotate important character wolf notable difference corpora lot corpora like ontonote hovy et al litbank bamman et al mention annotate ontonote annotate maximal extent span eine kleine süße dirne droc fairy tale annotate head eine kleine süße dirne experiment droc split fix split training set test set ratio document training evaluation fairy tale evaluate fold cross validation order assess capability domain adaptation german novel german fairy tale use rule base coreference resolution system model base neural network briefly present method follow way adaptation rule base approach use adaptation sieve algorithm lee et al german krug et al partition rule call sieve order precision rule apply document enable rule use decision previously apply rule rule use string matching resolve name noun phrase sieve use information direct speech resolve person pronoun speaker second person pronoun addressee resolve relative reflexive pronoun base dependency parse tree pronoun resolve end possess helpful information resolve unreliably compare lot name noun phrase deep learning architecture decide use lee et al adaptation approach follow rule base approach rule sieve algorithm previously skip family relation word try resolve antecedent fairy tale family relation word unique character call mother call father family relation word resolve antecedent precede definite article reflexive pronoun resolve help dependency parse tree possible reflexive pronoun fairy tale resolve pronoun lack information gender number hardly antecedent rule usually resolve check addition small change result error analysis fairy tale motivate domain probably slightly improve result droc deep learning approach train evaluate variant algorithm train droc d step train fairy tale ft step pre train droc step fine tune fairy tale additional step table display result sieve algorithm old adapted version train droc fairy tale droc row fairy tale metric use muc vilain et al bagga baldwin ceaf result multiple interesting aspect show domain adaptation rule base system deep learning base system yield substantial improvement coreference resolution target domain case fairy tale evaluation open possibility combination result rule base system deep learning base system leave work,"[('tale', 0.3475562101698743), ('fairy', 0.34276489158227325), ('rule', 0.2743877814369105), ('al', 0.2511230440728272), ('base', 0.22165236265157748), ('resolve', 0.216471087337467), ('droc', 0.21149910940823377), ('domain', 0.16840756850249478), ('coreference', 0.1586243320561753), ('pronoun', 0.1528751506916479)]"
2022,DHd2022,GÖGGELMANN_Michael_Auf_den_Spuren_einer_altnordischen_Saga__.xml,Auf den Spuren einer altnordischen Saga-Östhetik Poetologische Aussagen in den Erzählerbemerkungen der Isländersagas,"Michael Göggelmann (Universität Tübingen, Germany); Anna Katharina Heiniger (Universität Tübingen, Germany); Nils Reiter (Universität Köln, Germany); Angelika Zirker (Universität Tübingen, Germany)","Annotation, Östhetik, Isländersagas, Literarisierung, Poetologie","Annotieren, Stilistische Analyse, Visualisierung, Literatur, Text, Visualisierung","Der Vortrag stellt die systematischen Annotationen von Erzählerbemerkungen in den anonym überlieferten, mittelalterlichen Während narrative Texte und deren systematische Annotation bereits vielfach Untersuchungsobjekt innerhalb der Digital Humanities waren (Zinsmeister 2016; Gius/Jacke 2017; Adelmann et al. 2018; Ketschik et al. 2020), zeigt sich das Innovationspotenzial der vorliegenden Studie in zweierlei Hinsicht: es wird sowohl das bislang quantitativ gänzlich unerschlossene Die ca. 40 überlieferten Obwohl sich die Bei intratextuellen Verweisen handelt es sich um in der Forschung bislang kaum beachtete Phänomene in den Erzählerbemerkungen, die wir als Mittel des produktiven Austauschs zwischen der intradiegetischen literarischen Praxis und der extradiegetischen Welt des Publikums betrachten. Um diese zu sammeln, zu systematisieren und zu kontextualisieren sowie im Hinblick auf die narrative (Selbst-)Reflexion in den Isländersagas auszuwerten, wurden deshalb solche Öußerungen der Erzählstimme als Ausgangspunkt gewählt, die sich mit dem Erzählen selbst befassen. In Vorarbeiten zu dieser Studie wurden Erzählerbemerkungen in den Isländersagas in fünf Kategorien eingeteilt, die ihrerseits die Grundlage für die ersten Annotationsrichtlinien bilden. In der vorliegenden Studie liegt das Augenmerk auf vier näher untersuchte Sagas. Bereits zu Beginn des Annotationsprozesses (wobei wir der Anleitung in Reiter 2020 folgten) wurde deutlich, dass diese für eine produktive Umsetzung in mehreren Schritten geschärft und durch zusätzliche Kategorien ergänzt und ausdifferenziert werden müssen. Die Überarbeitung der Richtlinien ist bisher in fünf aufeinanderfolgenden Runden vorgenommen worden, so dass diese nun eine erste stabile Form mit sechs Annotationskategorien und meist mehreren Unterkategorien erreichten. Nachfolgend konzentrieren wir uns aus Platzgründen auf die Kategorie der intratextuellen Bezüge 'die Annotationen der anderen Kategorien Die Annotation der Erzählerbemerkungen in den Isländersagas wurde mit Hilfe der Software CorefAnnotator (Reiter 2018) vorgenommen. Die Kategorie umfasst alle intratextuellen Bezüge, die die Erzählstimme in einer Saga herstellt und gehört zu den am häufigsten annotierten Kategorien. In mehreren Unterkategorien werden bei der Annotation der Sagas verschiedene Arten intratextueller Verweise erfasst. Auf der intratextuellen Ebene nimmt die Erzählstimme eine narrative Selektion vor, erinnert an frühere Geschehnisse, kündigt Geschehnisse an, die erst noch erzählt werden und informiert darüber, welche Figuren neu eingeführt werden oder für die weitere Handlung keine Rolle mehr spielen. Als Beispiele dieser Kategorie lassen sich oft verwendete Phrasen wie ""sem fyrr var sagt"" (Laxd≈ìla saga: S. 71; ""Wie zuvor erzählt wurde""), Ebenfalls zu den intratextuellen Verweisen zählen häufig verwendete formelhafte Phrasen. Mit Phrasen dieser Art werden zum einen neue Figuren eingeführt (""M[a√∞r] er nefndr B√°r√∞r Heyangrs-Bjarnarson"" (B√°r√∞ar saga Sn√¶fells√°ss: S. 107) Eine weitere intratextuelle Spezifizierung ist die Vorahnung ( Die folgenden Analysen wurden auf Basis der bisherigen manuellen Annotationen vorgenommen. Zum jetzigen Zeitpunkt wurden vier Sagas vollständig annotiert, die in der Forschung als ""randständig"" innerhalb der Gattung der Die unterschiedlich langen Sagas machen einen direkten Vergleich der absoluten Zahlen von Annotationen in ihnen schwierig; in den folgenden Auswertungen werden Häufigkeiten daher normalisiert. Wir betrachten zunächst die Häufigkeit der Annotationen (Abb. 1). Bei den Diese Auswertung der vier Sagas deutet somit darauf hin, dass die Erzählerbemerkungen in jeder Saga ein eigenständiges Profil bilden. Die Hypothese, nach der wir den Isländersagas eine individuelle Ausgestaltung trotz den allen gemeinsamen Typen von Erzählerkommentaren attestierten, konnte also bereits an dieser Stelle plausibilisiert werden. Einen visuellen Eindruck von der Verteilung der Annotationen im Textverlauf liefert Abb. 2. Jeder senkrechte Strich markiert dabei die Annotation eines intratextuellen Verweises, wobei die verschiedenen Farben die Unterkategorien der intratextuellen Bezüge repräsentieren. Grundsätzlich verteilen sich die Annotationen, wie zu erwarten war, über den gesamten Text. Die sich dazwischen befindlichen, teilweise recht großen Lücken sollen kapitelweise anhand der Annotationsdichte nachfolgend genauer untersucht werden. Die Dichte der Annotationen wird in Abb. 3 gezeigt. Für jedes Kapitel und jede Kategorie ergibt sich dabei ein Datenpunkt, die Datenpunkte einer Kategorie sind dann durch Linien verbunden. Die Dichte der Annotationen ist hierbei als relative Anzahl an Annotationen pro Kapitel definiert. Anhand der Grafiken lassen sich die Höhe- und Wendepunkte der jeweiligen Saga ablesen und nachvollziehen. Die ersten zwei großen Ausschläge der Die hier für die Zu bedenken ist, dass Abbildung 3 gegenwärtig ausschließlich die Annotationsdichte der intratextuellen Verweise zeigt. Um ein umfassendes Bild der Annotationsverteilung zu erhalten, müssen in einem nächsten Analyseschritt auch die anderen Annotationskategorien berücksichtigt werden. Daran anschließend wird sich zeigen, ob die Schlüsselstellen in erster Linie mit intratextuellen Kommentaren versehen, oder ob diese auch mit anderen Arten der Erzählerkommentare gekoppelt sind. Durch die vorliegende quantitative Analyse auf Grundlage systematischer Annotationen konnten für die Erzählstimme der Isländersagas Textmerkmale aufgezeigt werden, die zwar zuvor im Einzelfall erkannt, aber nicht im Hinblick auf einen oder mehrere Gesamttexte systematisch erfassbar waren. Mithilfe ihrer visualisierten Distributionen wurde eine neue Perspektive auf die Erzählerbemerkungen geschaffen, deren textübergreifende Bedeutsamkeit bislang nicht erkannt worden war. So lässt die vorliegende Untersuchung annehmen, dass die Distribution der Erzählerkommentare nicht zufällig, sondern an den Handlungsverlauf der Isländersagas gekoppelt ist. Diese Verbindung aus Form und Inhalt wird in einem nächsten Schritt einerseits auf Grundlage der Ergebnisse aus der quantitativen Analyse wieder in die genaue Textanalyse ( Bereits jetzt aber zeigen sich die Erzählerbemerkungen in ihrem regelmäßigen Auftreten als Gestaltungsmittel von Höhe- und Wendepunkten der Sagas als derart prägend, dass sich diese als Teil einer Literarisierungsstrategie über Einzelbelege hinweg tatsächlich zu einer poetologischen Aussage verdichten und deren Annotation damit zur ästhetischen Verortung dieser Texte beitragen kann. Auf der Suche nach ästhetischem Potenzial in den Isländersagas trafen wir anhand unserer Methode auf differenzierte Ergebnisse, die auf ein großes Bewusstsein hinsichtlich der Literarisierung und der Östhetisierung hinweisen. Weitere Analyseschritte zielen auf eine Korpuserweiterung und eine vergleichende Auswertung der Ergebnisse über ein größeres Textkorpus hinweg sowie einer Verfeinerung der Annotations-Tools und ggf. -Kategorien. Darüber hinaus prüfen wir die Möglichkeit einer automatischen Erkennung von Erzählerkommentaren mittels maschineller Lernverfahren auf Grundlage unserer Annotationen. Bei einer hinreichenden Erkennungsrate können ggf. weitere Sagas automatisch annotiert werden. Durch die bei maschinellen Lernverfahren zunächst oft fehlerhaften Verallgemeinerungen erhoffen wir uns zudem auch weitere Einsichten in die Annotationskategorien und deren Verwendung.",de,Vortrag stellen systematisch Annotation erzählerbemerkungen Anonym überlieferen mittelalterlich narrativ Text systematisch Annotation vielfach Untersuchungsobjekt innerhalb Digital Humanitie zinsmeister Gius Jacke Adelmann et Ketschik et zeigen Innovationspotenzial vorliegend Studie zweierlei Hinsicht sowohl bislang quantitativ gänzlich unerschlossen überlieferen obwohl intratextuell verweisen handeln Forschung bislang beachtet Phänomen Erzählerbemerkung produktiv Austausch intradiegetisch literarisch Praxis extradiegetisch Welt Publikum betrachten sammeln systematisieren kontextualisieren Hinblick Narrative isländersagas auszuwerten öußerunge Erzählstimme Ausgangspunkt wählen erzählen befassen vorarbein Studie Erzählerbemerkung Isländersagas kategorien eingeteilen ihrerseits Grundlage Annotationsrichtlini bilden vorliegend Studie liegen Augenmerk nah untersuchen Sagas Beginn annotationsprozesses wobei Anleitung Reiter folgen deutlich produktiv Umsetzung mehrere Schritt schärfen zusätzlich kategorien ergänzen ausdifferenzieren Überarbeitung Richtlinie aufeinanderfolgend rund vornehmen stabil Form Annotationskategorie meist mehrere Unterkategori erreichen nachfolgend konzentrieren Platzgründe Kategorie intratextuell Bezüg annotation kategorien Annotation Erzählerbemerkung isländersagas Hilfe Software Corefannotator Reiter vornehmen Kategorie umfassen intratextuell bezüge Erzählstimm Saga herstellen gehören häufig annotierter kategorien mehrere Unterkategori Annotation Sagas verschieden Art intratextuell verweise erfasst intratextuell Ebene nehmen erzählstimmen narrativ Selektion erinnern früh geschehnisse kündigen Geschehniss erzählen informieren Figur neu einführen handlung Rolle spielen Beispiel Kategorie lassen verwendet Phras sem Fyrr var saga zuvor erzählen ebenfalls intratextuell verweisen zählen häufig verwendet formelhafte phrasen Phrase Art Figur einführen m nefndr saga ss intratextuell Spezifizierung Vorahnung folgend analyse Basis bisherig Manuelle Annotation vornehmen jetzig Zeitpunkt Sagas vollständig annotiert Forschung randständig innerhalb Gattung unterschiedlich lang Sagas direkt Vergleich absolut Zahl annotatio schwierig folgend Auswertung häufigkeien normalisieren betrachten Häufigkeit annotatio abb Auswertung Sagas deuten somit Erzählerbemerkung saga eigenständig Profil bilden Hypothese isländersagas individuell Ausgestaltung trotz gemeinsam Type erzählerkommentaren attestieren Stelle plausibilisieren visuell Eindruck Verteilung annotatio Textverlauf liefern abb senkrechter Strich markieren Annotation intratextuell Verweise wobei verschieden Farbe Unterkategori intratextuell Bezüg repräsentieren grundsätzlich verteilen Annotation erwarten gesamt Text befindlich teilweise Lücke kapitelweise anhand Annotationsdicht nachfolgend genau untersuchen Dichte annotatio abb zeigen jeder Kapitel Kategorie ergeben datenpunken datenpunkte Kategorie Linie verbinden Dichte Annotation hierbei relativ Anzahl annotatio pro Kapitel definieren anhand Grafik lassen wendepunken jeweilig saga ablesen nachvollziehen Ausschlag bedenken Abbildung gegenwärtig ausschließlich Annotationsdicht intratextuell Verweise zeigen umfassend Bild Annotationsverteilung erhalten nächster Analyseschritt Annotationskategorie berücksichtigen anschließend zeigen Schlüsselstelle Linie intratextuell kommentaren versehen Art erzählerkommentar koppeln vorliegend quantitativ Analyse Grundlage systematisch annotatio erzählstimme Isländersagas Textmerkmale aufzeigen zuvor Einzelfall erkennen Hinblick mehrere gesamttext systematisch erfassbar Mithilfe Visualisiert distributionen Perspektive Erzählerbemerkung schaffen textübergreifend Bedeutsamkeit bislang erkennen lässen vorliegend Untersuchung annehmen Distribution erzählerkommentare zufällig Handlungsverlauf Isländersagas koppeln Verbindung Form Inhalt nächster Schritt einerseits Grundlage Ergebnis quantitativ Analyse genau Textanalyse zeigen Erzählerbemerkung regelmäßig auftreten Gestaltungsmittel wendepunken Sagas derart prägend Literarisierungsstrategie Einzelbelege Hinweg tatsächlich poetologisch Aussage verdichten Annotation ästhetisch Verortung Text beitragen Suche ästhetisch Potenzial isländersagas treffen anhand Methode differenziert Ergebnis Bewusstsein hinsichtlich Literarisierung Östhetisierung hinweisen analyseschritte zielen Korpuserweiterung vergleichend Auswertung Ergebnis größeres Textkorpus hinweg Verfeinerung hinaus prüfen Möglichkeit automatisch Erkennung Erzählerkommentar mittels Maschineller lernverfahren Grundlage Annotation hinreichend erkennungsrat Sagas automatisch annotiert maschinell lernverfahren fehlerhaft Verallgemeinerunge erhoffen zudem Einsicht Annotationskategorie Verwendung,"[('intratextuell', 0.46809238130232395), ('sagas', 0.2978769699196607), ('isländersagas', 0.2978769699196607), ('erzählerbemerkung', 0.25532311707399485), ('saga', 0.21276926422832906), ('annotation', 0.1501238326996152), ('annotatio', 0.13426705603760064), ('unterkategori', 0.1189070635370721), ('verweise', 0.10394115548461987), ('annotationskategorie', 0.10061285095533636)]"
2022,DHd2022,PICHLER_Axel_Vom_Begriff_über_das_Phänomen_zur_Analyse___ein.xml,Vom Begriff über das Phänomen zur Analyse Ein CRETA-Workshop zur Operationalisierung in den DH,"Melanie Andresen (University of Stuttgart, Germany); Benjamin Krautter (Heidelberg University, Germany); Janis Pagel (University of Stuttgart, Germany); Axel Pichler (FU Berlin, Germany)","Operationalisierung, Modellierung, Textanalyse","Inhaltsanalyse, Modellierung, Annotieren, Theoretisierung, Methoden, Forschungsprozess","Der Workshop stellt eine weiterentwickelte und personell anders besetzte Version des auf der DHd 2020 abgehaltenen, beinahe gleichnamigen Workshops dar. Er adressiert eine der zentralen Herausforderungen für Arbeiten in den Digital Humanities 'die Operationalisierung geisteswissenschaftlicher Konzepte und Fragestellungen für computergestützte Forschungsansätze (vgl. Jannidis 2010:¬†109–132; Moretti 2013; Flanders/Jannidis 2015; Jacke 2014:¬†118–139; Pichler/Reiter 2020, Pichler/Reiter 2021). Während Geisteswissenschaftler*innen vor allem mit komplexen, häufig mehrere Textphänomene umfassenden Konzepten arbeiten und als relevant erachtete Kontexte zu deren Deutung heranziehen, ist die computergestützte Arbeit an identifizierbare Phänomene auf der Textoberfläche gebunden. Die hieraus erwachsende Diskrepanz zwischen theoretischen Erwartungen und konkreten Ergebnissen gilt es über eine adäquate Operationalisierung zu überbrücken (vgl. Moretti 2013:¬†1). Ziel ist es also, Verfahren zu entwickeln, die theoretische Begriffe über potenziell mehrere Teilschritte auf Textoberflächenphänomene zurückführen. Oder kurz gesagt: Als Anwendungsfälle stellen wir Phänomene vor, zu denen wir im Rahmen des ""Center for Reflected Text Analytics"" e.V. (CRETA) In einem ersten Anwendungsfall befassen wir uns mit dem Konzept der Entität und ihrer Referenz in literarischen Texten (vgl. Reiter u.a. 2017:¬†19–22; Blessing u.a. 2020). Dabei fassen wir den Begriff der Entität sehr weit: ""Alles, was man als Einheit denken kann, kann als Entität behandelt werden"" (Jannidis 2017:¬†103). Zu den Entitäten zählen dementsprechend Personen/Figuren, Orte, Organisationen sowie Ereignisse. Das Konzept ist also für verschiedene Forschungsfragen anschlussfähig. Auf Entitäten kann auf verschiedene Weise referiert werden, etwa über Eigen- und Gattungsnamen (z.¬†B. ""Angela Merkel"", ""die Kanzlerin""). Um Entitäten in einem Text zu extrahieren, müssen folglich die Entitätenreferenzen annotiert und kookkurrente Ausdrücke aufgelöst werden. Die Herausforderungen bestehen vor allem in der Festlegung der Referenzausdrücke (welche Ausdrücke werden berücksichtigt?), in der Abgrenzung von Entitätenreferenzen gegenüber generischen Ausdrücken sowie im Umgang mit Verschachtelungen, Metonymien und textspezifischen Besonderheiten. Der zweite Anwendungsfall setzt sich mit der Identifikation von Protagonisten im Drama auseinander, fokussiert also ein holistisches Textphänomen. Die verschiedenen Perspektiven der Literaturwissenschaft auf Protagonisten, Hauptfiguren und Helden von Dramen (vgl. die Ausführungen in Krautter u.a. 2018:¬†6–16 und Wulff 2002:¬†431–448) haben zur Folge, dass eine Reihe von Definitionen und Identifikationsstrategien koexistieren, die häufig an historische Normvorstellungen geknüpft sind. Diese historische Gebundenheit erschwert die operationale Definition von Protagonisten, wenn man auf größere Abschnitte der Literaturgeschichte blickt. Direkt anschlussfähig für die Methoden der Digital Humanities erscheint die in den späten 1970er Jahren von Manfred Pfister skizzierte Annahme, dass ""quantitative[] Dominanzrelationen"" (Pfister 2001:¬†227) hilfreich für die Differenzierung des Bühnenpersonals seien. Pfister nennt zwei Kriterien, die dabei helfen können, dramatische Figuren schon aufgrund quantitativer Eigenschaften als Haupt- oder Nebenfiguren zu identifizieren: nämlich die Zeitdauer, die sie auf der Bühne stehen, und ihr Anteil an der gesamten Figurenrede (vgl. Pfister 2001:¬†226–227). Diese Auffassung Pfisters lässt sich mit digitalen Methoden der Dramenanalyse um weitere Eigenschaften der Figuren, etwa durch Netzwerkmetriken oder Topic Modeling, zu einem multidimensionalen Ansatz ergänzen. Die größte Herausforderung stellt hierbei die Validierung der Ergebnisse dar, da diese an die Gültigkeit der operationalen Definition für die manuelle Annotation gebunden ist. Im Workshop stellen wir zwei Ansätze zur Operationalisierung vor, die sich 'in verschiedenen Phasen des Forschungsprozesses 'sehr gut gegenseitig ergänzen. Der erste Ansatz besteht in der Als zweiten Ansatz stellen wir eine Vorgehensweise vor, die Zielphänomene In einem Theorieteil führen wir in Geschichte und Praxis der Operationalisierung von geisteswissenschaftlichen Fragestellungen und Konzepten für die computergestützte Analyse ein. Anhand der oben genannten Beispiele aus der CRETA-Praxis thematisieren wir die Problematik und stellen Ansätze zur Operationalisierung im Detail vor. Je nach Interesse kann anschließend, im praktischen Teil, einer dieser Anwendungsfälle ausgewählt und bearbeitet werden. Dabei haben die Teilnehmenden die Möglichkeit, beide Operationalisierungsansätze an ihrem gewählten Anwendungsfall zu erproben. Hierfür befassen sie sich zunächst mit dem Konzept, indem sie es anhand eines Textauszugs manuell annotieren und parallel stichpunktartig die Richtlinien schärfen. In einer ersten Diskussionsrunde werden die verschiedenen Ergebnisse gesammelt und diskutiert. Zur Erprobung des zweiten Ansatzes stellen wir für jeden Anwendungsfall einen Operationalisierungs-""Baukasten"" vor. Dieser besteht aus einer Sammlung von Python-Skripten in einem Jupyter-Notebook, das auf das jeweilige Untersuchungsvorhaben zugeschnitten ist und den Teilnehmenden die Möglichkeit gibt, sich dem zu untersuchenden Phänomen über computergestützte Verfahren anzunähern. Die Teilnehmenden können in Kleingruppen in diesem Baukasten verschiedene Parameter einstellen sowie manuell Eigenschaften an- oder abwählen, wobei sie auf ihr Vorwissen über den Untersuchungsgegenstand aus der ersten Praxisrunde zurückgreifen können. Nachdem die Teilnehmenden die Eigenschaften ausgewählt und ggf. parametrisiert haben, können sie die Ergebnisse visualisieren und mit den Texten abgleichen. Damit erhalten die Teilnehmenden ein direktes Feedback zu den ausgewählten Parametern und können prüfen, ob das Untersuchungsvorhaben mit den festgelegten Einstellungen angemessen umgesetzt wird. Der Baukasten ist zur iterativen Nutzung vorgesehen, sodass der Einfluss verschiedener verwandter Eigenschaften auf die Ausgaben sichtbar wird und die Teilnehmenden sich einer geeigneten technischen Umsetzung sukzessiv annähern können. In einer abschließenden Diskussion werden die Ergebnisse gesammelt und es wird ausgewertet, wie adäquat sich die jeweiligen Zielphänomene mittels der gewählten Annahmen abbilden haben lassen. Ziel unseres Workshops ist es, die Teilnehmenden für die Wichtigkeit der Operationalisierung in den Digital Humanities zu sensibilisieren und ihnen Wege zu ihrer erfolgreichen Realisierung vorzustellen. Durch die interdisziplinäre Ausrichtung von DH-Arbeiten kommt der Operationalisierung eine Schlüsselposition zu, da sie eine Brücke zwischen geisteswissenschaftlichen Konzepten und computergestützter Umsetzung schlägt (vgl. Moretti 2013:¬†1). Mit den gewählten Anwendungsfällen wollen wir den Teilnehmenden ein ""Repertoire"" für die Operationalisierung verschiedener Aufgabentypen mitgeben. Wir zeigen zum einen, dass die Annotation eines Phänomens als Methode seiner Operationalisierung dienen kann (vgl. Gius/Jacke 2017:¬†233–254); zum anderen führen wir für textbasierte Phänomene eine indirekte Operationalisierung ein (vgl. Reiter/Willand 2018). Beide Verfahrensweisen sind auf andere Anwendungsfälle übertragbar. Gleichzeitig möchten wir deutlich machen, dass es für jedes Untersuchungsvorhaben nicht nur eine, sondern verschiedene Wege der Operationalisierung gibt. Die Spielräume, die bei der Operationalisierung geisteswissenschaftlicher Fragestellungen entstehen, machen es notwendig, Entscheidungen reflektiert zu treffen, sie offenzulegen und ihren Einfluss auf die Ergebnisse als Voraussetzung für eine angemessene Interpretation zu bedenken. (insgesamt 3 Stunden + 30 Min. Pause) 1. Einführung und Ablauf (10 Min.) 2. Theoretischer Teil (insgesamt 30 Min.) ‚Ä¢ Erläuterung der Problemstellung ‚Ä¢ Vorstellung der Anwendungsfälle 3. Praktischer Teil ‚Ä¢ Einführung in die Primärtexte und Tools, Ausgabe der skizzierten Guidelines (10 Min.) ‚Ä¢ Erste Praxisrunde (Kleingruppen): Manuelle Annotation eines Phänomens, parallele Erweiterung/Überarbeitung der Guidelines, iterativ (30-40 Min.) 'Kaffeepause (30 Min.) '‚Ä¢ Sammeln der Ergebnisse und Diskussion der Herangehensweisen (20 Min.) ‚Ä¢ Zweite Praxisrunde (Kleingruppen): Arbeit am Operationalisierungsbaukasten, Feedback über Ausgabedatei, iterativ (30-40 Min.) 4. Abschlussdiskussion: Sammeln der Ergebnisse, Diskussion der Erfahrungen und Lernziele (30 Min.) Die Durchführung des Workshops auf der DHd 2020 hat gezeigt, dass das gestraffte dreistündige Format gute didaktische Resultate zeitigt. Der Fokus auf die praktischen Dimensionen der Operationalisierung ist dabei gewollt: Aus der konkreten praktischen Arbeit heraus lässt sich unserer Ansicht nach am besten der theoretische Rahmen und die theoretischen Probleme bei der Operationalisierung reflektieren. Zwischen 15 und 25 Abgesehen von Beamer und ausreichend Steckdosen ist keine besondere technische Ausstattung erforderlich. Die Teilnehmenden arbeiten im praktischen Teil an ihrem eigenen Laptop. Informationen zu eventuellen Vorab-Installationen werden rechtzeitig mitgeteilt. Der Workshop wird von Mitgliedern des Center for Reflected Text Analytics (CRETA) e.V. veranstaltet, die bereits erfahrene Workshop-Leiter*innen im DH-Bereich sind (DHd 2017,DHd 2018, ESU 2018,DHd 2019, HCH 2019,DHd 2020). CRETA konzentriert sich auf die Entwicklung von Methoden zur kritisch-reflektierten Textanalyse im Forschungsbereich der Digital Humanities. Die Methoden werden fachübergreifend für textanalytische Fragestellungen aus der Literatur-, Sprach-, Geschichts- und Sozialwissenschaft sowie Philosophie erarbeitet und eingesetzt. Das bis 2020 vom BMBF geförderte eHumanities-Zentrum ist Ende 2020 mit der Gründung eines Vereines in eine neue Phase übergegangen. Mit der Vereinsgründung wird der Tatsache Rechnung getragen, dass über Stuttgart hinaus inzwischen Wissenschaftler*innen an ähnlichen Zielen arbeiten und CRETA in vielfältiger Weise verbunden sind, etwa durch gemeinsame Projekte. Melanie Andresen Universität Stuttgart Institut für Maschinelle Sprachverarbeitung Pfaffenwaldring 5b 70569 Stuttgart Melanie Andresen ist Postdoc am Institut für Maschinelle Sprachverarbeitung an der Universität Stuttgart. Sie hat Germanistische Linguistik an der Universität Hamburg studiert und ist dort 2020 im Bereich der Korpuslinguistik promoviert worden. Aus den Projekten Benjamin Krautter Benjamin.Krautter@uni-koeln.de Universität zu Köln Institut für Digital Humanities Albertus-Magnus-Platz 50931 Köln Benjamin Krautter ist Promotionsstudent am Germanistischen Seminar der Universität Heidelberg und Mitarbeiter im Projekt Q:TRACK. Dort arbeitet er u.¬†a. an der Operationalisierung literaturwissenschaftlicher Kategorien für die quantitative Dramenanalyse. Im Zentrum seines Forschungsinteresses steht dabei die mögliche Verbindung quantitativer und qualitativer Methoden für die Analyse und Interpretation literarischer Texte. Janis Pagel Universität zu Köln Institut für Digital Humanities Albertus-Magnus-Platz 50931 Köln Janis Pagel ist Promotionsstudent am Institut für Maschinelle Sprachverarbeitung der Universität Stuttgart und Mitarbeiter am Institut für Digital Humanities der Universität zu Köln. Er studierte Germanistik und Linguistik in Bochum, sowie Computerlinguistik in Stuttgart und Amsterdam. Er forscht zu Anwendungen von computerlinguistischen Methoden auf literaturwissenschaftliche Fragestellungen und Koreferenzresolution auf literarischen Texten. Axel Pichler Universität Stuttgart Institut für Maschinelle Sprachverarbeitung Pfaffenwaldring 5b 70569 Stuttgart Axel Pichler studierte Philosophie und Germanistik in Wien und Graz. Im Sommersemester 2021 war er Gastprofessor für Digital Humanities am EXC ""Temporal Communities‚Äù der FU Berlin. Zurzeit arbeitet er als Postdoc unter anderem an der Entwicklung und Reflexion von Methoden der computergestützten Textanalyse am Institut für Maschinelle Sprachverarbeitung der Universität Stuttgart. 1.",de,Workshop stellen weiterentwickelt personell besetzt Version dhd abgehaltenen beinahe gleichnamig Workshop dar adressieren zentral Herausforderung Arbeit Digital Humanitie Operationalisierung geisteswissenschaftlich Konzept Fragestellung computergestützt Forschungsansätz Jannidis moretti Flanders Jannidis jack Pichler Reiter Pichler Reiter komplex häufig mehrere textphänomen umfassend Konzept arbeiten relevant erachtet Kontexte Deutung heranziehen computergestützt Arbeit identifizierbar Phänomen Textoberfläche binden Hieraus erwachsend Diskrepanz theoretisch Erwartung konkret Ergebnis gelten adäquat Operationalisierung überbrücken moretti Ziel Verfahren entwickeln theoretisch begriffe Potenziell mehrere Teilschritte textoberflächenphänomen zurückführen anwendungsfäll stellen Phänomen Rahmen center for reflected Text analytics Creta Anwendungsfall befassen Konzept Entität Referenz literarisch Text Reiter blessing fassen Begriff Entität Einheit denken Entität behandeln Jannidis entität zählen Person Figur Ort Organisation Ereignis Konzept verschieden Forschungsfrag anschlussfähig entität verschieden Weise referieren gattungsnamen Angela Merkel Kanzlerin Entität Text extrahieren folglich Entitätenreferenze annotiert kookkurrent Ausdrück auflösen Herausforderung bestehen Festlegung Referenzausdrücke ausdrücke berücksichtigen Abgrenzung Entitätenreferenze generisch Ausdrücken Umgang Verschachtelung Metonymi textspezifisch Besonderheit Anwendungsfall setzen Identifikation Protagonist Drama auseinander fokussieren holistisch Textphänomen verschieden Perspektive Literaturwissenschaft protagonist hauptfigur Held Dram Ausführung krautt Wulff Folge Reihe Definition Identifikationsstrategien koexistieren häufig historisch normvorstellungen knüpfen historisch Gebundenheit erschweren operational Definition Protagonist groß Abschnitte Literaturgeschichte blicken direkt anschlussfähig Methode Digital Humanitie erscheinen spät Manfred Pfister Skizzierte annahme quantitativ dominanzrelationer pfister hilfreich Differenzierung Bühnenpersonal Pfister nennen kriterien helfen dramatisch Figur aufgrund quantitativer eigenschaften Nebenfigure identifizieren nämlich Zeitdauer Bühne stehen Anteil gesamt Figurenrede pfister Auffassung Pfisters lässt digital Methode Dramenanalyse Eigenschaft Figur Netzwerkmetrike Topic Modeling multidimensional Ansatz ergänzen groß Herausforderung stellen hierbei Validierung Ergebnis dar Gültigkeit operational Definition Manuelle Annotation binden Workshop stellen Ansatz Operationalisierung verschieden Phase Forschungsprozesse gegenseitig ergänzen Ansatz bestehen Ansatz stellen vorgehensweise zielphänomen Theorieteil fahren Geschichte Praxis Operationalisierung geisteswissenschaftlich Fragestellung Konzept computergestützt Analyse anhand genannt Beispiel thematisieren Problematik Stelle Ansatz Operationalisierung Detail Interesse anschließend praktisch anwendungsfäll auswählen bearbeiten Teilnehmende Möglichkeit operationalisierungsansätzen gewählt Anwendungsfall erproben hierfür befassen Konzept anhand Textauszug manuell annotieren parallel stichpunktartig Richtlinie schärfen diskussionsrunde verschieden Ergebnis sammeln diskutieren Erprobung ansatz stellen anwendungsfall bestehen Sammlung jeweilig untersuchungsvorhaben zuschneiden Teilnehmende Möglichkeit untersuchend Phänom computergestützt Verfahren annähern Teilnehmende Kleingruppe baukasten verschieden Parameter einstellen manuell eigenschaft abwählen wobei vorwissen Untersuchungsgegenstand praxisrunde zurückgreifen Teilnehmende eigenschaft auswählen parametrisieren Ergebnis visualisieren Text abgleichen erhalten Teilnehmende direkt Feedback ausgewählt Parameter prüfen untersuchungsvorhaben festgelegt einstellungen angemessen umsetzen bauka iterativ Nutzung vorsehen sodass einfluss verschieden verwandt eigenschaften Ausgabe sichtbar Teilnehmende geeignet technisch Umsetzung sukzessiv annähern abschließend Diskussion Ergebnis sammeln auswerten adäquat jeweilig zielphänomen mittels gewählt annahmen abbilden lassen Ziel unser Workshop Teilnehmend Wichtigkeit Operationalisierung Digital Humanitie sensibilisieren Weg erfolgreich Realisierung vorstellen interdisziplinär Ausrichtung Operationalisierung Schlüsselposition Brücke geisteswissenschaftlich konzept computergestützter Umsetzung schlagen moretti gewählt anwendungsfällen Teilnehmend repertoire Operationalisierung verschieden Aufgabentype mitgeben zeigen Annotation Phänomen Methode Operationalisierung dienen Gius jack führen textbasiert Phänomen indirekt Operationalisierung Reiter Willand Verfahrensweise anwendungsfäll übertragbar gleichzeitig möchten deutlich jeder untersuchungsvorhaben verschieden Weg Operationalisierung Spielraum Operationalisierung geisteswissenschaftlich Fragestellung entstehen notwendig Entscheidung reflektieren treffen Offenzuleg einfluss Ergebnis Voraussetzung angemessen Interpretation bedenken insgesamt Stunde pause Einführung Ablauf theoretisch insgesamt erläuterung Problemstellung Vorstellung anwendungsfäll praktisch Einführung Primärtexte Tools Ausgabe skizziert guidelin praxisrund Kleingruppen manuell Annotation Phänomen parallel Erweiterung überarbeitung guidelin iterativ Kaffeepause sammeln Ergebnis Diskussion herangehensweis praxisrunde Kleingruppe Arbeit Operationalisierungsbaukasten Feedback Ausgabedatei iterativ Abschlussdiskussion sammeln Ergebnis Diskussion Erfahrung lernziel Durchführung Workshop dhd zeigen gestrafft dreistündig Format didaktisch Resultat zeitigen Fokus praktisch Dimension Operationalisierung konkret praktisch Arbeit heraus lässen Ansicht theoretisch Rahmen theoretisch Problem Operationalisierung reflektieren absehen Beamer ausreichend steckdosen besonderer technisch Ausstattung erforderlich Teilnehmende arbeiten praktisch Laptop Information eventuellen rechtzeitig mitteilen Workshop Mitglied center for reflected Text analytics creta veranstalten erfahren dhd esu hch Creta konzentrieren Entwicklung Methode Textanalyse Forschungsbereich Digital Humanitie Methode fachübergreifend textanalytisch Fragestellung Sozialwissenschaft Philosophie erarbeiten einsetzen Bmbf gefördert Gründung Verein Phase übergehen Vereinsgründung Tatsache Rechnung tragen Stuttgart hinaus inzwischen ähnlich Ziel arbeiten Creta vielfältig Weise verbinden gemeinsam Projekt Melanie andresen Universität Stuttgart Institut maschinell sprachverarbeitung Pfaffenwaldring Stuttgart Melanie andresen Postdoc Institut maschinell Sprachverarbeitung Universität Stuttgart germanistisch Linguistik Universität Hamburg studieren Bereich Korpuslinguistik promovieren Projekt Benjamin Krautter Universität Köln Institut Digital Humanitie Köln Benjamin Krautter Promotionsstudent germanistisch Seminar Universität Heidelberg Mitarbeiter Projekt q Track arbeiten Operationalisierung literaturwissenschaftlich kategorien quantitativ Dramenanalyse Zentrum Forschungsinteresse stehen möglich Verbindung quantitativ qualitativ Methode Analyse Interpretation literarisch Text janis Pagel Universität Köln Institut Digital Humanitie Köln Janis Pagel Promotionsstudent Institut maschinell Sprachverarbeitung Universität Stuttgart Mitarbeiter Institut Digital Humanitie Universität Köln studiert Germanistik Linguistik Bochum Computerlinguistik Stuttgart Amsterdam forschen anwendungen computerlinguistisch Methode literaturwissenschaftlich Fragestellung Koreferenzresolution literarisch Text Axel Pichler Universität Stuttgart Institut maschinell sprachverarbeitung Pfaffenwaldring Stuttgart Axel Pichler studiert Philosophie Germanistik Wien graz Sommersemester gastprofessor Digital Humanitie exc Temporal Communitie äù fu Berlin Zurzeit arbeiten Postdoc Entwicklung Reflexion Methode computergestützt Textanalyse Institut maschinell Sprachverarbeitung Universität Stuttgart,"[('operationalisierung', 0.30472878293704314), ('stuttgart', 0.22005648815880283), ('institut', 0.19560576725226916), ('universität', 0.1931743432822548), ('teilnehmende', 0.16565897982483455), ('köln', 0.16371068577811826), ('entität', 0.13061610637677762), ('sprachverarbeitung', 0.12441757687314958), ('anwendungsfäll', 0.11692659837123748), ('pichler', 0.11692659837123748)]"
2022,DHd2022,SEIFERT_Sabine_Datenbiographik_im_Literaturarchiv__Konzept_u.xml,Datenbiographik im Literaturarchiv Konzept und Umsetzung digitaler Dienste am Theodor-Fontane-Archiv,"Sabine Seifert (Theodor-Fontane-Archiv, Universität Potsdam); Anna Busch (Theodor-Fontane-Archiv, Universität Potsdam); Peer Trilcke (Theodor-Fontane-Archiv, Universität Potsdam); Kristina Genzel (Theodor-Fontane-Archiv, Universität Potsdam); Juliane Heilmann (Theodor-Fontane-Archiv, Universität Potsdam); Klaus-Peter Möller (Theodor-Fontane-Archiv, Universität Potsdam)","Archiv, Datenbiographik, Metadaten, digitale Dienste, Chronik, Kulturdaten","Veröffentlichung, Bibliographie, Literatur, Metadaten, virtuelle Forschungsumgebungen","Die Arbeit von Literaturarchiven steht seit deren ersten Konzeptualisierungen im 19. Jahrhundert (Dilthey 1970 [1889]; vgl. Thaler 2011, Schöttker 2016) in einem komplexen Wechselverhältnis zu den philologischen Tätigkeiten der Editorik und Biographik, die im 20. und 21. Jahrhundert noch ergänzt werden u.a. um Textgenetik und Material Media Studies. Während das Zusammenspiel von Archiv und Editorik dabei zuletzt vor dem Horizont der Digitalisierung intensiv diskutiert wird (vgl. exemplarisch Nutt-Kofoth 2019), steht eine Neujustierung des Verhältnisses von Archiv und Biographik (vgl. Fetz 2009) im Zeichen der digitalen Transformation (Wettmann 2018) noch aus. Im Zuge der digitalen Erweiterung seiner Dienste (Trilcke 2019; Trilcke, Busch, Seifert 2021) hat das Theodor-Fontane-Archiv in den vergangenen Jahren nicht nur bio- und bibliographische Datenbestände zu Fontane erstellt und offen im Web nutzbar gemacht, es hat auch an einem Konzept für eine digital-biographische Ressource und deren Umsetzung gearbeitet. Anders als für die Buch-Biographik typisch, wurde dabei kein narrativer Ansatz gewählt. Ziel war es vielmehr, eine Konzeption Orientierungspunkt für die Konzeption und (Daten-)Grundlage für die Umsetzung dieses datenbiographischen Dienstes war die fünfbändige Druckausgabe der Die Die Einzeldatenbestände, für die individuelle Dienste mit eigenen Interfaces entwickelt wurden, umfassen die wichtigsten Forschungsdaten zu Fontane. Als digitale Dienste werden dabei die Die ursprünglich als Druck publizierte Die Aus XML-Druck-Daten der Auf den drei Einzeldatenbeständen aufbauend operiert die Mit der Implementierung des chronikalen Prinzips in Form einer Datenbiographik ist ein entscheidender Entwicklungsschritt im digitalen Ausbau des Fontane-Archivs abgeschlossen. Auf der nun bestehenden Infrastruktur aufbauend, steht vor allem die Qualitätssteigerung der Daten (Normdaten, LOD) sowie die Anbindung und Öffnung qua APIs im Vordergrund der Entwicklungsarbeiten. ",de,Arbeit literaturarchive stehen Konzeptualisierung Jahrhundert dilthey Thaler Schöttker komplex Wechselverhältnis philologisch Tätigkeit Editorik Biographik Jahrhundert ergänzen Textgenetik Material Media studies Zusammenspiel Archiv Editorik zuletzt Horizont Digitalisierung intensiv diskutieren exemplarisch stehen Neujustierung verhältnisses Archiv Biographik Fetz Zeichen digital Transformation Wettmann Zug digital Erweiterung dienst trilcke trilcken busch seiferen bibliographisch Datenbestände Fontane erstellen Web nutzbar Konzept Ressource Umsetzung arbeiten typisch narrativ Ansatz wählen Ziel vielmehr Konzeption orientierungspunken Konzeption Umsetzung datenbiographisch Dienst fünfbändig Druckausgabe einzeldatenbeständ individuell dienste Interfaces entwickeln umfassen wichtig Forschungsdat fontanen digital Dienst ursprünglich Druck publizieren einzeldatenbeständ Aufbauend operieren Implementierung chronikalen Prinzip Form Datenbiographik Entscheidender Entwicklungsschritt digital Ausbau abschließen bestehend Infrastruktur aufbauend stehen Qualitätssteigerung daten normdat Lod Anbindung Öffnung qua apis Vordergrund entwicklungsarbeiten,"[('dienst', 0.284057465805983), ('einzeldatenbeständ', 0.23258813206880635), ('biographik', 0.23258813206880635), ('editorik', 0.20532156381972314), ('archiv', 0.15895640819157725), ('konzeption', 0.14615515567250437), ('aufbauend', 0.13168983994249403), ('umsetzung', 0.11999675486749622), ('digital', 0.11705284659219439), ('öffnung', 0.11629406603440318)]"
2022,DHd2022,PIELSTRÖM_Steffen_Das_DFG_Schwerpunktprogramm_Computational_.xml,Das DFG Schwerpunktprogramm Computational Literary Studies,"Steffen Pielström (Julius-Maximilians-Universität Würzburg, Germany); Kerstin Jung (Universität Stuttgart, Germany)","Computational Literary Studies, Community, Literaturwissenschaft, Textanalyse","Community-Bildung, Literatur, Forschung","Die Computational Literary Studies (CLS) sind ein wachsendes, interdisziplinäres Forschungsfeld angesiedelt zwischen Literaturwissenschaft, Computerlinguistik und Informatik, in dem computergestützte Verfahren zur Analyse literaturwissenschaftlicher Fragestellungen zum Einsatz kommen. Insgesamt elf Einzelprojekte aus Deutschland und der Schweiz, die zur Zeit in diesem Emerging Field arbeiten, gehören dem seit 2020 aktiven Schwerpunktprogramm SPP 2207 ""Computational Literary Studies"" der Deutschen Forschungsgemeinschaft (DFG) an, davon erhalten 10 Projekte direkte Förderung aus dem Programm, ein weiteres Projekt ist mit dem Programm assoziiert. Hinzu kommt ein Zentralprojekt das, als Besonderheit neben der organisatorischen und inhaltilichen Koordination der Fortschungsvorhaben, über eine eigens eingerichteten Personalstelle für das projektübergreifende Forschungsdatenmanagement verfügt. So bietet das Programm eine enge Begleitung und Abstimmung der Projekte in Fragen des Forschungsdatenmanagements über die gesamte Laufzeit. Für das kooperative Arbeiten wird vom Zentralprojekt u.a. eine Gitlab-Instanz zur Verfügung gestellt. In den einzelnen Projekten kooperieren erfahrene Digital Humanists eng mit etablierten Literaturwissenschaftler*innen um an aktuell relevanten Fragen der Literaturwissenschaft zu arbeiten. Die Forschung im SPP 2207 konzentriert sich vor allem auf die deutschsprachige Literatur. Hier reicht das Spektrum der Forschungsgegenstände von Romanen über Dramen bis hin zur Poesie, die untersuchten Texte entstammen verschiedenen Epochen vom Mittelhochdeutschen bis ins späte 20. Jahrhundert. Hinzu kommen methodologische Untersuchungen die zum Ziel haben, das methodische Repertoire der Computational Literary Studies für die spezifischen Anforderungen des Faches zu validieren und weiter zu entwickeln. So haben sich für Sentimentanalyse, Wordembeddings und Annotationen projektübergreifende Arbeitsgruppen etabliert und ein ganzes Projekt widmet sich der Methodenforschung im Bereich der kontrastiven Stilometrie. Die Projekte in der ersten, dreijährigen Förderperiode sind im einzelnen: Angesichts der Vernetzung und Verankerung nahezu aller Programmbeteiligten in der nationalen wie internationalen Fachcommunity - so engagieren sich Mitglieder u.a. in der ADHO Special Interest Group ""Digital Literary Stylistics"", der EU COST Action ""Distant Reading for European Literary History"", dem EU-Programm ""Computational Literary Studies Infrastructure"" (CLSInfra) und der ACL Special Interest Group on Humanities (SIGHUM) - sieht sich SPP 2207 nicht nur als Einrichtung für eine begrenzte Zahl geförderter Projekte sondern auch als Multiplikator und ""Netzwerkknoten"" für die gesamte CLS-Community, insbesondere im deutschsprachigen Raum. Veranstaltungen des Schwerpunktprogramms wie Meetings und Workshops sind daher in der Regel ebenso offen für Interessierte wie die projektübergreifenden Arbeitsgruppen, um die aktive Beteiligung weiterer Teile der Fachcommunity an den Aktivitäten von SPP 2207 zu fördern. Mit dem vorliegenden Poster präsentiert sich SPP 2207 in seiner Gesamtheit und zeigt, wie die Vernetzung in einem solchen Programm Synergien und Gelegenheiten zur Zusammenarbeit schafft, die in der Zukunft auch in die weitere Foschungscommunity hinein wirken sollen.",de,Computational literary Studies cls wachsend interdisziplinär Forschungsfeld ansiedeln Literaturwissenschaft Computerlinguistik Informatik computergestützt Verfahren Analyse literaturwissenschaftlich Fragestellung Einsatz insgesamt einzelprojeken Deutschland Schweiz Emerging Field arbeiten gehören aktiv Schwerpunktprogramm Spp Computational literary studies deutsch Forschungsgemeinschaft dfg erhalten Projekt direkt Förderung Programm Projekt Programm assoziieren hinzu zentralprojekt Besonderheit organisatorisch inhaltilich Koordination Fortschungsvorhabe eigens Eingerichtet Personalstell projektübergreifend Forschungsdatenmanagement verfügen bieten Programm eng Begleitung Abstimmung Projekt Frage Forschungsdatenmanagement gesamt Laufzeit kooperativ arbeiten Zentralprojekt Verfügung stellen einzeln Projekt kooperieren erfahren Digital Humanist eng etabliert aktuell relevant Frage Literaturwissenschaft arbeiten Forschung Spp konzentrieren deutschsprachig Literatur reichen Spektrum Forschungsgegenstände Roman Dram Poesie untersucht Text entstamm verschieden Epoche mittelhochdeutsch spät Jahrhundert hinzu methodologisch Untersuchung Ziel methodisch Repertoire Computational literary Studies spezifisch Anforderung fach validieren entwickeln Sentimentanalyse wordembedding annotation projektübergreifend arbeitsgruppen etablieren Projekt widmen Methodenforschung Bereich kontrastiv Stilometrie Projekt dreijährig Förderperiode einzeln angesichts Vernetzung Verankerung nahezu Programmbeteiligt national international fachcommunity engagieren Mitglied Adho special Interest Group Digital Literary Stylistics EU cost action distant Reading for European literary History Computational Literary Studie infrastructure Clsinfra acl special interest Group -- humanities Sighum sehen spp Einrichtung begrenzt Zahl gefördert Projekt Multiplikator Netzwerkknote gesamt insbesondere deutschsprachig Raum Veranstaltung Schwerpunktprogramm Meeting Workshop Regel interessierter projektübergreifend arbeitsgruppe aktiv Beteiligung weit Teil fachcommunity Aktivität Spp fördern vorliegend Poster präsentieren spp Gesamtheit zeigen Vernetzung Programm synergien gelegenheiten Zusammenarbeit schaffen Zukunft Foschungscommunity hinein wirken,"[('spp', 0.3222262141374819), ('projektübergreifend', 0.2211730631958881), ('literary', 0.21636856201520283), ('programm', 0.2124101086648324), ('projekt', 0.171175304933633), ('zentralprojekt', 0.15830457341549983), ('forschungsdatenmanagement', 0.15830457341549983), ('computational', 0.14988752266307107), ('fachcommunity', 0.14744870879725872), ('interest', 0.1397463502732339)]"
2022,DHd2022,VAUTH_Michael_Inter_Annotator_Agreement_und_Intersubjektivit.xml,Inter Annotator Agreement und Intersubjektivität   Ein Vorschlag zur Messbarkeit der Qualität literaturwissenschaftlicher Annotationen,"Evelyn Gius (Technische Universität Darmstadt, Germany); Michael Vauth (Technische Universität Darmstadt, Germany)","Inter Annotator Agreement, Digitale Literaturwissenschaft, Annotation, Ereignisse","Strukturanalyse, Modellierung, Annotieren, Forschungsprozess, Forschungsergebnis, Standards","Die in den Sozialwissenschaften und der Computerlinguistik schon lange etablierte Praxis der computergestützten und häufig kollaborativen manuellen Annotation ist mittlerweile auch im Zentrum der digitalen Geisteswissenschaften angekommen. Deshalb möchten wir unsere Beobachtungen zu einem zentralen Punkt teilen: dem Inter Annotator Agreement bzw. Inter Coder Agreement. Wir betrachten dieses aus der Sicht der Computational Literary Studies (CLS) anhand unseres Projekts ""Evaluating Events in Narrative Theory  (EvENT)"" Es gibt eine Vielzahl von Inter Annotator Agreement-Metriken, die als Maß eingesetzt werden, um die Verlässlichkeit manuell erstellter Annotationen zu beurteilen, die zum Überprüfen einer These oder zur Entwicklung und zum Testen computationeller Modelle genutzt werden (Artstein & Poesio 2008:556). Da bei der Betrachtung des Inter Annotator Agreements von Menschen annotierte Daten 'und damit deren Analysen bestimmter Texte (oder anderer Artefakte) 'miteinander verglichen werden, ist dies auch literaturwissenschaftlich interessant. Der literaturwissenschaftliche Erkenntnisgewinn basiert nämlich, in Ermangelung objektiver Fakten, ganz wesentlich auf intersubjektiver Übereinstimmung bzw. deren Abgleich. Grundsätzlich lassen sich fünf Einsatzgebiete von Inter Annotator Agreement-Messungen unterscheiden: Für das EvENT-Projekt sind diese Einsatzbereiche unterschiedlich stark von Interesse. Literaturwissenschaftliche Befunde basieren meistens weder auf streng formalisierten Schlussfolgerungssystemen Mit Blick auf Intersubjektivität kann man in den fünf genannten Bereichen, in denen Inter Annotator Agreement-Messungen zum Einsatz kommen, feststellen: Im Kontext der Reliabilität von Annotator*innen (Fall 1) geht es um den Abgleich einer an sich aber Bei der Entwicklung bzw. Qualität von Guidelines (Fall 2 bzw. 3) geht es hingegen um die Frage, inwiefern eine Im Kontext der Qualität bzw. Validität der Daten und der Operationalisierbarkeit von Phänomenen (Fall 4 und 5) steht schließlich die intersubjektive Übereinstimmung bei der Beurteilung der Phänomene im Text im Fokus. Aus literaturwissenschaftlicher Sicht ist die Intersubjektivität insbesondere in den letzten beiden Fällen abgebildet. Bei der Frage nach Qualität bzw. Validität der Daten und der Operationalisierbarkeit von Phänomenen wird nämlich der Grad der Übereinstimmung zwischen Annotationen auf die oben erwähnten ""Eigenheiten bestimmter Texte"" bezogen. Die beiden Aspekte sind auch aus computationeller Sicht wichtig, denn sie betreffen die analysierten Phänomene und damit das zentrale Forschungsinteresse vieler literaturwissenschaftlicher Ansätze in den Digital Humanities. Wie bereits angesprochen, fehlen allerdings gerade zu diesen beiden Fällen Erfahrungswerte, auf die zurückgegriffen werden kann. Da die Inter Annotator Agreement-Werte in literaturwissenschaftlichen Annotationsprojekten zudem meist deutlich unter den in anderen Disziplinen gängigen Grenzwerten liegen, können diese nicht sinnvoll genutzt werden. Stattdessen müssen Strategien entwickelt werden, die eine Beurteilung der Annotationsqualität in philologischen Forschungskontexten ermöglichen. Wir stellen deshalb im Folgenden eine Anpassung des Verfahrens der Annotation und der Inter Annotator Agreement-Messung vor, mit der man diesem Manko in bestimmten Forschungszusammenhängen begegnen kann. Inter Annotator Agreement-Metriken basieren auf differenzierten Formeln, die typischerweise erwartete (Nicht-)Übereinstimmungswerte berücksichtigen und z.T. auch die Gewichtung bestimmter Aspekte der Annotationen zulassen (z.B. durch das Festlegen von Öhnlichkeiten zwischen Kategorien oder die Gewichtung der Segmentierungsentscheidungen). Die Wahl der eingesetzten Metrik sollte in Abhängigkeit von den Eigenschaften der Annotationen getroffen werden. Zu diesen Eigenschaften gehören die Anzahl und Verteilung der genutzten Annotationskategorien, die Häufigkeit, mit der Annotationskategorien auftreten, die Frage, ob die Bestimmung der zu annotierenden Textsegmente Teil der Annotationsaufgabe ist und viele mehr (vgl. dazu Artstein & Poesio 2008 sowie Mathet et al. 2015). Das Problem, vor dem wir zumindest bislang stehen, ist nicht nur, dass es eine ziemliche Herausforderung ist, diese Eigenschaften zu identifizieren, sondern noch mehr, dass uns etablierte Strategien fehlen, um diese zu beurteilen. Ein wesentlicher Grund dafür ist, dass literaturwissenschaftliche Textanalysen oft Phänomene in den Blick nehmen, die bei näherer Betrachtung keine Merkmale der Textoberfläche sind. Da diese Phänomene nicht direkt an bestimmten Texteigenschaften festgemacht werden können, muss man bei der Operationalisierung auf mit dem Phänomen mutmaßlich zusammenhängende Merkmale zurückgreifen, die sich textlich realisieren. Eine Folge dieser indirekten Annäherung an die untersuchten Phänomene ist, dass eine Agreement-Messung mit den üblichen Metriken für bestimmte literaturwissenschaftliche Einsatzgebiete nicht sinnvoll ist, da diese für die Annotation von Textphänomenen wie etwa Wortarten oder semantische Klassen entwickelt wurden. Nun könnte man versuchen neue, für literaturwissenschaftliche Fragestellungen passende Annotationsmetriken zu entwickeln. Öhnlich hilfreich und leichter umsetzbar ist allerdings eine Anpassung des Operationalisierungsverfahrens an das, was mit bestehenden Metriken gemessen wird. Konkret sollte man versuchen, die genutzten Annotationskategorien so zu gestalten, dass sie: Beim ersten Punkt ist es erstrebenswert, dass die genutzten Kategorien eine möglichst eindeutig festlegbare Texteinheit umfassen und im ganzen Text vorkommen. Eine mögliche Umsetzung dieser Punkte lässt sich an unserem Beispiel verdeutlichen. Ausgehend von den erzähltheoretischen Ereigniskonzepten haben wir im EvENT-Projekt vier Annotationskategorien definiert: Wir haben also eine syntaktisch weitgehend eindeutige Einheit 'die Verbalphrase 'identifiziert, die sich als Annotationseinheit eignet und deren Inhalt zur Bestimmung der Kategorisierung geeignet ist. Durch die Ausweitung der non_event-Kategorie auf nicht vollständige Verbalphrasen kann ein Text außerdem durchgängig mit unseren Kategorien annotiert werden kann. Auch die Überführung der kategorialen Skalierung in eine numerische Skalierung basiert auf dem literaturwissenschaftlichen Verständnis der Kategorien. Entsprechend dem literaturwissenschaftlichen Ereignisverständnis nehmen wir an, dass diese vier Kategorien in unterschiedlichem Maß die Ereignishaftigkeit eines Textes konstituieren: Zustandsveränderungen, aber auch Bewegungs- und Kommunikationsvorgänge tun dies in stärkerem Maß als Landschafts-, Raum- oder Figurenbeschreibungen, die in vielen erzählenden Texten eher Expositionsfunktionen erfüllen. Doch dies war noch nicht ausreichend, um ein Agreement zu erzielen, welches aus computerlinguistischer Sicht gut ist. Hinzu kommt, dass die Agreement-Werte unsere Intuition über die Qualität der Annotationen nicht widerspiegelten (vgl. Tabelle 1). Deshalb haben wir unser Vorgehen entsprechend weiterentwickelt. Der Schlüssel zu einer aussagekräftigeren Inter Annotator Agreement-Perspektive lag in der Erkenntnis, dass uns die Entwicklung von Ereignishaftigkeit im Textverlauf und entsprechend¬† Narrativitätsverläufe interessieren. Wir haben deshalb nicht nur die Ergebnisse der Annotationen als Verlauf visualisiert, sondern auch entschieden, die Einschätzung des Inter Annotator Agreement 'ebenso wie übrigens die Qualität der automatisierten Erkennung von Ereignissen –¬†anhand von Verläufen vorzunehmen. Für die Darstellung des Narrativitätsverlaufs wurden die Werte der Annotationen innerhalb eines Textabschnitts anhand der Narrativitätswerte der umliegenden 50 Verbalphrasen mit einer Kosinusgewichtung geglättet. Die Kosinusgewichtung sorgt dabei dafür, dass näher liegende Textsegmente einen stärkeren Einfluss auf den Narrativitätswert des untersuchten Textsegments haben. Auf Grundlage dieser Zuweisungen konnten wir die Narrativitätsverläufe in Einzeltexten wie in Abbildung 1 untersuchen:  Um die Stabilität des Verfahrens zu prüfen, haben wir mit der Zuweisung der Zahlen zu den Kategorien experimentiert, dabei aber ihre Anordnung gemäß ihrer Narrativität nicht verändert. Eine umfassende Evaluation steht noch aus, aber die bisherigen Versuche deuten darauf hin, dass die Narrativitätsverläufe dabei strukturell nicht stark variieren (vgl. Abbildung 2).  Wir konnten also auf der Grundlage unserer Wertzuweisung für die Ereignistypen die Annotationen der unterschiedlichen Annotator:innen miteinander vergleichen (vgl. Abbildung 3). Unsere Annäherung an ein Inter Annotator Agreement, das auf die Modellierung eines literarischen Phänomens ausgerichtet ist, scheint also unsere literaturwissenschaftlich fundierte Intuition besser abzubilden als gängige Inter Annotator Agreement-Metriken. Dafür sind zwei Aspekte entscheidend: Durch dieses Vorgehen gelingt es uns, den Fokus auf das eigentlich untersuchte Phänomen 'in unserem Fall die Ereignishaftigkeit von erzählenden Texten 'zu richten. Damit lässt sich die Intersubjektivität der Analysen besser messen als anhand der Annotationen, die das Phänomen anhand von Oberflächenphänomenen (Verbalphrasen) operationalisieren und die im Kontext von gängigen Inter Annotator-Metriken entsprechend nur bedingt aussagekräftig sind. Hinzu kommt, dass es zwei wichtige Fehlerquellen bei literaturwissenschaftlichen Annotationen –¬†nämlich einfache Fehler sowie divergierende Voranalysen (vgl. Gius & Jacke 2017) 'ausgleicht.",de,sozialwissenschaften Computerlinguistik etabliert Praxis computergestützt häufig kollaborativ manuell Annotation mittlerweile Zentrum digital Geisteswissenschafte ankommen möchten Beobachtunge zentral Punkt teilen int Annotator agreemenen int Coder agreemenen betrachten Sicht Computational literary Studies cls anhand unser Projekt Evaluating Event Narrative Theory Event Vielzahl int Annotator Maß einsetzen Verlässlichkeit manuell erstellt annotatio beurteilen überprüfen These Entwicklung test Computationeller Modell nutzen Artstein Poesio Betrachtung inter Annotator agreements Mensch annotiert daten Analyse bestimmt Text anderer artefaken miteinander vergleichen literaturwissenschaftlich interessant literaturwissenschaftlich Erkenntnisgewinn basieren nämlich Ermangelung objektiv Fakt wesentlich intersubjektiv übereinstimmung abgleich grundsätzlich lassen einsatzgebiet int annotator unterscheiden einsatzbereich unterschiedlich stark Interesse literaturwissenschaftlich Befund basieren meistens weder streng formalisiert Schlussfolgerungssysteme Blick Intersubjektivität genannt Bereich int Annotator Einsatz feststellen Kontext Reliabilität Fall abgleich Entwicklung Qualität guidelines Fall hingegen Frage inwiefern Kontext Qualität Validität daten operationalisierbarkeit Phänomen Fall stehen schließlich intersubjektiv Übereinstimmung Beurteilung Phänomen Text Fokus literaturwissenschaftlich Sicht Intersubjektivität insbesondere letzter Fall abbilden Frage Qualität Validität daten operationalisierbarkeit Phänomen nämlich Grad übereinstimmung annotation erwähnt eigenheiter bestimmt Text beziehen Aspekt computationell Sicht wichtig betreffen analysiert phänomen zentral Forschungsinteresse vieler literaturwissenschaftlich Ansatz Digital Humanitie ansprechen fehlen Fall erfahrungsweren zurückgegriffen int annotator literaturwissenschaftlich annotationsprojekten zudem meist deutlich disziplin Gängige grenzwerter liegen sinnvoll nutzen stattdessen strategien entwickeln Beurteilung Annotationsqualität Philologisch Forschungskontexte ermöglichen stellen folgend Anpassung verfahrens Annotation int Annotator Manko bestimmt Forschungszusammenhäng begegnen int Annotator basieren differenziert Formel typischerweise erwartet berücksichtigen Gewichtung bestimmt Aspekt annotation zulassen Festlegen öhnlichkeiten kategorien Gewichtung segmentierungsentscheidungen Wahl eingesetzt Metrik Abhängigkeit Eigenschaft annotatio treffen Eigenschaft gehören Anzahl Verteilung Genutzt annotationskategorien Häufigkeit Annotationskategorie auftreten Frage Bestimmung Annotierend textsegment Annotationsaufgabe artstein Poesio Mathet et Problem zumindest bislang stehen ziemlich Herausforderung eigenschaften identifizieren etabliert strategien fehlen beurteilen wesentlich Grund literaturwissenschaftlich textanalyse Phänomen Blick nehmen näh Betrachtung Merkmal Textoberfläche Phänomen direkt bestimmt Texteigenschaft festgemachen Operationalisierung phänom mutmaßlich zusammenhängend Merkmal zurückgreifen textlich realisieren Folge indirekt Annäherung untersucht Phänomen üblich metriken bestimmt Literaturwissenschaftliche einsatzgebien sinnvoll Annotation Textphänomen wortart semantisch Klasse entwickeln versuchen literaturwissenschaftlich Fragestellung passend Annotationsmetrik entwickeln öhnlich hilfreich leicht umsetzbar Anpassung Operationalisierungsverfahren bestehend metriken messen konkret versuchen genutzt annotationskategorien gestalten Punkt erstrebenswern genutzt kategorien möglichst eindeutig festlegbar Texteinheit umfassen Text vorkommen möglich Umsetzung Punkt lässen unser verdeutlichen ausgehend erzähltheoretisch Ereigniskonzept Annotationskategorie definieren syntaktisch weitgehend eindeutig Einheit Verbalphrase identifizieren Annotationseinheit eignen Inhalt Bestimmung Kategorisierung eignen Ausweitung vollständig verbalphrasen Text durchgängig unser Kategori annotiert Überführung kategorial Skalierung numerisch Skalierung basieren literaturwissenschaftlich Verständnis kategorien entsprechend literaturwissenschaftlich Ereignisverständnis nehmen kategorien unterschiedlich Maß Ereignishaftigkeit Text konstituieren zustandsveränderung kommunikationsvorgänge stärker Maß Figurenbeschreibunge erzählend Text eher Expositionsfunktion erfüllen ausreichend Agreement erzielen computerlinguistisch Sicht hinzu Intuition Qualität annotation widerspiegelt Tabelle vorgehen entsprechend weiterentwickeln Schlüssel aussagekräftiger inter Annotator liegen Erkenntnis Entwicklung Ereignishaftigkeit Textverlauf narrativitätsverläufen interessieren Ergebnis annotatio Verlauf visualisiern entscheiden Einschätzung inter Annotator agreemenen Qualität automatisiert Erkennung Ereignis verläufen vornehmen Darstellung narrativitätsverlaufs Wert annotatio innerhalb Textabschnitt anhand narrativitätswerte umliegend Verbalphrase Kosinusgewichtung glätten Kosinusgewichtung sorgen nah liegend textsegmenen stark einfluss Narrativitätswert untersucht Textsegment Grundlage weisungen narrativitätsverläufe Einzeltexte Abbildung untersuchen Stabilität Verfahren prüfen Zuweisung Zahl Kategorie experimentieren Anordnung gemäß Narrativität verändern umfassend Evaluation stehen bisherig Versuch deuten Narrativitätsverläuf strukturell stark variieren Abbildung Grundlage Wertzuweisung Ereignistype annotatio unterschiedlich Annotator innen miteinander vergleichen Abbildung Annäherung int Annotator agreemenen Modellierung literarisch Phänomen ausrichten scheinen literaturwissenschaftlich fundiert Intuition abzubilden gängig int Annotator Aspekt entscheidend vorgehen gelingen Fokus eigentlich untersucht Phänom unser Fall Ereignishaftigkeit erzählend Text richten lässen Intersubjektivität Analyse messen anhand Annotation Phänomen anhand oberflächenphänomen Verbalphrasen operationalisieren Kontext Gängig int entsprechend bedingt aussagekräftig hinzu wichtig fehlerquelle literaturwissenschaftlich Annotation einfach Fehler divergierend Voranalyse Gius Jacke ausgleichen,"[('int', 0.43765195978940674), ('annotator', 0.31441029037065643), ('literaturwissenschaftlich', 0.1995460290742019), ('phänomen', 0.1689531057253947), ('agreemenen', 0.1508327621655579), ('intersubjektivität', 0.12814744012105445), ('ereignishaftigkeit', 0.11312457162416843), ('inter', 0.11312457162416843), ('annotatio', 0.11231506519392213), ('annotation', 0.10959651345228522)]"
2022,DHd2022,ANDRESEN_Melanie_Nathan_nicht_ihr_Vater____Wissensvermittlun.xml,Nathan nicht ihr Vater? Wissensvermittlungen im Drama annotieren,"Melanie Andresen (Universität Stuttgart, Germany); Benjamin Krautter (Universität Stuttgart, Germany); Janis Pagel (Universität Stuttgart, Germany); Nils Reiter (Universität zu Köln, Germany)","Annotation, Modellierung, Drama","Beziehungsanalyse, Modellierung, Annotieren, Sprache, Literatur, Text","Die quantitative Dramenanalyse hat sich lange Zeit vor allem auf formale Merkmale der Textoberfläche konzentriert In diesem Beitrag gehen wir zunächst auf die Bedeutung von Wissen und Wissensvermittlungen für die Handlung wie auch die Wirkung von Dramen ein. Anschließend beschreiben wir, wie solche Prozesse der Wissensvermittlung in Annotationen erfasst und modelliert werden können. Die Interferenz von innerem und äußerem Kommunikationssystem im Drama, also die Kommunikation der fiktiven Figuren auf der einen Seite und die Wahrnehmung dieser Kommunikation durch das Publikum auf der anderen Seite, gilt als eine zentrale ""Differenzqualität dramatischer Kommunikation"" (Pfister 2001: 80). Die Bühnenfiguren zeichnen sich schon mit Blick auf die Vorgeschichte des Dramas potentiell durch einen unterschiedlichen Wissensstand aus, der sich im Laufe des Stücks fortwährend verändern kann, etwa hinsichtlich ihrer Handlungsziele. Dadurch wird zugleich das Verhältnis zwischen dem Informationsstand des Publikums und demjenigen der einzelnen Dramenfiguren immer wieder neu justiert. Die Exposition reduziert etwa den zu Beginn eines Dramas vorherrschenden Wissensrückstand des Publikums gegenüber den Figuren (vgl. etwa Asmuth 2015: 122). Die Unterschiede im ""Grad der Informiertheit"" 'Manfred Pfister spricht hierbei in Rekurs auf den Shakespeare-Forscher Bertrand Evans von ""diskrepante[r] Informiertheit"" (Pfister 2001: 80, vgl. Evans 1960: viii) 'lassen sich vor allem auf zwei ursächliche Unterschiede zwischen innerem und äußerem Kommunikationssystem zurückführen: Während das Publikum in seiner Beobachterrolle jede Szene des Stücks wahrnimmt und dadurch geäußertes partielles Wissen der Figuren abgleichen und aggregieren kann, bleibt bisweilen unklar, über welches Wissen die Figuren tatsächlich verfügen. Das gilt auch für mögliche Zeitsprünge, etwa zwischen zwei Akten des Dramas. Unklar kann zudem sein, inwieweit die Öußerungen einer Figur mit den ""Tatsachen"" der fiktionalen Welt übereinstimmen, ob die Öußerungen also glaubwürdig sind (vgl. Jeßing 2015: 50-51). Je nach Handlungsverlauf verfügt das Publikum also zu unterschiedlichen Zeitpunkten des Dramas über einen Informationsvorsprung oder einen Informationsrückstand gegenüber den auf der Bühne agierenden Figuren. Gleiches gilt isoliert betrachtet auch für das interne Kommunikationssystem des handelnden Bühnenpersonals. Die ""diskrepante Informiertheit"" zweier Figuren kann so zu unterschiedlichen Bewertungen derselben Situation führen. Figuren, die etwa über das Wissen verfügen, dass zwei verlobte Figuren Geschwister sind, werden diese Verlobung anders beurteilen, als Figuren, denen dieses Wissen fehlt. Diese Kluft zwischen dem Wissensstand der Figuren und demjenigen des Publikums ist als wichtiges Spannungselement des Dramas aufzufassen Ziel unseres Annotations- und Modellierungsvorhabens ist es deshalb, das sich verändernde Wissen über Familienrelationen sowohl im internen als auch im externen Kommunikationssystem abzubilden. Wir wollen dabei nicht nur die zentralen Szenen der Wiedererkennung annotieren, sondern vor allem die einzelnen Schritte nachvollziehen, die einen solchen für die dramatische Wirkung entscheidenden Wissensumschlag anleiten. In einem ersten Schritt werden Textstellen im Drama, an denen Wissen über Familienrelationen vermittelt wird, manuell annotiert. Entscheidend für die Annotation ist, dass sich der Wissensstand einer Figur oder des Publikums tatsächlich verändert. Relevante Textstellen werden mit einem strukturiert zusammengesetzten Label versehen, das sowohl das vermittelte Wissen als auch die Quelle und das Ziel der Wissensvermittlung benennt. Optional können Attribute hinzugefügt werden, sodass die Annotationslabel nach dem folgenden Schema funktionieren:  Quelle und Ziel sind in der Regel entweder Figuren des Dramas oder das Publikum (oder eine Liste mehrerer dieser Entitäten). Als Quelle kann aber auch ein Objekt oder Vorgang in der Welt in Betracht kommen (z. B. eine Beobachtung). Das für unsere Annotationen relevante Wissen ist auf Familienrelationen und Liebesbeziehungen zwischen den Figuren beschränkt, wobei die Annotationsrichtlinien ein festes Inventar von Relationen vorgibt. Formal können hierbei gerichtete Relationen wie  Durch die optionalen Attribute kann das vermittelte Wissen spezifiziert, also beispielsweise als unsicher oder als Lüge gekennzeichnet werden. Durch ein vorangestelltes Ausrufezeichen können Relationen oder Wissensbestände negiert werden. Beim Wissensstand kann es sich auf einer Metaebene auch um ein Wissen über Wissen handeln. So kann etwa annotiert werden, dass Daja dem Tempelherrn (und dadurch auch dem Publikum) anvertraut, dass Recha gar nicht bewusst ist, dass Nathan nicht ihr leiblicher Vater ist:  Weitere Details zur Annotation lassen sich den auf unserer Webseite veröffentlichten Richtlinien entnehmen. Indem wir erfahren, dass Figur¬†A Elternteil einer Figur B ist, lässt sich schließen, dass Figur¬†B das Kind von Figur A ist, ohne dass dies im Text explizit gemacht werden müsste. Falls weitere Verwandte von Figur¬†A bekannt sind, ergeben sich zudem weitere Verwandtschaftsverhältnisse für Figur¬†B. Die annotierten Wissensvermittlungen müssen deshalb im Anschluss an die Annotation um alle weiteren, logisch inferierbaren Figurenrelationen ergänzt werden. Ziel des Projektes ist es, diese logischen Schlüsse durch ein formalisiertes Regelsystem zu ziehen, das auf die annotierten Wissensveränderungen angewendet werden kann und diese automatisch ergänzt. An einem ersten Prototyp dieses Inferenzsystems arbeiten wir derzeit. Die zentrale Wiedererkennung in Lessings Für das Publikum wird dieses Wissen bereits durch die Figurentafel ersichtlich. Recha wird dort im Anschluss an Nathan als ""dessen angenommene Tochter"" (Lessing 1971: 206) eingeführt. Dies verleitet zu der Annahme , dass es sich dabei um ein von allen Figuren geteiltes Wissen handelt. Direkt im 1. Auftritt spielt Daja, Rechas Gesellschafterin, auf diese Tatsache an. Sie ist demnach eingeweiht. In Bezug auf Rechas Kenntnis über ihre Herkunft bleibt das Publikum zunächst im Dunkeln. Ihr Ausruf, ""Da kommen die Kamele meines Vaters"" (Lessing 1971: 209), ist auch für eine Pflegetochter, die sich dieses Umstands bewusst ist, denkbar. Dem Tempelherrn gegenüber stellt sich Nathan im 5. Auftritt des 2. Aufzugs als Rechas Vater vor. Dass Nathan tatsächlich Rechas Pflegevater ist, erfährt der Tempelherr zum Ende des 3. Aufzugs von Daja, die auf eine christliche Heirat von Recha hofft. Auf der Metaebene (""Wissen über Wissen"") wird dem Tempelherrn zudem offenbar, dass Recha sich ihrer tatsächlichen familiären Relation zu Nathan nicht bewusst ist, und klärt diese Frage damit ebenfalls für das möglicherweise noch zweifelnde Publikum. Der Tempelherr gibt dieses Wissen, empört über die zurückhaltende Reaktion Nathans auf seinen Heiratsantrag, an Saladin weiter (4. Aufzug, 4. Auftritt). In der folgenden Aussprache mit Nathan (5. Aufzug, 5. Auftritt) gibt der Tempelherr ihm gegenüber zu, von Daja bereits die wahren Verwandtschaftsverhältnisse erfahren zu haben. Abseits der Bühne hat Daja inzwischen auch Recha über ihren Status als Pflegetochter informiert. Dies erfährt das Publikum, indem Recha diesen Umstand auch Saladins Schwester Sittah berichtet (5. Aufzug, 6. Auftritt), sodass das Wissen nun alle Figuren im Kern des Dramas erreicht hat. Für gleich mehrere Figuren lässt sich aus den Annotationen jedoch nicht direkt ableiten, zu welchem Zeitpunkt sie erstmals über das relevante Wissen verfügen. Nathan und Daja wissen bereits vor Beginn der Dramenhandlung, dass Nathan nicht Rechas leiblicher Vater ist. Direkt aus der Figurentafel lässt sich dieser Umstand indes nicht ableiten. Dass ein Vater¬† darüber informiert ist, wer (nicht) seine leiblichen Kinder sind, ist auch in Dramen wahrscheinlich (principle of minimal departure, vgl. etwa Ryan 1980), aber nicht alternativlos. Im ersten Auftritt erfährt das Publikum also zunächst expositorisch, dass Nathan und Daja über dieses Wissen schon vor Handlungsbeginn verfügen. Öhnlich dazu wird auch der Moment, in dem Recha erfährt, nicht Nathans leibliche Tochter zu sein, nicht auf der Bühne dargestellt. Erst durch ihren Dialog mit Sittah wird offenbar, dass sie es zwischenzeitlich abseits der Bühne erfahren haben muss. Abbildung 2 stellt den Wissensverlauf für die Information, dass Recha und der Tempelherr Geschwister sind, dar. Nathan (und damit das Publikum) hegen einen entsprechenden Verdacht (in der Abbildung hell dargestellt), seit der Tempelherr im zweiten Akt seinen Familiennamen genannt hat. Erst nachdem sich dieser Verdacht im Gespräch mit dem Klosterbruder bestätigt, eröffnet Nathan allen anderen anwesenden Figuren am Ende des Dramas, dass Recha und der Tempelherr Geschwister sind. Liegt eine größere annotierte Stichprobe vor, können die annotierten Daten auf Muster untersucht werden, die im Hinblick auf zeitgenössische Dramenpoetiken und deren Normvorstellungen zu interpretieren sind. Anhand unserer bislang annotierten Dramen wollen wir dazu abschließend eine erste statistische Auswertung skizzieren. Das dazugehörige Analysekorpus umfasst zum gegenwärtigen Zeitpunkt elf Dramen. Darüber hinaus lässt sich feststellen, dass die Textstellen, an denen Wissensvermittlungen annotiert werden, ungleich über den Verlauf der Dramen verteilt sind. So treten zu Beginn und gegen Ende eines Dramas gehäuft Annotationen auf (jeweils 13% aller Annotationen), während die übrigen Annotationen relativ homogen über den Handlungsverlauf verteilt sind. Ausgehend von diesen ersten Auswertungen ergeben sich für künftige quantitative Analysen vielversprechende Perspektiven. Neben der bloßen Anzahl an Relationen, die im Verlauf der Stücke als neues Wissen an andere Figuren weitergegeben werden, und der Frage nach dem Zeitpunkt der Wissensweitergabe im Verlauf des Dramas, ergeben sich auch literaturwissenschaftlich avanciertere Fragestellungen. Unterscheiden sich die Muster der Wissensweitergabe für verschiedene Gattungen, also etwa die dramatischen Großgattungen Tragödie und Komödie? Welche Figuren geben das Wissen über familiäre Figurenrelationen weiter, an welche Figuren wird es weitergegeben? Lassen sich hierbei Muster identifizieren, etwa hinsichtlich des Geschlechts der Figuren? Ist es darüberhinaus möglich, die Szenen der Wissensweitergabe näher zu charakterisieren: Wie viele Figuren stehen in diesen Szenen auf der Bühne? Wie viele sind davon an der Wissensweitergabe aktiv beteiligt? Eine systematische Annotation von Prozessen der Wissensvermittlung im Drama ermöglicht eine Analyse, die über formale Merkmale der Textoberfläche hinausgeht. Liegen die Wissensbestände der Figuren und ihre Entwicklung im Verlauf des Dramas in maschinenlesbarer Form vor, lassen sich Zusammenhänge zwischen verschiedenen Textstellen identifizieren, an denen Widersprüche im Wissen der Figuren deutlich werden oder konflikthafte Relationen auftreten, wenn etwa zwei Figuren zugleich Geschwister und Liebespaar sind. Diese Widersprüche sollen über ein formalisiertes Regelsystem automatisch aus der Annotation der Familienrelationen inferiert werden. Die Erweiterung der quantitativen Analyse auf Phänomene jenseits der Textoberfläche ist naturgemäß mit größeren Herausforderungen für die Automatisierung verbunden. Vielfach zeigen sich aber sprachliche Muster, etwa Wiederholungen und Rückfragen, die einen als überraschend markierten Wissenszuwachs verdeutlichen (siehe Abbildung 3) und Hoffnung für die automatische Identifikation derartiger Textstellen machen. TEMPELHERR. Nicht mehr! Ich bitt"" Euch! 'Aber Rechas Bruder? Rechas Bruder ... NATHAN. Seid Ihr! TEMPELHERR. Ich? ich ihr Bruder? RECHA. Er mein Bruder? SITTAH. Geschwister! SALADIN. Sie Geschwister!",de,quantitativ Dramenanalyse formal Merkmal Textoberfläche konzentrieren Beitrag Bedeutung Wissen wissensvermittlungen handlung Wirkung Dram anschließend beschreiben prozeß Wissensvermittlung Annotation erfasst modellieren Interferenz inner äußerem Kommunikationssystem Drama Kommunikation fiktiv Figur Seite Wahrnehmung Kommunikation Publikum Seite gelten zentral Differenzqualität dramatisch Kommunikation pfister Bühnenfigure zeichnen Blick vorgeschichte Drama potentiell unterschiedlich Wissensstand Lauf Stück fortwährend verändern hinsichtlich Handlungsziele Verhältnis Informationsstand Publikum demjenigen einzelner dramenfiguren neu justieren Exposition reduzieren Beginn Drama vorherrschend Wissensrückstand Publikum Figur Asmuth Unterschied Grad Informiertheit Manfred Pfister sprechen hierbei Rekurs Bertrand Evan diskrepant r informiertheit pfister Evan viii lassen ursächlich Unterschied inner äußerem Kommunikationssystem zurückführen Publikum Beobachterrolle Szene Stück wahrnehmen geäußert partiell wissen Figur abgleichen aggregieren bleiben bisweilen unklar wissen Figur tatsächlich verfügen gelten möglich zeitsprüngen Akte dramas unklar zudem inwieweit öußerunge Figur Tatsach fiktional Welt übereinstimmen öußerunge glaubwürdig Jeßing handlungsverlauf verfügen Publikum unterschiedlich Zeitpunkt Drama Informationsvorsprung Informationsrückstand Bühne agierend Figur gleich gelten Isoliert betrachten intern Kommunikationssystem handelnd bühnenpersonals diskrepant informiertheit zwei Figur unterschiedlich Bewertung Situation führen Figur Wissen verfügen verlobt Figur schwister Verlobung beurteilen Figur wissen fehlen Kluft Wissensstand Figur Demjenigen Publikum wichtig Spannungselement Drama aufzufass Ziel unser Modellierungsvorhaben verändernd wissen familienrelationen sowohl intern extern Kommunikationssystem abbilden zentral Szene Wiedererkennung annotieren einzeln Schritt nachvollziehen dramatisch Wirkung entscheidend Wissensumschlag anleien Schritt Textstell Drama wissen Familienrelation vermitteln manuell annotieren entscheidend Annotation Wissensstand Figur Publikum tatsächlich verändern relevant Textstelle strukturiert zusammengesetzt Label versehen sowohl vermittelt wissen Quelle Ziel Wissensvermittlung benennen optional attribute hinzufügen sodass Annotationslabel folgend Schema funktionieren Quelle Ziel Regel Figur Drama Publikum Liste mehrere entität Quelle Objekt Vorgang Welt Betracht Beobachtung Annotation relevant wissen familienrelationen liebesbeziehungen Figur beschränken wobei Annotationsrichtlinien fest Inventar relationen vorgiben formal hierbei gerichtet Relation optional Attribut Vermittelte wissen spezifiziern beispielsweise unsicher lüge kennzeichnen vorangestellt ausrufezeichen relationen wissensbeständ negieren Wissensstand metaeben Wissen Wisse handeln annotiert daja Tempelherrn Publikum anvertrauen Recha bewussen Nathan leiblich Vater Detail Annotation lassen webseit veröffentlicht Richtlinie entnehmen erfahren Elternteil Figur b lässn schließen Kind Figur Text explizit müsste falls verwandte ergeben zudem verwandtschaftsverhältnisse Annotierte wissensvermittlungen Anschluss Annotation logisch inferierbar Figurenrelation ergänzen Ziel Projekte logisch schlüsse formalisiert Regelsystem ziehen annotierter wissensveränderungen anwenden automatisch ergänzen Prototyp Inferenzsystem arbeiten derzeit zentral Wiedererkennung Lessings Publikum wissen Figurentafel ersichtlich recha Anschluss Nathan angenommen Tochter Lessing einführen verleiten Annahme figuren geteilt Wissen handeln direkt Auftritt spielen daja Rechas Gesellschafterin Tatsache demnach einweihen Bezug Rechas Kenntnis Herkunft bleiben Publikum dunkeln ausruf kamele Vater Lessing Pflegetochter Umstands bewussen denkbar Tempelherrn stellen Nathan Auftritt Aufzug Rechas Vater Nathan tatsächlich Recha Pflegevater erfahren Tempelherr Aufzugs Daja christlich Heirat Recha hoffen metaeben wissen wissen Tempelherrn zudem offenbar recha tatsächlich familiär Relation Nathan bewussen klären Frage ebenfalls möglicherweise zweifelnd Publikum Tempelherr wissen empört zurückhaltend Reaktion Nathans Heiratsantrag Saladin Aufzug auftreten folgend Aussprache Nathan Aufzug auftreten Tempelherr daja wahr verwandtschaftsverhältnis erfahren abseits Bühne Daja inzwischen Recha Status Pflegetochter informieren erfahren Publikum Recha Umstand Saladin Schwester Sittah berichten Aufzug auftreten sodass wissen Figur Kern Drama erreichen mehrere Figur lässen annotationen direkt ableiten Zeitpunkt erstmals relevant Wissen verfügen Nathan Daja wissen Beginn Dramenhandlung Nathan Recha leiblich Vater direkt Figurentafel lässt Umstand indes ableiten informieren leiblich Kind Dram wahrscheinlich principle -- minimal departurer Ryan alternativlos Auftritt erfahren Publikum expositorisch Nathan daja wissen Handlungsbeginn verfügen öhnlich Moment Recha erfahren Nathans leiblich Tochter Bühne darstellen Dialog Sittah offenbar zwischenzeitlich abseits Bühne erfahren Abbildung stellen Wissensverlauf Information Recha Tempelherr geschwister dar Nathan Publikum hegen entsprechend Verdacht Abbildung Hell darstellen Tempelherr akt Familiennamen nennen verdachen Gespräch Klosterbruder bestätigen eröffnen nathan anwesend Figur Drama Recha Tempelherr geschwister liegen groß annotiert Stichprobe annotierter daten Muster untersuchen Hinblick Zeitgenössisch dramenpoetiken Normvorstellungen interpretieren anhand bislang annotierten dramen abschließend statistisch Auswertung skizzieren dazugehörig Analysekorpus umfassen gegenwärtig Zeitpunkt Dram hinaus lässen feststellen Textstelle wissensvermittlungen annotiert ungleich Verlauf Dram verteilen treten Beginn Drama häufen annotatio jeweils annotation übrig annotation relativ homogen Handlungsverlauf verteilen ausgehend Auswertung ergeben künftig quantitativ analyse vielversprechend Perspektive bloß Anzahl relationen Verlauf Stück neu Wissen Figur weitergeben Frage Zeitpunkt Wissensweitergabe Verlauf Drama ergeben literaturwissenschaftlich avanciert Fragestellung unterscheiden Muster Wissensweitergabe verschieden Gattung dramatisch großgattung Tragödie Komödie Figur geben wissen Familiär figurenrelationen Figur weitergeben lassen hierbei Muster identifizieren hinsichtlich Geschlecht Figur darüberhinaus Szene Wissensweitergabe nah charakterisieren Figur stehen Szene Bühne Wissensweitergabe aktiv beteiligen systematisch Annotation Prozesse Wissensvermittlung Drama ermöglichen Analyse formal Merkmal Textoberfläche hinausgehen liegen wissensbestände Figur Entwicklung Verlauf Drama Maschinenlesbarer Form lassen Zusammenhäng verschieden Textstell identifizieren widersprüche Wissen Figur deutlich Konflikthaft relationen auftreten Figur geschwister liebespaar widersprüche formalisiert Regelsystem automatisch Annotation familienrelation inferieren Erweiterung quantitativ Analyse Phänomene jenseits Textoberfläche naturgemäß groß Herausforderung Automatisierung verbinden vielfach zeigen sprachlich Muster Wiederholung Rückfrage überraschend markiert Wissenszuwachs verdeutlichen sehen Abbildung Hoffnung automatisch Identifikation derartig Textstelle Tempelherr beten Recha Bruder Rechas Bruder Nathan Tempelherr Bruder Recha Bruder Sittah Geschwister Saladin Geschwister,"[('recha', 0.3379741365582509), ('wissen', 0.31142780480191934), ('nathan', 0.29058210997772255), ('figur', 0.2781738397728983), ('publikum', 0.26651512696464497), ('tempelherr', 0.20798408403584673), ('drama', 0.18712259194059042), ('daja', 0.1819860735313659), ('geschwister', 0.12107587915738438), ('erfahren', 0.11996921370422105)]"
2022,DHd2022,BUSCH_Anna_Digitale_Archive_für_Literatur.xml,Digitale Archive für Literatur,"Anna Busch (Theodor-Fontane-Archiv, Universität Potsdam); Bernhard Fetz (Literaturarchiv und Literaturmuseum Österreichische Nationalbibliothek); Marcel Lepper (Goethe- und Schiller-Archiv Weimar); Irmgard Wirtz Eybl (Schweizerisches Literaturarchiv); Sandra Richter (Deutsches Literaturarchiv Marbach); Peer Trilcke (Theodor-Fontane-Archiv, Universität Potsdam)","Literaturarchiv, Digitalisierung, Transformation, Standards, Infrastruktur, DACH","Literaturarchiv, Digitalisierung, Transformation, Standards, Infrastruktur, DACH","Die Reflexion der institutionellen Transformation von Literaturarchiven angesichts der Digitalisierung überschreitet die Einzelinstitutionen notwendig, gerade dort, wo es gemeinsame Praktiken, Routinen, Standards und Infrastrukturen zu entwickeln gilt. Das Panel greift diesen Bedarf durch seinen internationalen und interinstitutionellen Ansatz auf. Literaturarchive stehen durch die Digitalisierung vor einer Vielzahl an Herausforderungen: Begriff, Praxis und Materialität des Literaturarchivs befinden sich in einem Transformationsprozess, den die Institution ""Literaturarchiv"" in dieser Grundsätzlichkeit seit ihrer konzeptionellen Erfindung im 19. Jahrhundert (Goethe 1823, Dilthey 1970 [1889], Thaller 2011) nicht durchlaufen hat. Die Anforderungen nach Partizipation (Theimer 2018), die Digitalisierung der Bestände und die Umsetzung von Open Access- und Data-Strategien (Szekely 2017), die Adressierung der Fragen, die born-digitals mit sich bringen, gehen in vielen Fällen einher mit einem Umbau von Routinen und Handlungsprogrammen wie mit einer Befragung und Neuerfindung der eigenen Identität als Institutionen (Cook 2013). Archivmitarbeiterinnen und -mitarbeiter werden mit immer größeren informatischen Herausforderungen und Aufgabenspektren betraut, immer öfter übernehmen Informatikerinnen und Informatiker entscheidende Rollen beim Sammlungszugang und bei der Überlieferungspräsentation. Digital- und Datenkompetenzen werden zum unverzichtbaren Handwerkszeug für moderne Literaturarchive, die sich zu Datendienstleistern wandeln 'ein Prozess, mit dem sich Bibliotheken bereits seit längerer Zeit beschäftigen (exemplarisch Stäcker 2019). Es gilt folglich, über die Aufgaben und Herausforderungen, die die fortschreitende Digitalisierung des kulturellen Gedächtnisses speziell für Literaturarchive mit sich bringt, nachzudenken und Lösungsansätze zu entwickeln, wie ihnen zukünftig begegnet werden kann. Mit dem Panel soll ein in der Community der Literaturarchive 'etwa im Netzwerk ""KOOP-LITERA"", in der Schriftenreihe Das Panel forciert diesen Austausch durch einen strukturiert-systematischen Impuls, bei dem wir die digitale Transformation in Literaturarchiven auf drei Ebenen adressieren: Das Digitale, das allerorten vermeintliche ""Archive"" hervorbringt, erweitert und stellt den gewachsenen Begriff des Archivs in Frage. Eine neue Phase der begrifflichen Reflexion setzt ein, in der auch Literaturarchive ihre Selbstbeschreibung überdenken. Digitale Werkzeuge und Infrastrukturen durchdringen die Praktiken heutiger ArchivarInnen, die beim Sammeln, Bewahren, Erschließen, Vermitteln immer häufiger zugleich Daten- und CodeexpertInnen sein müssen. Im Kontext der Digitalisierung ist ein neues Verständnis von Praktiken und Handlungsprogrammen der Tätigkeiten in Literaturarchiven zu entwickeln. Das Spektrum der Objekte, die von Literaturarchiven ""prozessiert"" werden, wandelt und weitet sich. Literaturarchive befinden sich in einer Situation, in der sie die Materialität und Objekthaftigkeit ihrer Bestände und Sammlungen neu begreifen müssen. Das Panel versammelt VertreterInnen von bedeutenden Literaturarchiven aus dem DACH-Raum: Bernhard Fetz, (Literaturarchiv und Literaturmuseum der Österreichische Nationalbibliothek, Wien), Marcel Lepper (Goethe- und Schiller-Archiv Weimar), Sandra Richter (Deutsches Literaturarchiv Marbach) und Irmgard Wirtz Eybl (Schweizerisches Literaturarchiv, Bern). Die Moderation übernehmen Anna Busch und Peer Trilcke (Theodor-Fontane-Archiv, Potsdam). Mit theoretischem, konzeptionell-institutionellem und praxeologischem Blick sucht das Panel nach dem neuen Selbstverständnis der ""Digitalen Archive für Literatur"". Um die Diskussion vorzubereiten, haben die vier VertreterInnen Positionierungen und Reflexionen zu den drei systematischen Ebenen ausformuliert. Die digitale Kommunikation des Archivs könnte eine Bewegung auslösen, so die Utopie des Archivs, die die Objekte und deren HüterInnen zu MediatorInnen eines umfassenden Bildungsbegriffs werden lässt, eines Prozesses, der Traditionen, (nationale) kulturelle Repräsentationen und die Werke der ""Großen"" fluide macht. Die ubiquitäre Verfügbarkeit der Archivalien geht mit Prozessen der Entkanonisierung einher. Die Hochkultur und das ""Gipfelprinzip"" verlieren an Geltung, der Fokus verschiebt sich von einzelnen Werken zu den diversen Lebens- und Arbeitsspuren in digitalen Archiven und sozialen Netzwerken (zu Tage- und Notizbüchern, biografischen Projekten, zu Recherche als Form und Selbstvergewisserung). Die ArchivarInnen als sichtende, selektierende, bewertende, bewahrende Instanzen mutieren zu den DatenkuratorInnen von morgen. Der Begriff des ""data curators"" ist schillernd: Die digitalen KuratorInnen sind die HerrscherInnen über die Schnittstellen, sie sind aber auch SammlungsmanagerInnen, mehr oder weniger kuratorische Freigeister, abhängig vom institutionellen Selbstverständnis der Archive. Sie stellen Corpora zu bestimmten Themen in Labs oder auf Plattformen zusammen, bieten digitale Werkzeuge zu deren Nutzung an und richten die virtuellen Archivräume der Zukunft ein, in denen wir forschen, uns weiterbilden und ""erleben"", in denen wir Teilhabe an Kultur erproben sollen. Die Archivzeugen in den Depots, wiedergeboren als digitale Objekte, multiplizieren deren kulturelle und soziale Erscheinungsformen 'als visualisierte, transkribierte und kommentierte Handschriften im Rahmen eines digitalen Editionsprojektes, als¬† Ausgangspunkt von Geschichten im analogen und virtuellen Museum, als Beweisstück aus dem Webportal in der öffentlichen Debatte, als Flaschenpost in den sozialen Medien, als im Kontext eines Nachlasses zu erschließendes Objekt in der Praxis des Archivs. Die digitalen Sammelobjekte der Zukunft 'seien es E-Mails, Social Media-Beiträge, Netzliteratur oder Serien 'transformieren den traditionellen Literaturbegriff. Die digitale Transformation, die gegenwärtig in den Wissenschafts- und Kultureinrichtungen zu gestalten ist, bringt ihre eigenen öffentlichen Irrtümer und ihre eigenen falschen Begriffe mit (Francis Bacon, In der öffentlichen Wahrnehmung kämpft hochgradig ausdifferenzierte Forschung 'und nicht allein historische und philologische 'aktuell gegen Erfahrbarkeitsdefizite. Archive haben in den vergangenen Jahren dazu beigetragen, den abstrakten sprachlichen Gegenstand erfahrbar und vorstellbar zu machen. Nicht die Wahl zwischen der Welt des Papiers und der Welt der Daten, sondern die erfindungsreiche Gestaltung von Anschaulichkeit und Erfahrung im digitalen Modus ist die Herausforderung, vor der Archive für Literatur gegenwärtig stehen. Flachware war lange die Krux der Literaturausstellungen. Wie arbeiten Archive im Zeitalter von 3D und 4D? Visualisierungspraktiken, die ein Manuskript nicht mehr als Pixelfläche, sondern als Datenkubus präsentieren, und digital erzeugte Objekte nicht mehr in genetisch-qualitativer, sondern in struktural-quantitativer Form, verändern das Grundverständnis vom Gegenstand der lesenden und schreibenden Fächer. Archive waren jahrhundertelang als räumliche Ordnungen gedacht: als Gebäude mit Gängen, Regalen, Schränken, in denen Schätze liegen, die jemand besitzt und die es aufgrund schwieriger konservatorischer Bedingungen am Ort zu untersuchen gilt. Zu den Versprechen des Digitalen gehört die virtuelle Verfügbarkeit, Durchsuchbarkeit und Erweiterbarkeit digitaler Daten, das Archiv als Literaturdatenzentrum. Damit löst sich, pointiert formuliert, der Begriff vom Archiv auf: Literaturdatenzentren kennen nurmehr virtuelle Räume, in denen existierende und künftige Daten überall zugänglich sind, sich teilen, verknüpfen und neu ordnen lassen. Aus den Praktiken der Datenspender und -nutzer entstehen Korpora und andere Forschungsdaten, die sich durch ihre Qualität und Anschlussfähigkeit zur Nachnutzung empfehlen. Die Vision von einem solchen Literaturdatenzentrum erscheint jedoch in mindestens zweierlei Hinsicht als unrealistisch: Zum einen lässt sich die Datenqualität und -vergleichbarkeit auf Dauer nicht nur durch temporär diese Daten Spendende und Nutzende sicherstellen, und die Daten lassen sich auch nicht einfach erhalten, ergänzen, pflegen. Zum anderen sind die physischen Objekte, die derzeit in Archiven liegen oder dort künftig eingehen, erst in digitale Daten zu übertragen; außerdem werden Sammlungen von Forschungsdaten in einigen Jahren selbst Archivobjekte. Die Objektgruppen der Archive vervielfältigen sich durch das Digitale ein weiteres Mal, und ihre Entwicklung zu Literaturdatenzentren ist möglicherweise bloß ein weiteres historisches Stadium einer erstaunlich stabilen epistemologischen Ordnung. Literaturarchive sind mehr als Sammlungen ihrer Vor- und Nachlässe. Sie entwickeln und verwalten Wissen um die Erhaltung und die Derzeit befinden wir uns in einer langwierigen technischen Transformationsphase, einer Materielle Dokumente oder digitale Daten 'das ist eine falsche Distinktion, auch digitale Daten haben materielle Träger. Daten lassen sich ablösen, unterscheiden sie sich darin vom Original? Wir stehen in einer Verunsicherung in Bezug auf das Original (nicht das Kunstwerk) und seine Reproduzierbarkeit (Walter Benjamin). Dabei ist das Sammeln digitaler Dokumente nicht zu verwechseln mit der Digitalisierung von Dokumenten: Diese setzt minimal beim Scan ein und geht maximal bis zur genetischen Edition. Eine andere Aufgabe der Literaturarchive ist das Sammeln und Aufbereiten digitaler Daten (digital born). Eine Herausforderung ist es, die technischen Zugänge und die Lesbarkeit der digitalen Datenträger aus den Anfängen des PC, der Mails und der fotografisch-/filmischen Selbstdokumentation zu gewährleisten. Wichtiger wird die Entwicklung der Standards & Normen, die Pflege und Sicherung der Metadaten und ihre intelligente Vernetzung. Erhalten (im doppelten Wortsinn) die Archive und verwalten sie künftig technisch und rechtliche aufbereitete Daten und dokumentieren deren Nutzung?",de,Reflexion institutionell Transformation literaturarchiven angesichts Digitalisierung überschreiten Einzelinstitution notwendig gemeinsam Praktik routin Standard infrastrukture entwickeln gelten Panel greifen Bedarf international interinstitutionell Ansatz Literaturarchive stehen Digitalisierung Vielzahl Herausforderung Begriff Praxis Materialität literaturarchivs befinden Transformationsprozess Institution literaturarchiv Grundsätzlichkeit konzeptionell Erfindung Jahrhundert Goethe dilthey thall durchlaufen Anforderung Partizipation Theimer Digitalisierung Beständ Umsetzung op szekely Adressierung Frage bringen Fall einher Umbau Routin Handlungsprogramme Befragung Neuerfindung Identität Institution Cook archivmitarbeiterinnen groß Informatisch Herausforderung aufgabenspektren betrauen öfter übernehmen Informatikerinn informatiker entscheidend Rolle Sammlungszugang Überlieferungspräsentation Datenkompetenze unverzichtbar Handwerkszeug modern Literaturarchive datendienstleistern wandeln Prozess bibliotheken lang beschäftigen exemplarisch stäck gelten folglich Aufgabe Herausforderung fortschreitend Digitalisierung kulturell gedächtnis speziell Literaturarchive bringen nachdenken Lösungsansätze entwickeln zukünftig begegnen Panel Community Literaturarchive Netzwerk Schriftenreihe Panel forcieren austausch Impuls digital Transformation literaturarchiven eben adressieren digital allerort vermeintlich archive hervorbringen erweitern stellen gewachsen Begriff Archiv Frage Phase begrifflich Reflexion setzen Literaturarchive Selbstbeschreibung überdenken digital Werkzeuge infrastruktur Durchdring praktik heutig Archivarinn Sammeln bewahren erschließen vermitteln häufig codeexpertinnen Kontext Digitalisierung neu Verständnis Praktik Handlungsprogramme Tätigkeit literaturarchive entwickeln Spektrum Objekt literaturarchive prozessieren wandeln weiten literaturarchive befinden Situation Materialität Objekthaftigkeit Beständ Sammlung neu begreifen Panel versammeln vertreterinnen bedeutend literaturarchiven Bernhard Fetz literaturarchiv Literaturmuseum österreichisch Nationalbibliothek Wien Marcel Lepper Weimar Sandra Richter deutsch Literaturarchiv Marbach Irmgard Wirtz Eybl schweizerisch literaturarchiv bern Moderation übernehmen Anna busch peer trilcken Potsdam theoretisch praxeologisch Blick suchen Panel Selbstverständnis digital Archiv Literatur Diskussion vorbereiten vertreterinn Positionierung Reflexion systematisch Ebene ausformulieren digital Kommunikation Archiv Bewegung auslösen Utopie Archivs objekt hüterinnen Mediatorinn umfassend bildungsbegriff lässt prozeß Tradition national kulturell Repräsentation Werk Fluide ubiquitär Verfügbarkeit Archivalie Prozessen Entkanonisierung einher Hochkultur Gipfelprinzip verlieren Geltung Fokus verschieben einzeln Werk diverser Arbeitsspur digital archiv sozial netzwerken Notizbücher biografisch Projekt recherch Form Selbstvergewisserung Archivarinn Sichtende selektierend bewertend bewahrend instanzen mutieren Datenkuratorinn Begriff data curators schillernd digital Kuratorinn Herrscherinn Schnittstelle sammlungsmanagerinnen kuratorisch Freigeister abhängig institutionell Selbstverständnis Archive stellen Corpora bestimmt Thema Labs Plattform bieten digital Werkzeug Nutzung richten virtuell Archivraum Zukunft forschen weiterbilden erleben Teilhabe Kultur erproben Archivzeuge Depot wiedergeboren digital Objekt multiplizieren kulturell sozial erscheinungsformen visualisiert transkribieren kommentiert handschriften Rahmen digital editionsprojekt Ausgangspunkt Geschicht Analog virtuell Museum Beweisstück webportal öffentlich Debatte Flaschenpost sozial Medium Kontext nachlasses erschließend Objekt Praxis Archivs digital Sammelobjekt Zukunft social Netzliteratur Serie transformieren traditionell Literaturbegriff digital Transformation gegenwärtig Kultureinrichtunge gestalten bringen öffentlich irrtümer falsch begriffe Francis Bacon öffentlich Wahrnehmung kämpfen hochgradig ausdifferenziert Forschung historisch philologisch aktuell Erfahrbarkeitsdefizite Archive beitragen abstrakt sprachlich Gegenstand erfahrbar vorstellbar Wahl Welt Papier Welt daten erfindungsreich Gestaltung Anschaulichkeit Erfahrung digital Modus Herausforderung Archive Literatur gegenwärtig stehen Flachwar Krux Literaturausstellunge arbeiten Archiv Zeitalter visualisierungspraktiken Manuskript pixelfläch datenkubus präsentieren Digital erzeugt Objekt Form verändern Grundverständnis Gegenstand lesend schreibend fäch Archive jahrhundertelang räumlich Ordnung denken Gebäude gäng regal Schränk Schätz liegen besitzen aufgrund schwierig konservatorisch Bedingung Ort untersuchen gelten Versprechen digital gehören virtuell Verfügbarkeit Durchsuchbarkeit Erweiterbarkeit digitaler daten Archiv Literaturdatenzentrum lösen pointiert formulieren Begriff Archiv Literaturdatenzentr kennen Nurmehr virtuell räume existierend künftig daten überall zugänglich teilen verknüpfen neu ordnen lassen Praktik Datenspender entstehen Korpora Forschungsdat Qualität Anschlussfähigkeit Nachnutzung empfehlen Vision Literaturdatenzentrum erscheinen mindestens zweierlei Hinsicht unrealistisch lässen Datenqualität Dauer Temporär daten spendend nutzend sicherstellen daten lassen einfach erhalten ergänzen pflegen physisch Objekt derzeit archiv liegen künftig eingehen digital daten übertragen Sammlung Forschungsdat archivobjeken objektgruppen Archiv vervielfältigen digital Mal Entwicklung literaturdatenzentren möglicherweise bloß historisch Stadium erstaunlich stabil epistemologisch Ordnung Literaturarchive Sammlung nachlässen entwickeln verwalt wissen Erhaltung derzeit befinden langwierig technisch Transformationsphase materiell dokument digital daten falsch Distinktion digital daten materiell Träger daten lassen ablösen unterscheiden Original stehen Verunsicherung Bezug Original Kunstwerk Reproduzierbarkeit Walter Benjamin sammeln Digitaler dokumenen verwechseln Digitalisierung dokumenten setzen minimal Scan maximal genetisch Edition Aufgabe Literaturarchive sammeln aufbereit Digitaler daten digital born Herausforderung technisch zugänge Lesbarkeit digital datenträger anfang pc mails Selbstdokumentation gewährleisten wichtig Entwicklung Standard Norm Pflege Sicherung metadaten intelligent Vernetzung erhalten doppelt Wortsinn Archiv verwalten künftig technisch rechtlich aufbereitet daten dokumentieren Nutzung,"[('literaturarchive', 0.3649562961196227), ('archiv', 0.26778351524620636), ('digital', 0.20705087791659296), ('archive', 0.1729459376347657), ('digitalisierung', 0.15772346970485993), ('praktik', 0.1324417446492071), ('literaturarchiv', 0.1324417446492071), ('daten', 0.13074749793472637), ('virtuell', 0.12760883329099743), ('objekt', 0.12741968627803288)]"
2022,DHd2022,SCHUMACHER_Mareike_GitMA_oder_CATMA_für_Fortgeschrittene___P.xml,GitMA oder CATMA für Fortgeschrittene Projektdaten via Git abrufen und mittels Python-Bibliothek weiterverarbeiten,"Mareike Schumacher (Technische Universität Darmstadt, Germany); Michael Vauth (Technische Universität Darmstadt, Germany); Dominik Gerstorfer (Technische Universität Darmstadt, Germany); Malte Meister (Technische Universität Darmstadt, Germany)","digitale Annotation, Datenanalyse, Visualisierung","Inhaltsanalyse, Annotieren, Einführung, Visualisierung, Literatur, Text","Dieser CATMA-6-Workshop richtet sich an fortgeschrittene CATMA User*innen mit Vorkenntnissen in digitaler Annotation, die im Rahmen der eigenen Arbeit oder von Forschungsprojekten mit größeren Mengen von Annotationsdaten operieren (wollen). Im Zentrum steht die Weiterverarbeitung und Analyse von Annotationsdaten. Wie greife ich über Git auf meine CATMA-Annotationsdaten zu? Wie erstelle ich individuelle, interaktive Visualisierungen meiner Annotationsdaten? Wie berechne ich die Übereinstimmung zwischen mehreren Annotator*innen? Diese und ähnliche Fragen werden während des Workshops beantwortet. CATMA (Gius et al. 2021) ist eine webbasierte, kollaborative Textannotations- und Analyse-Plattform, die seit 2008 an der Universität Hamburg und im Rahmen des DFG-geförderten Projektes forTEXT seit 2020 an der Technischen Universität Darmstadt entwickelt wird. Der Workshop bietet: Eine der wichtigsten Neuerungen von CATMA 6 gegenüber früheren Versionen ist die Umstellung auf eine projektzentrierte Nutzungsarchitektur. Am Beginn der Arbeit mit CATMA steht das Anlegen eines Projektes mit beliebig vielen Dokumenten, die analysiert werden sollen, und beliebig vielen Team-Mitgliedern, die daran arbeiten wollen. Zur Annotation können eigene Taxonomien entworfen oder auf der Plattform Niedrigschwelligkeit und Nähe zu traditionell-analogen Methoden der Geisteswissenschaften sind nach wie vor wichtige Grundsätze, die in CATMA implementiert sind. Doch mit zunehmender Verbreitung des Tools in den digitalen Geisteswissenschaften sind neben der Möglichkeit zu hermeneutisch-vielfältiger Textanalyse auch die Einhaltung von Best Practices und Standards, die innerhalb der Digital-Humanities-Community entwickelt wurden, von Bedeutung. Eine Verschmelzung von CATMA und Git zu ""GitMA"" ermöglicht beides. Dabei bleibt der Annotationsprozess selbst völlig frei gestaltbar. Die resultierenden Daten aber können zum Beispiel nach der Übereinstimmung der Annotierenden untereinander ausgewertet werden. Es ist möglich eine der Annotationen als ""Silver Annotation"" festzulegen und die anderen daran zu messen. Das festgestellte Disagreement kann zur Grundlage eines Disagreement-Tagsets werden, das über das Backend auch wieder ins Frontend der CATMA-GUI zurückgespielt werden kann (siehe Abb. 1). Dasselbe gilt für die nicht übereinstimmend annotierten Passagen, welche wiederum selbst durch Annotationen dargestellt/hervorgehoben werden können. So ergibt sich ein harmonischer Workflow vom Frontend zum Backend und zurück, der in Zukunft auch die Erstellung von Goldannotationen unterstützen wird. Die GitMA-Funktionalitäten werden im Rahmen dieses Workshops erstmals einem Fachpublikum vorgestellt. Neben der Vermittlung von Nutzungskompetenzen möchten wir darum auch eine kritische Diskussion anregen. Feedback zu Idee und Umsetzung der CATMA-Backend-Nutzung sind uns überaus willkommen! Der Workshop wird als ganztägiges hands-on Tutorial angeboten, das an einem oder an zwei aufeinander folgenden (halben) Tagen stattfinden kann. Teil 1 Pause Nutzer*innen, die Annotationen mit CATMA in Forschungsprojekten oder Lehrsituationen managen, sowie alle, die einen schnellen Workflow zwischen Annotation bzw. Annotationsbearbeitung und Annotationsauswertung benötigen. 30 Die benötigten Vorinstallationen von Git, Anaconda und Plotly können durch die Bereitstellung eines Docker-Image vermieden werden. Die Teilnehmer*innen sollten die Installation von Docker selbst auf einem eigenen Laptop (Touch Devices werden nicht unterstützt), den sie zum Workshop mitbringen, möglichst schon erledigt haben. Für die Durchführung des Workshops benötigen wir außerdem einen Beamer. Zur Vorbereitung sollten Teilnehmer*innen außerdem schon einen CATMA-Account erstellt (unter Die Teilnehmer*innen sollten über grundlegende Kenntnisse der Kommandozeile, Git und Python sowie Jupyter verfügen. Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Landwehrstraße 50A, 64293 Darmstadt Michael Vauth promoviert über ""Zur Annotation intradiegetischen Erzählens. Binnenerzählungen im literarischen Werk Heinrich von Kleists"" an der Technischen Universität Darmstadt. Er ist wissenschaftlicher Mitarbeiter im Forschungsprojekt EvENT (Evaluating Events in Narrative Theory) an der Technischen Universität Darmstadt. Zuvor hat er an der Technischen Universität Hamburg im Projekt hermA (Automatisierte Modellierung hermeneutischer Prozesse - Der Einsatz von Annotationen für sozial- und geisteswissenschaftliche Analysen im Gesundheitsbereich) gearbeitet. Er beschäftigt sich insbesondere mit der digitalen Narratologie und der Methodik der Netzwerkanalyse. Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Landwehrstraße 50A, 64293 Darmstadt Dominik Gerstorfer promoviert über ""Philosophische Fragen der Digital Humanities"" an der Universität Stuttgart. Derzeit ist er im DFG-Projekt forTEXT tätig, zuvor war er im Digital-Humanities-Projekt CRETA in Stuttgart beschäftigt. Dominik hat an der Universität Tübingen Philosophie, Politikwissenschaften und Soziologie (M.A.) studiert. Seine Forschungsschwerpunkte liegen in den Bereichen Wissenschaftstheorie, formale Methoden und Argumentationsanalyse. Im Rahmen von forTEXT beschäftigt sich Dominik u.a. mit Intertextualität, Ontologien und der Entwicklung von Kategoriensystemen. Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Landwehrstraße 50A, 64293 Darmstadt Malte Meister hat 2009 sein Informatik-Diplom (B.Sc.) in Kapstadt erworben. Im Rahmen des Abschlussprojekts für sein Diplom wurde er beauftragt, das Text-Annotations und -Analysetool CATMA, für die Universität Hamburg zu erstellen. Bis Anfang 2010 wirkte er im Team an CATMA mit, bevor er sich auf seine Karriere in der freien Wirtschaft konzentrierte. Nach mehr als zehn Jahren Berufserfahrung als Softwareentwickler und Teamleiter entschied er sich, wieder in die CATMA-Entwicklung einzusteigen. Er ist seit 2021 technischer Mitarbeiter an der TU Darmstadt und beschäftigt sich dort im Rahmen von forTEXT hauptsächlich mit dem Betrieb und der Weiterentwicklung von CATMA und den damit verbundenen Systemen. Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Landwehrstraße 50A, 64293 Darmstadt Mareike Schumacher koordiniert das DFG-Projekt forTEXT (",de,richten fortgeschritten Catma Vorkenntnissen Digitaler Annotation Rahmen Arbeit Forschungsprojekt groß Menge Annotationsdat operieren Zentrum stehen Weiterverarbeitung Analyse Annotationsdat greifen Git erstellen individuell interaktiv Visualisierung Annotationsdat berechnen Übereinstimmung mehrere ähnlich Frage Workshop beantworten Catma Gius et webbasiert kollaborativ Universität Hamburg Rahmen projekt Fortext technisch Universität Darmstadt entwickeln Workshop bieten wichtig Neuerung catma früh Version Umstellung projektzentriert Nutzungsarchitektur Beginn Arbeit Catma stehen Anlegen projektes beliebig dokumenten analysieren beliebig arbeiten Annotation Taxonomien entwerfen Plattform Niedrigschwelligkeit Nähe Methode geisteswissenschaften wichtig grundsatz Catma implementieren zunehmend Verbreitung Tools digital geisteswissenschaften Möglichkeit Textanalyse Einhaltung Best Practices Standard innerhalb entwickeln Bedeutung Verschmelzung Catma git gitma ermöglichen beide bleiben annotationsprozess völlig frei gestaltbar resultierend daten Übereinstimmung Annotierend untereinander auswerten annotatio silv Annotation festlegen messen festgestellt Disagreement Grundlage backend Frontend zurückgespielt sehen abb gelten übereinstimmend annotierten Passag wiederum annotation darstellen hervorheben ergeben harmonisch Workflow Frontend backend Zukunft Erstellung goldannotationen unterstützen Rahmen Workshop erstmals Fachpublikum vorstellen Vermittlung nutzungskompetenzen möchten kritisch Diskussion anregen Feedback Idee Umsetzung überaus willkommen Workshop ganztägig Tutorial anbieten aufeinander folgend halb stattfinden Pause Annotation Catma Forschungsprojekt lehrsituation managen schnell workflow Annotation Annotationsbearbeitung Annotationsauswertung benötigen Benötigte vorinstallationen git Anaconda plotly Bereitstellung vermeiden Installation Docker Laptop Touch devices unterstützen Workshop mitbringen möglichst erledigen Durchführung Workshop benötigen Beamer Vorbereitung erstellen grundlegend Kenntnisse Kommandozeil git python jupyter verfügen technisch Universität Darmstadt Institut Literaturwissenschaft Landwehrstraße Darmstadt Michael Vauth promovieren Annotation intradiegetisch Erzählen Binnenerzählunge literarisch Werk heinrich Kleist technisch Universität Darmstadt wissenschaftlich Mitarbeiter Forschungsprojekt Event Evaluating Event Narrative Theory technisch Universität Darmstadt zuvor technisch Universität Hamburg Projekt Herma automatisiert Modellierung hermeneutisch Prozesse Einsatz annotatio geisteswissenschaftlich Analyse gesundheitsbereich arbeiten beschäftigen insbesondere digital Narratologie Methodik Netzwerkanalyse technisch Universität Darmstadt Institut Literaturwissenschaft Landwehrstraße Darmstadt Dominik Gerstorfer promovieren philosophisch Frage Digital humaniteisen Universität Stuttgart derzeit fortext tätig zuvor Creta Stuttgart beschäftigen Dominik Universität tübingen Philosophie Politikwissenschaft Soziologie studieren Forschungsschwerpunkt liegen bereich Wissenschaftstheorie formal Methode Argumentationsanalyse Rahmen Fortext beschäftigen Dominik Intertextualität Ontologien Entwicklung Kategoriensysteme technisch Universität Darmstadt Institut Literaturwissenschaft Landwehrstraße Darmstadt malt Meister Kapstadt erwerben Rahmen Abschlussprojekt Diplom beauftragen Catma Universität Hamburg erstellen Anfang wirken Team Catma bevor Karriere frei Wirtschaft konzentrieren Berufserfahrung softwareentwickl Teamleiter entscheiden einsteigen technisch Mitarbeiter tu Darmstadt beschäftigen Rahmen fortext hauptsächlich Betrieb Weiterentwicklung Catma verbunden systemen technisch Universität Darmstadt Institut Literaturwissenschaft Landwehrstraße Darmstadt Mareike Schumacher koordinieren Fortext,"[('darmstadt', 0.4389608328101661), ('universität', 0.3066191366968474), ('catma', 0.29954470323161747), ('landwehrstraße', 0.1962404667765215), ('fortext', 0.18778685702854941), ('technisch', 0.18390532589582484), ('git', 0.1732349761996805), ('workshop', 0.15046394467456017), ('dominik', 0.12992623214976037), ('institut', 0.12936602257134205)]"
2022,DHd2022,KONLE_Leonard_Genitivmetaphern_in_der_Lyrik_des_Realismus_un.xml,Genitivmetaphern in der Lyrik des Realismus und der frühen Moderne,"Merten Kröncke (Universität Göttingen, Germany); Leonard Konle (Universität Würzburg, Germany); Fotis Jannidis (Universität Würzburg, Germany); Simone Winko (Universität Göttingen, Germany)","Metapher, Lyrik, Realismus, Moderne","Datenerkennung, Programmierung, Annotieren, Stilistische Analyse, Literatur, Text","Ein wichtiger Aspekt der sprachlichen Gestaltung literarischer Texte besteht im Einsatz von Metaphern, Metonymien und Tropen im Allgemeinen. Einzelnen Werken, aber auch ganzen Gattungen oder Epochen wird zugeschrieben, dass ihre Spezifik nicht zuletzt in einer jeweils charakteristischen Verwendungsweise uneigentlicher Rede gründe. Unter anderem betrifft das die Geschichte der Lyrik, das heißt die Geschichte einer Gattung, die laut Benjamin Specht ""in Bezug auf die Verwendung von Metaphern die weitesten Lizenzen besitzt"" (Specht, 2017: 90). Das Ziel dieses Beitrags besteht darin, den Gebrauch von Metaphern in der deutschsprachigen Lyrik des Realismus und der frühen Moderne Die literaturwissenschaftliche Forschung macht die Unterscheidung von realistischer und moderner Lyrik unter anderem am Aufkommen neuer, innovativer Formen uneigentlichen Sprechens fest. In der modernen Lyrik treten Metaphern auf, die als ""Radikalisierung, Komplizierung und Steigerung"" lyrischer Bildlichkeit (Hiebel 2005: 28), als Beitrag zur sprachlichen ""Verfremdung"" (Lamping, 2010: 148; vgl auch Lamping, 2008: 25f), als ""assoziativ-hermetische"" Muster (Specht, 2014: 5) oder auch als ""Blume[n] ohne Stiel auf der Oberfläche des Gedichts"" (Neumann, 1970: 195f) zu charakterisieren seien. Die Forschung dürfte sich einig sein, dass die moderne Metaphorik gegenüber der vorherigen, traditionellen Bildlichkeit zu größerer Individualität und Heterogenität tendiert (vgl. zur Homogenität der realistischen (Massen-)Lyrik und ihrer Sprachbilder z. B. Stockinger, 2010: 88). Doch durch welche Textmerkmale sich die neuen, modernen Metaphern im Einzelnen auszeichnen, wird unterschiedlich und zum Teil sogar gegensätzlich konzeptualisiert. Unser Beitrag untersucht nur Genitivmetaphern (""Das Lächeln der Natur"" usw.); andere Formen, zum Beispiel Adjektivmetaphern (""Die lächelnde Natur"" usw.), bleiben unberücksichtigt. Zumindest Hugo Friedrich ist allerdings der Auffassung, dass es sich bei Genitivmetaphern ohnehin um den häufigsten Typ von Metaphern in der (modernen) Lyrik handelt (Friedrich, 1992: 205). Im Normalfall der Genitivmetapher bezieht sich das Die hier untersuchten Phänomene (Metaphern in Genitivkonstruktionen) sind mithin nicht exakt identisch mit dem Gegenstand der literaturwissenschaftlichen Forschungsthesen (Metaphern im Allgemeinen), auch wenn man davon ausgehen darf, dass Aussagen über den einen Bereich ebenfalls relevant für den anderen Bereich sind. Eine weitere Relativierung betrifft das Untersuchungskorpus: Während sich sich die Forschungsaussagen in der Regel auf kanonisch-moderne sowie des Öfteren auf deutlich nach 1900 erschienene Gedichte beziehen, enthält das hier zu analysierende Korpus lediglich Texte der Jahrhundertwende um 1900 und damit der Die analysierten Gedichte stammen aus den 7 Anthologien des Realismus und den 13 Anthologien der Lyrik um 1900 mit insgesamt 6249 Texten, die wir im Rahmen des Projekts ""The beginnings of modern poetry - Modeling literary history with text similarities"" untersuchen. Bei den Anthologien um 1900 handelt es sich um Sammlungen, deren Herausgeber Gedichte aufgrund ihrer Modernität ausgewählt haben (siehe Tabelle 1). Aus diesem Korpus sind unter Verwendung von spaCy (Montani et al., 2021) 4300 Genitivkonstruktionen Drei Annotatoren haben Genitivkonstruktionen als metaphorisch oder nicht-metaphorisch annotiert. Annotiert wurden die beiden oben beschriebenen Formen von Genitivmetaphern, aber nur wenn sie in dem Muster Annotationen Realismus Annotationen Moderne Der annotierte Datensatz umfasst 625 Genitivkonstruktionen mit einem Agreement von 0.53 Für die automatische Metaphernerkennung verwenden wir neben den Annotationen ein deutsches Bert Model Die Erkennung der Metaphern geschieht in zwei Schritten. Im ersten Schritt werden regelbasiert Genitivkonstruktionen erkannt (siehe Ressourcen); im zweiten Schritt werden diese als ""nicht-metaphorisch"" oder ""metaphorisch"" klassifiziert. Das System zur Klassifikation von Genitivmetaphern setzt sich aus drei Komponenten zusammen: Supersenser, Affecter und Bert (siehe Abb. 1). Damit folgt der Aufbau dem in Tsvetkov et al. (2014) vorgestellten Ansatz Metaphern unter Berücksichtigung von Abstraktheit, Vorstellbarkeit und Wortklasse ihrer Komponenten zu klassifizieren. Das Training des Supersenser Moduls wird auf den FastText Vektoren der Wörter aus den 20 größten Supersense-Klassen für Substantive aus GermaNet durchgeführt. Diese werden durch überwachte Dimensionsreduktion (Szubert et al., 2019) in einen kleineren Raum mit 10 Dimensionen projiziert. Im Gegensatz zur direkten Verwendung von GermaNet als Eingabe in das System können so out-of-vocabulary Probleme vermieden werden. Außerdem wird durch die Projektion eine reichhaltigere Repräsentation erzeugt in der Supersense-Klassen in Beziehung gesetzt werden können. Eine Evaluation mittels kNN Klassifikation von Substantiven im projizierten Raum in ihre Supersense Klasse ergibt einen F-Score von 0.65. Das Affecter Modul erhält ebenfalls FastText Vektoren, sowie die zugehörigen Werte aus der Wortliste von Köper und Schulte im Walde (2016). Eine 4-fach Regression durch ein MLP erreicht R Für die Klassifikation von Metaphern werden die Ausgaben aus Supersenser und Affecter an ein nach (Gao et al., 2019) modifiziertes BERT Modell übergeben. Dieses reicht nicht nur das CLS-Token, sondern auch die Embeddings der Genitivkonstruktion weiter. Bei der Unterscheidung zwischen Metaphern und sonstigen Genitivkonstruktionen erreicht das System einen F1 Score von 0.75 (Details siehe Tabelle 3). Die Klassifikation von Metaphern aus dem Realismus wird zwar leicht besser evaluiert als die aus den modernen Anthologien, der Effekt ist für die weitere Analyse aber vernachlässigbar. Es gibt keinen Unterschied zwischen den Epochen in Hinsicht auf die Menge der Genitivkonstruktionen und den Anteil der Metaphern daran. Die These von der größeren Heterogenität und Individualität der modernen Metaphern haben wir mit zwei Operationalisierungen untersucht: Nimmt der Anteil an seltenen Wörtern zu und steigt der Type-Token-Ratio bei Metaphern der Moderne? Den Anteil der seltenen Wörter haben wir mit einem einfachen Verfahren überprüft, nämlich ob die Token im Wordembedding enthalten sind; wenn nicht, wurden sie als ""seltene Wörter"" identifiziert. Solche Wörter sind in der Lyrik häufig, zumeist handelt es sich um Komposita oder um Schreibvarianten aufgrund der Anpassung ans Metrum. Die Verwendung solcher seltenen Wörter ist auch schon im Realismus häufig: 26%, findet sich aber noch einmal häufiger in der Moderne 33%. Wir haben den Type-Token-Ratio beider Bestandteile der Genitivmetaphern jeder Epoche untersucht und können auch hier einen signifikanten Unterschied feststellen: Bei den Metaphern der Moderne ist die Variabilität des Wortmaterials deutlich größer (siehe Abb. 2 und 3). Insgesamt können wir also den Eindruck bestätigen, dass die Metaphern der Moderne -- und das noch vor dem Expressionismus -- heterogener und individueller sind. Die These zur Vergrößerung bzw. Verkleinerung der Bildspanne in den Metaphern haben wir überprüft, indem wir den Kosinusabstand der Substantive im FastText-Wordembedding gemessen haben. Wie Abb. 4 zeigt, hat der Abstand weder zu- noch abgenommen. Die These, dass in der Moderne der Abstand zwischen den Nomina einer Genitivmetapher in Hinsicht auf die Abstraktheit zugenommen hat, haben wir mit den Abstraktheits-Werten überprüft, die das oben dargestellten Affecter-Moduls vorhergesagt hat (siehe Abb. 5), ebenso wie die These, dass die Moderne insgesamt zu abstrakteren (oder gerade zu weniger abstrakten) Metaphern neigt. Die Forschungsthesen lassen sich durch unsere Daten nicht bestätigen: Weder verändert sich das Level der durchschnittlichen Abstraktheit oder Konkretheit noch die Ein Blick auf die beliebtesten Wörter der Metaphern, getrennt nach der Verwendung in der Kopfposition oder im Genitiv, zeigt einige interessante Verschiebungen. Bei den Kopfnomina hat ""Meer"" eine steile Karriere von Rang 10 auf Rang 1 gemacht, während ""Hauch"", ""Geist"" und ""Traum"" deutlich weniger verwendet werden (siehe Abb. 6). Insgesamt sind die Wiederholungen bei den Kopfnomina weniger häufig und sie nehmen in der Moderne noch ab. Die Genitivnomina (Abb. 7) dagegen zeichnen sich durch zahlreiche Wiederholungen aus, allerdings nimmt auch hier die Frequenz in der Moderne ab. Auffällig ist, wie stabil die Wortlisten in den oberen Rängen sind, mit der Ausnahme von ""Glücks"" und ""Todes"", die in der Moderne deutlich häufiger werden. Insgesamt konnten wir einige der von der Literaturwissenschaft aufgestellten Vermutungen bestätigen, z. B. dass die Komplexität der Metaphern in der Moderne ansteigt. Da viele der Forschungsthesen für die Moderne insgesamt formuliert wurden, müssen wir allerdings dort, wo wir diese nicht bestätigen konnten, es zumindest offen halten, ob sie nicht auf spätere Phasen zutreffen. Allerdings deuten wir unsere Befunde doch so, dass die bisherige Forschung den Bruch zwischen den Epochen deutlich überbetont hat. Unsere Ergebnisse weisen vielmehr darauf hin, dass es sich um eine semantische Evolution handelt. Die hier dargestellten Ergebnisse sollen außerdem in Zukunft noch verbessert werden, indem die regelbasierte Erkennung der Genitivkonstruktionen auf eine bessere Grundlage gestellt wird: Da wir zur Zeit kein Korpus an Gedichten haben, in dem alle Genitivmetaphern annotiert sind, können wir den Recall nicht einschätzen. Außerdem soll ein umfassendes Korpus aus Texten des 19. Jahrhundert das Training eines domänenangepassten Fasttext-Modells ermöglichen, das historisch angemessenere semantische Distanzen zurückgibt. Nicht zuletzt soll auch die Erkennungsgenauigkeit des Systems zur Erkennung der Metaphern durch eine Vergrößerung des Trainingskorpus, durch eine aufwendigere Hyperparameter-Optimierung und durch Arbeit an der Architektur verbessert werden. Ein etwas anspruchsvolleres Ziel besteht außerdem in dem Versuch, die Typisierung der Metaphern auf eine andere Grundlage zu stellen, etwa durch Anschluss an die Kategorien linguistischer Metapherntheorien oder an große Ontologien. Nicht zuletzt werden wir untersuchen, wo die kanonisierten Lyriker dieser Zeit, George, Hofmannsthal, Holz und Rilke, stehen.",de,wichtig Aspekt sprachlich Gestaltung literarisch Text bestehen Einsatz Metapher Metonymi Trope einzeln Werk Gattung epochen zuschreiben Spezifik zuletzt jeweils charakteristisch Verwendungsweise uneigentlich Rede gründen betreffen Geschichte Lyrik Geschichte Gattung laut Benjamin spechen Bezug Verwendung Metapher Weiteste Lizenz besitzen spechen Ziel Beitrag bestehen Gebrauch Metapher deutschsprachig Lyrik Realismus früh Moderne literaturwissenschaftlich Forschung Unterscheidung Realistischer modern Lyrik aufkommen neu innovativ Form uneigentlich Sprechen fest modern Lyrik treten Metapher Radikalisierung Komplizierung Steigerung lyrisch Bildlichkeit hiebel Beitrag sprachlich Verfremdung lamping vgl lampehen Muster spechen blume n Stiel Oberfläche Gedicht Neumann charakterisieren Forschung dürfen einig modern Metaphorik vorherig traditionell Bildlichkeit groß Individualität Heterogenität tendieren Homogenität realistisch Sprachbilder Stockinger Textmerkmal modern Metapher einzeln Auszeichn unterschiedlich sogar gegensätzlich konzeptualisieren Beitrag untersuchen genitivmetaphern lächeln Natur Form adjektivmetaphern lächelnd Natur bleiben unberücksichtigt zumindest Hugo friedrich Auffassung Genitivmetapher ohnehin häufig typ Metapher modern Lyrik handeln friedrich Normalfall Genitivmetapher beziehen untersucht phänomen Metapher genitivkonstruktion mithin exakt identisch Gegenstand literaturwissenschaftlich Forschungsthese Metapher ausgehen Aussage Bereich ebenfalls relevant Bereich Relativierung betreffen Untersuchungskorpus Forschungsaussage Regel öfteren deutlich erschienen Gedicht beziehen enthalten analysierend Korpus lediglich Text Jahrhundertwende analysiert Gedicht stammen Anthologie Realismus Anthologie Lyrik insgesamt texten Rahmen Projekt The beginning of modern poetry Modeling Literary History With Text similariteisen untersuchen Anthologie handeln Sammlung Herausgeber gedicht aufgrund Modernität auswählen sehen Tabell Korpus Verwendung Spacy Montani et genitivkonstruktion Annotator genitivkonstruktionen metaphorisch annotiert annotiert beschrieben Form Genitivmetapher Muster annotation Realismus annotation Moderne annotiert Datensatz umfassen genitivkonstruktionen Agreement automatisch Metaphernerkennung verwenden annotationen deutsch bern Model Erkennung Metapher geschehen Schritt Schritt Regelbasiert genitivkonstruktionen erkennen sehen Ressource Schritt metaphorisch klassifizieren System Klassifikation Genitivmetapher setzen Komponente supersens affect beren sehen abb folgen Aufbau tsvetkov Et vorgestellt Ansatz metaphern Berücksichtigung Abstraktheit Vorstellbarkeit wortklasse Komponente klassifizieren Training supersenser Modul Fasttext vektoren Wörter groß Substantiv rmanen durchführen überwacht Dimensionsreduktion szuberen Et klein Raum Dimension projizieren Gegensatz direkt Verwendung rmanen eingabe System Problem vermeiden Projektion reichhaltig Repräsentation erzeugen Beziehung setzen Evaluation mittels Knn Klassifikation substantiven projiziert Raum supersense Klasse ergeben Affecter Modul erhalten ebenfalls fasttext vektoren zugehörig Wert Wortliste Köper schulen Walde Regression mlp erreichen r Klassifikation Metapher Ausgabe Supersenser affecter gao et modifiziert bert Modell übergeben reichen embeddings Genitivkonstruktion Unterscheidung Metapher Sonstig genitivkonstruktion erreichen System score Detail sehen Tabell Klassifikation Metapher Realismus evaluieren modern Anthologie Effekt Analyse vernachlässigbar Unterschied Epoche Hinsicht Menge genitivkonstruktion Anteil Metapher These groß Heterogenität Individualität modern Metapher Operationalisierung untersuchen nehmen Anteil Selten wörtern steigen Metapher Moderne Anteil selten Wörter einfach Verfahren überprüfen nämlich Toke Wordembedding enthalten selten Wörter identifizieren Wörter Lyrik häufig zumeist handeln Komposita schreibvariann aufgrund Anpassung an Metrum Verwendung selten Wörter Realismus häufig finden häufig modern beide Bestandteil Genitivmetapher Epoche untersuchen signifikant Unterschied feststellen Metapher Moderne Variabilität Wortmaterial deutlich groß sehen abb insgesamt Eindruck bestätigen Metapher modern Expressionismus heterogen Individueller These Vergrößerung Verkleinerung Bildspann Metapher überprüfen Kosinusabstand Substantiv messen abb zeigen Abstand weder abnehmen These Moderne Abstand Nomina Genitivmetapher Hinsicht Abstraktheit zunehmen überprüfen dargestellt vorhersagen sehen abb These modern insgesamt abstrakteren abstrakt metaphern neigen Forschungsthese lassen daten bestätigen weder verändern Level durchschnittlich Abstraktheit Konkretheit Blick Beliebteste Wörter Metapher trennen Verwendung Kopfposition genitiv zeigen interessant Verschiebung Kopfnomina Meer steil Karriere Rang Rang Hauch isen traum deutlich verwenden sehen abb insgesamt wiederholungen Kopfnomina häufig nehmen Moderne Genitivnomina abb zeichnen zahlreich Wiederholung nehmen Frequenz moderne auffällig stabil wortlister oberer Räng Ausnahme glücks todes moderne deutlich häufig insgesamt Literaturwissenschaft aufgestellt vermutung Bestätig Komplexität Metapher Moderne ansteigt Forschungsthese modern insgesamt formulieren bestätigen zumindest halten spät Phase zutreffen deuten Befund bisherig Forschung Bruch Epoche deutlich überbetont Ergebnis weisen vielmehr semantisch Evolution handeln dargestellt Ergebnis Zukunft verbessern regelbasiert Erkennung genitivkonstruktionen gut Grundlage stellen Korpus Gedicht Genitivmetapher annotiert Recall einschätzen umfassend Korpus Text Jahrhundert Training domänenangepasster ermöglichen historisch angemessener Semantische distanzen zurückgiben zuletzt Erkennungsgenauigkeit System Erkennung Metapher Vergrößerung Trainingskorpus aufwendiger Arbeit Architektur verbessern anspruchsvoller Ziel bestehen Versuch Typisierung Metapher Grundlage stellen Anschluss Kategorie linguistisch Metapherntheorie Ontologi zuletzt untersuchen kanonisierten Lyriker George Hofmannsthal Holz Rilke stehen,"[('metapher', 0.5449142264223302), ('moderne', 0.2434217751317772), ('genitivmetapher', 0.2325345863584903), ('modern', 0.22570203894951688), ('lyrik', 0.17338179931619596), ('genitivkonstruktion', 0.15470595596426473), ('genitivkonstruktionen', 0.13287690649056588), ('realismus', 0.1208843933943892), ('gedicht', 0.11351415267557609), ('anthologie', 0.10472318011272737)]"
2022,DHd2022,MEISTER_Malte_GitMA_Poster__CATMA_Daten_via_Git_abrufen_und_.xml,GitMA-Poster   CATMA-Daten via Git abrufen und mittels Python-Bibliothek weiterverarbeiten,"Malte Meister (Technische Universität Darmstadt, Germany); Michael Vauth (Technische Universität Darmstadt, Germany); Dominik Gerstorfer (Technische Universität Darmstadt, Germany)","CATMA, Git, Python","Annotieren, Bearbeitung, Einführung, Visualisierung, Daten, Software","Etwas zu erinnern heißt nicht, es abzuspeichern, sondern auch, es abzurufen und weiter zu prozessieren. Denn nur im produktiven Anschluss erhält die Erinnerung eine Bedeutung. Diese Beobachtung trifft CATMA (Computer Assisted Text Markup and Analysis) ist eine kollaborative Textannotations- und Analyse-Plattform, die in den Digital Humanities gut etabliert ist und von vielen Projekten aktiv genutzt wird Der genaue Aufbau der Datenstrukturen wird auf der CATMA Webseite dokumentiert (Petris 2020): Jedes Dokument, jede Annotation Collection einschließlich der Annotationen, sowie jedes Tagset einschließlich der zugehörigen Tags werden im Backend einzeln repräsentiert. Besonders wichtig für die Weiterverarbeitung der Annotationsdaten sind die Informationen, mit denen die einzelnen Annotationen repräsentiert werden: Die Nutzer:innen können sowohl auf eigene als auch auf mit ihnen geteilte Daten in Form von Git Repositorien zugreifen. Diese stellen damit eine Art Programmierschnittstelle (API) zum Abruf von CATMA-Annotationen dar, welche auf den lokalen Rechner heruntergeladen oder in anderen Tools weiterverarbeitet werden können. Im Fachbereich für Digital Philology an der TU Darmstadt ist außerdem eine Insgesamt ist das zentrale Anliegen des Git Access, CATMA-Daten direkt verfügbar zu machen, damit Nutzer:innen nicht unbedingt an die schon in CATMA vorhandenen Funktionalitäten gebunden sind. Dadurch kann der Workflow zwischen Annotation, Annotationsauswertung und Annotationsüberarbeitung deutlich schneller werden. Das ist besonders für Nutzer:innen relevant, die sich 'unter anderem im Rahmen von Forschungsprojekten 'um die Organisation und Evaluierung von Annotationen kümmern. Mit unserem Poster werden wir diesen Workflow detailliert darstellen. Das Poster soll also auch als eine Art Bedienungsanleitung für die Nutzung des CATMA Git Access fungieren und Best Practices zeigen. Dabei werden wir folgende Schritte abdecken:",de,erinnern abspeichern abzurufen prozessieren produktiv Anschluss erhalten Erinnerung Bedeutung Beobachtung treffen Catma Computer Assisted Text Markup And Analysis kollaborativ Digital Humanitie etablieren Projekt aktiv nutzen genau Aufbau Datenstrukture Catma Webseite dokumentieren Petris jeder Dokument Annotation Collection einschließlich annotation jeder Tagset einschließlich zugehörig tags backend einzeln repräsentieren wichtig Weiterverarbeitung Annotationsdat Information einzeln annotatio repräsentieren Nutzer innen sowohl geteilt daten Form git Repositorien zugreifen Stelle Art programmierschnittstelle Api Abruf dar lokal Rechner heruntergeladen Tools weiterverarbeiten fachbereich Digital Philology tu Darmstadt insgesamt zentral Anliegen git access direkt verfügbar Nutzer innen unbedingt Catma vorhanden Funktionalität binden Workflow Annotation Annotationsauswertung Annotationsüberarbeitung deutlich schnell Nutzer innen relevant Rahmen forschungsprojekten Organisation Evaluierung annotation kümmern unser Poster Workflow detailliert darstellen Poster Art Bedienungsanleitung Nutzung catma git Access fungieren Best Practices zeigen folgend Schritt abdecken,"[('git', 0.3115435721737281), ('catma', 0.2873052664620644), ('nutzer', 0.20459648500574826), ('einschließlich', 0.19881634109301913), ('innen', 0.17875956128005846), ('access', 0.17123448716202566), ('annotation', 0.15091367334475847), ('workflow', 0.1275182829810326), ('repräsentieren', 0.12253776567774842), ('poster', 0.12026330642049925)]"
2022,DHd2022,GERSTORFER_Dominik_Kategorientheoretische_Ontologieentwicklu.xml,Kategorientheoretische Ontologieentwicklung und Wissensmodellierung für die Digital Humanities,"Dominik Gerstorfer (TU Darmstadt, Germany)","Ontologie, Kategorientheorie, Formalisierung, Wissensrepräsentation","Strukturanalyse, Modellierung, Theoretisierung","Formale Modellierung von Wissen spielt in den Digital Humanities eine nicht zu unterschätzende Rolle. Daten müssen organisiert, kategorisiert, gespeichert, abgerufen und verarbeitet werden. Hierzu werden häufig Datenbank- und Ontologiesprachen wie SQL und RDF/OWL oder auch semantische Netzwerke eingesetzt, die als In diesem Beitrag werden Die mathematische Kategorientheorie wurde in den 1940er-Jahren von Saunders Mac Lane und Samuel Eilenberg entwickelt, um verschiedene mathematische Felder und Theorien zu vergleichen. Die grundlegende Einsicht hinter der Kategorientheorie formuliert Mac Lane (1998: 1) wie folgt: ""Category theory starts with the observation that many properties of mathematical systems can be unified and simplified by a presentation with diagrams of arrows."" Eine Kategorie In der Regel wird die Kategorientheorie in der Mathematik verwendet, um Theorien zu analysieren und weiter zu verallgemeinern. Man könnte auch davon sprechen, dass es sich um Metamathematik handelt, die Mathematik als Anwendungsgegenstand hat. Und obwohl die Kategorientheorie als Die angewandte Kategorientheorie oder Dies ist unter anderem in der Physik (Abramsky / Coecke 2007; Baez / Stay 2010), Linguistik (Coecke et al.¬†2010), den Neurowissenschaften (Brown / Porter 2008), Informatik (Ehrig et al. 2001) und der Philosophie (Landry 2017) geschehen. Gerade in der Informatik spielt die Kategorientheorie eine ausgezeichnete Rolle, da formale Logik und Mengentheorie in der theoretischen Informatik und funktionale Programmiersprachen wie Haskell oder Module wie Catlab.jl für Julia in der angewandten Informatik durch sie verbunden sind. In den oben genannten Fällen hat sich gezeigt, dass die angewandte Kategorientheorie einige für die Digital Humanities attraktive Eigenschaften aufweist: Der hohe Abstraktionsgrad und der Umstand, dass nur Objekte und Pfeile in einer Kategorie vorkommen, erlauben einen leichten Einstieg in kategorienthoretische Modellierung und der Einsatz von kommutativen Diagrammen zur Analyse macht die Verwendung benutzerfreundlich. Gleichzeitig können Modellierungen aufgrund der Modularität und Kompositionalität der Theorie auch sehr komplexe Sachverhalte repräsentieren, ohne selbst unüberschaubar zu werden. Darüber hinaus bietet die Kategorientheorie vielfältige Anschlussmöglichkeiten an andere mathematische Bereiche. Sollte sich herausstellen, dass formale Logik oder Grafentheorie benötigt wird, ist es ein Leichtes die nötigen Übergänge herzustellen. In diesem Sinne kann die Kategorientheorie in den Digital Humanities als leichtgewichtiges Modellierungstool eingesetzt werden, das nach Bedarf erweitert und skaliert werden kann. Dies soll nun anhand der von Spivak und Kent (2012) entwickelten  Zu beachten ist, dass Da es jederzeit möglich ist ein bestehendes Da  Im Vergleich zu RDF/OWL zeichnen sich Im Vergleich mit Datenbanken zeichnen sich Semantische Netze und Der Vortrag ist zweigeteilt: Im ersten Teil werden Im zweiten Teil werden dann die im ersten Teil gewonnen Einsichten anhand von Beispielen erläutert und gezeigt, wie die Wissensmodellierung durch Das übergreifende Ziel des Vortrags ist es zu zeigen, dass eine mathematisch-theoretische Fundierung der Ontologieentwickung und Wissensmodellierung sowohl das geisteswissenschaftliche Denken und Arbeiten unterstützen kann, als auch die technische Entwicklung und informatische Implementation treiben kann. Die Kategorientheorie hat in der Mathematik und Informatik sowie in Naturwissenschaften wie Physik, Chemie und Genetik bereits erfolgreich unter Beweis gestellt, ein geeignetes Denkwerkzeug zu sein. Der größte Nutzen in diesen Feldern ist durch Systematisierung und Vergleichbarkeit mit bzw. Anschlussfähigkeit an andere Forschungsgebiete entstanden. Diese Effekte gilt es auch für die",de,formal Modellierung wissen spielen Digital Humanitie unterschätzend Rolle daten organisieren kategorisieren speichern abgerufen verarbeiten hierzu häufig ontologiesprach sql rdf owl semantisch netzwerke einsetzen Beitrag mathematisch Kategorientheorie Saunder Mac lan Samuel Eilenberg entwickeln verschieden mathematisch feld Theorie vergleichen grundlegend Einsicht Kategorientheorie formulieren Mac lane folgen Category Theory Starts with -- Observation thaen many properties of mathematical systems can be unified and Simplified by Presentation with diagrams of Arrow Kategorie Regel Kategorientheorie Mathematik verwenden Theorie analysieren verallgemeinern sprechen Metamathematik handeln Mathematik Anwendungsgegenstand obwohl Kategorientheorie angewandt Kategorientheorie Physik Abramsky Coecke Baez Stay Linguistik Coecke et Neurowissenschaft Brown Porter Informatik ehrig et Philosophie landry geschehen Informatik spielen Kategorientheorie ausgezeichnet Rolle formal logik Mengentheorie theoretisch Informatik funktional programmiersprach haskell Module Julia angewandt Informatik verbinden genannt Fall zeigen angewandt Kategorientheorie Digital Humanitie attraktiv eigenschaft Aufweist hoch abstraktionsgrad Umstand objekt Pfeil Kategorie vorkommen erlauben leicht Einstieg kategorienthoretisch Modellierung Einsatz kommutativ Diagramm Analyse Verwendung benutzerfreundlich gleichzeitig modellierungen aufgrund Modularität Kompositionalität Theorie komplex sachverhaln repräsentieren unüberschaubar hinaus bieten Kategorientheorie vielfältig anschlussmöglichkein mathematisch Bereich herausstellen formal logik Grafentheorie benötigen leicht nötig übergänge herstellen Sinn Kategorientheorie Digital Humanitie leichtgewichtig Modellierungstool einsetzen Bedarf erweitern skalieren anhand spivak Kent entwickeln beachten jederzeit Bestehend Vergleich rdf owl zeichnen Vergleich datenbanken zeichnen semantisch Netz Vortrag zweigeteilen gewinnen einsichen anhand Beispiel erläutern zeigen Wissensmodellierung übergreifend Ziel Vortrag zeigen Fundierung Ontologieentwickung Wissensmodellierung sowohl geisteswissenschaftlich denken arbeiten unterstützen technisch Entwicklung informatisch Implementation treiben Kategorientheorie Mathematik Informatik naturwissenschaften Physik Chemie Genetik erfolgreich Beweis stellen geeignet Denkwerkzeug groß Nutzen feldern Systematisierung Vergleichbarkeit Anschlussfähigkeit forschungsgebien entstehen effekte gelten,"[('kategorientheorie', 0.5893029373567941), ('informatik', 0.20854162612823468), ('mathematik', 0.16039244283551304), ('angewandt', 0.14530446527526814), ('mathematisch', 0.141523414048466), ('wissensmodellierung', 0.126538036001442), ('coecke', 0.126538036001442), ('mac', 0.11786058747135882), ('physik', 0.11170383975914372), ('theorie', 0.10508847888533877)]"
2022,DHd2022,FLÜH_Marie_Jung__wild__emotional__Rollen_und_Emotionen_Jugen.xml,"Jung, wild, emotional?   Rollen und Emotionen Jugendlicher in zeitgenössischer Fantasy-Literatur","Marie Flüh (Universität Hamburg, Germany); Mareike Schumacher (Technische Universität Darmstadt)","Netzwerkanalyse, Emotions- und Genderanalyse, NER","Entdeckung, Beziehungsanalyse, Annotieren, Netzwerkanalyse, Visualisierung, Text","Im literatur- und kulturwissenschaftlich ausgerichteten Projekt Dieser Beitrag zielt darauf ab, das für phantastische Literatur als genrekonstitutiv geltende, aber recht allgemein beschriebene ""Klima des Grauens"" (Caillois 1974: 56), das Unheimliche (Todorov 2018), auszudifferenzieren. Darüber hinaus betrachten wir die für phantastische Literatur als spezifisch herausgestellten Emotionstypen in Abhängigkeit zu den Genderrollen, die den Emotionssender:innen zugeschrieben werden. Dabei rücken wir die 'ebenfalls für das betrachtete Genre typischen 'jugendlichen Hauptfiguren in den Fokus. Der Beitrag widmet sich drei eng miteinander verknüpften Forschungsfragen: Welche Genderrollen werden den Protagonist:innen in Fantasyromanen zugeschrieben? Welche Emotionstypen bestimmen das Korpus und wie sehen die Rollen- und Emotionsprofile der Protaginist:innen aus? Fallen genderstereotype, statische Muster auf? Unser Beitrag knüpft direkt an eine allgemeinere Fallstudie zum Thema Genderrollen in zeitgenössischer Jugend-Fantasy-Literatur an (Flüh, Horstmann und Schumacher, im Erscheinen) und vertieft die gewonnenen Einsichten. Grundlage der bereits abgeschlossenen Fallstudie stellen 28 deutschsprachige kontemporäre (im Zeitraum zwischen 2015 und 2020 publizierte) Fantasy-Romane für Jugendliche dar. Hierbei haben wir im Rahmen eines Mixed-Methods-Ansatzes Emotionen in Abhängigkeit zu Genderkategorien (männlich, weiblich, neutral) betrachtet, indem ein eigens trainierter Gender-Classifier (Schumacher 2021), der auf Conditional-Random-Fields-Algorithmen (vgl. Sutton und McCallum 2010) basiert und mit dem Stanford Named Entity Recognizer (vgl. Finkel et al. 2005) kompatibel ist, zur automatischen Annotation von Genderrollen mit der digitalen manuellen Annotation von Emotionsinformationen kombiniert wurde. Das Korpus wurde zunächst mit dem Gender-Classifier annotiert, der in allen Texten männliche, weibliche und neutrale Figurenrollen markiert. Die Erkennungsgenauigkeit der genutzten Version erreicht über alle Kategorien hinweg bei gattungsspezifischem Testmaterial einen F1-Score von rund 72% (vgl. Schumacher 2021). Auf die automatische Vorannotation aufbauend, wurde in 25 Romanen mit dem Textanalysetool CATMA (Gius et al. 2021) eine taxonomiebasierte Emotionsanalyse durchgeführt. Die digitale manuelle Annotation funktioniert auf Grundlage eines für die Emotionsanalyse in literarischen Texten entworfenen Tagsets und hierfür entworfenen Guidelines (vgl. Flüh 2020). Die Emotionsanalyse bezog sich auf die Textstellen, an denen vom Gender-Classifier besonders zahlreiche Genderannotationen gemacht wurden. An diesen Gender-Peaks wurde jeweils das semantische Umfeld der Gender-Annotationen nach Emotionsinformationen untersucht. Wir fokussieren also das unmittelbare Textumfeld der Figurenreferenzen und bestimmen, ob und welche emotionstragenden Textstrukturen zu finden sind. Unabhängig von Genderrollen zeigt sich, dass im Korpus Basisemotionen mit negativer Qualität romanübergreifend die Erzählwelten bestimmen. Im gesamten Korpus etabliert sich ein Emotionsprofil, das sich mit abnehmender Häufigkeit zusammensetzt aus: Ein Klima des Grauens etabliert sich deutlich über die besonders häufig vorkommenden Angst-Emotionen. Ein genauer Blick auf die Vertreter dieser Kategorie zeigt ein facettenreiches Emotionsprofil, das unterschiedliche Angstzustände und Spielarten der Angst beinhaltet (s. Tabelle 1). Die Annotation der Romane zeigt darüber hinaus, dass gerade die beiden bewusst bedeutungsoffen gestalteten Annotationskategorien ""UNCATEGORIZABLE"" (797 Annotationen) und ""PROBLEMFÖLLE"" (813 Annotationen) quantitativ ins Gewicht fallen. Während in die Kategorie ""Problemfälle"" Emotionstypen fallen, die u.a. mehrere polare Gegensätze vereinen, also nicht eindeutig als positiv oder negativ kategorisiert werden können, versammelt die Oberkategorie ""Uncategorizable"" Emotionen, die nicht einer der Basisemotionen zugeordnet werden können, die aber in ihrer Polarität durchaus eindeutig sind. Viele Textpassagen lassen sich auf Grundlage der strukturorientierten Typologisierung, die sich an der Einteilung von Basisemotionen orientiert (Ekman 1972, Schwarz-Friesel 2007), nicht adäquat beschreiben. In diesen Fällen wurden weitere Emotionstypen definiert; besonders ins Gewicht fallen dabei: Die häufigsten Vertreter dieser Kategorien nuancieren das Emotionsprofil. Scham, Aggression und Bedauern weisen eine negative Qualität auf, während Interesse und Erstaunen eher in die Kategorie positiver Emotionstypen fallen. Auffällig ist, dass alle hier vertretenen Emotionstypen im Diskurs über Sprache und Emotionen als strittige Emotionstypen verhandelt werden. Scham, eine eher intrasubjektiv empfundene Emotion, wird häufig mit ähnlichen Emotionskategorien wie Schuld, Reue oder Bedauern in Verbindung gebracht. Unklar ist hierbei, ob Scham eine eigene Kategorie darstellt oder nicht. Deutlich explosiver und als Selbst- oder Fremdschädigung auftretende Aggressionen lassen sich als Trieb oder als Emotion beschreiben. Fraglich ist auch, ob Erstaunen und Interesse als Emotion klassifiziert werden sollen (Schwarz-Friesel 2007). Hier offenbart sich ein facettenreiches negatives Emotionsprofil, das neben recht eindeutig bestimmbaren negativen Basisemotionen auch eher nach innen gerichtete und Verbundemotionen wie Scham beinhaltet. Da im Annotationsprozess für jede Emotionsannotation die Values männlich, weiblich oder neutral festgelegt wurden, lässt sich nachvollziehen, welche Emotionen mit männlichen Figuren und welche mit weiblichen Figuren in Verbindung stehen. Innerhalb der untersuchten Textpassagen konnten weiblichen Figuren 2200 Mal eine emotionale Reaktion zugeordnet werden, männliche Figuren lediglich 1474 Mal. Es zeigt sich, dass Angst im untersuchten Korpus die zentrale Emotion für beide Gender darstellt. Charaktere beider Geschlechter bilden ein negativ geprägtes Emotionsprofil aus, das unterschiedliche Angstzustände beschreibt: Besorgnis, Erschrecken und Panik bestimmen das Korpus. Nach diesem übergeordneten Blick auf das Korpus, stellt sich nun die Frage, welche Genderrollen für die jungen Protagonist:innen besonders häufig sind, ob diese spezifisch für einzelne Erzähltexte sind, oder ob sich im gesamten Korpus romanübergreifende Muster bilden. Um herauszufinden, welche Genderrollen für die jungen Protagonist:innen besonders häufig sind, ob diese spezifisch für einzelne Erzähltexte sind, oder ob sich im gesamten Korpus romanübergreifende Muster bilden, haben wir für alle Protagonist:innen in einer relationalen Graphdatenbank mithilfe der Webapplikation Graphcommons (vgl. Arƒ±kan et al., o.J.) Rollenprofile angelegt. Die vom Classifier annotierten Genderrollen wurden durch eine Kollokationsanalyse mit den Protagonist:innen in Verbindung gebracht. Dabei haben wir nach den Gendertags gesucht, in deren Wortumfeld (fünf Wörter davor und danach) der Name der Hauptfigur steht. Anschließend wurde der Annotationskontext daraufhin überprüft, ob mit der annotierten Genderrolle die Hauptfigur bezeichnet wird. Wenn dies der Fall war, wurde die Rolle im Graph mit der Figur verknüpft. Da unser Korpus auch Romanreihen beinhaltet und einige der Hauptfiguren in mehr als einem Roman als Protagonist:in auftreten 'was die Vermutung nahelegt, dass die Protagonist:innen von Reihen aufgrund der größeren Textmenge auch mit mehr Rollen bezeichnet werden könnten 'wurden die Texte ebenfalls als Knoten im Graphen angelegt und die Hauptfiguren mit den jeweiligen Texten verknüpft. Auf diese Weise entstand im ersten Schritt ein komplexer Graph mit drei Ebenen von Knoten: Texte, Hauptfiguren und Rollen (vgl. Abb. 1). In einem weiteren Schritt wurden Emotionen als vierter Knotentyp hinzugefügt. Betrachtet man die Rollenstruktur des Fantasy-Korpus, fallen mehrere Eigenheiten auf. Zunächst einmal beinhalten die Romane mehr Protagonistinnen als Protagonisten. Elf weibliche Hauptfiguren der Romane und Romanzyklen stehen vier männlichen gegenüber. Zwei der vier männlichen Protagonisten fallen durch besonders vielfältige Rollenprofile auf, während die weiblichen Hauptfiguren insgesamt weniger vielfältige Rollenprofile ausbilden. Die einzelnen Rollen sind dabei häufig stereotyp angelegt, männlichen Protagonisten werden hauptsächlich männlich stereotype Genderrollen zugeschrieben und Protagonistinnen hauptsächlich weibliche. In seltenen Fällen weisen die Protagonist:innen zusätzlich zu ihren binär gegenderten Rollen auch genderneutrale Rollen auf wie ""Kind"", ""Findelkind"", ""Mensch"" und ""guter Mensch"". Die meisten Hauptfiguren bilden im Rollennetzwerk Cluster Darüber hinaus sind die Genderrollen der ""Freundin"" und des ""Mädchens"" romanübergreifend bedeutsam; sie sind jeweils mit sieben Protagonistinnen verknüpft. Die Hauptfiguren in dieser Stichprobe deutschsprachiger Jugend-Fantasy-Romane sind also meist Mädchen, für die einerseits das familiäre und andererseits das freundschaftliche Umfeld eine wichtige Bedeutung haben. Zwei der Hauptfiguren fallen dadurch auf, dass sie nur mit einer einzigen Rolle verbunden sind. Es handelt sich dabei um Edda aus der Um zu erproben, wie das Zusammenspiel von Genderrollen und Emotionen gemeinsam betrachtet werden kann, haben wir zunächst Emotionen in einem kompletten Roman manuell annotiert. Dabei handelt es sich um einen Text mit einer Protagonistin 'Ana aus der Trilogie Ana ist die Heldin einer Trilogie (manuell annotiert wurde allerdings nur der erste Teil) und wird stark über Emotionen charakterisiert. Fünf Rollen stehen hier vierzig Emotionen gegenüber. Zwar wurden die Genderrollen automatisch und die Emotionen manuell annotiert, dennoch ist die Differenz hier signifikant. Im gesamten Korpus wurden 68 Emotionen und 83 Genderrollen ausgemacht, die sich auf Protagonist:innen beziehen. Zwar erreicht der Classifier nur eine Erkennnungsgenauigkeit von 72% und annotiert somit höchstwahrscheinlich nicht alle Genderrollen, die mit einer Figur verbunden sind. Auf der anderen Seite handelt es sich aber um eine kontextsensitive automatische Annotation, d.h. die Anzahl der Genderrollen, die erkannt werden kann, ist potentiell unendlich. Die Protagonistin tritt als ""Herrin"", ""Erbin"" und ""Fürstin"" in Erscheinung, gleichzeitig nimmt sie auch die im Korpus insgesamt sehr bedeutsame familiäre Rolle der Tochter ein. Dass Ana eine Protagonistin ist, die vergleichsweise stark über Emotionen charakterisiert wird, zeigt eine Gegenüberstellung mit den anderen Hauptfiguren im Korpus (vgl. Abb. 4). Die in Abb. 4 in Pink dargestellten Emotionsrelationen überwiegen die dunkelblauen Genderrollenzuschreibungen deutlich; stärker als dies bei anderen Protagonist:innen der Fall ist. Neben unterschiedlichen positiven Emotionen (Erheiterung, Dankbarkeit, Vertrauen, Zufriedenheit, Zuneigung, Verlangen, Zuversicht und Gelassenheit) steht eine deutlich höhere Anzahl negativer Emotionen (Abneigung, Trübsal, Leid, Schrecken, Kummer, Nervosität, Wut, Bedauern, Bestürzung, Verzweiflung, Widerwille, Scham, Panik, Ratlosigkeit, Aversion, Entsetzen, Verärgerung und Melancholie), die diese Figur charakterisieren und dem negativen genrespezifischen Emotionsprofil entsprechen. Um zu prüfen, ob es sich hierbei um ein genderspezifisches Muster handelt, haben wir abschließend die anderen Romane im Korpus betrachtet. Um die größere Stichprobe analysieren zu können, haben wir die Romane nicht im Close-Reading-Verfahren annotiert, sondern lediglich Gender-Peaks 'Passagen, in denen der Gender-Classifier besonders viele Annotationen hinzugefügt hat 'und ein Fenster von sechs Sätzen pro Genderzuschreibung betrachtet (drei vor der Erwähnung einer Genderrolle innerhalb eines Peak-Abschnitts und drei danach). Um anschließend diejenigen Emotionsannotationen ausfindig zu machen, die Emotionen markieren, die den Protagonist:innen zugeschrieben wurden, haben wir erneut Kollokationsabfragen durchgeführt, die ausschließlich Emotionsannotationen im Wortumfeld von namentlichen Erwähnungen der Hauptfigur aufzeigen. Der Graph in Abb. 5 zeigt ein interessantes Bild: Nur eine männliche und zwei weibliche Hauptfiguren werden stärker über Rollen als über Emotionen charakterisiert. Bei drei Hauptfiguren sind die Rollen- und Emotionsprofile relativ ausgeglichen. Sechs Protagonistinnen zeigen ein eindeutig stärker über Emotionen als über Rollen definiertes Profil. Es zeigt sich also eine leichte Tendenz zu einer stärkeren Rollenprofilierung der männlichen Hauptfiguren. Die Mehrzahl der Protagonistinnen wird stärker über Emotionen als über Rollen ausgestaltet. An dieser Stelle sind zwei methodenkritische Aspekte zu berücksichtigen: Erstens findet der hier vorgestellte Classifier nicht alle relevanten Entitäten; hier zeigt sich eine grundsätzliche Schwäche von NLP-Ansätzen zur Analyse von (literarischen) Texten. Zweitens führt der vorgestellte Mixed-Methods-Ansatz automatische, d.h. weniger zuverlässige, Annotationen mit manuellen zusammen. Da der Gender-Classifier mit einer Quote von 72% F1-Score nicht alle Vorkommnisse von Genderrollen annotiert, ist es möglich, dass die Figuren eigentlich mehr Genderrollen zugeschrieben bekommen, als hier gezeigt. Die Kontextsensitivität des Tools gewährleistet allerdings, dass sehr viele unterschiedliche Genderrollen automatisch annotiert werden, davon aber nicht immer unbedingt alle Vorkommnisse. Wir gehen darum davon aus, dass die Mehrheit der vorhandenen Genderrollen berücksichtigt werden konnte und der Gender-Classifier eine valide Tendenz der Verteilung aufzeigt. Auch ist bei der hier vorliegenden vergleichenden Analyse vor allem die Balance zu den anderen Texten bedeutsam. Interessant ist auch, welche Emotionen mit den meisten weiblichen Hauptfiguren verknüpft sind. Erstaunen ist mit allen acht Protagonistinnen der Stichprobe verbunden. Besorgnis empfinden sechs der acht weiblichen Hauptfiguren. Bedauern ist bei fünf Charakteren zu finden und Neugier und Zuneigung bei jeweils vier von ihnen. Von diesen fünf am meisten verknüpften Emotionen ist nur eine der Basisemotion der Angst zuzuordnen, nämlich die Besorgnis. Dabei handelt es sich allerdings um eine relativ schwache Form von Angst. Das Klima des Grauens, das für das Genre der Phantastischen Literatur postuliert wurde und das die Gesamtbetrachtung der Emotionen in unserem Korpus bestätigen konnte, geht also nicht wesentlich von den überwiegend weiblichen Hauptfiguren aus. Diese wirken dem eher besorgt, mitfühlend und auch neugierig entgegen. Bis hierhin erweist sich das geschilderte Verfahren als sinnvoll, um das Zusammenspiel von Emotionen und Genderrollen zu analysieren. Es eignet sich, um häufig vorkommende Genderrollen und Emotionsprofile zu ermitteln. Genderstereotype Muster zeichnen sich zwar in dieser Fallstudie schon ab, müssten aber durch die Analyse eines größeren Korpus noch bestätigt oder revidiert werden. Der beispielhafte Vergleich mit einem ebenfalls zunächst im Close-Reading betrachteten männlichen Hauptcharakter steht noch aus. Die Betrachtung des Gesamtkorpus gibt einen vorläufigen Hinweis darauf, dass männliche Hauptfiguren etwas weniger stark durch Emotionen und eher durch stereotype Rollenbilder charakterisiert werden. Um diese Tendenz weiter zu untersuchen, müsste allerdings in einer Anschlussstudie die Stichprobe erweitert werden, um mehr Protagonisten in die Untersuchung einbeziehen zu können.",de,kulturwissenschaftlich ausgerichtet Projekt Beitrag zielen phantastisch Literatur genrekonstitutiv geltend allgemein beschrieben Klima Grauen caillois unheimlich Todorov ausdifferenzieren hinaus betrachten phantastisch Literatur spezifisch herausgestellt Emotionstype Abhängigkeit Genderroll Emotionssender innen zuschreiben rücken ebenfalls betrachtet Genre typisch jugendlich Hauptfigure Fokus Beitrag widmen eng miteinander verknüpfen forschungsfragen nderrollen Protagonist innen Fantasyroman zuschreiben Emotionstype bestimmen korpus sehen emotionsprofile Protaginist innen fallen Genderstereotype statisch Muster Beitrag knüpfen direkt allgemeiner Fallstudie Thema genderrollen zeitgenössisch Flüh Horstmann Schumacher erschein vertiefen gewonnen einsichen Grundlage abgeschlossen Fallstudie Stelle deutschsprachig Kontemporär Zeitraum publizieren jugendliche dar hierbei Rahmen Emotion Abhängigkeit genderkategori männlich weiblich neutral betrachten eigens trainiert schumacher Sutton mccallum basieren Stanford named entity Recognizer Finkel et kompatibel automatisch Annotation genderrolle digital manuell Annotation emotionsinformation kombinieren korpus annotiert Text männlich weiblich neutral Figurenroll markieren Erkennungsgenauigkeit genutzt Version erreichen Kategorie hinweg gattungsspezifisch Testmaterial Schumacher automatisch Vorannotation aufbauend Roman Textanalysetool Catma Gius et taxonomiebasiert Emotionsanalyse durchführen digital Manuelle Annotation funktionieren Grundlage Emotionsanalyse literarisch Text entworfen Tagset hierfür entworfen guidelin flüh Emotionsanalyse beziehen Textstelle zahlreich Genderannotation jeweils semantisch umfeld emotionsinformation untersuchen fokussieren unmittelbar Textumfeld Figurenreferenze bestimmen emotionstragend Textstruktur finden unabhängig genderrolle zeigen Korpus basisemotionen negativ Qualität romanübergreifend erzählwelten bestimmen gesamt Korpus etablieren emotionsprofil abnehmend Häufigkeit zusammensetzen Klima Grauen etablieren deutlich häufig vorkommend genau Blick Vertreter Kategorie zeigen facettenreich emotionsprofil unterschiedlich Angstzuständ Spielart Angst beinhalten Tabelle Annotation Roman zeigen hinaus bewusst Bedeutungsoff gestaltet Annotationskategorie uncategorizabel Annotation Problemföll Annotation quantitativ Gewicht fallen Kategorie problemfäll Emotionstype fallen mehrere polar Gegensätz vereinen eindeutig positiv negativ kategorisieren versammeln oberkategorie uncategorizabel Emotion Basisemotion zuordnen Polarität eindeutig Textpassage lassen Grundlage Strukturorientiert Typologisierung Einteilung Basisemotion orientieren ekman adäquat beschreiben Fall emotionstypen definieren Gewicht fallen häufig Vertreter Kategorie Nuancier Emotionsprofil Scham Aggression Bedauern weisen negativ Qualität Interesse erstaun eher Kategorie positiv emotionstypen fallen auffällig vertreten Emotionstype Diskurs Sprache Emotion strittig Emotionstype verhandeln Scham eher intrasubjektiv empfunden Emotion häufig ähnlich Emotionskategori schuld reu Bedauern Verbindung bringen unklar hierbei Scham Kategorie darstellen deutlich explosiv Fremdschädigung auftretend aggressionen lassen treiben Emotion beschreiben fraglich erstaun Interesse Emotion klassifizieren offenbaren facettenreich negativ emotionsprofil eindeutig bestimmbaren negativ Basisemotione eher innen gerichtet verbundemotion Scham beinhalten Annotationsprozess Emotionsannotation Values männlich weiblich neutral festlegen lässen nachvollziehen Emotion männlich Figur weiblich Figur Verbindung stehen innerhalb untersucht Textpassage weiblich Figur mal emotional Reaktion zuordnen männlich Figur lediglich mal zeigen Angst untersucht Korpus zentral Emotion gend darstellen charaktere beide Geschlechter bilden negativ geprägt emotionsprofil unterschiedlich Angstzustände beschreiben Besorgnis erschrecken Panik bestimmen korpus übergeordnet Blick korpus stellen Frage genderrolle jung Protagonist innen häufig spezifisch einzeln Erzähltext gesamt Korpus romanübergreifend Muster bilden herausfinden genderrolle jung Protagonist innen häufig spezifisch einzeln Erzähltext gesamt Korpus romanübergreifend Muster bilden protagonisen innen relational Graphdatenbank Mithilfe Webapplikation Graphcommons et rollenprofile anlegen Classifier annotierten genderrollen Kollokationsanalyse Protagonist innen Verbindung bringen Gendertag suchen Wortumfeld Wörter Name Hauptfigur stehen anschließend Annotationskontext daraufhin überprüfen annotierter Genderrolle Hauptfigur bezeichnen Fall Rolle Graph Figur verknüpfen Korpus Romanreihen beinhalten hauptfiguren Roman Protagonist auftreten Vermutung nahelegen Protagonist innen Reihe aufgrund groß Textmenge Rolle bezeichnen können Text ebenfalls knoten Graphen anlegen hauptfiguren jeweilig Text verknüpfen Weise entstehen Schritt komplex Graph Ebene knoten Text hauptfigur Rolle abb Schritt Emotion Knotentyp hinzufügen betrachten Rollenstruktur fallen mehrere eigenheiter beinhalen Roman Protagonistinn Protagonist weiblich hauptfiguren Roman Romanzykl stehen Männliche männlich Protagonist fallen vielfältig Rollenprofile weiblich hauptfiguren insgesamt vielfältig Rollenprofile ausbilden einzeln Rolle häufig stereotyp anlegen männlich Protagonist hauptsächlich männlich Stereotype genderrollen zuschreiben Protagonistinn hauptsächlich weiblich selten Fall weisen Protagonist innen zusätzlich Binär gegendert Rolle genderneutral Rolle Kind findelkind Mensch Mensch meister hauptfiguren bilden Rollennetzwerk Cluster hinaus genderrollen freundin mädchen romanübergreifend bedeutsam jeweils Protagonistinn verknüpfen hauptfiguren Stichprobe deutschsprachig meist Mädchen einerseits Familiäre andererseits freundschaftlich umfeld wichtig Bedeutung hauptfiguren fallen einzig Rolle verbinden handeln Edda erproben Zusammenspiel genderrolle Emotion gemeinsam betrachten Emotion komplett Roman manuell annotieren handeln Text Protagonistin ana Trilogie ana Heldin Trilogie manuell annotiert stark Emotion charakterisieren Rolle stehen vierzig Emotion genderrollen automatisch Emotion manuell annotieren dennoch Differenz signifikant gesamt Korpus Emotion genderrollen ausmachen Protagonist innen beziehen erreichen Classifier Erkennnungsgenauigkeit annotieren somit höchstwahrscheinlich nderrollen Figur verbinden Seite handeln kontextsensitiv automatisch Annotation Anzahl genderrollen erkennen potentiell unendlich Protagonistin treten Herrin Erbin Fürstin Erscheinung gleichzeitig nehmen Korpus insgesamt bedeutsam Familiäre Rolle Tochter ana Protagonistin vergleichsweise stark Emotion charakterisieren zeigen Gegenüberstellung hauptfiguren Korpus abb abb pink dargestellt Emotionsrelation überwiegen dunkelblau genderrollenzuschreibungen deutlich stark Protagonist innen Fall unterschiedlich positiv Emotion Erheiterung Dankbarkeit vertrauen Zufriedenheit Zuneigung verlangen Zuversicht Gelassenheit stehen deutlich hoch Anzahl negativ Emotion Abneigung Trübsal Leid Schrecken Kummer Nervosität Wut bedauern Bestürzung Verzweiflung Widerwille Scham Panik Ratlosigkeit Aversion Entsetzen Verärgerung Melancholie Figur Charakterisieren negativ Genrespezifischen emotionsprofil entsprechen prüfen hierbei genderspezifisch Muster handeln abschließend Roman Korpus betrachten groß Stichprobe analysieren Roman annotiert lediglich passagen Annotation hinzufügen Fenster satz pro Genderzuschreibung betrachten Erwähnung genderroll innerhalb anschließend emotionsannotation ausfindig Emotion markieren Protagonist innen zuschreiben erneut Kollokationsabfrag durchführen ausschließlich Emotionsannotation Wortumfeld namentlich Erwähnung Hauptfigur aufzeigen Graph abb zeigen interessant Bild männlich weiblich hauptfiguren stark Rolle Emotion charakterisieren hauptfiguren emotionsprofil relativ ausgleichen Protagonistinn zeigen eindeutig stark Emotion roll definiert Profil zeigen leicht Tendenz stark Rollenprofilierung männlich Hauptfigure Mehrzahl Protagonistinn stark Emotion Rolle ausgestalten Stelle methodenkritisch Aspekt berücksichtigen erstens finden vorgestellt Classifier relevanten entitäen zeigen grundsätzlich Schwäche Analyse literarisch texten zweitens führen vorgestellt automatischen zuverlässig Annotation Manuelle Quote vorkommnisse genderrolle annotieren Figur eigentlich genderrollen zugeschrieben bekommen zeigen Kontextsensitivität Tools gewährleisten unterschiedlich Genderrolle automatisch annotiert unbedingt vorkommnissen Mehrheit vorhanden genderrollen berücksichtigen valid Tendenz Verteilung aufzeigen vorliegend vergleichend Analyse Balance Text bedeutsam interessant Emotion meister weiblich hauptfiguren verknüpfen erstaunen Protagonistinn Stichprobe verbinden Besorgnis empfinden weiblich hauptfiguren bedauern Charaktere finden Neugier Zuneigung jeweils meister verknüpften Emotion Basisemotion Angst zuzuordnen nämlich Besorgnis handeln relativ schwach Form Angst Klima Grauen Genre phantastisch Literatur postulieren Gesamtbetrachtung Emotion unser Korpus bestätigen wesentlich überwiegend weiblich Hauptfigure wirken eher besorgt mitfühlend neugierig entgegen hierhin erweisen geschildert Verfahren sinnvoll Zusammenspiel Emotion Genderrolle analysieren eignen häufig vorkommend genderrollen emotionsprofil ermitteln Genderstereotype Muster zeichnen Fallstudie Müsst Analyse groß Korpus bestätigen revidieren beispielhaft Vergleich ebenfalls betrachten männlich Hauptcharakter stehen Betrachtung Gesamtkorpus vorläufig Hinweis männlich hauptfiguren stark Emotion eher Stereotype Rollenbilder charakterisieren Tendenz untersuchen müsster Anschlussstudie Stichprobe erweitern Protagonist Untersuchung einbeziehen,"[('emotion', 0.3988633165057993), ('hauptfiguren', 0.2525054837132408), ('protagonist', 0.24307417087856392), ('genderrollen', 0.2222018014783388), ('genderrolle', 0.21470520504316667), ('männlich', 0.17989036320791463), ('emotionsprofil', 0.17776144118267104), ('weiblich', 0.1772725851136886), ('innen', 0.15708724146802305), ('protagonistinn', 0.1431368033621111)]"
2022,DHd2022,VARACHKINA_Hanna_Reflexive_Passagen_und_ihre_Attribution.xml,Reflexive Passagen und ihre Attribution,"Hanna Varachkina (Georg-August-Universität Göttingen, Deutschland); Florian Barth (Georg-August-Universität Göttingen, Deutschland); Luisa Gödeke (Georg-August-Universität Göttingen, Deutschland); Anna Mareike Hofmann (Georg-August-Universität Göttingen, Deutschland); Tillmann Dönicke (Georg-August-Universität Göttingen, Deutschland)","reflexive Passagen, Attribution, Korpus, automatische Erkennung","Modellierung, Annotieren, Literatur","Im Projekt MONA (Modes of Narration and Attribution) werden Phänomene in fiktionaler Literatur untersucht, die mit reflexiven Passagen assoziiert sind. Reflexive Passagen kommentieren die Handlung im Text oder den Schreibprozess oder generalisieren über die fiktive und‚Äâ/‚Äâoder reale Welt. Da das Konzept der reflexiven Passagen in der Literaturwissenschaft bisher nicht formalisiert wurde, werden diese nicht direkt annotiert. Stattdessen annotieren wir drei Phänomene, die wir für starke Indikatoren reflexiver Passagen halten: Kommentar (Bonheim 1975; Chatman 1980), nicht-fiktionale Rede (Konrad 2017; Searle 1975) und Generalisierung (Leslie et al. 2016; Dönicke et al. 2021). Darüber hinaus beschäftigt sich das Projekt mit der Zuschreibung reflexiver Passagen zu Sprechinstanzen. Für die Identifikation und Klassifikation dieser Phänomene werden Modelle entwickelt. Dafür wird ein annotiertes Korpus deutschsprachiger fiktionaler Texte erstellt, das die Entwicklung dieser Phänomene über 350 Jahre der Literaturgeschichte abbildet. Basierend auf den bisherigen Arbeiten haben wir Definitionen für die mit reflexiven Passagen assoziierten Phänomene formuliert bzw. weiterentwickelt. Unter Generalisierungen werden quantifizierte Aussagen über angenommene Instanzen einer Klasse oder Gruppe von Objekten, Individuen oder (Zeit-)Räumen verstanden, auf die nicht kontextuell referiert wird. Kommentare schließen Textstellen ein, in denen die erzählte Zeit unterbrochen und eine ergänzende Information zu Erzählung, Figuren, Handlung oder dem Akt des Erzählens eingefügt wird (Bonheim 1975). Nicht-fiktionale Rede bezeichnet Passagen in fiktionalen Texten, die Behauptungen bzw. Hypothesen über die reale Welt nahelegen (Konrad 2017). Generalisierung, Kommentar und nicht-fiktionale Rede können sich vollständig oder teilweise überlappen. In diesem Beispiel treten alle drei Phänomene auf. Die erzählte Zeit, die in der Erzählerrede fließt, wird unterbrochen und ein Kommentar über Naturwesen vorgenommen. Zugleich wird eine Aussage über angenommene Instanzen der Klasse der Naturwesen getroffen. Da auch in der realen Welt Naturwesen (jeglicher Art) vorkommen, ist die Proposition grundsätzlich auf die reale Welt übertragbar. Für reflexive Passagen erstellen wir eine Goldannotation, auf der in einem nächsten Schritt eine Attributionsannotation vorgenommen wird. Die Attribution bestimmt, wem die in der Passage enthaltene Information zugeschrieben werden kann, wofür grundsätzlich Figuren, die Erzählinstanz oder die AutorIn in Frage kommen. Einige sprachliche Mittel im Text sind prädestiniert für bestimmte Attributionen, so markieren bestimmte Satzzeichen i. d. R. (in)direkte Rede und damit die Sprecher im Text. Dennoch gibt es Passagen, in denen sich die Sprechinstanz nicht eindeutig identifizieren lässt und sich unterschiedliche Interpretationen (Zuschreibungen) aufdecken lassen. Zur automatischen Erkennung und quantitativen Analyse erstellen wir das Korpus MONACO (Modes of Narration and Attribution Corpus) Die Annotation der Texte wird in CATMA 6.2 Bisher wurden Goldstandards für achtzehn Texte erstellt. Der älteste Text stammt aus dem Jahr 1616, der jüngste aus dem Jahr 1930. Die annotierten Texte weisen im Durchschnitt ein moderates (> 0,4) oder gutes (> 0,6) Inter-Annotator Agreement mit Œ∫-Werten (Fleiss et al. 1981) von 0,59 für Generalisierung, 0,44 für Kommentar und 0,66 für nicht-fiktionale Rede auf. Die Inter-Annotator Agreement-Werte für Œ≥ (Mathet et al. 2015) sind etwas höher: 0,66 für Generalisierung, 0,52 für Kommentar und 0,72 für nicht-fiktionale Rede. Mit der zunehmenden Menge annotierter Texte werden schrittweise regelbasierte, statistische und neuronale Tagger für die einzelnen Phänomene entwickelt. Ihre Anwendbarkeit wird dabei auch für andere Textsorten wie Essays und enzyklopädische Texte erprobt. Letzten Endes soll eine ausreichend große Menge annotierter Daten nicht nur bessere Modelle ermöglichen, sondern auch diachrone oder genreübergreifende Perspektiven auf reflexive Passagen und ihre Attribution eröffnen.",de,Projekt Mona modes -- narration -- Attribution phänomen Fiktionaler Literatur untersuchen reflexiv Passag assoziieren reflexiv Passage kommentieren Handlung Text schreibprozess generalisieren fiktiv real Welt Konzept reflexiv Passag Literaturwissenschaft formalisieren direkt annotieren Stattdesse annotieren Phänomen stark indikator reflexiv passagen halten Kommentar bonheim Chatman Rede Konrad Searle Generalisierung Leslie Et dönick et hinaus beschäftigen Projekt Zuschreibung reflexiv passagen sprechinstanzen Identifikation Klassifikation Phänomen Modell entwickeln annotiert Korpus deutschsprachig Fiktionaler Text erstellen Entwicklung Phänomen Literaturgeschichte abbildet basierend bisherig arbeiten Definition reflexiv Passag assoziiert Phänomen formulieren weiterentwickeln generalisierung quantifiziert Aussage angenommen Instanz Klasse Gruppe Objekt individuen verstehen Kontextuell referieren Kommentar schließen Textstelle erzählt unterbrechen ergänzend Information Erzählung Figur Handlung Akt erzählens einfügen bonheim Rede bezeichnen Passag fiktional Text Behauptunge Hypothese real Welt nahelegen Konrad Generalisierung Kommentar Rede vollständig teilweise überlappen treten Phänomen erzählt Erzählerrede fließen unterbrechen Kommentar Naturwesen vornehmen Aussage angenommen Instanz Klasse Naturwesen treffen real Welt naturwesen Jeglicher Art vorkommen Proposition grundsätzlich real Welt übertragbar reflexiv passag erstellen Goldannotation nächster Schritt Attributionsannotation vornehmen Attribution bestimmen Passage enthalten Information zuschreiben wofür grundsätzlich figuren Erzählinstanz Autorin Frage sprachlich Text prädestinieren bestimmt attributionen markieren bestimmt satzzeich direkt Rede Sprecher Text dennoch passagen Sprechinstanz eindeutig identifizier lässt unterschiedlich Interpretation zuschreibung aufdecken lassen automatisch Erkennung quantitativ Analyse erstellen korpus Monaco modes -- narration -- Attribution Corpus Annotation Text Catma goldstandards achtzehn Text erstellen alt Text stammen jung annotiert Text weisen Durchschnitt moderat agreemenen fleiss et Generalisierung Kommentar Rede Mathet et hoch Generalisierung Kommentar Rede zunehmend Menge annotiert Text schrittweise regelbasieren statistisch neuronal Tagger einzeln Phänomen entwickeln Anwendbarkeit Textsort Essay enzyklopädisch Text erproben letzter ende ausreichend Menge annotiert daten gut Modell ermöglichen Diachrone genreübergreifend Perspektive reflexiv Passage Attribution eröffnen,"[('reflexiv', 0.4653379298376507), ('generalisierung', 0.27089187723931907), ('kommentar', 0.24324211956223996), ('rede', 0.20746403186499324), ('passag', 0.1951192843605761), ('phänomen', 0.17894167409996575), ('attribution', 0.1781168761688634), ('naturwesen', 0.174501723689119), ('real', 0.15084083179388244), ('passagen', 0.1474589966566077)]"
2022,DHd2022,HINZMANN_Maria_Linked_Open_Data_für_die_Literaturgeschichtss.xml,"Linked Open Data für die Literaturgeschichtsschreibung Das Projekt ""Mining and Modeling Text""","Maria Hinzmann (Universität Trier, Germany); Christof Schöch (Universität Trier, Germany); Katharina Dietz (Universität Trier, Germany); Anne Klee (Universität Trier, Germany); Katharina Erler-Fridgen (Universität Trier, Germany); Julia Röttgermann (Universität Trier, Germany); Moritz Steffes (Universität Trier, Germany)","Linked Open Data, Informationsextraktion, Datenmodellierung, Literaturgeschichte","Datenerkennung, Modellierung, Identifizierung, Literatur, Metadaten","Im Umgang mit dem stetig wachsenden ""digitalen Kulturerbe"" bietet die Weiterentwicklung der systematischen Datenerschließung und Wissensrepräsentation bisher nicht ausgeschöpfte Potentiale für die Literaturgeschichtsschreibung. Vor diesem Hintergrund werden im Projekt Bezogen auf den Gegenstandsbereich ist ein Ausgangspunkt, dass die über rund zwei Jahrhunderte akkumulierten literaturhistorischen Forschungserkenntnisse größtenteils nicht unmittelbar nutzbar sind, da diese sehr umfangreich sind, nicht digital vorliegen oder auf unterschiedliche Orte und Quellen verteilt und in unterschiedlichen Sprachen publiziert sind. Für die Bereitstellung der Daten sowie die Arbeit und Infrastruktur im Projekt sind Open Science-Prinzipien tragend. Dies betrifft u.a. die Veröffentlichung FAIRer Daten (Röttgermann/Schöch 2020) im Open Access (Schöch 2021), die Nutzung von Open Source-Tools wie Für unsere Domäne ist die 1977 von Mylne, Martin und Frautschi veröffentlichte Im Teilprojekt zur Primärliteratur wird schrittweise ein Korpus von etwa 200 Romanen aufgebaut, wovon bereits reichlich 100 Texte in XML-TEI verfügbar sind (vgl. Röttgermann 2021; Klee/Röttgermann 2020). Das mittelfristige Ziel besteht darin, durch überwachtes Lernen die automatische Extraktion von Aussagen (RDF-Tripel) zu ermöglichen. Um Trainingsdaten zu generieren und Tripel in das Wissensnetzwerk einspeisen zu können, werden aktuell literaturgeschichtliche Texte in INCEpTION annotiert. Die Daten sollen als Statements über eine noch zu entwickelnde toolübergreifende Pipeline in unsere projektspezifische Wikibase importiert werden. Im Aufbau des LOD-Wissensnetzwerks werden literaturgeschichtliche Aussagetypen in einer systematischen Ontologie modelliert und die extrahierten Informationen als RDF-Tripel repräsentiert (vgl. Abb. 3). Der Mehrwert des Netzwerks wird durch exemplarische Nutzungsszenarien in Form von SPARQL-Abfragen konkretisiert. Frageoptionen wie ""Tritt Thema x in einem bestimmten Zeitraum y gehäuft auf?"" verdeutlichen, welcher Nutzen daraus für eine datenbasierte Literaturgeschichtsschreibung entstehen kann. Exemplarisch werden Einblicke in die Infrastruktur gegeben (vgl. Abb. 4). Ein Standardisierungsprozess wie er sich beispielsweise im Das Poster veranschaulicht die Integration der verschiedenen Informationsquellen in der Datenmodellierung sowie das Zusammenspiel der verschiedenen Teilprojekte und Tools im Aufbau und in möglichen Nutzungsszenarien des mehrsprachigen Wissensnetzwerks.",de,Umgang stetig wachsend digital Kulturerbe bieten Weiterentwicklung systematisch Datenerschließung Wissensrepräsentation ausgeschöpft Potential Literaturgeschichtsschreibung Hintergrund Projekt beziehen gegenstandsbereich Ausgangspunkt jahrhundert akkumuliert literaturhistorisch Forschungserkenntniss größtenteils unmittelbar nutzbar umfangreich digital vorliegen unterschiedlich Ort quellen verteilen unterschiedlich Sprache publizieren Bereitstellung daten Arbeit Infrastruktur Projekt Open tragend betreffen Veröffentlichung fair daten Röttgermann schöch op Access schöch Nutzung op Domäne Mylne Martin Frautschi veröffentlichen Teilprojekt Primärliteratur schrittweise korpus Roman aufbauen wovon reichlich Text verfügbar Röttgermann Klee Röttgermann mittelfristig Ziel bestehen überwacht lernen automatisch Extraktion Aussage ermöglichen trainingsdaten generieren Tripel Wissensnetzwerk einspeisen aktuell literaturgeschichtlich Text inception annotiert daten statement entwickelnd toolübergreifend Pipeline projektspezifisch Wikibase importieren Aufbau literaturgeschichtlich Aussagetype systematisch Ontologie modellieren extrahiert Information repräsentieren abb Mehrwert Netzwerks exemplarisch Nutzungsszenarie Form konkretisieren frageoptionen treten Thema x bestimmt Zeitraum Y häufen verdeutlichen nutzen datenbasiert Literaturgeschichtsschreibung entstehen exemplarisch einblicken infrastruktur geben abb standardisierungsprozess beispielsweise Poster veranschaulichen Integration verschieden informationsquelle Datenmodellierung Zusammenspiel verschieden teilprojekte Tools Aufbau möglich Nutzungsszenarie Mehrsprachigen Wissensnetzwerk,"[('röttgermann', 0.28926548475597824), ('wissensnetzwerk', 0.21845316689172772), ('nutzungsszenarie', 0.1928436565039855), ('literaturgeschichtsschreibung', 0.17786305326532736), ('op', 0.15546832730629415), ('literaturgeschichtlich', 0.14162463572850104), ('infrastruktur', 0.139384755186959), ('schöch', 0.1266440324898429), ('aufbau', 0.12229233640026889), ('ausgeschöpft', 0.10922658344586386)]"
2022,DHd2022,BÖRNER_Ingo_Einführung_in_DraCor___Programmable_Corpora_für_.xml,Einführung in DraCor Programmable Corpora für die digitale Dramenanalyse,"Ingo Börner (Universität Potsdam); Frank Fischer (National Research University Higher School of Economics Moscow, DARIAH-EU); Carsten Milling (Universität Potsdam); Henny Sluyter-Gäthje (Universität Potsdam)","Digitale Literaturwissenschaft, Dramenanalyse, API, Linked Data","Community-Bildung, Netzwerkanalyse, Stilistische Analyse, Einführung, Infrastruktur, Literatur","In dem ganztägigen Workshop wird DraCor ( Der Workshop richtet sich an Personen, die Es erfolgt eine Vorstellung des Konzepts der ""Programmable Corpora"" sowie einer Demonstration der exemplarischen Umsetzung in der Plattform DraCor inklusive einer Vorstellung aller Komponenten. In Form von Hands-on-Tutorials wird den Teilnehmer*innen eine praktische Einführung in das Erstellen und Kuratieren eigener Dramenkorpora zur Analyse mit DraCor geben. Ein weiterer Teil führt anhand praktischer Beispiele zu den Methoden Stilometrie und Netzwerkanalyse in die Verwendung der DraCor-API sowie der Python-Bibliothek PyDraCor ein. Die API-Schnittstelle (Application Programming Interface) ermöglicht den maßgeschneiderten direkten Zugriff auf bestimmte Teile der Korpora. Die Möglichkeiten zu korpusübergreifenden Abfragen und Einbeziehung von Informationen aus der Linked-Open-Data-Cloud mit SPARQL werden ebenso erprobt. Den Kern von DraCor bilden Korpora von Dramen in elf Sprachen (Deutsch, Russisch, Französisch, Italienisch, Schwedisch, Spanisch, Altgriechisch, Elsässisch, Lateinisch, Baschkirisch und Tatarisch) sowie zwei weitere Autoren-Korpora (Shakespeare, Calderón), zu denen die Plattform eine Vielzahl an möglichen Forschungszugängen bietet: Die Dramen sind als XML-Dateien entsprechend der TEI-Guidelines kodiert und unter einer offenen Lizenz frei über GitHub unter Neben diesem ""klassischen"" modus operandi der korpusbasierten Forschung bietet DraCor als offenes digitales Ökosystem jedoch noch weitere Schnittstellen und angeschlossene Tools (Netzwerkvisualisierungen, Shiny App, Easy Linavis). Grundlegend hierfür ist die DraCor REST API ( Für die Programmiersprachen Python (PyDraCor: Korpusbasierte, in der Regel quantitative Methoden verwendende Analysen von Dramen haben sich in den vergangenen Jahren zu einem eigenen Subfeld der Computational Literary Studies (CLS) entwickelt (vgl. Willand et al. 2017; Reiter 2021). Dabei hat sich die Bereitstellung gemeinsam kuratierter und offener Ressourcen wie DraCor als produktiv auch für angrenzende Disziplinen wie die Computerlinguistik erwiesen (vgl. beispielsweise Pagel, Reiter 2020). Auf Wortebene operierende Verfahren haben sich dabei etwa auf die Autorschaftsattribution (Schöch 2014) oder Genreklassifikation mit Topic Modeling (Schöch 2017) fokussiert. Aktuell werden vielversprechende Neukonzeptualisierungen stilometrischer Maße wie das Kontrastmaß Zeta entwickelt und angewendet (Schöch 2018). Auf der Grundlage von strukturell ausgezeichneten Korpora lassen sich darüber hinaus gezielte Analysen etwa von Bühnenanweisungen durchführen, die mit POS-Informationen oder semantischen Feldern operieren (Trilcke et al. 2020). Im Bereich der strukturellen Analyse wurden Dramenkorpora früh schon, beginnend mit den Arbeiten von Stiller, Nettle, Dunbar (2003) und fortgesetzt etwa bei Moretti (2011), mit netzwerkanalytischen Ansätzen untersucht. Typologische Arbeiten beispielsweise zum Konzept der Small Worlds (Trilcke et al. 2016) stehen hier u.a. neben Ansätzen zur quantitativen Klassifizierung von Figurentypen (Fischer et al. 2018). Wenngleich semantische Technologien mittlerweile zum festen Bestandteil des Methodenspektrums der Digitalen Geisteswissenschaften zählen, gelangen sie in den korpusbasierten CLS bisher selten Anwendung (zu Prosa bspw. Frank und Ivanovic 2018; Dittrich 2017). Die Erfassung von Metadaten als Linked Data und die Anbindung an externe Referenzressourcen, insbesondere Wikidata, ermöglichen jedoch weitreichende Abfragemöglichkeiten und lassen sich zur Analyse von literarischen Korpora gewinnbringend nutzen. Beispielsweise sind in den DraCor-Korpusdaten keine detaillierten Informationen zu Autor*innen und Aufführungsorten enthalten. Da aber zu den einzelnen Stücken die eindeutigen Wikidata-Identifikatoren hinterlegt sind, können diese Informationen per federated queries in SPARQL abgerufen und in unterschiedlichen Visualisierungsformen, wie zum Beispiel als Karte, dargestellt werden. Im ersten Teil des Workshops wird zunächst das Konzept der ""Programmable Corpora"" eingeführt und diskutiert. Daran anschließend werden die Plattform DraCor und die einzelnen Komponenten vorgestellt, wobei auch immer wieder kürzere Übungsphasen vorgesehen sind, in denen die Teilnehmer*innen die vorgestellten Komponenten und Tools unmittelbar ausprobieren können. Insbesondere werden die unterschiedlichen Möglichkeiten zum Bezug und zur Analyse der Korpusdaten erprobt. Ein Fokus liegt dabei auf der Verwendung der API. Anhand der interaktiven Dokumentation werden die API-Funktionalitäten erläutert und können von den Teilnehmer*innen ausgiebig getestet werden. Im Anschluss daran wird ein kurzer Überblick zur Korpuserstellung und zu den Besonderheiten der TEI-Kodierung geben, wie sie in DraCor zum Einsatz kommen. Den zweiten Teil des Workshops bilden Gruppenarbeitsphasen, in denen drei Themenbereiche vertieft werden können: (1) Korpuserstellung und -kuratierung mit DraCor: Die Teilnehmenden vertiefen die TEI-Kodierung von Dramen anhand von praktischen Übungen und lernen, wie eine lokale Instanz der Plattform mittels Docker aufgesetzt, gegebenenfalls angepasst und mit eigenen Korpora bestückt werden kann. (2) Dramenanalyse mit DraCor-API und Python: Mittels Jupyter Notebooks mit ausführlich dokumentiertem Python-Programmcode werden die Teilnehmer*innen an Methoden der digitalen Dramenanalyse unter Verwendung der DraCor-API herangeführt. Die Notebooks sollen es auch Teilnehmer*innen, die bisher noch keine Erfahrungen im Programmieren mit Python gemacht haben, im Sinne eines Literate-Programming-Ansatzes ermöglichen, die einzelnen Analyseschritte nachzuvollziehen und auch selbst adaptieren zu können. Die Notebooks setzen konkrete Forschungsfragen zur Dramenanalyse um, etwa zur literaturhistorischen Entwicklung netzwerkanalytischer Maße oder zur quantitativen Dominanz von Figuren. (3) Dramenanalyse mit Linked Data: Den Schwerpunkt bilden praktische Analysen, die aus der Anbindung von DraCor an die Linked Open Data Cloud möglich werden. Im Workshop wird ein kurzer Crashkurs in die Abfragesprache SPARQL gegeben, um dann im Anschluss gemeinsame Abfragen von DraCor und Wikidata vorzunehmen und die Ergebnisse zu visualisieren. Die Ergebnisse der Arbeitsgruppen werden anschließend im Plenum präsentiert und diskutiert. Anzahl der möglichen Teilnehmer*innen: 25 Teilnehmer*innen benötigen einen eigenen Laptop mit Internetzugang; Hinweise zu vorab zu installierender Software (Oxygen XML-Editor, Docker, ...) werden im Vorfeld bekanntgegeben. Die Materialien werden auf GitHub bereitgestellt; die Jupyter Notebooks werden unter ( Weitere benötigte technische Ausstattung am Veranstaltungsort: Beamer, WLAN   DraCor wird gegenwärtig im Rahmen des von der EU Horizon 2020 geförderten Projekts ""CLSInfra"" (Fördernummer: 101004984,",de,ganztägig Workshop Dracor Workshop richten Person erfolgen Vorstellung Konzept programmabel Corpora Demonstration exemplarisch Umsetzung Plattform Dracor inklusive Vorstellung Komponente Form praktisch Einführung erstell Kuratieren Dramenkorpora Analyse dracor geben weit führen anhand praktisch Beispiel method Stilometrie Netzwerkanalyse Verwendung Pydracor Application Programming interface ermöglichen maßgeschneiderten direkt Zugriff bestimmt Teil Korpora Möglichkeit Korpusübergreifend Abfrage Einbeziehung Information sparql erproben Kern dracor bilden Korpora Dram Sprache deutsch russisch französisch Italienisch schwedisch Spanisch altgriechisch elsässisch lateinisch baschkirisch tatarisch shakespear calderón Plattform Vielzahl möglich Forschungszugäng bieten Dram entsprechend kodieren offen Lizenz frei Github klassisch Modus operandin korpusbasiert Forschung bieten Dracor offen digital Ökosystem schnittstelle angeschlossen Tools Netzwerkvisualisierung Shiny app easy linavis grundlegend hierfür Dracor Rest Api programmiersprach Python Pydracor Korpusbasiert Regel quantitativ Methode Verwendend Analyse Dram Subfeld Computational literary Studies cls entwickeln Willand et Reiter Bereitstellung gemeinsam kuratiert offen Ressource dracor produktiv angrenzend disziplinen Computerlinguistik erweisen beispielsweise Pagel Reiter worteben operierend Verfahren Autorschaftsattribution schöch Genreklassifikation Topic Modeling schöch fokussieren aktuell vielversprechend neukonzeptualisierung Stilometrischer Maß Kontrastmaß zeta entwickeln anwenden schöch Grundlage strukturell ausgezeichnet korpora lassen hinaus gezielt analysen Bühnenanweisung Durchführ semantisch feldern operieren trilcke et Bereich strukturell Analyse dramenkorpora früh beginnend Arbeit Stiller Nettle dunbar fortsetzen moretti netzwerkanalytisch Ansatz untersuchen typologisch arbeiten beispielsweise Konzept Small Worlds trilcke et stehen Ansatz quantitativ Klassifizierung Figurentype Fischer et wenngleich semantisch Technologie mittlerweile fest Bestandteil Methodenspektrum digital geisteswissenschaften zählen gelangen korpusbasierten Cls selten Anwendung Prosa Frank Ivanovic dittrich Erfassung metadaten Linked data Anbindung extern Referenzressource insbesondere Wikidata ermöglichen weitreichend abfragemöglichkeiten lassen Analyse literarisch Korpora gewinnbringend nutzen beispielsweise detailliert Information aufführungsorten enthalten einzeln stücken Eindeutige hinterlegen Information per Federated queries Sparql abgerufen unterschiedlich Visualisierungsform Karte darstellen Workshop Konzept programmable Corpora einführen diskutieren anschließend Plattform Dracor einzeln Komponent vorstellen wobei kurz übungsphasen vorsehen vorgestellt Komponent Tools unmittelbar ausprobieren insbesondere unterschiedlich Möglichkeit Bezug Analyse Korpusdat erproben Fokus liegen Verwendung Api anhand interaktiv dokumentation erläutern ausgiebig testen Anschluss kurz Überblick Korpuserstellung Besonderheit geben Dracor Einsatz Workshop bilden gruppenarbeitsphasen Themenbereich Vertieft Korpuserstellung dracor Teilnehmend vertiefen Dram anhand praktisch übungen lernen lokal Instanz Plattform mittels Docker aufsetzen gegebenenfalls angepasst Korpora bestücken Dramenanalyse python mittels Jupyter Notebooks ausführlich dokumentiert Methode digital Dramenanalyse Verwendung heranführen notebooks Erfahrung Programmieren Python Sinn ermöglichen einzeln Analyseschritte nachvollziehen adaptieren Notebooks setzen konkret Forschungsfrag Dramenanalyse literaturhistorisch Entwicklung netzwerkanalytisch Maß quantitativ Dominanz Figur Dramenanalyse Linked data Schwerpunkt bilden praktisch analysen Anbindung Dracor Linked open Data Cloud Workshop kurz Crashkurs Abfragesprach sparql geben Anschluss gemeinsam Abfrag Dracor Wikidata vornehmen Ergebnis visualisieren Ergebnis Arbeitsgruppe anschließend Plenum präsentieren diskutieren Anzahl möglich benötigen Laptop Internetzugang Hinweis vorab installierend Software oxyg Docker Vorfeld bekanntgeben material Github bereitstellen Jupyter Notebooks benötigt technisch Ausstattung Veranstaltungsort Beamer Wlan Dracor gegenwärtig Rahmen EU Horizon gefördert Projekt clsinfra Fördernummer,"[('dracor', 0.5261995474062902), ('notebooks', 0.16190755304808926), ('dramenanalyse', 0.14620336790838134), ('workshop', 0.1312612166058657), ('plattform', 0.1290923138212895), ('korpora', 0.11365140272335707), ('sparql', 0.11213618407187483), ('linked', 0.11213618407187483), ('dram', 0.1091013349601095), ('praktisch', 0.10803217626577753)]"
2022,DHd2022,ARNOLD_Frederik_Lesen__was_wirklich_wichtig_ist__Die_Identif.xml,"Lesen, was wirklich wichtig ist Die Identifikation von Schlüsselstellen durch ein neues Instrument zur Zitatanalyse","Frederik Arnold (Humboldt-Universität zu Berlin, Germany); Benjamin Fiechter (Humboldt-Universität zu Berlin, Germany)","Schlüsselstelle, Praxeologie, Literaturanalyse, Zitaterkennung, Text Reuse","Entdeckung, Strukturanalyse, Modellierung, Visualisierung, Werkzeuge, Visualisierung","Literaturinterpretationen heben in der Regel einige wenige Bestandteile des analysierten Primärtextes hervor, die für die jeweilige These und die damit verbundene Interpretation besonders wichtig erscheinen, und interpretieren sie mehr oder weniger ausführlich. Diese Passagen fassen wir als Mithilfe des unten beschriebenen Konzepts, seiner Umsetzung als Algorithmus ( Im Folgenden gehen wir zunächst kurz auf verwandte Arbeiten ein, erläutern dann den Kontext von Konzept und Instrument, ehe wir den Algorithmus und die Webseite zur Exploration vorstellen; dabei wird der Fokus weniger auf technische Details des Algorithmus als auf dessen praktische Anwendung gelegt. Daran schließen sich Überlegungen zur Konzeption und eine Verortung in den Digital Humanities sowie ein Ausblick auf mögliche Erweiterungen des Instruments an. Die hier vorgestellte Herangehensweise ist ein zentrales Instrument, um im Rahmen des Projekts Exemplarisch haben wir die Untersuchung mit zwei kanonischen Texten der deutschen Literatur begonnen, zu denen zahlreiche Interpretationen vorliegen: Lotte ist ein in Python implementierter Algorithmus zur Erkennung von Zitaten. Gegeben einen Quelltext und einen Zieltext, findet er alle Instanzen ab einer Länge von fünf Wörtern, in denen der Zieltext (in unserem Fall eine Literaturinterpretation) einen Teil des Quelltextes, also des literarischen Textes, enthält. Die Idee der Verwendung von (interaktiven) Visualisierungen in den Digital Humanities zur Unterstützung der Arbeit mit Texten aller Art ist nicht neu. Es gibt eine große Anzahl von Ansätzen zur Visualisierung von Strukturen, Häufigkeiten, Mustern etc. 'sowohl zur Unterstützung beim Distant Reading als auch beim Close Reading bzw. zur Kombination beider Perspektiven, wie auch wir sie in dieser Arbeit vorstellen. Für einen ausführlichen Überblick sei verwiesen auf Jänicke et al. (2015a). Auch für die Visualisierung von wiederverwendetem Text, wie beispielsweise Zitaten, gibt es verschiedene Ansätze (Vgl. Jänicke et al. 2015b). Im Unterschied zu Annette liegt der Fokus dieser Ansätze auf der Visualisierung der Art und Häufigkeit des wiederverwendeten Texts oder der Alignierung von verschiedenen Varianten des gleichen Texts, zum Beispiel verschiedener Übersetzungen. Eine Webseite zur Visualisierung von (wörtlichen) Zitaten aus Shakespeares Werken wurde von Miller vorgestellt. Für Annette werden die von Lotte gefundenen Übereinstimmungen weiterverarbeitet, um Schlüsselstellen zu identifizieren. Hierfür kombinieren wir überlappende Übereinstimmungen zu einer Ein Screenshot der Webseite Rechts neben der Heatmap ist der literarische Text selbst dargestellt. Die Grauskala wird dadurch bestimmt, wie viele Interpretationstexte einen Teil einer Stelle oder die Stelle insgesamt zitieren. Die Farbe ist dabei für eine gesamte Stelle immer die gleiche. Die Schriftgröße wird dadurch bestimmt, wie oft ein minimales Segment zitiert wird, somit kann sie auch innerhalb einer Stelle variabel sein. Rechts unten neben dem literarischen Text wird eine Liste aller Interpretationstexte gezeigt. Ganz rechts werden die zehn häufigsten Passagen aufgelistet. Darüber wird bei einer entsprechenden Auswahl, wie unten beschrieben, der gesamte Text einer Interpretation angezeigt. Ausgehend vom Startbildschirm kann der*die Benutzer*in zwischen verschiedenen Zugriffsmöglichkeiten wählen. Die erste Möglichkeit ist die Auswahl einer Stelle, indem diese im Primärtext angeklickt wird. Anstelle der Gesamtliste werden nun rechts davon nur noch diejenigen Interpretationstexte angezeigt, die zu der ausgewählten Stelle beitragen, zusammen mit einer kurzen Vorschau des Textes. Durch Anklicken eines der Interpretationstexte können wir eine bestimmte Interpretation anzeigen lassen, deren Text oben rechts dargestellt wird. Wir können dann diesen Text durchgehen und andere zitierte Passagen auswählen. Rechts unten wird angezeigt, wie oft die ausgewählte Stelle zitiert wird und von wie vielen Interpretationstexten. Darunter finden wir die zehn meistzitierten Segmente dieser Stelle. Als weitere Möglichkeit kann über die Liste der zehn am häufigsten zitierten Stellen auf die möglicherweise relevantesten, jedenfalls am breitesten rezipierten Stellen zugegriffen werden. Nach Auswahl einer dieser Stellen passt sich wie oben beschrieben die Liste der Interpretationstexte an und es kann von hier aus weiter diese Liste oder ein einzelner Interpretationstext untersucht werden. Durch die beschriebene Funktionsweise werden im Primärtext Stellen konstituiert, die teilweise sehr häufig, teilweise nur wenige Male zitiert werden. Im Fokus unserer Projektarbeit stehen die besonders häufig zitierten Stellen, da sie am ehesten die Schlüsselqualität einer Stelle anzeigen. Dabei darf die reine Quantität selbstverständlich nicht absolut gesetzt werden; um ein Korpus im Querschnitt zu überblicken, scheint uns die Häufigkeit von zitierten Stellen aber ein wichtiger Anhaltspunkt zu sein. Das Konzept, nach dem diese Stellen konstituiert werden, ist dabei die Pointe an unserem Ansatz, da es weit über das bloße Auffinden von Zitaten hinausgeht. Die auf der Webseite visualisierten Stellen ergeben sich nämlich häufig erst durch die Überlappung verschieden langer Segmente bzw. Zusammenfassung verschieden langer Zitate aus unterschiedlichen Interpretationstexten (s.¬†o.). Somit sind Lotte und Annette im Zusammenspiel umso wirkungsvoller, je mehr Texte das Korpus umfasst. Umgekehrt können aber auch jene Stellen von besonderem Interesse sein, die überhaupt nicht zitiert werden. Bei einer überschaubaren Textlänge, wie sie Diese Funktionsweise eröffnet auch den Horizont für ein integratives Scalable Reading, das wir im Anschluss an Thomas Weitin als Überwindung der ""Frontstellung von Ein erstes Ziel ist die Einführung weiterer Visualisierungen, die einen umfangreicheren Zugang zu den Texten bieten sollen. Im Folgenden wollen wir eine davon genauer vorstellen, die sich mit der Entwicklung der zitierten Stellen über die Zeit befasst, sowie abschließend ein paar weitere kurz skizzieren. Darüber hinaus wollen wir in Zukunft Zitate, die kürzer als fünf Wörter sind, berücksichtigen. Hierfür muss in erster Linie Lotte angepasst werden. In Annette könnten diese dann so visualisiert werden, dass beispielsweise untersucht werden kann, ob kurze Zitate neue Informationen bringen und ob diese ähnlich zu langen Zitaten verteilt sind oder ob sich hier Unterschiede zeigen. Das Ziel dieser Visualisierung ist die Darstellung des Anteils neuer Zitate eines Interpretationstexts im Verhältnis zu allen Zitaten früherer Interpretationstexte. Die Abbildungen¬†2 und 3 zeigen diese Visualisierung für Bei genauerer Betrachtung von Abbildung¬†2 lässt sich erkennen, dass die drei Texte mit den wenigsten neuen Zitaten auch insgesamt nur sehr wenig zitieren. Gleichzeitig gibt es auch Texte, die verhältnismäßig wenig zitieren und dennoch einen hohen Anteil an neuen Zitaten aufweisen. Abbildung¬†3 zeigt analog die Auswertung für Diese Diagramme sollen in interaktiver Form auf der Webseite verfügbar gemacht werden. Es wird dann zum Beispiel möglich sein, einen Interpretationstext auszuwählen und sich anzeigen zu lassen, welche zitierten Stellen neu sind. Diese können dann auch mit allen früheren und zukünftigen Zitaten verglichen werden. Für die Zukunft sind noch weitere Visualisierungen angedacht. So soll zum Beispiel für alle Interpretationstexte ermittelt und visualisiert werden, ob diese in der Reihenfolge der Zitate dem literarischen Werk folgen. Weiterhin soll die Länge von Zitaten analysiert werden, was dann beispielsweise in die Bestimmung von Schlüsselstellen einfließen kann. Außerdem wollen wir untersuchen, welcher Anteil eines Werks mengenmäßig zitiert wird und wie sich die Zitate über den Interpretationstext verteilen. Im Fokus unseres Beitrags stand ein neues Konzept und dessen Umsetzung zur Identifikation von Schlüsselstellen in literarischen Texten. Neben der praktischen Realisierung als Algorithmus (Lotte) und der Visualisierung zur Erkundung von literarischen Texten (Annette) lag der Schwerpunkt auf den Möglichkeiten und zukünftigen Visionen für Annette. Darüber hinaus haben wir die Überlegungen zur Schlüsselstelle und zur Konstituierung von Stellen vorgestellt und als Beitrag zur Realisierung einer Scalable Reading-Methodik kontextualisiert, die auf Strukturidentifikation abzielt. Die Arbeit wurde durch das DFG-Schwerpunktprogramm (SPP) 2207",de,literaturinterpretationen heben Regel Bestandteil analysiert primärtextes hervor jeweilig These verbunden Interpretation wichtig erscheinen interpretieren ausführlich Passag fassen Mithilfe unten beschrieben Konzept Umsetzung Algorithmus folgend verwandte arbeiten erläutern Kontext Konzept Instrument ehe Algorithmus Webseite Exploration vorstellen Fokus technisch Detail Algorithmu praktisch Anwendung legen schließen Überlegung Konzeption Verortung Digital Humanitie Ausblick möglich Erweiterunge instruments vorgestellt Herangehensweise zentral Instrument Rahmen Projekt exemplarisch Untersuchung kanonisch Text deutsch Literatur beginnen zahlreich Interpretation vorliegen Lotte python implementierter Algorithmus Erkennung zitaten geben quelltext Zieltext finden instanzen Länge wörtern Zieltext unser Fall Literaturinterpretation quelltext literarisch Text enthalten Idee Verwendung interaktiv visualisierungen Digital Humanitie Unterstützung Arbeit Text Art neu Anzahl Ansatz Visualisierung Struktur häufigkeit Muster sowohl Unterstützung Distant Reading clos Reading Kombination beide Perspektive Arbeit vorstellen ausführlich Überblick verweisen Jänicke et Visualisierung wiederverwendet Text beispielsweise zitaten verschieden Ansatz Jänicke et Unterschied Annette liegen Fokus Ansatz Visualisierung Art Häufigkeit wiederverwendeter Texts Alignierung verschieden Variant gleich texts verschieden übersetzungen Webseite Visualisierung wörtlich zitaten Shakespeare Werk Miller vorstellen annette Lotte gefunden übereinstimmungen weiterverarbeiten Schlüsselstelle identifizieren hierfür kombinieren überlappend übereinstimmungen Screenshot Webseite rechts Heatmap literarisch Text darstellen grauskala bestimmen Interpretationstext Stelle Stelle insgesamt zitieren Farbe gesamt Stelle gleich schriftgröße bestimmen minimal Segment zitieren somit innerhalb Stelle variabel rechts unten literarisch Text Liste Interpretationstext zeigen rechts häufig Passag auflisten entsprechend Auswahl unten beschreiben gesamt Text Interpretation anzeigen ausgehend Startbildschirm verschieden Zugriffsmöglichkeit wählen Möglichkeit Auswahl Stelle Primärtext anklicken anstelle gesamtliste rechts Interpretationstext anzeigen ausgewählt Stelle beitragen kurz Vorschau Text Anklicke Interpretationstext bestimmt Interpretation anzeigen lassen Text rechts darstellen Text durchgehen zitiert Passag auswählen rechts unten anzeigen ausgewählt Stelle zitieren interpretationstexten finden meistzitierten Segment Stelle Möglichkeit Liste häufig zitiert Stelle möglicherweise relevantesen jedenfalls breite rezipiert Stelle zugegriffen Auswahl Stelle passen beschreiben Liste Interpretationstext Liste einzeln Interpretationstext untersuchen beschrieben Funktionsweise Primärtext stellen konstituieren teilweise häufig teilweise Mal zitieren Fokus Projektarbeit stehen häufig zitiert stellen eher Schlüsselqualität Stelle anzeigen rein Quantität selbstverständlich absolut setzen korpus querschnitt überblicken scheinen Häufigkeit Zitiert Stelle wichtig Anhaltspunkt Konzept Stelle konstituieren Pointe unser Ansatz bloß auffinden zitaten hinausgehen webseit visualisiert Stelle ergeben nämlich häufig Überlappung verschieden lang segmente Zusammenfassung verschieden lang Zitat unterschiedlich Interpretationstext somit Lotte Annette Zusammenspiel umso wirkungsvoll Text Korpus umfassen umgekehrt Stelle besonderer Interesse zitieren überschaubar Textlänge Funktionsweise eröffnen Horizont integrativ scalabel Reading Anschluss Thomas weitin Überwindung Frontstellung Ziel Einführung weit visualisierung umfangreicher Zugang Text bieten folgend genau vorstellen Entwicklung zitiert Stelle befassen abschließend paar skizzieren hinaus Zukunft zitaten kurz Wörter berücksichtigen hierfür Linie Lotte angepasst Annette können visualisiern beispielsweise untersuchen kurz zitate Information bringen ähnlich lang zitaten verteilen Unterschied zeigen Ziel Visualisierung Darstellung Anteil neu zitate interpretationstexts Verhältnis zitaten früh Interpretationstext zeigen Visualisierung genauer Betrachtung lässen erkennen Text wenigster zitaten insgesamt zitieren gleichzeitig Text verhältnismäßig zitieren dennoch hoch Anteil zitaten aufweisen zeigen analog Auswertung diagramme interaktiv Form Webseite verfügbar Interpretationstext auswählen anzeigen lassen zitiert Stelle neu früh zukünftig zitat vergleichen Zukunft visualisierung Angedacht Interpretationstext ermitteln visualisieren Reihenfolge Zitat literarisch Werk folgen weiterhin Länge Zitate analysieren beispielsweise Bestimmung Schlüsselstelle einfließen untersuchen Anteil Werk mengenmäßig zitieren Zitat Interpretationstext verteilen Fokus unser Beitrag stehen neu Konzept Umsetzung Identifikation Schlüsselstelle literarisch Text praktisch Realisierung Algorithmus Lotte Visualisierung Erkundung literarisch Text Annette liegen Schwerpunkt Möglichkeit zukünftig Vision annette hinaus Überlegung Schlüsselstelle Konstituierung Stelle vorstellen Beitrag Realisierung scalabel kontextualisieren Strukturidentifikation abzielen Arbeit Spp,"[('interpretationstext', 0.4032804966679676), ('stelle', 0.33512521690556946), ('zitaten', 0.29127556684397654), ('zitieren', 0.2478426673810349), ('annette', 0.2199711800007096), ('zitiert', 0.1941837112293177), ('lotte', 0.1707387314841062), ('rechts', 0.1600945317643843), ('anzeigen', 0.1403533283991336), ('visualisierung', 0.13922991733255605)]"
2022,DHd2022,BRANDL_Stephanie_Textexplorationen_in_der_digitalen_Literatu.xml,Textexplorationen in der digitalen Literaturwissenschaft Eine kritische und angewandte Auseinandersetzung mit Repräsentations- und Interpretationsansätzen von Text,"Stephanie Brandl (K√∏benhavns Universitet, Technische Universität Berlin); David Lassner (Technische Universität Berlin); Cora Krömer (Universitätsbibliothek Erlangen-Nürnberg); Anne Baillot (Le Mans Université)","Textrepräsentation, Lesen, Visualisierung, Maschinelles Lernen","Entdeckung, Programmierung, Inhaltsanalyse, Strukturanalyse, Kontextsetzung, Bereinigung","Maximalanzahl Teilnehmende: 25 Räumliche Anforderungen: Idealerweise bringen Teilnehmende ihre eigenen Laptops mit, die bestenfalls schon die nötige Software vorinstalliert haben. Wir werden kurz vor der Konferenz eine Willkommens-E-Mail mit einer Liste der relevanten Software verschicken. Die praktischen Sitzungen werden mithilfe von Jupyter Notebooks (Python3, Jupyter) abgehalten. Wir planen zusätzlich als Absicherung einen Online-Zugang zu einem JupyterHub Server mit vorinstallierten Paketen für Teilnehmende, bei denen die Installation Schwierigkeiten macht. Wir ermutigen die Teilnehmenden ausdrücklich einen eigenen Datensatz mitzubringen, an Beispiel dessen die Aufgaben ausgeführt werden können. Wir gehen davon aus, dass dies aufschlussreich für die jeweiligen Teilnehmenden ist und zusätzlich den Workshop bereichert. Wir haben außerdem bereits mit zwei Dateninstitutionen Kontakt aufgenommen. Beide haben Interesse bekundet sowohl Daten für den Workshop zur Verfügung zu stellen als auch am Workshop teilzunehmen. Damit stellen wir sicher, dass auch für Teilnehmende, die keine Daten mitbringen, Material vorhanden ist, und im Zweifel auch Expert:innen vor Ort sind, die etwaige Zwischenergebnisse der verwendeten Methoden evaluieren und in Kontext setzen können. Traditionelle Methoden und Theorien der Literaturwissenschaft können in vier Typen unterteilt werden: autor-, text-, leser- und kontextorientiert [Köppe & Winko, 2013]. Die Literaturwissenschaft zeichnet sich durch einen Methodenpluralismus aus [Nünning & Nünning 2020], der in den letzten Jahren vor allem durch kognitionswissenschaftliche [Zunshine, 2015] und empirische Ansätze [Kuiken & Jacobs, 2020] weiter ausdifferenziert wurde. Gerade diese haben zu neuen Erkenntnissen in leserorientierten Ansätzen geführt. Umgekehrt lässt sich auch vermuten, dass neue Lektüreweisen, –in unserem Fall maschinengenerierte Lektüren'literaturwissenschaftliche Innovationen direkt oder mittelbar als Folgewirkung hervorbringen [Parr & Honold, 2018]. Nicht nur die Literaturwissenschaft an sich, sondern auch ihre leserorientierten Ansätze zeichnen sich durch Theorien- und Methodenpluralismus aus [Rautenberg & Schneider, 2015], z. B. rezeptionstheoretische Ansätze [Willand, 2014]. Die Verbreitung von digitalen Lesemedien führt zu einer neuen Beschäftigung mit Repräsentationsformen von Texten und Textorganisation [Saemmer, 2015], da der Sinn eines Textes nicht davon unabhängig gedacht werden kann. Dies führt zu neuen und zu erlernenden Lesestrategien, die es zu erforschen gilt. Auf der anderen Seite prägen visuelle Repräsentationen von Literatur schon lange ihre Institutionalisierung: Literarische Bewegungen werden in Schulbüchern in Form von Zeitleisten dargestellt, dramatische Handlungen mit der Freytagschen Pyramide abgebildet, und dies schon seit dem 19. Jahrhundert. Bereits diese Ansätze setzen sich zum Ziel, übergeordnete Strukturen anschaulich zu machen, die sich aus dem literarischen Text selbst ableiten lassen. Mit der bourdieuschen Literatursoziologie etablierte sich in den 60er Jahren die Vorstellung der Literatur als ein Feld, das von diversen Dynamiken durchzogen wird. Die digitalen Methoden, spätestens seit Morettis Word Embeddings bilden häufig die Brücke zwischen dem Text als Zeichenfolge und der Darstellung, mit der digitale Literaturwissenschaftler:innen ihr Korpus lesen, also bspw. als Input für ein ML-Modell. In Eine weitere große Herausforderungen bei der Verwendung von Word Embeddings ist die zuverlässige Evaluation ihrer Qualität, das heißt, wie sehr sie mit der gewünschten Bedeutung übereinstimmen. Es gibt zwar eine Menge von Evaluationsmethoden, wie Analogy Tests oder die Applikation von Downstream Tasks, häufig klopfen diese allerdings nur Teile der zugrundeliegenden Repräsentation ab und geben kein umfassendes Bild über ihre Qualität. Ein in diesem Zusammenhang viel diskutierter Aspekt von Word Embeddings sind 'Biases', der zugrundeliegenden Daten, die durch die Modellarchitekturen reproduziert, oder sogar verstärkt werden können. Dies ist insbesondere bei Systemen ein Problem, die auf so großen und heterogenen Datenmengen trainiert wurden, dass es sich im Nachhinein schwer dokumentieren lässt, welche Biases dem jeweiligen Modell innewohnen (Bender et al., 2021). In Bezug auf die häufig kleineren, gut mit Metadaten versehenen Korpora in den DH, steckt in diesen reproduzierten Biases aber tatsächlich ein interessantes Forschungsfeld (Gebru et al., 2018). So lässt sich beispielsweise sehen, in welchen Zeiträumen welche Biases besonders vorherrschend waren, bzw. wann diese möglicherweise wieder verschwinden. Hierfür könnten mehrere Word Embeddings für verschiedene Zeiträume trainiert, und dann miteinander verglichen werden. Diese Art von Vergleich ist aber nicht nur auf Biases beschränkt, man kann sie ebenfalls als eine Form des 'Realitätschecks' durchführen, wenn man Dynamiken zwischen Word Embeddings betrachtet: Bei einem Korpus, das einen größeren Zeitraum umfasst, verändern sich darin allgemein die Repräsentationen der Worte so, wie man es von der Bedeutung der Worte auch erwarten würde? In dem Workshop möchten wir uns der beschriebenen Thematik von zwei Seiten nähern, wir geben (1) einen Überblick über traditionelle Theorien und Methoden der Literaturwissenschaft, insbesondere auch der Leseforschung und kontextualisieren letztere¬† mit Erkenntnissen aus neurowissenschaftlichen Laborstudien. Wir geben (2) eine Zusammenfassung der aktuellen Methoden zu Textrepräsentation [z.B. Glove, Bert] und Beispiele für DH-Projekte, in denen diese bereits verwendet werden, insbesondere fokussieren wir uns hier auch auf gängige Visualisierungsmethoden und die damit zusammenhängenden Limitationen. Beides wollen wir durch praktische Übungen an eigenen oder bereitgestellten Datensätzen näherbringen. Ziel ist es, dass im Laufe des Workshops Verknüpfungen und Schnittmengen der unterschiedlichen Lesebegriffe aus den verschiedenen Fachrichtungen hergestellt und gefunden werden. Zum Abschluss bieten wir konkrete Möglichkeiten an, welche Visualisierungen auf welche Weise ihren Platz als literaturwissenschaftliche Methode finden können. Der Ablauf des Workshops gliedert sich in Vorträge sowie praktische Sitzungen, bei denen das im Vortrag zuvor besprochene direkt von den Teilnehmenden ausprobiert werden soll. Im folgenden soll ein kurzer Überblick über die einzelnen Vortragsteile gegeben werden. (Cora Krömer) (David Lassner)  (Stephanie Brandl) In welcher Form und Geschwindigkeit Menschen Texte lesen wird bereits seit Jahrzehnten untersucht [Rayner, 1998] und modelliert [Reichle et al., 2003]. Oberflächliche Eigenschaften wie Worthäufigkeiten oder Wortlängen beeinflussen unsere Lesegewohnheiten. Neuere Forschung zeigt zudem, dass auch die Darstellung des Textes eine Rolle spielt, ob ein Gedicht beispielsweise in der ursprünglichen Form oder als Prosatext gezeigt wird [Fechino et al., 2020]. Dementsprechend ist es auch wichtig, Word Embeddings so darzustellen, dass der sehr dichte Informationsgehalt verarbeitet und evaluiert werden kann. (Stephanie Brandl) Word Embeddings sind stark abhängig vom zu Grunde liegenden Datensatz, beispielsweise lernen statische Methoden wie GloVe genau eine Repräsentation pro Wort, so dass zeitliche oder andere Entwicklungen im Datensatz nicht beachtet werden und in einem Vektor verschmelzen. Diese Abhängigkeit führt auch dazu, dass Verzerrungen (Biases) im Datensatz möglicherweise in den Wort-Vektoren auftauchen. Wenn beispielsweise alle Personen des medizinischen Personals im Datensatz weiblich sind, wird ein Algorithmus einen männlichen Bewerber möglicherweise für eher ungeeignet halten, um als Arzt zu arbeiten. Verschiedene Methoden wurden bereits veröffentlicht, um solche problematischen Strukturen (zeitliche Abhängigkeit, Biases uvm) in Word Embeddings aufzuzeigen [Basta et al., 2019; Brandl & Lassner, 2019; Gonen et al., 2020] und auch aufzulösen [Gonen et al., 2019; Manzini et al., 2019]. Wir werden einige davon besprechen und im Anschluss auch an den gelernten Word Embeddings anwenden. (Anne Baillot) Zum Schluss wird auf die Entwicklung literaturwissenschaftlichen Umgangs mit Analyse bzw. Interpretation von Text eingegangen: zuerst auf das Faszinosum Netzwerk, dann auf die Herausforderung der Definition von Beziehungen zwischen Textelementen und ihrer Bedeutung, schließlich auf veröffentlichungstechnische Fragen, die mit der Einbettung dieser graphischen Repräsentationen einhergehen.",de,Maximalanzahl Teilnehmende räumlich anforderungen idealerweise bringen Teilnehmend laptops bestenfalls nötig Software vorinstalliern Konferenz Liste relevant Software verschicken praktisch Sitzung Mithilf Jupyter Notebooks Jupyter abhalten planen zusätzlich Absicherung Jupyterhub Server vorinstalliert paketen Teilnehmende Installation Schwierigkeit ermutigen Teilnehmende ausdrücklich datensatz mitbringen Aufgabe ausführen aufschlussreich jeweilig teilnehmend zusätzlich Workshop bereichern dateninstitution Kontakt aufnehmen Interesse bekunden sowohl daten Workshop Verfügung stellen Workshop teilnehmen stellen sicher Teilnehmende daten mitbringen Material vorhanden Zweifel experen innen Ort etwaig zwischenergebnisse verwendet Methode evaluieren Kontext setzen traditionell Methode Theorie Literaturwissenschaft Type unterteilt kontextorientiert Köppe Winko Literaturwissenschaft zeichnen Methodenpluralismus Nünning Nünning letzter kognitionswissenschaftlich zunshine empirisch ansätz kuiken Jacobs ausdifferenzieren Erkenntnis leserorientiert Ansätze führen umgekehrt lässen vermuten Lektüreweis unser Fall maschinengeneriert Innovation direkt mittelbar Folgewirkung hervorbring parr Honold Literaturwissenschaft leserorientiert Ansatz zeichnen Methodenpluralismus Rautenberg Schneider rezeptionstheoretisch ansätz Willand Verbreitung digital Lesemedie führen Beschäftigung Repräsentationsforme Text Textorganisation Saemmer Sinn Text unabhängig denken führen erlernend Lesestrategien erforschen gelten Seite prägen visuell Repräsentatione Literatur Institutionalisierung literarisch Bewegung Schulbücher Form Zeitleist darstellen dramatisch Handlung freytagsch Pyramide abbilden Jahrhundert Ansatz setzen Ziel übergeordnet Struktur anschaulich literarisch Text ableiten lassen bourdieusch Literatursoziologie etablieren Vorstellung Literatur Feld diverser Dynamik durchziehen digital Methode spätestens Moretti Word Embedding bilden häufig Brücke Text Zeichenfolge Darstellung digital Literaturwissenschaftler innen Korpus lesen input Herausforderung Verwendung Word embeddings zuverlässig Evaluation Qualität gewünscht Bedeutung übereinstimmen Menge evaluationsmethoden Analogy tests Applikation Downstream Tasks häufig klopfen Teil zugrundeliegend Repräsentation geben umfassend Bild Qualität Zusammenhang Diskutierter Aspekt Word embeddings biases zugrundeliegend daten Modellarchitekture reproduzieren sogar verstärken insbesondere System Problem heterogen datenmenger trainieren nachhinein schwer dokumentieren lässn biases jeweilig Modell innewohnen bend et Bezug häufig klein metadaten versehen Korpora dh stecken reproduzierten Bias tatsächlich interessant Forschungsfeld Gebru et lässen beispielsweise sehen zeiträumen Bias vorherrschend möglicherweise verschwinden hierfür können mehrere Word Embedding verschieden zeiträume trainieren miteinander vergleichen Art Vergleich Bias beschränken ebenfalls Form realitätschecks durchführen Dynamik Word Embedding betrachten Korpus groß Zeitraum umfassen verändern allgemein repräsentatione Wort Bedeutung Wort erwarten Workshop möchten beschrieben Thematik Seite nähern geben Überblick traditionell Theorie Methode Literaturwissenschaft insbesondere Leseforschung Kontextualisier Erkenntnis neurowissenschaftlich Laborstudie geben Zusammenfassung aktuell Methode Textrepräsentation glove bern Beispiel verwenden insbesondere fokussieren gängig visualisierungsmethod zusammenhängend limitationer beide praktisch übungen bereitgestellt datensätzer näherbringen Ziel Lauf Workshop Verknüpfung Schnittmengen unterschiedlich Lesebegriffe verschieden Fachrichtung herstellen finden Abschluss bieten konkret Möglichkeit visualisierung Weise Platz literaturwissenschaftlich Methode finden Ablauf Workshop gliederen vorträge praktisch Sitzung Vortrag zuvor besprochen direkt Teilnehmend ausprobieren folgend kurz Überblick einzeln Vortragsteil geben Cora Krömer David Lassner Stephanie Brandl Form Geschwindigkeit Mensch Text lesen Jahrzehnt untersuchen Rayner modellieren reichlen et oberflächlich eigenschaften Worthäufigkeite Wortläng beeinflussen Lesegewohnheit neu Forschung zeigen zudem Darstellung Text Rolle spielen Gedicht beispielsweise ursprünglich Form Prosatext zeigen fechino et wichtig Word Embedding darstellen dicht informationsgehalt verarbeiten evaluieren Stephanie Brandl Word Embedding stark abhängig grund liegend Datensatz beispielsweise lernen statisch Methode glove genau Repräsentation pro Wort zeitlich Entwicklung Datensatz beachten Vektor verschmelzen Abhängigkeit führen verzerrungen biases Datensatz möglicherweise auftauchen beispielsweise Person medizinisch Personal Datensatz weiblich Algorithmus männlich Bewerber möglicherweise eher ungeeignet halten Arzt arbeiten verschieden Methode veröffentlichen problematisch Struktur zeitlich Abhängigkeit biases uvm Word Embedding aufzuzeig basta et Brandl Lassner gon et aufzulös gon et manzini et besprech Anschluss gelernt Word embeddings anwenden ann Baillot schluss Entwicklung literaturwissenschaftlich umgangs Analyse Interpretation Text eingehen Faszinosum Netzwerk Herausforderung Definition Beziehung Textelement Bedeutung schließlich veröffentlichungstechnisch Frage Einbettung graphisch Repräsentation einhergehen,"[('word', 0.25502663394183206), ('embedding', 0.23391839984596371), ('biases', 0.19786993550868673), ('workshop', 0.15171331131739763), ('datensatz', 0.1510159756649644), ('brandl', 0.14840245163151503), ('teilnehmende', 0.12625155731162832), ('embeddings', 0.1254043579417576), ('bias', 0.12082824678080166), ('methode', 0.11950666262408677)]"
2022,DHd2022,HERRMANN_J__Berenike_Lieblingsgegenden__Fenster_und_Mauern__.xml,"Lieblingsgegenden, Fenster und Mauern Zur emotionalen Enkodierung von Raum in Deutschschweizer Prosa zwischen 1850 und 1930","Berenike Herrmann (Universität Bielefeld, Germany); Giulia Grisot (Universität Bielefeld, Germany)","Schweizer Literatur, Raum, Sentiment","Inhaltsanalyse, Räumliche Analyse, Modellierung, Annotieren, Artefakte, Text","Raum ist eine wichtige Dimension von ""Kultur"", nicht zuletzt in literarischen Artefakten. Definiert als ""area, as perceived by people, whose character is the result of the action and interaction of natural and/or human factors"" (Council of Europe, 2000, S. 2) impliziert besonders die Landschaft, das ""Gebiet, wie wahrgenommen"", einen oftmals vergleichenden En- und Dekodierungsakt. Die Räume der deutschschweizer Literatur sind wie bei Spitteler offenbar regelmäßig solche ""Lieblingsgegenden"", die dann doch Untiefen offenbaren, wie die Dörfer Gotthelfs, die Kleinstädte Kellers ( Spittelers scheinbar zahme Alpen, Spyris urbanes Gefängnis und schließlich Hölderlins erhabener ""furchtbarherrlicher Haken"" des Hochgebirgsaufstiegs ( Unser Beitrag möchte die beiden beschriebenen Ebenen von Kultur mit der des ""digitalen Gedächtnisses"" zusammenbringen, indem wir computationelle Verfahren auf literarische Texte (als ""Schweizer digitales Kulturerbe"") anwenden, um die affektive Enkodierung dargestellter Raumtypen (als ""Reflektion der Reflektion"") zu untersuchen. Ausgehend vom übergreifenden Forschungsinteresse einer Komparatistik der deutschsprachigen Länder möchte unser Beitrag erste Ergebnisse berichten über die emotionale Enkodierung von fiktionalem Raum. Anhand des DCHLi (Deutschschweizer Literaturkorpus), zurzeit als Pilotkorpus mit 76 Texten, und ausgehend von einem semiotischen Zugang zu textuell enkodierten Emotionen (z.B. Schiewer, 2007; vgl. Anz, 2007; Winko, 2022) und Raumanalyse (Balshaw, & Kennedy, 2000; Bologna, 2020) legen wir die in gängigen Sentiment-Diktionären vorgehaltenen Affekt-Kategorien zwischen dimensionalen (Valenz, Arousal) und diskreten Emotionen (""Angst"", ""Freude"", ""Wut"", ""Trauer"", ""Ekel"") an. Wir fragen:  Unsere quantitativen Befunde sollen Bezüge herstellen zu ikonischen Kultur/Natur-Dichotomien im Erbe der Romantik, zu historischen Stadt/Land-Konstellationen, aber auch zu einem nationalliterarischen Rahmen mit vielbeklagtem Schweizer ""Mythos"" (Böhler, 2010) einerseits und identifikatorischen (oftmals Alpen-orientierten) Angeboten (Zimmer, 1998) für die ""imagined community"" (Anderson) der sogenannten Willensnation andererseits. Unser DCHLi Pilotkorpus umfasst derzeit 76 fiktionale Prosatexte von AutorInnen, die der deutschschweizer ""Nationalliteratur"" zugeordnet werden und die zwischen 1854 und 1930 zuerst publiziert wurden (N= 2,025,529 Wörter). DCHLi enthält das wachsende Deutschschweizer ELTeC-gsw (Grisot & Herrmann, 2021) das wiederum Teil der European Literary Text Collection (ELTeC, Odebrecht et al., 2021) ist. Ausgehend vom derzeit Im ersten Schritt erstellten wir ein möglichst umfassendes und feingranuliertes Diktionär ""räumlicher Entitäten"", das auf höchster Taxonomie-Ebene die Kategorien RURAL und URBAN zusammenfasst, die sich wiederum in fünf Subkategorien ""natural entity"", ""rural entity,"" und ""geographic entity"", sowie ""urban entity"" und ""geopolitical entity"" auffächern (vgl. Wartmann et al. 2018, p. 1580; siehe Abb. 1). Hier wurden unter Rückgriff auf Ressourcen wie Openthesaurus und das Schweizer Idiotikon historisch wie sprachlich relevante Elemente berücksichtigt (i.e. Im zweiten Schritt erstellten wir für einen systematischen Vergleich ein Repositorium mit acht der frei verfügbaren Sentiment-Diktionäre (ADU, BAWL, Germanlex, LANG, Klinger, Plutchik, SentiWS, SentiArt, siehe Tabelle 2). Deren unterschiedlichen Formate wurden für die automatische Sentiment-Annotation in einer processing pipeline vereinheitlicht. Abbildung 2 zeigt die lexikalische Abdeckung der acht Diktionäre auf dem DCHLi, die angibt, wie viele der Wörter von der jeweiligen Ressource erkannt wurden (geordnet in abfallender Reihenfolge). Im dritten Schritt errechneten wir das semiotische Emotionspotenzial innerhalb der räumlichen Seedword-Spannen. Abbildung 3 zeigt den Abbildungsprozess der räumlichen Entitäten auf das Korpus beispielhaft für das BAWL-R-Diktionär: Sobald eine Entität identifiziert ist, werden innerhalb einer Gesamtspanne von 101 Wörtern je 50 Wörter vor und nach der Entität für die Berechnung von Emotions- und Sentimentwerten einbezogen (ohne Stoppwörter). Mittels dieses Verfahrens kann die Repräsentation von Emotionen (Valenz, Diskrete Emotionen) und ihre Ausprägung (Arousal) bezüglich des fiktionalen Raums näherungsweise untersucht werden, wobei uns zunächst die potenzielle Differenz in der Emotionsrepräsentation zwischen ländlichen und städtischen Räumen interessierte. Wir verwendeten R (Version R 4.1.0, R Core Team, 2021) um mittels mixed linear models den Effekt des Entitätstyps (rural, urban) auf die jeweiligen Sentiment-Werte zu beobachten, mit AutorIn und Titel als randomisierte Faktoren. Verwendete Pakte waren v.a. tidyverse (Wickham et al., 2019); LmerTest (Kuznetsova et al., 2017), und tm (Feinerer, 2020). Wir konnten statistisch signifikante Effekte des Entitätstyps u.a. auf die Valenz/Polarität in verschiedenen Diktionären beobachten, wobei LANG und BAWL ""positives Sentiment"" häufiger für ""rural"" Passagen aufwiesen, Germanlex jedoch den entgegengesetzten Befund (Tabelle 3). Für die diskreten Emotionen berücksichtigte das mixed model jede einzelne Emotion als zusätzlichen ""fixed factor"" (Tabelle 4). Abweichungen zwischen den Diktionären konnten wieder beobachtet werden, wobei SentiArt signifikante Differenzen für fünf Basisemotionen (ausser Obwohl die Sentimentdetektion, das räumliche Matching der Emotionen und die Korpusgröße weiter verbessert werden sollen, interpretieren wir die vorliegenden Daten vorsichtig dahingehend, dass Textpassagen mit ländlichen und Natur-Referenzen in unserem Korpus häufiger positiv enkodiert sind. Es scheint, dass diese ""ruralen"" und ""Natur-"" Räume im Vergleich insgesamt mehr unterschiedliche und möglicherweise reichhaltigere Emotionen repräsentieren. Angesichts der Zusammensetzung des vorliegenden Korpus kann dies nicht nur auf eine topische Assoziation von positiv enkodierter Natur vs. negativ enkodierter Stadt/industrialisierter Zivilisation bezogen werden, sondern scheint auch Landschaft und Natur als vornehmlichen Schauplatz der Diegese abzubilden. Schlägt man den Bogen weiter, und projiziert noch hypothetisch auf die Grundgesamtheit der Deutschschweizer Prosa (Herrmann et al., 2021), könnte der Vorschlag, dass Deutschschweizer Literatur in dieser Zeit vornehmlich auf dem Lande und in der Natur stattfindet, im Luhmannschen Sinne als ""kultureller"" Differenzvorschlag verstanden werden: ein Identifikationsangebot, das ""Schweiz"" ebendort, und nicht anderswo, verortet. Wohlgemerkt wäre gerade unter solchen Bedingungen die evidente Rolle von Technik, Infrastruktur, Handel und Industrialisierung mitzumodellieren. Wir schließen mit einer unabdingbaren methodologischen Notiz. In der vorliegenden Studie war es unsere Absicht, diktionärbasierte Sentimentanalyse als im Feld der DH gegenwärtig noch kanonischen Ressourcentyp in Anschlag zu bringen (Kim & Klinger, 2019). Die niedrige lexikalische Abdeckung für die meisten Diktionäre, die im Umlauf sind (Abb. 2), zeigt auf, dass hier neue Ressourcen und ein erweitertes Methodenbewusstsein nötig sind. Untersucht man die Reliabilität und Domänenspezifik der einzelnen Diktionäre genauer, wie wir es getan haben, wird schnell deutlich, dass es sich für die DH lohnt, den Anschluss an den ",de,Raum wichtig Dimension Kultur zuletzt literarisch Artefakt definieren area As Perceived by people Whose character -- -- resulen -- the action and interaction of natural and or human factors council -- europe implizieren Landschaft Gebiet wahrnehmen oftmals vergleichend Dekodierungsakt räume Deutschschweizer Literatur spittel offenbar regelmäßig lieblingsgegend untiefen offenbaren dörfer gotthelfs kleinstädt Keller Spitteler scheinbar Zahme alpen Spyris urban Gefängnis schließlich hölderlin erhabener furchtbarherrlich Haken Hochgebirgsaufstieg Beitrag beschrieben Ebene Kultur digital gedächtnisses zusammenbringen computationell Verfahren literarisch Text Schweizer digital Kulturerbe anwenden affektiv Enkodierung dargestellter Raumtype Reflektion Reflektion untersuchen ausgehend übergreifend forschungsinteresse Komparatistik deutschsprachig Land Beitrag Ergebnis berichten emotional Enkodierung fiktional Raum anhand Dchli Deutschschweizer Literaturkorpus Zurzeit Pilotkorpus Text ausgehend semiotisch Zugang Textuell enkodiert Emotion Schiewer anz Winko Raumanalyse Balshaw Kennedy bologna legen gängig vorgehalten dimensional Valenz Arousal Diskret Emotion Angst Freude Wut Trauer Ekel fragen quantitativ befunde bezug herstellen ikonisch Kultur Erbe Romantik historisch Stadt nationalliterarisch Rahmen vielbeklagt Schweizer Mythos böhl einerseits identifikatorisch oftmals anbieten zimm imagined Community Anderson sogenannter Willensnation andererseits Dchli Pilotkorpus umfassen derzeit fiktional Prosatext Autorinne Deutschschweizer Nationalliteratur zuordnen publizieren wört Dchli enthalten wachsend Deutschschweizer grisot Herrmann wiederum European literary Text Collection Eltec odebrechen et ausgehend derzeit Schritt erstellen möglichst umfassend feingranuliert diktionär räumlich entitäen hoch kategorie Rural urban zusammenfassen wiederum Subkategorie natural entity rural entity Geographic entity urban entity Geopolitical entity auffächern Wartmann Et sehen abb Rückgriff Ressource Openthesaurus Schweizer idiotikon historisch sprachlich relevant elemente berücksichtigen Schritt erstellen systematisch Vergleich Repositorium frei verfügbar adu bawl Germanlex Klinger Plutchik Sentiws Sentiart sehen Tabell unterschiedlich Format automatisch Processing Pipeline vereinheitlichen Abbildung zeigen lexikalisch Abdeckung Diktionäre Dchli angeben Wörter jeweilig Ressource erkennen geordnet abfallend Reihenfolge Schritt errechneten semiotisch Emotionspotenzial innerhalb räumlich Abbildung zeigen Abbildungsprozess räumlich entitäten Korpus beispielhaft sobald Entität identifizieren innerhalb Gesamtspanne wörtern Wörter Entität Berechnung sentimentwert einbeziehen Stoppwörter mittels verfahrens Repräsentation Emotion Valenz diskret Emotion Ausprägung Arousal bezüglich fiktional Raum näherungsweise untersuchen wobei potenziell Differenz Emotionsrepräsentation ländlich städtisch räumen interessieren verwenden r Version r r Core Team mittels Mixed Linear models Effekt Entitätstyp Rural urban jeweilig beobachten Autorin Titel Randomisierte Faktor verwendet Pakt Tidyverse Wickham et Lmertest Kuznetsova et tm feiner statistisch signifikant Effekt Entitätstyp Valenz Polarität verschieden Diktionär beobachten wobei bawl positiv Sentiment häufig rural Passag aufwiesen Germanlex entgegengesetzten Befund Tabelle diskret Emotion berücksichtigen Mixed Model einzeln Emotion zusätzlich Fixed factor tabell Abweichung diktionärer beobachten wobei Sentiart signifikant Differenz basisemotioner obwohl Sentimentdetektion räumlich matching Emotion Korpusgröße verbessern interpretieren vorliegend daten vorsichtig dahingehend Textpassagen ländlich unser korpus häufig positiv enkodieren scheinen ruralen räume Vergleich insgesamt unterschiedlich möglicherweise reichhaltig Emotion repräsentieren angesichts Zusammensetzung vorliegend Korpus topisch Assoziation positiv enkodiert Natur negativ enkodiert Stadt industrialisiert Zivilisation beziehen scheinen Landschaft Natur vornehmlich Schauplatz Diegese abbilden schlagen Bogen projizieren hypothetisch Grundgesamtheit Deutschschweizer Prosa Herrmann et Vorschlag deutschschweiz Literatur vornehmlich Land Natur stattfinden luhmannsch Sinn kulturell differenzvorschlag verstehen identifikationsangebieten Schweiz Ebendort anderswo verorten wohlgemerkt Bedingung evident Rolle Technik Infrastruktur Handel Industrialisierung mitzumodellieren schließen unabdingbar methodologisch Notiz vorliegend Studie Absicht diktionärbasiert Sentimentanalyse Feld dh gegenwärtig kanonisch Ressourcentyp Anschlag bringen Kim Klinger niedrig lexikalisch Abdeckung meister diktionär Umlauf abb zeigen Ressource erweitert Methodenbewusstsein nötig untersuchen Reliabilität domänenspezifik einzeln Diktionäre genau tun schnell deutlich dh lohnen Anschluss,"[('deutschschweizer', 0.23079934694247514), ('emotion', 0.22867296691843753), ('dchli', 0.1846394775539801), ('rural', 0.1846394775539801), ('urban', 0.171977675508326), ('entity', 0.1450311079135832), ('diktionär', 0.13847960816548507), ('enkodiert', 0.13847960816548507), ('valenz', 0.13847960816548507), ('schweizer', 0.1289832566312445)]"
2022,DHd2022,SCHMIDT_Thomas_Evaluation_computergestützter_Verfahren_der_E.xml,Evaluation computergestützter Verfahren der Emotionsklassifikation für deutschsprachige Dramen um 1800,"Thomas Schmidt (Lehrstuhl für Medieninformatik, Universität Regensburg); Katrin Dennerlein (Institut für Deutsche Philologie, JMU Würzburg); Christian Wolff (Lehrstuhl für Medieninformatik, Universität Regensburg)","Emotionsklassifikation, Transformer-basierte Modelle, Drama, Sentiment Analysis, Maschinelles Lernen","Datenerkennung, Programmierung, Inhaltsanalyse, Sprache, Literatur, Text","Transformerbasierte Sprachmodelle wie BERT (Devlin et al. 2018) und ELECTRA (Clark et al. 2020) gelten als state-of-the-art und Ausgangspunkt für zahlreiche Aufgaben des Natural Language Processing (NLP) (Shmueli / Ku 2019; Munikar et al. 2019; Cao et al. 2020; Dang et al. 2020; Gonz√°les-Carvajal et al. 2021; Cortiz 2021). Als ein entscheidender Vorteil dieser Modelle hat sich die dynamische Repräsentation von Tokens in Abhängigkeit von ihrem Kontext herausgestellt. Der Großteil dieser Modelle wird jedoch mit zeitgenössischer Sprache, vor allem mit Sach- und Fachtexten aus dem Web (z.B. In den Digital Humanities (DH) werden Sentiment-Analyse (die Einteilung, ob ein Text eher positiv/negativ konnotiert ist) und Emotionsklassifikation (die Erkennung bzw. Zuordnung distinkter Emotionskonzepte in Texten) in den letzten Jahren immer populärer. Sie werden verwendet, um moderne Textsorten wie Songtexte (Schmidt et al. 2020a), Filmtexte (Schmidt et al. 2020b) und Texte aus den sozialen Medien zu analysieren (Moßburger et al. 2020; Schmidt et al. 2020c; 2020d) finden aber auch Einsatz für literarische Genres wie beispielsweise Märchen (Alm / Sproat 2005; Mohammad 2011), Romane (Kakkonen / Kakkonen 2011; Mohammad et al. 2011; Reagan et al. 2016; Zehe et al. 2016) oder Dramen (Mohammad 2011; Schmidt / Burghardt 2018; Schmidt et al. 2018a; 2018b; Schmidt 2019; Schmidt et al. 2019a; 2019b; 2019c; Yavuz 2020; Schmidt et al. 2021). Die Ziele variieren dabei von der Exploration von Sentiment- und Emotionsverläufen in einzelnen Werken bis zu Gruppenvergleichen (siehe Kim / Klinger 2019). Die steigende Popularität ist wenig überraschend, da die hermeneutische Analyse von Emotionen eine lange Tradition in der Literaturwissenschaft hat, z.B. in der Dramenanalyse (Pikulik 1965; Wiegmann 1987; Anz 2011; Schonlau 2017). Im folgenden Proposal präsentieren wir eine Studie aus dem DFG-Projekt Zur Evaluation und zum Training von Algorithmen wurde ein Goldstandard für ein Sub-Korpus unseres Gesamtkorpus"" annotiert. Emotion wird definiert als der Bewusstseinszustand einer Figur, wie sie sich auch in Text ausdrückt. Annotiert wird die eigene oder zugeschriebene Emotion von Figuren in Abhängigkeit von Kontext und Interpretation. Das Schema hebt sich von üblichen Schemata, die meist von der Psychologie inspiriert sind (Wood et al. 2018a; 2018b) ab, um literarische Interessen zu integrieren. Es besteht aus 13 Ein Sonderfall des Schemas ist Das zu analysierende Hauptkorpus unseres Gesamtprojektes setzt sich aus unterschiedlichen Dramenkollektionen für die Jahre 1650-1815 aus TextGrid Für die Annotation wurde das Tool CATMA (Gius et al. 2020) verwendet. Die Dramen wurden vollständig von Anfang bis Ende annotiert. Die Lektüre des gesamten Dramas ist notwendig, da kontextabhängig annotiert wird. Je zwei studentische Hilfskräfte haben jedes Werk unabhängig voneinander annotiert. Die Hilfskräfte wurden vor der Annotation mittels Pilotstudien von einer Expertenannotatorin trainiert und hatten Zugriff auf eine Annotationsanleitung. Je nach Länge des Textes hatten die Annotator*innen 1-2 Wochen Zeit pro Drama. Der Goldstandard besteht insgesamt aus 6.596 Emotionsannotationen (Abbildung 1). Auf Polaritätsebene sind die meisten Annotationen negativ (56%), 34% positiv und 11% mit der Klasse ""emotionale Bewegtheit"" markiert. Einige Kategorien (z.B. Lust und Freundschaft) wurden selten markiert. Die Token-Statistiken verdeutlichen die Varianz in den Annotationslängen: im Schnitt besteht eine Annotation aber aus 25 Tokens für alle Kategorien. Da Texteinheiten von variabler Länge und überlappende Texteinheiten annotiert werden können, muss zur Berechnung von Übereinstimmungsmetriken eine Festlegung auf eine Texteinheit getroffen werden. Dazu wird folgende Heuristik angewendet: Für jede Replik oder Regieanweisung wird pro Annotator*in diejenige Annotation markiert, die am meisten (gemessen an der Zahl an annotierten Token) markiert wurde. Keine Annotation pro Replik/Regieanweisung wird als zusätzliche Klasse markiert und dann replikenweise Übereinstimmungen kalkuliert (vgl. Abbildung 2). Zur Interpretation von Cohen""s Œ∫ werden im Folgenden in Klammern die Wertebereiche für einzelne Intervalle gemäß Landis und Koch (1977) mitangegeben. Im Schnitt kann man für die Polarität eine moderate Übereinstimmung (laut Landis und Koch gilt moderat für 0,4<Œ∫<=0,6) und für die anderen Kategorien eine schwache Übereinstimmung (0,1<Œ∫<= 0,4) feststellen. Im Vergleich zu anderen Textsorten ist dies eine geringe Übereinstimmung (Wood et al. 2018a; 2018b), die jedoch vergleichbar mit anderen Sentiment- und Emotionsannotationsprojekten mit literarischen und/oder historischen Texten ist (Alm / Sproat 2005; Sprugnoli et al. 2016; Schmidt et al. 2018b; Schmidt et al. 2019b; 2019d). Mehr Erläuterungen und Ergebnisse zur Annotation findet man bei Schmidt et al. (2021c). Im Folgenden werden die Ergebnisse für denjenigen Fall präsentiert, bei dem als Trainings- und Evaluationsmaterial (""Goldstandard"") alle Annotationen des obigen Annotationskorpus"" verwendet werden (also je zwei Annotationssätze pro Drama). Dadurch liegt folgende Besonderheit vor: Eindeutige und partielle Annotationswidersprüche werden nicht aufgelöst, sondern dem Modell mit als Trainingsmaterial übergeben. Je nach kategorialem System gibt es eine unterschiedliche Menge an partiellen und absoluten Widersprüchen (ca. 16% für Polarität, 14% für Dreifach-Polarität, 28% für Hauptklassen, 47% für Sub-Emotionen). Dieses Verfahren wurde dennoch gewählt, da aufgrund der variablen Annotationspraxis die Auflösung eindeutiger Annotationswidersprüche schwerfällt (siehe Kapitel Wir definieren die Emotionsklassifikation als single-label-Klassifikationsaufgabe für Textsequenzen variabler Länge für folgende Klassengruppen: Alle Verfahren wurden in Obschon die Leistung lexikonbasierter Sentiment-Analyse meist von Wir evaluieren zudem zwei klassische Methoden des maschinellen Lernens: (1) Repräsentation über Termfrequenzen in einem bag-of-words-Modell und dem Lern-Algorithmus Statische Sprachmodelle repräsentieren Wörter als Vektoren in Vektorräumen, so dass geometrische Verhältnisse der jeweiligen Semantik entsprechen. Diese Repräsentationen ( Als transformerbasierte Sprachmodelle werden dynamische Für die Klassifikationsaufgabe werden die Modelle in einem ""Fine-Tuning""-Schritt mit dem Goldstandard trainiert. Für die konkrete Implementierung folgen wir den jeweiligen Empfehlungen für die gewählte Architektur (Devlin et al. 2018; Clark et al. 2020) Die Performanz von Klassifikations-Aufgaben kann verbessert werden, indem Texte der gleichen Domäne zum Vortraining von transformerbasierten Modellen genutzt werden (siehe Rietzler et al. 2020; Gururangan et al. 2020). Man kann entweder (1) selbst ein Modell von Grund auf mit domänennahen Texten erstellen oder (2) Modelle zeitgenössischer Sprache mit domänenspezifischen historischen Texten nachtrainieren. Beide Methoden wurden bereits erfolgreich im Kontext deutscher, historischer Sprache angewendet (Labusch et al. 2019; Schweter / Baiter 2019; Schweter / März 2020; Brunner et al. 2020). Auch hier evaluieren wir etablierte vortrainierte Modelle, die über die Hauptmetrik zur Interpretation der Ergebnisse ist die Alle gewählten Methoden übertreffen in den einzelnen Settings die Obschon die Menge an annotiertem Material im Vergleich zu Studien auf der Basis anderer Textsorten limitiert ist, konnten wir erste Erkenntnisse für die Optimierung computergestützter Methoden sammeln. Für Polarität und Dreifach-Polarität erreichen die besten Modelle in ihren Default-Settings bereits Ergebnisse, die durchaus vergleichbar sind mit state-of-the-art-Resultaten für Sentiment- und Emotionsklassifikation in anderen Bereichen (Yang et al. 2019; Munikar et al. 2019; Cao et al. 2020; Dang et al. 2020). Die besten Ergebnisse erzielen grundsätzlich die derzeit größten transformerbasierten Modelle für die deutsche Sprache. Die Optimierung für historische oder poetische Sprache hat lediglich geringfügige Verbesserungen gegenüber den äquivalenten kontemporären Modellen aufgezeigt. Ein Grund dafür ist möglicherweise, dass die gewählten historischen Modelle noch zu viele Texte aus dem 19. und 20. Jahrhundert enthalten, die doch zu weit entfernt von unserer Zeitepoche sind. Wir befinden uns momentan im Prozess der Akquise großer Textmengen aus dem entsprechenden Zeitraum, um vortrainierte Modelle zu evaluieren, die noch stärker an unsere Domäne angepasst sind. Für die mehrklassigen Kategoriensysteme können keine zufriedenstellenden Ergebnisse erzielt werden. Dies ist ohne größere Optimierung für derartige Klassifikationsverfahren nicht ungewöhnlich. Wir planen sowohl die Anwendung verschiedener empfohlener Verfahren, um mit dem Klassenungleichgewicht umzugehen (Buda et al. 2018) und die Optimierung von Hyperparametern als auch die Exploration des Einsatzes einer neutralen ""Nicht-annotiert""-Klasse. Im Bereich der Annotation soll eine Expertenannotation eingefügt werden, welche die Entscheidungen der ersten beiden Annotationen berücksichtigt, aber eine eigenständig verwendbare, widerspruchsfreie Annotationsschicht darstellt. Evaluationsergebnisse mittels der Anwendung von manuellen Widerspruchsauflösungen findet man bei Schmidt et al. (2021b). Wir lassen derzeit weitere Texte annotieren und explorieren historische",de,transformerbasiern Sprachmodelle bert devlin et Electra Clark et gelten Ausgangspunkt zahlreich Aufgabe natural language Processing nlp Shmueli ku Munikar et cao et Dang et et Cortiz entscheidend Vorteil Modell dynamisch Repräsentation Token Abhängigkeit Kontext herausstellen Großteil Modell zeitgenössisch Sprache Fachtext Web Digital Humanitie dh Einteilung Text eher positiv negativ konnotieren Emotionsklassifikation Erkennung Zuordnung distinkt emotionskonzepen Text letzter populär verwenden modern Textsort Songtext schmidt et filmtexte Schmidt et Text sozial Medium analysieren Moßburger et schmidt et finden Einsatz literarisch Genre beispielsweise märch alm sproat mohammad Roman Kakkon Kakkon Mohammad et Reagan et zehen et dramen mohammad Schmidt Burghardt schmidt et Schmidt schmidt et Yavuz schmidt et ziele variieren Exploration emotionsverläufen einzeln Werk gruppenvergleichen sehen Kim Klinger steigend Popularität überraschend hermeneutisch Analyse Emotion Tradition Literaturwissenschaft Dramenanalyse Pikulik wiegmann anz schonlau folgend Proposal präsentieren Studie Evaluation Training Algorithm Goldstandard unser Gesamtkorpus annotieren Emotion definieren Bewusstseinszustand Figur Text ausdrücken annotiert zugeschrieben Emotion Figur Abhängigkeit Kontext Interpretation Schema heben üblich Schemata meist Psychologie inspirieren Wood et literarisch Interesse integrieren bestehen Sonderfall schemas analysierend Hauptkorpus unser Gesamtprojekt setzen unterschiedlich Dramenkollektion Textgrid Annotation Tool Catma Gius et verwenden Dram vollständig Anfang annotiert lektüre gesamt Dramas notwendig kontextabhängig annotiert studentisch Hilfskräfte jeder Werk unabhängig voneinander annotiert hilfskräfte Annotation mittels Pilotstudi Expertenannotatorin trainieren Zugriff Annotationsanleitung Länge Text Woche pro Drama Goldstandard bestehen insgesamt Emotionsannotation Abbildung polaritätsebener meister Annotation negativ positiv Klasse emotional Bewegtheit markieren Kategorie Lust Freundschaft selten markieren verdeutlichen Varianz Annotationsläng Schnitt bestehen Annotation Token Kategorie texteinhein variabl Länge überlappend Texteinheit annotiert Berechnung übereinstimmungsmetriken Festlegung Texteinheit treffen folgend Heuristik anwenden Replik Regieanweisung pro Annotation markieren meister messen Zahl Annotierte Tok markieren Annotation pro Replik Regieanweisung zusätzlich Klasse markieren replikenweise übereinstimmungen kalkulieren Abbildung Interpretation coh -- folgend klammern wertebereich einzeln intervalle gemäß Landi koch mitangegeben Schnitt Polarität moderat Übereinstimmung laut Landis koch gelten Moderat kategorien schwach Übereinstimmung feststellen Vergleich Textsort gering übereinstimmung Wood et vergleichbar Emotionsannotationsprojekt literarisch historisch Text alm sproat Sprugnoli et schmidt et Schmidt et erläuterung Ergebnis Annotation finden Schmidt et folgend Ergebnis derjenige Fall präsentieren Evaluationsmaterial goldstandard annotatio obig Annotationskorpus verwenden Annotationssätz pro Drama liegen folgend Besonderheit eindeutig Partielle annotationswidersprüch auflösen Modell Trainingsmaterial übergeben kategorial System unterschiedlich Menge Partielle absolut widersprüch Polarität Hauptklassen Verfahren dennoch wählen aufgrund variabl Annotationspraxis Auflösung eindeutig annotationswidersprüch Schwerfällt sehen Kapitel definieren Emotionsklassifikation textsequenzen variabl Länge folgend Klassengruppe Verfahren obschon Leistung Lexikonbasierter meist evaluieren zudem klassisch Methode maschinell lernens Repräsentation Termfrequenze statisch Sprachmodelle repräsentieren Wörter Vektor Vektorräum geometrisch Verhältnis jeweilig Semantik entsprechen Repräsentation transformerbasiern Sprachmodelle Dynamische Klassifikationsaufgabe Modell Goldstandard trainieren konkret Implementierung folgen jeweilig Empfehlung gewählt Architektur devlin et Clark et Performanz verbessern Text gleich Domäne Vortraining Transformerbasiert Modellen nutzen sehen Rietzler et gururangan et Modell Grund domänennah Text Erstell Modell zeitgenössisch Sprache domänenspezifisch historisch Text nachtrainieren Methode erfolgreich Kontext deutsch historisch Sprache anwenden labusch et Schweter Baiter Schweter März Brunner et evaluieren etabliert vortrainiert Modell Hauptmetrik Interpretation Ergebnis gewählt Methode übertreffen einzeln Setting obschon Menge annotiert Material Vergleich Studie Basis anderer Textsort limitieren erkenntnis Optimierung Computergestützter Methode sammeln Polarität erreichen Modell Ergebnis vergleichbar Emotionsklassifikation Bereich Yang et Munikar et cao et Dang et Ergebnis erzielen grundsätzlich derzeit groß transformerbasiert Modell deutsch Sprache Optimierung historisch poetisch Sprache lediglich geringfügig Verbesserung äquivalenten kontemporären modellen aufzeigen Grund möglicherweise gewählt historisch Modell Text Jahrhundert enthalten entfernt zeitepocher befinden momentan Prozess Akquise Textmeng entsprechend Zeitraum vortrainiert Modell evaluieren stark Domäne angepasst mehrklassig Kategoriensystem zufriedenstellend Ergebnis erzielen groß Optimierung derartig Klassifikationsverfahren ungewöhnlich planen sowohl Anwendung verschieden empfohlen Verfahren Klassenungleichgewicht umgehen buda et Optimierung Hyperparameter Exploration einsatzes neutral Bereich Annotation Expertenannotation einfügen Entscheidung Annotation berücksichtigen eigenständig verwendbar widerspruchsfrei Annotationsschicht darstellen Evaluationsergebnis mittels Anwendung Manuelle Widerspruchsauflösung finden Schmidt et lassen derzeit Text annotieren explori historisch,"[('et', 0.382021013740462), ('schmidt', 0.35276114449338813), ('modell', 0.1856733111658884), ('annotation', 0.11922751611251187), ('emotionsklassifikation', 0.11542096012561058), ('optimierung', 0.11515557534934567), ('goldstandard', 0.11084794110197062), ('markieren', 0.10968372439273111), ('variabl', 0.10471495827483897), ('mohammad', 0.10471495827483897)]"
2022,DHd2022,RÖTTGERMANN_Julia_Literaturgeschichtsschreibung_datenbasiert.xml,"Literaturgeschichtsschreibung datenbasiert und wikifiziert?   Automatische Extraktion thematischer Statements aus französischen Primärtexten mithilfe von Topic Modeling, RDF und eines kontrollierten Vokabulars in LOD","Julia Röttgermann (Universität Trier, Germany); Anne Klee (Universität Trier, Germany); Maria Hinzmann (Universität Trier, Germany); Christof Schöch (Universität Trier, Germany)","TEI/XML, Metadaten, Topic Modeling, Linked Open Data, Literaturgeschichte","Inhaltsanalyse, Modellierung, Bibliographie, Literatur, Metadaten","Welche Formalisierungs- und Modellierungsarbeit ist nötig, um Kulturen des kollektiven Gedächtnisses wie die Literaturgeschichtsschreibung als Daten abfragbar zur Verfügung zu stellen? Wir sehen aktuell einige Umbrüche in den Strategien der Gedächtnisinstitutionen, die sich zunehmend dem ""Linked Open Data""-Paradigma verpflichtet sehen. Anhand von drei verschiedenen Informationsquellen (Primärliteratur, Sekundärliteratur und bibliographische Daten) werden literaturhistoriographische Aussagen extrahiert und in Form eines Wissensnetzwerks modelliert, das die heterogenen Daten integriert und über einen SPARQL-Endpunkt abfragbar macht. Das interdisziplinäre Projekt vereint informationswissenschaftliche, juristische, literaturwissenschaftliche und computerlinguistische Expertise. Den Prinzipien der Open Science verbunden, wurde eine Infrastruktur aufgebaut, die freie Software wie Dies soll zunächst konkret anhand der Extraktion thematischer Aussagen mittels Topic Modeling (1.) sowie der Extraktion von Themenaussagen aus bibliographischen Daten (2.) veranschaulicht werden. Die Relevanz eines kontrollierten Vokabulars im Sinne der Vergleichbarkeit thematischer Aussagewerte unterschiedlicher Informationsquellen (3.) in einem über SPARQL abfragbaren LOD-Wissensnetzwerk (4.) wird im Anschluss dargelegt. Das kontrollierte Vokabular resultiert dabei aus einem Grundstock zeitgeschichtlich relevanter Themen, erweitert um Themenkonzepte, die sich aus der Informationsextraktion der drei Datenquellen ergeben. Als Datengrundlage zur Modellierung von literaturhistorisch relevanten Aussagen dienen uns drei Kategorien an Texten: Primärliteratur (Romane), Sekundärliteratur (Fachliteratur) und bibliographische Quellen. Die erste der drei Informationsquellen besteht aus einem Korpus aus französischen Romanen der zweiten Hälfte des 18. Jahrhunderts (Röttgermann 2021). Dieses umfasst derzeit 115 Texte und wird laufend durch Volltextdigitalisierung mit dem auf historische Drucke spezialisierten OCR-Tool Alle Input-Dateien wurden in TEI-konformes XML nach den Richtlinien der Text Encoding Initiative (vgl. Burnard 2014) nach dem Schema der Mithilfe von Topic Modeling (vgl. Blei 2011) ist es möglich, ""Topics"" aus den Primärtexten zu extrahieren, die Aufschluss über Themen-Cluster innerhalb des Korpus geben können. Die Methode generiert auf der Grundlage der Kookkurrenz von Wörtern Gruppen semantisch verwandter Wörtern, die Topics. Hier wurden mit Mallet (vgl. McCallum 2002) zunächst 30 Topics generiert, welchen im Anschluss Label zugewiesen wurden. Diese wiederum wurden mit Konzepten aus einem Themenvokabular verknüpft. Die gewonnenen Topics verteilen sich folgendermaßen über das Korpus (s. Abb. 1): Neben einigen Topic Modeling Artefakten wie [temps para√Ætre encore] haben sich Topics im Romankorpus herauskristallisiert, die auf literarische Gattungen des 18. Jahrhunderts wie Briefroman (Topic ""correspondance"") oder Reiseroman (Topic ""migration_voyage"") oder auf häufig thematisch verhandelte Konzepte wie Philosophie (Topic ""philosophie"") oder Erziehung (Topic ""éducation_enfance"") hinweisen. Für das Werk Candide (1759) von Voltaire konnte beispielsweise ein Topic extrahiert werden, das mit dem Label ""migration_voyage"" versehen wurde (Abb. 2). Aus diesem Einzelergebnis leiten wir folgendes angedeutetes Statement über das Werk Candide ab: [Candide] ABOUT [label @en: travel | label @de: Reise | label@fr: voyage]. Ein Mapping der Entitäten auf Wikidata ergibt daraufhin dieses hier in menschenlesbarer Form angedeutete Statement: Q215894 [label @fr: Candide] ABOUT Q61509 [label @en: travel | label @de: Reise | label@fr: voyage]. Für den Piloten wurden für das Romankorpus [Stand: 92 Romane] Bei der zweiten Informationsquelle handelt es sich um bibliographische Nachweissysteme zur französischen Literatur 1751-1800 (s. Abb. 3). Im Fokus steht Die Bibliographie bietet in Kombination mit den Ergebnissen des Topic Modelings die Möglichkeit eines Mensch-Maschine-Vergleichs 'wurden die enthaltenen thematischen Schlagworte doch in den 1970er Jahren durch Lektüre und Zusammentragen von Informationen aus anderen Nachschlagewerken erhoben. Die Bibliographie wurde in mehreren Arbeitsschritten aufwendig erschlossen. [Les enfans de la nature] ABOUT [sentiment | Gefühl | sentiment] [Les enfans de la nature] ABOUT [pedagogy | Pädagogik | pédagogie] [Les enfans de la nature] ABOUT [philosophy | Philosophie | philosophie]. Technisch unterscheiden sich die RDF-Triple zu Themen je nach Datenquelle nicht, werden jedoch entsprechend ihrer Herkunft in Wikibase mit der Property In den bibliographischen Daten sind insgesamt knapp 2700 Items (Veröffentlichungen fiktionaler Prosa in französischer Sprache inklusive Übersetzungen) enthalten, von denen 349 das thematische Schlagwort ""voyage"" enthalten. Wie lassen sich die thematischen Muster in der Primärliteratur mit den Daten aus den bibliographischen Nachweissystemen vergleichen? Ein wichtiger Modellierungsschritt ist zunächst das Erstellen eines kontrollierten Vokabulars aus thematischen Konzepten der französischen Aufklärung, auf das die Ergebnisse des Topic Modeling und die Bibliographie-Schlagworte gemappt werden. Das Vokabular der Themenbegriffe besitzt eine hohe Relevanz für mehrere Teilprojekte: Es stellt zum einen die Labelbegriffe für die Topics aus dem Topic Model bereit, liefert daneben aber auch die Konzept-Items für die Objektposition solcher thematischer Statements, die aus der Sekundärliteratur und der Bibliographie extrahiert wurden. An das Vokabular sind somit mehrere Anforderungen geknüpft: Die Begriffe müssen die Themenkonzepte der französischen Aufklärung abdecken, sollen ein gewisses Abstraktionslevel aufweisen, damit sie als kategorische Begriffe fungieren können und die Zusammenstellung der Begriffe sollte transparent und nachvollziehbar sein. Eine erste Grundlage bildet das Themeninventar des Um die multilinguale Vergleichbarkeit zwischen französischsprachigen Primärtexten und deutschsprachiger Sekundärliteratur zu gewährleisten, und im Sinne der Anschlussfähigkeit an und Interoperabilität mit anderen Datenbeständen, werden die Themenkonzepte auf einen Normdatensatz (Wikidata) gemappt, wodurch das kontrollierte Vokabular konsolidiert und multilingual erfasst ist (siehe Abb. 4). Ziel ist es, eine Vielzahl an Statements zu aggregieren und die Items auf vielfältige Weise miteinander in Beziehung zu setzen, sodass durch zunehmende Skalierung der Daten aus einzelnen Subjekt-Prädikat-Objekt Aussagen in RDF (Resource Description Framework) ein dichter ""Knowledge Graph"" (vgl. zum Begriff: Ehrlinger/Wöß 2016) entsteht (s. Abb. 5). Dieser Graph lässt sich sodann auch über einen SPARQL-Endpoint abfragen (s. Abb.6). Ausgehend von der Beobachtung, dass das Themenkonzept ""Reise"" in den Bibliographie-Daten bei immerhin 14,7 % der Einträge vermerkt ist, ließe sich beispielsweise fragen, in welchen Werken auch laut Topic Modeling das mit dem Themenkonzept ""Reise"" verbundene Topic als dominantes Topic vorkommt. Durch Topic Modeling wurden dabei sowohl Werke identifiziert, die auch laut Bibliographie-Daten die Themenkategorie ""Voyages"" enthalten, als auch solche, in denen dieses Schlagwort nicht vorkam. Für das Werk Für das Werk SPARQL-Abfragen zu den Ergebnissen des Topic Modelings und/oder der bibliographischen Schlagworten ermöglichen es, auch weniger bekannte Werke zu spezifischen Themen zu ermitteln. Zudem zeichnen sich Muster an Themenkomplexen im Zeitverlauf ab. Für das Thema ""voyage"" innerhalb der bibliographischen Daten zeigt sich eine (auch statistisch signifikante) ansteigende Entwicklung (vgl. Abb. 8). Eine Erklärung für den Anstieg der Themenkategorie ""voyage"" in den 1780er und 1790er Jahren könnte sein, dass viele Autor:innen die Handlung ihrer Werke aus politischen Gründen in andere Länder ""verlegen"" und zudem, dass der Themenkomplex der Reise im Kontext von Emigration im Zuge der politischen Ereignisse zunehmend in Romanen verhandelt wird. Beispiele hierfür wären Insgesamt ist das Thema ""Reise"" derzeit laut Topic Modeling im Romankorpus in 14,13% der Werke als dominantes Topic vertreten, in den bibliographischen Daten in 14,74% der Werke. Ein makrostruktureller Blick zeigt demnach in diesem Beispiel eine vergleichbare Größenordnung der thematischen Aussage über das gesamte Korpus hinweg, auch wenn in der Bewertung der Einzelwerke nicht immer Kongruenz besteht. Das Projekt MiMoText modelliert die Geschichte des französischen Romans der zweiten Hälfte des 18. Jahrhunderts in Form von RDF-Tripeln in Wikibase als Knowledge Graph. Im Zuge eines Pilotprojekts wurden in einem ersten Schritt aus bibliographischen Daten und aus einem Romankorpus Relationen zwischen Werken und Themen extrahiert, die in Form von Tripeln in eine eigene Wikibase-Instanz eingelesen wurden. Zur Modellierung der Themen des französischen Romans der Aufklärung wurde ein kontrolliertes Vokabular erstellt, welches auf Wikidata gemappt wurde, um anschlussfähig an die Linked Open Data Cloud zu sein. Die Ergebnisse der Informationsextraktion aus den Romanen (mithilfe von Topic Modeling) und der Informationsextraktion aus bibliographischen Daten können nun per SPARQL-Endpoint abgefragt werden. Zu den nächsten Schritten gehört neben der Extraktion weiterer Statements über quantitative Romananalysen der Import von Themen-Statements aus dem dritten Typus von Informationsquellen (Fachliteratur) in unsere Wikibase-Instanz.",de,Modellierungsarbeit nötig Kultur kollektiv Gedächtnisse Literaturgeschichtsschreibung daten abfragbar Verfügung stellen sehen aktuell umbrüche Strategie gedächtnisinstitution zunehmend linked op verpflichten sehen anhand verschieden informationsquelle Primärliteratur Sekundärliteratur bibliographisch daten literaturhistoriographisch Aussage extrahiern Form Wissensnetzwerks modellieren heterogen daten integrieren abfragbar interdisziplinär Projekt vereinen informationswissenschaftlich juristisch literaturwissenschaftlich computerlinguistisch Expertis prinzipien op Science verbinden infrastruktur aufbauen frei Software konkret anhand Extraktion Thematischer Aussage mittels Topic Modeling Extraktion Themenaussag bibliographisch daten veranschaulichen Relevanz kontrolliert Vokabular Sinn Vergleichbarkeit Thematischer Aussagewert unterschiedlich informationsquelle sparql abfragbar Anschluss darlegen Kontrollierte Vokabular resultieren Grundstock zeitgeschichtlich relevant Thema erweitern Themenkonzept Informationsextraktion Datenquell ergeben Datengrundlage Modellierung literaturhistorisch relevant Aussage dienen Kategorie Text Primärliteratur Roman Sekundärliteratur Fachliteratur bibliographisch quellen informationsquelle bestehen Korpus französisch Roman Hälfte Jahrhundert Röttgermann umfassen derzeit Text laufend Volltextdigitalisierung historisch Druck spezialisiert xml Richtlinie Text encoding Initiative Burnard Schema Mithilfe Topic Modeling blei Topics Primärtext extrahieren aufschluss innerhalb Korpus geben Methode neriern Grundlage Kookkurrenz wörtern Gruppe semantisch verwandt wörtern Topics Mallet mccallum Topic neriern Anschluss label zuweisen wiederum Konzept Themenvokabular verknüpfen gewonnen Topics verteilen folgendermaßen korpus abb Topic Modeling artefakten temps encore Topic Romankorpus herauskristallisieren literarisch Gattung Jahrhundert briefroman Topic Correspondance Reiseroman Topic häufig thematisch verhandelt Konzept Philosophie Topic Philosophie Erziehung Topic hinweisen Werk Candide Voltaire beispielsweise Topic extrahieren Label versehen abb Einzelergebnis leiten folgend angedeutet Statement Werk Candide candide about label travel label reise Voyage Mapping Entität Wikidata ergeben daraufhin Menschenlesbarer Form angedeuteen Statement label Candide about label travel label reise Voyage Pilot Romankorpus stehen Roman informationsquelle handeln bibliographisch nachweissysteme französisch Literatur abb Fokus stehen Bibliographie bieten Kombination Ergebnis Topic modelings Möglichkeit enthalten thematisch Schlagwort lektür zusammentragen Information Nachschlagewerk erheben Bibliographie mehrere arbeitsschritter aufwendig erschließen ler enfans -- -- nature about Sentiment Gefühl Sentiment ler enfans -- -- nature about Pedagogy Pädagogik Pédagogie ler enfans -- -- nature about philosophy Philosophie Philosophie technisch unterscheiden Them Datenquelle entsprechend herkunft Wikibase Property bibliographisch daten insgesamt knapp item veröffentlichung Fiktionaler Prosa französisch Sprache inklusive übersetzung enthalten thematisch Schlagwort Voyage enthalten lassen thematisch Muster Primärliteratur daten Bibliographisch Nachweissystemen vergleichen wichtig Modellierungsschritt Erstellen kontrolliert Vokabular thematisch Konzept französisch Aufklärung Ergebnis Topic Modeling mappen Vokabular Themenbegriffe besitzen hoch Relevanz mehrere teilprojekte stellen Labelbegriffe Topics Topic Model bereit liefern Objektposition Thematischer Statements Sekundärliteratur Bibliographie extrahiern vokabular somit mehrere Anforderung knüpfen begriffe Themenkonzept französisch Aufklärung abdecken gewiß Abstraktionslevel aufweisen kategorisch begriffe fungieren Zusammenstellung begriffe Transparent nachvollziehbar Grundlage bilden Themeninventar multilingual Vergleichbarkeit französischsprachig Primärtext deutschsprachig Sekundärliteratur gewährleisten Sinn Anschlussfähigkeit Interoperabilität datenbeständer Themenkonzept Normdatensatz Wikidata mappen wodurch Kontrollierte Vokabular konsolidieren Multilingual erfassen sehen abb Ziel Vielzahl Statement aggregieren item vielfältig Weise miteinander Beziehung setzen sodass zunehmend Skalierung daten einzeln aussagen Rdf resource description Framework Dichter knowledg Graph Begriff Ehrlinger Wöß entstehen abb Graph lässen Sodann abfrag ausgehend Beobachtung Themenkonzept reise immerhin eintrag vermerken lassen beispielsweise fragen Werk laut Topic Modeling Themenkonzept reise verbunden Topic dominantes Topic vorkommen Topic Modeling sowohl Werk identifizieren laut Themenkategorie voyag enthalten Schlagwort vorkommen Werk Werk Ergebnis topic Modeling Bibliographisch Schlagwort ermöglichen bekannt Werk spezifisch Thema ermitteln zudem zeichnen Muster Themenkomplexe zeitverlauf Thema Voyage innerhalb bibliographisch daten zeigen statistisch signifikant ansteigend Entwicklung abb Erklärung Anstieg Themenkategorie Voyage Autor innen Handlung Werk politisch Grund Land verlegen zudem Themenkomplex Reise Kontext Emigration Zug politisch Ereignis zunehmend Roman verhandeln Beispiel hierfür sein insgesamt Thema reise derzeit laut Topic Modeling Romankorpus Werk dominantes Topic vertreten bibliographisch daten Werk makrostrukturell Blick zeigen demnach vergleichbar Größenordnung thematisch Aussage gesamt korpus hinweg Bewertung einzelwerk Kongruenz bestehen Projekt Mimotext modellieren Geschichte französisch Roman Hälfte Jahrhundert Form Wikibase Knowledge Graph Zug Pilotprojekt Schritt bibliographisch daten Romankorpus relationen Werk Thema extrahieren Form tripeln eingelesen Modellierung Thema französisch Roman Aufklärung kontrolliert Vokabular erstellen Wikidata mappen anschlussfähig Linked open Data Cloud Ergebnis Informationsextraktion Roman Mithilfe Topic Modeling Informationsextraktion Bibliographisch daten per abfragen nächster Schritt gehören Extraktion weit Statement quantitativ Romananalyse Import Typus informationsquelle Fachliteratur,"[('topic', 0.37985417432141527), ('bibliographisch', 0.2923198503884928), ('modeling', 0.1759305272949865), ('reise', 0.17096023776934405), ('about', 0.1685940772772066), ('themenkonzept', 0.1685940772772066), ('voyage', 0.1685940772772066), ('informationsquelle', 0.15703260157962934), ('label', 0.15302092053937547), ('vokabular', 0.1506008007586471)]"
2022,DHd2022,DU_Keli_Kontrastive_Textanalyse_mit_pydistinto___Ein_Python_.xml,Kontrastive Textanalyse mit pydistinto Ein Python-Paket zur Nutzung unterschiedlicher Distinktivitätsmaße,"Keli Du (Universität Trier, Germany); Julia Dudar (Universität Trier, Germany); Cora Rok (Universität Trier, Germany); Christof Schöch (Universität Trier, Germany)","Computational Literary Studies, Distinktivitätsmaße, Python-Implementierung, pydistinto","Entdeckung, Programmierung, Stilistische Analyse, Methoden, Software, Text","   Digital Humanities im deutschsprachigen Raum 2022 Kontrastive Textanalyse mit pydistinto Ein Python-Paket zur Nutzung unterschiedlicher Distinktivitätsmaße Du, Keli duk@uni-trier.de Universität Trier, Germany Dudar, Julia dudar@uni-trier.de Universität Trier, Germany Rok, Cora rok@uni-trier.de Universität Trier, Germany Schöch, Christof schoech@uni-trier.de Universität Trier, Germany Viele Wissenschaftsbereiche, die sich mit der quantitativen Textanalyse beschäftigen, wie die Korpuslinguistik oder die Computational Literary Studies (CLS) setzen verschiedene statistische Distinktivitätsmaße ein, um Elemente (z.B. Wortformen oder Wortarten) zu bestimmen, die charakteristisch für eine Textgruppe im Vergleich mit einer anderen Textgruppe sind. Tools wie z.B. WordSmith (Scott 2020) oder AntConc (Anthony 2005), die solche Analysen ermöglichen, sind weit verbreitet, haben jedoch einige Nachteile: Die meisten bieten nur häufigkeitsbasierte Maße (z.B. Log-Likelihood-Ratio Test oder Chi-Squared Test) an, die in vielen Fällen Ergebnisse produzieren, die für die kontrastive (explorative) Textanalyse nicht hilfreich sind (siehe u.a. Baker 2004 und Johnson and Ensslin 2006). Dispersionsmaße wie z.B. DP (Gries 2008) oder dispersionsbasierte Distinktivitätsmaße wie z.B. Zeta (Burrows 2007), die besser interpretierbaren Ergebnisse liefern (siehe Gries 2021; Schöch 2018), werden dagegen nicht implementiert. Eine Ausnahme bildet stylo , das Zeta implementiert (Eder et al. 2016). Ein weiterer Nachteil ist, dass bei den meisten Tools nur ein oder zwei Maße für die Analyse ausgewählt werden können, was einen Vergleich der unterschiedlichen Maße erschwert. Gerade wenn Nutzende ihre Analysen anpassen und eigene Parametereinstellungen vornehmen oder verschiedene Datenformate nutzen wollen, erweisen sich die meisten Tools als ungeeignet. Um den Einsatz relevanter Maße für die kontrastive Textanalyse zu erleichtern und das Bewusstsein für die Vielfalt der Maße zu schärfen, entwickeln wir im Rahmen des Projekts ""Zeta and Company"" ein Python-Paket mit dem Namen pydistinto . 1 Ziel unseres Projekts ist es, zu einem tieferen Verständnis der verschiedenen Distinktivitätsmaße zu gelangen und Verbesserungen für deren Implementierung und Anwendung vorzuschlagen. Mithilfe von pydistinto können zwei Textkorpora mit unterschiedlichen Maßen verglichen werden. Hierfür haben wir zunächst ein konzeptionelles Framework erstellt, auf dessen Basis die Maße in pydistinto implementiert werden (Du et al. 2021a). Das Framework definiert die Bereiche Preprocessing, Berechnung von Häufigkeiten, Korpusaufteilung sowie der eigentlichen Berechnung der Distinktivitätswerte, Visualisierung sowie quantitative und qualitative Evaluation der Ergebnisse. In der Implementierung umfasst das Preprocessing die Tokenisierung, Lemmatisierung und das POS-Tagging der Texte. Danach werden die Texte je nach Parameter entweder segmentiert (dies wird bei der Berechnung von dispersionsbasierten Maßen empfohlen) oder als ganze Dokumente belassen. Die (absoluten, binären, relativen usw.) Worthäufigkeiten in den Segmenten bzw. Dokumenten werden in einer Matrix zusammengefasst. Als Nächstes werden die Segmente bzw. Dokumente in zwei Gruppen, ein Zielund ein Vergleichskorpus, aufgeteilt. Anschließend werden die Distinktivitätswerte auf Basis der Worthäufigkeits-Matrizen berechnet und die distinktiven Wörter für das Zielkorpus visualisiert. Die Implementierung des Moduls zur quantitativen Evaluation steht noch aus. Geplant ist hier, die statistischen Eigenschaften der Wortlisten zu analysieren und die Korrelation verschiedener Maße zu untersuchen (siehe, für Zwischenergebnisse, Du et al. 2021c). Bei der qualitativen Evaluation werden die ausgegebenen Wörter manuell interpretiert und ihre Relevanz für das Zielkorpus wird beurteilt. Das Python-Paket wird auf Github veröffentlicht und steht somit zur freien Nutzung, eigenen Anpassung und weiteren Entwicklung zur Verfügung (Du et al. 2021b). Im pydistinto sind derzeit folgende Distinktivitätsmaße implementiert: Zeta, Ratio of Relative Frequencies, Gris"" Deviation of Proportions based measure (Eta, siehe Du et al. 2021c), Welch's T-test, Wilcoxon Rank-sum Test, Kullback-Leibler Divergence, Chi-Squared Test, Log-Likelihood-Ratio Test, TF-IDF. Ein besonderer Vorteil des Pakets ist, dass es in einem Beginner-Modus und einem ProfiModus genutzt werden kann. Im Beginner-Modus können auch weniger erfahrene Nutzende mit geringen Programmierund Statistikkenntnissen Textkorpora vergleichen. Zielund Vergleichskorpus müssen hierfür lediglich als ""plain text"" vorbereitet und einige Parameter wie z. B. Segmentlänge, Feature-Typen oder Anzahl der Top-Features eingestellt werden. Die Analyse wird dann automatisch durchgeführt und eine Visualisierung angeboten. Wer sich für die statistischen Eigenschaften der unterschiedlichen Maße interessiert und diese vergleichen möchte, kann den Profi-Modus verwenden. Die Nutzenden können dann selbst darüber bestimmen, welche Maße und statistischen Eigenschaften der Features (z.B. absolute Häufigkeit, relative Häufigkeit, Dispersion) für die Berechnung der Distinktivität kombiniert werden sollen. Es gibt in diesem Modus außerdem zusätzliche Möglichkeiten, die Daten zu visualisieren: so kann die Abhängigkeitsstruktur zweier statistischer Merkmale (z.B. Zeta-Wert und absolute Häufigkeiten der Features) auch durch ein Streudiagramm dargestellt werden. Durch die Entwicklung des Pakets möchten wir auf der einen Seite eine reflektierte Nutzung statistischer Distinktivitätsmaße für die kontrastive Textanalyse erleichtern. Auf der anderen Seite soll das Paket ermöglichen, die Eigenschaften und Leistungsfähigkeit der Maße empirisch zu ermitteln und systematisch zu vergleichen. Fußnoten 1. Das Projekt gehört zum DFG-geförderten Schwerpunktprogramm ""Computational Literary Studies"" (SPP 2207) und läuft 1 Digital Humanities im deutschsprachigen Raum 2022 von 2020-2023. Weitere Informationen unter https://zeta-project.eu/de/. Bibliographie Baker, Paul (2004): ""Querying keywords: questions in difference, frequency, and sense in keyword analysis"", in: Journal of English Linguistics 32 (4), pp. 346–59 Du, Keli / Dudar, Julia / Rok, Cora / Schöch, Christof (2021a): Implementation framework of measures of distinctiveness. Zenodo. http://doi.org/10.5281/zenodo.5092328 Du, Keli / Dudar, Julia / Schöch, Christof (2021b): pydistinto. Version 0.1.0. Verfügbar unter: https://github.com/Zeta-and-Company/pydistinto . DOI: https://doi.org/10.5281/zenodo.5094346 . Du, Keli / Dudar, Julia / Rok, Cora / Schöch, Christof (2021c): "" Zeta & Eta: An Exploration and Evaluation of Two Dispersion-based Measures of Distinctiveness"", in: CHR 2021: Computational Humanities Research Conference , November 17'19, 2021, Amsterdam, The Netherlands, https://2021.computational-humanities-research.org/conference/ Eder, Maciej / Rybicki, Jan / Kestemont, Mike (2016): "" Stylometry with R: a package for computational text analysis"", in: R Journal , 8(1): 107-21. https://journal.r-project.org/archive/2016/RJ-2016-007/index.htm l Gries, Stephan (2008): ""Dispersions and adjusted frequencies in corpora"", in: International Journal of Corpus Linguistics, Volume 13(4): 403–437. DOI: https://doi.org/10.1075/ijcl.13.4.02gri Gries, Stephan (2021): ""A New Approach to (Key) Keywords Analysis: Using Frequency, and Now Also Dispersion"", in: Research in Corpus Linguistics, 9, 1–33. DOI: https://doi.org/10.32714/ricl.09.02.02 Johnson, Sally / Ensslin, Astrid (2006): ""Language in the news: some reflections on keyword analysis using WordSmith Tools and the BNC"", in: Leeds Working Papers in Linguistics and Phonetics 11, pp. 96–109. https://www.latl.leeds.ac.uk/wpcontent/uploads/sites/49/2019/05/Johnson-Ensslin_2006.pdf Laurence, Anthony (2005): "" AntConc: A learner and classroom friendly, multi-platform corpus analysis toolkit"", in: Proceedings of IWLeL 2004: An Interactive Workshop on Language eLearning . 7–13. Schöch, Christof (2018): "" Zeta für die kontrastive Analyse literarischer Texte. Theorie, Implementierung, Fallstudie"", in: Bernhart, T., et al. (eds.), Quantitative Ansätze in der Literaturund Geisteswissenschaften. Berlin: de Gruyter, 77'94. https://www.degruyter.com/viewbooktoc/product/479792 . Scott, Mike (2020): WordSmith Tools, Version 8, Stroud: Lexical Analysis Software. 2",de,Digital Humanitie deutschsprachig Raum kontrastiv Textanalyse Pydistinto Nutzung unterschiedlich Distinktivitätsmaße Keli Universität Trier Germany dudar Julia Universität Trier Germany rok Cora Universität Trier Germany schöch Christof Universität Trier Germany wissenschaftsbereich quantitativ Textanalyse beschäftigen Korpuslinguistik Computational literary Studies cls setzen verschieden statistisch Distinktivitätsmaße element wortformen wortaren bestimmen charakteristisch Textgruppe Vergleich Textgruppe Tools wordsmith Scott Antconc Anthony analyse ermöglichen verbreiten Nachteil meister bieten häufigkeitsbasiert Maß Test test Fall Ergebnis produzieren kontrastiv explorativ Textanalyse hilfreich sehen Baker Johnson and ensslin Dispersionsmaß dp gri dispersionsbasiert Distinktivitätsmaße zeta burrows interpretierbar Ergebnis liefern sehen Grie schöch implementieren Ausnahme bilden Stylo Zeta implementieren ed et weit Nachteil meister Tool Maß Analyse auswählen Vergleich unterschiedlich Maß erschweren Nutzend Analyse anpassen Parametereinstellunge vornehmen verschieden Datenformat nutzen erweisen meister Tool ungeeignet Einsatz Relevanter Maß kontrastiv Textanalyse erleichtern Bewusstsein Vielfalt Maß schärfen entwickeln Rahmen Projekt zeta and company Name Pydistinto Ziel unser Projekt tief Verständnis verschieden Distinktivitätsmaße gelangen Verbesserung implementierung Anwendung vorschlagen Mithilfe Pydistinto Textkorpora unterschiedlich maß vergleichen hierfür konzeptionell Framework erstellen Basis Maß Pydistinto implementieren et Framework definieren Bereich Preprocessing Berechnung häufigkeit Korpusaufteilung eigentlich Berechnung distinktivitätswerte Visualisierung quantitativ qualitativ Evaluation Ergebnis Implementierung umfassen Preprocessing Tokenisierung Lemmatisierung Text Text parameter segmentieren Berechnung Dispersionsbasiert maßen empfehlen Dokument belassen absoluten binär relativen worthäufigkeien segmenten dokumenten Matrix zusammengefasst nächster segment dokumenen Gruppe zielund Vergleichskorpus aufteilen anschließend distinktivitätswerte Basis berechnen distinktiv Wörter Zielkorpus visualisiern Implementierung Modul quantitativ Evaluation stehen planen statistisch Eigenschaft Wortlisten analysieren Korrelation verschieden Maß untersuchen sehen Zwischenergebnisse et qualitativ Evaluation ausgegeben Wörter manuell interpretieren Relevanz Zielkorpus beurteilen Github veröffentlichen stehen somit frei Nutzung Anpassung Entwicklung Verfügung et Pydistinto derzeit folgend distinktivitätsmaß implementieren zeta ratio -- relativ frequencies Gris Deviation of proportions based measure eta sehen et Wilcoxon test Divergence Test Test besonderer Vorteil Paket Profimodus nutzen erfahren Nutzend gering Programmierund Statistikkenntnisse Textkorpora vergleichen zielund Vergleichskorpus hierfür lediglich Plain Text vorbereiten parameter Segmentlänge Anzahl einstellen Analyse automatisch durchführen Visualisierung anbieten statistisch Eigenschaft unterschiedlich Maß interessiert vergleichen verwenden Nutzend bestimmen Maß statistisch Eigenschaft Features absolut Häufigkeit relativ Häufigkeit Dispersion Berechnung Distinktivität kombinieren Modus zusätzlich Möglichkeit daten visualisieren Abhängigkeitsstruktur zwei statistisch merkmal absolut häufigkeien Features Streudiagramm darstellen Entwicklung Paket möchten Seite reflektiert Nutzung statistisch Distinktivitätsmaße kontrastiv Textanalyse erleichtern Seite Paket ermöglichen eigenschaft Leistungsfähigkeit Maß empirisch ermitteln systematisch vergleichen fußnot Projekt gehören Schwerpunktprogramm Computational literary studies spp laufen Digital Humanitie deutschsprachig Raum Information Bibliographie Baker Paul querying keywords Questions Difference frequency And sense Keyword Analysis journal -- english linguistics pp Keli dudar Julia Rok Cora schöch Christof Implementation Framework of measures -- distinctiveness Zenodo Keli dudar Julia schöch Christof Pydistinto Version verfügbar doi Keli dudar Julia Rok Cora schöch Christof zeta eta Exploration and evaluation -- two measures -- distinctiveness chr computational humanities research conference November Amsterdam The netherlands Eder maciej Rybicki Jan Kestemont Mike stylometry with r Package for computational Text analysis r Journal l Grie Stephan dispersions and Adjusted frequenceisen Corpora international Journal of Corpus linguistics volum doi Grie Stephan New approach to key Keywords analysis Using frequency And now Dispersion research Corpus Linguistic doi johnson Sally ensslin Astrid language -- -- some reflections -- Keyword Analysis Using Wordsmith Tools and The bnc Leeds working Paper linguistics and Phonetics pp Laurence Anthony antconc Learner And Classroom Friendly Corpus Analysis Toolkit Proceedings of iwlel interactive workshop on language Elearning schöch Christof zeta kontrastiv Analyse literarisch Text Theorie Implementierung Fallstudie Bernhart et EDS quantitativ Ansatz Literaturund geisteswissenschaften Berlin de Gruyter Scott Mike Wordsmith Tools Version stroud Lexical Analysis Software,"[('maß', 0.23049448997951535), ('pydistinto', 0.2226831193980287), ('zeta', 0.1946557256452168), ('distinktivitätsmaße', 0.1758755833196318), ('kontrastiv', 0.1758755833196318), ('and', 0.164609705820564), ('christof', 0.16221310470434733), ('dudar', 0.15938539582424668), ('keli', 0.1484554129320191), ('schöch', 0.13860093813959307)]"
2022,DHd2022,DU_Keli_Evaluating_Hyperparameter_Alpha_of_LDA_Topic_Modelin.xml,Evaluating Hyperparameter Alpha of LDA Topic Modeling,"Keli Du (Universität Trier, Germany)","LDA Topic Modeling, Evaluation, Hyperparameter Alpha","Inhaltsanalyse, Modellierung, Methoden, Text","   Digital Humanities im deutschsprachigen Raum 2022 Evaluating Hyperparameter Alpha of LDA Topic Modeling derstand the influence of hyperparameter Alpha from two perspectives: topic modeling based single-label document classification and topic coherence, representing the quality of the topic model and the quality of the topics, respectively. Method Du, Keli duk@uni-trier.de Universität Trier, Germany Introduction As a quantitative text analysis method, Latent Dirichlet Allocation (LDA), also often referred to as topic modeling (Blei 2012), has been widely used in Digital Humanities in recent years to explore numerous unstructured text data. When topic modeling is used, one has to deal with many parameters that could influence the result, such as the hyperparameter Alpha and Beta, the topic number, document length, or the number of iterations when updating the model. To understand the impact of these parameters, they must be systematically evaluated. In the last few years, there have been several studies evaluating LDA topic modeling in Digital Humanities or Computational Literary Studies (e.g., Jockers 2013; Schöch 2017; Du 2020; Uglanova & Gius 2020) and the presented paper focuses on evaluating the impact of hyperparameter Alpha on LDA topic models. Hyperparameter Alpha can refer to two different types of parameters in the context of LDA topic modeling: LDA model parameter and inference algorithm parameter. As a parameter of the LDA model, Alpha determines the properties of a Dirichlet distribution, which is the prior probability distribution of the topic-document distribution. Together, the hyperparameter Alpha and the prior probability distribution determine which topics we expect to occur more frequently in the corpus and how confident we are about them. In practice, when we employ Gibbs Sampling to train our topic model, Alpha is the parameter, which has the smoothing effect on the topic-document distribution and ensures that the probability of each topic in each document is not 0 throughout the entire inference procedure. More importantly, Alpha represents the assumption about the data on how topics are distributed in documents before inferencing the topic model. In other words, the hyperparameter Alpha affects how often each topic occurs in each document. When the alpha value of a topic is set larger in a document, it means that the topic has a greater chance of appearing in that document. And vice versa. For this reason, the setting of Alpha can affect the quality of the inferenced topic model. Therefore, this paper focuses on evaluating the impact of inference algorithm parameter alpha systematically. According to Griffiths & Steyvers (2004), the topic model has the best quality when the sum of Alphas of all topics is equal to 50. This is probably the reason that in MALLET 2.0.7, the default value of the sum of Alphas was set to 50, while in MALLET 2.0.8, the value is reduced to 5. According to the supervisor of MALLET, David Mimno: ""The general experience was that 50 was too large, and that 5 is a better default.""1 Since there are different opinions on this issue, it is interesting to test how Alpha affects LDA topic modeling, especially on different types of text collections that are not in English. Therefore, this paper presents a study on evaluating Alpha on two German text collections and aims to unTwo collections of German texts were built for the study. The first corpus is a collection of 2000 newspaper articles published between 2001 and 2014. The articles belong to ten different thematic classes, and each class contains 200 articles. The ten classes are ""Digital"", ""Society"", ""Career"", ""Culture"", ""Lifestyle"", ""Politics"", ""Travel"", ""Sports"", ""Study"" and ""Economy"". The corpus contains over 3.4 million words in total, and the average text length is about 1800 words. The second corpus consists of 439 dime novels published between 1961 and 2016, and they belong to five subgenres, namely 100 fantasy novels, 51 horror novels, 88 crime novels, 100 romance novels, and 100 science fiction stories. The corpus contains about 13.4 million words, and the average text length is about 30,000 words. All texts are lemmatized. Since the average document length of the newspaper articles is 1800 words, the novels are also split into 1800 words segments. Thus, the document length is no longer a confounding factor when comparing the test results on the newspaper corpus with the results on the novel corpus. The goal of the following tests is to explore the influence of the hyperparameter Alpha. While training topic models, the setting is varied by the value of Alpha _ {0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 30, 40, 50, 100} and number of topics _ {10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500}. For all other parameter settings, the default values of the topic modeling software were taken. All models were trained without applying hyperparameter optimization, which means that if Alpha is set to 0.1, the Alpha value for each topic is set to 0.1 during the whole training process. Common stop words were removed from both corpora. For technical reasons, namely random initialization in the topic assignment and Gibbs sampling, two topic models from one corpus are not completely identical even if the parameter settings during training are the same. Therefore, ten models were trained for each setting to balance the randomness from the technical side. The topic models were trained using MALLET (McCallum 2002). As a result, a topic-document distribution and the topics are obtained for each topic model. In a topic-document distribution, each document is represented by an N -dimensional vector, while N is the number of topics of the topic model. Based on the topic-document distribution, the document classification was performed, and the classification was done as a 10-fold cross-validation with a linear SVM classifier. For the newspaper corpus, the articles were classified according to their thematic classes. For the novel corpus, the novel segments were classified according to their subgenre. The topic coherence was automatically calculated by the Java program Palmetto (Röder et al. 2015), and the first ten most important words of each topic were taken for the calculation. The reference corpus for the calculation of the topic coherence is the lemmatized German Wikipedia. Several topic coherence mea-sures have been implemented in Palmetto. For this work, the Nor-malized Pointwise Mutual Information (NPMI) based coherence measure proposed in Aletras & Stevenson (2013) was taken. The theoretical range of NPMI based coherence measure is between -1 and 1. The higher the score, the better the topic. By performing Bag-of-Words (BoW) model-based classification, a baseline of document classification has been defined for both corpora. The tests were also done as a 10-fold cross-valida1 Digital Humanities im deutschsprachigen Raum 2022 tion with a linear SVM classifier. The F1(macro) score for the newspaper articles and for the novel segments was 0.758 and 0.993, respectively. A baseline of the NPMI value was also defined for each corpus. With only one iteration, 14 topic models were first trained on each corpus, containing 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500 topics, respectively. In this way, 1,950 ""topics before topic modeling"" have been trained for each corpus. The NPMI scores of these topics were then calculated, and the average NPMI score is the NPMI baseline, which is -0.0619 for the newspaper corpus and -0.1153 for the novel corpus. A black line represents the baselines in Figure 3 and Figure 4. Results Document classification : Figure 1 and Figure 2 show the classification results based on topic models of newspaper articles and novel segments, respectively. It can be seen in both figures that the classification results gradually become worse with the increase of the setting of Alpha, regardless of how many topics have been trained. Especially if Alpha is set to greater than 1, the classification results based on topic models with more topics (the blue lines) show a stronger decreasing trend than the results based on topic models with fewer topics (the red lines). In comparison, most F1 scores change less when Alpha is set to a value smaller than 1. However, we can still see that the blue lines start to decrease when the Alpha is raised from 0.5 to 1. The highest F1-score of classifying newspaper articles and novel segments in this test are 0.752 and 0.998, respectively, which do not differ much from the predefined baseline based on BoW-model. Topic coherence: Compared to the classification results, the evaluation from the perspective of topic coherence shows some differences between the two corpora. Firstly, it can be observed in Figure 3 that the maximum of the NPMI-score distributions decreases with the increase of Alpha from almost 0.3 to about 0.12. In addition, the median of the distributions also shows a decreasing trend. At Alpha = 0.01, the median is lower than the NPMI baseline if the number of topics is set higher than 100. However, at Alpha = 100, the median is already lower than the NPMI baseline if the number of topics is set to 70. Apart from that, we can observe that the topic models with a higher number of topics contain more topics with low NPMI scores, regardless of the setting of Alpha. Compared to the test on the newspaper corpus, the test results on the novel corpus are slightly different. When Alpha is set smaller than 1, the NPMI-score distributions do not show an evident change as Alpha increases, and the range of distribution is often broader when the number of topics is set between 60 and 300. Starting from Alpha being raised to greater than 1, the distributions of the NPMI-scores clearly change, and the results then are similar to the previous test on the newspaper corpus: the maximum of the NPMI-score distributions decreases with the increase of the Alpha, and topic models with a higher number of topics contain more topics with low NPMI scores. Fig. 3: NPMI-score distributions of topics from newspaper articles Fig. 1: Average F1(macro)-scores of topic modeling based classification of newspaper articles Fig. 4: NPMI-score distributions of topics from novel segments Conclusion Fig. 2: Average F1(macro)-scores of topic modeling based classification of novel segments The presented research evaluates the influence of hyperparameter Alpha in topic modeling on a German newspaper corpus and a German literary text corpus from two perspectives, single-label document classification, and topic coherence. Based on the results of the presented investigation, it can be stated that one should 2 Digital Humanities im deutschsprachigen Raum 2022 avoid training topic models with a setting of the Alpha of each topic to greater than 1 in order to ensure better topic modeling based document classification results and to get more coherent topics. In addition to that, LDA topic models with many topics are more vulnerable to changes in Alpha. Therefore, with the result of the presented investigations in this study, one can confirm the explanation of Mimno mentioned earlier that a smaller Alpha is better suitable for LDA Topic Modeling. Footnotes 1. https://stackoverflow.com/questions/45162186/mallet-topic-modeling-topic-keys-output-parameter (15.07.2021) Bibliography Aletras, Nikolaos / Stevenson, Mark (2013): ""Evaluating topic coherence using distributional semantics"". In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) ‚Äì Long Papers (pp. 13-22). Blei, David M. (2012): ""Probabilistic topic models"", in: Communications of the ACM , 55(4), 77-84. Du, Keli (2020): ‚ÄûDer Spielraum zwischen ""zu wenig"" und ""zu viel"""". Presented at the DHd 2020 Spielräume: Digital Humanities zwischen Modellierung und Interpretation. 7. Tagung des Verbands ""Digital Humanities im deutschsprachigen Raum"" (DHd 2020), Paderborn: Zenodo. http://doi.org/10.5281/zenodo.4621770 . Griffiths, Thomas L. / Steyvers, Mark (2004): ""Finding scientific topics"", in: Proceedings of the National Academy of Sciences, 101 (Supplement 1), 5228‚Äì5235. https://doi.org/10.1073/pnas.0307752101 . Jockers, Matthew L. (2013): Macroanalysis: Digital methods and literary history . University of Illinois Press. McCallum, Andrew K. (2002): MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu . Röder, Michael / Both, Andreas / Hinneburg, Alexander (2015): ""Exploring the space of topic coherence measures"", in: Proceedings of the eighth ACM international conference on Web search and data mining , 399‚Äì408. Schöch, Christof (2017): ""Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama."", in: Digital Humanities Quarterly 11, no. 2. ¬ß1-53. http://www.digitalhumanities.org/dhq/vol/11/2/000291/000291.html . Uglanova, Inna / Gius, Evelyn (2020): ""The Order of Things. A Study on Topic Modelling of Literary Texts"", in: Online Workshop on Computational Humanities Research, Proceedings . http://ceur-ws.org/Vol-2723/long7.pdf .",en,digital humanity m deutschsprachigen raum evaluate hyperparameter alpha lda topic modeling derstand influence hyperparameter alpha perspective topic model base single label document classification topic coherence represent quality topic model quality topic respectively method du keli universität trier germany introduction quantitative text analysis method latent dirichlet allocation lda refer topic modeling blei widely digital humanity recent year explore numerous unstructured text datum topic modeling deal parameter influence result hyperparameter alpha beta topic number document length number iteration update model understand impact parameter systematically evaluate year study evaluate lda topic model digital humanity computational literary study jockers schöch du uglanova gius present paper focus evaluate impact hyperparameter alpha lda topic model hyperparameter alpha refer different type parameter context lda topic model lda model parameter inference algorithm parameter parameter lda model alpha determine property dirichlet distribution prior probability distribution topic document distribution hyperparameter alpha prior probability distribution determine topic expect occur frequently corpus confident practice employ gibb sample train topic model alpha parameter smooth effect topic document distribution ensure probability topic document entire inference procedure importantly alpha represent assumption datum topic distribute document inference topic model word hyperparameter alpha affect topic occur document alpha value topic set large document mean topic great chance appear document vice versa reason setting alpha affect quality inference topic model paper focus evaluate impact inference algorithm parameter alpha systematically accord griffith steyver topic model good quality sum alpha topic equal probably reason mallet default value sum alpha set mallet value reduce accord supervisor mallet david mimno general experience large well default different opinion issue interesting test alpha affect lda topic model especially different type text collection english paper present study evaluate alpha german text collection aim untwo collection german text build study corpus collection newspaper article publish article belong different thematic class class contain article class digital society career culture lifestyle politics travel sport study economy corpus contain million word total average text length word second corpus consist dime novel publish belong subgenre fantasy novel horror novel crime novel romance novel science fiction story corpus contain million word average text length word text lemmatize average document length newspaper article word novel split word segment document length long confound factor compare test result newspaper corpus result novel corpus goal follow test explore influence hyperparameter alpha train topic model setting vary value alpha number topic parameter setting default value topic modeling software take model train apply hyperparameter optimization mean alpha set alpha value topic set training process common stop word remove corpora technical reason random initialization topic assignment gibb sample topic model corpus completely identical parameter setting training model train setting balance randomness technical topic model train mallet mccallum result topic document distribution topic obtain topic model topic document distribution document represent n vector n number topic topic model base topic document distribution document classification perform classification fold cross validation linear svm classifier newspaper corpus article classify accord thematic class novel corpus novel segment classify accord subgenre topic coherence automatically calculate java program palmetto röder et al important word topic take calculation reference corpus calculation topic coherence lemmatize german wikipedia topic coherence mea sure implement palmetto work malize pointwise mutual information npmi base coherence measure propose aletras stevenson take theoretical range npmi base coherence measure high score well topic perform bag word bow model base classification baseline document classification define corpora test fold cross digital humanity m deutschsprachigen raum tion linear svm classifier score newspaper article novel segment respectively baseline npmi value define corpus iteration topic model train corpus contain topic respectively way topic topic model train corpus npmi score topic calculate average npmi score npmi baseline newspaper corpus novel corpus black line represent baseline figure figure result document classification figure figure classification result base topic model newspaper article novel segment respectively see figure classification result gradually bad increase setting alpha regardless topic train especially alpha set great classification result base topic model topic blue line strong decrease trend result base topic model few topic red line comparison score change alpha set value small blue line start decrease alpha raise high score classify newspaper article novel segment test respectively differ predefine baseline base bow model topic coherence compare classification result evaluation perspective topic coherence show difference corpora firstly observe figure maximum npmi score distribution decrease increase alpha addition median distribution show decrease trend alpha median low npmi baseline number topic set high alpha median low npmi baseline number topic set apart observe topic model high number topic contain topic low npmi score regardless setting alpha compare test newspaper corpus test result novel corpus slightly different alpha set small npmi score distribution evident change alpha increase range distribution broad number topic set start alpha raise great distribution npmi score clearly change result similar previous test newspaper corpus maximum npmi score distribution decrease increase alpha topic model high number topic contain topic low npmi score fig npmi score distribution topic newspaper articles fig average topic model base classification newspaper articles fig npmi score distribution topic novel segment conclusion fig average topic model base classification novel segment present research evaluate influence hyperparameter alpha topic model german newspaper corpus german literary text corpus perspective single label document classification topic coherence base result present investigation state digital humanity m deutschsprachigen raum avoid train topic model setting alpha topic great order ensure well topic model base document classification result coherent topic addition lda topic model topic vulnerable change alpha result present investigation study confirm explanation mimno mention early small alpha well suitable lda topic model footnote bibliography aletra nikolaos stevenson mark evaluate topic coherence distributional semantic proceeding international conference computational semantic iwcs äì long paper pp blei david probabilistic topic model communication acm du keli äûder spielraum zwischen zu wenig und zu viel present dhd spielräume digital humanity zwischen modellierung und interpretation tagung des verband digital humanity m deutschsprachigen raum dhd paderborn zenodo griffith thomas steyver mark find scientific topic proceeding national academy sciences supplement jocker matthew macroanalysis digital method literary history university illinois press mccallum andrew mallet machine learn language toolkit röder michael andreas hinneburg alexander explore space topic coherence measure proceeding eighth acm international conference web search datum mining schöch christof topic modeling genre exploration french classical enlightenment drama digital humanity quarterly uglanova inna gius evelyn order thing study topic modelling literary text online workshop computational humanity research proceeding,"[('topic', 0.5728824281431846), ('alpha', 0.4329826684380329), ('model', 0.2493723510740921), ('npmi', 0.2010209213656478), ('document', 0.16564675394088801), ('newspaper', 0.13801814377664248), ('distribution', 0.13738260567049082), ('coherence', 0.12872457710319898), ('novel', 0.12826205410835256), ('corpus', 0.1279844609490846)]"
2023,DHd2023,SCHUMACHER_Mareike_DisKo__Zur_Einbindung_von_Citizen_Humanit.xml,DisKo: Zur Einbindung von Citizen Humanities beim Aufbau eines Diversitäts-Korpus,"Mareike Schumacher (Technische Universität Darmstadt, Deutschland); Flüh Marie (Universität Hamburg); Leinen Peter (Deutsche Nationalbibliothek)","Diversität, Korpusbildung, Citizen Humanities","Sammlung, Kommunikation, Community-Bildung, Identifizierung, Crowdsourcing, Literatur"," Die Korpuskonstituierung ist für Projekte, die Verfahren des maschinellen Lernens einsetzen, ein Dreh- und Angelpunkt. Alle weiteren Verfahrensschritte und Ergebnisse wie z.B. die Performanz eines Classifiers oder Analyseergebnisse, die durch dessen Einsatz erzielt werden, werden vom genutzten Korpus massiv beeinflusst. Unser Konzept der Korpuskonstituierung greift sowohl arbeitspraktische Ansätze der Digital Humanities wie das",de,Korpuskonstituierung Projekt Verfahren maschinell lernens einsetzen anlpunken Verfahrensschritt Ergebnis performanz Classifier analyseergebnisse Einsatz erzielen genutzt Korpus massiv beeinflussen Konzept Korpuskonstituierung greifen sowohl arbeitspraktisch Ansatz Digital humaniteisen,"[('korpuskonstituierung', 0.5422307527263198), ('verfahrensschritt', 0.2711153763631599), ('anlpunken', 0.2711153763631599), ('analyseergebnisse', 0.25252341936393286), ('arbeitspraktisch', 0.25252341936393286), ('lernens', 0.2393322159446606), ('humaniteisen', 0.2136719249422387), ('massiv', 0.20754905552616126), ('performanz', 0.2021483019462065), ('classifier', 0.1787252062439073)]"
2023,DHd2023,√áAKIR_D√Ælan_Canan_Vom_Finden__Filtern_und_Auswerten_der_rele.xml,"Vom Finden, Filtern und Auswerten der relevanten Daten im digitalen Nachlass von Friedrich Kittler im Deutschen Literaturarchiv Marbach","Alex Holz (Deutsches Literaturarchiv Marbach, Deutschland); Dîlan Canan Çakir (Deutsches Literaturarchiv Marbach, Deutschland)","Literaturarchiv, Born-Digitals, Nachlass, Friedrich Kittler, NSRL","Datenerkennung, Bereinigung, Archivierung, Identifizierung, Literatur, Metadaten","   In einem nächsten Schritt wurde die Menge auf die Dateien begrenzt, die von Seiten der Nachlassverwaltung begutachtet und mit einem Status versehen wurden (freigegeben, vorläufig gesperrt, gesperrt). Unser Arbeitskorpus ist also als Momentaufnahme zu sehen, da er nur auf den bereits bewerteten Dateien basiert. Ohne die bislang unbekannten bzw. noch nicht bewerteten Dateien kommen wir auf 219.989 Dateien. Aus dieser Menge wurden zuletzt nun die für die Forschung vollständig freigegebenen Dateien (Metadaten und Inhalt) herausgefiltert, die zudem von Kittler erstellt wurden. Wir landeten bei etwa 30.000 Dateien, also etwa 0,88% von den 3,3 Millionen Dateien. ",de,nächster Schritt Menge dateien begrenzen Seite Nachlassverwaltung begutachten Status versehen freigeben vorläufig sperren sperren Arbeitskorpus Momentaufnahme sehen bewerteter dateien basieren bislang unbekannt bewerten dateien dateien Menge zuletzt Forschung vollständig freigegeben Datei Metadat Inhalt herausgefiltern zudem Kittler erstellen landen dateie Million dateien,"[('dateien', 0.6548539805634116), ('sperren', 0.3217192627236166), ('menge', 0.1615065062176734), ('begutachten', 0.1608596313618083), ('freigegeben', 0.1608596313618083), ('freigeben', 0.1608596313618083), ('arbeitskorpus', 0.1608596313618083), ('momentaufnahme', 0.1608596313618083), ('nachlassverwaltung', 0.1608596313618083), ('bewerteter', 0.1608596313618083)]"
2023,DHd2023,SCHUMACHER_Mareike_GitMA_oder_CATMA_für_Fortgeschrittene.xml,GitMA oder CATMA für Fortgeschrittene,"Mareike Schumacher (Technische Universität Darmstadt, Deutschland); Malte Meister (Technische Universität Darmstadt, Deutschland); Dominik Gerstorfer (Technische Universität Darmstadt, Deutschland)","manuelle Annotation, Textanalyse, kollaboratives Annotieren","Inhaltsanalyse, Annotieren, Netzwerkanalyse, Kollaboration, Einführung, Visualisierung","  Der Workshop bietet: Abbildung 1: Im Workshop vermittelter Workflow zur Annotationsauswertung und -überarbeitung mit dem CATMA-Backend Ablauf: CATMA 6 (45 Minuten) Explorative Annotationsauswertungen (60 Minuten) 15 Minuten Pause Statistische Annotationsauswertungen (45 Minuten) Diskussion und Feedback (30 Minuten) Nutzer*innen, die Annotationen mit CATMA in Forschungsprojekten oder Lehrsituationen managen, sowie alle, die einen schnellen Workflow zwischen Annotation bzw. Annotationsbearbeitung und Annotationsauswertung benötigen. 30   Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Residenzschloss 1, 64283 Darmstadt Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Residenzschloss 1, 64283 Darmstadt  Malte Meister, B.Sc. Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Residenzschloss 1, 64283 Darmstadt  Mareike Schumacher, M.A. Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Residenzschloss 1, 64283 Darmstadt Mareike Schumacher koordiniert das DFG-Projekt forTEXT (",de,Workshop bieten Abbildung Workshop vermittelt workflow Annotationsauswertung Ablauf catma Minute explorativ annotationsauswertung Minute Minute Pause statistisch annotationsauswertungen Minute Diskussion Feedback Minute Annotation Catma Forschungsprojekt lehrsituation managen schnell workflow Annotation Annotationsbearbeitung Annotationsauswertung benötigen technisch Universität Darmstadt Institut Literaturwissenschaft Residenzschloss Darmstadt technisch Universität Darmstadt Institut Literaturwissenschaft Residenzschloss Darmstadt malen Meister technisch Universität Darmstadt Institut Literaturwissenschaft Residenzschloss Darmstadt mareik Schumacher technisch Universität Darmstadt Institut Literaturwissenschaft Residenzschloss Darmstadt Mareike Schumacher koordinieren Fortext,"[('darmstadt', 0.5853281276097407), ('residenzschloss', 0.39251246259562395), ('minute', 0.32916616460207954), ('institut', 0.25875282977951863), ('annotationsauswertung', 0.2487632758832199), ('universität', 0.2044291788211729), ('technisch', 0.16348453289091006), ('literaturwissenschaft', 0.15016297931323666), ('schumacher', 0.12937641488975932), ('catma', 0.119827506583351)]"
2023,DHd2023,DU_Keli_Evaluating_token_based_DTFs_on_authorship_classifica.xml,Understanding the impact of three derived text formats on authorship classification with Delta,"Keli Du (Universität Trier, Deutschland)","derived text formats, authorship classification, evaluation","Umwandlung, Bearbeitung, Bewertung, Stilistische Analyse, Methoden, Text"," However, as far as I know, there is not much research dedicated to the question, how much the loss of information caused by DTFs affects the TDM results. In Applying the first and the second DTFs to frequency-based authorship attribution does not present any challenge. Take the most well-known method in authorship attribution Burrows""s Delta ( In comparison, if the texts are transformed into the third DTF, although the frequency information of some words in the text will not match the original situation, the sequence information of words could be kept. This opens the possibility of using the data in this form for other TDM tasks such as sentiment analysis or named entity recognition that require the sequence information of words. If a corpus is published with the expectation that it can be applied to multiple TDM tasks, it makes more sense to prepare the corpus in this format. And of course, it is important to understand how much this format will affect the outcome of different TDM tasks. Therefore, this paper evaluates the usefulness of the third token-based DTF on authorship attribution as a start. In the next sections, the method and the results of the evaluation are reported. For the evaluation, three corpora representing different languages and text types have been constructed: deu_DraCor (German plays), fra_ELTeC (French novels) and eng_RSC (English journal articles). The relevant information about the corpora is shown in Table 1. The test is designed as follows: First, for each document in a corpus, a certain percentage of words (0%, 10%, ..., 100%) were randomly selected and replaced by their corresponding POS-tags. Since function words are crucial to authorship attribution, instead of only replacing function words as suggested in Before presenting the classification results, three text passages are prepared to give an impression of readability of the texts in DTF. The original text, the texts with 10% and 50% of words replaced by their corresponding POS-tags, are listed in Table 2. 0% (original) The classification results on the German play collection, the English article collection, and the French novel collection are presented in Figures 1, 2 and 3, respectively. The y-axis is the F1(macro)-score, and the x-axis shows the portion of words that are replaced or removed. The blue boxplots and the yellow boxplots represent the classification results, when the words in texts are replaced with POS-tags or removed, respectively. As the reference value, the classification results for the original data are also shown in the figures. The Welch""s t-test is also performed to determine the difference in classification results. The ""ns"" in the figures means non-significance. In all the three figures, the same trend can be observed: Step by step, the median of F1-score distributions get worse as the percentage increases. Especially when more than half of the words were replaced or removed, the tendency for the classification results to become worse became particularly obvious. In addition, the variance of the F1-scores always becomes larger, if a certain percentage of words in texts are replaced or removed. According to the Welch""s t-test, in all cases, whether the words are replaced or removed does not affect the classification. This observation indicates that the POS-tags do not contribute to the distinction of authorships. Another interesting observation is, when all words in texts are replaced by POS-tags, the classification results improve, relative to a reduction of 90%, in the case of the German and English data, but not for the French data. To understand this situation, the change in the number of word types in each corpus was checked. As presented in Figure 4, when 90% of the words are replaced or removed, there are still around 20,000 word types in each text collection. But when all the words are replaced, only a few dozen types remain. Their number becomes so small that it looks like it is reduced to zero in the Figure 4. Since the classification is based on the most frequent 2000 types, although 90% of the words are replaced or removed, the 2000 features used for classification are still mostly from the remaining 10% of words. In the German and English collections, these words bring apparently noise to the classification task. In contrast, the remaining 10% of words in the French corpus are still able to guarantee a relatively good classification result. From the data in Table 1 we can see that number of authors in the French corpus is smaller, which indicates the classification task on the French corpus is easier. More importantly, as presented in previous studies (e.g., This paper provides an exploration of the usefulness of three token-based DTFs for frequency-based authorship classification with Delta. As presented, selectively reducing information on individual tokens could ensure, to a certain extent, that the authorship classification results are not affected too much. The impact of token-based DTFs on the results of Delta test can be reduced by considering only replacing or removing content words, while all function words remain unchanged. But this limits the application of the texts on other TDM tasks such as topic modeling. For the future work, a series of tests are planned on evaluating the usefulness of token-based DTFs on other TDM tasks. The goal is to find DTFs that could balance various factors (e.g. word frequency, sequence information, content vs. function words, copyright) so that texts could be published and used for as many TDM tasks as possible without violating copyright law.",en,far know research dedicate question loss information cause dtfs affect tdm result apply second dtfs frequency base authorship attribution present challenge know method authorship attribution delta comparison text transform dtf frequency information word text match original situation sequence information word keep open possibility datum form tdm task sentiment analysis name entity recognition require sequence information word corpus publish expectation apply multiple tdm task make sense prepare corpus format course important understand format affect outcome different tdm task paper evaluate usefulness token base dtf authorship attribution start section method result evaluation report evaluation corpora represent different language text type construct german play french novel english journal article relevant information corpora show table test design follow document corpus certain percentage word randomly select replace correspond pos tag function word crucial authorship attribution instead replace function word suggest present classification result text passage prepared impression readability text dtf original text text word replace correspond pos tag list table original classification result german play collection english article collection french novel collection present figure respectively y axis x axis show portion word replace remove blue boxplot yellow boxplot represent classification result word text replace pos tag remove respectively reference value classification result original datum show figure t test perform determine difference classification result ns figure mean non significance figure trend observe step step median score distribution bad percentage increase especially half word replace remove tendency classification result bad particularly obvious addition variance score large certain percentage word text replace remove accord t test case word replace remove affect classification observation indicate pos tag contribute distinction authorship interesting observation word text replace po tag classification result improve relative reduction case german english datum french datum understand situation change number word type corpus check present figure word replace remove word type text collection word replace dozen type remain number small look like reduce zero figure classification base frequent type word replace remove feature classification remain word german english collection word bring apparently noise classification task contrast remain word french corpus able guarantee relatively good classification result datum table number author french corpus small indicate classification task french corpus easy importantly present previous study paper provide exploration usefulness token base dtfs frequency base authorship classification delta present selectively reduce information individual token ensure certain extent authorship classification result affect impact token base dtfs result delta test reduce consider replace remove content word function word remain unchanged limit application text tdm task topic modeling future work series test plan evaluate usefulness token base dtfs tdm task goal find dtfs balance factor word frequency sequence information content function word copyright text publish tdm task possible violate copyright law,"[('word', 0.4300621084370888), ('replace', 0.34463416411087816), ('classification', 0.30785748114416683), ('result', 0.2231564669691825), ('tdm', 0.21021611357960793), ('remove', 0.1956074303906399), ('dtfs', 0.1801852402110925), ('authorship', 0.16567589136760824), ('french', 0.15226173227487896), ('task', 0.13927858155393558)]"
2023,DHd2023,HÖUSSLER_Julian_Fanfiction_Semantics___Eine_quantitative_Ana.xml,Fanfiction Semantics - Eine quantitative Analyse sensibler Themen in deutscher Fanfiction,"Julian Häußler (Technische Universität Darmstadt, Deutschland)","Digitale Literaturwissenschaft, Quantitative Textanalyse, Fanfiction","Inhaltsanalyse, Visualisierung, Literatur","Als Fanfiction können all jene Texte bezeichnet werden, die zur Veröffentlichung in eigens dafür eingerichteten Internetforen bestimmt sind und in ""appropriativ-derivativer bzw. [‚Ä¶] transformativer"" (Stemberger 2021, 10) Weise Bezug auf Mainstreammedien (meist Romane, Serien oder Filme) nehmen. Für die Autor*innen von Fanfiction, die meist unter Pseudonym schreiben, ist es somit möglich, in einem geschützten Raum (der Fancommunity) neben ersten Schreibversuchen, zu ihren Lieblingsfiguren auch das Verfassen von die Bezugsmedien kontrastierenden Erzählungen zu wagen. Catherine Tosenberger beschreibt letztere Kategorie als ""a freedom especially felt with regard to non-normative and taboo forms and representations of sexuality"" (Tosenberger 2014, 17). Fanfiction, die Literatur als Bezugsquelle gewählt haben, bauen dabei meistens auf großen Mainstream-Jugendbuchreihen aus dem angelsächsischen Raum auf. So sammeln sich in der deutschsprachigen Community auf fanfiktion.de unter der Kategorie ""Bücher"" die meisten Autor*innen in den Fandoms zu Harry Potter (über 55.000 Texte gesamt), Bis(s) (knapp 14.000) und Herr der Ringe (als allg. Gruppe Mittelerde definiert mit über 8.000 Texten). Praktisch befassen sich die Autor*innen oft mit Rekombinationen des Figurenarsenals der Originaltexte. Eine wichtige Rolle spielt dabei das Stereotyp der Slashes (männliche Figuren, die in eine romantisch-sexuelle Beziehung gesetzt werden; vgl. Brottrager et al. 2022). Die Moralvorstellungen der Originale können dabei auch übergangen werden, wenn zum Beispiel der Schüler Harry mit seinem Lehrer Severus Snape eine (romantisch-sexuelle) Beziehung eingeht. Das Masterprojekt ""Fanfiction Semantics 'A Quantitative Analysis of Sensitive Topics in German Fanfiction"" hat zum Ziel, diese Kontrastierungen in verschiedenen Fandoms zu betrachten. Gefragt wird dabei einerseits, wie virulent sensible Themen (sensitive topics) in Fanfiction sind und andererseits, wie diese Themen Wortbedeutungen beeinflussen. Als sensibel werden dabei jene Themen definiert, die in Originaltexten mehrheitlich ausgespart und von Fanfiction-Autor*innen demonstrativ eingeführt werden (v. a. extreme Gewalt und sexuelle Inhalte). 1) Im ersten Schritt, bei der Analyse der Metadaten, kann beispielsweise eine Verteilung davon, welche der Kategorien zur Altersbeschränkung für wie viele Texte verwendet wurde, 2) Die Analyse der Texte erfolgt im folgenden Schritt mithilfe von Word Embedding Modellen (hier word2vec; vgl. Mikolov et al. 2013). In einem Word Embedding Modell können Öhnlichkeiten zwischen verschiedenen Schlüsselwörtern effizient verglichen werden. Die algorithmische Grundlage bietet dabei die Verwendung der Wörter in einem festgesetzten Kontextfenster (vgl. Distributionshypothese nach Firth; vgl. Firth 1957). Durch den Abgleich von Schlüsselwörtern, die auf verschiedene Themen verweisen, können Schnittmengen und Unterschiede untersucht werden. Die Schlüsselwörter wurden hierzu aus den Wortfeldern Gewalt und sexuelle Inhalte gewählt, welche Wörter unter den (hier) 30 ähnlichsten zu einem Zielwort zu finden sind, beschreibt in gewisser Weise wie dieses Zielwort verwendet wird (vgl. Abb. 1). Für das Subkorpus der Das Poster soll die Ergebnisse der oben beschriebenen Analysen grafisch darstellen und einen Eindruck über die thematische Zusammensetzung der Fanfiction-Korpora bieten. Das zugrundeliegende Korpus kann aus urheberrechtlichen Gründen nicht in der verwendeten Form veröffentlicht werden. Dennoch ist es beabsichtigt, nach Abschluss des Projekts alle verwendeten Codes auf GitHub bereitzustellen, um Transparenz zu schaffen und die Methodik für andere Projekte nachnutzbar zu machen (orientiert an der in den FAIR-Prinzipien definierten Wiederverwendbarkeit (vgl. GO FAIR 2022)). Anonymisierte Metadatentabellen sowie die Word Embedding-Modelle sollen zudem auch zugänglich gemacht werden.",de,Fanfiction all Text bezeichnen Veröffentlichung eigens eingerichtet Internetforen bestimmen Transformativer Stemberger Weise Bezug Mainstreammedien meist roman Serie Film nehmen Fanfiction meist pseudonym schreiben somit geschützt Raum Fancommunity schreibversuch lieblingsfiguren verfassen Bezugsmedien kontrastierend erzählungen wagen Catherine Tosenberger beschreiben letzterer Kategorie freedom especially feln with regard to and Taboo Forms and representations of sexuality tosenberger Fanfiction Literatur Bezugsquelle wählen bauen meistens angelsächsisch Raum sammeln deutschsprachig Community Kategorie büch meister Fandoms Harry Potter Text samen s knapp Herr Ringe Gruppe Mittelerde definieren Text praktisch befassen Rekombination Figurenarsenal Originaltexte wichtig Rolle spielen Stereotyp slash männlich Figur Beziehung setzen Brottrager Et Moralvorstellung originale übergehen Schüler Harry Lehrer Severus Snape Beziehung eingehen Masterprojekt Fanfiction semantics quantitative Analysis of sensitiv Topics German Fanfiction Ziel Kontrastierunge verschieden Fandoms betrachten fragen einerseits virulent sensibel Thema sensitiv Topics Fanfiction andererseits Thema wortbedeutungen beeinflussen sensibel Thema definieren Originaltext mehrheitlich aussparen demonstrativ einführen extrem Gewalt sexuell inhalen Schritt Analyse Metadat beispielsweise Verteilung Kategori Altersbeschränkung Text verwenden Analyse Text erfolgen folgend Schritt Mithilfe Word Embedding modellen mikolov et Word Embedding Modell Öhnlichkeit verschieden schlüsselwörtern effizient vergleichen algorithmisch Grundlage bieten Verwendung Wörter festgesetzt Kontextfenster Distributionshypothese Firth Firth abgleich schlüsselwörtern verschieden Thema verweisen schnittmengen Unterschied untersuchen Schlüsselwörter hierzu Wortfelder Gewalt sexuell Inhalt wählen Wörter ähnlichsten Zielwort finden beschreiben gewiß Weise Zielwort verwenden abb Subkorpus Poster Ergebnis beschrieben Analyse grafisch darstellen Eindruck thematisch zusammensetzung bieten zugrundeliegend korpus urheberrechtlich Grund verwendet Form veröffentlichen dennoch beabsichtigen Abschluss Projekt verwendet codes Github bereitstellen Transparenz schaffen Methodik Projekt nachnutzbar orientieren Definiert Wiederverwendbarkeit go fair anonymisierte metadatentabellen Word zudem zugänglich,"[('fanfiction', 0.3880054387090183), ('sensibel', 0.14651076834953683), ('zielwort', 0.14651076834953683), ('firth', 0.14651076834953683), ('schlüsselwörtern', 0.14651076834953683), ('sensitiv', 0.14651076834953683), ('thema', 0.14455792918268073), ('fandoms', 0.13646367348675928), ('gewalt', 0.13646367348675928), ('tosenberger', 0.13646367348675928)]"
2023,DHd2023,JANNIDIS_Fotis_Korpuszusammensetzung_und_Verlässlichkeit_des.xml,Korpuszusammen-setzung und Verlässlichkeit des deutschsprachigen Google Ngram-Viewers,"Fotis Jannidis (Universität Würzburg, Deutschland)","Google Ngram-Viewer, Culturomics, Deutsche Korpora","Inhaltsanalyse, Metadaten, Text","Als Google im Jahre 2009 die erste Version des Ngram-Viewer publizierte, hat die Digital Humanities-Community recht schnell die positiven und negativen Aspekte dieses Werkzeugs analysiert: Die Möglichkeiten einer wort- und begriffsgeschichtlichen Forschung sind sprunghaft erweitert worden, wie auch der begleitende Aufsatz von Michel deutlich belegte (Michel et al. 2011). Dessen teilweise zu naive Umgang mit dem Quellenmaterial machte aber auch deutlich, dass die Autorinnen und Autoren kein Bewusstsein für die möglichen Fallen von korpusbasierten Forschungen hatten. Die wechselnde Zusammensetzung der zugrundeliegenden Textsammlung, die Unmöglichkeit auf die dahinterliegenden Texte zuzugreifen, das Fehlen von Metadaten für die Texte, falsche Jahreszahlen, OCR-Fehler u.a.m. sind dem Ngramm-Korpus wiederholt vorgeworfen worden (z.B. Underwood 2012). Die Arbeit von (Pechenick et al. 2015) hat gezeigt, wie die zunehmende Menge von wissenschaftlicher Literatur die Korpusanteile in der zweiten Hälfte des 20. Jahrhundert merklich verschiebt; unklar bleibt allerdings, ob dies nicht auch eine gesellschaftliche Entwicklung reflektiert, also keineswegs nur als Manko zu betrachten ist. Besonders einschlägig ist die Arbeit (Koplenig 2017), in der Veränderungen der Korpuszusammensetzung während des zweiten Weltkriegs untersucht werden: ""the German GB corpus was strongly biased toward volumes published in Switzerland during WWII"" (Koplenig 2017) Google hat zwei größere Updates vorgelegt, die die zugrundeliegende Textmenge deutlich erweitert haben. Hier die Entwicklung des Umfangs der deutschsprachigen Korpora, auf die ich mich im Folgenden beschränke: Dadurch dass die Anzahl der digitalisierten Bücher noch einmal deutlich gesteigert werden konnte 'und das über den gesamten Zeitraum, den der In diesem Sinne wollte auch eine Kollegin, die zu Fragen der literarischen Kanonisierung arbeitet, den Viewer verwenden, aber bei der Analyse der Ergebnisse fiel uns schnell auf, dass die Namen einer Reihe der hochkanonischen deutschsprachigen Autoren 'Thomas Mann, Goethe, Schiller, Kafka, Brecht 'nach 2005 einen auffälligen Abwärtstrend aufweisen (siehe Fig. 1). Bevor wir nun allgemeinere kulturanalytische Thesen über das Ende der Bildungskultur formulierten, wollte ich die Solidität der Daten prüfen. Doch wie kann man ein Korpus auf möglichen Bias untersuchen, wenn weder die Liste der Texte geschweige die Texte selbst vorliegen, sondern nur eine Reihe von generischen Metadaten und 1-5 Gramme? Da die NGramme, die dem  Wie lässt sich der Widerspruch zwischen den Ergebnissen erklären? Der  Wenn die Rohdaten aus Fig. 3 nun mit den Daten über die Anzahl der Token pro Jahr normalisiert werden, dann ergibt sich der Trend, der im Ngram-Viewer sichtbar wurde, alle Werte stürzen nach 2005 mehr oder weniger steil ab.  Allerdings ist der sehr steile Anstieg der Buchzahlen in Fig. 4 ziemlich überraschend. Wir wissen, dass Google Books auf den Ergebnissen der Digitalisierungskampagne Googles in Kooperation mit einer ganzen Reihe von internationalen Forschungsbibliotheken beruht. Die deutschsprachigen Ergebnisse verdanken sich nicht zuletzt den Kooperationen mit der Österreichischen Nationalbibliothek und der Bayerischen Staatsbibliothek. Deren Bestände speisen sich in den letzten Jahrzehnten aus Pflichtabgabeexemplaren und einer umfassenden Erwerbspolitik. Woher kommt also der plötzliche Anstieg? Sind 'den Klagen der Verlage zum Trotz 'seit 2005 sehr viel mehr Bücher als früher gedruckt worden?. Für die Buchproduktion konnte ich zwei Quellen verwenden, die die Daten gleich in digitaler Form anbieten: Statista, ein kommerzieller Datenanbieter, der Zahlen des Börsenvereins des deutschen Buchhandels zur Anzahl der Neuerscheinungen aufbereitet hat (Börsenverein 2022),  Die Grafik enthält neben den Werten für das aktuelle Korpus von 2019 auch die Werte für die früheren Ngramm-Korpora von Google aus den Jahren 2009 und 2012 , die deutlich kleiner waren. Beginnen wir bei den Werten vor 1995: Es ist auffällig, dass Google mit dem letzten Update den Anteil an der Buchproduktion eines Jahres, der digitalisiert vorliegt, deutlich steigern konnte, so dass bis in die Mitte der 1960er Jahre teils 50% und mehr im Korpus enthalten sind. Für den Zeitraum von den späten 1960ern bis in die späten 1990er stagnieren die Werte der Korpora, während die Buchproduktion in diesen Jahren steil angestiegen ist. Während im Jahr 1967 erstaunliche 67% der nach Rahlfs publizierten Bücher im Korpus liegen, sind es 1997 ""nur"" noch 23%. Erst danach, 1998 bis 2006, wächst der Anteil wieder. In den Korpora von 2009 und 2012 geschieht dies noch relativ langsam, während die Erweiterung von 2019 hier deutlich stärker zulegt. Von 2006 auf 2007 springen die Werte allerdings steil nach oben. Das gilt für alle drei Stufen des Korpus, aber auch hier ist der Anstieg im 2019-Korpus noch einmal deutlich ausgeprägter. Danach, zwischen 2008 und 2010 verhalten sich die Werte im Korpus auf einem hohen Niveau parallel zu den Werten der Buchproduktion und fallen mit dieser sogar leicht ab. Das ändert sich wiederum 2011-2013, wo wir einen weiteren Anstieg beobachten können, der diesmal sogar das Niveau der Buchproduktion übertrifft.  Wenn wir noch einmal auf Grafik 3 und 4 blicken, dann sehen wir dort, dass die Rohwerte für Goethe und zwei andere kanonisierte Autoren in den späten 1990er Jahren deutlich ansteigen, sie zugleich in der normalisierten Darstellung schon abfallen. Das bedeutet, dass bereits in der Phase von 1998-2006 ""andere"" Texte hinzugefügt wurden, deren Zusammensetzung anders war als das bisherige Korpus. Um welche Texte könnte es sich hierbei handeln? Die oben erwähnten verschiedenen Phasen der Veränderung legen es nahe zu vermuten, dass nicht ein einzelner Eingriff in das Korpus, sondern eine Reihe verschiedener Texthinzufügungen die beobachteten Phänomene bedingen. In einem anderen Projekt, in dem es um die Analyse von Heftromanen geht, war bereits aufgefallen, dass sich die einschlägigen Verlage der Komplexität der Feststellung der richtigen Metadaten, insbesondere des Publikationsdatums, dadurch entledigt haben, dass sie einfach alle retrodigitalisierten Texte unter dem Datum veröffentlichen, an dem die digitale Kopie publiziert wird. Um zu testen, ob diese Texte auch im Ngram-Korpus sind, wurden die entsprechenden Serienautoren und -helden gesucht:  Die deutliche Steigerung ab 2010 spricht dafür, dass auch hier die Retrodigitalisate unter dem jeweiligen Jahr aufgeführt sind. Allerdings können selbst einige Tausend Heftromane nicht alleine verantwortlich sein. Wie könnte man nun weitere Faktoren erkennen? Ein offensichtlicher Weg geht über das sprachliche Material. Die neuen Texte würden für bestimmte Worte zu einer relativen Erhöhung führen, selbst wenn viele andere Worte einen relativen Rückgang aufweisen. Da Google die Daten im 2019-Korpus mit Wortklasseninformation ausliefert, war es einfach, alle Substantive aus den 1-Grammen zu extrahieren, rd. 13 mio. Anschließend wurden die Substantive herausgefiltert, die von 1995 bis 2019 jedes Jahr auftauchen und zwar insgesamt mindestens 2 500 mal. Für diese restlichen rd. 350.000 Wörtern wurde für die Daten jedes Wortes eine lineare Regression berechnet und die Steigung der Geraden als Filterungsfaktor verwendet, um die rd. 250 Wörter zu finden, die in dieser Zeit den größten Anstieg verzeichnen. Eine manuelle Sichtung dieser Wortliste zeigte den Einfluss mehrerer Textsorten. Vor allem aber fiel der einzige Name in der Liste auf: GRIN. Es handelt sich um eine deutsche Verlagsgruppe, zu der neben dem GRIN-Verlag selbst u.a. auch die Webseite hausarbeiten.de gehört. Der Verlag, der von Beobachtern als ""vanity publisher"" oder ""predatory publisher"" (Shrestha 2021) eingeschätzt wird, publiziert alle Texte digital oder als Book on Demand. Eine ""Lektorierung findet nicht statt"" (Wikipedia). Laut Verlagswebseite wurden bis ins Jahr 2018 200.000 Texte publiziert. Seitdem sind schätzungsweise mindestens weitere 40.000 Titel publiziert worden.  Die Texte des GRIN Verlags haben zwar alle eine ISBN-Nummer, aber die meisten sind deutlich kürzer als Bücher im herkömmlichen Sinn, viele sind Aufsätze, die nur 20-30 Seiten lang sind. Beide Faktoren, die Unmenge an quasi-wissenschaftlichen kurzen Texten des GRIN-Verlags und die falsch datierten Heftromane führen dazu, dass die durchschnittliche Länge von Büchern im Ngramm-Korpus sich seit 2000 deutlich verringert hat (siehe Fig. 10), allerdings sind die Werte seit 2017 fast wieder auf dem alten Niveau:  Fassen wir zusammen: Nach 1998 ändert sich die Zusammensetzung des deutschsprachigen Ngramm-Korpus einschneidend, so dass es für die meisten Analysen zur Entwicklung von Sprache und Kultur weitgehend unbrauchbar wird. Dazu tragen eine Reihe von Faktoren bei, von denen zwei identifiziert werden konnten: Schwerwiegender ist, schon aus Umfanggründen, der Anteil der Publikationen des GRIN-Verlags. Sie geben zwar einen Einblick in eine bestimmte Form universitärer Wissenschaftskommunikation, haben aber nichts mit der sonstigen Buchproduktion zu tun. Hinzukommen die falsch datierten Retrodigitalisierungen einiger Verlage. Zugleich zeigt die Analyse der Daten, dass dies nicht die einzigen Faktoren sind, die hier ins Gewicht fallen. Wenn man sich auf die Wörter mit den steilsten Karrieren in den letzten 25 Jahren konzentriert (Fig. 11), dann fällt auf, dass dies allgemeine Token sind, die sich eher in Romanen als in Fachtexten finden (besonders die Anführungszeichen, mit denen in den meisten deutschen Drucktexten direkte Rede markiert wird): ""Augen Blick Du Frau Gesicht Hand Kopf Leben Mal Mann Moment Mutter Stimme Tag Tür Vater ¬´ ¬ª"" Da zugleich die Länge ansteigt und die Anzahl der Texte sehr hoch bleibt, außerdem diese Texte aber wohl nicht in den offiziellen Verlagsstatistiken auftauchen, handelt es sich vermutlich um Texte aus literarischen An diese explorative Studie könnte nun eine Untersuchung anschließen, die die Veränderungen der Korpuszusammensetzung als überdurchschnittlich starke Veränderung der Token-Verteilungen formalisiert (Koplenig 2017) und so den hier etwas vernachlässigten Aspekt, wann sich genau die Veränderungen ergeben, herausarbeitet. Die ""typische"" Verwendung des Ngramm-Korpus und -viewers, nämlich die Untersuchung der Verwendungshäufigkeit von Termen in der schriftlichen Öffentlichkeit, ist durch die starken Schwankungen in der Zusammensetzung des Korpus sehr fragwürdig geworden. Da nach 2000 sonst eher randständige Bereiche, quasi-wissenschaftliche Texte und die Produktion der selbstverlegten Autorinnen und Autoren, das Korpus dominieren, ist es auch für rein sprachanalytische Untersuchungen, etwa zur Kollokationsanalyse, kaum verwendbar. Aber insgesamt verdient die Frage, ob und unter welchen Vorzeichen die Daten nicht doch für bestimmte Analysen herangezogen werden können, eine genauere Untersuchung. ",de,Google Version publizieren Digital schnell positiv negativ Aspekt Werkzeugs analysieren Möglichkeit begriffsgeschichtlich Forschung sprunghaft erweitern begleitend Aufsatz Michel deutlich belegen Michel et teilweise naiv Umgang Quellenmaterial deutlich autorinn Autor Bewusstsein möglich Falle korpusbasiert Forschung wechselnd Zusammensetzung zugrundeliegend Textsammlung Unmöglichkeit dahinterliegend Text zuzugreifen fehlen metadaten Text falsch Jahreszahl wiederholt vorwerfen underwood Arbeit Pechenick et zeigen zunehmend Menge wissenschaftlich Literatur Korpusanteile Hälfte Jahrhundert merklich verschieben unklar bleiben gesellschaftlich Entwicklung reflektieren keineswegs Manko betrachten einschlägig Arbeit koplenig Veränderung Korpuszusammensetzung weltkriegs untersuchen The German Gb Corpus Strongly Biased Toward volumes Published Switzerland During Wwii koplenig Google groß Updates vorlegen zugrundeliegend Textmenge deutlich erweitern Entwicklung Umfang deutschsprachig korpora folgend Beschränk Anzahl digitalisierter büch deutlich steigern gesamt Zeitraum Sinn Kollegin Frage literarisch Kanonisierung arbeiten viewer verwenden Analyse Ergebnis fallen schnell Name Reihe hochkanonisch deutschsprachig Autor Thomas Mann Goethe Schiller Kafka Brecht auffällig Abwärtstrend aufweisen sehen fig bevor allgemeiner kulturanalytisch These Bildungskultur formulieren Solidität daten prüfen korpus möglich Bia untersuchen weder Liste Text Text vorliegen Reihe generisch Metadat Gramm ngrammen lässen Widerspruch Ergebnis erklären Rohdate fig daten Anzahl Tok pro normalisieren ergeben Trend sichtbar Wert stürzen steil steil Anstieg Buchzahle fig ziemlich überraschend wissen Google Books Ergebnis Digitalisierungskampagne Google Kooperation Reihe international Forschungsbibliothek beruhen deutschsprachig Ergebnis verdanken zuletzt Kooperation österreichisch Nationalbibliothek bayerisch Staatsbibliothek Beständ speisen letzter Jahrzehnt Pflichtabgabeexemplar umfassend Erwerbspolitik woher Plötzliche Anstieg Klage Verlage trotz büch drucken Buchproduktion quellen verwenden daten Digitaler Form anbieten Statista kommerziell Datenanbieter Zahl Börsenverein deutsch buchhandels Anzahl Neuerscheinung aufbereiten börsenverein Grafik enthalten Wert aktuell Korpus Wert früh Google deutlich beginnen Wert auffällig Google letzter Update Anteil Buchproduktion Jahr digitalisieren vorliegen deutlich steigern Mitte teils Korpus enthalten Zeitraum spät spät stagnieren Wert Korpora Buchproduktion steil ansteigen erstaunlich Rahlf publiziert büch Korpus liegen wachsen Anteil Korpora geschehen relativ langsam Erweiterung deutlich stark zulegen springen Wert steil gelten Stufe korpus Anstieg deutlich ausgeprägt verhalten Wert Korpus hoch Niveau parallel Wert Buchproduktion fallen sogar ändern wiederum Anstieg beobachten diesmal sogar Niveau Buchproduktion übertreffen Grafik blicken sehen Rohwerte Goethe kanonisiert Autor spät deutlich ansteigen normalisiert Darstellung abfallen bedeuten Phase Text hinzufügen zusammensetzung bisherig korpus Text hierbei handeln erwähnt verschieden Phase Veränderung legen nahe vermuten einzeln Eingriff korpus Reihe verschieden Texthinzufügung beobachtet phänomen Beding Projekt Analyse Heftroman auffallen einschlägig Verlage Komplexität Feststellung richtig metadaten insbesondere Publikationsdatum entledigen einfach retrodigitalisiert Text Datum veröffentlichen digital Kopie publizieren testen Text entsprechend Serienautor suchen deutlich Steigerung sprechen Retrodigitalisate jeweilig aufführen tausend Heftroman alleine verantwortlich Faktor erkennen offensichtlich Weg sprachlich Material Text bestimmt Wort relativ Erhöhung führen Wort relativ Rückgang aufweisen Google daten Wortklasseninformation ausliefert einfach Substantiv extrahieren rd Mio anschließend Substantiv herausgefiltern jeder auftauchen insgesamt mindestens Mal restlich rd wörtern daten jeder Wort linear Regression berechnen Steigung gerade Filterungsfaktor verwenden rd Wörter finden groß Anstieg verzeichnen Manuelle Sichtung Wortliste zeigen einfluss mehrere Textsort fallen einzig Name Liste Grin handeln deutsch Verlagsgruppe webseit gehören Verlag Beobachter vanity publish predatory publish Shrestha einschätzen publizieren Text Digital Book -- Demand Lektorierung finden wikipedia laut Verlagswebseite Text publizieren schätzungsweise mindestens Titel publizieren Text Grin Verlag meister deutlich kurz büch herkömmlich Sinn Aufsatz Seite Faktor unmeng kurz Text falsch datiert heftromane führen durchschnittlich Länge büchern deutlich verringern sehen fig Wert fast alt Niveau fassen ändern Zusammensetzung Deutschsprachig einschneidend meister Analyse Entwicklung Sprache Kultur weitgehend unbrauchbar tragen Reihe Faktor identifizieren schwerwiegend Umfanggründ Anteil Publikation geben Einblick bestimmt Form universitär Wissenschaftskommunikation sonstig Buchproduktion hinzukommen falsch datiert Retrodigitalisierung Verlage zeigen Analyse daten einzig Faktor Gewicht fallen Wörter steil karrieren letzter konzentrieren fig fallen allgemein token eher Roman Fachtexte finden anführungszeich meister deutsch drucktexten direkt Rede markieren Auge Blick Frau Gesicht Hand Kopf leben mal Mann Moment Mutter Stimme Tür Vater Länge ansteigt Anzahl Text bleiben Text offiziell verlagsstatistiken auftauchen handeln vermutlich Text literarisch explorativ Studie Untersuchung anschließen Veränderung Korpuszusammensetzung überdurchschnittlich stark Veränderung formalisieren koplenig vernachlässigt Aspekt genau Veränderung ergeben herausarbeiten typisch Verwendung nämlich Untersuchung Verwendungshäufigkeit Terme schriftlich Öffentlichkeit stark Schwankung Zusammensetzung Korpus fragwürdig eher randständig bereich Text Produktion selbstverlegten autorinnen Autor Korpus dominieren rein sprachanalytisch Untersuchung Kollokationsanalyse verwendbar insgesamt verdienen Frage Vorzeichen daten bestimmt analysen heranziehen genau Untersuchung,"[('buchproduktion', 0.23204870488862206), ('steil', 0.19337392074051835), ('google', 0.19160643212828948), ('deutlich', 0.18531931555078365), ('wert', 0.17153284519732254), ('anstieg', 0.16903544805893117), ('fig', 0.1477522053216375), ('text', 0.14097505243648348), ('publizieren', 0.13459559653931155), ('korpus', 0.13191349581076514)]"
2023,DHd2023,SCHENK_Nicolas_Vom_Heben_verborgener_Schätze___Literarische_.xml,Vom Heben verborgener Schätze 'Literarische Blogs als Ressource,"Nicolas Schenk (Deutsches Literaturarchiv Marbach); André Blessing (Universität Stuttgart, Institut für Maschinelle Sprachverarbeitung); Pascal Hein (Universität Stuttgart, Institut für Literaturwissenschaft); Jan Hess (Deutsches Literaturarchiv Marbach); Kerstin Jung (Universität Stuttgart, Institut für Maschinelle Sprachverarbeitung); Claus-Michael Schlesinger (Universität Stuttgart, Institut für Literaturwissenschaft)","Literarische Blogs, Aufbereitung, Ressource, Korpus, WARC","Sammlung, Strukturanalyse, Bereinigung, Literatur, Metadaten, Text","Bereits seit Ende der 90er Jahre gewannen Weblogs als Medium zur öffentlichen Darstellung unterschiedlicher Themen und Inhalte immer mehr an Popularität. Findige Literatur- und Kulturschaffende zögerten nicht lange, um das neue Medium auch für literarische Zwecke umzunutzen. In dem Beitrag zur Jahrestagung der DHd 2022 (Blessing et al 2022) haben die Autor:innen des vorliegenden Beitrags bereits am Beispiel des u. a. von der Autorin Kathrin Passig ins Leben gerufenen Techniktagebuch Typische Bausteine in Blogs sind Blogposts als (meist datierte) Inhaltseinheiten verschiedenster Länge und unter Verwendung unterschiedlicher Modalitäten wie Text, Bild, Animation, Video, Referenz (z. B. Hyperlinks), etc. Weiterhin finden sich auf der Ebene der Blogposts oft Kommentarformulare und Kommentare sowie eine Etikettierung von Einträgen in Form von Tags, die der Gruppierung und Beschreibung der Einträge dienen und sich auf Themen, Stimmungen, Autoren, etc. des Blogposts beziehen können (vgl. zu den Bausteinen von Blogs: Ernst 2010, 286f.).¬†Zusätzliche Elemente wie Übersichtsseiten, die als Einstiegsseiten der jeweiligen Blogs die einzelnen Blogposts (zusätzlich) in einer bestimmten Reihenfolge auflisten, oder Archivseiten, die den Nutzer:innen einen Zugang zu älteren Blogposts ermöglichen sollen, prägen die Struktur der Blogs und damit ggf. deren Analyse.  Die Spiegelungen der Blogs, die am DLA durchgeführt werden, erfolgen zunächst mit einem Crawling-Vorgang, der der Hyperlinkstruktur im Blog folgt und die clientseitig (wie durch einen Browser) empfangenen Daten gemeinsam mit einigen Metadaten zum Crawlingprozess im Web ARChive- oder kurz: WARC-Format ablegt. Das aus dem ARC-Format des Internet Archive weiterentwickelte Archivformat hat sich inzwischen als internationaler Standard für die Archivierung von Webinhalten etabliert¬†(IIPC, n.d.).¬†Beim Crawling können Inhalte, die ggf. nicht Teil des Blogs sind, wie z. B. zufällig eingebundene Werbeanzeigen oder externe Inhalte, auf die von diesen Werbeanzeigen verwiesen wird, Teil des Archivobjekts werden. Aber auch bezüglich der tatsächlichen Blog-Elemente sind im Archivobjekt Inhalte (z. B. bestimmte Textpassagen) so oft abgelegt, wie der Crawler ihnen auf verschiedenen Seiten begegnet ist. Der Inhalt eines Blogposts kann an verschiedenen Stellen für den Crawler erreichbar sein: auf der Übersichtsseite, auf der eigenen Seite des Posts, auf der Seite jedes Schlagworts, mit dem der Post versehen ist, im Archiv, etc. Aber auch Textpassagen aus Strukturelementen wie Kopf- und Fußzeilen führen zu mehrfachen Vorkommen von Textpassagen oder Begriffen. Werkzeuge, die Strukturelemente ausblenden und (Text-)Duplikate erkennen, sind daher bei der Vorverarbeitung der Daten für Analysen notwendig, arbeiten oft aber statistisch und müssen ggf. auf jedes zu untersuchende Blog neu angepasst werden, was im Spannungsfeld mit dem maschinell unterstützten Distant Reading steht. Im Folgenden wird daher das Vorgehen bei der Aufbereitung der Blogs beschrieben, das notwendig ist, um die Texte auch für quantitative Analysen verfügbar zu machen. Die am DLA archivierten Blogs sind nicht nur inhaltlich und stilistisch sehr heterogen, sondern auch in Bezug auf die technische Umsetzung. In den meisten Fällen werden bekannte Blog-Hoster wie wordpress (40%), twoday (15%), blogger, blogspot usw. verwendet. Diese Blog-Hoster wiederum setzen Content Management Systeme (CMS) ein, die die Blogpost-Erstellung, -Verwaltung sowie die Blogdarstellung sowohl für die Blogverfasser:innen, als auch für die Blogleser:innen vereinfachen. Für die Analyse des Inhalts eines Blogs sind nur die Blogposts relevant, da alle Übersichtsseiten (Home, Tags, Categories) eines Blogs automatisch daraus generiert werden. Darüber hinaus hat sich gezeigt, dass beim Erstellen von WARC-Crawls auch einiges an ""Beifang"", also Webseiten, die nicht zum Blog gehören, aber für das Abspielen teilweise nützlich sein könnten, mit-archiviert werden, was wiederum bei der Aufbereitung der Blogs beachtet werden muss. Die hier beschriebene Umsetzung zielt darauf ab, ein sauberes Textkorpus für jedes Blog zu extrahieren und alle irrelevanten und redundanten Inhalte zu ignorieren. Bei der Aufbereitung werden Verfahren aus dem Information Retrieval eingesetzt (vgl. Manning, Raghavan und Schütze 2008, 443–459). Das Besondere am aktuellen Datensatz liegt darin, dass die Daten bereits im WARC-Format vorliegen. Cormack, Smucker und Clarke (2011) haben bereits gezeigt, wie bei sehr großen Web-Crawls Die Umsetzung läuft in mehreren Schritten: In Schritt 1 werden alle HTML-Seiten des Blogs aus den WARCs extrahiert und erkannte Fremdinhalte entfernt. Schritt 2 erkennt das verwendete CMS-System. Schritt 3 basiert auf einem Regelsystem, das für jedes Blog die Posts ermittelt.¬†Die URL-Pfade der Blogs sind ein Hauptmerkmal, welches in den Regelsystemen verwendet wird. In Schritt 4 werden zu jedem Post Text- und Metadaten mittels trafilatura Da in allen vier dieser Umsetzungsschritte Anpassungen notwendig sind, gehen Weiterentwicklung und Optimierung der Aufbereitung immer einher mit dem Einsatz von Analysewerkzeugen. Bei der Verwendung solcher Werkzeuge können systematische Auffälligkeiten sichtbar werden, wodurch beispielsweise noch vorhandene Redundanzen zum Vorschein kommen. Ein Werkzeug, mit dem große Textkorpora durchsucht und auf grammatikalische sowie syntaktische Strukturen hin untersucht werden können, ist CQPweb  Das Durchsuchen und Abspielen von WARC-Dateien wird durch die Web-Applikation SolrWayback  Der Erfolg bei der Aufbereitung lässt sich anhand der Zahlen in Tabelle 1 ablesen. Hier wird auch die Heterogenität der Blogs sichtbar: Die Aufbereitungsschritte haben je nach Blog unterschiedlich starke Auswirkungen. Während der Unterschied der Anzahl von WARC-Records und HTML-Seiten deutlich macht, wie viele externe Inhalte und Nicht-HTML-Inhalte entfernt werden müssen, zeigt sich an der Differenz zwischen der Anzahl an HTML-Seiten und der Anzahl an Blogposts, wie hoch die Redundanz durch generierte Übersichtsseiten ausfällt. Die Zahlen zur Ressourcengröße aus Tabelle 1 veranschaulichen, dass die Aufbereitung der Blogs nicht vollständig manuell validiert werden kann. Trotzdem ist eine Aussage über die Qualität der aufbereiteten Blog-Daten wichtig. Zu diesem Zweck wurde ein Annotations-Jupyter-Notebook entworfen, mit dessen Hilfe automatisch aus den jeweiligen CMS-Familien wie beispielsweise Wordpress oder Blogger repräsentative Datenmengen entnommen werden, die anschließend von mehreren Annotator:innen bewertet werden. Im ersten Schritt geht es um das Erkennen der Blog-Posts in Abgrenzung zu Übersichtsseiten oder weiteren Seiten wie beispielsweise dem Impressum. Wenn es sich bei der Seite um einen Post handelt, dann wird im zweiten Schritt die Qualität der Metadaten- und Textextraktion bewertet. Dabei wird u. a. darauf geachtet, ob Datum und Überschrift richtig extrahiert wurden und ob der extrahierte Text vollständig ist oder beispielsweise Teile aus der Seitennavigation enthalten sind. Es hat sich gezeigt, dass die beschriebene manuelle Bewertung auch für Menschen nicht immer trivial ist. Daher ist der Annotationsprozess aktuell noch nicht abgeschlossen. Die Evaluationsergebnisse werden jedoch mit dem Release der Daten bereitgestellt. In Blessing et al (2022) wurde am Fallbeispiel des Techniktagebuchs bereits exemplarisch aufgezeigt, welche komplexeren computergestützten Analysen durch die Verwendung der extrahierten Text- und Metadaten vorgenommen werden können. Unter anderem wurde bereits untersucht, welche Zusammenhänge zwischen Inhalt 30 Millionen Zeichen, 140.000 Blogposts, über 200 Blogs: Schon wegen seiner Größe ist das hier vorgestellte, aufbereitete Korpus für viele Bereiche der Digital Humanities, beispielsweise die Computational Literary Studies, die Digital History oder für NLP-Untersuchungen, eine wichtige Quelle für Inhaltsanalysen oder das Trainieren von Sprachmodellen. Wie in diesem Beitrag gezeigt stellt vor allem die Struktur der Weblogs eine Herausforderung dar, enthalten die WARC-Dateien, in denen die Blogs zunächst vorliegen, doch sehr viel Redundantes, das für eine Vielzahl von Inhaltsanalysen nicht nur uninteressant, sondern sogar hinderlich ist. Mit den im Zuge der Veröffentlichung der SDC4Lit-Plattform 2023",de,gewann Weblogs Medium öffentlich Darstellung unterschiedlich Them inhalen Popularität findig Kulturschaffend zögerten Medium literarisch Zweck umzunutzen Beitrag Jahrestagung dhd blessing et -- Autor innen vorliegend Beitrag Autorin Kathrin passig Leben gerufen Techniktagebuch typisch Baustein Blogs Blogpost meist datiert inhaltseinheit verschiedenst Länge Verwendung unterschiedlich Modalität Text Bild Animation Video Referenz hyperlinks weiterhin finden Ebene blogposts kommentarformular Kommentar Etikettierung Einträg Form tags Gruppierung Beschreibung eintrag dienen Thema Stimmung Autor Blogpost beziehen Baustein Blogs Ernst element übersichtsseiten Einstiegsseit jeweilig Blogs einzeln Blogpost zusätzlich bestimmt Reihenfolge auflisen archivseiten Nutzer innen Zugang alt Blogpost ermöglichen prägen Struktur blogs Analyse Spiegelunge blogs dla durchführen erfolgen Hyperlinkstruktur Blog folgen clientseitig Browser empfangen daten gemeinsam Metadat crawlingprozess Web ablegen Internet archiven weiterentwickelt Archivformat inzwischen international Standard Archivierung webinhalt Crawling inhalen Blog zufällig eingebunden Werbeanzeig extern Inhalt werbeanzeig verweisen Archivobjekt bezüglich tatsächlich Archivobjekt inhalt bestimmt Textpassagen ablegen Crawler verschieden Seite begegnen Inhalt Blogpost verschieden Stelle Crawler erreichbar Übersichtsseite Seite Post Seite jeder Schlagwort Post versehen Archiv Textpassagen Strukturelement Fußzeile fahren mehrfach Vorkomm Textpassagen begreifen Werkzeug strukturelement ausblenden erkennen Vorverarbeitung daten Analyse notwendig arbeiten statistisch jeder untersuchend Blog neu angepasst Spannungsfeld maschinell unterstützen distant Reading stehen folgend vorgehen Aufbereitung blogs beschreiben notwendig Text quantitativ Analyse verfügbar dla archiviert blogs inhaltlich stilistisch heterogen Bezug technisch Umsetzung meister Fall bekannt wordpress Twoday Blogger blogspot verwenden wiederum setzen Content Management System cms Blogdarstellung sowohl Blogverfasser innen Blogleser innen vereinfachen Analyse inhalts blogs blogposts relevant übersichtssein home tags categories blogs automatisch neriern hinaus zeigen Erstell beifang webseien Blog gehören abspielen teilweise nützlich können wiederum Aufbereitung blogs beachten beschrieben Umsetzung zielen sauber Textkorpus jeder Blog extrahieren Irrelevant redundanen Inhalt ignorieren Aufbereitung Verfahren Information Retrieval einsetzen Manning Raghavan schützen besonderer aktuell Datensatz liegen daten vorliegen Cormack Smucker Clarke zeigen Umsetzung laufen mehrere Schritt Schritt blogs Warcs extrahiert erkennen Fremdinhalte entfernen Schritt erkennen verwendet Schritt basieren regelsyst jeder Blog posts Blogs Hauptmerkmal Regelsysteme verwenden Schritt Post Metadat mittels Trafilatura Umsetzungsschritte Anpassung notwendig Weiterentwicklung Optimierung Aufbereitung einher Einsatz Analysewerkzeug Verwendung Werkzeug systematisch Auffälligkeit sichtbar wodurch beispielsweise vorhanden Redundanze Vorschein Werkzeug Textkorpora durchsuchen grammatikalisch syntaktisch Struktur untersuchen cqpweb durchsuchen abspielen Solrwayback Erfolg Aufbereitung lässen anhand Zahl Tabelle ablesen Heterogenität blogs sichtbar Aufbereitungsschritte Blog unterschiedlich stark Auswirkung Unterschied Anzahl deutlich extern inhalt entfernen zeigen Differenz Anzahl Anzahl Blogpost Redundanz Generierte übersichtsseiten ausfallen Zahl Ressourcengröße Tabelle veranschaulichen Aufbereitung blogs vollständig manuell validieren Aussage Qualität aufbereitet wichtig zweck entwerfen Hilfe automatisch jeweilig beispielsweise wordpress Blogger Repräsentative datenmenger entnehmen anschließend mehrere Annotator innen bewerten Schritt erkennen Abgrenzung übersichtsseiten Seite beispielsweise Impressum Seite Post handeln Schritt Qualität Textextraktion bewerten achten Datum überschrifen extrahiern extrahiert Text vollständig beispielsweise Teil Seitennavigation enthalten zeigen beschrieben Manuelle Bewertung Mensch trivial annotationsprozess aktuell abschließen Evaluationsergebnis Release daten bereitstellen Blessing et al Fallbeispiel Techniktagebuch exemplarisch aufzeigen komplex Computergestützt Analyse Verwendung extrahiert Metadat vornehmen untersuchen zusammenhänge Inhalt Million Zeichen Blogpost blogs Größe vorgestellten aufbereiten Korpus Bereich Digital Humanitie beispielsweise computational literary Studie Digital History wichtig Quelle Inhaltsanalys Trainieren sprachmodellen Beitrag zeigen stellen Struktur Weblogs Herausforderung dar enthalten blogs vorliegen redundant Vielzahl Inhaltsanalyse uninteressant sogar hinderlich Zug Veröffentlichung,"[('blogs', 0.6143565513353516), ('blog', 0.2503491165667481), ('blogpost', 0.2503491165667481), ('aufbereitung', 0.1426632225412382), ('inhalt', 0.12755412821552506), ('post', 0.12505121071124228), ('übersichtsseiten', 0.11519185337537842), ('seite', 0.0990495407000439), ('schritt', 0.09780211373893327), ('innen', 0.09724516396629183)]"
2023,DHd2023,GIOVANNINI_Luca_Onboard_onto_DraCor.xml,Onboard onto DraCor.   Prototyping Workflows to Homogenize Drama Corpora for an Open Infrastructure,Ingo Börner (Universität Potsdam); Frank Fischer (Freie Universität Berlin); Luca Giovannini (Universität Potsdam); Christopher Lu (University of Oxford); Carsten Milling (Universität Potsdam); Daniil Skorinkin (Universität Potsdam); Henny Sluyter-Gäthje (Universität Potsdam); Peer Trilcke (Universität Potsdam),"corpus, drama, onboarding","Annotieren, Bearbeitung, Community-Bildung, Kollaboration, Literatur, Text","Comparative endeavors in Computational Literary Studies typically require corpora which are both diverse, i.e., including texts in different languages and from different sources, and homogenized, i.e., formal and structural consistent. One way to tackle this issue is to establish upstream internal guidelines, such as the ones developed within the ELTeC initiative (Schöch et al. 2021). DraCor, based on the concept of Programmable Corpora (Fischer et al. 2019), is an open platform as well as a growing network for hosting, accessing, and analyzing theater plays. DraCor relies on the general TEI model for dramatic texts, with minimal enhancements, and thus facilitates contributions by external scholars who want to onboard their corpora onto its ecosystem. Once integrated, corpora can benefit from the platform""s APIs and services, ranging from the computation of network metrics via various extraction functions to SPARQL queries. Typically, corpora for DraCor are not built from scratch, but are created either by aggregating formally heterogeneous texts from different sources or by transforming existing corpora. Unlike in ELTeC, the homogenization of texts for DraCor usually does not stand at the beginning of the corpus creation process, but is rather an intervention in existing corpora which are sometimes subject to amendment and growth, hence ""living"". This approach poses a number of challenges, for which we are currently prototyping several workflows. Here, we present the pipelines for mounting to DraCor two new corpora: the English-language From a technical point of view, onboarding corpora onto DraCor is a series of automated and manual transformations of the source data, which depend crucially on the format and markup of the files. Texts from a single, homogeneous collection with pre-existing markup and metadata will require different workflows and pipelines than those coming, for example, from a variety of raw text sources. This heterogeneous point of departure is what shapes our onboarding approach. Consequently, we are developing a modular workflow made up of a set of demand-dependent components. In addition to guideline-based manual revisions (e.g. pre-structuring texts with Markdown), we use XSLT scripts for automated transformations. Edits specific to theater plays, such as the task of speaker identification, are supported by an Oxygen framework; A particular challenge is posed by living corpora. Here, the manual transformations performed during onboarding should be reapplicable in case of edits to the source data. Accordingly, we implemented routines for a ""backward compatibility"" of the markup: the changes made by us during onboarding can later be applied again to a newer version of the source files. To develop our workflows and pipelines, two corpora with very different requirements are currently in the process of onboarding. While UDraCor originates from a growing collection of heterogeneous sources, EPDraCor is based on semantically rich TEI files from the Early Print project. Due to the heterogeneity of the sources, a more case-specific solution must be found for UDraCor in this initial step. Here, the conversion is a semi-automatic procedure with heavy use of string patterns and regex. At the same time, UDraCor takes a community-based corpus-building approach by inviting scholars specializing in Ukrainian studies to work on both technical and content-related tasks. This work on UDraCor once again shows how the technical task of corpus building and community activities are crucially intertwined.",en,comparative endeavor computational literary study typically require corpora diverse include text different language different source homogenize formal structural consistent way tackle issue establish upstream internal guideline one develop eltec initiative schöch et al dracor base concept programmable corpora fischer et al open platform grow network hosting accessing analyze theater play dracor rely general tei model dramatic text minimal enhancement facilitate contribution external scholar want onboard corpora ecosystem integrate corpora benefit apis service range computation network metric extraction function sparql query typically corpora dracor build scratch create aggregate formally heterogeneous text different source transform exist corpora unlike eltec homogenization text dracor usually stand beginning corpus creation process intervention exist corpus subject amendment growth live approach pose number challenge currently prototype workflow present pipeline mount dracor new corpora english language technical point view onboarde corpora dracor series automated manual transformation source datum depend crucially format markup file text single homogeneous collection pre existing markup metadata require different workflow pipeline come example variety raw text source heterogeneous point departure shape onboarding approach consequently develop modular workflow set demand dependent component addition guideline base manual revision pre structuring text markdown use xslt script automate transformation edit specific theater play task speaker identification support oxygen framework particular challenge pose live corpora manual transformation perform onboarding reapplicable case edit source datum accordingly implement routine backward compatibility markup change onboarding later apply new version source file develop workflow pipeline corpora different requirement currently process onboarding udracor originate grow collection heterogeneous source epdracor base semantically rich tei file early print project heterogeneity source case specific solution find udracor initial step conversion semi automatic procedure heavy use string pattern regex time udracor take community base corpus building approach invite scholar specialize ukrainian study work technical content relate task work udracor show technical task corpus building community activity crucially intertwine,"[('corpora', 0.32370053258216736), ('source', 0.26682717874012013), ('dracor', 0.24718818529992534), ('udracor', 0.2090947540510439), ('onboarding', 0.2090947540510439), ('heterogeneous', 0.15682106553828293), ('different', 0.15536967128520698), ('file', 0.13843675569254849), ('technical', 0.12768262383115508), ('base', 0.1259408679237798)]"
2023,DHd2023,FLÜH_Marie_Analyse__Produktion__Reflexion__Nachnutzungsszena.xml,"Analyse, Produktion, Reflexion: Nachnutzungsszenarien für Forschungsdaten am Beispiel der Daten des Projekts Dehmel digital","Sandra Bläß (Universität Hamburg, Deutschland); Marie Flüh (Universität Hamburg, Deutschland); Julia Nantke (Universität Hamburg, Deutschland); Christian Reul (Universität Würzburg, Deutschland)","Digitale Edition, HTR, Named Entity Recognition, Briefanalyse","Teilen, Datenerkennung, Sammlung, Bilderfassung, Bearbeitung, Daten","Das Ziel wissenschaftlicher Editionen besteht seit jeher in der Nachnutzung durch die (wissenschaftliche) Community. Unter den Vorzeichen von Open Data und Open Science ändern sich allerdings die Möglichkeiten der Bereitstellung und Nachnutzung des erschlossenen Materials. Gleichzeitig haben Wissenschaftler:innen, die mit Methoden der Digital Humanities arbeiten, andere Anforderungen und Bedarfe an bereitgestellte Daten z.B. im Hinblick auf den Umfang der Korpora und die spezifischen Datentypen. Ziel unseres Beitrags ist es, anhand der unterschiedlichen, von uns im digitalen Editions- und Forschungsprojekt Wir beziehen uns auf folgende Datentypen: 1) Metadaten von Briefen unterschiedlicher Schreibender aus dem Korrespondenznetz von Ida und Richard Dehmel, 2) digitale Bilder der Dokumente, 3) maschinenlesbarer Text der Briefe, 4) Annotationen von Entitäten sowie 5) algorithmische Modelle. In Abhängigkeit vom jeweiligen Datentyp ergeben sich unterschiedliche Nachnutzungsszenarien. Diese reichen von dem aus editorischer Sicht klassischen Szenario der Nutzung der bereitgestellten Daten in (literatur)wissenschaftlichen Analysen über die Nutzung zur Produktion eigener Korpora und Modelle, die dann wiederum Gegenstand der Nachnutzung werden können, bis hin zur Algorithmen-gestützten Reflexion der konzeptuellen Grundlagen einer solchen Datensammlung. Szenario 1: Analyse von Briefinhalten auf der Basis von Datentyp 3 Briefe stellen relevante Quellen für die Rekonstruktion historischer Diskurse dar (Baillot 2011). Diese in einem Gesamtüberblick und nicht nur in Einzelbeispielen zu erfassen, ist mittels Close Reading-Verfahren kaum zu bewältigen. Ein zentrales computergestütztes Nachnutzungsszenario für unsere Daten sind daher Analysen mittels Distant Reading-Verfahren. Auf der Basis der erzeugten Transkripte kann u.a. eine automatisierte Exploration der zentralen Briefinhalte über Topic Modeling umgesetzt werden (Andorfer 2017; Henny-Kramer/Neuber 2023). Ergänzend hierzu lassen sich z.B. stimmungsmäßige Gewichtungen in den Briefen durch Sentiment Analysis ermitteln und zu den Topics ins Verhältnis setzen. Szenario 2: Korrespondenznetze sichtbar machen auf der Basis von Datentyp 4 In den im Projekt Szenario 3: Vernetzung mit anderen Briefeditionen auf Basis von Datentyp 1 und 3 Die von uns erzeugten Briefmetadaten können über die Plattform Szenario 4: Texte erschließen auf der Basis von Datentyp 5 Neben den erschlossenen Dokumenten stellen wir auch die im Projekt von uns trainierten HTR- und NER-Modelle zur Nachnutzung zur Verfügung. Auf Basis dieses Datentyps können weitere Dokumente, die nicht Teil des Projekts sind, erschlossen und somit neue Daten für die weitere Nachnutzung produziert werden. Dies gilt zum einen für handschriftliche Dokumente der Schreibenden, für die wir HTR-Modelle trainiert haben (z.B. Stefan Zweig, Detlev v. Liliencron, Julie Wolfthorn). Zum anderen können die Named Entity-Classifier für die Erschließung weiterer deutschsprachiger Briefe aus einem ähnlichen Zeitraum genutzt werden. Es besteht auch die Möglichkeit, auf der Basis unserer Trainingsdaten spezifische Modelle für andere Anwendungsfälle nachzutrainieren (Flüh/Lemke 2022). Szenario 5: gemischte HTR-Modelle trainieren auf Basis von Datentyp 2 und 3 Die in Szenario 6: Reflexion der theoretischen Fundierung von Datensammlungen auf Basis von Datentyp 5 Unsere NER-Classifier wurden auf den Dokumententyp ""Brief um 1900"" trainiert. Eine experimentelle Anwendung z.B. des Orte-Classifiers auf ein Korpus mit Texten eines deutlich abweichenden Dokumententyps (z.B. fiktionale Texte) kann insbesondere in Kombination mit einem auf den Dokumententyp zugeschnittenen Classifier dazu beitragen, die theoretisch-konzeptuelle Fundierung offenzulegen, welche in die Modellierung des Classifiers eingegangen ist, indem die Ergebnisse der Classifier vergleichend betrachtet werden (vgl. dazu die Fallstudie von Flüh/Schumacher/Nantke im Erscheinen).",de,Ziel wissenschaftlich Edition bestehen jeher Nachnutzung wissenschaftlich Community Vorzeichen Open Data open Science ändern Möglichkeit Bereitstellung Nachnutzung erschlossen Material gleichzeitig Wissenschaftler innen Methode Digital Humanitie arbeiten anforderungen Bedarfe bereitgestellt daten Hinblick Umfang Korpora spezifisch Datentyp Ziel unser Beitrag anhand unterschiedlich digital forschungsprojekt beziehen folgend datentypen metadaen Briefe Unterschiedlicher schreibend Korrespondenznetz ida Richard Dehmel digital Bild dokument Maschinenlesbarer Text Brief annotatio entität algorithmisch Modell Abhängigkeit jeweilig Datentyp ergeben unterschiedlich Nachnutzungsszenarie reichen editorisch Sicht klassisch Szenario Nutzung bereitgestellt daten Literatur wissenschaftlich Analyse Nutzung Produktion Korpora Modelle wiederum Gegenstand Nachnutzung Reflexion konzeptuell Grundlage Datensammlung szenario Analyse Briefinhalt Basis Datentyp Brief stellen relevant quellen Rekonstruktion historisch Diskurse dar baillot Gesamtüberblick einzelbeispielen erfassen mittels clos bewältigen zentral computergestütztes Nachnutzungsszenario daten Analyse mittels distant Basis erzeugt transkripte automatisiert Exploration zentral briefinhalte Topic Modeling umsetzen Andorfer neuber ergänzend hierzu lassen stimmungsmäßig Gewichtunge Brief Sentiment Analysis ermitteln Topics Verhältnis setzen szenario korrespondenznetze sichtbar Basis datentyp Projekt szenario Vernetzung Briefedition Basis Datentyp erzeugt briefmetadaten Plattform Szenario Text erschließen Basis Datentyp erschlossen dokument stellen Projekt trainierten Nachnutzung Verfügung Basis Datentyps Dokument Projekt erschließen somit daten Nachnutzung produzieren gelten handschriftlich dokumente schreibend trainieren Stefan Zweig Detlev Liliencron Julie Wolfthorn named Erschließung weit deutschsprachig Brief ähnlich Zeitraum nutzen bestehen Möglichkeit Basis trainingsdat spezifisch Modell anwendungsfäll nachzutrainieren Flüh Lemke szenario gemischt trainieren Basis Datentyp Szenario Reflexion theoretisch Fundierung Datensammlung Basis Datentyp dokumententyp Brief trainieren experimentell Anwendung Korpus Text deutlich abweichend Dokumententyp fiktional Text insbesondere Kombination dokumententyp zugeschnitten Classifier beitragen Fundierung offenlegen Modellierung Classifier eingehen Ergebnis Classifier vergleichend betrachten Fallstudie Flüh Schumacher Nantke erscheinen,"[('datentyp', 0.46856110850499755), ('szenario', 0.34691261498468223), ('nachnutzung', 0.2559918518737721), ('brief', 0.2344308121027434), ('basis', 0.22161746637860355), ('dokumententyp', 0.1886470395519913), ('classifier', 0.12436026869265256), ('erschlossen', 0.11102115665792034), ('datensammlung', 0.11102115665792034), ('trainieren', 0.10681068816046824)]"
2023,DHd2023,KONLE_Leonard_Gattungen_und_Emotionen_in_der_Lyrik_des_Reali.xml,Gattungen und Emotionen in der Lyrik des Realismus und der frühen Moderne,"Merten Kröncke (Universität Göttingen, Deutschland); Leonard Konle (Universität Würzburg, Deutschland); Simone Winko (Universität Göttingen, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland)","Emotion, Gattung, Lyrik, Realismus, Moderne","Inhaltsanalyse, Modellierung, Annotieren, Literatur, Text","Der literarische Wandel vom Realismus zur Moderne ist nach wie vor Gegenstand vielfältiger literaturwissenschaftlicher Debatten. Eine dieser Debatten betrifft die Frage, wie sich die Gestaltung von Emotionen in lyrischen Texten veränderte. Während einige typologisch argumentierende Forscher:innen (z. B. Zu bedenken ist, dass die damalige Lyrik keine homogene Einheit bildet. Aussagen darüber, inwiefern sich die Emotionsgestaltung ""der"" Lyrik veränderte, lassen sich differenzieren. Ein naheliegendes, wichtiges Differenzmerkmal lyrischer Texte 'und damit ein potentiell relevanter Einflussfaktor auf die Emotionsgestaltung 'ist die Gattung. Ziel dieses Beitrags ist deshalb, zu prüfen, inwiefern sich lyrische Gattungen in unserem Untersuchungszeitraum durch spezifische Emotionsprofile auszeichnen und ob die Entwicklung der Lyrik vom Realismus zur frühen Moderne unter der Perspektive der Emotionsgestaltung, die wir in¬† ( Das zu analysierende Korpus besteht aus Texten in Lyrikanthologien aus dem Untersuchungszeitraum, die sich auf Gedichte von Zeitgenoss:innen konzentrieren. Das Teilkorpus ""Realismus"" umfasst Gedichte aus Anthologien, die zwischen 1850 und den frühen 1880er Jahren publiziert wurden; das Teilkorpus ""Moderne"" enthält Texte aus Anthologien, die um 1900 erschienen sind und deren Herausgeber:innen die Gedichte aufgrund ihrer Modernität ausgewählt haben. Welche Texte als ""realistisch"" und welche als ""modern"" gelten, wird in diesem Beitrag also aus zeitgenössischer Sicht (und nicht aus Sicht heutiger Forscher:innen) modelliert. Tabelle 1: Korpus Statistik Für 1412 Korpustexte wurden die Gattungszugehörigkeit und die Emotionsgestaltung annotiert. Die Gattungsannotation hält fest, ob ein Text bestimmten im Untersuchungszeitraum relevanten thematischen Gattungen (Naturlyrik, Liebeslyrik usw.) sowie nicht-thematischen Gattungen (Ballade, Lied usw.) angehört und ob er situativ bestimmt oder situativ unbestimmt ist. Wenn im Folgenden von ""Gattungen"" die Rede ist, sind in aller Regel die thematischen Gattungen gemeint, auf die sich dieser Beitrag erst einmal konzentriert. Die Gattungszuordnung ist weder exklusiv noch zwingend: Während der Annotation konnten einem Text genau eine, aber auch keine oder mehrere Gattungen zugewiesen werden. Tabelle 2: Annotierte Gattungen Jedes Gedicht wurde zunächst von zwei Annotator:innen annotiert; anschließend haben die beiden Annotator:innen ihre Annotationen miteinander verglichen, die Disagreements diskutiert und eine Konsensannotation erstellt. Auf den Konsensannotationen beruhen alle weiteren Auswertungen. Das Agreement der Einzelannotationen beträgt 0.69 (Krippendorffs Alpha). Um Gattungslabel für das gesamte Korpus herstellen zu können, sollen Classifier trainiert und angewandt werden. Das Modell wird mit den ermittelten Hyperparametern verwendet, um binäre Classifier für alle übrigen Gattungen zu trainieren. Da die Klasse der nicht zur fokussierten Gattung gehörenden Gedichte in jedem Fall größer ist, wird epochenweise Random Undersampling angewandt. Für politische Lyrik wird kein Modell trainiert, da nicht genügend Beispiele vorhanden sind (siehe Tabelle. 2). Tabelle 4 zeigt die Qualität der trainierten Classifier. Da die Performance für die Gattungen Kultur und Poetologie nicht ausreicht, um solide Analysen zu ermöglichen, werden diese im Weiteren nicht behandelt und fallen, wie Politik, der Kategorie ""Sonstige"" zu, die daneben diejenigen Gedichte umfasst, die keiner thematischen Gattung zugeordnet wurden. Für Informationen zur Annotation von Emotionen und Modellen zur Emotionsdetektion siehe Verteilung von thematischen Gattungen in Realismus und Moderne. Absolute Anzahl über Balken. Die Kategorie ""Sonstige"" repräsentiert diejenigen Gedichte, die keiner der übrigen in dieser Abbildung gezeigten thematischen Gattungen zugeordnet wurden. 60% der untersuchten Gedichte konnten (mindestens) einer der fünf Gattungen Liebeslyrik, Naturlyrik, religiöse Lyrik, philosophische Lyrik oder Geschichtslyrik zugeordnet werden. Liebeslyrik und Naturlyrik sind im Korpus deutlich verbreiteter als religiöse Lyrik, philosophische Lyrik und Geschichtslyrik. Im Epochenvergleich werden nur begrenzte Verschiebungen sichtbar; die Rangfolge der Häufigkeiten bleibt konstant (siehe Abb. 1). Wie die Gattungen kommen auch die Emotionen im Korpus unterschiedlich oft vor: Emotionen der Gruppen Liebe, Freude und Trauer werden deutlich häufiger gestaltet als Emotionen der Gruppen Erregung/Überraschung, Angst und Wut. Die Häufigkeit positiver Emotionen 'Liebe und Freude 'nimmt zur Moderne hin ab Die Ergebnisse (Abb. 2) zeigen, dass Gattungen eigenständige Emotionsprofile ausbilden; dies gilt auch für die nicht durch Emotionen definierten Gattungen. Einige Gattungen verhalten sich in Hinsicht auf bestimmte Emotionen ähnlich, z.B. philosophische Gedichte und Geschichtslyrik in Hinsicht auf die Gruppen Freude und Trauer, weichen aber in anderen Emotionen stark voneinander ab. Während z.B. in Liebeslyrik 'wenig überraschend 'Emotionen der Gruppe Liebe dominant sind, werden in Geschichtslyrik Emotionen der Gruppen Liebe und Freude unter- und Emotionen der Gruppen Wut und Angst überproportional dargestellt. Ein kontrastiver Blick auf Gattungen unter der Perspektive von Epochen (Abb. 3) zeigt, dass sich (1) die lyrische Emotionsgestaltung in der frühen Moderne gegenüber dem Realismus verändert, und zwar (2) je nach Gattung in unterschiedlicher Weise. Der summarische Trend über alle Gattungen zu weniger Emotionen, verursacht vor allem durch den Rückgang positiver Emotionen in der Moderne (siehe Konle u.a. 2022), betrifft nicht alle Gattungen in gleicher Weise. Während sich die Ergebnisse zu Liebes-, Natur- und philosophischer Lyrik noch diesem Trend zuordnen lassen, entwickeln sich Geschichts- und religiöse Lyrik durch Zunahme negativer Emotionen eigenständiger. Religiöse Lyrik läuft dem Trend sogar durch ein vermehrtes Auftreten von Emotionen der Gruppe Liebe entgegen. Diese Befunde erlauben einen differenzierten Blick auf Emotionen, Gattungen und deren Veränderung in der frühen Moderne. Anschließend an unsere Ergebnisse stellt sich die Frage nach den Faktoren, welche die gattungsspezifischen Emotionsprofile zu unterschiedlichen Zeitpunkten beeinflussen. Für Geschichtslyrik bietet sich die These an, dass mit der Thematisierung von Konflikten und Feindseligkeiten Die Entwicklung der Emotionen in religiöser Lyrik ist noch komplexer, es nehmen nicht nur negative Emotionen zu, sondern auch die der Gruppe Liebe, bei gleichzeitigem Rückgang der Gruppe Freude. Wald Kirche Kaiser Bischof Segen Grab Priester Glocke Mönch Heil Werk Sieg Haus Friede Dom Andacht Lenz Sonntag Mahl Ehr Gott Seele Nacht Auge Mensch Tod Weib Licht Welt Stimme Traum Kreuz Brust Blut Volk Erde Herz König Kind Leib Freude Ausgeglichenheit Lust (nicht-sexuell) Aufregung Dankbarkeit Liebe Abneigung Sehnsucht Leid Lust (sexuell) Die gleichzeitige Beobachtung von Gattung und Emotion zeigt nicht nur, dass Gattungen wesentlichen Einfluss auf die Verteilung von Emotionen haben, sondern auch, dass der Übergang von Realismus zu früher Moderne innerhalb von Gattungen eigenen Dynamiken folgt. Im Fall religiöser Lyrik und Geschichtslyrik verlaufen diese Dynamiken sogar in Widerspruch zum Gesamttrend. Im Anschluss ergeben sich drei weitere Perspektiven: Wie sehen die Emotionsprofile der nicht-thematischen Gattungen aus? Welche anderen differenzierenden Faktoren neben der Gattung können wir identifizieren, z.B. Autorschaft, Publikationsort, Intertextualität usw.? Ein wichtiger Aspekt könnte die Zeit sein, bezogen auf kleinere Einheiten als Epochen: Wie entwickeln sich die Lyrik und die einzelnen Gattungen unter der Perspektive der Emotionsgestaltung in der Zeit, etwa von Jahrzehnt zu Jahrzehnt?",de,literarisch Wandel Realismus Moderne Gegenstand vielfältig literaturwissenschaftlicher debatten debatter betreffen Frage Gestaltung Emotion lyrisch Text verändern typologisch argumentierend Forscher innen bedenken damalig Lyrik homogen Einheit bilden Aussage inwiefern Emotionsgestaltung Lyrik verändern lassen differenzieren naheliegend wichtig differenzmerkmal lyrisch Text potentiell relevant Einflussfaktor Emotionsgestaltung Gattung Ziel Beitrag prüfen inwiefern lyrisch Gattunge unser Untersuchungszeitraum spezifisch emotionsprofil auszeichn Entwicklung Lyrik Realismus früh Moderne Perspektive Emotionsgestaltung analysierend Korpus bestehen Text Lyrikanthologien Untersuchungszeitraum Gedicht Zeitgenoss innen konzentrieren Teilkorpus Realismus umfasst gedicht Anthologien früh publizieren Teilkorpus modern enthalten Text Anthologien erscheinen Herausgeber innen Gedicht aufgrund Modernität auswählen Text realistisch modern gelten Beitrag zeitgenössisch Sicht Sicht heutig Forscher innen modellieren Tabell Korpus Statistik Korpustext Gattungszugehörigkeit Emotionsgestaltung annotiert Gattungsannotation halten fest Text bestimmen Untersuchungszeitraum relevant thematisch Gattung Naturlyrik Liebeslyrik Gattung Ballade Lied angehören situativ bestimmt situativ unbestimmt folgend gattung Rede Regel thematisch Gattung meinen Beitrag konzentrieren Gattungszuordnung weder exklusiv zwingend Annotation Text genau mehrere gattung zuweisen Tabell annotiert Gattunge jeder dichen Annotator innen annotiert anschließend Annotator innen annotatio miteinander vergleichen disagreements diskutieren Konsensannotation erstellen Konsensannotation beruhen Auswertung Agreement Einzelannotation betragen Krippendorff Alpha Gattungslabel gesamt Korpus herstellen Classifier trainieren anwenden Modell ermittelt hyperparametern verwenden binär Classifier übrig Gattung trainieren Klasse fokussiert Gattung gehörend Gedicht Fall groß epochenweise Random Undersampling anwenden politisch Lyrik Modell trainieren genügend Beispiel vorhanden sehen Tabell Tabell zeigen Qualität trainiert Classifier Performance gattung Kultur Poetologie ausreichen solide Analyse ermöglichen behandeln fallen Politik Kategorie sonstig gedicht umfassen thematisch Gattung zuordnen Information Annotation Emotion modellen Emotionsdetektion sehen Verteilung thematisch Gattung Realismus Moderne absolut Anzahl Balken Kategorie sonstig repräsentieren gedichen übrig Abbildung gezeigt thematisch Gattung zuordnen untersucht Gedicht mindestens gattung Liebeslyrik Naturlyrik religiös Lyrik philosophisch Lyrik Geschichtslyrik zuordnen Liebeslyrik Naturlyrik Korpus deutlich verbreitet religiös Lyrik philosophisch Lyrik Geschichtslyrik Epochenvergleich begrenzt Verschiebung sichtbar Rangfolge häufigkeit bleiben konstant sehen abb gattung Emotion Korpus unterschiedlich Emotion Gruppe Liebe Freude Trauer deutlich häufig gestalten Emotion Gruppe Erregung überraschung Angst Wut Häufigkeit positiv Emotion Liebe Freude nehmen moderne Ergebnis abb zeigen gattung eigenständig emotionsprofile ausbilden gelten emotion Definiert gattungen gattung verhaln Hinsicht bestimmt Emotion ähnlich philosophisch Gedicht Geschichtslyrik Hinsicht Gruppe Freude Trauer weichen Emotion stark voneinander liebeslyrik überraschend Emotion Gruppe lieben dominant Geschichtslyrik Emotion Gruppe Liebe Freude Emotion Gruppe Wut Angst überproportional darstellen kontrastiv Blick Gattung Perspektive Epoche abb zeigen lyrisch Emotionsgestaltung früh Moderne Realismus verändern Gattung unterschiedlich Weise summarisch Trend Gattung Emotion verursachen Rückgang positiv Emotion modern sehen Konle betreffen Gattung gleich Weise Ergebnis philosophisch Lyrik Trend zuordnen lassen entwickeln religiös Lyrik Zunahme negativ Emotion Eigenständiger religiös Lyrik laufen Trend sogar vermehrt auftreten Emotion Gruppe Liebe entgegen befunde erlauben differenziert Blick Emotion gattung Veränderung früh Moderne anschließend Ergebnis stellen Frage Faktor gattungsspezifisch emotionsprofile unterschiedlich Zeitpunkt beeinflussen Geschichtslyrik bieten These Thematisierung Konflikt feindseligkein Entwicklung Emotion Religiöser Lyrik komplexer nehmen negativ Emotion Gruppe lieben gleichzeitig Rückgang Gruppe Freude Wald Kirche Kaiser Bischof Segen Grab Priester Glocke mönch Heil Werk Sieg Haus Fried dom andacht Lenz Sonntag Mahl Ehr Gott Seele Nacht Auge Mensch Tod Weib Licht Welt Stimme Traum kreuz Brust Blut Volk Erde Herz König Kind Leib Freude ausgeglichenheit Lust aufregung dankbarkeit lieb Abneigung Sehnsucht Leid Lust sexuell gleichzeitig Beobachtung Gattung Emotion zeigen gattung wesentlich einfluss Verteilung Emotion Übergang Realismus modern innerhalb gattung Dynamik folgen Fall religiös Lyrik Geschichtslyrik verlaufen Dynamik sogar Widerspruch Gesamttrend Anschluss ergeben Perspektive sehen Emotionsprofil Gattung differenzierend Faktor Gattung identifizieren Autorschaft Publikationsort Intertextualität wichtig Aspekt beziehen klein Einheit epochen entwickeln Lyrik einzeln gattungen Perspektive Emotionsgestaltung Jahrzehnt Jahrzehnt,"[('emotion', 0.4162050454280674), ('gattung', 0.4003502449696299), ('lyrik', 0.33409637347278526), ('geschichtslyrik', 0.1920343978658616), ('emotionsgestaltung', 0.17886548315734235), ('moderne', 0.1563530747682844), ('gedicht', 0.15311449545120961), ('gruppe', 0.15297714646444474), ('religiös', 0.14905456929778527), ('freude', 0.1431841600597651)]"
2023,DHd2023,KETSCHIK_Nora__textklang____Ein_Mixed_Methods_Workshop_zu_Ly.xml,"""textklang"" 'Ein Mixed-Methods-Workshop zu Lyrik in Text und Ton","Nora Ketschik (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Toni Bernhart (Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland); Markus Gärtner (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Julia Koch (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Nadja Schauffler (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Jonas Kuhn (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland)","Lyrik der Romantik, Korpusexploration, Sprachsynthese","Sammlung, Kollaboration, Sprache, Methoden, Ton, Werkzeuge","Die Überlieferung von Texten ist vorwiegend an die schriftliche Form gebunden, die bis zur Erfindung von Tonaufnahmetechniken in der zweiten Hälfte des 19. Jahrhunderts die einzige Möglichkeit war, von Sprachbeiträgen nicht nur den Inhalt, sondern weitgehend auch die Darbietung festzuhalten. So haben sich literarische Traditionen und die wissenschaftliche Auseinandersetzung mit Literatur überwiegend entlang der schriftlichen Überlieferung entwickelt. Selbst bei Gattungen wie der Lyrik, in der Klang eine wichtige inhaltliche und ästhetische Rolle spielt (vgl. Richter et al. 2022), steht die Textform im Zentrum der kanonischen Überlieferung. Erst am Ende des 20. Jahrhunderts (u.a. angeregt durch die Sound Studies) haben sich in der Literaturwissenschaft Forschungsfelder zu Stimme, Klang, Akustik, Auditivität und Audioliteralität etabliert (Göttert 1998, Meyer-Kalkus 2001, Schulz 2018, Meyer-Kalkus 2020, Meyer-Sickendiek 2020). Bis auf wenige Ausnahmen (z.B. Rhythmicalizer, vgl. Meyer-Sieckendiek et al. 2017) folgen die Digital Humanities bislang recht stark dieser eingespielten Zugangsweise 'obgleich seit etwa 1900 unzählige Tonaufnahmen von Rezitationen vorliegen. Auch in der linguistischen Prosodieforschung und in der Sprachtechnologie wurde über die letzten Jahrzehnte ein Methodeninventar entwickelt, das eine sehr differenzierte Formulierung von Hypothesen zur Beziehung zwischen Text und lautlicher Realisierung erlaubt. Unser Workshop führt empirische Methoden aus der Phonetik mit aktuellen Technologien der Sprachsynthese und literaturwissenschaftlicher Forschung zur Lyrik der Romantik in einem Mixed-Methods-Workflow zusammen und bietet den Teilnehmenden auf diese Weise die Möglichkeit, das Wechselspiel von Textlichkeit und lautlicher Realisierung im Gedichtekorpus explorativ zu erkunden. Der Workshop knüpft an Arbeiten aus dem BMBF-geförderten Projekt ""textklang"" Das beim Workshop verwendete Forschungskorpus zur Lyrik der Romantik speist sich aus der Mediendokumentation des DLA Marbach, die etwa 2700 Audioaufzeichnungen verschiedener Sprecher*innen seit den 1920ern beherbergt. Diese werden im Zuge des Projekts digitalisiert und um die dazugehörigen Metadaten und Transkripte ergänzt; darüber hinaus werden Texte und Rezitationen mit automatisch erzeugten Annotationen angereichert (siehe Schauffler et al. 2022b für eine Übersicht). Aktuell umfasst das ""textklang""-Korpus 1261 Audioaufnahmen zu 786 Gedichten. Metadaten, Textdateien und lizenzfreie Audiodaten werden kontinuierlich über eine interaktive Webseite veröffentlicht. In unserem Workshop kommen alle Bereiche des Mixed-Methods-Workflows zum Einsatz, indem Ansätze aus traditionell sehr unterschiedlich arbeitenden Disziplinen zusammengeführt werden. Das Analysetool ICARUS (Gärtner et al. 2015) unterstützt den korpus- und textorientierten Zugang, bildet dabei aber neben morphosyntaktischen Annotationen der Texte auch die phonetischen Annotationen der Rezitationen ab. Hierfür kommen Verfahren aus der Phonetik zum Einsatz, die die Eigenschaften des Sprachsignals systematisch erfassen. Sprachtechnologische Verfahren der Signalanalyse und -manipulation ermöglichen es sodann, bestimmte Annahmen über ein Re-Synthese-Tool kontrolliert zu testen. Der Bedarf für ein so weit gefasstes Methodenspektrum folgt aus den Grundeigenschaften des Untersuchungsgegenstands selbst. Der Workshop leistet einen Beitrag, die fachspezifischen Ansätze methodologisch zusammenzuführen und auf diese Weise den insbesondere für Lyrik zentralen Zusammenhang von Text und Klang in den Blick zu rücken. Idee des Workshops ist, dass die Teilnehmenden ihre eigenen Fragestellungen an Rezitationen von Lyrik der Romantik mitbringen können und darauf aufbauend während der Datenexploration Hypothesen entwickeln. Alternativ können die von uns vorgeschlagenen Fragestellungen aufgegriffen werden. Im Workshop thematisieren wir mehrere Use-Cases aus dem Projektkontext, darunter die Realisierung paralleler Strukturen (z.B. Reim, Satzbau), die unter strukturellen, semantischen und melodischen Aspekten von Interesse sind. Eine andere Fallstudie untersucht unterschiedliche Realisierungen von Enjambements (Schauffler et al. 2022a), die im Spannungsfeld von Vers- und Satzstruktur stehen. In Rezitationen können Sprecher*innen die syntaktische Einheit betonen, die Versgrenze markieren oder einen Mittelweg wählen (vgl. Tsur und Gafni 2019). Ein weiterer Anwendungsfall, der exemplarisch etwas näher erläutert werden soll, beschäftigt sich mit Interjektionen. Interjektionen bezeichnen Ausrufe- oder Empfindungsworte (z. B. ""ach"", ""oh"", ""juchhe"") und stehen im Grenzbereich von Schriftlichkeit und Mündlichkeit (Wharton 2003, Liedtke 2019). Sie nehmen eine syntaktische Sonderrolle ein und werden in der Linguistik als eigenständige Klasse behandelt, den Partikeln zugeordnet oder als Satzäquivalente angesehen (Liedtke 2019). Sie tragen einerseits denotativ keine Bedeutung, bringen andererseits Emotionen verschiedenster Art und in unterschiedlichen Intensitätsgraden zum Ausdruck (Schwarz-Friesel 2013, 155-157). Mit dem hier vorgestellten Mixed-Methods-Ansatz soll der Spielraum und der besondere textlich-klangliche (Zwischen-)Status von Interjektionen untersucht werden. Dabei interessiert zum einen die syntaktische Stellung von Interjektionen, zum anderen ihr Bedeutungsspektrum sowie, als dritter Aspekt, ihre lautliche Ausprägung. Die ""Offenheit"" dieser Wortart legt die Hypothese nahe, dass die verschiedenen Ebenen sich gegenseitig beeinflussen können, beispielsweise das syntaktische Umfeld die lautlichen Realisierungen in der Rezitation prägt oder bestimmte klangliche Merkmale die Bedeutung von Interjektionen ausmachen. Die abgedruckten Beispiele deuten die syntaktisch-lautlichen Spielräume der Interjektion ""Ach"" im Gedichtekorpus an: Während sie im ersten Beispiel syntaktisch isoliert steht (markiert durch den Tonhöhenverlauf und die Sprechpause), wird sie im zweiten Beispiel syntaktisch und lautlich in den Satz integriert. Auch die mit dem ""Ach"" ausgedrückten Emotionen (im ersten Beispiel Schwermut, im zweiten Freude) changieren und werden 'neben dem semantischen Kontext des Wortes 'von der jeweiligen sprachlichen Realisierung beeinflusst. Mögliche Leitfragen für weitere Untersuchungen könnten sein: Welche syntaktischen Merkmale von Interjektionen gehen mit welchen lautlichen Merkmalen einher? Werden Interjektionen in gleicher (syntaktischer) Position lautlich parallel realisiert? Welche Varianz ist zwischen unterschiedlichen Sprecher*innen zu beobachten? Inwiefern beeinflusst die lautliche Realisierung die Bedeutung oder Wahrnehmung von Interjektionen? Für die Exploration und Visualisierung des Korpus mit allen Annotationsebenen verwenden wir ICARUS (Gärtner 2015) als Anfrageschnittstelle. ICARUS erlaubt eine gemeinsame Visualisierung von prosodischen Informationen und klassischen morphosyntaktischen Annotationen. Darüber hinaus können gezielt Anfragen unter Einbeziehung aller im Korpus verfügbaren Annotationsebenen gestellt werden, um Instanzen bestimmter Phänomene zu finden. An Annotationen stehen sämtliche für das GRAIN Korpus (Schweitzer et al. 2018) beschriebenen morphosyntaktischen und prosodischen Ebenen zur Verfügung. Darüber hinaus sind die Gedichte auch mit Markierungen zu Vers- und Strophenenden versehen, welche ebenfalls in Abfragen benutzt werden können. Je nach Entwicklungsfortschritt wird ICARUS als Desktop-Applikation Die durch die Datenexploration entwickelten Hypothesen über Zusammenhänge zwischen Text und lautsprachlicher Realisierung sollen in Perzeptionsexperimenten untersucht werden. Mittels Sprachsynthese erstellen wir zu diesem Zweck eine prosodische Replikation der Originalaufnahmen, wobei phonetische Details (z.B. Lautdauer, Tonhöhe) gezielt manipuliert werden können (Koch et al. 2022). Unser Synthesemodell basiert auf der Modellarchitektur von FastSpeech 2 (vgl. Ren 2021), für die Implementierung nutzen wir das open-source Toolkit IMS Toucan Wir beginnen den Workshop mit einer Einführung in den multimodalen Ansatz und adressieren die methodologisch wie wissenschaftstheoretisch relevante Frage, wie die Spezialisierungen der Fachgebiete innerhalb der DH sinnvoll zusammengeführt werden können. Anschlie√üend präsentieren wir mögliche Forschungsbeispiele und führen in die verwendeten Tools ein. In zwei Praxisrunden haben die Teilnehmenden die Möglichkeit, das Lyrikkorpus zu erforschen, eigene Forschungsfragen zu entwickeln sowie diese exemplarisch zu untersuchen. Dies kann individuell oder in Kleingruppen geschehen. Die erste Praxisrunde dient der Exploration des Korpus und der Entwicklung möglicher Hypothesen. Hierfür kommt das Tool ICARUS zum Einsatz, über das die Teilnehmer*innen die verschiedenen Annotationsebenen (u.a. morphosyntaktisch, phonetisch) sichten und komplexe Suchanfragen an die Texte modellieren können. Auf Grundlage der Annotationen zur Text- und Lautgestalt können Forschungsfragen entwickelt oder eine der vorgestellten Fragestellungen aus der theoretischen Einführung exploriert werden. Nach einer Zusammenschau der Hypothesen dient die zweite Praxisrunde dazu, ausgewählte Fragestellungen probeweise zu validieren, indem die Annahmen in das Sprachsynthesemodell überführt werden. Wenn beispielsweise die Annahme besteht, dass die Längung und die Tonhöhe einen Einfluss darauf haben, ob die ""bedeutungsfreie"" Interjektion ""Ach"" negativ oder positiv konnotiert ist, können ebendiese Merkmale in der Sprachsynthese gezielt modifiziert und die Effekte dieser Veränderungen getestet werden. Die Ziele des Workshops bestehen folglich darin, die Möglichkeiten des Mixed-Methods-Ansatzes auszuschöpfen und Lyrik in ihrer Multimodalität erforschbar zu machen. Dabei liegt ein besonderer Schwerpunkt darauf, zu zeigen, wie fruchtbar das Zusammenspiel von textlicher und klanglicher Ebene sein kann. Zwar können die zu behandelnden Fragestellungen im Rahmen des Workshops nur ansatzweise durchgespielt werden, sie können dabei aber die Potenziale des interdisziplinären Ansatzes offenlegen. Unser Workshop ist für ca. 20 Teilnehmer*innen geeignet und richtet sich an Interessierte aus den digitalen Geisteswissenschaften. Bestimmte technische Vorkenntnisse sind nicht erforderlich. Die Teilnehmenden arbeiten an ihren eigenen Laptops. Ausreichend Steckdosen, stabiles Wifi und ein Beamer sollten vorhanden sein. Installationshinweise werden im Vorfeld an die Teilnehmer*innen verschickt. Nora Ketschik (Institut für Maschinelle Sprachverarbeitung (IMS), Universität Stuttgart, Toni Bernhart (Institut für Literaturwissenschaft, Universität Stuttgart, Markus Gärtner (IMS, Universität Stuttgart, Julia Koch (IMS, Universität Stuttgart, Nadja Schauffler (IMS, Universität Stuttgart, Jonas Kuhn (IMS, Universität Stuttgart,",de,Überlieferung Text vorwiegend schriftlich Form binden Erfindung Tonaufnahmetechnik Hälfte Jahrhundert einzig Möglichkeit sprachbeiträgen Inhalt weitgehend Darbietung festhalten literarisch Tradition wissenschaftlich Auseinandersetzung Literatur überwiegend entlang schriftlich Überlieferung entwickeln gattung Lyrik Klang wichtig inhaltlich ästhetisch Rolle spielen Richter Et stehen Textform Zentrum kanonisch Überlieferung Jahrhundert anregen Sound studies Literaturwissenschaft forschungsfelder stimmen klingen Akustik Auditivität Audioliteralität etablieren göttert schulz Ausnahme Rhythmicalizer et folgen Digital Humanitie bislang stark eingespielt zugangsweise obgleich unzählig Tonaufnahm rezitationen vorliegen linguistisch Prosodieforschung sprachtechnologie letzter Jahrzehnt Methodeninventar entwickeln differenziert Formulierung Hypothesen Beziehung Text lautlich Realisierung erlauben Workshop führen empirisch Methode Phonetik aktuell Technologie Sprachsynthese literaturwissenschaftlich Forschung Lyrik Romantik bieten Teilnehmend Weise Möglichkeit Wechselspiel Textlichkeit lautlich Realisierung Gedichtekorpus explorativ erkunden Workshop knüpfen Arbeit Projekt textklang Workshop verwendet Forschungskorpus Lyrik Romantik speisen Mediendokumentation dla Marbach audioaufzeichnungen verschieden beherbergen Zug Projekt digitalisieren dazugehörig metadaten transkripen ergänzen hinaus Text rezitationen automatisch erzeugt annotation Angereichert sehen schauffl Et übersichen aktuell umfassen audioaufnahmen Gedicht Metadat Textdateien Lizenzfreie audiodaten kontinuierlich interaktiv Webseite veröffentlichen unser Workshop Bereich Einsatz Ansatz traditionell unterschiedlich arbeitend disziplinen zusammenführen Analysetool Icarus gärtn et unterstützen textorientierten Zugang bilden Morphosyntaktischen annotatio Text phonetisch annotationen Rezitation hierfür Verfahren Phonetik Einsatz Eigenschaft sprachsignals systematisch erfassen sprachtechnologisch Verfahren Signalanalyse ermöglichen Sodann bestimmt annahmen kontrollieren testen Bedarf gefasst Methodenspektrum folgen grundeigenschafter untersuchungsgegenstands Workshop leisten Beitrag fachspezifisch Ansatz Methodologisch zusammenführen Weise insbesondere Lyrik zentral Zusammenhang Text klingen Blick rücken Idee Workshop Teilnehmende Fragestellung Rezitation Lyrik Romantik mitbringen aufbauend Datenexploration Hypothesen entwickeln alternativ vorgeschlagen Fragestellung aufgreifen Workshop thematisieren mehrere Projektkontext Realisierung parallel Struktur Reim Satzbau strukturell semantisch melodisch Aspekt Interesse Fallstudie untersuchen unterschiedlich Realisierung Enjambement Schauffler et Spannungsfeld Satzstruktur stehen Rezitation syntaktisch Einheit betonen Versgrenze markieren Mittelweg wählen Tsur Gafni weit Anwendungsfall exemplarisch nah erläutern beschäftigen Interjektion Interjektione bezeichnen empfindungsworen oh juchhe stehen Grenzbereich Schriftlichkeit Mündlichkeit Wharton Liedtke nehmen syntaktisch Sonderrolle Linguistik eigenständig Klasse behandeln partikel zuordnen satzäquivalenter ansehen Liedtke tragen einerseits denotativ Bedeutung bringen andererseits emotion verschiedenst Art unterschiedlich intensitätsgraden Ausdruck vorgestellt Spielraum besonderer Interjektion untersuchen interessiert syntaktisch Stellung Interjektion Bedeutungsspektrum Aspekt lautlich Ausprägung Offenheit Wortart legen Hypothese nahe verschieden Ebene gegenseitig beeinflussen beispielsweise syntaktisch umfeld lautlich Realisierung Rezitation prägen bestimmt klanglich Merkmal Bedeutung Interjektion ausmachen abgedruckt Beispiel deuten Spielraum Interjektion Gedichtekorpus syntaktisch isolieren stehen markieren Tonhöhenverlauf Sprechpause syntaktisch lautlich Satz integrieren ausgedrückt Emotion Schwermut Freude changieren semantisch Kontext Wort jeweilig sprachlich Realisierung beeinflussen möglich Leitfrag Untersuchung können syntaktisch Merkmal Interjektion lautlich Merkmale einh Interjektionen gleich syntaktisch Position lautlich parallel realisieren Varianz unterschiedlich beobachten inwiefern beeinflussen lautlich Realisierung Bedeutung Wahrnehmung Interjektion Exploration Visualisierung Korpus annotationsebenen verwenden Icarus Gärtner Anfrageschnittstell Icarus erlauben gemeinsam Visualisierung prosodisch Information klassisch Morphosyntaktische annotationen hinaus gezielt Anfrag Einbeziehung Korpus verfügbar Annotationseben stellen Instanze bestimmt Phänomen finden annotation stehen sämtlicher Grain Korpus Schweitzer et beschrieben morphosyntaktisch prosodisch Ebene Verfügung hinaus gedicht Markierung Strophenenden versehen ebenfalls abfrag benutzen entwicklungsfortschriten Icarus Datenexploration entwickelt Hypothese Zusammenhäng Text lautsprachlich Realisierung Perzeptionsexperiment untersuchen mittels Sprachsynthese erstellen Zweck prosodisch Replikation Originalaufnahmen wobei phonetisch Detail Lautdauer tonhöhen gezielt manipulieren koch et Synthesemodell basieren Modellarchitektur fastspeech rer Implementierung nutzen Toolkit ims Toucan beginnen Workshop Einführung multimodal Ansatz adressieren Methodologisch wissenschaftstheoretisch relevant Frage Spezialisierunge fachgebiete innerhalb dh sinnvoll zusammenführen präsentieren möglich Forschungsbeispiel führen verwendet Tools Praxisrund Teilnehmende Möglichkeit Lyrikkorpus erforschen Forschungsfrag entwickeln exemplarisch untersuchen individuell Kleingruppe geschehen praxisrunde dienen Exploration korpus Entwicklung möglich Hypothese hierfür Tool Icarus Einsatz verschieden Annotationseben morphosyntaktisch phonetisch Sichte komplexe suchanfragen Text modellieren Grundlage annotatio Lautgestalt forschungsfragen entwickeln vorgestellt Fragestellung theoretisch Einführung explorieren zusammenschau Hypothesen dienen Praxisrund ausgewählt Fragestellung probeweise validieren annahmen sprachsynthesemodell überführen beispielsweise Annahme bestehen Längung Tonhöhe einfluss bedeutungsfrei Interjektion negativ positiv konnotieren ebendieser Merkmal Sprachsynthese gezielt modifizieren Effekt Veränderung testen Ziel Workshop bestehen folglich Möglichkeit ausschöpfen Lyrik Multimodalität erforschbar liegen Besonderer schwerpunken zeigen fruchtbar Zusammenspiel Textlicher klanglich Ebene behandelnd Fragestellung Rahmen Workshop ansatzweise durchgespieln Potenziale interdisziplinär Ansatz offenlegen Workshop geeignet richten interessierter Digital geisteswissenschaften bestimmt technisch Vorkenntnisse erforderlich Teilnehmende Arbeit laptops ausreichend steckdosen stabil Wifi Beamer vorhanden installationshinweis Vorfeld verschicken Nora Ketschik Institut maschinell Sprachverarbeitung ims Universität Stuttgart Toni Bernhart Institut Literaturwissenschaft Universität Stuttgart Markus Gärtner ims Universität Stuttgart Julia Koch ims Universität Stuttgart Nadja Schauffler ims Universität Stuttgart Jona Kuhn ims Universität Stuttgart,"[('interjektion', 0.29482999735532256), ('lautlich', 0.29482999735532256), ('ims', 0.2211224980164919), ('workshop', 0.2072177638871397), ('realisierung', 0.20149401858935012), ('icarus', 0.1842687483470766), ('syntaktisch', 0.18257105989721586), ('lyrik', 0.16487274936506027), ('stuttgart', 0.14576880364848108), ('rezitation', 0.13013337262636354)]"
2023,DHd2023,HAIDER_Thomas_Nikolaus_Barockpoetik_als_Wikibase__Eine_Daten.xml,Barockpoetik als Wikibase: Eine Datenbank zu konfessions- geschichtlichen Aspekten in deutschen Barockpoetiken,"Thomas Nikolaus Haider (Universität Göttingen, Deutschland); Stephanie Schennach (Universität Göttingen, Deutschland); Julius Thelen (Universität Göttingen, Deutschland); Jörg Wesche (Universität Göttingen, Deutschland)","Wikibase, Poetik, Barock, Konfession, Religion, Linked Open Data","Modellierung, Organisation, Literatur, Metadaten, Text","Dieses Poster stellt eine Datenbank vor, die sich¬† auf die thesaurierende Erschließung sämtlicher Konfessionsaspekte in den deutschen Poesielehrbüchern der Barockzeit richtet. Das barocke Poetikparadigma, das sich zeitlich von Opitz"" Die Datenbank entsteht im Rahmen des Teilprojekts ""Uneindeutige Barockdichtung. Poetische und konfessionelle Ambiguität in Schlesien als kulturdynamische Faktoren einer neuen deutschen Dichtkunst (1620 bis 1742)"" der DFG-Forschungsgruppe ""Ambiguität und Unterscheidung. Historisch-kulturelle Dynamiken"", das Prof. Wesche gegenwärtig an der Universität Göttingen leitet. Die Wikibase ist zu finden unter Im Fokus steht, die Modellierung der Daten so vorzunehmen, dass die Inhalte dynamisch abrufbar sind. Dies wurde umgesetzt durch eine dedizierte Wikibase, die unsere spezifischen Inhalte im Stil von Wikidata darstellt und per Volltextsuche und einem SPARQL Query Interface zugänglich macht. Die Daten sind dabei als ""Items"" und ""Properties"" organisiert, wobei Items als Knoten und Properties als Kanten in einem Graph verstanden werden können, was es uns erlaubt beliebige Beziehungen im Graphen darzustellen und zu suchen. Zentrale Elemente der Datenbank sind Autoren und ihre Werke, welche als Items gespeichert wurden. Siehe etwa das Item:Q29 ( Das assoziierte Werk von Opitz, Einzelne Textstellen sind dabei als Items angelegt, um sie mehrfach verschlagworten zu können (wobei eine Textstelle unter verschiedenen Schlagworten geführt werden kann). Zudem besitzt jede Textstelle eine Angabe zu ihrer Fundstelle, um sie im Digitalisat ausfindig zu machen. Die Volltextsuche erlaubt es, nach bestimmten Inhalten zu suchen. Etwa ergibt eine Suche nach dem Wort ""Mensch"" alle Textstellen, in denen dieses Wort vorkommt, oder eine Suche nach ""[aq]"" liefert alle Textstellen mit einer Auszeichnung für die Schriftart ""Antiqua"", welche in frühneuzeitlichen Drucken konventionell für fremdsprachliches Material verwendet wird. Diese Textstellen können wiederum durch eine ""Inverse Suche"" dem jeweiligen Werk zugeordnet werden. Der SPARQL Endpoint erlaubt es uns, beliebige Daten zu extrahieren, wie etwa eine Übersicht der Autoren mit ihren Werken, oder zum Beispiel alle Werke, die ""Exempelpolitik"" enthalten, mit ihren Autoren, und den entsprechenden Textstellen, sortiert (oder gefiltert) nach Publikationsjahr. Die Datenbank leistet im Bereich der Geschichte der Poetik als einem zentralen Forschungsgebiet der germanistischen Literaturwissenschaft einen substantiellen Beitrag zur Exploration aktueller Methoden der Linked Open Data (Chiarcos et al., 2022; Sturgeon, 2022) und der Nutzung von Wikidata in den Digital Humanities (Zhao, 2022) in einem (frühneuzeit-)historischen Forschungsgebiet, indem es nicht nur um die Digitalisierung und (forschungs-)öffentliche Bereitstellung von Textdaten geht, sondern diese zugleich mit einem auf Fragen der Konfessionalität gerichteten Erkenntnisinteresse digital aufbereitet und empirisch ausgewertet werden. Übergeordnetes Forschungsziel ist es dabei, von der Universität Göttingen aus ein erweiterbares Portal ""Barockpoetik digital"" zu etablieren, in dem die Forschungsdaten zum Thema zentral gebündelt und verfügbar gehalten werden. Als konkrete Erweiterungsperspektive erschließt das Team derzeit im Rahmen des DFG-Schwerpunkprogramms ""Übersetzungskulturen der Frühen Neuzeit"" sämtliche Aspekte, die im Poetikkorpus translationsgeschichtlich relevant sind.",de,Poster stellen datenbank thesaurierend Erschließung sämtlich Konfessionsaspekt deutsch Poesielehrbücher Barockzeit richten barock Poetikparadigma zeitlich Opitz datenbank entstehen Rahmen Teilprojekt uneindeutig Barockdichtung poetisch konfessionell Ambiguität Schlesie kulturdynamisch Faktor deutsch dichtkunst Ambiguität Unterscheidung dynamiken Prof wesch gegenwärtig Universität Göttingen leiten Wikibase finden Fokus stehen Modellierung daten vornehmen inhalt Dynamisch abrufbar umsetzen dediziert Wikibase spezifisch Inhalt Stil Wikidata darstellen per volltextsuch sparql Query interface zugänglich daten item properties organisieren wobei item knoten properties Kant Graph verstehen erlaubt beliebig Beziehung graph darstellen suchen zentral Elemente datenbank Autor Werk item speichern sehen Item assoziiert Werk Opitz einzeln Textstelle item anlegen mehrfach verschlagworten wobei Textstelle verschieden Schlagwort führen zudem besitzen Textstelle Angabe Fundstelle Digitalisat ausfindig volltextsuch erlauben bestimmt Inhalt suchen ergeben Suche Wort Mensch Textstell Wort vorkommen Suche aq liefern Textstell Auszeichnung Schriftart Antiqua Frühneuzeitlich drucken konventionell fremdsprachlich Material verwenden Textstell wiederum invers Suche jeweilig Werk zuordnen Sparql Endpoint erlauben beliebig daten extrahieren übersichen Autor Werk Werk Exempelpolitik enthalten Autor entsprechend Textstell sortieren filtern Publikationsjahr datenbank leisten Bereich Geschichte Poetik zentral Forschungsgebiet germanistisch Literaturwissenschaft substantiell Beitrag Exploration aktuell Methode Linked Open Data chiarcos et Sturgeon Nutzung Wikidata Digital Humanitie zhao forschungsgebiet Digitalisierung Bereitstellung Textdat Frage Konfessionalität gerichtet Erkenntnisinteresse Digital aufbereiten empirisch auswerten übergeordnet Forschungsziel Universität Göttingen erweiterbar Portal Barockpoetik Digital etablieren Forschungsdat Thema zentral bündeln verfügbar halten konkret Erweiterungsperspektive erschließen Team derzeit Rahmen übersetzungskulturen früh Neuzeit sämtlicher Aspekt Poetikkorpus translationsgeschichtlich relevant,"[('item', 0.33151041670390524), ('datenbank', 0.21448953264295528), ('textstell', 0.21448953264295528), ('forschungsgebiet', 0.1569226441176584), ('opitz', 0.1569226441176584), ('werk', 0.14838284290206089), ('textstelle', 0.14791201904127577), ('suche', 0.14177524983898787), ('volltextsuch', 0.1385264261008468), ('göttingen', 0.1385264261008468)]"
2023,DHd2023,PICHLER_Axel_Urheberrechtlich_geschützte_Texte_nachnutzen___.xml,Urheberrechtlich geschützte Texte nachnutzen 'Der XSample-Workflow,"Melanie Andresen (Universität Stuttgart, Deutschland); Markus Gaertner (Universität Stuttgart, Deutschland); Janina Jacke (Universität Stuttgart, Deutschland); Nora Ketschik (Universität Stuttgart, Deutschland); Axel Pichler (Universität Stuttgart, Deutschland)","Forschungsdaten, Textauszüge, Nachnutzung","Archivierung, Veröffentlichung, Daten, Methoden, Forschungsprozess, virtuelle Forschungsumgebungen","Eines der Hindernisse, die der freien Weitergabe von Forschungsdaten im Sinne der Open-Science-Bewegung im Wege stehen, ist das Urheberrecht (UrhG). Dieses erschwert die Einhaltung der guten wissenschaftlichen Praxis und der FAIR-Prinzipien (Wilkinson et al. 2016) insbesondere bei Forschung zu zeitgenössischen Texten. Für urheberrechtlich geschützte Texte besteht bisher nur die Möglichkeit, sog. abgeleitete Textformate (Schöch et al. 2020) für ""non-consumptive access"" (Organisciak und Downie 2021) zu veröffentlichen, etwa in Form von Frequenzlisten. Geisteswissenschaftliche Forschungsprojekte sind allerdings häufig auf die Verfügbarkeit von textuellem Kontext angewiesen, der erst eine angemessene Interpretation der Daten erlaubt. Um die Nachnutzbarkeit von Textdaten in dieser Hinsicht zu unterstützen, wurde im Projekt XSample Im deutschen UrhG unterliegen Texte bis 70 Jahre nach dem Tod der Autor*innen dem urheberrechtlichen Schutz, der die Vervielfältigung und die öffentliche Zugänglichmachung von Texten erheblich einschränkt. Hiervon sind vor allem zeitgenössische Texte betroffen, die aus diesem Grund womöglich gar nicht erst als Gegenstand von Forschungsprojekten in Erwägung gezogen werden. Seit 2018 ist zumindest die Verwendung von urheberrechtlich geschützten Texten zu Zwecken des Text- und Dataminings durch ¬ß60d¬†UrhG legitimiert (vgl. Raue 2021). Die Nachnutzung der Daten nach Abschluss eines Projektes ist aber weiterhin nur unklar geregelt (vgl. Kleinkopf et al. 2021, Andresen et al. 2022). Der XSample-Workflow kombiniert den ¬ß60d¬†UrhG mit ¬ß¬†60c Abs. 1 Nr. 1 UrhG, der es erlaubt, bis zu 15 Prozent von Werken und auch vollständige Werke geringen Umfangs zu Zwecken der nicht-kommerziellen wissenschaftlichen Forschung zu vervielfältigen und an bestimmt abgegrenzte Personenkreise für deren eigene wissenschaftliche Forschung weiterzugeben. Die Weitergabe von nur 15 Prozent eines Textes erscheint auf den ersten Blick nicht hinreichend. Um die Nützlichkeit dieser Textauszüge zu maximieren, wird im XSample-Workflow über eine Benutzeroberfläche eine gezielte Textauswahl unterstützt. So können Forschende die Textauswahl genau auf die eigenen Forschungsanliegen zuschneiden. Dafür werden auch im Korpus enthaltene Annotationen berücksichtigt, sodass bei Interesse an einem bestimmten Phänomen systematisch die mit den relevanten Kategorien annotierten Textstellen extrahiert werden können (vgl. Gärtner 2020). Die Möglichkeiten und Grenzen des Auszugskonzepts sind am Beispiel zweier Anwendungsfälle aus der Literaturwissenschaft und Linguistik erprobt worden (vgl. Andresen et al. 2022). Sie zeigen beispielsweise, dass viele Forschungsfragen davon profitieren, wenn abgeleitete Textformate, die quantitative Analysen auf dem Gesamtkorpus ermöglichen, und Auszüge, die die qualitative Interpretation erlauben, kombiniert werden. Abb. 1: Der XSample-Workflow im Überblick. Abbildung 1 fasst das Auszugskonzept zusammen: Forscher*innen übermitteln ihre (annotierten) Forschungsdaten an Forschungsinfrastruktureinrichtungen (z.¬†B. wissenschaftliche Bibliotheken), die diese verwalten. Nachnutzer*innen können Zugriffsanfragen an die Infrastrukturbetreiber stellen, um Auszüge aus den Forschungsdaten zu erhalten. Der in Abbildung 1 beschriebene Workflow wurde im Rahmen des XSample-Projekts prototypisch in Form eines Webservices implementiert. ""Open Access"" ist und bleibt die Zielvorstellung für offene und reproduzierbare Forschung auch in den digitalen Geisteswissenschaften. Das hier vorgestellte Auszugskonzept stellt demgegenüber eine ""Behelfslösung"" dar, um die Nachnutzung urheberrechtlich geschützter Daten zu ermöglichen und der Tendenz entgegenzuwirken, diese Texte per se aus Forschungsprojekten auszuschließen (vgl. Gärtner et al. 2021). Die Lösung ist an der Forschungspraxis der digitalen Literatur- bzw. Geisteswissenschaften ausgerichtet, für die andere ""verfremdende"" Verfahren wie abgeleitete Textformate (Schöch et al. 2020, Organisciak und Downie 2021) nur eingeschränkt nachnutzbar sind. Das Auszugskonzept ermöglicht hingegen eine größere Nähe zum Text, indem die ursprüngliche Textgestalt beibehalten wird, die für die Interpretation der Daten häufig unabdingbar ist. Darüber hinaus wird der rechtliche Rahmen durch die individuelle Auszugsgenerierung optimal ausgeschöpft und den individuellen Forschungsinteressen angepasst.",de,hindernisse frei Weitergabe Forschungsdat Sinn Weg stehen urheberrecht Urhg erschweren Einhaltung gut wissenschaftlich Praxis Wilkinson et insbesondere Forschung zeitgenössisch Text urheberrechtlich geschützt Text bestehen Möglichkeit abgeleitet Textformate schöch et access organisciak Downie veröffentlichen Form Frequenzlist geisteswissenschaftlich forschungsprojekte häufig Verfügbarkeit textuell Kontext angewiesen angemessen Interpretation daten erlauben Nachnutzbarkeit Textdat Hinsicht unterstützen Projekt xsample deutsch Urhg unterlieg Text Tod urheberrechtlich Schutz Vervielfältigung öffentlich Zugänglichmachung Text erheblich einschränken hiervon zeitgenössisch Text betreffen Grund womöglich Gegenstand Forschungsprojekt Erwägung ziehen zumindest Verwendung urheberrechtlich geschützt Text zwecken Datamining legitimieren Raue Nachnutzung daten Abschluss Projekte weiterhin unklar regeln Kleinkopf et andresen et kombinieren abs nr Urhg erlauben Prozent Werk vollständig werk gering Umfangs zwecken wissenschaftlich Forschung vervielfältigen bestimmt abgegrenzt Personenkreise wissenschaftlich Forschung weiterzugeben Weitergabe Prozent Text erscheinen Blick hinreichend Nützlichkeit Textauszüge maximieren benutzeroberfläch gezielt Textauswahl unterstützen forschende Textauswahl genau forschungsanlieg zuschneiden Korpus enthalten annotationen berücksichtigen sodass Interesse bestimmt Phänomen Systematisch relevant Kategorie Annotiert Textstell extrahiern gärtn Möglichkeit Grenze Auszugskonzept zwei Anwendungsfäll Literaturwissenschaft Linguistik erproben andresen et zeigen beispielsweise Forschungsfrag profitieren abgeleitet Textformat quantitativ Analyse Gesamtkorpus ermöglichen auszüge qualitativ Interpretation erlauben kombinieren abb Überblick Abbildung fasst Auszugskonzept übermitteln Annotiert Forschungsdat forschungsinfrastruktureinrichtung wissenschaftlich Bibliothek verwalten Zugriffsanfrag infrastrukturbetreiber Stelle auszug Forschungsdat erhalten Abbildung beschrieben Workflow Rahmen prototypisch Form webservices implementieren open access bleiben Zielvorstellung offen reproduzierbar Forschung Digital geisteswissenschaften vorgestellt Auszugskonzept stellen Behelfslösung dar Nachnutzung Urheberrechtlich geschützt daten ermöglichen Tendenz entgegenwirken Text per se Forschungsprojekt ausschließen gärtn et Lösung Forschungspraxis digital Geisteswissenschaft ausrichten verfremdend Verfahren abgeleitet Textformate schöch et organisciak Downie eingeschränkt nachnutzbar Auszugskonzept ermöglichen hingegen groß Nähe Text ursprünglich Textgestalt beibehalten Interpretation daten häufig unabdingbar hinaus rechtlich Rahmen individuell Auszugsgenerierung optimal ausschöpfen individuell Forschungsinteresse angepasst,"[('auszugskonzept', 0.2797459629764958), ('urheberrechtlich', 0.2363934186274332), ('urhg', 0.2098094722323718), ('geschützt', 0.19542161737109062), ('abgeleitet', 0.1653554084772957), ('wissenschaftlich', 0.14856622781016965), ('forschungsdat', 0.1462292166837183), ('downie', 0.1398729814882479), ('organisciak', 0.1398729814882479), ('textformate', 0.1398729814882479)]"
2023,DHd2023,DOMINIK_GERSTORFER_Dominik_Konflikte_als_Theorie__Modell_und.xml,"Konflikte als Theorie, Modell und Text 'Ein kategorientheoretischer Zugang zur Operationalisierung von Konflikten","Dominik Gerstorfer (TU-Darmstadt, Deutschland); Evelyn Gius (TU-Darmstadt, Deutschland)","Kategorientheorie, Modellierung, Operationalisierung, Theoriebildung","Inhaltsanalyse, Modellierung, Annotieren, Theoretisierung, Literatur, Text","Für eine theoriegeleitete Analyse von Texten muss man eine Operationalisierung finden, die die theoretischen Konzepte im Text identifizierbar und damit messbar macht. Dies gilt in nicht-digitalen Forschungskontexten gleichermaßen wie in den Digital Humanities, allerdings ist in letzteren das Problem virulenter. Dies hat Moretti (2013) bereits prominent festgestellt. Jenseits von konkreten Fragestellungen (wie etwa von Moretti selbst oder in Fischer & Trilcke 2016) fehlen allerdings generell umsetzbare Vorschläge. In diesem Beitrag wollen wir an einem literaturwissenschaftlichen Anwendungsfall zeigen, wie die angewandte Kategorientheorie Operationalisierung als einen deutlich(er) definierten Workflow ermöglicht, mit dem man von einer theoretischen Grundlage über ein Modell zur Textanalyse –¬†und zurück 'kommen kann. Durch den theorieorientierten Fokus der operationalisierten Konzepte, ihre höhere Granularität sowie ihre Kompositionalität bietet der Workflow zudem eine Reihe von Vorteilen, die computationelle Analysen zugleich besser und einfacher machen können. Um den Anforderungen sowohl der geisteswissenschaftlichen als auch der informatischen Komponenten der Digital Humanities gerecht zu werden, ist es nötig, einen möglichst flexible und abstrakte 'd.h. inhaltsagnostische 'Grundlage zu finden. Wir greifen bei unserem Unterfangen auf die angewandte mathematische Kategorientheorie zurück, da sie die Möglichkeit bietet, sowohl die Explikation und Modellierung geisteswissenschaftlicher Fragestellungen zu unterstützen als auch, die Anschlussfähigkeit an informatische Methoden zu gewährleisten (Ehrig 2001). Die Kernidee der mathematischen Kategorientheorie ist es, beliebige Strukturen als Sammlungen von Objekten und ihren wechselseitigen Beziehungen zueinander zu charakterisieren. Im einfachsten Fall besteht eine Kategorie Durch wechselseitiges Ersetzen 'markiert durch einen Asterisk (*) 'von Morphismen oder Objekten durch Strukturen können Abstraktionsebenen integriert oder Modellierungen mit mehr Details angereichert werden: Der ursprüngliche Zweck der mathematischen Kategorientheorie war der Vergleich mathematischer Theorien (Mac Lane 2010), um so strukturelle Öhnlichkeiten zwischen unterschiedlichen mathematischen Disziplinen zu entdecken und sie zu vereinheitlichen. Um dies zu ermöglichen, werden Objekte rein formal, d.h. unter Absehung des konkreten Inhalts, betrachtet, was es uns erlaubt, auch nicht-mathematische Gegenstände zu modellieren. Hinzu kommt, dass dieser hohe Abstraktionsgrad die Abbildbarkeit verschiedener Theorien 'oder in unserem Fall: Konzepte 'aufeinander und damit auch ihren Vergleich ermöglicht. Wir verwenden Elemente der angewandten Kategorientheorie als Gerüst, um klar und präzise darzustellen, worüber wir sprechen. Diese Darstellung ist nicht reduktionistisch in dem Sinne, dass wir behaupten, Literaturtheorie könne letztlich durch mathematische Strukturen ersetzt werden. Vielmehr zielen wir darauf ab, literarische Konzepte zu explizieren (vgl. Carnap 1945; Dutilh Novaes 2017) und zu operationalisieren. Mit Operationalisierung ist hier lediglich der Prozess oder Workflow gemeint, um Konflikte in literarischen Texten zu identifizieren und zu analysieren (vgl. Pichler & Reiter 2021). Diese Form der Operationalisieren soll nicht mit Operationalismus verwechselt werden, d.¬†h. der streng positivistischen Vorstellung von Operationalisierung, die Bedeutung mit empirischen Operationen gleichsetzt. Ziel dieses Beitrags ist es, ein Framework zur Operationalisierung von Analysekonzepten am Beispiel von literarischen Konflikten zu skizzieren. Dieses Framework besteht im wesentlichen aus drei Formalisierungs- bzw. Abbildungsschritten. Hierzu entwickeln wir einen der mathematischen Kategorientheorie entlehnten Formalismus, der es erlaubt drei Ebenen zu integrieren: 1. Theorie, 2. Modell und 3. Text. Wir zeigen die Schritte im Folgenden am Beispiel einer Analyse von Konflikten in literarischen Texten. Die Aufgabe einer Theorie literarischer Konflikte ist es, festzulegen, unter welchen Bedingungen ein Konflikt vorliegt. Nun gibt es zahlreiche Möglichkeiten, Konfliktkonzepte in der literaturwissenschaftlichen Textanalyse zu nutzen. Für diesen Beitrag fokussieren wir uns auf eine Analyse von Konflikten zwischen Figuren und nutzen ein Konzept aus den Sozialwissenschaften, welches bereits in Gius (2015) im Rahmen einer narratologischen Analyse erprobt wurde. Im Sinne des Um diese informelle Definition zu operationalisieren und einen Konflikt Glasl charakterisiert zwei Beziehungen zwischen den Aktoren, erlebte Unvereinbarkeit  Zwischen den zwei Aktoren gibt es mindestens eine Beeinträchtigung des Verwirklichens  Des Weiteren soll sich die Unvereinbarkeit Ein Konflikt An diesem Diagramm ist eine für die literaturwissenschaftliche Analyse schwerwiegende Einschränkung der Theorie Glasls zu sehen: Die Beobachtung des Konflikts ist in der Beziehung zwischen den Aktoren versteckt, d.h. Erzähl- bzw. Beobachtungsinstanzen kommen nicht explizit vor. Dies ist dem Umstand geschuldet, dass Glasl Konfliktbegriff aus den Sozialwissenschaften stammt und entsprechend, anders als in literarischen Texten, die Konfliktparteien zu ihrer Einschätzung befragt werden können. Da es das Ziel unserer Operationalisierung ist, den Konfliktbegriff so zu explizieren, dass auch komplexe Erzählsituationen erfasst werden können, erweitern wir Im paradigmatischen Fall des von Der erweiterte Konfliktbegriff umfasst nunmehr als Objekte Ein Modell eines Konflikts liegt dann vor, wenn sich in literarischen Texten Kandidaten für Objekte und Morphismen finden, sodass es eine strukturerhaltende Abbildung auf den oben explizierten, erweiterten Konfliktbegriff  Figurenkonstellationen und Beobachtungs- bzw. Erzählinstanzen können aufgrund ihrer Kombinierbarkeit sehr flexibel modelliert werden; ebenso lassen sich weitere Konflikttheorien integrieren, die auf andere Aspekte abstellen, und weitere Objekte, wie gesellschaftlichen Wandel, oder andere Relationstypen zwischen den Objekten einführen. So könnte man Dahrendorfs (1972) Unterscheidung zwischen latenten und manifesten Konflikten durch zusätzliche Bedingungen integrieren, u.a. dadurch, dass beide oder keine der Konfliktparteien den Konflikt wahrnehmen. Oder man kann Definitionen psychischer Konflikte integrieren, indem man u.a. die Identität Auf der Textebene gilt es jene Textphänomene zu bestimmen, welche auf die Figurenkonstellationen und Erzählinstanzen abgebildet werden können. Diese sollen schließlich in der Analyse manuell oder automatisch identifiziert werden. Es geht also um die Bestimmungen von am Konflikt beteiligten Figuren und ihn wahrnehmenden Instanzen und die weiteren, zu Unvereinbarkeiten bzw. wahrgenommener Einschränkung führenden Aspekte. Für jeden Aspekt muss in einer Analyse festgelegt werden, wie er im Text realisiert werden kann. Die Operationalisierung auf der Textebene hängt auch vom anvisierten Untersuchungsmodus ab und kann auf sehr unterschiedliche Weisen erfolgen. So kann es sinnvoll sein, die Figuren als per NER-Analyse erkennbare Personen-Entitäten zu fassen 'oder diese entsprechend zu erweitern –, wenn man eine automatisierte Analyse anstrebt. In einer manuellen Analyse ist es vermutlich eher möglich, Figuren als komplexere Phänomene zu fassen und auch Charakterzüge u.ä. zu annotieren. Bei der wahrgenommenen Unvereinbarkeit könnte der Fokus auf repräsentierte Prozesse des Denkens und Fühlens etc. liegen, die zumindest teilweise automatisch erkannt werden können. Oder es könnte eine Sentimentanalyse in Abhängigkeit von den Erzählinstanzen und den beteiligten Figuren durchgeführt werden, bei welcher automatische und manuelle Verfahren kombiniert werden können. Eventuell bietet es sich auch an, Indikatoren für Konflikthaftigkeit herauszuarbeiten. Unabhängig von den gewählten Phänomenen und ihrer Bestimmung im Text gilt: Durch Gruppierung von Textphänomenen in hierarchischen Tagsets und Kategoriensystemen können Texteigenschaften der Modellebene zugeordnet werden (vgl. Abb. 2, in der Das hier vorgeschlagene Framework erlaubt es, den Workflow für die Operationalisierung auf jeder der drei Ebenen zu beginnen und bei Bedarf auf eine der anderen Ebenen zu wechseln. Es gibt keine starre Beschränkung des Workflows auf top-down, middle-out oder bottom-up. Vielmehr ist es je nach Fragestellung möglich, in die Tiefe zu gehen und mehr Daten und Details einzuarbeiten oder zu abstrahieren, um größere Strukturen sichtbar zu machen. Das bedeutet, dass die genutzte Konfliktdefinition ebenso verändert werden kann wie die Umsetzung der Objekte und Relationen für literarische Texte (wie wäre es etwa, wenn man die Definition auf eine Analyse von poetologischen ‚ÄúKonflikten‚Äù zwischen Autor:innen ausweiten wollte?) oder deren Realisierung auf der Textebene. Dabei kann die Önderung auf einer der Ebenen jeweils Anpassungen auf den anderen Ebenen nötig machen. Mit dieser Formalisierung eines Konfliktbegriffs für die Literaturwissenschaft und seiner skizzierten √úberführung in ein Modell, welches in der Textanalyse genutzt werden kann, haben wir gezeigt, wie man ausgehend von einer Theorie zu ihrer Anwendung auf Texte gelangen kann. Der Prozess ist –¬†wie die angewandte Kategorientheorie selbst auch –¬†ein allgemein anwendbarer Workflow für die Operationalisierung von Analysekonzepten für eine quantifizierende und/oder computationell-algorithmische Analyse. Die angewandte Kategorientheorie ermöglicht dabei die in den Digital Humanities im Bereich von computationellen Analysen zwingend notwendige Formalisierung mit Fokus auf die Theorie. Insgesamt sehen wir vier substanzielle Vorteile in diesem Workflow:  ",de,theoriegeleitet Analyse texten Operationalisierung finden theoretisch Konzept Text identifizierbar messbar gelten forschungskontexten gleichermaßen Digital Humanitie letzter Problem virulent moretti Prominent feststellen jenseits konkret Fragestellung moretti Fischer Trilcke fehlen generell umsetzbar Vorschlag Beitrag literaturwissenschaftlich Anwendungsfall zeigen angewandt Kategorientheorie Operationalisierung deutlich Definiert Workflow ermöglichen theoretisch Grundlage Modell Textanalyse theorieorientiert Fokus operationalisiert Konzept hoch Granularität Kompositionalität bieten Workflow zudem Reihe vorteilen computationelle analysen einfach Anforderung sowohl Geisteswissenschaftlich informatisch Komponente Digital Humanitie gerecht nötig möglichst flexibel abstrakt inhaltsagnostisch Grundlage finden greifen unser Unterfang angewandte mathematisch Kategorientheorie Möglichkeit bieten sowohl Explikation Modellierung geisteswissenschaftlich Fragestellung unterstützen Anschlussfähigkeit informatisch Methode gewährleisten ehrig Kernidee mathematisch Kategorientheorie beliebig Struktur Sammlung objekten wechselseitig Beziehung Zueinander Charakterisieren einfach Fall bestehen Kategorie Wechselseitig ersetzen markieren Asterisk Morphism objekten Struktur Abstraktionseben integrieren modellierungen Detail angereicheren ursprünglich Zweck mathematisch Kategorientheorie Vergleich mathematisch Theorie mac lan strukturell öhnlichkeiten unterschiedlich mathematisch Disziplin entdecken vereinheitlichen ermöglichen Objekt rein formal Absehung konkret inhalts betrachten erlauben Gegenstände modellieren hinzu hoch abstraktionsgrad Abbildbarkeit verschieden Theorie unser Fall Konzept aufeinander Vergleich ermöglichen verwenden Elemente angewandt Kategorientheorie Gerüst klar präzis darstellen worüber sprechen Darstellung reduktionistisch Sinn behaupten Literaturtheorie können letztlich mathematisch Struktur ersetzen vielmehr zielen literarisch Konzept explizieren Carnap dutilh Novaes operationalisieren Operationalisierung lediglich Prozess Workflow meinen Konflikt literarisch Text identifizieren analysieren Pichler Reiter Form Operationalisieren operationalismus verwechseln streng positivistisch Vorstellung Operationalisierung Bedeutung empirisch operationen gleichsetzt Ziel Beitrag Framework Operationalisierung Analysekonzept literarisch Konflikt skizzieren Framework bestehen wesentlich Abbildungsschritt hierzu entwickeln mathematisch Kategorientheorie entlehnten formalismus erlauben Ebene integrieren Theorie Modell Text zeigen Schritt folgend Analyse Konflikt literarisch Text Aufgabe Theorie literarisch Konflikt festlegen Bedingung Konflikt vorliegen zahlreich Möglichkeit konfliktkonzepen literaturwissenschaftlich Textanalyse nutzen Beitrag fokussieren Analyse Konflikt Figur nutzen Konzept Sozialwissenschaft Gius Rahmen narratologisch Analyse erproben Sinn informell Definition operationalisieren Konflikt Glasl charakterisieren Beziehung Aktor erleben Unvereinbarkeit Aktoren mindestens Beeinträchtigung Verwirklichen Unvereinbarkeit Konflikt Diagramm literaturwissenschaftlich Analyse schwerwiegend Einschränkung Theorie glasls sehen Beobachtung Konflikt Beziehung Aktore verstecken beobachtungsinstanzen explizit Umstand schulden Glasl konfliktbegriff sozialwissenschaften stammen entsprechend literarisch Text Konfliktpartei Einschätzung befragen Ziel Operationalisierung Konfliktbegriff explizieren Komplex erzählsituationen erfasst erweitern paradigmatisch Fall erweitert Konfliktbegriff umfassen nunmehr Objekt Modell Konflikt liegen literarisch text Kandidat objekte Morphism finden sodass strukturerhaltend Abbildung explizierten erweiterten konfliktbegriff figurenkonstellationen erzählinstanzen aufgrund Kombinierbarkeit flexibel modellieren lassen konflikttheori integrieren Aspekt abstellen Objekt gesellschaftlich Wandel Relationstype Objekt einführen Dahrendorfs Unterscheidung Latente Manifest Konflikt zusätzlich Bedingung integrieren Konfliktpartei Konflikt wahrnehmen Definition psychisch Konflikt integrieren Identität Textebene gelten Textphänomene bestimmen figurenkonstellationen Erzählinstanze abbilden schließlich Analyse manuell automatisch identifizieren Bestimmung Konflikt beteiligt Figur wahrnehmend instanzen unvereinbarkeiten wahrgenommen Einschränkung führend Aspekt Aspekt Analyse festlegen Text realisieren Operationalisierung Textebene hängen Anvisiert untersuchungsmodus unterschiedlich weise erfolgen sinnvoll Figur per erkennbar fassen entsprechend erweitern automatisiert Analyse anstreben manuell Analyse vermutlich eher figuren komplex Phänomen fassen charakterzug annotieren wahrgenommen Unvereinbarkeit Fokus repräsentiert Prozesse Denken Fühlen liegen zumindest teilweise automatisch erkennen Sentimentanalyse Abhängigkeit erzählinstanzen beteiligt Figur durchführen automatisch Manuelle Verfahren kombinieren eventuell bieten indikatoren konflikthaftigkeit herauszuarbeiten unabhängig gewählt Phänomen Bestimmung Text gelten Gruppierung Textphänomen hierarchisch Tagset Kategoriensystemen texteigenschafen Modellebene zuordnen abb vorgeschlagen Framework erlauben Workflow Operationalisierung Ebene beginnen Bedarf Ebene wechseln Starre Beschränkung Workflow vielmehr Fragestellung Tief daten Detail einzuarbeiten abstrahieren groß Struktur sichtbar bedeuten genutzt Konfliktdefinition verändern Umsetzung Objekt Relation literarisch Text Definition Analyse poetologisch äúkonflikn äù Autor innen ausweiten Realisierung Textebene Önderung Ebene jeweils Anpassung eben nötig Formalisierung Konfliktbegriff Literaturwissenschaft skizziert Modell Textanalyse nutzen zeigen ausgehend Theorie Anwendung Text gelangen Prozess angewandt Kategorientheorie allgemein anwendbar Workflow Operationalisierung Analysekonzept quantifizierend Analyse angewandte Kategorientheorie ermöglichen Digital Humanitie Bereich computationell Analyse zwingend notwendig Formalisierung Fokus Theorie insgesamt sehen substanziell Vorteil Workflow,"[('konflikt', 0.4163641000118727), ('kategorientheorie', 0.30448929315411405), ('mathematisch', 0.21327920598840544), ('konfliktbegriff', 0.20431701324961538), ('operationalisierung', 0.20143699943331947), ('theorie', 0.15837087796312463), ('workflow', 0.15503312448449197), ('analyse', 0.13229375304165733), ('unvereinbarkeit', 0.12259020794976921), ('integrieren', 0.11568292213813629)]"
2023,DHd2023,JACKE_Janina_Vom_sprachlichen_Indikator_zum_komplexen_Phänom.xml,Vom sprachlichen Indikator zum komplexen Phänomen?,"Janina Jacke (Georg-August-Universität Göttingen, Deutschland)","Operationalisierung, Literaturwissenschaft, Komplexität, Theorie, unzuverlässiges Erzählen, Modellierung","Entdeckung, Beziehungsanalyse, Modellierung, Theoretisierung, Literatur, Text","Im Feld der Digital Humanities hat sich die computationelle Literaturwissenschaft als Teildisziplin etabliert. Von literaturwissenschaftlicher Seite wird allerdings immer wieder angemahnt, die computationelle Auseinandersetzung mit Literatur sei zu reduktionistisch 'unter anderem weil Textanalysen nur statistisch-deskriptiv und weitgehend kontextfrei möglich seien (vgl. Gius/Jacke 2022). Derartigen Bedenken lässt sich auf unterschiedliche Weise begegnen. Eine Möglichkeit besteht in der Akzeptanz, dass die computationelle und die traditionelle Literaturwissenschaft schlichtweg unterschiedliche Fragen an Texte stellen. Soll die Idee eines Brückenschlags zwischen traditioneller und computationeller Literaturwissenschaft dagegen nicht verworfen werden, gibt es zwei weitere Möglichkeiten: Es kann der ambitionierte Versuch unternommen werden, auch für komplexere literaturwissenschaftliche Fragestellungen (vollständig) computationelle Lösungen zu finden. Solche Versuche sind zwar wünschenswert, sollten aber (durch den großen Aufwand und die oft eingeschränkten Erfolgsaussichten) nicht die einzige Möglichkeit darstellen, computationelle Modellierung für traditionelle literaturwissenschaftliche Fragen fruchtbar zu machen. Der vorliegende Beitrag stellt eine andere Möglichkeit vor, wie ein Brückenschlag aussehen kann: Es kann es fruchtbar sein, dezidiert nur eine computationelle Teil-Operationalisierung komplexer literarischer Phänomens anzustreben und möglichst genau zu ergründen und zu explizieren, welcher Status den entwickelten computationellen Analysemethoden im Zusammenhang mit komplexeren literarischen Phänomenen und Fragestellungen zukommt. Computationelle Modelle können deskriptiv-quantitativ Textmerkmale feststellen, die als Indikatoren für komplexere literaturwissenschaftlich interessante Phänomene verstanden werden können. Sinnvoll verwertbar sind solche Analysen, wenn das Indikationsverhältnis (also die genaue Beziehung zwischen Indikator und komplexem Phänomen) spezifiziert wird. Dabei geht es zum einen um die (aus literaturwissenschaftlicher Perspektive nachvollziehbare) Die vorgeschlagenen Ideen zur Im Folgenden sollen zunächst kurz die für den vorgestellten Ansatz zentralen Begriffe ""Operationalisierung"" und ""Komplexität"" diskutiert werden (Abschnitt 2). Im Anschluss wird die vorgeschlagene Analyse des Indikationsverhältnisses genauer expliziert (Abschnitt 3), die auf einer theoretischen Analyse des Unzuverlässigkeitskonzepts sowie ersten Annotations- und Analyseerfahrungen im Rahmen von CAUTION basiert. Die Operationalisierung geisteswissenschaftlicher Konzepte für die computationelle Textanalyse ist bereits zum Thema extensiver Auseinandersetzung geworden (vgl. Moretti 2013, Döring/Bortz 2016, Pichler/Reiter 2021, Krautter et al. im Erscheinen). Unter der Operationalisierung von Begriffen ist die Angabe von Handlungsschritten zu verstehen, die ausgeführt werden müssen, um das Phänomen identifizieren (bzw. messen und quantifizieren) zu können. Im Feld der Digital Humanities wurde der Begriff von Moretti eingeführt (vgl. Moretti 2013). Seine literaturwissenschaftlichen Beispiele zeigen aber, dass die operationalisierten Modelle oft nicht oder nur lose an die zu operationalisierenden literaturwissenschaftlichen Konzepte angeknüpft sind (vgl. Krautter/Pichler/Reiter im Erscheinen). Es liegt der Schluss nahe, dass viele literaturwissenschaftliche Konzepte zu komplex sind, um sich unter Erhaltung ihrer ursprünglichen Bedeutung und Funktion vollständig und eindeutig operationalisieren bzw. computationell modellieren zu lassen. Dennoch scheinen die bisherigen Beiträge nicht grundsätzlich von dem Ziel Abstand zu nehmen, mit computationellen Operationalisierungen vollständige Übersetzungen bzw. direkte Entsprechungen komplexer Konzepte zu entwickeln. Reduzierte Ansprüche lassen sich lediglich insofern feststellen, als im Falle umstrittener Konzepte eine begründete Auswahl aus dem Definitionsangebot (vgl. Döring/Bortz 2016, 226) oder möglicher Kontexte (vgl. Pichler/Reiter 2021, 6) getroffen wird. Grundsätzlich wird aber davon ausgegangen, dass durch die Aufgliederung in Teilschritte (vgl. Pichler/Reiter 2021, 19–23) bzw. die Kombination unterschiedlicher Indikatoren (vgl. Döring/Bortz 2016, 229) die entwickelten Modelle direkt auf die komplexen geisteswissenschaftlichen Phänomene zielen. Es wird nicht in Betracht gezogen, dass es 'um Reduktionismus zu vermeiden 'notwendig oder sinnvoll sein könnte, mit dem computationellen Modell dezidiert nur einen Baustein zu ihrer Analyse beizutragen und dessen genaue Funktion zu explizieren. Das Kriterium der Validität (vgl. Drost 2011) computationeller Modelle wäre aus dieser Perspektive neu zu denken: Es ist nicht notwendig, dass die Modelle dasjenige messen, nach dem Literaturwissenschaftler:innen fragen 'sofern die Relation zwischen Modell und Phänomen so expliziert wird, dass ein (nicht-computationelles) literaturwissenschaftliches Weiterarbeiten mit den erzielten Ergebnissen ermöglicht wird (vgl. hier auch Flick 2012 zu Triangulation).  Unzuverlässiges Erzählen ist insofern zusammengesetzt, als es in distinkte Typen mit unterschiedlichen Eigenschaften zerfällt (vgl. Jacke 2020, 17–57). Zudem muss (zumindest laut einigen Definitionen) eine Kombination aus mehreren (Text-)Eigenschaften gegeben sein, damit unzuverlässiges Erzählen vorliegt. Diese Probleme ließen sich allerdings noch durch begründete Auswahl und ein Zergliedern in Analyseschritte adressieren, wie von Döring/Bortz bzw. Pichler/Reiter vorgeschlagen. Zum anderen können literarische Phänomene dadurch komplex sein, dass ihre Feststellung in einem Text interpretationsabhängig ist. Interpretationsabhängigkeit ist dabei als gradierbare Eigenschaft zu verstehen 'der Grad bemisst sich danach, in welchem Maße nicht-wahrheitserhaltende Schlüsse und strittige (Kontext-)Annahmen notwendig sind, um das Vorliegen des Phänomens festzustellen. Auch bei der Feststellung des Vorliegens interpretationsabhängiger Phänomene spielen aber in der Regel der literarische Text selbst bzw. konkrete identifizierbare Texteigenschaften eine zentrale Rolle 'die alleinige Bezugnahme auf sie ist aber eben nicht ausreichend, um für ihr Vorliegen zu argumentieren. Bei starker Interpretationsabhängigkeit ist es in der Regel extrem kompliziert (bzw. möglicherweise unmöglich), ein literaturwissenschaftliches Konzept vollständig (computationell) zu operationalisieren. Dies ergibt sich zum einen durch die Schwierigkeit, die im Rahmen von Interpretation stattfindenden zahlreichen und schwer zu fassenden Prozesse überhaupt zu rekonstruieren, zum anderen durch die Herausforderung, extratextuelles Wissen in computationellen Analysen abzubilden. Einige zentrale Aspekte, die für das Vorliegen unzuverlässigen Erzählens notwendig sind, sind interpretationsabhängig. Beispielsweise ist es für faktenbezogene Unzuverlässigkeit notwendig, dass eine Erzählfigur falsche Aussagen über die erzählte Welt tätigt (oder relevante Informationen auslässt, vgl. Kindt 2008, 53). Eine derartige Diagnose erfordert eine Entscheidung darüber, was in der erzählten Welt wahr und relevant ist. Für Entscheidungen dieser Art sind zwar Textargumente (vgl. Descher/Petraschka 2019, 88–93) sehr wichtig. Aber zum einen müssen diese Textargumente unter Umständen in komplizierter Weise gegeneinander abgewägt werden und zum anderen sind 'gerade bei potenziell unzuverlässig erzählten Texten 'Kontextannahmen in der Regel unerlässlich für die Rekonstruktion der fiktiven Welt. Während eine vollständige (computationelle) Operationalisierung unzuverlässigen Erzählens also wenig aussichtsreich erscheint, bietet die Relevanz von Textargumenten dennoch einen aussichtsreichen Ausgangspunkt für die Entwicklung computationeller Modelle, die für die Analyse von Unzuverlässigkeit in Texten unterstützend herangezogen werden können. So werden in der (nicht-computationellen) Unzuverlässigkeitsforschung auch tatsächlich Indikatorenlisten zusammengestellt, die unter anderem auch wenig komplexe sprachliche Phänomene enthalten, deren computationelle Modellierung aussichtsreich bzw. bereits umgesetzt ist. Die Indikatoren reichen von konkreten sprachlichen Einzelphänomenen (""Ausrufe, Ellipsen, Wiederholungen"") und nicht spezifizierten linguistischen Sammelphänomenen (""linguistische Signale für Expressivität und Subjektivität"") über Eigenschaften bzw. Zustände von Erzähler:innen (""Hinweise auf kognitive Einschränkungen"") sowie Sprachhandlungen und Absichten (versuchte ""Rezeptionslenkung durch den Erzähler"", Nünning 1998, 27–28) bis hin zu inhaltlich-strukturellen (verschiedene Arten von Widersprüchen) und inhaltlich-kontextuellen Phänomenen (stark unwahrscheinliche oder unmögliche Aussagen). Eine Analyse der Indikationsbeziehungen wird in der Unzuverlässigkeitsforschung allerdings nicht vorgenommen. Um Klarheit über die Relevanz konkreter computationell modellierter sprachlicher Indikatoren im Zusammenhang mit komplexen literarischen Phänomenen zu erlangen, sollte zum einen reflektiert und kommuniziert werden, welche Funktion dem Modell zukommen soll, also ob es beispielsweise als Heuristik zum Auffinden für eine Forschungsfrage potenziell relevanter Texte (bzw. zur Exploration von Korpora), in argumentativen Zusammenhängen genutzt (vgl. Gerstorfer 2020) oder in anderen Funktionen eingesetzt werden soll. Hiervon ist abhängig, wie stark die Indikationsbeziehung überhaupt sein muss, um valide (oder: plausible) Ergebnisse erzielen zu können (vgl. Gius/Jacke 2022). Zwei weitere Aspekte der Indikationsbeziehung, die für eine literaturwissenschaftliche Verwertbarkeit computationeller Modelle im Zusammengang mit komplexen Phänomenen analysiert und kommuniziert werden sollten, werden im Folgenden etwas genauer vorgestellt. Grundsätzlich gilt, dass computationelle Modelle, die für die Textanalyse eingesetzt werden, für viele Literaturwissenschaftler:innen insbesondere dann interessant sind, wenn (zumindest ansatzweise) nachvollziehbar wird, Warum, beispielsweise, soll ein sprachliches Phänomen wie Ausrufe unzuverlässiges Erzählen indizieren? Ausrufe sind ein Merkmal expressiver Sprache. Expressive Sprache weist auf eine emotional aufgewühlte Erzählinstanz hin. Eine emotional aufgewühlte Erzählinstanz neigt dazu, etwas durcheinanderzubringen. Eine Erzählinstanz, die etwas durcheinanderbringt, ist disponiert, inkorrekte Aussagen zu treffen. Eine Erzählinstanz, die inkorrekte Aussagen tätigt, lässt sich als unzuverlässige Erzählinstanz einordnen. Während diese Reihe das Fortschreiten von nicht zu stark interpretationsabhängigen Texteigenschaften illustriert, sollten insbesondere bei dem hier gewählten Beispiel auch kausale Zusammenhänge bzw. Richtungen beachtet werden. Eine entsprechende Analyse zeigt, dass das Vorliegen sprachlicher Merkmale wie Ausrufe im Text und das Vorliegen unzuverlässigen Erzählens nicht im eigentlichen Sinn kausal verbunden sind, sondern dass beide Phänomene (intrafiktionaler Logik folgend) die gleiche Ursache haben können 'nämlich eine emotional aufgewühlte Erzählinstanz (siehe auch Reichenbachs Konkrete sprachliche Texteigenschaften, die als Indikatoren für komplexere literarische Phänomene betrachtet werden, können diese Phänomene in unterschiedlicher Art und mit unterschiedlicher Stärke indizieren: (1) Das Vorkommen bestimmter sprachlicher Indikatoren (ggf. mit einer bestimmten Frequenz, in bestimmten Kombinationen oder an bestimmten Stellen in einem Text) kann notwendig oder (meist in Kombination mit anderen Indikatoren) hinreichend für das Vorliegen eines bestimmten komplexen Phänomens sein. Ein Indikator ist dann notwendig für ein Phänomen, wenn er vorliegen muss, sofern das Phänomen vorliegt; hinreichend ist er dann, wenn sein Vorliegen das Vorliegen des Phänomens garantiert (vgl. Brennan 2022). Während ein einzelnes sprachliches Phänomen in der Regel weder notwendig noch hinreichend für das Vorliegen eines komplexen literarischen Phänomens ist, lassen sich unter Einbeziehung der oben genannten Zwischenphänomene als Verbindungsglieder zwischen sprachlichem Indikator und komplexem Phänomen aussagekräftigere Ergebnisse erzielen. (2) Basierend auf dem Vorkommen bestimmter sprachlicher Indikatoren (ggf. mit einer bestimmten Frequenz, in bestimmten Kombinationen oder an bestimmten Stellen in einem Text) kann dem Vorliegen eines bestimmten komplexen Phänomens eine hohe Wahrscheinlichkeit zugeschrieben werden. Solche bedingte Wahrscheinlichkeit lässt sich zum einen ebenfalls auf einer Mikroebene analysieren: So ist beispielsweise anzunehmen, dass expressive Sprache zwar weder notwendig noch hinreichend für eine emotional aufgewühlte Erzählinstanz ist, diese aber mit hoher Wahrscheinlichkeit indiziert. Auch bei der Analyse von Wahrscheinlichkeitszusammenhängen ist es wichtig, Kausalitätsrichtungen zu beachten, um die Relevanz eines computationellen Modells richtig einzuschätzen: Selbst wenn beispielsweise Emotionalität der Erzählinstanz sich mit sehr hoher Wahrscheinlichkeit in Form (automatisch messbarer) emotionaler Sprache niederschlägt, und wenn dieselbe Emotionalität ebenfalls mit hoher Wahrscheinlichkeit unzuverlässiges Erzählen hervorbringt, muss untersucht werden, ob die sprachlichen Indikatoren und unzuverlässiges Erzählen nicht mit ebenso hoher Wahrscheinlichkeit jeweils andere (verschiedene) Ursachen haben können. Insgesamt kann die Identifikation von mittelkomplexen Phänomenen als Verbindungsgliedern zwischen sprachlichen Indikatoren und komplexen literarischen Phänomenen also nicht nur die Relevanz computationell modellier- und auswertbarer Texteigenschaften für literaturwissenschaftlich interessante Phänomene logisch-inhaltlich besser begreifbar machen. Sie ermöglicht auch eine aussagekräftigere (theoretische und methodologische) Analyse des komplexen literaturwissenschaftlichen Konzepts sowie eine Analyse der Indikationsverhältnisse und schafft die Grundlage für Teilerfolge (bspw. falls zwar letztlich kein signifikantes Indikationsverhältnis zwischen sprachlichem Indikator und komplexem Phänomen festgestellt werden kann, wohl aber zwischen sprachlichem Indikator und literaturwissenschaftlich ebenfalls relevanten Zwischenphänomenen). Der hier vorgeschlagene Weg, komplexe literarische Phänomene im Rahmen computationeller Zugänge bewusst nur partiell zu operationalisieren und zu modellieren, stellt auf diese Weise eine Möglichkeit dar, wie",de,Feld Digital Humanitie computationelle Literaturwissenschaft Teildisziplin etablieren literaturwissenschaftlich Seite anmahnen computationelle Auseinandersetzung Literatur reduktionistisch textanalyse weitgehend kontextfrei Gius jack derartig Bedenken lässen unterschiedlich Weise begegnen Möglichkeit bestehen Akzeptanz computationelle traditionell Literaturwissenschaft schlichtweg unterschiedlich Frage Text stellen Idee Brückenschlag traditionell Computationeller Literaturwissenschaft verwerfen Möglichkeit ambitioniert Versuch unternehmen komplex literaturwissenschaftlich Fragestellung vollständig computationelle Lösung finden versuch wünschenswert Aufwand eingeschränkt Erfolgsaussicht einzig Möglichkeit darstellen computationelle Modellierung traditionell literaturwissenschaftlich Frage fruchtbar vorliegend Beitrag stellen Möglichkeit Brückenschlag aussehen fruchtbar dezidiern computationelle Komplexer literarisch Phänomen anstreben möglichst genau ergründen explizieren Status entwickelt computationelel Analysemethoden Zusammenhang komplex literarisch Phänomen Fragestellung zukommen computationelle Modell Textmerkmal feststellen indikatoren komplex literaturwissenschaftlich interessant phänomen verstehen sinnvoll verwertbar analysen Indikationsverhältnis genau Beziehung Indikator Komplexem Phänome spezifizieren literaturwissenschaftlich Perspektive nachvollziehbar vorgeschlagen Idee folgend vorgestellt Ansatz zentral begriffe Operationalisierung Komplexität diskutieren Abschnitt Anschluss vorgeschlagen Analyse indikationsverhältnisses genau explizieren Abschnitt theoretisch Analyse Unzuverlässigkeitskonzept Analyseerfahrung Rahmen Caution basieren Operationalisierung geisteswissenschaftlich Konzept computationelle Textanalyse Thema extensiv Auseinandersetzung moretti döring Bortz Pichler Reiter Krautter Et erscheinen Operationalisierung begreifen Angabe Handlungsschritt verstehen ausführen Phänomen identifizier messen quantifizieren Feld Digital Humanitie Begriff moretti einführen moretti literaturwissenschaftlich Beispiel zeigen operationalisiert Modell Lose operationalisierend literaturwissenschaftlich Konzept anknüpfen Krautter Pichler Reiter erscheinen liegen schluss nahe literaturwissenschaftlich Konzept komplex Erhaltung ursprünglich Bedeutung Funktion vollständig eindeutig operationalisier Computationell Modelliere lassen dennoch scheinen bisherig Beitrag grundsätzlich Ziel Abstand nehmen computationell Operationalisierung vollständig übersetzung direkt Entsprechung komplex Konzept entwickeln reduzierte ansprüche lassen lediglich insofern feststellen Fall umstritten Konzept begründet Auswahl Definitionsangebot döring Bortz möglich Kontexte Pichler Reiter treffen grundsätzlich ausgehen Aufgliederung Teilschritte Pichler Reiter Kombination unterschiedlich indikatoren döring Bortz entwickelt Modell direkt komplex geisteswissenschaftlich phänomen Ziel betracht ziehen Reduktionismus vermeiden notwendig sinnvoll computationell Modell dezidieren Baustein Analyse beitragen genau Funktion explizieren Kriterium Validität drost Computationeller Modell Perspektive neu denken notwendig modell dasjenig Messe Literaturwissenschaftler innen Frage sofern Relation Modell Phänom explizieren Literaturwissenschaftliches weiterarbeiten erzielt Ergebnis ermöglichen flick Triangulation unzuverlässiges erzählen insofern zusammensetzen Distinkt Type unterschiedlich eigenschaften zerfallen Jacke zudem zumindest laut Definition Kombination mehrere geben unzuverlässig erzählen vorliegen Problem lassen begründet Auswahl Zergliedern analyseschritte adressieren döring Bortz Pichler Reiter vorschlagen literarisch Phänomen komplex Feststellung Text interpretationsabhängig Interpretationsabhängigkeit gradierbar Eigenschaft verstehen Grad bemissen Maß Schlüsse strittig notwendig vorliegen Phänomen feststellen Feststellung Vorliegen interpretationsabhängig Phänomen spielen Regel literarisch Text konkret identifizierbare texteigenschafen zentral Rolle alleinig Bezugnahme ausreichend vorliegen argumentieren stark Interpretationsabhängigkeit Regel extrem kompliziert möglicherweise unmöglich literaturwissenschaftlich Konzept vollständig computationell operationalisieren ergeben Schwierigkeit Rahmen Interpretation stattfindend zahlreich schwer fassend prozesse rekonstruieren Herausforderung extratextuell wissen computationell Analyse abzubilden zentral Aspekt vorlieg unzuverlässig erzählens notwendig interpretationsabhängig beispielsweise faktenbezogen Unzuverlässigkeit notwendig Erzählfigur falsch Aussage erzählt Welt tätigen relevant Information auslässt Kindt derartig Diagnose erfordern Entscheidung erzählt Welt relevant Entscheidung Art textargument desch Petraschka wichtig Textargument Umstände kompliziert Weise gegeneinander abgewägt potenziell unzuverlässig erzählt Text kontextannahmen Regel unerlässlich Rekonstruktion fiktiv Welt vollständig computationelle Operationalisierung unzuverlässigen erzählens aussichtsreich erscheinen bieten Relevanz Textargument dennoch aussichtsreich Ausgangspunkt Entwicklung computationell Modell Analyse Unzuverlässigkeit Text unterstützend heranziehen Unzuverlässigkeitsforschung tatsächlich indikatorenlisen zusammenstellen komplex sprachlich Phänomen enthalten computationelle Modellierung aussichtsreich umsetzen indikatoren reichen konkret sprachlich einzelphänomener ausrufen ellipsen wiederholung spezifizierten linguistischen sammelphänomenen linguistisch Signal Expressivität Subjektivität eigenschaften zuständ Erzähler innen Hinweis kognitiv einschränkung sprachhandlung Absicht versucht Rezeptionslenkung Erzähler Nünning verschieden Art Widersprüch phänomen stark unwahrscheinlich unmöglich Aussage Analyse indikationsbeziehungen Unzuverlässigkeitsforschung vornehmen Klarheit Relevanz konkret Computationell Modellierter sprachlich indikatoren Zusammenhang komplex literarisch Phänomen erlangen reflektieren kommunizieren Funktion Modell zukommen beispielsweise Heuristik auffind forschungsfrag Potenziell relevant Text Exploration Korpora argumentativ Zusammenhäng nutzen Gerstorfer Funktion einsetzen hiervon abhängig stark Indikationsbeziehung Valide plausibel Ergebnis erzielen Gius jack Aspekt Indikationsbeziehung literaturwissenschaftlich Verwertbarkeit Computationeller Modell zusammengang komplex Phänomen analysieren kommunizieren folgend genau vorstellen grundsätzlich gelten computationelle Modell Textanalyse einsetzen Literaturwissenschaftler innen insbesondere interessant zumindest ansatzweise nachvollziehbar beispielsweise sprachlich phänomen ausrufe unzuverlässiges erzählen indizieren ausrufe Merkmal expressiv Sprache expressiv Sprache weisen emotional aufgewühlt erzählinstanz emotional aufgewühlt Erzählinstanz neigen durcheinanderbringen Erzählinstanz durcheinanderbringen disponieren inkorrekt Aussage treffen Erzählinstanz inkorrekt Aussage tätigen lässen unzuverlässig Erzählinstanz einordnen Reihe Fortschreiten stark interpretationsabhängigen texteigenschaften illustrieren insbesondere gewählt kausal zusammenhänge Richtung beachten entsprechend Analyse zeigen vorliegen sprachlich Merkmal ausrufe Text vorlieg unzuverlässig Erzählen eigentlich Sinn Kausal verbinden phänomen intrafiktional Logik folgend gleich ursache nämlich emotional aufgewühlt Erzählinstanz sehen Reichenbach konkret sprachlich Texteigenschaft indikatoren komplex literarisch Phänomen betrachten Phänomen unterschiedlich Art unterschiedlich Stärke indizieren vorkommen bestimmt sprachlich indikatoren bestimmt Frequenz bestimmt Kombination bestimmt Stelle Text notwendig meist Kombination indikatoren hinreichend vorliegen bestimmt komplex Phänomen Indikator notwendig Phänomen vorliegen sofern Phänomen vorliegen hinreichend vorliegen vorliegen Phänomen garantieren Brennan einzeln sprachlich Phänomen Regel weder notwendig hinreichend vorliegen komplex literarisch Phänomen lassen Einbeziehung genannt Zwischenphänomene Verbindungsglieder sprachlich Indikator Komplexem Phänom aussagekräftiger Ergebnis erzielen basierend vorkomm bestimmt sprachlich indikatoren bestimmt Frequenz bestimmt Kombination bestimmt Stelle Text vorliegen bestimmt komplex Phänomen hoch Wahrscheinlichkeit zuschreiben bedingt Wahrscheinlichkeit lässen ebenfalls Mikroebene analysieren beispielsweise annehmen expressiv Sprache weder notwendig hinreichend emotional aufgewühlt Erzählinstanz hoch Wahrscheinlichkeit indizieren Analyse Wahrscheinlichkeitszusammenhäng wichtig Kausalitätsrichtung beachten Relevanz computationell Modell einschätzen beispielsweise Emotionalität Erzählinstanz hoch Wahrscheinlichkeit Form automatisch Messbarer emotional Sprache niederschlägen Emotionalität ebenfalls hoch Wahrscheinlichkeit unzuverlässig erzählen hervorbringen untersuchen sprachlich indikatoren unzuverlässig erzählen hoch Wahrscheinlichkeit jeweils verschieden ursachen insgesamt Identifikation mittelkomplex Phänomen verbindungsgliedern sprachlich indikatoren komplex literarisch Phänomen Relevanz computationell auswertbar texteigenschafen literaturwissenschaftlich interessant phänomen begreifbar ermöglichen aussagekräftiger theoretisch methodologisch Analyse komplex literaturwissenschaftlich Konzept Analyse Indikationsverhältnisse schaffen Grundlage Teilerfolge falls letztlich signifikant indikationsverhältnis sprachlich Indikator Komplexem Phänom feststellen sprachlich indikator literaturwissenschaftlich ebenfalls relevant zwischenphänomenen vorgeschlagen Weg komplex literarisch Phänomen Rahmen Computationeller Zugänge bewussen partiell operationalisieren Modelliere stellen Weise Möglichkeit dar,"[('phänomen', 0.33051220688006655), ('computationelle', 0.25891248245429915), ('komplex', 0.20810027840596784), ('erzählinstanz', 0.196709688088169), ('indikatoren', 0.19036155344133177), ('sprachlich', 0.18609367634892593), ('computationell', 0.1747190750622028), ('unzuverlässig', 0.17212097707714788), ('vorliegen', 0.1582384500939359), ('literaturwissenschaftlich', 0.1409630428520964)]"
2023,DHd2023,HILGER_Agnes_Dunkelgrün__blassgrün__fenchelgrün_oder__Über_d.xml,"Dunkelgrün, blassgrün, fenchelgrün oder: Über die Konkretisierung des Vokabulars im deutschsprachigen Roman (1760–1920)","Agnes Hilger (Julius-Maximilians-Universität Würzburg, Deutschland)","Beschreibung, Telling, Showing, Literaturgeschichte, Roman","Inhaltsanalyse, Modellierung, Stilistische Analyse, Literatur, Forschungsergebnis, Text","Im Jahr 2012 entdecken Ryan Heuser und Long Le-Khac, dass eine Reihe von semantisch verwandten Wörtern in englischsprachigen Romanen über das 19. Jahrhundert hinweg immer häufiger verwendet wird (vgl. Heuser/Le-Khac 2012). Diese Wörter sind tendenziell konkret. Eine Masterarbeit, die dem Poster zugrunde liegt, verfolgte das Ziel, die Beobachtungen von Heuser, Le-Khac und Underwood zunächst versuchsweise für die deutschsprachige Literatur nachzuvollziehen und sodann eine erste Eingrenzung derjenigen Bereiche zu leisten, die von der Entwicklung betroffen sind. Die Ergebnisse sollen hier vorgestellt werden. Das Korpus basiert auf den bei TextGrid und Projekt Gutenberg digital zur Verfügung gestellten Texten (vgl. Neuroth u.a. 2015; Reuters o.J.). Es enthält 1147 zwischen 1760 und 1920 erschiene Romane. Diese sind jedoch nicht gleichmäßig über den Zeitraum verteilt (s. Figure 1). Die Unausgewogenheit soll in anschließenden Arbeiten angegangen werden. Um die Wortfrequenzen zu ermitteln, wurden zunächst Wortlisten erstellt. Als Heuristik diente eine sehr grobe Einteilung in drei Gruppen: 1) Informationen zu Figuren, 2) Informationen zu Räumen und 3) Informationen zu Beschaffenheit und Material. Auf Basis dieser Unterscheidung wurde eine Liste von 31 Wortfeldern erstellt. Um der historischen Sprachstufe und der Domäne Roman gerecht zu werden, wurden die Wortlisten anschließend mit einem Word-Embedding-Modell erweitert. Dafür wurde ein auf CommonCrawl trainiertes Fasttext-Modell auf dem Roman-Korpus weitertrainiert (vgl. Bojanowski 2016). Aufgrund guter Performance in ähnlichen Tasks schien ein solches Fasttext-Modell ausreichend (vgl. Ehrmanntraut u.a. 2021). Um die Wortlisten zu erweitern, wurden zu den extrahierten Wörtern abhängig von der Länge der Liste die zwei bis zwanzig nächsten Nachbarn ermittelt und, sofern nicht schon vorhanden, der Liste angehängt. Neben Wörtern wie Die Romane wurden mit dem Python-Paket spaCy lemmatisiert und für jedes Wortfeld die zugehörigen Wortfrequenzen berechnet (vgl. Honnibal/Montani 2017). Nimmt man die Wortfrequenzen aller 31 konkreten Wortfelder zusammen, ergibt sich ein signifikanter Anstieg (Mann-Kendall-Test, Œ±=0,01, p=2,22e-16). In Figure 2 repräsentiert jeder Punkt einen Roman, die y-Achse gibt jeweils die Wortfrequenz an. Im Korpus gibt es also einen ähnlichen Trend wie in den englischsprachigen Texten. Insbesondere bei den Wortfeldern, die Figuren, Gebäude und Innenräume beschreiben, gibt es signifikante Anstiege. Bei den Wortfeldern, die der Darstellung von Naturräumen (z.B. die Wortfelder Bau/Gebäude Gebäudeteil Zimmer Einrichtungsgegenstand/ Möbel Heimtextilie Haushaltsgegenstand/ Haushaltsprodukt Dekorationsgegenstand/ Ziergegenstand Gras/Grünfläche Gartenanlage/ Grünanlage Weg Gewächs/Pflanze Bewaldung/Wald Kunstobjekt Bild Körperteil Bekleidung/Kleidung Bekleidungsteil/ Kleidungsteil Aufmachung/Outfit Aussehensspezifisch Tasche Geschmeide/Schmuck Gewebe/Stoff/Textil Farbspezifisch Helligkeitsspezifisch Oberflächenspezifisch Muster/Musterung Formspezifisch Geruch Ornament/Verzierung Holz Die Ergebnisse der Arbeit deuten auf eine grundlegende Veränderung im untersuchten Korpus hin, die von der germanistischen Literaturgeschichte bislang nicht wahrgenommen wurde: Die Art und Weise, in der Romane ihre fiktive Welt physisch gestalten, wandelt sich über einen mehrere Epochen umfassenden Zeitraum hinweg erheblich. Zudem ermöglichen die Ergebnisse der Arbeit eine erste Differenzierung. Besonders betroffen scheinen Informationen über das physische Erscheinungsbild von Figuren und Orten, an denen Figuren leben. Eine anschließende Arbeit soll diese Ergebnisse konkretisieren.",de,entdecken Ryan heus Long Reihe semantisch verwandt wörtern englischsprachig Roman Jahrhundert hinweg häufig verwenden heuser Wörter tendenziell konkret Masterarbeit Poster zugrunde liegen verfolgen Ziel Beobachtung Heuser Underwood versuchsweise deutschsprachig Literatur nachvollziehen Sodann Eingrenzung Bereich leisten Entwicklung betreffen Ergebnis vorstellen Korpus basieren Textgrid Projekt gutenberg Digital Verfügung gestellt Text Neuroth Reuters enthalten erschien Roman gleichmäßig Zeitraum verteilen Figure Unausgewogenheit anschließend Arbeit angehen Wortfrequenz ermitteln wortli erstellen Heuristik dienen grob Einteilung Gruppe Information Figur Information räumen informationen Beschaffenheit Material Basis Unterscheidung Liste Wortfelder erstellen historisch sprachstufe Domäne Roman gerecht wortli anschließend erweitern Commoncrawl trainiertes weitertrainieren Bojanowski aufgrund Performance ähnlich Tasks scheinen ausreichend ehrmanntrauen Wortliste erweitern extrahiert wörtern abhängig Länge Liste nächster Nachbar ermitteln sofern vorhanden Liste anhängen wörtern Roman Spacy lemmatisieren jeder Wortfeld zugehörig wortfrequenz berechnen honnibal Montani nehmen wortfrequenz Konkrete wortfelder ergeben signifikant Anstieg Figure repräsentieren Punkt Roman jeweils Wortfrequenz Korpus ähnlich Trend englischsprachig Text insbesondere Wortfelder Figur Gebäude innenräume beschreiben signifikant Anstieg Wortfelder Darstellung Naturräum wortfelder Bau Gebäude gebäudeteil zimm Einrichtungsgegenstand Möbel Heimtextilie Haushaltsgegenstand haushaltsprodukt Dekorationsgegenstand ziergegenstand Gras grünfläch Gartenanlage Grünanlage Weg gewächs pflanz Bewaldung Wald kunstobjekt Bild Körperteil Bekleidung Kleidung Bekleidungsteil Kleidungsteil Aufmachung Outfit aussehensspezifisch tasch Geschmeide Schmuck Gewebe Stoff Textil farbspezifisch helligkeitsspezifisch oberflächenspezifisch Muster Musterung formspezifisch geruch ornament Verzierung Holz Ergebnis Arbeit deuten grundlegend Veränderung untersucht Korpus germanistisch Literaturgeschichte bislang wahrnehmen Art Weise Romane fiktiv Welt physisch gestalten wandeln mehrere epoch umfassend Zeitraum Hinweg erheblich zudem ermöglichen Ergebnis Arbeit Differenzierung betroffen scheinen Information physisch Erscheinungsbild Figur Ort Figur leben anschließend Arbeit Ergebnis konkretisieren,"[('wortfelder', 0.3203028743339946), ('wortfrequenz', 0.23898606734247188), ('roman', 0.16317582078930018), ('heuser', 0.1412201881973363), ('gebäude', 0.1412201881973363), ('anstieg', 0.12344590054072115), ('wortli', 0.12344590054072115), ('liste', 0.12078049399053968), ('figur', 0.12016878658517112), ('arbeit', 0.11804631459815233)]"
2023,DHd2023,GLAWION_Anastasia_Einfluss_des_häufigen_Lesens_auf_Textwahrn.xml,Einfluss des häufigen Lesens auf Textwahrnehmung: Ergebnisse eines Leseexperiments,"Anastasia Glawion (TU Darmstadt, Deutschland); Thomas Weitin (TU Darmstadt, Deutschland)","Literaturrezeption, Leser:innenforschung, Emotion","Literatur, Personen, Text","In dem Vortrag werden Ergebnisse eines Leseexperiments vorgestellt, welches unter anderem darauf abzielte, die Lücke zwischen psychologisch orientierten Lesereaktionsstudien und literaturwissenschaftlich fundierten Rezeptionsstudien (Kavanagh, 2021) zu schließen. Die Stimuli umfassten Passagen aus der beliebten ""Harry Potter""-Buchreihe in deutscher Sprache sowie Auszügen aus ""Harry Potter""-Fanfictions. In dem Experiment sollten folgende Forschungsfragen beantwortet werden: In der Studie wurde eine Reihe von Messmethoden verwendet, darunter die Messung der Augenbewegungen (inklusive der Pupillengröße) und des Hautleitwerts (GSR) der Teilnehmer:innen. Diese beiden Messmethoden werden am häufigsten als Marker von emotionaler Reaktion in Betracht gezogen. Die Originaltexte unter den Stimuli umfassten 40 ""neutrale"" Texte, 40 Texte, die als ""furchteinflößend"", und 40 Texte, die als ""fröhlich"" gekennzeichnet waren (s. Tabelle 1). Diese Textstellen sowie ihre Sentimentmarkierungen wurden aus einer früheren Lesestudie von Hsu (2015) übernommen. Die Liste der Stimuli wurde um Fanfiction-Texte erweitert, die zuvor von 82 Fanfiction-Leser:innen in einer Umfrage ausgewählt wurden, weil sie besonders starke Emotionen bei ihnen ausgelöst hatten. Die vielseitige Auswahl der Stimuli in der Studie von Hsu deckte unterschiedliche Aspekte auf, die mit Wirkung von Literatur verbunden sind. Einer davon war für unsere Analysen besonders anregend: es wurde ein Zusammenhang zwischen Immersion und emotionalen Inhalten, ""especially negative, arousing and suspenseful ones"" (Hsu, Conrad, Jacobs 2014; 1359) festgestellt. Daher interessierten wir uns zunächst dafür, ob die gemessenen Indikatoren für Erregung, Pupillengröße und Hautleitwert (Bradley et al. 2008) vergleichbare Ergebnisse wie andere Studien zur Fiction-Feeling-Hypothese aufzeigen, z. B. dass als ""furchteinflößend"" markierte Passagen stärkere Reaktionen hervorrufen als diejenigen, die das Label ""fröhlich"" oder ""neutral"" tragen (Hsu, Conrad, Jacobs 2014, Eekhof et al. 2021). Diese Reaktion würde in der Klassifikation der Leseemotionen von Miall und Kuiken (2002) in den Bereich der ""narrative feelings"" fallen, also Gefühlen, die gegenüber literarischen Figuren entwickelt werden bzw. auf eine Resonanz mit der Stimmung und dem Schauplatz eines literarischen Textes hindeuten. Von dieser Art der Emotionen erwartet man, dass sie den Emotionsgehalt des Textes ""spiegeln"" (Miall, Kuiken, 2002; 224). Dies ist das erste Experiment in einer Reihe von geplanten Studien am LitLab der TU Darmstadt, die als Ziel die Erforschung des Zusammenhangs zwischen Textsentiment und empirischen Untersuchungen von Leseprozessen haben. Insgesamt haben im Rahmen des aktuellen Experiments 40 deutsche Muttersprachler:innen 150 Textpassagen gelesen (120 Originale, 15 Fanfictions und 15 Badfictions). Anschließend füllten die Teilnehmer:innen einen Fragebogen aus. Entgegen der Vorläuferstudie, wurden keine Fragen zur Immersion gestellt: Die Stimuli waren recht kurz (40-50 Wörter) und wurden in einer zufälligen Reihenfolge präsentiert, was die Immersion behindern würde. Wir erwarteten, dass andere Faktoren das Leseverhalten beeinflussen würden und befragten die 40 Teilnehmer auf drei verschiedene Arten zu ihren Lesegewohnheiten. Da bereits in Experimenten zur Verbindung zwischen Lesen und Theory of Mind die Unterteilung in Gruppen nach Leseerfahrung signifikante Unterschieden aufgedeckt hat (Kidd und Castano 2013, Panero et al., 2016), wollten wir den Einfluss auch bei der emotionalen Reaktion überprüfen. Zunächst wurden Lesegewohnheiten der Teilnehmer:innen mit Hilfe des Reading Habits Questionnaire (Kuijpers et al., 2020) erfasst. Der Fragebogen nimmt die selbst angegebene Lesemenge im Laufe des letzten Jahres auf. Obwohl der Fragebogen vielseitig in seinen Auswertungsmöglichkeiten ist, kann er als Selbstauskunft nur bedingt als zuverlässig eingestuft werden. Intensive Lesephasen und Mehrfachnennungen sind hierbei nur schwer erfassbar. Für die Auswertung wurden Angaben über unterschiedliche Genres summiert und drei fast gleich große Gruppen gebildet: die Teilnehmenden wurden in Vielleser:innen (13 Teilnehmer, Summe der Punktzahlen: 24-80), Durchschnittsleser:innen (13 Teilnehmer, Summe der Punktzahlen: 13-22) und Selten-Leser:innen (14 Teilnehmer, Summe der Punktzahlen: 4-11) eingeteilt. Außerdem haben Proband:innen die deutsche Version des Tests zur Autorenerkennung ausgefüllt (Grolig et al. 2020), ein bewährte Methode um die Kenntnis des Literatursystems oder die langfristige Auseinandersetzung mit Literatur zu erfassen (Panero et al. 2016; Stanovich et al. 1989). Auch hier teilten wir die Proband:innen in drei Gruppen auf: Literaturkenner:innen (14, erkannten 13-38 Autoren richtig), Literatureinsteiger:innen (12, erkannten 1-6 Autoren richtig) und Mittelfeld (14, erkannten 7-12 Autoren richtig). Wie erwartet, gab es einige Überschneidungen zwischen den Gruppierungen, doch die Rangkorrelation zwischen den beiden Angaben war schwach (tau = 0,21). Der dritte Bereich, den wir in Hsus Originalexperiment als nicht ausreichend untersucht betrachteten, war die Einbeziehung des Fandom-Wissens: es wird lediglich erwähnt, dass alle Proband:innen mindestens ein Buch aus der ""Harry Potter""-Reihe gelesen haben. Der Fragebogen der aktuellen Studie enthielt zwei Fragen zum Wissen über das Harry-Potter-Fandom, differenziert nach Filmen und Büchern. Wir erwarteten, dass Fans stärker auf die präsentierten Stimuli reagieren würden. Ein Wert von 0 stand für Teilnehmer, die keines der Bücher gelesen und keinen der Filme gesehen haben, während 5 bedeuten würde, dass alle Filme, alle Bücher und zusätzliches Material gelesen wurden. Insgesamt wurden Teilnehmer mit einer Punktzahl von 4 und 5 der Gruppe ""Fans"" (17) zugeordnet, Teilnehmer mit einer Punktzahl von 0 und 1 galten als ""Nicht-Fans"" (10) und Teilnehmer mit einer Punktzahl von 2 und 3 wurden dem ""Fandom-Mittelfeld"" (13) zugerechnet. Die Rangkorrelation zwischen dem Context Score und den beiden anderen Gruppeneinteilungen ist ebenfalls schwach (tau = 0,26 mit den Autorenerkennungsergebnissen; tau = 0,35 mit der selbstberichteten Lesehäufigkeit). Die Hautleitwertdaten wurden mit Brainvision Recorder aufgenommen und mit Hilfe von Ledalab (Benedek 2010) analysiert und exportiert. Ledalab ist eine Software, die die Segmentierung von Hautleitwertdaten sowie eine automatische Ermittlung von Hautleitwertreaktionen durchführt. Dafür werden zwei unterschiedlichen Methoden verwendet: die TTP-Analyse (trough-to-peak), die auf vorgegebenen zeitlichen Kriterien basiert (Bouscein et al. 2012), und die CDA (continuous decomposition analysis), die das Signal zunächst in seine kontinuierlichen (tonischen) und stimulusbezogenen (phasischen) Komponenten unterteilt und dann mit Hilfe eines Die Pupillometriedaten wurden aus der Eyetracking-Software SR Research Data Viewer exportiert und normalisiert: für jede Testperson wurde eine Baseline der Pupillengröße ermittelt, die auf der durchschnittlichen Pupillengröße basiert, die zwischen den Trials aufgenommen wurde. Um eine mittlere Veränderung der Pupillengröße zu bestimmen, wurde von der mittleren Pupillengröße pro Trial die Baseline subtrahiert. Bei der Datenverarbeitung der Hautleitwertreaktion und der Pupillometrie-Daten wurde besonders darauf geachtet, ob die Daten die Voraussetzungen für einen ANOVA-Test erfüllten: Unabhängigkeit (die durch das Experimentdesign gegeben war), Normalverteilung und Homogenität der Varianz. Es zeigte sich, dass die Pupillengröße und die Daten zur Anzahl der Hautleitwertpeaks normalverteilt waren, die anderen Hautleitwertdaten jedoch nicht. Die normalverteilten Daten wurden mit einem ANOVA-Test untersucht, während für die nicht normal verteilten Werte der Kruskal-Wallis-Test durchgeführt wurde. Nach den Berechnungen mit ANOVA und dem Kruskal-Wallis-Test wurde eine Auswertung der Effekte mit Epsilon-Quadrat durchgeführt, die zeigte, dass die signifikanten Ergebnisse starke Effekte aufweisen und die meisten Ergebnisse, die sich der Signifikanz näherten, mittlere Effekte zeigten. Unsere Ergebnissen zufolge gab es keinen signifikanten Einfluss von Textsentiment auf die Anzahl oder Stärke der Reaktionen, weder beim Hautleitwert noch bei der Pupillengröße. Doch wir konnten einen weiteren signifikanten Faktor ausfindig machen, der bei der Erforschung der Leser:innenreaktionen eine Rolle spielt. Sobald Proband:innen in Gruppen nach den Ergebnissen des Author Recognition Tests eingeteilt wurden, konnte man sehen, dass Literaturkenner:innen signifikant stärkere Reaktionen im Bereich des Hautleitwertes gezeigt haben im Vergleich zu Literatureinsteiger:innen und den Mittelfeld-Proband:innen. Tabelle 2 zeigt die p-Werte des Kruskal-Wallis-Tests. Bei den fettgedruckten Werten wurde der Effekt der Gruppenaufteilung als ""mittel"" eingestuft, während bei unterstrichenen Werten der Effekt als ""stark"" bewertet wurde. Wir sehen, dass die durch den Autorenerkennungstest gebildeten Gruppen bei den meisten Werten signifikante Unterschiede in ihren Mittelwerten aufweisen. Tabelle 3 zeigt, dass eine ähnliche Tendenz in den normalverteilten Variablen auffindbar ist: während die Gruppen, die auf der Basis des Fandomscores und des Reading Habit Questionnaire gebildet wurden, keine signifikanten Unterschiede aufzeigen, zeigen die Gruppen der Autorerkennungstests mittlere bis starke Effekte. Meistens manifestieren sich die signifikanten Effekte in Unterschieden zwischen dem Verhalten der Literaturkenner:innen auf der einen und Literatureinsteiger:innen und dem Mittelfeld auf der anderen Seite, wie in Abbildung 1. Es scheint, als würde die Kenntnis des literarischen Feldes Voraussetzung für häufigere Hautleitwertreaktionen sein. Wird eine Unterteilung nach der Leistung im Autorenerkennungstest vorgenommen, so zeigt sich, dass Literaturkenner:innen bei den meisten Werten signifikant größere Reaktionen zeigten: Es gibt eine höhere Anzahl von Peaks, die Summe der Amplituden ist höher und die durchschnittliche Pupillengröße ist größer. Bei den Literaturkenner:innen sind die Reaktionen auf fröhliche Stimuli am höchsten und auf neutrale Stimuli fast immer am niedrigsten. Die Literatureinsteiger:innen hingegen zeigen meist minimale Werte bei fröhlichen Stimuli und am häufigsten höchste Werte bei neutralen Stimuli. Keiner dieser Werte wich signifikant vom Mittelwert ab. Unsere Ergebnisse zeigen, dass die nach unterschiedlichem Sentiment gelabelten Texte keine signifikanten Unterschiede in der Hautleitwertreaktion und in der Pupillengröße aufzeigen. Die Analysen der Lesegewohnheiten der Proband:innen lassen hingegen darauf schließen, dass diejenigen, die mehr lesen, auch stärkere Reaktionen auf Texte insgesamt aufweisen. Vor allem die Kenntnis des Literatursystems ‚Äì wie der Autorenerkennungstest oft interpretiert wird ‚Äì beeinflusst die körperlichen Reaktionen auf das Lesen in erheblichem Maße. Leser:innen, die mehr Erfahrung mit Literatur haben, reagieren stärker auf literarische Werke und spiegeln dabei den Textsentiment wieder, die ein Text enthält (stärkere Reaktionen auf emotionale Inhalte, schwächere auf neutrale Passagen). Leser mit geringerem Wissen über Literatur scheinen auch auf neutrale Stimuli stark zu reagieren, vielleicht weil sie einen emotionalen Stimulus erwarten und diesen nicht erhalten. Diesen Ergebnissen zufolge ist das Wissen über Literatur für eine andere Art der Reaktion auf Texte verantwortlich. Entgegen unseren Erwartungen zeigte sich kein signifikanter Einfluss von höherer Kenntnis des Werks, obwohl der Gesamtmittelwert des Hautleitwerts bei fröhlichen Stimuli bei Fans höher war. Möglicherweise ist dies auf eine Kombination aus Nostalgie und narrativen Gefühlen zurückzuführen. Schließlich hat die jüngste Leseaktivität, die mit dem RHQ ermittelt wurde, fast keinen Einfluss auf die physiologischen Reaktionen - nur als zusätzlicher Faktor bei der Berücksichtigung der Pupillengröße. Diese korrelativen Zusammenhänge bieten allerdings noch keine Antwort auf die Frage nach der Kausalität ‚Äì die Frage, ob Lektüre die emotionale Reaktion trainiert oder ob empfindsame Menschen sich mehr zu Literatur hingezogen fühlen, bleibt offen. Die Aussagekraft der Ergebnisse ist durch einige Schwachstellen eingeschränkt: beispielsweise sind die Proband:innen überwiegend Studierende und können daher nur schwer als absolute Wenigleser:innen bezeichnet werden. Vielleicht ist das der Grund, warum die Daten so selten Unterschiede zwischen Literatureinsteiger:innen und Mittelfeld-Proband:innen aufzeigen. Darüber hinaus hat die Anzahl der Proband:innen eine eher geringe statistische Aussagekraft (40 Teilnehmer), wovon allerdings nur die Analyse der Proband:innengruppen betroffen ist: Für die Analyse des Sentimenteinflusses auf die Leser:innenreaktion wird die statistische Signifikanz durch die große Anzahl an Trials derselben Sentimentklasse wieder angehoben. Die Ergebnisse dienen zum Anlass, über mehrere Studien hinweg den Einfluss der Lesekompetenz zu berücksichtigen. Zuletzt wären Vergleichsstudien mit anderen literarischen Gegenständen interessant, um die Zusammenhänge der hier vorgestellten Variablen über ""Harry Potter"" hinaus zu beobachten und weitere Aspekte von literarischen Texten wie Stil und Epoche ebenfalls in ihrer Wirkung zu untersuchen.",de,Vortrag Ergebnis leseexperiments vorstellen abzielen Lücke psychologisch orientiert lesereaktionsstudien literaturwissenschaftlich fundiert Rezeptionsstudie Kavanagh schließen Stimuli umfasst Passag beliebt Harry deutsch Sprache Auszüge Harry Experiment folgend Forschungsfrag beantworten Studie Reihe Messmethode verwenden Messung Augenbewegung inklusive Pupillengröße Hautleitwert gsr Teilnehmer innen Messmethode häufig Marker emotional Reaktion betracht ziehen Originaltext Stimuli umfasst neutral Text Text furchteinflößend Text fröhlich kennzeichnen Tabelle Textstell Sentimentmarkierung früh Lesestudie Hsu übernehmen Liste Stimuli erweitern zuvor innen Umfrage auswählen stark Emotion auslösen vielseitig Auswahl Stimuli Studie Hsu decken unterschiedlich Aspekt Wirkung Literatur verbinden analyse anregend Zusammenhang Immersion emotionale inhalen especially negativ arousing and suspenseful oner hsu Conrad jacobs feststellen interessieren gemessen indikatoren Erregung Pupillengröße hautleitweren Bradley et vergleichbar Ergebnis Studie aufzeigen furchteinflößend markiert Passag stark Reaktion hervorrufen Label fröhlich neutral tragen hsu Conrad jacobs Eekhof et Reaktion Klassifikation leseemotionen Miall Kuiken Bereich narrativ Feelings fallen gefühlen literarisch Figur entwickeln Resonanz Stimmung Schauplatz literarisch Text hindeuen Art Emotion erwarten Emotionsgehalt Text spiegeln Miall Kuiken Experiment Reihe geplant Studie Litlab tu Darmstadt Ziel Erforschung Zusammenhang Textsentiment empirisch Untersuchung Leseprozessen insgesamt Rahmen aktuell Experiment deutsch Muttersprachler innen Textpassage lesen original Fanfiction Badfiction anschließend fülln Teilnehmer innen Fragebogen entgegen Vorläuferstudie Frage immersion stellen Stimuli Wörter zufällig Reihenfolge präsentieren Immersion behindern erwarten Faktor Leseverhalt beeinflussen befragen Teilnehmer verschieden Art lesegewohnheiten experimenten Verbindung Lese theory of Mind Unterteilung Gruppe Leseerfahrung signifikant unterschied aufdecken Kidd Castano Panero et einfluss emotional Reaktion überprüfen lesegewohnheiten Teilnehmer innen Hilfe Reading habits Questionnaire Kuijper et erfasst Fragebogen nehmen angegeben Lesemenge Lauf letzter Jahr obwohl Fragebogen vielseitig Auswertungsmöglichkeit Selbstauskunft bedingt zuverlässig einstufen intensiv Lesephase mehrfachnennungen hierbei schwer erfassbar Auswertung Angabe unterschiedlich genre summieren fast Gruppe bilden Teilnehmende Vielleser innen Teilnehmer Summe Punktzahl durchschnittsleser innen Teilnehmer Summe Punktzahl innen Teilnehmer Summe Punktzahl eingeteilen Proband innen deutsch Version Test Autorenerkennung ausgefüllt grolig et bewährt Methode Kenntnis literatursystems langfristig Auseinandersetzung Literatur erfassen panero et stanovich et teilen Proband innen Gruppe literaturkenner innen erkannt Autor Literatureinsteiger innen Erkannt Autor Mittelfeld erkannt Autor erwarten überschneidungen Gruppierung Rangkorrelation Angabe schwach tau Bereich hsus Originalexperiment ausreichend untersuchen betrachten Einbeziehung lediglich erwähnen Proband innen mindestens Buch Harry lesen Fragebogen aktuell Studie enthalten Frage Wissen differenzieren Film büchern erwarten Fan stark präsentiert Stimuli reagieren Wert Stand Teilnehmer keiner büch lesen Film sehen bedeuten Film büch zusätzlich Material lesen insgesamt Teilnehmer Punktzahl Gruppe Fan zuordnen Teilnehmer Punktzahl gelten Teilnehmer Punktzahl zurechnen Rangkorrelation context score Gruppeneinteilung ebenfalls schwach tau autorenerkennungsergebnissen tau selbstberichteten Lesehäufigkeit hautleitwertdat Brainvision Recorder aufnehmen Hilfe Ledalab Benedek analysieren exportieren Ledalab Software Segmentierung Hautleitwertdat automatisch Ermittlung Hautleitwertreaktion durchführen unterschiedlich Methode verwenden vorgegeben zeitlich kriterien basieren bouscein et cda Continuous Decomposition Analysis Signal kontinuierlich tonisch stimulusbezogen phasisch Komponent unterteilt Hilfe Pupillometriedat sr research data viewer exportieren normalisieren Testperson baseline Pupillengröße ermitteln durchschnittlich Pupillengröße basieren Trials aufnehmen mittlerer Veränderung Pupillengröße bestimmen mittlerer Pupillengröße pro trial baselin subtrahieren Datenverarbeitung Hautleitwertreaktion achten daten Voraussetzung erfüllen Unabhängigkeit experimentdesign geben Normalverteilung Homogenität Varianz zeigen Pupillengröße daten Anzahl Hautleitwertpeak normalverteilen hautleitwertdaten normalverteilten daten untersuchen normal verteilt Wert durchführen Berechnung Anova Auswertung effekte durchführen zeigen signifikant ergebniss stark effekte aufweisen meister Ergebnis Signifikanz nähern mittlerer effekte zeigen Ergebnisse zufolge Signifikant einfluss Textsentiment Anzahl Stärke Reaktion weder Hautleitwert Pupillengröße Signifikant Faktor ausfindig Erforschung Leser Innenreaktion Rolle spielen sobald Proband innen Gruppe Ergebnis Author Recognition Test eingeteilen sehen literaturkenner innen Signifikant stark Reaktion Bereich hautleitwertes zeigen Vergleich literatureinsteige innen innen Tabelle zeigen Fettgedruckt werten Effekt Gruppenaufteilung einstufen unterstrichen Wert Effekt stark bewerten sehen autorenerkennungstest gebildet Gruppe meister Wert signifikant Unterschied Mittelwert aufweisen Tabell zeigen ähnlich Tendenz normalverteilten variabl auffindbar Gruppe Basis fandomscores Reading habit questionnaire bilden signifikanten Unterschied aufzeigen zeigen Gruppe Autorerkennungstests mittlere stark effekte meistens manifestieren signifikant effekte unterschiede verhalten Literaturkenner innen Literatureinsteiger innen Mittelfeld Seite Abbildung scheinen Kenntnis literarisch feld Voraussetzung häufig Hautleitwertreaktione Unterteilung Leistung Autorenerkennungstest vornehmen zeigen Literaturkenner innen meister wert Signifikant groß Reaktion zeigen hoch Anzahl Peaks Summe amplituden hoch durchschnittlich Pupillengröße groß literaturkenner innen Reaktion Fröhlich Stimuli hoch neutral Stimuli fast niedrig Literatureinsteiger innen hingegen zeigen meist minimal Wert Fröhlich Stimuli häufig hoch Wert neutral Stimuli Wert wich signifikant Mittelwert Ergebnis zeigen unterschiedlich Sentiment gelabelt Text signifikant Unterschied Hautleitwertreaktion Pupillengröße aufzeigen Analyse lesegewohnheiten Proband innen lassen hingegen schließen lesen stark Reaktion Text insgesamt aufweisen Kenntnis literatursystem äì Autorenerkennungstest interpretieren äì beeinflussen körperlich Reaktion lesen erheblich Maß Leser innen Erfahrung Literatur reagieren stark literarisch Werk spiegeln Textsentiment Text enthalten stark Reaktion emotional Inhalt schwäch neutral Passage Leser Geringerem wissen Literatur scheinen neutral Stimuli stark reagieren emotional stimulus erwarten erhalten Ergebnis zufolge wissen Literatur Art Reaktion Text verantwortlich entgegen unser Erwartung zeigen Signifikanter einfluss hoch Kenntnis werks obwohl Gesamtmittelwert Hautleitwert fröhlich Stimuli Fan hoch möglicherweise Kombination Nostalgie narrativ gefühlen zurückführen schließlich jung Leseaktivität rhq ermitteln fast einfluss physiologisch Reaktion zusätzlich Faktor Berücksichtigung Pupillengröße korrelativ zusammenhänge bieten Antwort Frage Kausalität äì Frage lektüren emotional Reaktion trainieren empfindsam Mensch Literatur hinziehen fühlen bleiben Aussagekraft Ergebnis Schwachstelle einschränken beispielsweise Proband innen überwiegend studierend schwer absolut Wenigleser innen bezeichnen Grund daten selten Unterschied Literatureinsteiger innen innen aufzeigen hinaus Anzahl Proband innen eher gering statistisch Aussagekraft Teilnehmer wovon Analyse Proband innengruppe betreffen Analyse sentimenteinflusses Leser Innenreaktion statistisch Signifikanz Anzahl Trials Sentimentklasse anheben Ergebnis dienen Anlass mehrere Studie hinweg Einfluss Lesekompetenz berücksichtigen zuletzt sein vergleichsstudien literarisch Gegenständ interessant Zusammenhäng vorgestellt Variablen Harry Potter hinaus beobachten Aspekt literarisch Text Stil epochen ebenfalls Wirkung untersuchen,"[('innen', 0.3679346634768811), ('stimuli', 0.30057749157788), ('reaktion', 0.29632943896892205), ('pupillengröße', 0.2755293672797234), ('teilnehmer', 0.20542199646850223), ('proband', 0.17689363617665263), ('punktzahl', 0.15028874578894), ('signifikant', 0.14022198379866657), ('fröhlich', 0.12524062149078335), ('literaturkenner', 0.12524062149078335)]"
2023,DHd2023,KRAUTTER_Benjamin_Skalierungspraktiken_in_der_computergestüt.xml,Skalierungspraktiken in der computergestützten Analyse von literarischen Texten,"Benjamin Krautter (Universität zu Köln, Deutschland)","scalable reading, Netzwerkanalyse, Drama","Inhaltsanalyse, Theoretisierung, Netzwerkanalyse, Literatur","In den vergangenen Jahren haben Publikationen aus dem Bereich der digitalen Literaturwissenschaft vermehrt auf das durch den Altphilologen und Anglisten Martin Mueller geprägte Konzept s In einem ersten Schritt meines Beitrags werde ich die verschiedenen Dimensionen, auf die sich Muellers Konzeption von In einem programmatisch ausgerichteten Blogbeitrag hob Mueller 2012 hervor, wie ihn ""[t]he charms of Google Earth"" (Mueller 2012,¬†o.S.) zu Die Skalenpluralität beginnt erstens bei der Textgrundlage: Literarische Texte liegen in ""einer weiten ""Scale"" von Surrogaten"" (Weitin 2015,¬†10) vor, die nebeneinander koexistieren: ""Our typical encounter with a text is through a surrogate"" (Mueller 2013,¬†o.S.). Mueller spricht an dieser Stelle von Surrogaten, da immer schon mit unterschiedlich gearteten Repräsentationen des Originals gearbeitet wurde und wird: Das können beispielsweise Faksimiles, Text- und Werkausgaben, Digitalisate oder auch speziell kodierte Textsammlungen sein (vgl. dazu Mueller 2014,¬†¬ß 4–20). Surrogate können darüber hinaus in stark transformierter oder abstrahierter Form auftreten, beispielsweise in Gestalt von Häufigkeitswortlisten. Auch die Netzwerkanalyse fußt demnach auf Surrogaten. Peer Trilcke und Frank Fischer sprechen von einem ""Zwischenformat"", das in ihrem Fall nur noch diejenigen Strukturinformationen der Dramen vorhalte, die zur Netzwerkerstellung herangezogen werden (Trilcke, Fischer 2018,¬†Kap.¬†3). Der Dramentext selbst ist nicht mehr Teil des Zwischenformats. An diese unterschiedlichen Repräsentationsformen von Literatur ist zweitens die Frage des Umfangs geknüpft: Wie groß ist der Untersuchungsgegenstand? Handelt es sich nur um einen einzelnen Text, vielleicht sogar nur um einen Ausschnitt des Textes, oder aber um eine größere Sammlung von Texten? Wie umfangreich ist diese Sammlung? Nicht nur die Zahl der zu betrachtenden Texte, auch die Textsorte kann hier Teil der Skalierungsfrage sein: Sollen kurze Novellen oder 1000-seitige Langromane untersucht werden, ein kurzer Einakter oder Karl Kraus"" monumentales Lesedrama Drittens stellt sich die Frage nach der Größe der Analyseeinheiten. Morettis Mueller denkt die methodische Bezugsgröße viertens vielmehr selbst auf einer Art Skala. Wie Weitin gemeinsam mit Thomas Gilli und Nico Kunkel (2016,¬†115) herausstellt, umfasse Im folgenden Abschnitt möchte ich die mit einer Praxis des Die Automatisierung führt jedoch zu einigen Einschränkungen. So ist die oben dargelegte Formalisierung von Figureninteraktionen zwar ähnlich, aber nicht deckungsgleich mit dem von Solomon Marcus (1973,¬†358) vorgeschlagenen und zum kodifizierten Handbuchwissen (vgl. etwa Pfister 2011,¬†235–240) gewordenen Begriff der Konfiguration. Die Figurenkonfiguration eines Dramas ändert sich immer dann, wenn eine Figur die Bühne betritt oder verlässt, also das am Bühnengeschehen beteiligte Personal zumindest in Teilen wechselt. Dramen, die Prinzipien des französischen Klassizismus folgen, sind durch die im Nebentext markierten Auf- und Abtritte strukturiert. Konfiguration und Szenengrenze fallen dann 'zumindest in der Theorie 'zusammen. Anders ist das bei Stücken, die sich an Shakespeares Poetik orientieren. Hier sind die Szenengrenzen zumeist an einen Ortswechsel gebunden. Daher können Figuren auf- oder abtreten, ohne dass zwangsläufig eine neue Szene konstituiert wird. Da Auf- und Abtritte von Figuren im Nun stellt sich die Frage, wie sich solche Kopräsenznetzwerke sinnvollerweise in die etablierte Dramenanalyse integrieren lassen. Einen prominenten Versuch, Netzwerkanalysen in den Verstehensprozess literarischer Texte zu integrieren, unternimmt Moretti in seinem Essay Abbildung¬†1: Kopräsenznetzwerk von Friedrich Schillers Die Räuber (GEM force directed layout algorithm). Die Knotengröße repräsentiert den Grad. Wie verhält sich Morettis Studie aber zu Muellers Abbildung 1 verdeutlicht zudem, dass nicht alle Kopräsenznetzwerke vom """"intermediate"" status of visualization"" profitieren, den Moretti in seinem Essay als so wichtig erachtet (Moretti 2011,¬†11). Das Netzwerk von Schillers Der Mehrwert solcher Figurennetzwerke wird jedoch meist auf die Analyse größerer Textsammlungen verschoben, die schon aufgrund ihrer Menge nur schwer durch  Abbildung 2 zeigt ein Beispiel für eine diachrone Analyse anhand 583 deutschsprachiger Dramen, die zwischen 1730 und 1930 veröffentlicht oder uraufgeführt wurden (German Drama Corpus). Die Abbildung reproduziert eine Untersuchung von Trilcke und Fischer (2018, Abbildung¬†6). Wie Trilcke und Fischer habe ich aus dem durchschnittlichen Grad der einzelnen Dramen die Mittelwerte für jedes Jahrzehnt von 1730 bis 1930 ermittelt. Rein deskriptiv ist festzuhalten, dass der durchschnittliche Grad ab dem späten 18. Jahrhundert langsam ansteigt. Zwischen 1830 und 1880 sind dann nur relativ geringe Schwankungen zu erkennen, ehe auf einen Anstieg bis etwa 1890 ein abrupter Fall und ein erneuter starker Anstieg folgen. Trilcke und Fischer haben diese Werte als Indikator dafür gedeutet, dass Dramatiker:innen mit ihren Stücken ""auf die gesellschaftliche Modernisierung und Ausdifferenzierung seit der zweiten Hälfte des 18. Jahrhunderts"" reagieren. Sie weisen im Anschluss gleichwohl darauf hin, dass diese Erkenntnis nichts Neues sei (Trilcke, Fischer 2018, Kap.¬†4.1). Mit Fotis Jannidis (2019,¬†65) gesprochen lässt sich diese Art der Wissenskonsolidierung als Form der ""Kreuzpeilung"" begreifen. Wie überzeugend ist diese Deutung der Werte aber? Vergleicht man den Werteverlauf des durchschnittlichen Grads in Abbildung¬†2 mit der Zahl der auftretenden Figuren aus Abbildung¬†3, scheint ein Zusammenhang zu bestehen. Die Berechnung der Korrelation zwischen Grad und Figurenzahl bestätigt diese Relation: Abbildung 3: Zahl der Figuren (schwarz) und durchschnittlicher Grad (grau) von 583 deutschsprachigen Dramen. Die Abbildung zeigt die Mittelwerte pro Dekade. Die vier von mir beschriebenen Dimensionen des",de,Publikation Bereich digital Literaturwissenschaft vermehrt altphilolog anglist Martin Mueller geprägt Konzept s Schritt Beitrag verschieden dimensionen Muellers Konzeption programmatisch ausgerichtet Blogbeitrag heben Mueller hervor t he charms -- google earth muell Skalenpluralität beginnen erstens Textgrundlage literarisch Text liegen weit scale surrogaten weitin nebeneinander koexistieren our Typical Encounter with Text -- Through surrogaten muell Mueller sprechen Stelle Surrogat unterschiedlich geartet Repräsentatione originals arbeiten beispielsweise faksimile Werkausgaben digitalisat speziell kodiert Textsammlung muell surrogat hinaus stark transformiert abstrahiert Form auftreten beispielsweise Gestalt Häufigkeitswortlist Netzwerkanalyse fußen demnach surrogaten Peer trilcken Frank Fischer sprechen Zwischenformat Fall Strukturinformation dramen vorhalen Netzwerkerstellung heranziehen trilck fisch Dramentext Zwischenformat unterschiedlich Repräsentationsforme Literatur zweitens Frage Umfang knüpfen Untersuchungsgegenstand handeln einzeln Text sogar Ausschnitt Text groß Sammlung Text umfangreich Sammlung Zahl betrachtend Text Textsort Skalierungsfrage kurz Novell langromane untersuchen kurz einakt Karl Kraus monumental Lesedrama drittens stellen Frage Größe Analyseeinheit Moretti Mueller denken methodisch Bezugsgröße viertens vielmehr Art Skala weitin gemeinsam Thomas Gilli nico Kunkel herausstellt umfasse folgend abschneten Praxis Automatisierung führen Einschränkung dargelegt Formalisierung Figureninteraktion ähnlich deckungsgleich Solomon Marcus vorgeschlagen kodifiziert Handbuchwissen pfister geworden Begriff Konfiguration Figurenkonfiguration Drama ändern Figur Bühne betreten verlässt bühnengeschehen beteiligt Personal zumindest Teil wechseln dramen prinzipien französisch Klassizismus folgen Nebentext markiert abtritte strukturieren Konfiguration Szenengrenze fallen zumindest Theorie Stück Shakespeare Poetik orientieren Szenengrenze zumeist Ortswechsel binden figuren abtreten zwangsläufig Szene konstituieren Abtritt Figur stellen Frage Kopräsenznetzwerke sinnvollerweise etabliert Dramenanalyse integrieren lassen prominent Versuch Netzwerkanalyse verstehensprozess literarisch Text integrieren unternehmen moretti Essay Kopräsenznetzwerk Friedrich Schiller Räuber gem force directed Layout Algorithm Knotengröß repräsentieren Grad verhalten Moretti Studie Muellers Abbildung verdeutlichen zudem Kopräsenznetzwerk intermediat status -- visualization profitieren moretti Essay wichtig erachten moretti Netzwerk Schiller Mehrwert Figurennetzwerk meist Analyse groß Textsammlung verschieben aufgrund Menge schwer Abbildung zeigen Diachrone Analyse anhand deutschsprachig dramen veröffentlichen uraufführen German Drama Corpus Abbildung reproduzieren Untersuchung Trilcke Fischer trilcken Fischer durchschnittlich Grad einzeln Dram Mittelwerte jeder Jahrzehnt ermitteln rein deskriptiv festhalten durchschnittlich Grad spät Jahrhundert langsam ansteigt relativ gering Schwankung erkennen ehe Anstieg abrupt Fall erneut stark Anstieg folgen trilcke Fischer Wert Indikator deuten Dramatiker innen Stücke gesellschaftlich Modernisierung Ausdifferenzierung Hälfte Jahrhundert reagieren weisen Anschluss gleichwohl Erkenntnis neu trilcken Fischer Fotis Jannidis sprechen lässt Art Wissenskonsolidierung Form Kreuzpeilung begreifen überzeugend Deutung Wert vergleichen Werteverlauf durchschnittlich Grads Zahl auftretenden Figur scheinen Zusammenhang bestehen Berechnung Korrelation Grad Figurenzahl bestätigen Relation Abbildung Zahl Figur schwarz durchschnittlich Grad Grau Deutschsprachig dramen Abbildung zeigen Mittelwerte pro Dekade beschrieben Dimension,"[('mueller', 0.20752955183830682), ('grad', 0.18465859960893183), ('fischer', 0.18197143377999844), ('muell', 0.17631700457993346), ('surrogaten', 0.17631700457993346), ('moretti', 0.16450679676545873), ('dramen', 0.14557714702399877), ('durchschnittlich', 0.13798072032988606), ('trilcken', 0.12049934578275462), ('muellers', 0.11754466971995564)]"
2023,DHd2023,ANDRESEN_Melanie_Klassifikation_von_Figurenauf__und__abtritt.xml,Klassifikation von Figurenauf- und -abtritten in XML-kodierten Dramen,"Lena Ehlers (Universität Stuttgart, Deutschland); Melanie Andresen (Universität Stuttgart, Deutschland)","Drama, Annotation, Computational Literary Studies","Datenerkennung, Strukturanalyse, Annotieren, Literatur, Text","In diesem Beitrag wird eine regelbasierte Methode vorgestellt, um Figurenauf- und -abtritte in den Regieanweisungen dramatischer Texte zu klassifizieren. In der Forschung wurde Regieanweisungen meist nur wenig Beachtung geschenkt, etwa weil sie während einer Theateraufführung nicht textuell in Erscheinung treten (Schößler, 2017, S.3). Als eine der wenigen quantitativen Untersuchungen stellen Trilcke et al. (2020) fest, dass sich die vermutete Episierung des Dramas im Laufe der Jahrhunderte am Korpus GerDraCor bestätigt. Eine Differenzierung nach Funktionen der Regieanweisungen erfolgt nicht. Dabei ist die (Ko-)Präsenz von Figuren auf der Bühne eine häufig genutzte Grundlage für quantitative Dramenanalysen, insbesondere in der Netzwerkanalyse (z.B. Marcus 1973, Krautter et al. 2018, Fischer et al. 2018). Trilcke et al. (2017) konnten bereits am Beispiel von Lessings Emilia Galotti zeigen, dass in statischen Netzwerken, die das ganze Drama auf einmal darstellen, wichtige Informationen zur Dynamik der Beziehungen zwischen den Figuren verloren gehen können. Unserer Ansicht nach ist außerdem zu berücksichtigen, dass Figuren auch innerhalb von Szenen auf- und abtreten und die Anwesenheit von zwei Figuren in einer Szene nicht zwangsläufig bedeutet, dass diese Figuren auch gleichzeitig auf der Bühne stehen. Das Deutsche Dramenkorpus GerDraCor enthält über 550 deutschsprachige, TEI-kodierte Dramentexte aus dem Zeitraum von 1650 bis 1947 (Fischer et al. 2019). Die Regieanweisungen sind als Insgesamt wurden 16 Dramentexte manuell annotiert, wovon vier zur Implementierung Das entwickelte Verfahren klassifiziert den unstrukturierten Text innerhalb der Wurde im ersten Schritt ein Auf- oder Abtritt erkannt, folgt als zweiter Schritt die Zuordnung der betroffenen Figuren. Hierfür wird die im XML enthaltene Liste der Namen aller sprechenden Figuren genutzt, die auch die Abbildung auf die Figuren-IDs ermöglicht. Der Auf- oder Abtritt wird entweder einer in der Regieanweisung genannten Figur oder derjenigen Figur, deren Rede die Regieanweisung zugeordnet ist (vgl. Fall in Abb. 1), zugeschrieben. Die Evaluation erfolgt anhand von zwölf manuell annotierten Texten, die nicht zur Aufstellung der Regeln herangezogen wurden. Evaluiert werden 1) die Klassifikation der Regieanweisungen in Figurenauf- und -abtritte und 2) die Zuordnung der betroffenen Figuren. Im zweiten Schritt werden nur die Elemente in die Evaluation einbezogen, die im ersten Schritt korrekt klassifiziert wurden. Tabelle 1 zeigt, dass die durchschnittlichen Werte für Precision, Recall und F1-Score für die Auf-/Abtritterkennung bei 0,85 liegen. Auch die Figurenerkennung liefert gute Ergebnisse (F1 = 0,87). Zwischen den Texten zeigt sich allerdings eine erhebliche Variation in der Qualität. Aufgrund des regelbasierten Verfahrens schneiden Texte, die stark von den zur Erstellung der Regeln verwendeten Texten abweichen, in der Evaluation schlechter ab. Besonders Texte mit langen Regieanweisungen sorgen dafür, dass viele Schlüsselwörter auch in anderen Kontexten vorkommen. Das zeigt sich insbesondere beim Abbildung 2 zeigt, dass die meisten Auf- und Abtritte tatsächlich innerhalb von Szenen stattfinden (ca. 46%). Etwas weniger erfolgen am Beginn einer Szene (41%) und etwa 13% am Ende. Erwartungsgemäß handelt es sich am Anfang der Szene fast ausschließlich um Auftritte, am Ende um Abtritte. Innerhalb der Szenen komme Auf- und Abtritte zu jeweils gleichen Anteilen vor. In diesem Paper haben wir ein mit Figurenauf- und -abtritten annotiertes Teilkorpus zu GerDraCor präsentiert und einen regelbasierten Algorithmus vorgestellt, der diese Annotationen mit einem mittleren F1-Wert von über 0,85 reproduzieren kann. Ein Großteil der annotierten Auf- und Abtritte erfolgt innerhalb von Szenen. Diese Veränderungen in den Figurenkonstellationen werden bei einer szenenweisen Betrachtung der Figurenpräsenz nicht berücksichtigt, haben aber potenziell Auswirkungen auf beispielsweise netzwerkanalytische Arbeiten. Alle Daten und Skripte zu diesem Beitrag sind unter",de,Beitrag regelbasiert Methode vorstellen Regieanweisunge dramatisch Text klassifizieren Forschung regieanweisungen meist Beachtung schenken Theateraufführung Textuell Erscheinung treten Schößler weniger quantitativ Untersuchung Stelle trilcken et fest vermutet Episierung Drama Lauf jahrhunderte Korpus Gerdracor bestätigen Differenzierung Funktion Regieanweisung erfolgen Figur Bühne häufig genutzt Grundlage quantitativ dramenanalysen insbesondere Netzwerkanalyse Marcus Krautter Et Fischer Et trilcken et lessings Emilia Galotti zeigen statisch Netzwerke Drama darstellen wichtig Information Dynamik Beziehung Figur verlieren Ansicht berücksichtigen Figur innerhalb szen abtret Anwesenheit Figur Szene zwangsläufig bedeuten Figur gleichzeitig Bühne stehen deutsch Dramenkorpus Gerdracor enthalten deutschsprachig dramentexte Zeitraum Fischer et Regieanweisunge insgesamt dramentext manuell annotieren wovon Implementierung entwickelt Verfahren klassifizieren unstrukturiert Text innerhalb Schritt Abtritt erkennen folgen Schritt Zuordnung betroffen Figur hierfür xml enthalten Liste Name sprechend Figur nutzen Abbildung ermöglichen Abtritt Regieanweisung genannt Figur Figur Rede Regieanweisung zuordnen Fall abb zuschreiben Evaluation erfolgen anhand zwölf manuell Annotiert Text Aufstellung Regel heranziehen evaluieren Klassifikation Regieanweisung Zuordnung betroffen Figur Schritt Element Evaluation einbeziehen Schritt korrekt klassifizieren Tabell zeigen durchschnittlich Wert Precision Recall liegen Figurenerkennung liefern Ergebnis Text zeigen erheblich Variation Qualität aufgrund regelbasiert verfahren schneid Text stark Erstellung Regel verwendet texten abweichen Evaluation schlecht Text lang Regieanweisunge Sorge Schlüsselwörter Kontext vorkommen zeigen insbesondere Abbildung zeigen meister abtritten tatsächlich innerhalb szen Stattfinde erfolgen Beginn Szene erwartungsgemäß handeln Anfang Szene fast ausschließlich Auftritt abtritte innerhalb Szene kommen abtritte jeweils gleich Anteil Paper annotiert Teilkorpus Gerdracor präsentieren regelbasiert Algorithmus vorstellen Annotation mittlerer reproduzieren Großteil annotierter abtritte erfolgen innerhalb Szene Veränderung figurenkonstellationen szenenweis Betrachtung Figurenpräsenz berücksichtigen potenziell Auswirkung beispielsweise netzwerkanalytische arbeiten daten Skript Beitrag,"[('figur', 0.30603685751972626), ('regieanweisung', 0.2515062125447935), ('szene', 0.2391066199113317), ('regieanweisunge', 0.23167681939005844), ('abtritte', 0.21578939344765088), ('gerdracor', 0.18258954039279557), ('regelbasiert', 0.16146992536953939), ('innerhalb', 0.14687783472720783), ('betroffen', 0.14385959563176726), ('abtritt', 0.14385959563176726)]"
2023,DHd2023,VARACHKINA_Hanna_Pipelines_für_Natural_Language_Processing_u.xml,Pipelines für Natural Language Processing und digitale Literaturanalyse in spaCy,"Hanna Varachkina (Seminar für Deutsche Philologie, Georg-August-Universität Göttingen); Florian Barth (Göttingen Centre for Digital Humanities, Georg-August-Universität Göttingen); Tillmann Dönicke (Göttingen Centre for Digital Humanities, Georg-August-Universität Göttingen); Johannes Biermann (Niedersächsische Staats- und Universitätsbibliothek Göttingen); Friederike Altmann (Seminar für Deutsche Philologie, Georg-August-Universität Göttingen); Thorben Neitzke (Göttingen Centre for Digital Humanities, Georg-August-Universität Göttingen); Caroline Sporleder (Göttingen Centre for Digital Humanities, Georg-August-Universität Göttingen)","Textanalyse, Pipelines, Python","Programmierung, Inhaltsanalyse, Strukturanalyse, Literatur, Text","In diesem halbtägigen Workshop stellen wir ein auf spaCy basierendes Pipeline-System für das Natural Language Processing (NLP) narrativer Texte vor und erproben mit den Teilnehmer*innen dessen praktische Anwendung, besonders im Hinblick auf Untersuchungsgegenstände der digitalen Literaturanalyse. Die Analyse von literarischen Texten ist eine besondere Herausforderung für die automatische Sprachverarbeitung, da sie oft komplexe Interaktionen linguistischer Strukturen auf der syntaktischen, semantischen und pragmatischen Ebene betrifft. Für die Interpretation solcher Texte ist es zum Beispiel wichtig, neben traditionellen NLP-Verarbeitungsschritten wie Eigennamenerkennung, Sentiment-Analyse etc., auch komplexere Analysen durchzuführen, um z.¬†B. die Sprechinstanzen im Text zu identifizieren, Bezüge zur realen Welt zu erkennen oder zeitliche Strukturen im Text zu analysieren. Auf der praktischen Ebene bedeutet dies, dass automatische Analysen in der digitalen Literaturwissenschaft in der Regel die (oft komplexe) Kombination mehrerer basaler Sprachverarbeitungswerkzeuge auf Token-, Teilsatz-, Satz- und Passagen-/Diskursebene erfordert. Dies ist in der Praxis nicht immer trivial, z.¬†B. weil Ein- und Ausgabeformate verschiedener Werkzeuge nicht kompatibel sind.  Der Workshop ist als Tutorial geplant und richtet sich an Literaturwissenschaftler*innen, Linguist*innen, DH-Forschende, und andere Personen, die an Textanalyse interessiert sind. Die Teilnehmer*innen bekommen die Möglichkeit, die Funktionalitäten von MONAPipe auszuprobieren und in vorbereiteten Texten eine Reihe von Phänomenen automatisch zu identifizieren. Die Teilnehmerzahl ist auf 30 beschränkt. Lernziele und Methodik Der Workshop verfolgt mehrere Ziele: (1) Er soll die Teilnehmer*innen mit spaCy und dessen Kernkomponenten vertraut machen und Ihnen praktische Erfahrung in der Nutzung von MONAPipe für typische Textanalysekomponenten auf Token-, Satz-/Teilsatz- und Passagenebene vermitteln. (2) Darüber hinaus erproben die Teilnehmer*innen die Einbindung neuer Komponenten, um damit wie sie MONAPipe für eigene Zwecke anpassen können. Aufbauend auf diesen Grundlagen lernen die Teilnehmer*innen an einem konkreten Beispiel, (3) wie sie MONAPipe konkret für Forschungsprojekte insbesondere in der digitales Literaturanalyse nutzen können. Dies umfasst die Auswahl geeigneter Komponenten für die Forschungsfrage sowie die Reflektion der Ergebnisse. Am Ende des Workshops haben die Teilnehmer*innen zum einen (i) ein besseres theoretisches Verständnis für die verschiedenen Sprachanalyseschritte, können komplexe Analysen durch Kombination mehrerer basaler Werkzeuge durchführen und die Qualität der automatischen Analyse beurteilen; Zum anderen (ii) haben die Teilnehmer*innen praktische Erfahrung im Umgang mit spaCy und verschieden Sprachverarbeitungswerkzeugen erworben¬† und Problemlösungsstrategien für den Umgang mit NLP-Werkzeugen gelernt. Methodisch kombiniert der Workshop Theorie und Praxis, wobei der Praxisanteil überwiegt. Um das Gelernte zu festigen und zu vertiefen, bekommen die Teilnehmer*innen zunächst kurze Arbeitsaufträge (zu den Sprachverarbeitungskomponenten) und später komplexere Aufgaben (zur Analyse narrativer Texte), deren Lösungen im Anschluss diskutiert werden. Der Praxisteil im zweiten Teil des Workshops bietet außerdem die Möglichkeit, MONAPipe für ein eigenes Forschungsproblem anzuwenden und dazu Feedback von den Organisator*innen des Workshops zu bekommen. Auf technischer Ebene arbeiten wir mit der interaktiven Programmierumgebung Jupyter-Notebook und stellen vorbereitete und ausführlich dokumentierte Notebooks zur Verfügung, um einen möglichst reibungslosen Ablauf zu ermöglichen und den Teilnehmer*innen zu helfen, sich auf die Workshopinhalte zu konzentrieren. Organisation und Ablauf Wir planen einen vierstündigen Workshop bestehend aus zwei Blöcken. Der erste Block (1:45 h) beinhaltet aus einem einführenden Vortrag sowie einem Zeitslot zur Einrichtung der Jupyter-Notebooks , wobei die Organisator*innen nach Bedarf Hilfestellung bei der Einrichtung leisten. Anschließend erfolgt eine 45-minütige Session mit vorbereiteten Notebooks, bei der zunächst kürzere textuelle Phänomene auf Token-Ebene (wie Named Entities), Phänomene auf Teilsatz-Ebene (z.¬†B. Zeitformen) sowie Phänomene, die längere Textpassagen umfassen (z.¬†B. Redeformen), behandelt werden. Im zweiten Block des Workshops (1:45 h) erstellen die Teilnehmer*innen eine eigene Komponente in spaCy. Anschließend erhalten die Teilnehmer*innen die Möglichkeit durch Lektüre narrative Strukturen in exemplarischen Textpassagen qualitativ zu bestimmen. Anhand der zur Verfügung stehenden spaCy-Komponenten soll evaluiert werden, welche Features sich zur Identifikation komplexer narrativer Strukturen eignen. Alternativ können die Teilnehmer*innen an eigenen Texten und Fragenstellungen arbeiten und hierfür Unterstützung durch die Workshoporganisator*innen erhalten.   ",de,halbtägig Workshop stellen Spacy basierend natural Language Processing nlp narrativ Text erproben praktisch Anwendung Hinblick Untersuchungsgegenstände digital Literaturanalyse Analyse literarisch Text besonderer Herausforderung automatisch Sprachverarbeitung komplex interaktionen linguistisch Struktur syntaktisch semantisch pragmatisch Ebene betreffen Interpretation Text wichtig traditionell Eigennamenerkennung komplex analysen durchführen Sprechinstanze Text identifizieren bezüge real Welt erkennen zeitlich Struktur Text analysieren praktisch Ebene bedeuten automatisch Analyse digital Literaturwissenschaft Regel komplex Kombination mehrere basal Sprachverarbeitungswerkzeuge erfordern Praxis trivial Ausgabeformat verschieden Werkzeug kompatibel Workshop Tutorial planen richten Person Textanalyse interessiert bekommen Möglichkeit funktionalitäten Monapipe ausprobieren vorbereitet Text Reihe Phänomen automatisch identifizieren Teilnehmerzahl beschränken Lernziele Methodik Workshop verfolgen mehrere Ziel Spacy kernkomponenten vertraut praktisch Erfahrung Nutzung Monapipe typisch Textanalysekomponente passageneben vermitteln hinaus erproben Einbindung neu Komponente Monapipe Zweck anpassen aufbauend Grundlag lernen konkret monapipe konkret Forschungsprojekt insbesondere digitalesr Literaturanalyse nutzen umfassen Auswahl geeignet Komponente Forschungsfrage Reflektion Ergebnis Workshop i besser theoretisch Verständnis verschieden sprachanalyseschritte Komplex Analyse Kombination mehrere basal Werkzeuge durchführen Qualität automatisch Analyse beurteilen ii praktisch Erfahrung Umgang Spacy verschied sprachverarbeitungswerkzeug Problemlösungsstrategie Umgang lernen methodisch kombinieren Workshop Theorie Praxis wobei Praxisanteil überwiegen Gelernte festigen vertiefen bekommen kurz Arbeitsauftrag Sprachverarbeitungskomponent komplex Aufgabe Analyse narrativ Text Lösung Anschluss diskutieren Praxisteil Workshop bieten Möglichkeit monapipe Forschungsproblem anwenden Feedback Workshop bekommen technisch Ebene arbeiten interaktiv Programmierumgebung stellen vorbereiten ausführlich dokumentieren Notebooks Verfügung möglichst Reibungslose Ablauf ermöglichen helfen Workshopinhalt konzentrieren Organisation ablauf planen Vierstündige workshop bestehend Blöck Block h beinhalten einführend Vortrag Zeitslot Einrichtung wobei Bedarf hilfestellung Einrichtung leisten anschließend erfolgen Session Vorbereitet Notebooks kürz textuell Phänomen named entities phänomen Zeitforme phänomen lang Textpassage umfassen redeformen behandeln Block Workshop h erstellen Komponente Spacy anschließend erhalten Möglichkeit lektür narrativ Struktur exemplarisch Textpassage qualitativ bestimmen anhand Verfügung stehend evaluieren Feature Identifikation komplex narrativ Struktur eignen alternativ Text fragenstellungen arbeiten hierfür Unterstützung erhalten,"[('monapipe', 0.3475263653925772), ('workshop', 0.31975171283053105), ('spacy', 0.21283564178536976), ('komplex', 0.1832753231717107), ('praktisch', 0.14620326592107513), ('narrativ', 0.14479953890096636), ('vorbereitet', 0.12947778511905617), ('bekommen', 0.12731280192711425), ('struktur', 0.12666516335360156), ('block', 0.12271418352485788)]"
2023,DHd2023,NEUBER_Frederike_Open_Jean_Paul.xml,Open Jean Paul.   Funktionen und Potentiale offener Editionsdaten,"Frederike Neuber (Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland); Axelle Lecroq (Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland)","digitale Edition, Briefedition, Open Data","Teilen, Transkription, Kontextsetzung, Veröffentlichung, Daten, Forschungsergebnis","Jean Paul (1763–1825) zählt zu den bedeutendsten Schriftstellern der deutschen Literatur um 1800 und war ein überaus produktiver und geistreicher Briefeschreiber, der mit bekannten Persönlichkeiten wie Heinrich Jacobi, Caroline und Johann Gottfried Herder, Charlotte von Kalb und Rahel Levin Varnhagen korrespondierte. Die Briefe Jean Pauls erschienen bereits Mitte des 20. Jahrhunderts in der Historisch-kritischen Ausgabe (Berend 1952–1964); Anfang des 21. Jahrhunderts folgten die Briefe an Jean Paul (Begemann et al. 2003–2017), ebenfalls im Druck. Seit 2018 ist Jean Pauls Briefkosmos auf dem Weg in die digitale Welt: Die 5562 Von-Briefe, die zunächst buchzentriert retrodigitalisiert Aus methodisch-technischer Perspektive, setzt die Edition mit der Verwendung von XML/TEI und dem Basisformat des Deutschen Textarchivs (DTA 2011–2020) sowie der Anreicherung mit Normdaten (GND, GeoNames) auf Standards. Im Zeichen von ""Open Data"" erscheinen die XML/TEI-Dokumente der Briefe unter Creative Commons-Lizenz (CC-BY-SA 4.0), und zwar in drei Publikationsmodi, die verschiedene Funktionen hinsichtlich ihrer Nutzung erfüllen: (1) Zur (2) Zur (3) Zur Der Beitrag, der die Publikationsmodi der Jean Paul Briefedition und ihre jeweilige Funktion illustriert, ist für die DHd-Konferenz höchst relevant, da ""Open Data"" im Editionskontext immer noch eher die Ausnahme als die Regel ist. Aus Greta Franzinis Editionenkatalog (2016–2022) geht hervor, dass von 320 digitalen Editionen lediglich ~27% CC-Lizenzen verwenden, ~23% TEI-Daten zum Download bereitstellen und ~5% APIs anbieten. Die Zahlen sind bedauerlich, da Daten das primäre Forschungsergebnis digitaler Editionen sind: Im Kontext der Jean Paul-Edition gelten die Daten den Herausgeberinnen als",de,Jean Paul zählen bedeutend Schriftsteller deutsch Literatur überaus produktiv geistreich Briefeschreiber bekannt Persönlichkeit Heinrich Jacobi Caroline Johann Gottfried Herder Charlotte kalb Rahel Levin varnhagen korrespondieren Brief Jean Pauls erscheinen Mitte Jahrhundert Ausgabe berend Anfang Jahrhundert folgen Brief Jean Paul Begemann et ebenfalls Druck Jean Pauls Briefkosmos Weg digital Welt buchzentrieren retrodigitalisieren Perspektive setzen Edition Verwendung xml tei Basisformat deutsch Textarchivs dta Anreicherung Normdate gnd Geonam Standard Zeichen open data erscheinen xml Brief Creative Publikationsmodi verschieden Funktion hinsichtlich Nutzung erfüllen Beitrag Publikationsmodi Jean Paul Briefedition jeweilig Funktion illustrieren höchst relevant open data Editionskontext eher Ausnahme Regel Greta Franzini Editionenkatalog hervor digital editionen lediglich verwenden Download bereitstellen apis anbieten Zahl bedauerlich daten primär Forschungsergebnis Digitaler editionen Kontext Jean gelten daten Herausgeberinn,"[('jean', 0.5268396512955724), ('brief', 0.22249353369952674), ('paul', 0.21717616466159584), ('publikationsmodi', 0.19893453649808934), ('pauls', 0.19893453649808934), ('editionen', 0.13346331569284248), ('open', 0.11532848098330725), ('xml', 0.11014199629327727), ('data', 0.10896132947326682), ('jacobi', 0.09946726824904467)]"
2023,DHd2023,BRUNNER_Annelen_KoMuX___Der_Kompositamuster_Explorer.xml,KoMuX - Der Kompositamuster-Explorer,"Annelen Brunner (Leibniz-Institut für Deutsche Sprache, Deutschland); Hein Katrin (Leibniz-Institut für Deutsche Sprache, Deutschland)","Komposita, Muster, Webanwendung","Strukturanalyse, Annotieren, Webentwicklung, Visualisierung, Daten, Sprache","KoMuX, der Kompositamuster-Explorer, (www.owid.de/plus/komux) ist eine Webanwendung, die es ermöglicht, mehr als 50.000 nominale Komposita des Deutschen gezielt nach abstrakten oder lexikalisch-teilspezifizierten Mustern zu durchsuchen. Unterschiedliche Visualisierungen helfen dabei, Strukturen und Zusammenhänge innerhalb der Ergebnismenge zu erfassen. Mit KoMuX machen wir einen Teil der Datengrundlage frei verfügbar, auf der unsere empirischen Forschungen zur Wortbildung basieren und integrieren Analysen und Visualisierungen aus unseren Arbeiten. Der Explorer ist damit auch ein Beitrag zu OpenScience, indem er es ermöglicht, unsere Forschungsergebnisse in Teilen nachzuvollziehen und zu reproduzieren.  D Die musterbasierte Suche in KoMuX beruht darauf, dass grammatische Merkmale (Wortbildungstyp oder Wortart) oder lexikalische Eigenschaften (konkretes Lemma) für das Erst- und Zweitglied spezifiziert werden. Dies erlaubt es beispielsweise, gezielt alle Adjektiv+Nomen-Komposita (z.B. Visualisierungen helfen dabei, die Ergebnismenge näher zu analysieren. Auch hier steht die musterhafte Betrachtung von Erst- und Zweitgliedposition im Mittelpunkt. Quantitative Verteilungen in Hinblick auf Wortart, Wortbildungstyp und Lemma werden mit Hilfe von mehrstufigen Tortendiagrammen sichtbar gemacht. Die Konstituenten-Ansicht zeigt alle Erst- und Zweitglied-Lemmata der Ergebnismenge, sowie deren Vorkommenshäufigkeiten in den jeweiligen Positionen. So lässt sich untersuchen, in welcher Position die lexikalische Vielfalt größer ist und welche Lemmata starke Tendenzen zu einer der beiden Positionen aufweisen. Die Verknüpfungsansicht zeigt Komposita, deren Erst- oder Zweitglied-Lemma in mindestens einem weiteren Kompositum der Ergebnismenge auftritt und weist so auf produktive Bildungsmuster hin. ",de,komux Webanwendung ermöglichen Nominale Komposita deutsch Gezielt Abstrakt mustern durchsuchen unterschiedlich Visualisierung helfen Struktur Zusammenhäng innerhalb Ergebnismenge erfassen Komux Datengrundlage frei verfügbar empirisch Forschung Wortbildung basieren integrier Analyse visualisierung unser arbeiten Explorer Beitrag openscience ermöglichen Forschungsergebnisse Teil nachvollziehen reproduzieren d musterbasiert Suche Komux beruhen grammatisch merkmal wortbildungstyp wortart lexikalisch eigenschaft konkret Lemma zweitglied spezifiziern erlauben beispielsweise gezielt visualisierung helfen ergebnismengen nah analysieren stehen musterhaft Betrachtung Zweitgliedposition Mittelpunkt quantitativ verteilungen Hinblick Wortart wortbildungstyp Lemma Hilfe mehrstufig Tortendiagramm sichtbar zeigen ergebnismenge vorkommenshäufigkeiten jeweilig Position lässen untersuchen Position lexikalisch Vielfalt groß Lemmata stark Tendenz Position aufweisen Verknüpfungsansicht zeigen Komposita mindestens Kompositum Ergebnismenge auftreten weisen produktiv Bildungsmuster,"[('komux', 0.3544095656041241), ('ergebnismenge', 0.3301056419675673), ('wortbildungstyp', 0.23627304373608277), ('position', 0.21346613163607644), ('komposita', 0.19965754623945886), ('wortart', 0.17616925763344865), ('lemma', 0.1681503427937105), ('helfen', 0.1681503427937105), ('visualisierung', 0.1495481196544933), ('gezielt', 0.1484707030462237)]"
2023,DHd2023,FISCHER_Frank_Internationale_Autor_innen_zu_Gast_in_der_DDR_.xml,Internationale Autor*innen zu Gast in der DDR: Die Einreisekartei des Schriftstellerverbandes und ihre digitale Aufbereitung,Frank Fischer (Freie Universität Berlin); Viktor Jonathan Illmer (Freie Universität Berlin); Lukas Nils Regeler (Freie Universität Berlin); Jutta Müller-Tamm (Freie Universität Berlin); Luise von Berenberg-Gossler (Freie Universität Berlin); Franziska Diehr (Robert Koch-Institut),"Deutscher Schriftstellerverband, Literaturwissenschaft, DDR","Archivierung, Literatur","Marcel Reich-Ranicki kam 1955 und 1956. Der sowjetische Autor Michail Scholochow besuchte die DDR 1964, zwei Jahre bevor er den Nobelpreis erhielt; ähnlich der guatemaltekische Schriftsteller und spätere Nobelpreisträger Miguel Asturias, der 1965 nach Ostberlin reiste. Friederike Mayröcker folgte einer Einladung im Mai 1987. Andere kamen wiederholt, wie der ungarische Dichter G√°bor Hajnal, der sich zwischen 1957 und 1986 dreizehn Mal in Ostberlin aufhielt. Eingeladen hatte jeweils der Deutsche Schriftstellerverband (DSV), über den der Großteil der internationalen literarischen Kontakte in der DDR organisiert wurde. Tausende Daten zur Einladungspolitik des Verbandes sind in einer Kartei in der Akademie der Künste in Berlin hinterlegt, deren Bestand im Rahmen der Archivarbeiten für das Forschungsprojekt ""Writing Berlin"" digitalisiert wurde. ""Writing Berlin"" ist Teil des Exzellenzclusters ""Temporal Communities. Doing Literature in a Global Perspective"" (EXC 2020) und befasst sich mit den facettenreichen Aktivitäten zur Förderung des internationalen literarischen Austauschs in der geteilten Stadt nach dem Bau der Berliner Mauer. Ein besonderes Augenmerk liegt dabei auf den Auswahlprozessen und den kulturpolitischen Implikationen dieser Aktivitäten, ihrem Niederschlag in literarischen Texten sowie auf der Frage, inwiefern die sich verändernde politische Gemengelage Biografien und die soziale Stellung der betreffenden Autor*innen beeinflusste. Die Internationalisierung der Berliner Literaturszene ist bislang nur in einigen wenigen Fallstudien untersucht worden, vor allem im Hinblick auf die Netzwerktätigkeit einzelner Schriftsteller*innen (vgl. Böttiger 2005, Berbig 2005). Der institutionalisierte Austausch, der einen Großteil der internationalen Kontakte im Osten der Stadt ausmachte, war bislang noch nicht Gegenstand weitergehender Studien 'zwar liegen allgemeine Untersuchungen zum Schriftstellerverband der DDR vor, diese erwähnen die politisch so relevante Auslandsarbeit der Organisation jedoch bestenfalls beiläufig (vgl. zum DSV allgemein Pamperrien 2004, Walther 2006, Michael et al. 1997) und betrachten lediglich einen sehr eingeschränkten Zeitraum (vgl. insbesondere zu den 1950er-Jahren Degen 2011, Gansel 1997). Die Einreisekartei des DSV, der wichtigsten nichtstaatlichen Literaturinstitution im Ostteil der Stadt, erlaubt es nun, die internationalen Kontakte und ihre Konjunkturen insbesondere in der spannungsgeladenen Zeit während des Bestehens der Berliner Mauer zu erforschen: den Verlauf dieser Aktivitäten insgesamt, die länderbezogene Einladungspolitik, die Umstände individueller Aufenthalte und ihre politische Rolle für das Herkunftsland. Sie ermöglicht auch, Literaturkontakte weniger um besonders hervorstechende Einzelpersonen zentriert zu denken und dabei gerade auch Autor*innen zu berücksichtigen, die durch Kanonisierungsprozesse der Vor- und Nachwendezeit ggf. in Vergessenheit geraten sind. Zunächst wurden die Einreisekarteien im Archiv des DSV transkribiert. Als Grundlage dafür wurden die nach Ländern und Autor*innennamen geordneten Karteien verwendet. Für jede*n einreisende*n Autor*in existiert so mindestens ein separates Blatt, auf dem die verschiedenen Aufenthalte vermerkt sind. Die Mitarbeiter*innen der Auslandsabteilung des DSV ergänzten ggf. noch biografische Informationen oder auch ein Presse- oder Passfoto. Über die Jahrzehnte änderte sich vielfach die Art der Aufzeichnung, ein Großteil der etwa 3.000 Karteien orientiert sich jedoch an dem in Tabelle 1 wiedergegebenen Schema, das am Beispiel des kubanischen Dichters Nicol√°s Guillén in Abbildung 1 illustriert werden soll.  Um die Informationen aus der nach Autor*innennamen sortierten Einreisekartei zu komplettieren, wurden auch die chronologischen Karteien herangezogen sowie etwa punktuell weitere Akten aus dem Archiv des DSV, etwa die zu etlichen Aufenthalten vorhandenen Freundschaftsverträge, Korrespondenzen und Zeitpläne. Als Ergebnis dieser Transkriptionsarbeit entstand eine Excel-Tabelle mit insgesamt 3.709 Einträgen. Die Tabelle enthält Informationen zum Zeitraum des jeweiligen Aufenthalts, den Autor*innen (Name und Staatsangehörigkeit), zu beteiligten Institutionen sowie Angaben zum Anlass bzw. Einladungsgrund. Mit OpenRefine (Version 3.5.0) wurden die in der Excel-Tabelle enthaltenen Daten vereinheitlicht. So konnten Einträge, die zwar denselben Anlass betrafen, aber unterschiedlich verschriftlicht waren, zusammengeführt werden. In einem weiteren Schritt wurden die teils in problematischer Weise notierten Autor*innennamen über OpenRefine aufbereitet und mit Normdatensätzen verknüpft. Dadurch wurde zum einen die Verifizierung bzw. Identifizierung der in der Kartei verzeichneten Einträge vereinfacht; zum anderen konnten aus den verknüpften Datenbanken weitere Informationen zu den Autor*innen importiert werden. Ein erstes umfangreiches Reconciling erfolgte mit dem Virtual International Authority File (VIAF). Als Grundlage hierfür diente der von Jeff Chiu über Codefork bereitgestellte Reconciliation Service (Version 3.0.5, Ein weiteres Reconciling wurde 'über das in OpenRefine integrierte Tool 'mit Wikidata vorgenommen. Auch hier konnte durch einige Nachjustierungen eine hohe Trefferquote von 75 % erzielt werden. Der erfolgreiche Abgleich ermöglichte nun den Import weiterer Informationen aus Wikidata, etwa Angaben zu Sprachen, Parteizugehörigkeit oder Geschlecht der Autor*innen. Zudem konnten weitere Identifier über Wikidata importiert und somit Schnittstellen zur Gemeinsamen Normdatei der Deutschen Nationalbibliothek (GND) und zum WorldCat geschaffen werden, wodurch nun auch bibliografische Informationen zu den eingeladenen Autor*innen recherchierbar sind. Jeder einzelne der hier dargestellten Schritte stellt eine Interpretationsleistung der Daten dar, die ihrerseits wieder nur heuristisch erfolgen, unvollständig und fehlerbehaftet sein kann. Bei dem über OpenRefine bereinigten und abgeglichenen Datensatz handelt es sich somit nur um eine mögliche Lesart der ursprünglichen Einreisekarteien, die der fortwährenden Überprüfung und Modifizierung bedarf.  Wegen der teils unvollständigen Datumsangaben haben wir auf das Extended Date/Time Format (EDTF) gesetzt. Dieses 2019 von der International Organization for Standardization als Erweiterung zu ISO-8601 gedachte Datumsformat erlaubt es unter anderem, verschiedene Arten von Ungewissheit formalisiert auszudrücken. Für die Zwecke dieses Projekts besonders fruchtbar ist die Einbeziehung von ""unspecified digits"" (Library of Congress 2019), die unbekannte Teile eines Datumsformats explizieren: Ein nicht spezifizierter Tag im Februar 1972 kann etwa als ""1972-02-XX"" dargestellt werden, der gleiche Fall bezogen auf einen Tag im Jahr 1986 als ""1986-XX-XX"". Darüber hinaus muss die Ungewissheit nicht zwingend von den niedrigstwertigen Stellen herrühren 'auch ""XXXX-09-24"" oder sogar ""19XX-05-XX"" sind gültige EDTF-Werte. Zwar existiert eine JavaScript-Bibliothek zum Parsen von EDTF-Datumsangaben (vgl. Keil 2022), nicht jedoch zur menschenlesbaren Darstellung. Die Logik zur sprachenübergreifenden Darstellung unvollständiger Angaben wurde deshalb eigens in TypeScript implementiert. Mit den vorliegenden Daten kann ein spezifischer Aspekt des literarischen Lebens in der DDR nun zum ersten Mal auch statistisch ausgewertet werden. Durch chronologische Verlaufsdiagramme zeichnen sich Einladungstendenzen ab, die sich unter anderem politisch deuten lassen. Ein Blick auf süd- und westeuropäische Länder zeigt nur sporadische Besuche, mit der Ausnahme Frankreichs, dessen breit aufgestellte Linke teils verstärkt mit dem Schriftstellerverband der DDR kooperierte (vgl. Fabre-Renault 2015). Auch mit Autor*innen aus dem englischsprachigen Ausland, vor allem den USA und Australien, gab es noch in den 1960er-Jahren einen vergleichsweise regen Austausch, der in den 1970er-Jahren allerdings vollends zum Erliegen kam. Dokumentieren lässt sich auch ein hohes Interesse des DSV an Autor*innen aus den sich als neutral verstehenden Staaten Finnland und Schweden, die in den 1960er-Jahren von der SED zu Schwerpunktländern auslandspropagandistischer Aktivitäten erkoren wurden: Autor*innen aus Ostblockstaaten waren jedoch weitaus regelmäßiger bei literarischen Terminen in Ostberlin zu Gast. Hier zeigen die Daten, dass sowjetische Besucher*innen stets in der Überzahl waren, ein Beleg für die Quotenregelung, die der Einladungspolitik zugrunde lag. Die von uns angebotene Schnittstelle ermöglicht viele weitere statistische Anfragen. Ihre Funktion ist aber nicht auf die projektbezogene Auswertung beschränkt. Vielmehr kann der von uns erstellte, semantisch angereicherte Datensatz auch langfristig eine Funktion im wachsenden √ñkosystem der digitalen Literaturwissenschaft übernehmen und bietet sich für den Austausch mit komplementären Projekten wie der ""Forschungsplattform Literarisches Feld DDR"" an (vgl. Gefördert durch die Deutsche Forschungsgemeinschaft (DFG) im Rahmen der Exzellenzstrategie des Bundes und der Länder innerhalb des Exzellenzclusters Temporal Communities: Doing Literature in a Global Perspective 'EXC 2020 'Projekt-ID 390608380.",de,Marcel sowjetisch Autor Michail Scholochow besuchen DDR bevor Nobelpreis erhalten ähnlich guatemaltekisch Schriftsteller spät Nobelpreisträger Miguel asturias Ostberlin reisen Friederike Mayröcker folgen Einladung Mai kommen wiederholt ungarisch Dichter bor hajnal dreizehn Mal Ostberlin aufhielt einladen jeweils deutsch Schriftstellerverband dsv Großteil international literarisch Kontakt DDR organisieren Tausend daten Einladungspolitik Verband Kartei Akademie kunst Berlin hinterlegt Bestand Rahmen archivarbeiter Forschungsprojekt Writing Berlin digitalisieren writing Berlin Exzellenzcluster Temporal communities doing literature global perspectiv exc befassen facettenreich Aktivität Förderung international literarisch Austausch geteilt Stadt Bau Berliner Mauer besonderer Augenmerk liegen Auswahlprozesse kulturpolitisch implikationen Aktivität Niederschlag literarisch Text Frage inwiefern verändernd politisch Gemengelage Biografie sozial Stellung betreffend beeinflussten Internationalisierung Berliner literaturszener bislang weniger Fallstudi untersuchen Hinblick Netzwerktätigkeit einzeln böttig berbig institutionalisiert austausch Großteil international Kontakt Osten Stadt ausmachen bislang Gegenstand weitergehend Studie liegen allgemein Untersuchung Schriftstellerverband DDR erwähnen politisch relevant Auslandsarbeit Organisation bestenfalls beiläufig Dsv allgemein Pamperrien walth Michael Et betrachten lediglich eingeschränkt Zeitraum insbesondere deg Gansel Einreisekartei dsv wichtig nichtstaatlich Literaturinstitution Ostteil Stadt erlauben international Kontakt konjunktur insbesondere spannungsgeladen Bestehen Berliner Mauer erforschen Verlauf Aktivität insgesamt länderbezogen Einladungspolitik umständ individuell Aufenthalt politisch Rolle Herkunftsland ermöglichen literaturkontaken hervorstechend Einzelpersone zentrieren denken berücksichtigen kanonisierungsprozesse Nachwendezeit Vergessenheit geraten Einreisekarteie Archiv dsv transkribieren Grundlage Land geordnet Karteien verwenden existieren mindestens separat Blatt verschieden aufenthalte vermerken Auslandsabteilung dsv ergänzten biografisch Information Passfoto Jahrzehnt ändern vielfach Art Aufzeichnung Großteil Karteien orientieren Tabelle wiedergegeben Schema kubanisch Dichters -- Guillén Abbildung illustrieren Information sortiert Einreisekartei komplettieren chronologisch Kartei heranziehen punktuell Akte Archiv dsv etlicher aufenthalt vorhanden freundschaftsverträgen Korrespondenz zeitpläne Ergebnis Transkriptionsarbeit entstehen insgesamt Einträg Tabell enthalten Information Zeitraum jeweilig Aufenthalt Name Staatsangehörigkeit beteiligt Institution Angabe Anlass einladungsgrund openrefin Version enthalten daten vereinheitlichen einträgen Anlass betreffen unterschiedlich verschriftlichen zusammenführen Schritt teils problematisch Weise notiert openrefine aufbereiten normdatensätzen verknüpfen Verifizierung Identifizierung Kartei verzeichnet eintrag vereinfachen verknüpfter datenbanker Information importieren umfangreich Reconciling erfolgen virtual international Authority Fil viaf Grundlage hierfür dienen Jeff Chiu Codefork bereitgestellt Reconciliation Service Version Reconciling openrefin integriert tool Wikidata vornehmen Nachjustierungen hoch Trefferquote erzielen erfolgreich abgleich ermöglichen Import weit Information Wikidata Angabe sprechen Parteizugehörigkeit Geschlecht zudem Identifier Wikidata importieren somit schnittstelle gemeinsam Normdatei deutsch Nationalbibliothek gnd Worldcat schaffen wodurch bibliografisch Information eingeladen recherchierbar einzelner dargestellt Schritte stellen Interpretationsleistung daten dar ihrerseits heuristisch erfolgen unvollständig fehlerbehaften openrefin bereinigt abgeglichen datensatz handeln somit möglich Lesart ursprünglich Einreisekarteie fortwährend überprüfung Modifizierung bedürfen teils unvollständig datumsangaben Extended dat Time Format Edtf setzen international Organization for Standardization Erweiterung Gedachte Datumsformat erlauben verschieden Art Ungewissheit formalisieren auszudrücken Zweck Projekt fruchtbar Einbeziehung Unspecified digits library -- congress unbekannt Teil Datumsformat explizieren spezifiziert Februar darstellen gleich Fall beziehen hinaus Ungewissheit zwingend niedrigstwertig Stelle herrühren sogar gültig existieren Pars Keil menschenlesbar Darstellung Logik sprachenübergreifend Darstellung unvollständig Angabe eigens Typescript implementieren vorliegend daten spezifisch Aspekt literarisch Leben DDR Mal statistisch auswerten chronologisch verlaufsdiagramme zeichnen einladungstendenzen politisch deuten lassen Blick westeuropäisch Land zeigen sporadisch besuch Ausnahme Frankreich breit aufgestellt linker teils verstärkt Schriftstellerverband DDR kooperierte englischsprachig Ausland USA Australien vergleichsweise Regen Austausch vollends erliegen dokumentieren lässt hoch Interesse dsv neutral verstehend Staat Finnland Schweden sed Schwerpunktländer auslandspropagandistisch Aktivität erkoren Ostblockstaat weitaus regelmäßig literarisch Termin Ostberlin Gast zeigen daten sowjetisch stets Überzahl Beleg Quotenregelung Einladungspolitik zugrunde liegen angeboten Schnittstelle ermöglichen statistisch Anfrag Funktion projektbezogen Auswertung beschränken vielmehr erstellen semantisch angereichert Datensatz langfristig Funktion wachsend digital Literaturwissenschaft übernehmen bieten Austausch komplementär Projekt forschungsplattform literarisch Feld DDR fördern deutsch Forschungsgemeinschaft dfg Rahmen Exzellenzstrategie Bund Land innerhalb Exzellenzcluster Temporal Communities doing literature global perspectiv exc,"[('dsv', 0.2994057371050897), ('ddr', 0.25663348894721977), ('international', 0.14877834171349874), ('aktivität', 0.1309750826674659), ('einladungspolitik', 0.12831674447360988), ('schriftstellerverband', 0.12831674447360988), ('ostberlin', 0.12831674447360988), ('kartei', 0.12831674447360988), ('openrefin', 0.12831674447360988), ('politisch', 0.12176028337514704)]"
2023,DHd2023,JUNG_Kerstin_Die_Wahl_der_Mittel___Jupyter_Notebooks_als_For.xml,Die Wahl der Mittel 'Jupyter-Notebooks als Forschungsinfrastruktur,"Kerstin Jung (Universität Stuttgart, Institut für Maschinelle Sprachverarbeitung); Pascal Hein (Universität Stuttgart, Institut für Literaturwissenschaft); André Blessing (Universität Stuttgart, Institut für Maschinelle Sprachverarbeitung); Jan Hess (Deutsches Literaturarchiv Marbach); Volodymyr Kushnarenko (Höchstleistungsrechenzentrum, Universität Stuttgart)","Jupyter-Notebooks, ausführbarer Quellcode, Python","Teilen, Programmierung, Veröffentlichung, Visualisierung, Infrastruktur, Software","Mit Python als vielgenutzter Programmiersprache in den Digital Humanities Im Forschungskontext werden solche Notebooks daher verwendet, um auf einer (Web-)Seite Datensätze einzulesen, zu analysieren, visualisieren und die verwendete Methodik zu erläutern, ohne dies auf verschiedene Orte oder Zugänge verteilen zu müssen. In der (Nach-)Nutzung können z. B. Parameter in der Analyse oder Visualisierung direkt im Browser verändert werden und eine Anpassung ohne Programmierkenntnisse oder -erfahrung ermöglichen. Die Notebook-Dateien können wiederum über entsprechende Softwareentwicklungs-Repositorien zur Verfügung gestellt werden, was Anpassungen für weitere Datensätze oder Forschungsfragen erlaubt. Jupyter-Notebooks sind dabei als JSON-Dokumente strukturiert verarbeitbar. Im Rahmen unseres Projekts geht es uns um die Möglichkeit, Jupyter-Notebooks so zur Verfügung zu stellen, dass sie für eine sehr heterogene Nutzendengruppe (u. a. Autor*innen, Forschende, Schüler*innen) einen Mehrwert bedeuten. Zu den Vorteilen der Bereitstellung von Zugängen zu Daten und Analysen durch Notebooks gehören (i) die Möglichkeit, ein Angebot an eine breite Nutzendengruppe zu machen: Je nach Aufbereitung der Notebooks (interaktive Elemente wie Dropdown-Menüs oder Range-Sliders sind möglich) können sie fast ohne Vorkenntnisse mit Python betrieben werden und an das individuelle Forschungsinteresse angepasste Ergebnisse produziert werden, (ii) dass die technischen Voraussetzungen, z. B. benötigte Pakete, im Notebook selbst spezifiziert sind. Diese Vorteile kommen allerdings nur in einer konfigurierten Ausführungsumgebung zum Tragen. Werden nur die Jupyter-Notebook-Dateien bereitgestellt, setzt das bei den Nutzenden Kenntnisse in Python, Bash o. Ö. sowie im Umgang mit Jupyter voraus. Oft sind Pakete in aufeinander abgestimmten Versionen erforderlich oder in Abhängigkeit vom Betriebssystem verfügbar, so dass nur eine vorkonfigurierte Umgebung den Nutzenden tatsächlich die technischen Hürden abnimmt. So stellt sich die Frage, in welchem Rahmen ausführbare Jupyter-Notebooks zur Verfügung gestellt werden können. Der Betrieb einer zugänglichen Ausführungsumgebung (""Hub"") setzt Hardware, Administrations- und Wartungskapazitäten voraus. Eine Nutzungsverwaltung (Vergabe und Pflege von Accounts, Monitoring von Speicher- und Rechenkapazitäten) ist dabei ebenso unerlässlich wie Aktualisierungen mittels Updates auf Ebene von Maschine, Hub und Paketen und damit verbundene Wartungsarbeiten durch Abhängigkeiten in den Notebooks. Der Betrieb einer nachhaltigen Ausführungsumgebung setzt dies für einen längeren Zeitraum voraus, so dass die Idee der eigenen Ausführungsumgebung den Rahmen eines Forschungsprojekts oft übersteigt. Des Weiteren muss der Sicherheitsaspekt berücksichtigt werden, da es sich bei ausführbaren Jupyter-Notebooks um ausführbaren Quellcode handelt, der gewollt oder ungewollt Schaden am eigenen oder an externen Systemen verursachen kann. Mit dem Service Colaboratory Eine Alternative hierzu kann der Betrieb einer stark restringierten Ausführungsumgebung sein, die zwar die vorhandenen Notebooks abspielen kann und Nutzende ggf. aus vorgegebenen Parametern wählen lässt, Forschenden aber kaum Flexibilität bezüglich einer eigenen Exploration oder Einbindung weiterer Pakete ermöglicht. Sofern spezifische technische Expertise angenommen werden kann, ist eine weitere Möglichkeit, Docker-Container zum Download zur Verfügung zu stellen oder eine detaillierte Dokumentation zur Nutzung eines Notebooks innerhalb einer integrierten Entwicklungsumgebung zu liefern. Die Zielgruppe wird damit allerdings auf Nutzende der entsprechenden Infrastruktur einschränkt. Notebooks, die über bestimmte Repositorien öffentlich zur Verfügung gestellt werden, können über Binder Ein entsprechender Ansatz für die breite Forschungscommunity wäre ein großer Gewinn bezüglich der Verfügbarmachung, Nachnutzung und Dokumentation von Forschungsmethoden und -ergebnissen.",de,python vielgenutzt Programmiersprache Digital Humanitie Forschungskontext notebooks verwenden datensätz einzulesen analysieren visualisieren verwendet Methodik erläutern verschieden Ort zugäng verteilen parameter Analyse Visualisierung direkt Browser verändern Anpassung Programmierkenntnisse ermöglichen wiederum entsprechend Verfügung stellen anpassungen datensätz forschungsfragen erlauben strukturieren verarbeitbar Rahmen unser Projekt Möglichkeit Verfügung stellen heterogen Nutzendengruppe forschend Mehrwert bedeuten vorteilen Bereitstellung Zugäng daten Analyse Notebooks gehören i Möglichkeit Angebot breit Nutzendengruppe Aufbereitung Notebooks interaktiv Element fast Vorkenntnisse python betreiben individuell forschungsinteresse angepasste Ergebnis produzieren ii technisch Voraussetzung benötigt paken Notebook spezifizieren Vorteil konfiguriert Ausführungsumgebung tragen bereitstellen setzen nutzend kenntnisse Python Bash Umgang Jupyter voraus pakete aufeinander abgestimmt Version erforderlich Abhängigkeit Betriebssystem verfügbar vorkonfiguriert Umgebung Nutzend tatsächlich technisch Hürde abnehmen stellen Frage Rahmen ausführbar Verfügung stellen Betrieb zugänglich Ausführungsumgebung hub setzen hardware wartungskapazitän voraus Nutzungsverwaltung Vergabe Pflege Accounts Monitoring rechenkapazitäter unerlässlich Aktualisierunge mittels updates Ebene Maschine Hub paken verbunden Wartungsarbeit Abhängigkeit Notebooks Betrieb nachhaltig Ausführungsumgebung setzen lang Zeitraum voraus Idee Ausführungsumgebung Rahmen Forschungsprojekt übersteigen Sicherheitsaspekt berücksichtigen ausführbar ausführbar Quellcode handeln ungewollt schaden extern Systeme verursachen Service Colaboratory Alternative hierzu Betrieb stark Restringiert Ausführungsumgebung vorhanden Notebooks abspielen nutzend vorgegeben Parameter wählen lässt forschenden Flexibilität bezüglich Exploration Einbindung weit pakete ermöglichen sofern spezifisch technisch Expertis annehmen Möglichkeit Download Verfügung stellen detailliert dokumentation Nutzung Notebook innerhalb integriert Entwicklungsumgebung liefern zielgruppe nutzende entsprechend Infrastruktur einschränken Notebooks bestimmt repositorien öffentlich Verfügung stellen bind entsprechend Ansatz breit Forschungscommunity Gewinn bezüglich Verfügbarmachung Nachnutzung Dokumentation forschungsmethod,"[('ausführungsumgebung', 0.354724707440677), ('notebooks', 0.33547950874713056), ('ausführbar', 0.2128348244644062), ('betrieb', 0.1878839586767933), ('nutzend', 0.1732886378305133), ('voraus', 0.16293309288918037), ('verfügung', 0.1440525216753629), ('nutzendengruppe', 0.1418898829762708), ('pakete', 0.1418898829762708), ('paken', 0.1418898829762708)]"
2023,DHd2023,HINZMANN_Maria_SPARQL_für__digitale__Geisteswissenschaftler_.xml,SPARQL für (digitale) Geisteswissen-schaftler:innen 'Querying Wikidata und die MiMoTextBase,"Maria Hinzmann (Trier Center for Digital Humanities, Universität Trier, Deutschland); Anne Klee (Trier Center for Digital Humanities, Universität Trier, Deutschland); Johanna Konstanciak (Trier Center for Digital Humanities, Universität Trier, Deutschland); Julia Röttgermann (Trier Center for Digital Humanities, Universität Trier, Deutschland); Christof Schöch (Trier Center for Digital Humanities, Universität Trier, Deutschland); Moritz Steffes (Trier Center for Digital Humanities, Universität Trier, Deutschland)","Linked Open Data, SPARQL, Literaturgeschichte, Wikidata","Modellierung, Visualisierung, Literatur, Metadaten, benannte Entitäten (named entities), Software","Nicht nur in Kultur- und Gedächtnisinstitutionen, auch in DH-Projekten ist derzeit eine Zunahme des Linked Open Data-Paradigmas sichtbar. Wie können Daten im Sinne von ""Open Data, Open Cultures"" offen, gut zugänglich, interoperabel vernetzt, maschinenlesbar und langfristig verfügbar dargeboten werden? Im Projekt "" Der Workshop setzt es sich zum Ziel, theoretisches und praktisches Wissen zur Modellierung geisteswissenschaftlichen und speziell literaturgeschichtlichen Wissens in Form von Linked Open Data (LOD) zu vermitteln, Einblick in die Syntax der Abfragesprache SPARQL zu geben und den Mehrwert der Aufbereitung von Daten als Wissensgraphen in Anwendungsszenarien aufzuzeigen. Dabei liegt der Schwerpunkt auf der Vermittlung von SPARQL in theoretischen und praktischen Sessions. Teilnehmende sollen die Kompetenz erlangen, die Struktur von SPARQL zu verstehen und eigenständig Queries zu schreiben. Es ist zu beobachten, dass es ein zunehmendes Interesse in der DH-Community gibt, die eigenen Daten in Form von LOD zu veröffentlichen und mit dem Semantic Web zu vernetzen oder die aktuellen Entwicklungen zu reflektieren (Hogan et al. 2021; Ikoniƒá Ne≈°iƒá et al. 2021; Thornton et al. 2021; Alves 2022; Dörpinghaus 2022; Ohmukai / Yamada 2022; Zhao 2022). Auch das Projekt "" SPARQL (SPARQL Protocol and RDF Query Language) ist eine 2008 vom W3C veröffentlichte, graphenbasierte Abfragesprache für RDF (Resource Description Framework). RDF ist ein Datenmodell, mit dem sich Ressourcen im World Wide Web darstellen lassen. Es ist der zentrale Standard des W3C, der semantische Daten in der charakteristischen Tripel-Struktur bestehend aus ""Subjekt 'Prädikat 'Objekt"" repräsentiert. Ausgehend von einem einzelnen solchen Tripel wird die Struktur eines Knowledge Graphen im Workshop entfaltet und die ""Übersetzung"" von Forschungsfragen in natürlicher Sprache in die SPARQL-Syntax erläutert. Die Abfragesprache SPARQL setzt sich aus mehreren Bausteinen zusammen: SPARQL-Abfragen werden häufig innerhalb eines einzelnen Knowledge Graphen gestellt. Es besteht jedoch auch die Möglichkeit, über mehrere Knowledge Graphen hinweg Abfragen zu stellen, sogenannte  Der Workshop vermittelt Grundlagenwissen und Möglichkeiten, die das LOD-Paradigma bietet. Der im Projekt erstellte multilinguale Wissensgraph MiMoTextBase zur Domäne der französischen Literatur des 18. Jahrhunderts soll dabei als Anschauungsbeispiel dienen. Der Workshop möchte praktisches Wissen vermitteln: Wie schreibt man SPARQL-Queries? Welchen Mehrwert kann ein Knowledge Graph für literaturgeschichtliche Fragen im Besonderen und die Geisteswissenschaften im Allgemeinen bieten? In dem halbtägigen Workshop wird der Wissensgraph Konkrete Lernziele sind: Erwerb von Grundlagenwissen zu Semantic Web und RDF, LOD, Wikidata Graph; vertiefte Kenntnisse zu SPARQL und die praktische Fähigkeit, eigene SPARQL-Queries zu formulieren; Kennenlernen der Software Wikibase und Exploration der Visualisierungsmöglichkeiten des SPARQL-Endpoints. Der Workshop wendet sich an digitale Geisteswissenschaftler:innen mit Interesse an LOD und SPARQL. Spezielle Vorkenntnisse sind nicht notwendig. Teilnehmende benötigen einen Laptop. Der Workshop setzt sich aus aufeinander aufbauenden Sessions zusammen, die jeweils Input-Phasen und Übungsphasen verbinden. Es wird vorab eine ausführliche Tutorial-Seite (inklusive Verlinkung auf weitere hilfreiche Ressourcen zum SPARQL-Lernen) zur Verfügung gestellt, die den Teilnehmenden (und allen weiteren Interessierten) in der Vorbereitung sowie zur Vertiefung nützlich sein kann (Hinzmann et al. 2022b). Im Zentrum des Workshops stehen drei Blöcke mit jeweils unterschiedlichem Schwerpunkt, in denen das Formulieren von SPARQL-Queries geübt wird (vgl. für Details den Ablauf im Appendix). Auch Teilnehmende ohne Vorkenntnisse werden schrittweise an zunehmend komplexere Queries herangeführt. Der Schwierigkeitsgrad wächst innerhalb der einzelnen Blöcke, wobei der Fokus auf dem eigenständigen Formulieren sowie Anpassen von Beispiel-Queries und dem Klären aller dabei auftretenden Fragen liegen wird. 1. Im ersten Teil liegt der Fokus auf Abfragen zu literarischen Werken. Im Hinblick auf SPARQL geht es hier zunächst um die zentralen Grundlagen wie das Schreiben einfacher 2. Im zweiten Teil widmen wir uns Wikidata als größtem öffentlichen Wissensgraphen, der sich zugleich als ""Hub"" begreifen lässt (Neubert 2017), und fokussieren Autor:innen als Entitäten. Autor:innen sind in allen geisteswissenschaftlichen Disziplinen relevant und ein wichtiges Scharnier zwischen verschiedenen Wissensgraphen. Bezogen auf die SPARQL-Syntax gehen wir einen Schritt weiter und integrieren Funktionen wie OPTIONAL und FILTER, um das Spektrum der Abfragemöglichkeiten zu erweitern. Ein Einstieg wird hier mit Queries zu Literat:innen der MiMoText-Domäne gemacht. Im nächsten Schritt können die Teilnehmenden die Daten von Autor:innen in ihrer jeweiligen Domäne in Wikidata explorieren. 3. Der dritte Teil verknüpft die beiden vorigen Teile auf mehreren Ebenen. Der Schwerpunkt liegt auf Es soll in der abschließenden Diskussion auch Raum sein, einen kritischen Blick auf Entwicklungen im Bereich des Semantic Web zu werfen, beispielsweise die Frage, welche Monopolisierungskräfte und Marktkräfte Einfluss nehmen (van Hooland / Verborgh 2014, 247–48; Singhal 2012). Zum Abschluss werden die wichtigsten Anwendungsmöglichkeiten und Fragen zusammengetragen und weiterführende Ressourcen (DuCharme 2013; van Hooland / Verborgh 2014; Lincoln 2015; Blaney 2017) sowie bei Interesse Möglichkeiten der Kooperation thematisiert. Maximale Zahl der Teilnehmenden: 25. Wir benötigen einen Raum mit WLAN und Beamer und bieten gern ein Hybrid-Szenario an. ""Mining and Modeling Text"" (Universität Trier, Trier Center for Digital Humanities) wird von der Forschungsinitiative des Landes Rheinland-Pfalz 2019-2023 gefördert. Der Workshop wird von Mitarbeiter:innen des LOD-Projekts ""Mining and Modeling Text"" durchgeführt. Das interdisziplinäre Projekt verfügt über einen eigenen SPARQL-Endpoint und wurde in Wikibase implementiert. Maria Hinzmann; hinzmannm@uni-trier.de; Trier Center for Digital Humanities, Universität Trier | Historisches Seminar: Digital Humanities, Bergische Universität Wuppertal; Forschungsinteressen: Datenmodellierung, LOD, Textanalyseverfahren. Anne Klee; klee@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: Digitale Textverarbeitung; Digitale Lexikographie. Johanna Konstanciak; konstanciak@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: Digitale Textverarbeitung; XML/Web-Technologien. Julia Röttgermann; roettger@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: LOD, Textmining-Verfahren wie Topic Modeling, NER und Sentiment Analysis. Christof Schöch; schoech@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: Computational Literary Studies. Moritz Steffes; steffesm@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: Softwaresysteme, Semantic Web Technologien, Forschungsinfrastrukturen.",de,gedächtnisinstitution derzeit Zunahme Linked op sichtbar daten Sinn open data op Culture zugänglich interoperabel vernetzen maschinenlesbar langfristig verfügbar dargeboten Projekt Workshop setzen Ziel theoretisch praktisch wissen Modellierung geisteswissenschaftlich speziell literaturgeschichtlich wissens Form Linked open Data lod vermitteln Einblick Syntax Abfragesprache Sparql geben Mehrwert Aufbereitung daten Wissensgraphen Anwendungsszenarie aufzuzeigen liegen Schwerpunkt Vermittlung sparql Theoretisch praktisch sessions Teilnehmende Kompetenz erlangen Struktur sparql verstehen eigenständig queries schreiben beobachten zunehmend Interesse daten Form lod veröffentlichen Semantic Web vernetzen aktuell Entwicklung reflektieren hogan et ikoniƒá iƒá et thornton et alv dörpinghaus ohmukai Yamada zhao Projekt sparql Sparql Protocol -- rdf query Language veröffentlichen graphenbasiert Abfragesprach Rdf resource description Framework Rdf datenmodell Ressource World Wide Web darstellen lassen zentral Standard semantisch daten charakteristisch bestehend subjekt Prädikat Objekt repräsentieren ausgehend einzeln Tripel Struktur knowledge Graphen Workshop entfalten Übersetzung forschungsfragen natürlich Sprache erläutern Abfragesprach Sparql setzen mehrere Baustein häufig innerhalb einzeln Knowledge Graphen stellen bestehen Möglichkeit mehrere Knowledge graph Hinweg abfragen stellen sogenannter Workshop vermitteln Grundlagenwissen Möglichkeit bieten Projekt erstellt multilingual Wissensgraph Mimotextbase Domäne französisch Literatur Jahrhundert anschauungsbeispiel dienen Workshop praktische wissen vermitteln schreiben mehrweren knowledg Graph literaturgeschichtlich Frage besonderer geisteswissenschaften bieten halbtägig workshop Wissensgraph konkret lernziel Erwerb Grundlagenwisse Semantic Web Rdf lod Wikidata graph vertieft Kenntnisse sparql praktisch Fähigkeit formulieren kennenlernen Software Wikibase Exploration visualisierungsmöglichkeit Workshop wenden digital Geisteswissenschaftl innen Interesse lod Sparql speziell Vorkenntnisse notwendig Teilnehmende benötigen Laptop Workshop setzen aufeinander aufbauend sessions jeweils übungsphasen verbinden vorab ausführlich inklusive Verlinkung hilfreich Ressource Verfügung stellen teilnehmend Interessiert Vorbereitung Vertiefung nützlich Hinzmann et Zentrum Workshop stehen blöcke jeweils unterschiedlich schwerpunken Formulieren üben Detail Ablauf appendix Teilnehmende Vorkenntnisse schrittweise zunehmend komplex queries heranführen Schwierigkeitsgrad wachsen innerhalb einzeln blöcke wobei Fokus eigenständig formulieren anpassen Klären Auftretende Frage liegen liegen Fokus abfrag literarisch Werk Hinblick Sparql zentral Grundlag Schreiben einfach widmen wikidata Größtem öffentlich Wissensgraphe hub begreifen lässt neubern fokussieren Autor innen entitäen Autor innen geisteswissenschaftlich disziplin Relevant wichtig Scharnier verschieden wissensgraphen bezogen Schritt integrier Funktion optional Filter Spektrum Abfragemöglichkeite erweitern Einstieg queries literat innen nächster Schritt Teilnehmend daten Autor innen jeweilig Domäne Wikidata explorieren verknüpfen vorig Teil mehrere Ebene Schwerpunkt liegen abschließend Diskussion Raum kritisch Blick Entwicklung Bereich Semantic Web werfen beispielsweise Frage Monopolisierungskräft marktkräfte einfluss nehmen van Hooland verborgh singhal Abschluss wichtig Anwendungsmöglichkeit Frage zusammengetragen weiterführend Ressource ducharme van Hooland verborgh Lincoln blaney Interesse Möglichkeit Kooperation thematisieren maximal Zahl Teilnehmend benötigen Raum Wlan Beamer bieten Mining and modeling Text Universität Trier Trier Center for Digital humaniteisen Forschungsinitiative Land fördern Workshop Mitarbeiter innen Mining and modeling Text durchführen interdisziplinär Projekt verfügen Wikibase implementieren Maria Hinzmann Trier Center for Digital Humanitie Universität trier historisch Seminar Digital Humanitie bergisch Universität Wuppertal Forschungsinteresse Datenmodellierung lod textanalyseverfahren Anne klee Trier Center for Digital humaniteisen Forschungsinteresse digital Textverarbeitung digital Lexikographie Johanna Konstanciak Trier Center for Digital humaniteisen Forschungsinteresse digital Textverarbeitung xml Julia Röttgermann Trier Center for Digital humaniteisen Forschungsinteresse lod Topic Modeling ner Sentiment Analysis Christof schöch Trier Center for Digital humaniteisen Forschungsinteresse Computational literary studies Moritz stefefsen Trier Center for Digital humaniteisen Forschungsinteresse Softwaresystem Semantic web Technologie forschungsinfrastrukturen,"[('trier', 0.31607298550820273), ('sparql', 0.2825339844041941), ('lod', 0.24105504949696996), ('center', 0.20635072585372552), ('humaniteisen', 0.20396799858326617), ('workshop', 0.19843240519916414), ('forschungsinteresse', 0.17060838771310646), ('for', 0.16035103943520723), ('web', 0.13355118427486987), ('innen', 0.1310889904010493)]"
2023,DHd2023,CUGLIANA_Elisa_Coding_editions__Computational_approaches_to_.xml,Coding editions. Computational approaches to the editing of pre-modern texts.,"Elisa Cugliana (CCeH - Universität zu Köln, Deutschland)","computational edition, normalisation, processing","Programmierung, Modellierung, Annotieren, Bearbeitung, Manuskript, Text","The endlessness of the digital space allows scholarly editors to conceive edition projects on a grand scale. Indeed, the eternal mission of philology can be identified in the quest for the best way to represent and/or reconstruct primary sources, in order to save them from oblivion and grant the public the most informed access to them. Consequently, the lack of space boundaries characterising the digital environment is one of the first aspects making such an environment perfectly suitable for hosting ambitious edition projects. Considering then the possibilities disclosed by hypertextuality, multimodality and multimediality, one can hardly argue against the digital way of editing, despite the ongoing challenges it must face (cf. for instance Rosselli Del Turco 2016), of which sustainability is clearly in the forefront. However, there are reasons to believe that state-of-the-art digital editions have not yet overcome the limits of a still rather bookish paradigm, obeying a mostly representational logic (Van Zundert 2018, Cugliana and Van Zundert 2022). In this contribution, I will argue that the key to the next level of digital scholarly editing is to be found in computation, that is, in the actual coding of the whole editorial workflow. I will present the theory, the advantages and the challenges of such a computational approach to the editing of pre-modern texts, backing up my claims with examples from the praxis and from my own scholarly work in the field of digital philology. Of course, the field of computational editing is still in its infancy, which means that fully computational editions have not been published yet. However, there are projects While the digital paradigm has been widely described and commented upon (among others by Stella 2007 and Sahle 2016), there is a crucial aspect that has not been sufficiently underlined, which however could represent a turning point in the history of (digital) philology. It is the case of the use of programming code throughout the different phases of the editorial process, aiming at the realisation of what Barabucci and Fischer (2017) defined as the formalisation of textual criticism. The authors, in their conclusions, state that a ""shared formalization would lead to the semi-automatization of the editorial process"", where ""the responsibility of the editors would be to describe their choices and decisions"", while that of the computers would be to ""deal with applying these rules and decisions in the best way"". The act of formalising the competence of the editor is to be seen as an achievement If methods are there to realise theoretical principles in the best possible way, at the same time they can in turn influence the principles themselves, opening up new perspectives uncovering, for instance, implicit biases and illogicalities. Indeed, the use of computation for the modelling and operationalisation of editorial knowledge can contribute to perfecting the editorial workflow in the way envisioned by McCarty (2005), who referred to the ""meaningful surprise"" often arising from the process of modelling and computing. Some examples from the actual application of this approach will hopefully prove its potential for the field of Digital Scholarly Editing. During my PhD, which I completed in February 2022, I edited an Early High German version of Marco Polo""s travel account, also known as ""Version DI"" of the Together with my colleague Gioele Barabucci (Norwegian University of Science and Technology), we developed a method based on a rule-and-exception principle, featuring three XProc pipelines, one for each level of normalisation (Cugliana and Barabucci 2022). Each pipeline consists of a series of XSLT stylesheets which deal with the different steps of the normalisation process, such as the levelling of allographs, the expansion of abbreviations, the regularisation of capitalisation etc. This proved to be a very suitable strategy for dealing with the complexity of the normalisation process. Concerning the choice of XProc pipelines, it has already been shown that small-step pipelines making use of a stateless language such as XSLT prove to be advantageous in that they reduce the complexity of computer programs, they can be easily shared with the peers and they improve the sustainability of the code (Barabucci and Schaeben 2021). Not only were the pipelines successful in generating three levels of normalisation of the texts, but they could also be applied to the transcriptions of all the witnesses edited, despite the fact that some were written in the East-Swabian dialect, and some in Bavarian. This was probably due to the geographical proximity and the contemporaneity of their production, which leads to hypothesise the possibility of creating ""pools of rules"" for the editing of witnesses written in specific areas and historical periods. In my talk I will present the system at the basis of the normalisation of the texts featured in the edition of DI, giving some insights into the very development of the pipelines, both from a strictly philological and from a computational perspective. In particular, I will focus on some tricky aspects such as the cases of ambiguities and exceptions, which might represent a hurdle for the full systematisation of the editorial workflow. A case of reuse of the normalisation pipelines written for Version DI of the  As can be evinced from this abstract, my contribution has a twofold purpose: on the one hand giving a short, but hopefully convincing introduction to the theoretical aspects justifying the scholarly value of a computational approach to editing and, on the other, presenting some meaningful results obtained in the praxis. In particular, the focus will mainly be on the success and challenges of the normalisation pipelines developed in collaboration with Gioele Barabucci, showing what it actually meant to translate into code the usually very analogue task of normalising medieval texts, what problems we encountered and how we solved them. Finally, as an outlook for the possible applications of the computational principle, I will briefly illustrate the work I am doing for my next project, the edition of the",en,endlessness digital space allow scholarly editor conceive edition project grand scale eternal mission philology identify quest good way represent reconstruct primary source order save oblivion grant public inform access consequently lack space boundary characterise digital environment aspect make environment perfectly suitable host ambitious edition project consider possibility disclose hypertextuality multimodality multimediality hardly argue digital way editing despite ongoing challenge face cf instance rosselli del turco sustainability clearly forefront reason believe state art digital edition overcome limit bookish paradigm obey representational logic van zundert cugliana van zundert contribution argue key level digital scholarly editing find computation actual coding editorial workflow present theory advantage challenge computational approach editing pre modern text back claim example praxis scholarly work field digital philology course field computational editing infancy mean fully computational edition publish project digital paradigm widely describe comment stella sahle crucial aspect sufficiently underline represent turning point history digital philology case use programming code different phase editorial process aim realisation barabucci fischer define formalisation textual criticism author conclusion state share formalization lead semi automatization editorial process responsibility editor describe choice decision computer deal apply rule decision good way act formalise competence editor see achievement method realise theoretical principle good possible way time turn influence principle open new perspective uncover instance implicit bias illogicality use computation modelling operationalisation editorial knowledge contribute perfect editorial workflow way envision mccarty refer meaningful surprise arise process modelling computing example actual application approach hopefully prove potential field digital scholarly editing phd complete february edit early high german version marco travel account know version di colleague gioele barabucci norwegian university science technology develop method base rule exception principle feature xproc pipeline level normalisation cugliana barabucci pipeline consist series xslt stylesheet deal different step normalisation process levelling allograph expansion abbreviation regularisation capitalisation etc prove suitable strategy deal complexity normalisation process concern choice xproc pipeline show small step pipeline make use stateless language xslt prove advantageous reduce complexity computer program easily share peer improve sustainability code barabucci schaeben pipeline successful generate level normalisation text apply transcription witness edit despite fact write east swabian dialect bavarian probably geographical proximity contemporaneity production lead hypothesise possibility create pool rule editing witness write specific area historical period talk present system basis normalisation text feature edition di give insight development pipeline strictly philological computational perspective particular focus tricky aspect case ambiguity exception represent hurdle systematisation editorial workflow case reuse normalisation pipeline write version di evince abstract contribution twofold purpose hand give short hopefully convince introduction theoretical aspect justify scholarly value computational approach editing present meaningful result obtain praxis particular focus mainly success challenge normalisation pipeline develop collaboration gioele barabucci show actually mean translate code usually analogue task normalise medieval text problem encounter solve finally outlook possible application computational principle briefly illustrate work project edition,"[('normalisation', 0.27929299000495084), ('editing', 0.2360106433832438), ('pipeline', 0.2141429129803162), ('editorial', 0.20229483718563754), ('barabucci', 0.19949499286067918), ('scholarly', 0.17610797053153943), ('process', 0.1419762106786164), ('principle', 0.13486322479042503), ('way', 0.13151147759281642), ('edition', 0.12125809560629101)]"
2023,DHd2023,CALVO_TELLO_José_GND_und_Normdaten_für_europäische_Literatur.xml,GND und Normdaten für europäische Literatur? Personen und Werke in den multilingualen Korpora von ELTeC,"José Calvo Tello (Niedersächsische Staats- und Universitätsbibliothek Göttingen, Georg-August-Universität Göttingen); Nanette Rißler-Pipka (Gesellschaft für wissenschaftliche Datenverarbeitung mbH (GWDG)); Florian Barth (Niedersächsische Staats- und Universitätsbibliothek Göttingen, Georg-August-Universität Göttingen)","Normdaten, Mehrsprachigkeit, Korpora, GND, ELTeC, Wikidata, VIAF, Literatur","Bereinigung, Identifizierung, Literatur, Metadaten, Personen, Text","Viele Projekte in den Digital Humanities verwenden Identifier für Entitäten wie Personen, Werke, Körperschaften oder Orte, die auf eindeutige Einträge in Normdaten-Verzeichnissen oder in Knowledge Bases verweisen Nichtsdestotrotz ist die Sprache des Forschungsobjekts (z.¬†B. von Textkorpora) und die Wahl der Normdaten-Ressource stark voneinander abhängig. Einige Projekte, die mit deutschsprachigen Texten arbeiten, haben sich für die GND entschieden, um Personen oder Werke zu identifizieren, u.¬†a. die Digitale Bibliothek im TextGrid Repository, Die Bibliotheken und Fachinformationsdienste (FIDs) vieler Philologien in Deutschland verwenden die GND für die Sacherschließung ihrer Titel. Sie reichern umgekehrt die GND mit immer mehr Daten zu fremdsprachigen Autor*innen und deren Werke an. Es liegt nahe, dass die GND weiterhin hauptsächlich Autor*innen und Werke aus dem deutschsprachigen Raum verzeichnet. Das heißt jedoch nicht, dass die GND eine Quelle ist, die sich nur für die Germanistik eignet. Generell ist die GND in Form von Agenturen organisiert, die sich auf viele Bibliotheken und andere Institutionen in Deutschland verteilen und von der DNB koordiniert werden. Daher fragen wir uns: Wie stark ist das Ungleichgewicht innerhalb der GND zwischen Einträgen zu deutsch- und fremdsprachiger Literatur? Unsere Frage beantworten wir anhand der multilingualen Korpora von ELTeC. Dabei handelt es sich um literarische Korpora in verschiedenen europäischen Sprachen, die in der COST-Action Distant Reading erstellt wurden Die Personen und Werke in ELTeC sind teilweise bereits mit Wikidata, GND oder VIAF eindeutig identifiziert. Die Abbildungen 1 und 2 zeigen im Vergleich die Annotation mit Normdaten für Autor*innen und Werke pro Sprache und die Wahl der Normdatenressource für die Identifikation von Autor*innen. Für die Sprachen Griechisch (GRE), Kroatisch (HR), Litauisch (LIT) und Rumänisch (ROM) konnten offenbar keine Normdaten verwendet werden. Um die Vergleichbarkeit der Ergebnisse zu gewährleisten, werden diese Sprachen bei den folgenden Analysen ausgeschlossen. Außer für das französische (FR) und in kleinen Teilen für das spanische (SPA), serbische (SRP) und ukrainische (UKR) Korpus wurden keine Werknormdaten eingetragen. Für die Autor*innen wurde überwiegend VIAF genutzt. Nur das französische und norwegische Korpus wurde auch mit Wikidata und das deutsche Korpus als einziges mit GND-IDs versehen. Wir gehen in zwei Schritten vor, um ein vollständiges Bild über die mögliche Abdeckung mit Normdaten zu erhalten. Zunächst extrahieren wir die IDs der Autor*innen aus den TEI-Dokumenten der ELTeC-Korpora. Anhand der IDs werden die fehlenden Identifier aus Wikidata, GND und VIAF extrahiert. Das gelingt für die GND über die API von Lobid, Im nächsten Schritt werden die Werke mit Rückgriff auf die Autor*innen-ID identifiziert. Auch wenn für vier ELTeC-Korpora Werk-IDs (überwiegend mit VIAF) bereits vom Projekt erfasst wurden, ignorieren wir diese, um die gleiche Methode für alle Korpora anzuwenden. Wir führen drei parallele Ausgehend von den bereits in ELTeC identifizierten Autor*innen (vgl. Abb. 1) wird überprüft, ob diese auch in den jeweils anderen Normdatenressourcen (GND, Wikidata, VIAF) vorhanden sind. Daher sind hier Sprachen, für die keine Normdaten in ELTeC existieren (Griechisch, Kroatisch, Litauisch, Rumänisch), nicht berücksichtigt.  Abbildung 3 zeigt die Summe der Normdaten zu Autor*innen pro Ressource, die in den hier betrachteten Korpora gefunden oder ergänzt werden konnten. Für alle drei Ressourcen ist die Abdeckung hier sehr gut. Die GND liegt im Vergleich nur leicht zurück. Die Verteilung dieser Daten pro Sprache wird in Abbildung 4 gezeigt. Das Bild entspricht der Zusammenfassung aus Abbildung 3. Erwartungsgemäß hat das deutsche Korpus die höchste Quote in der GND. Das norwegische Korpus kann mehr Treffer mit Wikidata als mit VIAF erzielen. Für Tschechisch und Polnisch erreichen sowohl VIAF als auch Wikidata sehr gute Ergebnisse. Neben dem Deutschen bietet die GND eine gute Abdeckung für Sprachen wie Englisch, Französisch, Italienisch, Norwegisch, Schwedisch und Ukrainisch.  Das Bild ändert sich erwartungsgemäß, wenn nicht Autor*innen, sondern Werke in den drei Ressourcen gesucht werden (vgl. Abb.5). Während VIAF und Wikidata mehr als 700 Werke aus ELTeC verzeichnen, erreicht die GND nur knapp über 200.  Um diese Zahlen besser zu verstehen, zeigt Abbildung 6, dass nur das deutsche Korpus akzeptable Ergebnisse aus der GND erreicht (80 % der Werke). Alle anderen Sprachen bewegen sich zwischen null und knapp über 40 %. Die Abdeckung von Wikidata oder VIAF ist für viele Sprachen deutlich höher: von 60 % bis zu 100 %.  Entscheidend für eine Bewertung der Ressource ist nicht nur, ob eine Entität vorhanden ist, sondern wie gut sie mit Metadaten beschrieben ist. Für die GND ist zu erwarten, dass Entitäten aus dem deutschsprachigen Raum ausführlicher beschrieben werden als Entitäten aus anderen Regionen. Um dies zu messen, werden die Daten von allen ELTeC-Autor*innen und deren Werke als XML-RDF-Dokumente aus der GND heruntergeladen und die Anzahl der XML-Elemente quantifiziert. Ohne den semantischen Gehalt der Elemente zu bewerten, gehen wir davon aus, dass mehr Elemente auch mehr Informationen pro Entität bedeuten. Abbildung 7 zeigt daher für die GND die Anzahl der Elemente pro Autor*in. Während für Autor*innen aus dem deutschsprachigen Raum 200-330 Elemente vorhanden sind, werden nur 100-240 für andere Sprachen verzeichnet. Auch wenn Sprachen wie Französisch oder Ukrainisch mittlere Werte (bis 240) zeigen, ist der Abstand zwischen diesen Sprachen und dem Deutschen immer noch sehr groß.  Für die Werke (vgl. Abb. 8) erreichen die deutschsprachigen Entitäten wieder deutlich höhere Werte als alle anderen Sprachen in der GND. Hier ist der Unterschied im Vergleich weniger groß, weil insgesamt für Werke weniger Elemente angelegt werden.  Für einen Vergleich wurden diese Daten auch aus Wikidata extrahiert (Abbildungen 9 und 10). Wir prüfen, ob in Wikidata ähnliche Verzerrungen gegenüber dem Englischen oder anderen Sprachen zu beobachten sind. Jedoch hat in Wikidata keine Sprache einen so klaren Vorsprung im Vergleich zu allen anderen Sprachen wie das Deutsche in der GND. Insgesamt ist ein möglicher Grund für die bessere Abdeckung für Autor*innen und Werke in Wikidata und VIAF ist die Tatsache, dass die Teilnehmenden von ELTeC die Entitäten in Wikidata selbst eingetragen haben Eine weitere Hypothese ist, dass der Kanonisierungsgrad die Abdeckung in den drei Ressourcen beeinflusst. In den ELTeC Korpora wurde dies anhand des Metadatenfelds ""reprints"" belegt. Für Autor*innen und Werke, die keine oder wenige ""reprints"" (""low"") haben, zeigt Abbildung 13, dass alle drei Ressourcen eine niedrigere Abdeckung haben. Dabei ist der Unterschied für die GND deutlich größer als bei VIAF und Wikidata. Die Daten deuten darauf hin, dass die GND stärker vom Kanonisierungsgrad beeinflusst ist als die anderen zwei Ressourcen. Besonders niedrig ist die Abdeckung von nicht kanonisierten Werken in der GND (Abb. 13, ""low reprints""). Zu beachten ist, dass die Verteilung von solchen Metadaten in den ELTeC Korpora nicht gleichmäßig ist. Die Ergebnisse können dementsprechend allein durch die Zusammenstellung der Korpora und die Metadatenanreicherung beeinflusst sein.  Wie gut können nicht-germanistische Projekte aus dem deutschsprachigen Raum mit der GND Autor*innen und Werke identifizieren? Sollten sie lieber auf Wikidata oder VIAF zurückgreifen? Um das zu beantworten, wurden die multilingualen Korpora von ELTeC analysiert. Auch wenn diese Korpora nicht vollständig ausgewogen hinsichtlich Repräsentation der Inhalte und der Persistenz ihrer Identifier sein können, sind sie eine wertvolle Ressource von und für die Community. Weitere ähnliche Evaluationen könnten in Zukunft durchgeführt werden, wenn umfassenderes Vergleichsmaterial identifiziert oder zusammengestellt wird. Das ELTeC Korpus ist zeitlich (19. Jh.), quantitativ und sachlich (100 Romane pro Sprache) notwendig beschränkt und vor diesem Hintergrund sind auch die vorliegenden Ergebnisse zu betrachten. Generell zeigt die GND eine gute Abdeckung von Personendaten und ist damit sehr nah an Wikidata oder VIAF. Jedoch füllt die GND deutlich mehr Felder (d.h. mehr Informationen) zu deutschen Autor*innen als zu anderen europäischen Autor*innen (für Personendaten außerhalb der Literatur mag das anders aussehen). Hinsichtlich der Werknormdaten kann die GND nur für das Deutsche akzeptable Ergebnisse liefern. Auch für andere große Sprachen wie Französisch, Englisch oder Spanisch enthält die GND nur 40 % der enthaltenen Werke in ELTeC. Nicht nur die Abdeckung, sondern auch der Informationsgehalt ist für Werke deutschsprachiger Autor*innen höher als für alle anderen Sprachen. Darüber hinaus scheint die GND stärker vom Kanonisierungsgrad abhängig zu sein als VIAF oder Wikidata. Wenn die GND damit als national-ausgerichtete Normdateninstitution erwartbar schlechter abschneidet, dann wäre zu prüfen, ob die Normdaten anderer Einrichtungen (vor allem von Nationalbibliotheken) eine ähnliche oder sogar stärkere Favorisierung der eigenen Sprache verzeichnen. Durch die Einbindung der GND (der DNB und GND-Agenturen in anderen Bibliotheken) in die NFDI öffnet sich die GND nicht nur der Community, sondern es werden auch wichtige Diskussionen zu Multilingualität (GNDmul)",de,Projekt Digital Humanitie verwend Identifier entität Person werk Körperschaft Ort eindeutig einträge Knowledge base verweisen nichtsdestotrotz Sprache Forschungsobjekt Textkorpora Wahl stark voneinander abhängig Projekt deutschsprachig Text arbeiten Gnd entscheiden Person Werk identifizieren digital Bibliothek Textgrid Repository Bibliothek fachinformationsdienste Fid vieler Philologie Deutschland verwenden Gnd Sacherschließung Titel reichern umgekehrt gnd daten fremdsprachig Werk liegen nahe gnd weiterhin hauptsächlich Werk deutschsprachig Raum verzeichnen gnd Quelle Germanistik eignen generell gnd Form Agentur organisieren bibliothek Institution Deutschland verteilen Dnb koordinieren fragen stark Ungleichgewicht innerhalb Gnd einträgen fremdsprachig Literatur Frage beantworten anhand multilingual Korpora Eltec handeln literarisch Korpora verschieden europäisch Sprache distant Reading erstellen Person Werk Eltec teilweise Wikidata Gnd viaf eindeutig identifizieren abbildungen zeigen Vergleich Annotation normdaten Werk pro Sprache Wahl Normdatenressource Identifikation Sprache griechisch gre kroatisch hr litauisch lit rumänisch Rom offenbar Normdat verwenden Vergleichbarkeit Ergebnis gewährleisten Sprache folgend analyse ausschließen französisch fr Teil spanisch Spa serbischen srp ukrainisch ukr korpus werknormdaten eintragen überwiegend Viaf nutzen französisch norwegisch Korpus Wikidata deutsch Korpus einziger versehen Schritt vollständig Bild möglich Abdeckung normdaten erhalten extrahieren ids anhand ids fehlend Identifier Wikidata Gnd Viaf extrahiern gelingen Gnd Api Lobid nächster Schritt Werk Rückgriff identifizieren überwiegend Viaf Projekt erfasst ignorieren gleich Methode Korpora anwenden führen parallel ausgehend eltec identifiziert abb überprüfen jeweils Normdatenressourc gnd Wikidata viaf vorhanden sprechen Normdat Eltec existieren griechisch kroatisch litauisch rumänisch berücksichtigen Abbildung zeigen Summe Normdat pro Ressource betrachtet Korpora finden ergänzen Ressource Abdeckung gnd liegen Vergleich Verteilung daten pro Sprache Abbildung zeigen Bild entsprechen Zusammenfassung Abbildung erwartungsgemäß deutsch Korpus hoch Quote gnd norwegisch korpus Treffer Wikidata Viaf erzielen tschechisch polnisch erreichen sowohl Viaf Wikidata Ergebnis deutsche bieten gnd Abdeckung Sprache englisch französisch Italienisch norwegisch schwedisch ukrainisch Bild ändern erwartungsgemäß Werk Ressource suchen Viaf Wikidata Werk Eltec verzeichnen erreichen gnd knapp Zahl verstehen zeigen Abbildung deutsch Korpus akzeptabel Ergebnis Gnd erreichen Werk Sprache bewegen null knapp Abdeckung Wikidata viaf Sprache deutlich hoch entscheidend Bewertung Ressource Entität vorhanden metadaten beschreiben Gnd erwarten entitäen deutschsprachig Raum ausführlich beschreiben Entität Region messen daten Werk Gnd heruntergeladen Anzahl quantifizieren semantisch Gehalt Elemente bewerten elemenen Information pro Entität bedeuten Abbildung zeigen Gnd Anzahl Elemente pro deutschsprachig Raum element vorhanden Sprache verzeichnen sprechen französisch ukrainisch mittlerer wert zeigen Abstand Sprache deutsch werk abb erreichen Deutschsprachig entitäen deutlich hoch Wert Sprache Gnd Unterschied Vergleich insgesamt Werk element anlegen Vergleich daten Wikidata extrahiert abbildung prüfen Wikidata ähnlich Verzerrung englisch Sprache beobachten Wikidata Sprache klar Vorsprung Vergleich Sprache deutsch Gnd insgesamt möglich Grund gut Abdeckung Werk Wikidata viaf Tatsache Teilnehmende Eltec Entität Wikidata eintragen Hypothese Kanonisierungsgrad Abdeckung Ressource beeinflussen Eltec Korpora anhand Metadatenfeld reprint belegen Werk reprint low zeigen Abbildung Ressource niedrig Abdeckung Unterschied gnd deutlich groß viaf Wikidata daten deuten gnd stark Kanonisierungsgrad beeinflussen Ressource niedrig Abdeckung kanonisiert Werk Gnd abb low reprints beachten Verteilung metadaten Eltec Korpora gleichmäßig Ergebnis Zusammenstellung Korpora Metadatenanreicherung beeinflussen Projekt deutschsprachig Raum Gnd Werk identifizieren Wikidata Viaf zurückgreifen beantworten multilingual Korpora Eltec analysieren Korpora vollständig ausgewog hinsichtlich Repräsentation inhalt Persistenz Identifier wertvoll Ressource Community ähnlich evaluationen können Zukunft durchführen umfassender Vergleichsmaterial identifizieren zusammenstellen eltec korpus zeitlich jh quantitativ sachlich Roman pro Sprache notwendig beschränken Hintergrund vorliegend Ergebnis betrachten generell zeigen gnd Abdeckung Personendat nah Wikidata Viaf füllen gnd deutlich feld Information deutsch europäisch Personendat außerhalb Literatur aussehen hinsichtlich Werknormdat gnd deutsch akzeptabl ergebnisse liefern Sprache französisch Englisch Spanisch enthalten gnd enthalten Werk Eltec Abdeckung Informationsgehalt Werk deutschsprachig hoch Sprache hinaus scheinen gnd stark Kanonisierungsgrad abhängig viaf Wikidata gnd Normdateninstitution erwartbar schlecht abschneiden prüfen Normdat anderer Einrichtung Nationalbibliotheken ähnlich sogar stark Favorisierung Sprache verzeichnen Einbindung Gnd dnb Bibliothek nfdi öffnen gnd Community wichtig Diskussion Multilingualität gndmul,"[('gnd', 0.619223533419805), ('viaf', 0.318638205672543), ('wikidata', 0.29954509392447276), ('eltec', 0.23091007537573333), ('abdeckung', 0.2099182503415757), ('sprache', 0.2001784314196589), ('werk', 0.18528252566650275), ('ressource', 0.10447449582489321), ('korpora', 0.10269665549886572), ('normdat', 0.08127857934982119)]"
2023,DHd2023,HATZEL_Hans_Ole_Narrativität_und_Handlung.xml,Narrativität und Handlung: Zum Verhältnis von Handlungs- zusammenfassungen und relevanten Ereignissen,Hans Ole Hatzel (Universität Hamburg); Evelyn Gius (Technische Universität Darmstadt); Haimo Stiemer (Technische Universität Darmstadt); Chris Biemann (Universität Hamburg),"Narrativität, Handlung, Erzählwürdigkeit, Zusammenfassung, Ereignisse, Events, Ereignis, Event, pyramide, pyramid, n-gram, bleu, ruge, Ereignishaftigkeit, Pyramiden-Methode, Pyramiden Methode","Inhaltsanalyse, Annotieren, Theoretisierung, Bearbeitung, Daten, Literatur","Welche Ereignisse in Erzähltexten sind besonders relevant? Diese Frage wird in der Literaturwissenschaft im Kontext von verschiedenen Konzepten verhandelt. So können relevante Ereignisse identifiziert werden, indem man die für die Textinterpretation als besonders wichtig erachteten Stellen (so genannte ""Schlüsselstellen"") betrachtet (Arnold & Fiechter, 2022). Auf die Rezeption orientiert sind ebenfalls die empirische Leser:innenforschung (Groeben 1977; Miall & Kuiken 2001) oder die Rezeptionsästhetik (Iser 1976). Steht hingegen der Text im Fokus, kann die Frage nach der Wichtigkeit von Ereignissen in Bezug auf Ereignishaftigkeit oder die so genannte Erzählwürdigkeit untersucht werden (z. B. Hühn 2014; Baroni 2012). Allen Ansätzen gemeinsam ist, dass sie bestimmte Qualitäten von Texten bzw. Textbestandteilen betrachten.  Im Fortgang des Projektes wollen wir überprüfen, inwiefern zwischen diesen grundlegenden Ereignistypen, die auch unter dem Konzept ""Event I"" subsumiert werden können, und besonders erzählwürdigen Ereignissen bzw. so genannten Events II, eine Verbindung besteht. Was bereits mit unseren bestehenden Daten und ohne die weitere Operationalisierung des Event II-Konzepts möglich ist, ist der Abgleich unserer Annotationen mit als besonders handlungsrelevant markierten Textstellen. Diesen Vergleich stellen wir im vorliegenden Beitrag an, indem wir unsere Annotationen von Ereignissen in literarischen Texten mit Zusammenfassungen der entsprechenden Texte abgleichen. Wir gehen davon aus, dass Textstellen durch ihre Erwähnung in Zusammenfassungen als für die Handlung wichtig markiert werden. Für den Abgleich dieser Textstellen mit unseren Narrativitätsverläufen nutzen wir drei Typen von Zusammenfassungen: (1) semiprofessionelle Zusammenfassungen, die Studierende der Literaturwissenschaft verfasst haben, (2) professionelle Zusammenfassungen aus Kindlers Literatur Lexikon und (3) nutzer:innengenerierte Zusammenfassungen der Online-Enzyklopädie Wikipedia. Die Verwendung dieser verschiedenen Zusammenfassungstypen zielt darauf ab, zu analysieren, welche Aspekte bzw. Textstellen für die jeweiligen Zusammenfassungstypen relevant sind und dadurch Rückschlüsse auf ihre Qualität zu ziehen. Die (1) Zusammenfassungen der Studierenden waren Teil der Studienleistungen in einem Seminar. Sie wurden als explizit auf die Handlung bezogene Zusammenfassungen verfasst, die eine maximale Länge von 20 Sätzen haben durften. Außerdem wurden die Studierenden aufgefordert, keine Hilfsmittel (wie Zusammenfassungen auf Wikipedia oder aus Literaturlexika) zu nutzen. Für die vier genutzten Primärtexte Aus den Beiträgen des (2) Kindler-Literaturlexikons und von (3) Wikipedia wurden nur jene Passagen verwendet, die sich auf die Handlung in den Primärtexten beziehen. Passagen, die sich der Autor:in, Rezeption oder Interpretation widmen, wurden nicht berücksichtigt. Durch eine kollaborative Annotation der einzelnen Sätze wurde deren Bezug auf die Handlung des Textes annotiert und ein entsprechender Goldstandard erstellt, der den weiteren Analysen zugrunde liegt. So wurde sichergestellt, dass alle drei Zusammenfassungstypen handlungsorientiert sind. Die Zusammenfassungen wurden dahingehend annotiert, dass jeder Satz der Zusammenfassung mit einer Referenz auf alle Spannen des Primärtextes versehen wurde, auf die er Bezug nimmt. Um die Qualität der Zusammenfassungen und mögliche Unterschiede zwischen den drei Zusammenfassungstypen zu analysieren, evaluieren wir deren Öhnlichkeit. Dazu nutzen wir drei Metriken, mit denen implizit drei unterschiedliche Auffassungen von Öhnlichkeit verbunden sind: Eine weitgehend lexikalische (N-Gramme), eine Metrik auf Basis distributioneller Semantik (Word Embeddings) und eine, die sich weitgehend von der sprachlichen Struktur löst und inhaltsbezogene Vergleiche vornimmt (adaptierte Pyramiden-Methode). Wir nehmen zunächst an, dass die semiprofessionellen Zusammenfassungen durchweg handlungsbezogen sind. Deshalb vergleichen wir jeweils eine semiprofessionelle Zusammenfassung mit allen anderen semiprofessionellen und jede Zusammenfassung aller anderen Typen mit allen semiprofessionellen. Als erstes berechnen wir BLEU- (Papineni et al., 2002) und ROUGE-Scores (Lin et al., 2004), die Öhnlichkeiten unter Zusammenfassungen als N-Gramm-Öhnlichkeit abbilden. Wir gewichten BLEU-{1,2,3} und ROUGE-{1,2,3} jeweils gleich und quantifizieren so die Überlappung von 1-, 2- und 3-Grammen zwischen den unterschiedlichen Texten und geben für BLEU die Precision und ROUGE den F1-Score an. Anhand der Scores in Tab. 1 und Tab. 2 wird ersichtlich, dass die semiprofessionellen Zusammenfassungen nahezu durchgehend die höchsten Öhnlichkeitswerte aufweisen. N-Gramm basierte Metriken haben den Nachteil, dass kleine Unterschiede in der Wortwahl zu einer deutlich geringeren Öhnlichkeit führen können. Um Vergleiche stärker auf die Semantik zu fokussieren, wurde mit BERTScore (Zhang et al., 2020) eine embedding-basierte Methode etabliert. Wenden wir diese auf unsere Texte an, zeigt sich ein deutlich geringerer Unterschied der Zusammenfassungstypen (siehe Tab. 3). Dies weist darauf hin, dass Unterschiede in der N-Gramm-basierten Bewertung zu einem großen Teil auf Unterschiede in der Wortwahl zurückzuführen sind. Für den letzten Vergleich der Zusammenfassungen adaptieren wir die Pyramiden-Methode, die für die automatische Evaluation maschinell generierter Zusammenfassungen entwickelt wurde (Nenkova et al., 2004). Die Zusammenfassungen werden auf Basis von sogenannten Summary Content Units (SCU) mit Referenzzusammenfassungen verglichen. Eine SCU repräsentiert dabei eine semantische, inhaltliche Aussage aus dem Zusammengefassten. Die namensgebende Pyramide repräsentiert dabei das Vorkommen der unterschiedlichen SCUs in der Menge der Referenzzusammenfassungen, wobei die Höhe der Pyramide n der Anzahl der Referenzzusammenfassungen entspricht. Dabei ist in der Regel eine Verteilung zu beobachten, die tatsächlich eine Pyramide aufbaut: eine SCU taucht in allen Wir passen die Pyramiden-Methode in zwei Punkten an unsere Fragestellung an. Zum einen haben wir keine Referenztexte, sondern benutzen das Verfahren zum Vergleich verschiedener Zusammenfassungen. Zum anderen enthalten unsere Daten keine SCUs, diese werden deshalb über Textspannen approximiert. Dafür nehmen wir zunächst an, dass jede Spanne des Textes eine Menge von SCUs enthält, die insofern eindeutig ist, als sie keine Schnittmenge mit den SCUs anderer, disjunkter Textabschnitte hat. Insofern kann jede Textspanne auf eine oder mehrere SCUs abgebildet werden. Textspannen werden derart in Unterspannen zerlegt, dass Spannen sich nur überlagern, wenn sie identisch sind. Somit erhalten wir Spannen, die gemäß unserer Annahme semantisch eindeutig sind (siehe Abb. 2). Eine Textspanne kann nach unseren Annahmen mehrere SCUs enthalten die wir als eine behandeln, dies entspricht einer Ereignis Modellierung in gröberer Granularität.  Die Auswertung der Zusammenfassungen mit der Pyramiden-Methode ist in Tab. 4 zu sehen. Nahezu alle semiprofessionellen Zusammenfassungen liegen dabei über dem Wert 0,70 (siehe Abb. 3), während die anderen Zusammenfassungen im Vergleich zum Mittelwert schlechter abschneiden (siehe Tab. 4).  Insgesamt wird so deutlich, dass eine Betrachtung auf Pyramiden-Ebene Unterschiede offenbart, die zwar denen der N-Gramm-Methoden ähnlich, jedoch nicht grundsätzlich anhand oberflächlichen maschinellen Textauswertungen (z.B. BERTScore) festzumachen sind. Wir wollen nun evaluieren, wie Erzählwürdigkeit, repräsentiert durch die Handlungszusammenfassungen, und Narrativität, repräsentiert durch unsere Narrativitätsgraphen, zusammenhängen. Wir überprüfen dafür, ob der Teil des Originaltextes, auf den sich die Zusammenfassungen beziehen, einen großen Narrativitätswert aufweist. Als erste Analyse berechnen wir dazu den Narrativitätswert der in der Zusammenfassung referenzierten Passagen. Wir setzen diesen ins Verhältnis zum erwarteten Gesamtscore, gegeben der Länge der in der Zusammenfassung enthaltenen Textstellen (in Ereignissen). Auch ein Vergleich von Ereignissen, die in Zusammenfassungen genannt werden, mit jenen die es nicht werden, bestätigt dies anhand der Narrativitätswerte: im Mittel 3,13 für genannte und 2,86 für nicht genannte Ereignisse. Für den Vergleich der Ausschläge der Narrativtiätskurven verwenden wir den Gipfelprominenzfaktor. Dabei handelt es sich um ein Maß, welches die Wichtigkeit eines Ausschlags und damit seinen Wert im Vergleich zum umliegenden Kurvenverlauf quantifiziert. Für diesen Vergleich werden alle lokalen Maxima in der cosinusgeglätteten Narrativitätskurve (window size=50) berücksichtigt und für jedes lokale Maximum wird die Gipfelprominenz berechnet.  Die vorgestellten Ergebnisse deuten darauf hin, dass die Nutzung von Zusammenfassungen für die weitere Arbeit mit Ereignissen und Ereignishaftigkeit produktiv ist. Hervorzuheben ist, dass wir einen Relevanzzusammenhang zwischen Ereignissen und Handlung nachweisen konnten, der auf einer Operationalisierung ersterer anhand der sprachlichen Oberfläche aufbaut. Damit kann unser Ereigniskonzept mit reduziertem Handlungsbezug anhand von handlungsbezogenen Informationen aus Zusammenfassungen weiterentwickelt werden. Die bereits umgesetzte, vergleichsweise erfolgreiche Automatisierung der Ereigniserkennung und damit der Narrativitätsverläufe wird nun in Bezug auf die handlungsbezogene Relevanz von Ereignissen erweiterbar, ohne dass Handlungsinformationen mühevoll manuell für die einzelnen Ereignisse bestimmt werden müssen. Dafür erscheint es vielversprechend, den Handlungsbezug von Zusammenfassungen weiter zu evaluieren und dabei ein Verfahren zu entwickeln, das besonders relevante Stellen identifizieren kann. Dieser Beitrag entstand im von der DFG im Schwerpunktprogramm Computational Literary Studies (SPP 2207) geförderten Projekt ‚ÄûEvelautaing Events in Narrative Theory"" (EvENT).",de,Ereignis Erzähltext relevant Frage Literaturwissenschaft Kontext verschieden Konzept verhandeln relevant Ereignis identifizieren Textinterpretation wichtig erachtet Stelle genannt Schlüsselstelle betrachten Arnold Fiechter Rezeption orientieren ebenfalls empirisch Leser Innenforschung Groeben Miall kuik Rezeptionsästhetik Iser stehen hingegen Text Fokus Frage Wichtigkeit Ereignis Bezug Ereignishaftigkeit genannt Erzählwürdigkeit untersuchen Hühn baroni Ansatz gemeinsam bestimmt qualitäten Text Textbestandteil betrachten Fortgang projektes überprüfen inwiefern grundlegend Ereignistype Konzept Event i subsumieren erzählwürdig Ereignis genannt Event ii Verbindung bestehen unser bestehend daten Operationalisierung Event abgleich annotatio handlungsrelevant markiert Textstelle Vergleich stellen vorliegend Beitrag annotatio Ereignis literarisch Text zusammenfassungen entsprechend Text abgleichen Textstell Erwähnung zusammenfassungen Handlung wichtig markieren abgleich Textstelle unser Narrativitätsverläuf nutzen Type zusammenfassungen semiprofessionell zusammenfassungen Studierende Literaturwissenschaft verfassen professionell zusammenfassungen Kindler Literatur Lexikon Nutzer innengeneriert Zusammenfassunge wikipedia Verwendung verschieden Zusammenfassungstyp zielen analysieren Aspekt Textstell jeweilig zusammenfassungstypen relevant rückschluß Qualität ziehen zusammenfassungen studierender Studienleistunge Seminar explizit Handlung bezogen zusammenfassungen verfassen maximal Länge satz Studierend auffordern Hilfsmittel zusammenfassungen wikipedia Literaturlexika nutzen genutzt Primärtexte Beitrag wikipedia Passag verwenden Handlung Primärtext beziehen passagen Autor Rezeption Interpretation widmen berücksichtigen kollaborativ Annotation einzeln Sätz Bezug Handlung Text annotiert entsprechend Goldstandard erstellen analysen zugrunde liegen sicherstellen zusammenfassungstypen handlungsorientieren zusammenfassunger dahingehend annotiert Satz Zusammenfassung Referenz Spann primärtext versehen Bezug nehmen Qualität zusammenfassunger möglich Unterschied Zusammenfassungstypen analysieren evaluieren Öhnlichkeit nutzen metriken Implizit unterschiedlich Auffassung öhnlichkeit verbinden weitgehend lexikalisch Metrik Basis distributionell Semantik Word embeddings weitgehend sprachlich Struktur lösen inhaltsbezogen Vergleich vornehmen adaptiert nehmen semiprofessionell Zusammenfassunge durchweg handlungsbezogen vergleichen jeweils semiprofessionell Zusammenfassung semiprofessionell Zusammenfassung Typ Semiprofessionelle berechnen papineni et lin et öhnlichkeiten zusammenfassungen abbilden gewicht jeweils quantifizieren Überlappung unterschiedlich Text geben Bleu Precision Roug anhand Scores tab tab ersichtlich semiprofessionell zusammenfassunger nahezu durchgehend hoch öhnlichkeitswerte aufweisen basieren metriken Nachteil Unterschied Wortwahl deutlich gering Öhnlichkeit führen vergleichen stark Semantik fokussieren Bertscore zhang et Methode etablieren wenden Text zeigen deutlich gering Unterschied Zusammenfassungstypen sehen tab weisen Unterschied Bewertung Unterschied Wortwahl zurückführen letzter Vergleich zusammenfassunger adaptieren automatisch evaluation Maschinell Generierter zusammenfassungen entwickeln nenkova et zusammenfassunger Basis sogenannter Summary Content units scu referenzzusammenfassung vergleichen Scu repräsentieren semantisch inhaltlich Aussage Zusammengefasst Namensgebende Pyramide repräsentieren vorkommen unterschiedlich Scus Menge referenzzusammenfassung wobei Höhe Pyramide n Anzahl Referenzzusammenfassung entsprechen Regel Verteilung beobachten tatsächlich Pyramide aufbauen Scu tauchen passen Punkt Fragestellung Referenztexte benutzen Verfahren Vergleich verschieden zusammenfassungen enthalten daten scus Textspann approximiert nehmen Spanne Text Menge scus enthalten insofern eindeutig Schnittmenge scus anderer disjunkt textabschniten insofern Textspanne mehrere Scus abbilden textspannen derart unterspannen zerlegt spannen überlagern identisch somit erhalten spannen gemäß Annahme semantisch eindeutig sehen abb Textspanne unser annahmen mehrere Scus enthalten behandeln entsprechen Ereignis Modellierung gröber Granularität Auswertung zusammenfassunger tab sehen nahezu semiprofessionell Zusammenfassunge liegen Wert sehen abb Zusammenfassunge Vergleich Mittelwert schlecht Abschneid sehen tab insgesamt deutlich Betrachtung Unterschied offenbaren ähnlich grundsätzlich anhand oberflächlich maschinell textauswertungen bertscore festzumachen evaluieren erzählwürdigkeit repräsentieren handlungszusammenfassungen Narrativität repräsentieren Narrativitätsgraphe zusammenhängen überprüfen Originaltexte zusammenfassunge beziehen narrativitätsweren Aufweist Analyse berechnen narrativitätsweren Zusammenfassung referenzierten passagen setzen Verhältnis erwartet Gesamtscore geben Länge Zusammenfassung enthalten Textstelle Ereignis Vergleich Ereignis Zusammenfassunge nennen bestätigen anhand narrativitätswerte genannt genannt Ereignis Vergleich Ausschlag narrativtiätskurv verwenden Gipfelprominenzfaktor handeln Maß Wichtigkeit Ausschlag Wert Vergleich umliegend Kurvenverlauf quantifiziern Vergleich lokal maxima Cosinusgeglätteten Narrativitätskurve Window berücksichtigen jeder lokal Maximum Gipfelprominenz berechnen vorgestellt Ergebnis deuten Nutzung zusammenfassungen Arbeit Ereignis ereignishaftigkeit produktiv hervorzuheben Relevanzzusammenhang Ereignis Handlung nachweisen Operationalisierung ersterer anhand sprachlich Oberfläche aufbauen Ereigniskonzept reduziert Handlungsbezug anhand handlungsbezogen Information zusammenfassunger weiterentwickeln umgesetzt vergleichsweise erfolgreich Automatisierung Ereigniserkennung Narrativitätsverläuf Bezug handlungsbezogen Relevanz Ereignis erweiterbar handlungsinformation Mühevoll manuell einzeln Ereignis bestimmen erscheinen vielversprechend Handlungsbezug zusammenfassungen evaluieren Verfahren entwickeln relevant Stelle identifizieren Beitrag entstehen Dfg Schwerpunktprogramm Computational literary Studies spp gefördert Projekt äûevelautaing Event Narrative Theory Event,"[('zusammenfassungen', 0.4380071253605013), ('ereignis', 0.27600638928950827), ('zusammenfassunger', 0.23584999057873146), ('scus', 0.2170408899852146), ('semiprofessionell', 0.2170408899852146), ('zusammenfassunge', 0.19159694240721478), ('zusammenfassungstypen', 0.14469392665680975), ('event', 0.13846082902434578), ('tab', 0.13485786000027097), ('vergleich', 0.13220937329660304)]"
2023,DHd2023,HORSTMANN_Jan_Textliche_Relationen_maschinenlesbar_formalisi.xml,Textliche Relationen maschinenlesbar formalisieren: Systeme der Intertextualität,"Jan Horstmann (Westfälische Wilhelms-Universität Münster, Deutschland); Christian Lück (Westfälische Wilhelms-Universität Münster, Deutschland); Immanuel Normann (Westfälische Wilhelms-Universität Münster, Deutschland)","Intertextualität, Kategoriensystem, Formale Methoden, Beschreibungslogik, Semantic Web","Beziehungsanalyse, Modellierung, Annotieren, Theoretisierung, Literatur, Standards","Wie können intertextuelle Beziehungen formalisiert und annotiert werden? Was wäre ein kohärentes Kategoriensystem der Intertextualität und welche Formalisierung ist geeignet, um es computergestützt berechenbar zu machen, ohne seine Aussagekraft zu verlieren. Intertextualität ist eine komplexe und zugleich sehr zentrale Kategorie in der Literaturanalyse. Per Definition betrifft sie nicht nur In der Forschung finden sich daher zahlreiche Ansätze, das von Julia Kristeva (1967) benannte theoretische Konzept Intertextualität als einen Beschreibungsbegriff für die Beziehung zwischen Texten zu systematisieren. Zu nennen sind in diesem Zusammenhang insbesondere Barthes (1984), Genette (1982), Pfister (1985), oder in digitaler Hinsicht Scheirer et al. (2016), Schlupkothen und Nantke (2019) und Burghardt und Liebl (2020). Die einzelnen Ansätze und Systematisierungen haben in der Regel verschiedene theoretische oder praxeologische Hintergründe (z.B. Strukturalismus, Poststrukturalismus oder Digital Humanities) und damit verbunden verschiedene Fokusse. Für Kristeva (1967) etwa bildet das Konzept Intertextualität einen theoretischen Zugang zur Dialogizität literarischer Texte. Mit Bezug auf Michail Bachtin entwickelt sie ein Verständnis von Text als ""Mosaik von Zitaten"" (Kristeva 1967, 348). Genette (1982) beschreibt 'ebenfalls mit dem Ziel einer Gattungstypologie 'textuelle Bezugnahmen als Transformation oder Nachahmung hypertextueller Textgattungen. Er differenziert fünf Formen: 1. Intertextualität durch Zitate, Plagiate oder Anspielungen, 2. Paratextualität, womit er Rahmungen wie Titel, Genreklassifikationen, Autorname etc. meint, 3. Metatextualität (d.h. kritische Kommentare), 4. Architextualität (d.h. externe, z.B. durch Kritiker zugewiesene Rahmungen) sowie 5. Hypertextualität, bei der ein späterer Text (Hypertext) ohne einen vorherigen Bezugstext (Hypotext) nicht denkbar ist. Hypertextualität unterteilt er schließlich in Transformation (James Joyce transformiert in In Operationalisierungsansätzen der Digital Humanities wird Intertextualität als Ziel des Beitrags ist 'statt von bestimmten digitalen Verfahren auszugehen 'die theoriegeleitete Modellierung eines maschinenlesbaren Schemas, eines Kategoriensystems, das strukturell und grundlegend Analysen von Intertextualität, wie sie in literaturwissenschaftlichen und -theoretischen Abhandlungen zu finden sind, repräsentieren kann. Schlupkothen und Nantke (2019) verfolgen ein ähnliches Ziel. Bei ihnen ist aber nicht weiter ausgearbeitet, wie sich das Vorhaben, analytisch-interpretatorische Lektürepraktiken zu repräsentieren, zur eingesetzten Technologie X-Link verhält und welche Beziehung diese zu der von den Autoren ins Spiel gebrachten Situationslogik hat. Ob ein logisches System (Prädikatenlogik, Beschreibungslogik, Situationslogik etc.) für die Formalisierung eines Forschungsgegenstandes geeignet ist, hängt von den Zielen ab, die mit der formalen Repräsentation des Gegenstandes erreicht werden sollen. Ist der Gegenstand formal repräsentiert, lassen sich durch ein formalen Kalkül Entscheidungfragen hinsichtlich ihres Wahrheitswertes auswerten und (neue) Aussagen aus dem Formalisierten ableiten, worunter auch das Abfragen der 'Fakten'-Basis zählt. Ganz allgemein sind bei der Wahl eines logischen Systems folgende Aspekte ausschlaggebend: Es sollte so ausdrucksmächtig sein, dass es für die formale Repräsentation des Gegenstandes geeignet ist (Ausdrucksmächtigkeit). Und für die DH ist es wünschenswert, dass der formale Kalkül von einem Computer ausgeführt werden kann (Implementierung), und zwar zudem effizient (Komplexität). Ziel unserer Formalisierung der Domäne Intertextualität ist die Repräsentation von Intertextualitätsanalysen. Eine Einschränkung auf eine bestimmte Intertextualitätstheorie oder auf eine bestimmtes Teilphänomen, etwa werkästhetisch manifeste Intertextualität, soll zunächst nicht erfolgen. Stattdessen versuchen wir, einen gemeinsamen Kern von Intertextualitätskonzepten freizulegen, und zwar so, dass er für spezielle Theorien erweiterbar ist. Ziel ist also, intertextuelle Relationen zu annotieren, abzufragen und ggf. Aussagen abzuleiten. Die Formalisierung wird zunächst mit halbformalen Mitteln durchgeführt: mit einer Liste dessen (der Aspekte oder Merkmale), was repräsentiert werden soll. Einen ersten Zugang zum gemeinsamen Kern der verschiedenen kursierenden Intertextualitätstheorien bietet eine Analyse des Wortes Intertextualität. Es besteht aus den lexikalischen Morphemen Der auf diese halb-formale Art entworfene Kern von Intertextualität lässt sich nicht mit allen formalen Methoden repräsentieren. In der Ontologie ist ausgedrückt, dass intertextuelle Relationenen Mediatoren sein können: Der Kern ist erweiterbar, indem die Klassen Vor allem aber gewinnt Genette aus der Analyse der Hypertextualität eine Gattungstypologie. Dazu unterteilt er diese Beziehung nach zwei Gesichtspunkten: Es kann sich entweder im Hinblick auf den Relationstyp um eine Imitation oder um eine Transformation handeln, und sie kann im Hinblick auf die Art und Weise entweder spielerisch, satirisch oder ernst sein. Daraus gewinnt er durch Kombination sechs Gattungen. In RDFS/OWL formalisiert heißt das: Die Formalisierung wäre dadurch zu ergänzen, dass eine solche hypertextuelle Relation auch über eine Vermittlungsinstanz verfügen muss. Die hier vorgeschlagene Formalisierung auf Grundlage der Beschreibungslogik kommt an bestimmten Stellen an ihre Grenzen. Eine davon steckt im Begriff Situation: Eine der fundamentalen Unterscheidungen intertextueller Relationen ist die zwischen solchen, die werkästhetisch manifest sind (etwa durch paratextuelle Signale oder andere Marker), und solchen, die rezeptionsästhetisch gefunden worden und damit unklarer sind (vgl. Pfister 1985, 23f.). Mit einem rezeptionstheoretischen Hintergrund schlagen Schlupkothen und Nantke (2019) die Situtationslogik nach Barry und Parwise als angemessene formale Methode vor. Die hier vorgeschlagene auf der Beschreibungslogik bzw. OWL basierende Formalisierung bietet die Möglichkeit, die Situation durch Metadaten (Name, Datum) zu kodieren und einem Datensatz anzuhängen. Allerdings ist das keine Implementierung der Situationslogik, bei dem es insbesondere um eine Fromalisierung des Zusammenhangs von Situation und Konsistenz geht. Denselben Einwand wird man auch gegenüber dem Beitrag von Schlupkothen und Nantke einwenden können, denn die von ihnen eingesetzt Technologie X-Link ist ebenfalls keine Implementierung der Situtationslogik. Unser Beitrag hat das Ziel, in der Pluralität unterschiedlicher Konzeptionen von Intertextualität einen Kern von Intertextualität herauszuschälen und so zu formalisieren, dass er durch Theorien erweiterbar ist. Er bahnt damit einen Weg zur Repräsentation intertextueller Beziehungen, welche einerseits der Komplexität der literaturtheoretischen Konzepte gerecht wird und die andererseits Berechenbarkeit gewährleistet. Der Beitrag richtet sich damit einerseits an Intertextualitätstheoretiker*innen und -praktiker*innen. Erstere können durch unsere Formalisierung ihren Intertextualitätsbegriff schärfen: durch eine weitere Verfeinerung des von uns vorgeschlagenen Modells oder durch eine klare Abgrenzung des eigenen Intertextualitätsbegriffs. Intertextualitätspraktiker*innen wird ein (erweiterbares) Modell an die Hand gegeben, um Intertextualität zu identifizieren/zu annotieren und zu analysieren (z.B. mittels Netzwerkvisualisierung, Netzwerkanalyse oder durch eine synoptische Gegenüberstellung von Textpassagen mit intertextuellem Bezug). Andererseits kann unsere theoretische Konzeption die Grundlage für die Architektur einer möglichen Forschungsumgebung bilden, die den Intertextualitätsforschenden sowohl eine Weiterentwicklung oder Anpassung des vorgeschlagenen Intertextualitätsmodells als auch die Erforschung der Intertextualität eines annotierten Textkorpus auf Basis dieses Modells erlaubt.",de,intertextuell Beziehung formalisieren annotieren kohärent Kategoriensystem Intertextualität Formalisierung geeignet computergestützen berechenbar Aussagekraft verlieren Intertextualität komplex zentral Kategorie Literaturanalyse per Definition betreffen Forschung finden zahlreich Ansatz Julia Kristeva benannt theoretisch Konzept Intertextualität Beschreibungsbegriff Beziehung Text systematisieren nennen Zusammenhang insbesondere barth Genette Pfister Digitaler Hinsicht Scheirer et schlupkoth Nantke Burghardt Liebl einzeln Ansatz systematisierungen Regel verschieden theoretisch praxeologisch Hintergründ Strukturalismus Poststrukturalismus Digital Humanitie verbinden verschieden Fokusse Kristeva bilden Konzept Intertextualität theoretisch Zugang Dialogizität literarisch Text Bezug Michail Bachtin entwickeln Verständnis Text Mosaik zitaten kristeva Genette beschreiben ebenfalls Ziel Gattungstypologie textuelle Bezugnahm Transformation Nachahmung Hypertextueller textgattungen differenzieren Form Intertextualität zitat plagiat anspielungen Paratextualität womit Rahmunge Titel Genreklassifikatione autornam meinen Metatextualität kritisch Kommentar Architextualität extern kritik zugewiesen Rahmung Hypertextualität spät Text Hypertext vorherig Bezugstext Hypotext denkbar Hypertextualität unterteilen schließlich Transformation James Joyce transformieren Operationalisierungsansätz Digital Humanitie Intertextualität Ziel Beitrag bestimmt digital Verfahren ausgehen theoriegeleitet Modellierung maschinenlesbaren schemas Kategoriensystem strukturell grundlegend analysen Intertextualität literaturwissenschaftlich abhandlungen finden repräsentieren Schlupkothe Nantke verfolgen ähnlich Ziel ausarbeiten Vorhaben Lektürepraktike repräsentieren eingesetzt Technologie verhalten Beziehung Autor Spiel gebracht Situationslogik logisch System prädikatenlogik Beschreibungslogik Situationslogik Formalisierung forschungsgegenstand geeignet hängen Ziel formal Repräsentation Gegenstand erreichen Gegenstand formal repräsentieren lassen formal Kalkül entscheidungfrag Hinsichtlich wahrheitswert auswerten Aussage Formalisiert ableiten worunter abfragen zählen allgemein Wahl logisch System folgend Aspekt ausschlaggebend ausdrucksmächtig formal Repräsentation Gegenstand geeignet Ausdrucksmächtigkeit dh wünschenswert formal Kalkül Computer ausführen Implementierung zudem effizient Komplexität Ziel Formalisierung Domäne Intertextualität Repräsentation intertextualitätsanalysen Einschränkung bestimmt Intertextualitätstheorie bestimmt Teilphänomen werkästhetisch manife Intertextualität erfolgen stattdessen versuchen gemeinsam Kern intertextualitätskonzept freilegen speziell Theorie erweiterbar Ziel intertextuell Relation annotieren Abzufrag Aussage abzuleiten Formalisierung halbformal Mittel durchführen Liste aspekt merkmal repräsentieren Zugang gemeinsam Kern verschieden Kursierend Intertextualitätstheorien bieten Analyse Wort Intertextualität bestehen lexikalisch Morpheme Art entworfen Kern Intertextualität lässen formal Methode repräsentieren Ontologie ausdrücken intertextuell Relationene mediatoren Kern erweiterbar Klasse gewinnen Genette Analyse Hypertextualität Gattungstypologie unterteilen Beziehung Gesichtspunkt Hinblick relationstyp Imitation Transformation handeln Hinblick Art Weise Spielerisch satirisch ernst gewinnen Kombination Gattung Rdfs owl formalisieren Formalisierung ergänzen hypertextuell Relation Vermittlungsinstanz verfügen vorgeschlagen Formalisierung Grundlage Beschreibungslogik bestimmt Stelle Grenze stecken Begriff Situation fundamental unterscheidung Intertextueller relationen werkästhetisch Manifest paratextuell signal Marker rezeptionsästhetisch finden unklar Pfister rezeptionstheoretisch Hintergrund schlagen schlupkothen Nantke Situtationslogik Barry parwise angemessen formal Methode vorgeschlagen Beschreibungslogik owl basierend Formalisierung bieten Möglichkeit Situation Metadat Name Datum kodieren Datensatz anzuhängen Implementierung Situationslogik insbesondere Fromalisierung Zusammenhang Situation Konsistenz Einwand Beitrag schlupkoth Nantke einwenden einsetzen Technologie ebenfalls Implementierung Situtationslogik Beitrag Ziel Pluralität unterschiedlich Konzeption Intertextualität Kern Intertextualität herauszuschälen formalisieren Theorie erweiterbar bahnen Weg Repräsentation Intertextueller Beziehung einerseits Komplexität literaturtheoretisch Konzept gerecht andererseits Berechenbarkeit gewährleisten Beitrag richten einerseits erstere Formalisierung intertextualitätsbegriff schärfen Verfeinerung vorgeschlagen Modell klar Abgrenzung Intertextualitätsbegriff erweiterbar Modell Hand geben Intertextualität identifizieren annotieren analysieren mittels Netzwerkvisualisierung Netzwerkanalyse synoptisch Gegenüberstellung Textpassagen intertextuell Bezug andererseits theoretisch Konzeption Grundlage Architektur möglich Forschungsumgebung bilden intertextualitätsforschend sowohl Weiterentwicklung Anpassung vorgeschlagen intertextualitätsmodells Erforschung Intertextualität Annotiert Textkorpus Basis Modell erlauben,"[('intertextualität', 0.4647138506502757), ('formalisierung', 0.24784738701348039), ('kern', 0.16571859702930705), ('formal', 0.16524929066024355), ('nantke', 0.1569611959869687), ('erweiterbar', 0.14476803680520717), ('beschreibungslogik', 0.13335415446407675), ('situationslogik', 0.13335415446407675), ('intertextuell', 0.1265403023074555), ('kristeva', 0.12420928507775562)]"
2024,DHd2024,20240108_SCHAUFFLER_Nadja_Annotieren__Visualisieren__Explorieren___ei.xml,"Annotieren, Visualisieren, Explorieren 'ein integrativer Ansatz zur Erschließung von Lyrik in Text und Rezitation","Nora Ketschik (Uni Stuttgart, Deutschland); Nadja Schauffler (Uni Stuttgart, Deutschland); André Blessing (Uni Stuttgart, Deutschland); Markus Gärtner (Uni Stuttgart, Deutschland); Kerstin Jung (Uni Stuttgart, Deutschland); Florin Rheinwald (Uni Stuttgart, Deutschland); Toni Bernhart (Uni Stuttgart, Deutschland); Anna Kinder (Deutsches Literatur Archiv Marbach, Deutschland); Julia Koch (Uni Stuttgart, Deutschland); Sandra Richter (Uni Stuttgart, Deutschland, Deutsches Literatur Archiv Marbach, Deutschland); Rebecca Sturm (Deutsches Literatur Archiv Marbach, Deutschland); Gabriel Viehhauser (Uni Stuttgart, Deutschland); Thang Vu (Uni Stuttgart, Deutschland); Jonas Kuhn (Uni Stuttgart, Deutschland)","Text und Ton, Analysetools, Lyrik, Exploration, Visualisierung, Metadaten","Annotieren, Visualisierung, Metadaten, Ton, Text, Werkzeuge"," Darüber hinaus besteht die Möglichkeit, die Primärdaten 'd.h. die Transkripte der Gedichte sowie die Rezitationen 'einzusehen und (sogar wortweise) abzuspielen, sodass makroanalytische Queries mit mikroanalytischen Untersuchungen kombiniert werden  ",de,hinaus bestehen Möglichkeit primärdaen Transkript Gedicht rezitation einsehen sogar wortweise abzuspielen Sodas makroanalytisch queries mikroanalytisch Untersuchung kombinieren,"[('einsehen', 0.30854168770439544), ('wortweise', 0.30854168770439544), ('mikroanalytisch', 0.30854168770439544), ('primärdaen', 0.30854168770439544), ('sodas', 0.28738319102589965), ('abzuspielen', 0.28738319102589965), ('transkript', 0.28738319102589965), ('makroanalytisch', 0.27237099872448467), ('rezitation', 0.27237099872448467), ('queries', 0.26072663773041865)]"
2024,DHd2024,SLUYTER_G_THJE_Henny_Zur_Perspektive_in_Erz_hltexten__Ein_An_final.xml,Zur Perspektive in Erzähltexten. Ein Ansatz der Computational Literary Studies.,"Henny Sluyter-Gäthje (Universität Potsdam, Deutschland)","Erzählperspektive, Computational Literary Studies, Computational Narratology, Annotation","Erzählperspektive, Computational Literary Studies, Computational Narratology, Annotation","  Daraus ergeben sich zwei Fokusse: Das Vorhaben ist in drei Arbeitspakete unterteilt: Die Erarbeitung der narratologischen Modellierung, die Operationalisierung und Annotation sowie die Implementierung eines Systems zur automatischen Identifizierung. Am Anfang steht die Erarbeitung narratologischer Modellierungen von Perspektive unter dem Gesichtspunkt der Operationalisierbarkeit. Dabei werden etablierte Definitionen auf den Grad der Abstraktheit bzw. in Bezug auf ihre Textnähe geprüft. In dem Projekt wird mit der Modellierung von Schmid (2014) gearbeitet, der fünf Parameter (räumlich, zeitlich, sprachlich, perzeptiv, ideologisch), die die Perspektive bedingen, unterscheidet. Der sprachliche und ideologische Parameter sind weiter unterteilt, für den zeitlichen und räumlichen Parameter werden textuelle Indikatoren zur Erkennung genannt. Aufgrund dieser Ausdifferenziertheit und der Textnähe bildet die Modellierung nach Schmid eine gute Grundlage für die Operationalisierung. Gius (2015) erarbeitete darauf aufbauend bereits ein erstes, rudimentäres Tagset zur Bestimmung von Perspektive.  ",de,ergeben Fokusse Vorhaben arbeitspakete unterteilen Erarbeitung narratologisch Modellierung Operationalisierung Annotation Implementierung System automatisch Identifizierung Anfang stehen Erarbeitung narratologischer Modellierunge Perspektive Gesichtspunkt Operationalisierbarkeit etabliert Definition Grad Abstraktheit Bezug textnäh prüfen Projekt Modellierung Schmid arbeiten Parameter räumlich zeitlich sprachlich perzeptiv ideologisch Perspektive bedingen unterscheiden sprachlich ideologisch parameter unterteilt zeitlich räumlich parameter Textuelle indikatoren Erkennung nennen aufgrund Ausdifferenziertheit Textnähe bilden Modellierung Schmid Grundlage Operationalisierung Gius erarbeiten aufbauend rudimentär Tagset Bestimmung Perspektive,"[('ideologisch', 0.295530158840634), ('erarbeitung', 0.2500619470345365), ('parameter', 0.24327538605813429), ('schmid', 0.2365757597216326), ('modellierung', 0.22354452098457842), ('räumlich', 0.2168426894865115), ('perspektive', 0.20771054026533284), ('zeitlich', 0.17378668632610272), ('operationalisierung', 0.17378668632610272), ('perzeptiv', 0.15864423672576525)]"
2024,DHd2024,FISCHER_Frank_Das__ureigenste_theatralische_Element____Autom.xml,"Das ""ureigenste theatralische Element"" 'Automatische Extraktion von Requisiten aus deutschsprachigen Dramentexten","Jonah Lubin (Freie Universität Berlin, Deutschland); Anke Detken (Georg-August-Universität Göttingen, Deutschland); Frank Fischer (Freie Universität Berlin, Deutschland)","Drama, Theater, Requisiten, Regieanweisungen, DraCor","Sammlung, Strukturanalyse, Literatur"," Der vorliegende Beitrag fokussiert auf einen wichtigen Teilaspekt, nämlich die Requisiten, die in Nebenbemerkungen Erwähnung finden (vgl. die grundlegende Studie von Sofer 2003). Das Requisit ist ""das ureigenste dramatische Element"", das ""den Übergang von der sprachlichen zur theatralischen Ebene"" markiert. Es ist ""zur Kompetenz des Dichters"" zu rechnen und darf ""bei der Untersuchung eines dramatischen Werkes nicht übergangen werden"" (Schwarz 1974, S. 12). Die bisherige Forschung konzentrierte sich meist auf einzelne herausgehobene Requisiten, etwa das Attributsrequisit, das zur Verdeutlichung einer dramatischen Figur dient, und das Emblemrequisit, das unabhängig von den Ziel dieses Beitrags ist es, den Blick über diese speziellen Requisiten hinaus tendenziell auf die Gesamtheit der Requisiten zu richten. Dabei greifen wir auf folgende Definition zurück, ohne diese jedoch beim derzeitigen Stand vollständig operationalisieren zu können, besonders was den Interaktionsaspekt betrifft: ""Im Unterschied zu den Dingen, die der dekorationsmäßigen Ausgestaltung des Bühnenraums dienen, sind Requisiten im engeren Sinne ""Gegenstände, mit denen der Schauspieler bei der Aufführung von Bühnenstücken agiert""."" (Schwarz 1974, S. 18) Anhand des German Drama Corpus (GerDraCor; Fischer et al. 2019), das als ""living corpus"" beständig wächst und derzeit über 600 deutschsprachige Dramen vom 16. bis zum 20. Jahrhundert im Volltext enthält, soll die Verteilung dramatischer Requisiten quantifiziert werden, sowohl chronologisch als auch genrebezogen. Durch die im Vergleich zu bisherigen Arbeiten größere Anzahl an Texten geraten auch nicht-kanonische Texte mit ins Bild und ermöglichen einen repräsentativeren Blick auf die Dramenproduktion des betrachteten Zeitraums. Bezogen auf den Dramentext unterscheidet Roman Ingarden in seiner formalen Betrachtungsweise die Requisiten von nur im Haupttext, also in der Figurenrede erwähnten Gegenständen. Allein die Nennung im Haupttext macht ein Ding also noch nicht zwingend zu einem Requisit. Dies ist erst der Fall, wenn der Gegenstand zusätzlich in den Regiebemerkungen (oder auch nur dort) erwähnt wird (vgl. Ingarden 1965, S. 405). Wir folgen Ingarden hierin und betrachten nur Requisiten, die in Regiebemerkungen genannt werden. Im Folgenden wird zunächst der Workflow für die Extraktion der Requisiten vorgestellt, gefolgt von einigen exemplarischen Analysen.  Aufgrund der verwendeten historischen Orthografie in einer Vielzahl der in GerDraCor enthaltenen digitalisierten Editionen wurde ein Normalisierungsschritt nötig, um die Lemmata zu vereinheitlichen (""Schwerd"" bzw. ""Schwerdt"" wird zu ""Schwert""). Dies geschah mithilfe des DTA::CAB Web Service (vgl. Jurish 2012). Die resultierende Liste modifizierter Wörter haben wir manuell moderiert, um die Ergebnisse zu optimieren. Um im nächsten Schritt die Requisiten zu extrahieren, wurden die Regiebemerkungen zunächst mit ""spaCy"" POS-getaggt, um die Suche auf Substantive bzw. Mithilfe von GermaLemma ( Als Ansatz für die Disambiguierung haben wir dann einen Simplified Lesk Algorithmus mit Glossen von Wiktionary und lexikalischen Feldern von GermaNet verwendet (wie beschrieben in Henrich/Hinrichs 2012). Konnte diese Methode keine definitive Word Sense Disambiguation liefern, haben wir den ranghöchsten Sinn von Wiktionary übernommen. Obwohl wir dank GerDraCor zwar sehr viel Text, also auch Regieanweisungen haben, sind die Ergebnislisten doch überschaubar und taugen für einen explorativen Abbildung 2 zeigt die Zahlen für das gesamte Korpus in chronologischer Darstellung. In dieser Darstellung finden wir bestätigt, dass sich die relative Häufigkeit von Requisiten in unseren Daten ab Ende des 19. Jahrhunderts ändert, parallel zur viel beschriebenen Krise des Dramas, die dann unter anderem Epifizierungstendenzen zeitigte (vgl. Weber 2017, S. 216). Allerdings sind die Outlier nach oben Stücke ohne Szeneneinteilung, die Segmente entsprechen hier umfangreichen Akten: Hermann Bahrs ""Das Konzert"" (1909) und ""Das Phantom"" (1913) sowie wiederum ""Ignorabimus"" von Arno Holz.  Die berechneten Prozentzahlen geben an, in wie vielen Komödien und Tragödien bzw. Prosa- und Versdramen ein bestimmtes Requisit bzw. ein bestimmter Typ von Requisit mindestens einmal vorkommt. Die Unterteilung in Vers- bzw. Prosadramen haben wir auf einfache Weise getroffen: Enthält ein Werk mehr Verszeilen (in TEI kodiert mit dem Element ""l"") als Prosaabsätze (TEI-Element ""p""), dann gilt es als Versdrama, ist das Verhältnis umgekehrt, dann klassifizieren wir es als Prosadrama. Kaffee lässt sich also als Element der Prosakomödie bezeichnen, der Dolch ist ein Requisit der Verstragödie. Diese zwar nicht überraschenden, aber nun bezifferbaren Tendenzen mögen als Fingerzeig dafür dienen, wie sich die Quantifizierung von Requisiten sinnvoll einsetzen lässt. Neben der Fokussierung auf einzelne Requisiten in größeren Korpora gerät auch die Breite des gesamten Arsenals in den Blick, was am Beispiel von erwähnten Waffen in Regieanweisungen erfolgen soll. So lassen sich mindestens 38 individuelle Waffentypen (bzw. Munition) ausmachen, inklusive spezifizierenden Komposita: Armbrust, Bajonett, Bogen, Bombe, Büchse, Degen, Dienstgewehr, Dolch, Doppelflinte, Dreizack, Fangmesser, Flinte, Florett, Geißel, Gewehr, Hellebarde, Hetzpeitsche, Kanone, Keule, Klinge, Knüppel, Knüttel, Lanze, Messer, Muskete, Peitsche, Pfeil, Pistole, Rasiermesser, Revolver, Schild, Schläger, Schnitzmesser, Schwert, Seitengewehr, Speer, Spieß, und Säbel. Waffen im Drama sind immer auch chronologisch kodiert und verorten ein Stück in der Zeit der Handlung. Unter Rückgriff auf die Terminologie des Militärhistorikers Trevor N. Dupuy stammen die meisten Waffentypen im Korpus aus dem ""Age of Muscle""; dem ""Age of Gunpowder"" sind nur ein knappes Dutzend zuzuordnen (Dupuy 1980, S. 288f.).  Der hier präsentierte quantitative Ansatz soll einen Eindruck von der Häufigkeit und Distribution von Requisiten innerhalb deutschsprachiger Dramentexte vermitteln und es ermöglichen, auch die Rolle bisher wenig beachteter Typen von Requisiten zu erforschen. Durch Anpassung der verwendeten computerlinguistischen Tools ließe er sich auch auf Dramenkorpora in anderen Sprachen übertragen. Insgesamt funktioniert die Extraktion von Requisiten recht zuverlässig. Die beobachteten Es wäre wünschenswert, wenn die Befunde dieser kleinen Studie mittelfristig dazu führen würden, dass ein größeres deutschsprachiges Dramenkorpus hinsichtlich vorhandener Requisiten mit entsprechendem Markup versehen wäre, das als Evaluierungsbasis wie auch als Trainingsdatenset dienen könnte.",de,vorliegend Beitrag fokussieren wichtig Teilaspekt nämlich Requisit nebenbemerkung Erwähnung finden grundlegend Studie sofer Requisit ureigen dramatisch Element Übergang sprachliche theatralisch Ebene markieren Kompetenz Dichter rechnen Untersuchung dramatisch Werk übergehen schwarz bisherig Forschung konzentrieren meist einzeln Herausgehobene Requisit Attributsrequisit Verdeutlichung dramatisch Figur dienen Emblemrequisit unabhängig Ziel Beitrag Blick speziell Requisit hinaus tendenziell Gesamtheit Requisit richten greifen folgend Definition derzeitig stehen vollständig operationalisieren interaktionsaspekt betreffen Unterschied Ding dekorationsmäßig Ausgestaltung Bühnenraum dienen Requisit eng Sinn gegenstände Schauspieler Aufführung Bühnenstücke agieren schwarz anhand German Drama Corpus Gerdracor Fischer et living Corpus beständig wachsen derzeit deutschsprachig Dram Jahrhundert Volltext enthalten Verteilung dramatisch Requisit quantifiziern sowohl chronologisch genrebezogen Vergleich bisherig arbeit groß Anzahl Text geraten Text Bild ermöglichen repräsentativer Blick Dramenproduktion betrachtet zeitraums bezogen Dramentext unterscheiden Roman Ingard formal Betrachtungsweise Requisit Haupttext Figurenrede erwähnt genständen Nennung Haupttext Ding zwingend Requisit Fall Gegenstand zusätzlich regiebemerkung erwähnen ingarden folgen ingard Hierin betrachten Requisit Regiebemerkung nennen folgend Workflow Extraktion Requisit vorstellen folgen exemplarisch Analyse aufgrund verwendet historisch Orthografie Vielzahl Gerdracor enthalten digitalisierter editionen Normalisierungsschritt nötig Lemmata vereinheitlichen Schwerd Schwerdt schweren geschehen Mithilfe web Service Jurish resultierend Liste Modifizierter Wörter manuell moderieren Ergebnis optimieren nächster Schritt Requisit extrahieren Regiebemerkung spacy Suche substantiv Mithilfe Germalemma Ansatz Disambiguierung Simplified Lesk Algorithmus Glossen Wiktionary lexikalisch feldern rmanen verwenden beschreiben Henrich Hinrichs Methode definitiv Word sense Disambiguation liefern ranghöchst Sinn Wiktionary übernehmen obwohl Gerdracor text regieanweisungen ergebnislist überschaubar taugen explorativ Abbildung zeigen Zahl gesamt korpus chronologisch Darstellung Darstellung finden bestätigen relativ Häufigkeit Requisit unser daten Jahrhundert ändern parallel beschrieben Krise Drama epifizierungstendenzen zeitigen Weber Outlier Stück Szeneneinteilung Segment entsprechen umfangreich akten Hermann Bahrs Konzert Phantom wiederum Ignorabimus Arno Holz berechnet Prozentzahle geben Komödie Tragödie versdramen bestimmt Requisit bestimmt Typ Requisit mindestens vorkommen Unterteilung Prosadram einfach Weise treffen enthalten Werk verszeil Tei kodieren Element l Prosaabsätz p gelten Versdrama Verhältnis umgekehrt klassifizieren Prosadrama Kaffee lässen Element Prosakomödie bezeichnen Dolch Requisit verstragödie überraschenden bezifferbar Tendenz Fingerzeig dienen Quantifizierung Requisit Sinnvoll einsetzen lässt Fokussierung einzeln Requisit groß Korpora geraten Breit gesamt Arsenal Blick erwähnt Waffe Regieanweisung erfolgen lassen mindestens individuell Waffentype Munition ausmachen inklusive Spezifizierenden Komposita Armbrust bajonett Bogen Bombe Büchse deg Dienstgewehr Dolch Doppelflinte Dreizack fangmess flint Florett Geißel Gewehr hellebarden hetzpeitsch Kanone Keule Klinge Knüppel Knüttel Lanze messe musket peitsch Pfeil Pistole Rasiermesser revolv Schild Schläger schnitzmess schweren Seitengewehr speer speißen säbel Waffe Drama chronologisch kodieren verorten Stück Handlung Rückgriff Terminologie Militärhistoriker trevor Dupuy stammen meister waffentypen Korpus age -- muscle age -- gunpowd knapp Dutzend Zuzuordn Dupuy präsentiert quantitativ Ansatz Eindruck Häufigkeit Distribution Requisit innerhalb deutschsprachig dramentext vermitteln ermöglichen Rolle beachtet Typ Requisit erforschen Anpassung verwendet computerlinguistisch Tool lassen Dramenkorpora Sprache übertragen insgesamt funktionieren Extraktion Requisit zuverlässig beobachtet wünschenswert befunde Studie mittelfristig führen größ deutschsprachig dramenkorpus hinsichtlich vorhanden Requisit entsprechend Markup versehen Evaluierungsbasis trainingsdatenset dienen,"[('requisit', 0.7920050926065078), ('regiebemerkung', 0.10800069444634194), ('dramatisch', 0.0856009099905314), ('gerdracor', 0.08511769633654824), ('chronologisch', 0.07527237346412564), ('waffe', 0.07200046296422798), ('dolch', 0.07200046296422798), ('wiktionary', 0.07200046296422798), ('age', 0.07200046296422798), ('dupuy', 0.07200046296422798)]"
2024,DHd2024,HILGER_Agnes_Figurenbeschreibungen_in_deutschsprachigen_Roma_final.xml,Figurenbeschreibungen in deutschsprachigen Romanen (1789–1914),"Agnes Hilger (Universität Würzburg, Deutschland)","Figurenbeschreibung, Computational Literary Studies, Mixed Methods, Distant Reading, Close Reading, Annotation, Literaturgeschichte","Figurenbeschreibung, Computational Literary Studies, Mixed Methods, Distant Reading, Close Reading, Annotation, Literaturgeschichte","Das Dissertationsprojekt verortet sich im Bereich Computational Literary Studies (CLS) und erforscht die Entwicklung der Figurenbeschreibung in deutschsprachigen Romanen im ""langen 19. Jahrhundert"". Den Ausgangspunkt bildet die für englischsprachige Romane des 19. Jahrhunderts gezeigte Zunahme konkreter Wörter (Heuser und Le-Khac, 2012; Underwood 2019; Piper 2022; Reeve 2023). In Vorarbeiten konnte gezeigt werden, dass in deutschsprachigen Romanen ein ähnlicher Trend existiert: Die relativen Häufigkeiten von Wörtern, die physisch Wahrnehmbares bezeichnen, etwa Kleidung, Möbel oder Köperteile, steigen im untersuchten Korpus über das 19. Jahrhundert hinweg an (Hilger, 2023). Diese Entwicklung lässt sich mit herkömmlichen literaturwissenschaftlichen Darstellungen nicht erklären. Das Dissertationsprojekt setzt hier an und untersucht in einer Kombination aus Distant und Close Reading wie sich der Teilbereich der Beschreibung von Figuren verändert. Dafür werden textuelle Phänomene annotiert: Figurenbeschreibungen, deskriptive Elemente, direkte Charakte-risierung und charakterisierende Elemente. Figurenbeschreibungen und deskriptive Elemente werden als Texteinheiten verstanden, in denen der physisch wahrnehmbaren Außenseite der Figur vergleichsweise stabile Eigenschaften zugeschrieben werden. Die Annotation erfolgt zunächst manuell in CATMA (Gius, 2022), wird jedoch später automati-siert und auf alle Texte im Korpus ausgeweitet. Das Korpus besteht im Moment aus 925 zwischen 1789 und 1914 erschienenen Romanen. Es basiert größtenteils auf den bei TextGrid und im Projekt Gutenberg offen verfügbaren Texten, soll aber noch erweitert und ausbalanciert werden. Neben anderen Metadaten wird die jeweilige ""Kanonizität"" eines Texts erfasst, einerseits um die Zusammensetzung des Korpus transparent zu machen, andererseits, um später bei den Ergebnissen differenzieren zu können. Grundlegend ist dabei Winkos Beschreibung von Kanonisierung als Phänomen der unsichtbaren Hand (2002): Zahlreiche Handlungen auf einer Mikroebene führen gemeinsam auf einer Makroebene zur Kanonisierung eines Autors/einer Autorin, ohne dass dies im Einzelnen beabsichtigt sein muss. Die Rekonstruktion folgt der Logik, dass diese Handlungen zugleich Indikator für die Kanonizität zu einer bestimmten Zeit und in Bezug auf eine bestimmte Gruppe sein können. ""Kanonizität"" wird dementsprechend gemessen über: Nennungen in Literaturgeschichten (Jannidis, 2013; Brottrager u.a., 2021), in der BDSL, in universitären Kurskatalogen und auf Leselisten. Im Analyse-Teil wird untersucht, wie sich die Figurenbeschreibung über das lange 19. Jahrhundert hin im Korpus entwickelt: Nimmt ihr Auftreten zu, und, wenn ja, in welcher Form? Welche Unterscheide gibt es hinsichtlich der Kanonizität? In welchem Verhältnis stehen direkte Charakterisierung und Figurenbeschreibung? Wie werden männliche, wie weibliche Figuren beschrieben? Wie entwickeln stereotype Zuschreibungen, etwa hinsichtlich Ethnizität über die Zeit hin und welche Eigenschaften werden Gruppen als ""typisch"" markiert? Im Anschluss an die Forschung zu Literatur und Physiognomik interessieren Fragen nach der Verknüpfung von äußeren und inneren Eigenschaften. Erkenntnisgewinn verspricht sich die Arbeit vor allem von der Kombination quantitativer und qualitativer Verfahren in einem Mixed Methods-Design. Ein solches wird in den CLS seit mehreren Jahren unter verschiedenen Begriffen eingefordert.",de,Dissertationsprojekt verorten Bereich Computational literary Studies cls erforschen Entwicklung Figurenbeschreibung Deutschsprachig Roman lang Jahrhundert Ausgangspunkt bilden englischsprachig Roman Jahrhundert gezeigt Zunahme konkret Wörter Heuser underwood piper reeve vorarbeite zeigen Deutschsprachig Roman ähnlich Trend existieren relativ häufigkeiten wörtern physisch Wahrnehmbar bezeichnen Kleidung Möbel Köperteil steigen untersucht Korpus Jahrhundert hinweg hilg Entwicklung lässen herkömmlich literaturwissenschaftlich Darstellung erklären Dissertationsprojekt setzen untersuchen Kombination distant clos Reading Teilbereich Beschreibung Figur verändern textuell phänomen annotiert figurenbeschreibungen deskriptive element direkt Charakterisierende elemenen Figurenbeschreibung deskriptiv Element Texteinheit verstehen physisch wahrnehmbar Außenseite Figur vergleichsweise stabil Eigenschaft zuschreiben Annotation erfolgen manuell Catma Gius Text Korpus ausweiten korpus bestehen Moment erschienen Roman basieren größtenteils Textgrid Projekt gutenberg verfügbaren texen erweitern ausbalancieren metadaten jeweilig Kanonizität texts erfassen einerseits Zusammensetzung Korpus transparent andererseits Ergebnis differenzieren grundlegend Winko Beschreibung Kanonisierung Phänom unsichtbar Hand zahlreich Handlung mikroebene fahren gemeinsam Makroebene Kanonisierung Autor Autorin einzeln beabsichtigen Rekonstruktion folgen Logik Handlung Indikator Kanonizität bestimmt Bezug bestimmt Gruppe Kanonizität messen Nennung literaturgeschichten jannidis Brottrager Bdsl universitär Kurskatalog Leseliste untersuchen Figurenbeschreibung Jahrhundert Korpus entwickeln nehmen auftreten Form Unterscheide hinsichtlich Kanonizität Verhältnis stehen direkt Charakterisierung Figurenbeschreibung männlich weiblich Figur beschreiben entwickeln Stereotype zuschreibungen hinsichtlich Ethnizität eigenschaft Gruppe typisch markieren Anschluss Forschung Literatur Physiognomik interessieren Frage Verknüpfung äußerer innerer eigenschaften Erkenntnisgewinn versprechen Arbeit Kombination quantitativ qualitativ Verfahren Mixed Cls mehrere verschieden Begriff einfordern,"[('figurenbeschreibung', 0.3285597649568388), ('kanonizität', 0.2980838319210289), ('wahrnehmbar', 0.17637493694334827), ('dissertationsprojekt', 0.1642798824784194), ('roman', 0.1518565015654977), ('kanonisierung', 0.14904191596051444), ('korpus', 0.14008309332847652), ('jahrhundert', 0.13979101989441006), ('physisch', 0.13150819583618376), ('cls', 0.11434504041142803)]"
2024,DHd2024,20240108_BRUNNER_Annelen_Das_kleine_W_rterbuch_der_Redeeinleiter.xml,Das kleine Wörterbuch der Redeeinleiter,"Annelen Brunner (Leibniz-Institut für Deutsche Sprache, Deutschland); Ngoc Duyen Tanja Tu (Leibniz-Institut für Deutsche Sprache, Deutschland); Lukas Weimer (Niedersächsische Staats- und Universitätsbibliothek Göttingen, Deutschland)","Redewiedergabe, Linguistik, Redeeinleiter","Sammlung, Archivierung, Sprache, Literatur","Das Poster Redeeinleiter sind sprachliche Ausdrücke, die relativ zu einer direkten oder indirekten Rede- oder Gedankenwiedergabe in Voran-, Mittel- oder Nachstellung stehen und diese einleiten (Breslauer, 1996; Michel, 1966; Jäger, 1968). Auch wenn Verben wie (1) (2) (3) [‚Ä¶] Durch ihre Vielfältigkeit sind Redeeinleiter ein interessanter Untersuchungsgegenstand, sowohl aus linguistischer als auch literaturwissenschaftlicher Perspektive. Sie eignen sich, um dynamische Prozesse im lexikalischen Inventar der Sprache zu untersuchen (Tu, erscheint 2024) und spielen eine wichtige Rolle bei der Einbettung von Figurenrede in den narrativen Kontext (McHale 2014, Abschnitt 2).  Grundlage für die Ressource ist das Kernkorpus ""Redewiedergabe"" (RW-Korpus; Brunner et al., 2020a), welches im Rahmen eines DFG-Projekts entstand. Das Korpus umfasst ca. 49.000 Tokens und enthält Ausschnitte aus Erzählungen sowie Zeitungs- und Zeitschriftenartikeln aus dem Zeitraum 1850-1919. Das Textmaterial ist balanciert nach Dekaden sowie dem Merkmal fiktional vs. nicht-fiktional und wurde aufwendig manuell nach Formen von Rede-, Gedanken- und Schriftwiedergabe annotiert (Brunner et al., 2020b): Eine Konsensannotation wurde auf Basis zweier unabhängiger Annotationen erstellt. Zwar ist das RW-Korpus sowohl in TEI-konformem XML-Format als auch in einem spaltenbasierten Textformat vollständig frei verfügbar(https://zenodo.org/records/3739239), es bedarf jedoch technischer Kenntnisse, spezialisierte Informationen wie die über Redeeinleiter zu extrahieren. Die vorgestellte Ressource bietet hierfür einen bequemen und niedrigschwelligen Zugang. Alle 3059 Einleiter-Vorkommen von direkter und indirekter Rede- oder Gedankenwiedergabe wurden mit ihren Attributen extrahiert und zu einer Häufigkeitsliste zusammengefasst. Für jeden der 523 Redeeinleiter-Typen bietet die Ressource einen Überblick über die Vorkommensverteilung nach den Dimensionen ""Medium"" (Rede- oder Gedankenwiedergabe), ""Wiedergabetyp"" (direkt oder indirekt), ""Position"" (initial, medial oder final) und ""Textsorte"" (fiktional oder nicht-fiktional). Abbildung 1 illustriert dies für den Einleiter  Für jede mögliche Attributkombination, mit der ein Redeeinleiter vorkommt, wurde ein zufällig gewählter Korpusbeleg extrahiert. Dieser besteht aus dem Satz, in dem der Redeeinleiter vorkommt, sowie dem vorangehenden und dem nachfolgenden Satz. Eine Verlinkung verknüpft ihn mit dem entsprechenden Dokument im RW-Korpus. Die Liste der Redeeinleiter kann zudem nach Attributwerten gefiltert werden und Redeeinleiter sowie die zugehörigen Belege können als Excel- oder CSV-Tabellen exportiert werden. Mit Hilfe der Ressource bekommt man nicht nur einen grundsätzlichen Überblick darüber, welche Einleiter in welchen quantitativen Verhältnissen verwendet werden, sondern sie erlaubt auch Kontrastierungen in unterschiedlichen Dimensionen. Die Filterfunktion ermöglicht zudem, gezielt solche Einleiter anzuzeigen, die in mehreren Kontexten vorkommen können (z.B. als Rede- und als Gedankeneinleitung). Neben den interessanten Beobachtungen, die die Ressource zu dem Datenbestand des RW-Korpus selbst erlaubt, bietet es sich an, diese mit Daten aus anderen Textgrundlagen zu vergleichen (z.B. moderne Literatur). Das ""Kleine Wörterbuch der Redeeinleiter"" ist unter der Adresse",de,Poster Redeeinleiter sprachlich Ausdrück relativ direkt indirekt Gedankenwiedergabe Nachstellung stehen Einleit Breslauer Michel Jäger verben Vielfältigkeit Redeeinleiter interessant Untersuchungsgegenstand sowohl linguistisch literaturwissenschaftlich Perspektive eignen dynamisch Prozesse lexikalisch Inventar Sprache untersuchen tu erscheinen spielen wichtig Rolle Einbettung Figurenrede narrativ Kontext mchal abschneten Grundlage Ressource kernkorpus Redewiedergabe Brunner et Rahmen entstehen Korpus umfassen Token enthalten Ausschnitt erzählung Zeitschriftenartikel Zeitraum textmaterial balancieren dekad Merkmal fiktional aufwendig manuell Form Schriftwiedergabe annotiert Brunner et Konsensannotation Basis zwei unabhängig annotation erstellen sowohl spaltenbasiert Textformat vollständig frei verfügbar records bedürfen technisch kenntnisse spezialisiert Information Redeeinleiter extrahieren vorgestellt Ressource bieten hierfür Bequeme niedrigschwellig Zugang direkt indirekt Gedankenwiedergabe attributen extrahiern Häufigkeitsliste zusammengefasst bieten Ressource Überblick Vorkommensverteilung dimension Medium Gedankenwiedergabe Wiedergabetyp direkt indirekt Position initial medial final textsoren fiktional Abbildung illustrieren Einleiter möglich Attributkombination Redeeinleiter vorkommen zufällig gewählt Korpusbeleg extrahiern bestehen Satz Redeeinleiter vorkommen vorangehend nachfolgend Satz Verlinkung verknüpfen entsprechend Dokument Liste Redeeinleiter zudem attributwert filtern Redeeinleiter zugehörig Beleg exportieren Hilfe Ressource bekommen grundsätzlich Überblick Einleiter quantitativen verhältnissen verwenden erlauben Kontrastierunge unterschiedlich dimensionen Filterfunktion ermöglichen zudem gezielt Einleiter anzeigen mehrere Kontext vorkommen Gedankeneinleitung interessant Beobachtung Ressource Datenbestand erlauben bieten daten Textgrundlag vergleichen modern Literatur Wörterbuch redeeinleiter Adresse,"[('redeeinleiter', 0.5828525375385147), ('gedankenwiedergabe', 0.23466182682729833), ('einleiter', 0.23466182682729833), ('ressource', 0.1981019243790359), ('indirekt', 0.16037362181964865), ('vorkommen', 0.11677182675196907), ('brunner', 0.11133578337353257), ('direkt', 0.09589221017529502), ('extrahiern', 0.09422686307406547), ('fiktional', 0.0929960034953164)]"
2024,DHd2024,H_USSLER_Julian_Lautst_rke_und_Konflikt_in_Realismus_und_Nat.xml,Lautstärke und Konflikt in Realismus und Naturalismus,"Julian Häußler (Technische Universität Darmstadt, fortext lab, Deutschland); Svenja Guhr (Technische Universität Darmstadt, fortext lab, Deutschland); Evelyn Gius (Technische Universität Darmstadt, fortext lab, Deutschland)","Konflikt, Sound, 19. Jahrhundert, Prosa, Annotation, Naturalismus, Realismus","Annotieren, Kontextsetzung, Visualisierung, Literatur, Methoden, Text","Der Naturalismus gilt als ""radikale Form des Realismus"" (Fricke et al., 2000, S. 684). Die Autor:innen des Naturalismus widmeten sich den aus ihrer Sicht drängenden sozialen Fragen der Zeit. Mit den dadurch veränderten thematischen Schwerpunkten ging auch der Anspruch einher, wirklichkeitsnahe Literatur zu schreiben. Dies steht im Kontrast zu den Themen und poetologischen Prinzipien der parallel weiter existierenden Strömung des Realismus. Auf der Ebene der Textgestaltung lässt sich der Gegensatz der beiden Strömungen u. a. daran festmachen, dass Expositionen, also Anfänge von literarischen Texten, im Realismus typischerweise ereignisarm und gleichzeitig ausführlich gestaltet sind, während im Naturalismus das Prinzip des zeitdeckenden Erzählens (i. e., die Übereinstimmung von erzählter Zeit und Erzählzeit) wichtig wurde. Dieser sogenannte ""Sekundenstil"" sollte als Erzähltechnik eine gewisse Unmittelbarkeit erzeugen. Neben der thematischen Neuausrichtung bzw. Zuspitzung des Naturalismus gibt es also zwischen Realismus und Naturalismus auch damit zusammenhängende Unterschiede in der sprachlichen Darstellung. In unserem Beitrag untersuchen wir deutschsprachige Texte des Realismus und des Naturalismus auf Lautstärke und Konflikthaftigkeit. Wir nehmen an, dass die eben erläuterten poetologischen Prinzipien auch Auswirkungen auf die textliche Gestaltung der so genannten Die Erkennung von Geräuschen des fiktionalen Mit diesen Methoden gehen wir im folgenden Beitrag auf die folgenden Hypothesen ein: (1) Aufgrund der grundsätzlich ablehnenden Haltung des Naturalismus gegenüber dem bürgerlichen Realismus erwarten wir zum einen, dass naturalistische Texte sowohl lauter als auch konflikthafter sind als realistische. (2) Zum anderen vermuten wir, dass die poetologischen Unterschiede der beiden Epochen sich auch auf eventuelle Zusammenhänge der beiden untersuchten Phänomene auswirken. Lautstärke und Konflikthaftigkeit im Realismus sollten eher negativ korrelieren, da dort Konflikte vermutlich eher abgeschwächt dargestellt werden, während im Naturalismus Konflikte auch lautstark ausgetragen werden und eine positive Korrelation zwischen Lautstärke und Konflikthaftigkeit bestehen dürfte. Zur Untersuchung dieser Hypothesen verwenden wir 192 Prosatexte aus dem Realismus und 69 Prosatexte aus dem Naturalismus. Es sind fiktionale Texte unterschiedlicher Länge, die in einschlägiger Sekundärliteratur dem Realismus und Naturalismus des deutschen Sprachraums zugeordnet werden (Böttcher und Geerdts, 1983; Brenner, 2011, Killy, 2016). Die Texte wurden dem deutschsprachigen Prosakorpus Durchschnittliche Textlänge in Wörtern längster Text kürzester Text Tokensumme manuell annotiertes Testset in Wörtern 39.116 169.510 2.773 7.510.306 8.805 33.470 159.161 2.713 2.309.440 10.358 Die systematische Untersuchung von Geräuschen und ihrer Lautstärke ist ein Ansatz aus dem literaturwissenschaftlichen Anwendungsbereich der Für die Operationalisierung von Umgebungsgeräuschen und ihrer Lautstärke verwenden wir die in Guhr und Algee-Hewitt (2023b) fürs Englische entwickelte Geräuschdefinition mit ihrer Unterscheidung zwischen expliziten und interpretationsbedürftigen impliziten Geräuschbeschreibungen sowie die Methoden zur manuellen und lexikonbasierten Annotation von Geräuschen und übertragen diese auf deutschsprachige Prosa. Zum Beispiel wird im Satz ""Der Zug fährt ratternd in den Bahnhof ein"" das ratternde Geräusch des einfahrenden Zuges explizit auf der Wortebene des literarischen Textes angegeben, sodass das Geräusch einer lexikalischen Einheit 'hier dem Adverb ""ratternd"" 'zugeordnet werden kann. In der Geräuschannotation wird dieses Wort als Annotationseinheit betrachtet und lexikonbasiert um die Angabe eines Lautstärkelevels (1-5) von leisen bis sehr lauten Geräuschen erweitert. Als ersten Schritt in Richtung einer Automatisierung der Geräuschannotation verwendeten wir einen lexikonbasierten Ansatz, in dem tokenisierte und lemmatisierte Korpustexte mit einem deutschsprachigen Geräuschwortlexikon abgeglichen wurden. Die Lexikoneinträge sind Schlüssel-Wertpaare von Geräuschwörtern, die einem nach Sachgruppen sortierten Wörterbuch (Dornseiff und Wiegand, 2004) entnommen (u. a. die Sachgruppen: ""Geräusch"", ""lautlos"", ""Stimme"") und hinsichtlich der Dezibel-Heuristik mit Lautstärkelevels ausgezeichnet wurden (Geräuschlexikon = {Lemma : Geräuschlautstärkelevel}). Mithilfe eines String-Matching-Algorithmus und Lexikonabgleichs werden die Korpustexte automatisiert mit dem Tag ""sound"" sowie einem Lautstärkelevel annotiert. Anschließend wird pro Korpustext und auch pro Korpus (Realismus- und Naturalismuskorpus) ein durchschnittlicher Lautstärkewert berechnet, der die annotierten Lautstärkelevels in Relation zur Anzahl der Geräuschwörter und der absoluten Anzahl an Wörtern pro Text abbildet. Für die Evaluation wurden die Annotationen aus dem Lexikonansatz mit manuell erstellten Annotationen verglichen. Dafür wurden die bewährten Evaluationsmetriken   0.987 0.2 0.28 0.23 Die Analyse von Konflikthaftigkeit basiert auf der Anwendung und Adaption der im Projekt Diese Methodik wurde bisher genutzt, um Sentiment- und Konfliktwerte in Romantik-, Realismus-, und Naturalismus-Korpora zu erheben, mit dem Ziel konflikthafte Textstellen zu ermitteln und die Korpora im Hinblick auf ihre Konflikthaftigkeit zu betrachten. Darüber hinaus wurden Sentimentwerte erhoben, in der Erwartung, dass Sentiment ein Signal für Konflikt sein kann (vgl. Häußler und Gius, 2023a und b). Für unseren Anwendungsfall wurden für das Realismus- und Naturalismuskorpus je ein Um unsere Hypothesen zu überprüfen, dass (1) der Naturalismus durchschnittlich lauter und konflikthafter als der Realismus ist, und, dass (2) Konflikte im Naturalismus eher in laute bzw. im Realismus in leise Mit Blick auf die dadurch erhaltenen Ergebnisse zeigt sich im Vergleich der Lautstärkewerte in den beiden Subkorpora zunächst keine auffälliger Unterschied zwischen Realismus und Naturalismus (vgl. Abb. 1). Betrachtet man jedoch die durchschnittlichen Konfliktwerte der einzelnen Texte, so fallen die Texte aus dem Naturalismus als konflikthafter auf (vgl. Abb. 2). In der Betrachtung der einzelnen Texte hinsichtlich ihrer Durchschnittswerte zeigt sich bei den lautesten bzw. leisesten Texte eine Gruppierung nach Autor:innen. So sind die drei durchschnittlich lautesten Texte des gesamten Korpus Texte der Naturalistin Clara Viebig (vgl. Tab. 4) und die drei konflikthaftesten Texte vom Naturalisten Ludwig Thoma. Der Realist Theodor Fontane fällt zudem als wenig ""konflikthafter"" Autor auf (vg. Tab. 4). Um die Korrelation von Lautstärke und Konflikt zu prüfen, betrachten wir als nächstes, welchen Texten sowohl extreme Lautstärke-, als auch extreme Konfliktwerte zugewiesen wurden. Wir definieren hier als extrem die obersten bzw. untersten 10% in der Rangordnung der lautesten bzw. konflikthaftesten Texte und heben jene Texte hervor, die in überschneidenden extremen Gruppen auftreten (vgl. Tab. 5). Zwar bestätigen die Ergebnisse die Vermutung, dass die eher konflikthaften auch die eher lauten Texte sind (Thoma) und, dass die eher weniger konflikthaften auch die eher leisen Texte sind (Fontane), doch zeigt sich für den Naturalisten Thoma, dass auch eine starke negative Korrelation (hohe Konflikthaftigkeit, geringe Lautstärke) im Naturalismus vertreten ist. laute und konfliktarme Texte laute und konflikthafte Texte leise und konfliktarme Texte leise und konflikthafte Um Konflikt und Lautstärke sowie deren Korrelation auch qualitativ zu betrachten, analysieren wir im Folgenden den Text Hinsichtlich der Lautstärke- und Konfliktwerte fallen höhere Konfliktwerte und mehr Lautstärkewörter in der zweiten Hälfte des Textes auf. Zwar entspricht die Eskalation von verbaler zu physischer Gewalt dem Höhepunkt der Konfliktwerte, doch treten z.B. Beleidigungen auch mit unterschiedlicher Konflikthaftigkeit auf. Eine Eskalation des Konfliktes korreliert hier mit einer Häufung der Lautstärkewörter, doch sind die Lautstärkewörter mit dem höchsten Wert nicht gleichbedeutend mit dem Höhepunkt der Eskalation. Die Knaben sprechen vor der eigentlichen Explosion darüber, ob Modellschiffe auch Munition verschießen können (‚Äöschießt‚Äò, ‚Äöschießen‚Äò bzw. ‚Äöknallen‚Äò). Arthur fragt z.B. den Protagonisten, ob der Verschuss der Munition ""recht knallen wird"". Die Eskalation kündigt sich dann mit der Erwartung der Explosion an (‚Äöknallt‚Äò). Die Explosion selbst wird mit Worten beschrieben, die nicht den Lautstärkewert 5 besitzen bzw. nicht im Geräuschwortlexikon enthalten sind. Erst in den Momenten physischer Gewalt, den Ohrfeigen des Weiherbesitzers, knallt es wieder. Dieser unterstellt ihnen zudem, sein Haus sprengen zu wollen. Durch unsere Untersuchungen konnten wir zeigen, dass mit diesem ersten Ansatz zur Analyse von Konflikthaftigkeit und Geräuschlautstärke in fiktionalen Texten bereits einige vergleichende Rückschlüsse auf Texte des Realismus und Naturalismus getroffen werden können. Wir konnten herausstellen, dass naturalistische Texte auffällig konflikthafter sind (vgl. Tab. 4), während sie sich hinsichtlich ihrer Lautstärke kaum von Texten des Realismus unterscheiden (vgl. Tab. 3). Interessant ist, dass sich mit Blick auf die Texte mit den höchsten/niedrigsten durchschnittlichen Lautstärkewerten und den höchsten/niedrigsten Konfliktwerten Autor:innencluster herausstellen lassen. Dabei entspricht die Beobachtung der Hypothese 1), da die lautesten Texte (Viebig) und konflikthaftesten Texte (Thoma) dem Naturalismuskorpus zugehören, während die am wenigsten konflikthaften Texte realistisch sind (Fontane). Die Verschränkung von Lautstärke und Konflikthaftigkeit (vgl. Hypothese 2) scheint zwar einerseits zu bestätigen, dass die konflikthaften Texte von Thoma (Naturalismus) auch laut sind (positive Korrelation), doch trifft diese Beobachtung nicht auf alle Texte des Autoren (hier: Bei der Analyse des Beispieltextes (",de,Naturalismus gelten radikal Form Realismus fricke et Autor innen Naturalismus widmen Sicht drängend sozial Frage verändert thematisch Schwerpunkt Anspruch einher wirklichkeitsnah Literatur schreiben stehen Kontrast Thema poetologisch prinzipien parallel existierend Strömung Realismus Ebene Textgestaltung lässen Gegensatz Strömung festmachen exposition Anfang literarisch Text Realismus typischerweise ereignisarm gleichzeitig ausführlich gestalten naturalismus Prinzip zeitdeckend erzählens Übereinstimmung erzählt Erzählzeit wichtig sogenannter Sekundenstil Erzähltechnik gewiß Unmittelbarkeit erzeugen thematisch Neuausrichtung Zuspitzung Naturalismus Realismus naturalismus zusammenhängend Unterschied sprachlich Darstellung unser Beitrag untersuchen deutschsprachig Text Realismus Naturalismus Lautstärke konflikthaftigkeit nehmen erläutert poetologisch prinzipien Auswirkung textlich Gestaltung genannt Erkennung geräuschen Fiktionale Methode folgend Beitrag folgend Hypothese aufgrund grundsätzlich ablehnend Haltung Naturalismus bürgerlich Realismus erwarten naturalistisch Text sowohl laut konflikthaft Realistische vermuten poetologisch Unterschied epochen eventuell Zusammenhäng untersucht Phänomen auswirken Lautstärke konflikthaftigkeit Realismus eher negativ korrelieren Konflikt vermutlich eher Abgeschwächt darstellen Naturalismus Konflikt lautstark austragen positiv Korrelation Lautstärke konflikthaftigkeit bestehen dürfen Untersuchung Hypothese verwenden Prosatext Realismus Prosatext Naturalismus fiktional Text unterschiedlich Länge einschlägig Sekundärliteratur Realismus naturalismus deutsch Sprachraum zuordnen böttch geerdt Brenner killy Text deutschsprachig Prosakorpus durchschnittlich Textlänge wörtern längst Text kürzester Text Tokensumme manuell annotiertes Testset wörtern systematisch Untersuchung geräuschen Lautstärke Ansatz literaturwissenschaftlich anwendungsbereich Operationalisierung umgebungsgeräusch Lautstärke verwenden Guhr für englisch entwickelt Geräuschdefinition Unterscheidung explizit interpretationsbedürftigen impliziten Geräuschbeschreibung Methode manuell lexikonbasiert Annotation geräuschen übertragen deutschsprachig Prosa Satz Zug fahren ratternd Bahnhof ratternd Geräusch einfahrend zug Explizit Wortebene literarisch Text angeben sodass Geräusch lexikalisch Einheit Adverb ratternd zuordnen Geräuschannotation Wort Annotationseinheit betrachten lexikonbasieren Angabe lautstärkelevels leise laut geräuschen erweitern Schritt Richtung Automatisierung Geräuschannotation verwenden lexikonbasiert Ansatz Tokenisiert lemmatisieren korpustexte deutschsprachig Geräuschwortlexikon abgeglichen lexikoneinträge Geräuschwörter Sachgrupp sortiert Wörterbuch Dornseiff Wiegand entnehmen Sachgruppe geräusch lautlos Stimme hinsichtlich Lautstärkelevels auszeichnen Geräuschlexikon Lemma geräuschlautstärkelevel Mithilfe lexikonabgleichs Korpustexte automatisieren Sound Lautstärkelevel annotiert anschließend pro Korpustext pro Korpus Naturalismuskorpus durchschnittlich lautstärkewern berechnen annotierter lautstärkelevels Relation Anzahl Geräuschwörter absolut Anzahl wörtern pro Text abbildet Evaluation annotatio Lexikonansatz manuell erstellt Annotation vergleichen bewährt evaluationsmetriker Analyse konflikthaftigkeit basieren Anwendung Adaption Projekt methodik nutzen Konfliktwert erheben Ziel konflikthaft Textstell ermitteln Korpora Hinblick Konflikthaftigkeit betrachten hinaus sentimentwert erheben Erwartung Sentiment Signal Konflikt Häußler Gius -- unser Anwendungsfall Naturalismuskorpus Hypothese überprüfen Naturalismus durchschnittlich laut Konflikthafter Realismus Konflikt Naturalismus eher Laute Realismus Leise Blick erhalten Ergebnis zeigen Vergleich Lautstärkewerte Subkorpora auffällig Unterschied Realismus naturalismus abb betrachten durchschnittlich Konfliktwert einzeln Text fallen Text Naturalismus Konflikthafter abb Betrachtung einzeln Text hinsichtlich Durchschnittswert zeigen lautest leisest Text Gruppierung Autor innen durchschnittlich lautest Text gesamt Korpus Text Naturalistin Clara viebig tab konflikthaftest Text naturalist Ludwig Thoma Realist Theodor Fontane fallen zudem konflikthaft Autor vg tab Korrelation Lautstärke Konflikt prüfen betrachten nächster Text sowohl extrem extrem Konfliktwert zuweisen definieren extrem oberer untersen Rangordnung Lautest konflikthaftest Text heben Text hervor überschneidend extrem Gruppe auftreten tab bestätigen Ergebnis Vermutung eher Konflikthaft eher laut Text thoma eher Konflikthaft eher leise Text fontan zeigen naturalist Thoma stark negativ Korrelation hoch konflikthaftigkeit gering Lautstärke Naturalismus vertreten lauten konfliktarm Text laut konflikthaft Text leise konfliktarm Text leise konflikthafte Konflikt Lautstärke Korrelation qualitativ betrachten analysieren folgend Text hinsichtlich Konfliktwert fallen hoch konfliktwert lautstärkewört Hälfte Text entsprechen Eskalation Verbaler physisch Gewalt Höhepunkt Konfliktwert treten Beleidigung unterschiedlich konflikthaftigkeit Eskalation konflikt korrelieren Häufung Lautstärkewörter Lautstärkewörter hoch Wert gleichbedeutend Höhepunkt Eskalation Knabe sprechen eigentlich Explosion modellschiffe Munition verschießen äöschiessen äò äöschießen äò äöknalle äò Arthur fragen Protagonist verschuss Munition knallen Eskalation kündigen Erwartung Explosion äöknalln äò Explosion Wort beschreiben Lautstärkewert besitzen Geräuschwortlexikon enthalten momenten physisch Gewalt ohrfeig Weiherbesitzer knallen unterstellen zudem Haus sprengen Untersuchung zeigen Ansatz Analyse konflikthaftigkeit Geräuschlautstärke fiktional Text vergleichend rückschluß Text Realismus naturalismus treffen herausstellen naturalistisch Text auffällig konflikthaft tab hinsichtlich Lautstärke Text Realismus unterscheiden tab interessant Blick Text hoch niedrig durchschnittlich lautstärkewerter hoch niedrig Konfliktwerte Autor innenclust herausstellen lassen entsprechen Beobachtung Hypothese lautest Text viebig konflikthaftest Text Thoma Naturalismuskorpus zugehören wenig konflikthaft Text realistisch fontan Verschränkung Lautstärke konflikthaftigkeit Hypothese scheinen einerseits bestätigen konflikthaft Text Thoma Naturalismus laut positiv Korrelation treffen Beobachtung Text Autor Analyse Beispieltext,"[('naturalismus', 0.4080938535091432), ('realismus', 0.30041560172152154), ('lautstärke', 0.2602739760358236), ('konflikthaft', 0.2342465784322412), ('konflikthaftigkeit', 0.2342465784322412), ('text', 0.21198210076035345), ('konfliktwert', 0.17690291302898697), ('konflikt', 0.15020780086076077), ('thoma', 0.1474190941908225), ('leise', 0.1301369880179118)]"
2024,DHd2024,G_GGELMANN_Michael_Automatische_Erkennung_von_Bez_gen_zwisch_final.xml,Automatische Erkennung von Bezügen zwischen Epistolographie und Literatur,"Michael Göggelmann (Universität zu Köln, Universität Tübingen, Deutschland)","Machine Learning, Briefe, Literatur, Briefkorpora, Computational Literary Studies, Referenzen, Intertextualität, Korpus, Epistolographie, Charles Dickens, Quotation Detection, Named Entity Recognition, Text Reuse","Machine Learning, Briefe, Literatur, Briefkorpora, Computational Literary Studies, Referenzen, Intertextualität, Korpus, Epistolographie, Charles Dickens, Quotation Detection, Named Entity Recognition, Text Reuse","Die Epistolographie von Schriftstellerinnen und Schriftstellern tritt vor dem eigentlichen literarischen Werk naturgemäß eher in den Hintergrund. Dabei können einige Briefsammlungen sowohl quantitativ als auch hinsichtlich ihrer ästhetischen Tragweite als ""Werk neben dem Werk"" Das Projekt verspricht in zweifacher Hinsicht Innovationspotenzial: neben einem Beitrag zur Entwicklung quantitativer Methoden der Textanalyse soll die Beantwortung solcher literaturwissenschaftlicher Forschungsfragen vereinfacht oder ermöglicht werden, die eine stärkere Verknüpfung von Briefen und fiktionalen Werken voraussetzen. Die Digitalisierung und Edition von Briefkorpora rückte schon früh in das Blickfeld computergestützter Geisteswissenschaft (Cheney, 1983). Auch aktuelle Projekte zur digitalen Briefedition knüpfen häufig an ältere Editionsprojekte an, die nach Jahrzehnten analogen Arbeitens um ein Digitalisierungsvorhaben ergänzt wurden. Computergestützte Projekte zu Briefsammlungen verbindet somit, dass sie als dezidierte Editionsprojekte zumeist spezifisch-epistolare Fragestellungen zur Aufbereitung eines Datensatzes behandeln und sich computergestützter Analysen vorrangig zur Visualisierung vorhandener, oder der Generierung neuer Metadaten bedienen. Methodische Schnittpunkte hingegen ergeben sich insbesondere mit Projekten, die sich computergestützt mit (Teil-)Aspekten der Intertextualität befassen. Hierzu gab es auch in den vergangenen Jahren immer wieder Beiträge bei der DHd (u.a. Liebl und Burkhardt, 2020). Das Arbeitsvorhaben stützt sich zunächst auf das digitale Briefkorpus von Charles Dickens, das etwa 14.000 Briefen umfasst und auf der zwölfbändigen Pilgrim-Edition seiner Briefsammlung basiert (House et al., 2001). Das im Projekt bislang noch nicht weiter untersuchte Teilkorpus der literarischen Werke von Dickens wurde aus dem Gutenberg-Projekt zusammengestellt. Als erster Orientierungspunkt für die automatische Erkennung von Bezügen dient die methodische Zweiteilung in eine vorangestellte Hinsichtlich der Daraus ergeben sich für die Während nicht auszuschließen ist, dass sich der Schritt der",de,Epistolographie Schriftstellerinne Schriftsteller treten eigentlich literarisch Werk naturgemäß eher Hintergrund Briefsammlung sowohl quantitativ hinsichtlich ästhetisch Tragweit Werk Werk Projekt versprechen zweifach Hinsicht innovationspotenzial Beitrag Entwicklung quantitativ Methode Textanalyse Beantwortung literaturwissenschaftlich Forschungsfrag vereinfachen ermöglichen stark Verknüpfung Brief fiktional Werk voraussetzen Digitalisierung Edition Briefkorpora rücken früh Blickfeld Computergestützter Geisteswissenschaft Cheney aktuell Projekt digital Briefedition knüpfen häufig alt editionsprojekte Jahrzehnt analogen arbeitens Digitalisierungsvorhaben ergänzen computergestützt Projekt Briefsammlung verbinden somit dezidiert editionsprojekte zumeist Fragestellung Aufbereitung datensatz behandeln Computergestützter Analyse vorrangig Visualisierung vorhanden Generierung neu metadaten bedienen Methodische schnittpunken hingegen ergeben insbesondere Projekt computergestützen Intertextualität befassen hierzu beitrag dhd Liebl Burkhardt arbeitsvorhaben stützen digital Briefkorpus Charles Dicken Brief umfassen zwölfbändiger Briefsammlung basieren house et Projekt bislang untersuchen Teilkorpus literarisch Werk dickens zusammenstellen orientierungspunken automatisch Erkennung Bezüge dienen methodisch Zweiteilung Vorangestellte hinsichtlich ergeben ausschließen Schritt,"[('briefsammlung', 0.35139887478967213), ('werk', 0.22151705947032035), ('editionsprojekte', 0.2068026597188618), ('projekt', 0.18093755614700147), ('brief', 0.1746727089586358), ('computergestützter', 0.16010304705373332), ('house', 0.11713295826322405), ('vorangestellte', 0.11713295826322405), ('digitalisierungsvorhaben', 0.11713295826322405), ('zweiteilung', 0.11713295826322405)]"
2024,DHd2024,FISCHER_Frank_Literatur_im_Wikiversum___Eine_praktische_Ann_.xml,Literatur im Wikiversum 'Eine praktische Annäherung über API-Abfragen und Wikipedia-Metriken,"Viktor J. Illmer (Freie Universität Berlin, Deutschland); Bart Soethaert (Freie Universität Berlin, Deutschland); Lilly Welz (Freie Universität Berlin, Deutschland); Frank Fischer (Freie Universität Berlin, Deutschland); Robert Jäschke (Humboldt-Universität zu Berlin, Deutschland)","Wikipedia, Literatur, API, Python","Crowdsourcing, Einführung, Lehre, Literatur, Werkzeuge","Die kollaborativ erstellte Online-Enzyklopädie Wikipedia bietet mit derzeit über 60 Millionen Artikeln in über 300 Sprachversionen Neben der individuellen Lektüre der Fließtexte (und ihrer jeweiligen Versionshistorie) über die Website bietet die Online-Plattform über eine API weitere Möglichkeiten zur Analyse der enzyklopädischen Inhalte und des Community-Engagements. Die Vielzahl an Metadaten, sowohl zu den einzelnen Themen selbst als auch zur Bearbeitung und Nutzung durch die aktiv partizipierende Community bzw. die Leser*innen sowie die semantischen Verknüpfungen lassen sich auch mit digitalen Methoden sammeln, quantifizieren und auswerten. Auch die rezeptionsorientierte Literaturwissenschaft hat das Projekt inzwischen als Forschungsgegenstand und Datenressource entdeckt (vgl. Hube et al., 2017; Chiu, 2022; Fischer et al. 2023b), da es viele enzyklopädische Beiträge und Metadaten zur Literatur und zum literarischen Leben versammelt, zu Autor*innen, literarischen Werken, Genres, Epochen und weiteren literaturgeschichtlich relevanten Kategorien. Jüngste Untersuchungen in diesem Bereich werten die inhaltliche Reichweite von Wikipedia etwa im Hinblick auf die Aufnahme und Darstellung von einzelnen Autor*innen (Blakesley, 2018; Bronner, 2018; Fischer et al., 2019; Blakesley, 2022b), Gruppen (Blakesley, 2020; Carrillo-Jara, 2023), literarischen Werken (Blakesley, 2022a), literarischen Figuren (Picard et al., 2023; Wojcik et al., 2023), Gattungen (Figlerowicz, 2023) und Kanones (Miller et al., 2016; van der Deijl et al., 2018; Wojcik et. al, 2019; Lippolis, 2023) aus. Unterschiede in der Verteilung enzyklopädischer Artikel zu bestimmten Themen in verschiedenen Sprachen geben Aufschluss über das unterschiedliche Interesse und die attestierte Relevanz dieser Themen für bestimmte Sprachgemeinschaften. Darüber hinaus können Veränderungen der Seitenaufrufe, der Überarbeitungen und der Beitragenden auch im Zeitverlauf analysiert werden, um das sich entwickelnde Interesse an und die Auseinandersetzung mit bestimmten literarischen Autor*innen und Werken zu verfolgen. Die datenanalytische Auswertung anhand solcher Wikipedia-Metriken ermöglicht es somit, die Auseinandersetzung mit Literatur in Wikipedia evaluierbar zu machen und Aussagen über literarische Kanonizität, Wertungspraktiken und Popularität im Kontext offener Enzyklopädieprojekte weiter zu diversifizieren. In kritischer Auseinandersetzung mit der Kanon- und Popularitätsforschung in globaler Perspektive wird unter anderem besonders deutlich, dass sich in der Wikipedia kein monolithischer Kanon zeigt, sondern viele, sich zudem dynamisch verändernde Kanones manifestieren. Im Zentrum des Hands-On-Workshops steht die Wikipedia-API, mit deren Funktionsweise die Teilnehmer*innen vertraut gemacht werden. Sukzessive werden Abfrageskripte in Form eines Jupyter Notebooks erarbeitet. Um eine benutzerfreundliche Programmierumgebung anzubieten und langwierige Installationsprozesse zu umgehen, wird für das Ausführen des Notebooks auf Google Colaboratory zurückgegriffen. Im Folgenden werden drei Typen von Abfragen kurz vorgestellt, die im Workshop jeweils im Hinblick auf eigene, von den Teilnehmer*innen mitgebrachte Fragestellungen und Forschungsinteressen modifiziert werden können. Der Programmiercode wurde um Annotationen ergänzt, die es auch Python-Anfänger*innen ermöglichen, über die bereitgestellten Formulare eigene Anfragen auszuführen. Eine mögliche Abfragestrategie ist die autor*innenzentrierte Abfrage, wie am Beispiel von Theodor Fontane demonstriert werden soll. Über die Wikipedia-API lässt sich herausfinden, wie viele der über 300 Sprachversionen der Wikipedia einen eigenen Artikel über den Autor bereithalten 'die Anzahl dieser Sitelinks gilt in der Forschung als ""a simple measure of canonicity"" (Kukkonen 2020). Diese können dann etwa diagrammatisch auf ihre Artikelgröße hin verglichen werden (Abb. 1). Auf diese Weise können ebenfalls die Anzahl der Überarbeitungen des Artikels, die Anzahl der Bearbeiter*innen, die Backlinks oder das Datum der Artikelerstellung untersucht werden. Diese Datenpunkte können sprachübergreifend Aufschluss über etwaige Konjunkturen der Fontane-Rezeption geben. Es lassen sich Rückschlüsse auf Anlässe ziehen, die eine Erweiterung der Informationsbasis in der Wikipedia ausgelöst haben könnten (Preise, Jubiläen, Übersetzungen, Schulstoff). Im Folgenden kann als digitale Entsprechung der literaturhistorischen Praxis ein Vergleichspool anderer Autor*innen zusammengestellt werden, um Fontane und sein literarisches Werk mit denen zeitgenössischer Kolleg*innen zu vergleichen. Fontanes Geburtsjahr ist 1819, eine mögliche Operationalisierung von Zeitgenossenschaft wäre etwa die Zusammenstellung anderer deutschsprachiger Autor*innen, die bis zu 20 Jahre vor und nach Fontane geboren wurden. Diese Festlegung ist natürlich kontingent und kann individuellen Informationsbedürfnissen angepasst werden. Die Visualisierung der Artikellänge und ein entsprechendes Ranking ergeben dann beispielsweise, dass Wilhelm Busch unter den zeitgenössischen literarisch schreibenden Autor*innen den aktuell umfangreichsten Artikel vorweisen kann, Fontane aber immerhin in den Top-5 rangiert (Abb. 2). Auch wenn sich diese Artikellängen, die oft über mehr als 20 Jahre gewachsen sind, größenordnungsmäßig nicht so schnell ändern, sind diese Werte in einer communitybetriebenen digitalen Enzyklopädie natürlich durch die Zeit variabel. Dass Wilhelm Wundt, als Psychologe und Philosoph ebenfalls einflussreicher Autor, bei diesem Ranking mit dem umfangreichsten Artikel ganz vorn steht, zeigt auch, dass es eines weiteren Schritts bedürfte, wollte man die Ergebnisliste auf vorderhand literarisch schreibende Zeitgenoss*innen eingrenzen. Außerdem zeigt sich ein systematischer Bias in der Artikellänge: Zu umfangreiche ""Werk""-Abschnitte werden oft in eigene Artikel für Werke ausgegliedert, während die Werke ""kleinerer"" Autor*innen oft Teil der Personenartikel bleiben. Die genaue Kenntnis der Gepflogenheiten innerhalb von Wikipedia erweist sich daher als Voraussetzung für eine sinnvolle Einschätzung der Quantifizierungen.  Neben den inhaltsbezogenen Informationen, die über Wikipedia direkt bezogen werden können, lassen sich über die internen Verweise zwischen Artikeln (Wikilinks) auch Netzwerkmetriken wie der PageRank berechnen (vgl. Thalhammer, 2016; Hube et al., 2017). Öhnlich wie dies Suchmaschinenalgorithmen zur Bestimmung der Rangfolge von Ergebnissen tun, können diese Zahlen dazu verwendet werden, den Wert und die Bedeutung eines Wikipedia-Artikels und seines Themas innerhalb des Hyperlink-Netzwerks der Plattform einzuschätzen. Der Wert, der einem Wikipedia-Artikel auf der Grundlage dieser Metriken zugewiesen wird, gibt Aufschluss über die relative Bedeutung und Konnektivität eines Themas innerhalb der vernetzten Informationen von Wikipedia. Die Abbildung der Literatur in Wikipedia und in darauf aufbauenden oder damit verwandten Projekten (DBpedia, Wikidata) wird in den Literaturwissenschaften zunehmend als möglicher Forschungsgegenstand wahrgenommen. Einen repräsentativen Überblick über solche Zugänge und Fragestellungen bietet der Sonderband des Der Workshop zielt auf Literaturwissenschaftler*innen, aber auch auf Kolleg*innen angrenzender Gebiete. Vorkenntnisse der Programmiersprache Python sowie zu Programmierschnittstellen (APIs) sind hilfreich, aber keine Voraussetzung zur Teilnahme am Workshop. Den Teilnehmer*innen soll das nötige Praxiswissen vermittelt werden, um eigenständig weiterzuarbeiten. max. 20 Personen Die Teilnehmer*innen benötigen einen eigenen Laptop Gefördert durch die Deutsche Forschungsgemeinschaft (DFG) im Rahmen der Exzellenzstrategie des Bundes und der Länder innerhalb des Exzellenzclusters Temporal Communities: Doing Literature in a Global Perspective 'EXC 2020 'Projekt-ID 390608380.",de,kollaborativ erstellt wikipedia bieten derzeit Million artikeln Sprachversion individuell lektüre Fließtexte jeweilig Versionshistorie Website bieten api Möglichkeit Analyse enzyklopädisch Inhalt Vielzahl Metadat sowohl einzeln Thema Bearbeitung Nutzung aktiv partizipierend Community semantisch Verknüpfung lassen digital Methode sammeln Quantifizieren auswerten rezeptionsorientiert Literaturwissenschaft Projekt inzwischen Forschungsgegenstand datenressource entdecken Hube et Chiu Fischer et enzyklopädisch beitrag metadaten Literatur literarisch Leben versammeln literarisch Werk genr epoch literaturgeschichtlich relevant Kategorie jung Untersuchung Bereich werten inhaltlich Reichweite Wikipedia Hinblick Aufnahme Darstellung einzeln Blakesley Bronner Fischer et Blakesley Gruppe Blakesley literarisch Werk Blakesley literarisch Figur picard et wojcik Et gattung figlerowicz kanon miller et van Deijl et wojcik et Al lippolis Unterschied Verteilung enzyklopädisch Artikel bestimmt Thema verschieden Sprache geben aufschluss unterschiedlich Interesse attestiert Relevanz Thema bestimmt Sprachgemeinschaft hinaus Veränderung Seitenaufrufe überarbeitung beitragender zeitverlauf analysieren entwickelnd Interesse Auseinandersetzung bestimmt literarisch Werk verfolgen datenanalytisch Auswertung anhand ermöglichen somit Auseinandersetzung Literatur wikipedia evaluierbar Aussage literarisch Kanonizität wertungspraktiken Popularität Kontext offen enzyklopädieprojeken diversifizieren kritisch Auseinandersetzung Popularitätsforschung global Perspektive deutlich Wikipedia monolithisch Kanon zeigen zudem dynamisch verändernd kanon manifestieren Zentrum stehen Funktionsweise vertraut sukzessive Abfrageskript Form Jupyter Notebooks erarbeiten benutzerfreundlich Programmierumgebung anbieten langwierig installationsprozesse umgehen Ausführen Notebook Google Colaboratory zurückgegriffen folgend Type Abfrag vorstellen Workshop jeweils Hinblick mitgebracht Fragestellung forschungsinteresse modifizieren Programmiercode Annotation ergänzen ermöglichen bereitgestellt formular Anfrage ausführen möglich Abfragestrategie Abfrage Theodor Fontane demonstrieren lässn herausfinden Sprachversione wikipedia Artikel Autor bereithalt Anzahl Sitelink gelten Forschung simple measure -- canonicity kukkon diagrammatisch Artikelgröße vergleichen abb Weise ebenfalls Anzahl überarbeitung Artikel Anzahl backlinks Datum Artikelerstellung untersuchen Datenpunkt sprachübergreifend aufschluss etwaig Konjunkturen geben lassen rückschluß Anlässe ziehen Erweiterung Informationsbasis wikipedia auslösen können Preis Jubiläen übersetzungen Schulstoff folgend digital Entsprechung literaturhistorisch Praxis Vergleichspool anderer zusammenstellen Fontane literarisch Werk zeitgenössisch vergleichen fontan Geburtsjahr möglich Operationalisierung Zeitgenossenschaft Zusammenstellung anderer deutschsprachig Fontan gebären Festlegung kontingent individuell informationsbedürfnisse angepasst Visualisierung Artikelläng entsprechend Ranking ergeben beispielsweise Wilhelm busch zeitgenössisch literarisch schreibend aktuell umfangreich Artikel vorweisen fontanen immerhin rangieren abb artikellängen wachsen größenordnungsmäßig schnell ändern Wert Communitybetriebenen digitalen enzyklopädie variabel Wilhelm wundt Psychologe Philosoph ebenfalls einflussreich Autor Ranking umfangreich Artikel vorn stehen zeigen Schritt bedürfen Ergebnislist Vorderhand literarisch schreibend eingrenzen zeigen systematisch Bias Artikellänge umfangreich Artikel Werk ausgliedern werk klein Personenartikel bleiben genau Kenntnis gepflogenheit innerhalb wikipedia erweisen Voraussetzung sinnvoll Einschätzung quantifizierung inhaltsbezogen Information wikipedia direkt beziehen lassen intern verweise artikeln Wikilinks netzwerkmetriken Pagerank berechnen Thalhammer Hube et öhnlich suchmaschinenalgorithmen Bestimmung Rangfolge Ergebnis Zahl verwenden Wert Bedeutung Thema innerhalb Plattform einschätzen Wert Grundlage Metrike zugewiesen aufschluss relativ Bedeutung Konnektivität Thema innerhalb vernetzt Information wikipedia Abbildung Literatur wikipedia aufbauend verwandt Projekt Dbpedia Wikidata literaturwissenschaften zunehmend möglich Forschungsgegenstand wahrnehmen repräsentativ Überblick zugäng Fragestellung bieten Sonderband Workshop zielen Angrenzender Gebiet Vorkenntnisse Programmiersprache python programmierschnittstell apis hilfreich Voraussetzung Teilnahme Workshop nötig praxiswissen vermitteln eigenständig weiterzuarbeiten Person benötigen Laptop fördern deutsch Forschungsgemeinschaft dfg Rahmen Exzellenzstrategie Bund Land innerhalb Exzellenzcluster Temporal Communities doing literature global perspectiv exc,"[('wikipedia', 0.3805591591286936), ('artikel', 0.21048174667953945), ('blakesley', 0.2091565436271564), ('literarisch', 0.135940766905967), ('enzyklopädisch', 0.13255749033045636), ('thema', 0.12898043759151928), ('aufschluss', 0.12772035531437312), ('et', 0.12435328629887195), ('werk', 0.11866448999557998), ('kanon', 0.11416774773860809)]"
2024,DHd2024,JANNIDIS_Fotis_Bedeutung_in_Zeiten_gro_er_Sprachmodelle.xml,Bedeutung in Zeiten großer Sprachmodelle,"Tessa Gengnagel (Cologne Center for eHumanities, Universität zu Köln, Deutschland); Fotis Jannidis (Julius-Maximilians-Universität Würzburg); Rabea Kleymann (Technische Universität Chemnitz, Deutschland); Julian Schröter (Ludwig-Maximilians-Universität München, Deutschland); Heike Zinsmeister (Universität Hamburg, Deutschland)","Sprachmodelle, Bedeutung, Theorie der Digital Humanities","Sprachmodelle, Bedeutung, Theorie der Digital Humanities","Die Performanz künstlicher Intelligenz ist, nicht zuletzt durch die großen Sprachmodelle (LLMs) in den letzten Jahren rasant angestiegen. Das hat zu einer intensiven Diskussion um die Definition anthropologisch relevanter Konzepte geführt; so wurde etwa die Diskussion des Begriffs Insbesondere die Entwicklung großer Sprachmodelle hat zuletzt eine Auseinandersetzung mit menschlicher und maschineller Sinnbildung provoziert (Kirschenbaum 2023), weswegen wir uns auf sprachliche Bedeutung, also die Bedeutung von Worten, Sätzen und Texten, konzentrieren werden. Das Panel wird daher vier Perspektiven zusammenführen:  Nach einer gemeinsamen einführenden Einleitung werden alle Panelist:innen ihre oben skizzierten Perspektiven in 5–7-minütigen Statements erläutern. Auf diese Impulse wird eine 10-minütige Phase folgen, in der die Panelist:innen auf die Statements der anderen Diskussionsteilnehmer:innen reagieren können. Anschließend wird die Diskussion für das Publikum geöffnet, um eine engagierte Debatte zu ermöglichen. Je nach Publikumspartizipation soll so außerdem der Raum geschaffen werden, weitere relevante Aspekte einzubringen, so etwa aus dem Bereich der Leseforschung und Kognitionspsychologie. Das Panel verspricht nicht nur menschliche und maschinelle Bedeutungsverfahren in den DH zu explorieren, sondern stellt auch einen ersten Versuch dar, ein geisteswissenschaftliches Vokabular für die Beschreibung und Evaluierung von intelligenten Systemen zu entwickeln. Insbesondere die Konjunktur des Bedeutungsbegriffes in den Datenwissenschaften (vgl. Donoho 2017, 746) macht eine systematische Auseinandersetzung mit der geisteswissenschaftlichen Begriffstradition erforderlich, um die Rolle der Geisteswissenschaften zukünftig zu vermessen. Vor dem Hintergrund der projektbasierten Arbeit in den DH stellt sich außerdem die Frage, wie sich diese in ihren Aufgaben und Zielen durch die Fortschritte in der generativen KI sowohl unmittelbar als auch langfristig verändern wird. Hierzu wird das Panel unter Einbeziehung des Konferenzthemas ""Quo vadis?"" wichtige Impulse in einer Zeit des Umbruchs liefern.",de,performanz künstlich Intelligenz zuletzt Sprachmodelle llms letzter rasant ansteigen intensiv Diskussion Definition anthropologisch relevant Konzept führen Diskussion Begriff insbesondere Entwicklung Sprachmodelle zuletzt Auseinandersetzung menschlich Maschineller Sinnbildung provozieren Kirschenbaum weswegen sprachlich Bedeutung Bedeutung Wort Satz Text konzentrieren Panel Perspektive zusammenführen gemeinsam einführend Einleitung panelisen innen skizziert Perspektive Statement erläutern Impuls Phase folgen panelisen innen Statements Diskussionsteilnehmer innen reagieren anschließend Diskussion Publikum öffnen engagiert Debatte ermöglichen Publikumspartizipation Raum schaffen relevant Aspekt einzubringen Bereich Leseforschung Kognitionspsychologie Panel versprechen menschlich maschinell bedeutungsverfahren dh explorieren stellen Versuch dar geisteswissenschaftlich Vokabular Beschreibung Evaluierung intelligenten systemen entwickeln insbesondere Konjunktur Bedeutungsbegriff Datenwissenschaft donoho systematisch Auseinandersetzung geisteswissenschaftlich Begriffstradition erforderlich Rolle geisteswissenschaften zukünftig vermessen Hintergrund projektbasiert Arbeit dh stellen Frage Aufgabe Ziel Fortschritt generativ Ki sowohl unmittelbar langfristig verändern hierzu Panel Einbeziehung konferenzthemas -- vadis wichtig Impuls Umbruch liefern,"[('panelisen', 0.23135611131775405), ('panel', 0.2148980641515687), ('impuls', 0.17711183717549595), ('innen', 0.17578011021096024), ('diskussion', 0.17269026739927737), ('sprachmodelle', 0.16838045925891348), ('auseinandersetzung', 0.15521464631439888), ('menschlich', 0.14326537610104578), ('dh', 0.12809250924326984), ('zuletzt', 0.12410985698128847)]"
2024,DHd2024,20240108_HATZEL_Hans_Ole_Narrativit_t_visualisieren___Eine_Rezeptions.xml,Narrativität visualisieren - Eine Rezeptionsstudie zur Evaluation der heuristischen Qualität von Narrativitätsgraphen,"Hans Ole Hatzel (Universität Hamburg, Deutschland); Haimo Stiemer (Technische Unversität Darmstadt, Deutschland); Chris Biemann (Universität Hamburg, Deutschland); Evelyn Gius (Technische Unversität Darmstadt, Deutschland)","graphen, studie, narrativität, interpretierbarkeit","Programmierung, Modellierung, Annotieren, Interaktion, Literatur, Metadaten","Die visuelle Repräsentation von literarischen Phänomenen ist ein etablierter Ansatz in den Computational Literary Studies, um die aus Texten extrahierten Daten bzw. abgeleiteten Strukturen zu explorieren und zu interpretieren (cf. Baillot u. Lassner 2022; Krämer 2014).¬† Vor diesem Hintergrund soll das vorgeschlagene Poster die Ergebnisse einer Rezeptionsstudie präsentieren, mit der die heuristische Qualität von den im Projekt EvENT (""Evaluating Events in Narrative Theory"") bislang generierten Narrativitätsgraphen überprüft wurde. Ziel der Studie war es, die Erkennbarkeit der den Graphen zugrunde liegenden Texte zu untersuchen, um hieraus Rückschlüsse für die Weiterentwicklung des EvENT-Ansatzes wie auch die Anwendbarkeit der Graphen für die literaturwissenschaftlich-hermeneutische Praxis zu ziehen. Dabei stellen die Graphen das Ausmaß der Narrativität (auf der y-Achse) über den Textverlauf (auf der x-Achse) dar. Die Narrativitätsgraphen basieren auf den vier, im EvENT-Projekt auf der Grundlage des narratologischen Forschungsstands konzipierten Ereigniskategorien (Zustandsveränderungen, Prozesseereignisse, statische Ereignisse und Nicht-Ereignisse) und der ihnen zugewiesenen Narrativitätsgrade (cf. Vauth u. Gius 2021). Die über die automatisierte Annotation von Verbalphrasen auf der Textoberfläche detektierten Ereignisse¬† (cf. Hatzel 2022) werden im EvENT-Projekt verwendet, um die Narrativität von Texten über den Textverlauf als Narrativitätsgraphen abzubilden und damit auch die Handlung ihrer Geschichten zu modellieren (cf. Vauth et al. 2021; Gius u. Vauth 2022). Eine Annahme bei dieser Modellierung¬† war, dass die Graphen ebenso die ""Erzählwürdigkeit"" der Ereignisse in den Texten indizieren und sich damit dem in der Narratologie als Event II diskutierten Phänomen annähern (cf. Hühn 2009, S. 80). Event II stellt einen Ereignistyp mit zusätzlichen Merkmalen im interpretativen Kontext dar, wie z. B. Relevanz oder Unerwartetheit, geht also über die vier oben genannten, grundlegenden Ereigniskategorien hinaus.  Die Studie wurde als Webanwendung konzipiert. React und Plotly wurden im Front-End verwendet und die Antworten an ein fastAPI-basiertes Back-End übermittelt, welches diese in einer PostgreSQL-Datenbank protokollierte. Auf diese Weise wurde eine schnelle Iteration des Studiendesigns sowie die Teilnahme auf allen wichtigen Plattformen ermöglicht. Die 19 Teilnehmer:innen der Studie (aktuelle und ehemalige Studierende der Germanistik), wurden gebeten, einem literarischen Text den richtigen Narrativitätsgraphen zuzuordnen, wobei für jeden Text vier Graphen als Antwortmöglichkeiten ausgegeben wurden. Die Textdarbietung erfolgte auf drei unterschiedlichen Abstraktionsniveaus bzw. in drei Phasen. In der ersten Phase wurden die Teilnehmenden gebeten, zwei kurze deutschsprachige literarische Texte zu lesen und diese jeweils dem richtigen Graphen zuzuordnen. In der zweiten Phase erfolgte die Auswahl der Graphen auf der Grundlage von Zusammenfassungen, in denen die wesentlichen Ereignisse des jeweiligen Textes in chronologischer Reihenfolge präsentiert werden. In der dritten Phase wurden den Teilnehmenden nur die Titel der Texte angeboten, denen ein entsprechender Graph zugewiesen werden sollte. Bei diesen Texten handelt es sich um kanonische Texte (z. B. Hänsel und Gretel von den Brüdern Grimm), deren allgemeine Bekanntheit vorausgesetzt werden konnte. Zu den auf dem Poster zu präsentierenden Resultaten der Studie gehört, dass der Anteil der korrekten Zuordnungen der Graphen zu den Texten durch die Teilnehmenden bei 25,56 % und damit nicht signifikant über dem Zufallsprinzip liegt. Allerdings gibt es eine positive Korrelation bei Teilnehmenden mit EvENT-Erfahrung (durch Mitarbeit im Projekt, Kenntnis der Annotationsguidelines), welche die korrekten Texte in 47.5% der Fälle ausgewählt haben, ein Wert der mittels Binomialtest als statistisch signifikant identifiziert wurde (p < 0.01). Wir konnten zudem mit statistischer Signifikanz zeigen, dass die Zeit, die sich Teilnehmende zur Beantwortung einer Frage nahmen, mit der Quote der richtigen Antworten korreliert. Auch waren lange Texte für unsere Annotator:innen statistisch signifikant schwieriger zu identifizieren als kurze (hier ist anzumerken, dass alle vier Optionen für eine Identifikation stets so gewählt waren, dass sie ähnliche Längen aufwiesen). Offenkundig ist die Identifikation der textzugehörigen Graphen voraussetzungsreich. Im Anschluss an die Zuordnungsaufgaben beantworteten die Teilnehmenden zwei offene Fragen nach ihren Entscheidungsgrundlagen im Verlauf der Studie. Die Antworten legen nahe, dass die Narrativitätsverläufe der Graphen als Repräsentationen von Event II-Vorkommen und die Amplitudenausschläge damit als Verweise auf besonders handlungsrelevant erscheinende Textpassagen interpretiert wurden. Als entscheidungsrelevant wurden von den Teilnehmenden außerdem nicht nur die Peaks der Graphen, sondern auch deren Anzahl sowie Anfang und Ende eines Narrativitätsverlaufs ausgewiesen.",de,visuell Repräsentation literarisch Phänomen etabliert Ansatz Computational literary Studie text extrahiert daten abgeleitet Struktur explorieren interpretieren cf Baillot Lassner kräm Hintergrund vorgeschlagen Poster Ergebnis Rezeptionsstudie präsentieren heuristisch Qualität Projekt Event Evaluating Event Narrative Theory bislang generiert Narrativitätsgraph überprüfen Ziel Studie Erkennbarkeit graph zugrunde liegend Text untersuchen hieraus rückschluß Weiterentwicklung Anwendbarkeit Graph Praxis ziehen stellen graphen Ausmaß Narrativität Textverlauf dar Narrativitätsgraph Basiere Grundlage narratologisch Forschungsstand konzipiert Ereigniskategorien zustandsveränderung Prozesseereignisse statisch Ereignis zugewiesen Narrativitätsgrade cf vauth Gius automatisiert Annotation Verbalphrasen Textoberfläche detektierten cf hatzel verwenden Narrativität Text textverlauf narrativitätsgraph abzubilden Handlung Geschicht Modelliere cf vauth et Gius Vauth Annahme graphen Erzählwürdigkeit Ereignis Text indizier Narratologie Event ii diskutiert Phänom Annäher cf Hühn Event ii stellen ereignistyp zusätzlich Merkmale interpretativ Kontext dar Relevanz unerwartetheit genannt grundlegend Ereigniskategorie hinaus Studie Webanwendung konzipieren react plotly verwenden Antwort übermitteln protokollieren Weise schnell Iteration Studiendesigns Teilnahme wichtig Plattform ermöglichen Teilnehmer innen Studie aktuell ehemalig Studierende Germanistik bitten literarisch Text richtig Narrativitätsgraph Zuzuordn wobei Text graphen Antwortmöglichkeit ausgeben Textdarbietung erfolgen unterschiedlich Abstraktionsniveaus Phase Phase Teilnehmende bitten kurz deutschsprachig literarisch Text lesen jeweils richtig Graphen Zuzuordn Phase erfolgen Auswahl Graph Grundlage zusammenfassungen wesentlich Ereignis jeweilig Text chronologisch Reihenfolge präsentieren Phase Teilnehmend Titel Text anbieten entsprechend Graph zuweisen Text handeln kanonisch Text Hänsel Gretel brüdern Grimm allgemein Bekanntheit voraussetzen Poster präsentierend Resultat Studie gehören Anteil korrekt Zuordnung Graph Text Teilnehmende signifikant Zufallsprinzip liegen positiv Korrelation Teilnehmend Mitarbeit Projekt Kenntnis annotationsguidelines korrekt Text Fall auswählen Wert Mittels binomialtest statistisch signifikant identifizieren p zudem statistisch Signifikanz zeigen Teilnehmend Beantwortung Frage nehmen Quote richtig Antwort korrelieren Text Annotator innen statistisch signifikant schwierig identifizieren kurz anzumerken Option Identifikation stets wählen ähnlich läng aufweisen offenkundig Identifikation textzugehörig graphen voraussetzungsreich Anschluss Zuordnungsaufgabe beantworten teilnehmend offen Frage Entscheidungsgrundlag Verlauf Studie Antwort legen nahe Narrativitätsverläuf graphen Repräsentatione Event Amplitudenausschlag verweise handlungsrelevant erscheinend Textpassage interpretieren entscheidungsrelevanen Teilnehmend Peaks graph Anzahl Anfang narrativitätsverlauf ausweisen,"[('graphen', 0.2767162513676886), ('graph', 0.2489834275150285), ('narrativitätsgraph', 0.24741581605540838), ('event', 0.2367576842808324), ('cf', 0.22010051180856036), ('teilnehmend', 0.20748618959585707), ('text', 0.17294517882490562), ('studie', 0.16196892456891482), ('vauth', 0.15680513301477458), ('richtig', 0.1510831809944361)]"
2024,DHd2024,SCHMIDT_Thomas_Fanfictions___Literatur_von_Frauen__ber_M_nne.xml,Fanfictions 'Literatur von Frauen über Männer? Korpusbasierte Analyse der Geschlechterrollen bei Texten und Autor*innen deutschsprachiger Fanfictions,"Thomas Schmidt (Lehrstuhl für Medieninformatik, Universität Regensburg, Deutschland); Jonathan Sasse (Lehrstuhl für Medieninformatik, Universität Regensburg, Deutschland); Christian Wolff (Lehrstuhl für Medieninformatik, Universität Regensburg, Deutschland)","Fanfiction, Gender, Fan Studies, Internet Studies, Social Media, NER","Sammlung, Inhaltsanalyse, Literatur, Metadaten, benannte Entitäten (named entities), Text","Fanfictions sind literarische Texte, erstellt von Fans und Hobby-Autor*innen, die Figuren und Geschichten aus bereits bestehenden Medien wie Filmen oder Büchern nutzen, um neue Geschichten über diese zu schreiben und auf Online-Plattformen zu veröffentlichen (Dym et al., 2018). Dieses spezielle literarische Genre wurde in den letzten Dekaden mit dem Aufstieg des Internets immer populärer und deswegen auch vielseitig in den Geistes- und Kulturwissenschaften in Hinblick auf Geschichte und kulturellen Einfluss untersucht (siehe Hellekson und Busse, 2006; Jamison, 2013). Die Verfügbarkeit von großen narrativen Textmengen mit detaillierten Metadaten macht Fanfictions auch zu einer beliebten Quelle für verschiedene Aufgaben im Geschlechtsspezifische Fragestellungen spielen eine wichtige Rolle im Kontext von Fanfictions. Bisherige Analysen für mehrheitlich englischsprachige Texte deuten auf eine erhöhte weibliche Autorschaft in diesem Genre hin (Barnes, 2015; Duggan, 2020). Motivation und Bedeutung für die Popularität von Slash-Fanfictions (Fanfictions mit Fokus auf homo-romantischen Beziehungen zwischen Männern) und damit die Dominanz von männlichen und Vernachlässigung weiblicher Figuren wurden vielfach anhand begrenzter Mengen von Texten diskutiert (Jung, 2002; Tosenberger, 2008; Rossdal, 2015; Busse und Lothian, 2017). Dem entgegen argumentiert andere Forschung mit ähnlichem methodischem Zugang (""close reading"", vgl. Busse, 2009; Leow, 2011; Handley, 2012; Duggan, 2017; 2020; 2022), dass die Autorschaft wesentlich diverser ist und weibliche Charaktere eine wichtige und nicht-stereotype Rolle spielen. Derartige Analysen werden computergestützt in größeren Rahmen auch von den Untersuchungen von Milli und Bamman (2016) getragen, während Fast et al. (2016) eine stereotype und negative Repräsentation von weiblichen Figuren identifizieren. Wir präsentieren im Folgenden die Ergebnisse eines Projekts, dass die bisher vorliegenden computergestützten Analysen mit Fokus auf den deutschsprachigen Bereich weiterführt. Unsere Forschungsbeiträge sind (1) die Akquise und Bereitstellung eines strukturierten Korpus speziell für die Analyse deutschsprachiger Texte und Communites, (2) allgemeine Korpus- und Metadatenanalysen und (3) erste Analysen zur Verteilung von Geschlechtern bezüglich Figuren und Autor*innen in diesem Korpus. Als Plattformen für die Korpusakquise wurden Fanfiktion.de (FF.de) Die Inhalte beider Plattformen (Texte, Metadaten, Kommentare/Reviews, Nutzer*innen-Profile) wurden mittels Tabelle 1 illustriert die allgemeinen Statistiken des Korpus in Summe und aufgeteilt nach Plattform. Mit 394.848 einzelnen Fanfictions liefert FF.de im Vergleich zu AO3 (18.075) deutlich mehr deutschsprachigen Inhalt. In beiden Plattformen muss man sich mittels eindeutigem Nutzernamen anmelden, um Texte zu posten oder damit in Kommentaren und Bewertungen zu interagieren 'hierauf beziehen sich die Nutzerstatistiken. Als Reviews bezeichnen wir im Folgenden Kommentare. Token-Statistiken wurden mittels Als Fandom wird die mediale Referenz, also das fiktionale Universum bzw. Thematik bezeichnet, in dem eine Fanfiction spielt. Tabelle 2 und 3 illustrieren die Top 10 Fandoms für FF.de und AO3 respektive. Die grundsätzlichen Fandom-Verteilungen verhalten sich konform zu Analysen auf größeren englischsprachigen Plattformen mit Fandoms wie Harry Potter, Marvel und Supernatural als besonders häufigen Fandoms. Im Fall von FF.de wird die besondere historische Bedeutung von Anime (Naruto, One Piece) für die deutsche Fanfiction-Community deutlich (siehe auch Cuntz-Leng und Meintzinger, 2015). Für AO3 kristallisieren sich spezielle nationale Besonderheiten heraus wie die Häufigkeit von Tatort-, Die drei ??? Ein weiteres wichtiges Metadatum im Kontext dieser Forschung sind Beziehungstypen, die für beide Plattformen äquivalent vorliegen. Dadurch wird markiert, ob eine romantische/erotische Beziehung zwischen Figuren eine wichtige Rolle spielt und welcher Geschlechternatur diese ist. Tabelle 4 zeigt die kumulierte Verteilung für beide Plattformen. Dabei ist zu beachten, dass eine Fanfiction im Fall von AO3 auch mehrere Angaben bezüglich Beziehungstypen haben kann. Obwohl der Großteil der Geschichten als Generisch (Generic) (66,5%) gekennzeichnet ist und damit definitionsgemäß keinen spezifischen Beziehungstyp fokussiert, spielen Slash-Fanfictions (M/M) eine bedeutende Rolle, da sie den größten Teil der verbleibenden Fanfictions ausmachen. Im Vergleich dazu sind Beziehungen, welche weibliche Figuren beinhalten, eher selten. Zur vertieften Analyse wurde eine Geschlechtsklassifikation genannter Personennamen in den Fanfictions durchgeführt. Dazu wurde zunächst eine Nach ersten Experimenten mit vortrainierten Modellen zur geschlechtsbasierten Namenserkennung Insbesondere die Liste von babynames.com hat einen besonderen Mehrwert da hier fiktionale Namen aus Kunst und Kultur enthalten sind. Für Analysen, die das Autor*innen-Geschlecht verwenden, wird das Korpus auf den FF.de-Anteil beschränkt, da nur in diesem Nutzer*innen über ihr Profil freiwillige Geschlechtsangaben machen können. Eine Geschlechtserkennung auf Nutzernamen ist aufgrund ihrer Beliebigkeit and Abstraktheit in diesem Kontext nicht sinnvoll. Tabelle 5 zeigt das Verhältnis von männlichen zu weiblichen Namen bezüglich der fiktionalen Charaktere in den Fanfictions auf. Diejenigen Namen, die keine Erkennungssicherheit von mindestens 80% erreicht haben, wurden als unsicher markiert. Insgesamt zeigt sich, dass die Nennungen von männlichen Eigennamen überwiegen, im Schnitt in einem Verhältnis von 61% zu 33% mit ca. 6% Namen, die nicht eindeutig klassifiziert werden konnten. In Tabelle 6 wird die Verteilung der freiwilligen Selbstangaben von Nutzer*innen auf FF.de aufgezeigt, wobei eine Gesamtübersicht sowie eine Unterteilung nach Autor*innen und Reviewer*innen gegeben ist. Unter letzteren werden die Geschlechtsangaben der Poster*innen von Reviews/Kommentaren verstanden. Jede Autor*in und jede Reviewer*in werden dabei einmal gezählt, unabhängig von der Zahl der veröffentlichten Geschichten oder Reviews. Es ist bei der Interpretation der Zahlen zu beachten, dass viele Autor*innen gleichzeitig auch als Reviewer*innen aktiv sind und diese beiden Kategorien Duplikate enthalten, die Gesamt-Information bezieht sich aber auf die tatsächliche Gesamtzahl aller eindeutig differenzierbaren Nutzer*innen. Ein hoher Teil der Nutzer*innen gibt kein Geschlecht an (29%). Bezogen auf Nutzer*innen, die ein Geschlecht angeben, zeigt sich jedoch eine deutliche Dominanz von weiblichen Personen. Abstrahiert man von den Nicht-Angaben (N/A), ist das Verhältnis sogar ca. 92% zu 8%. Es gibt keinen wesentlichen Unterschied beim Vergleich von Autor*innen und Reviewer*innen. Die Plattform wird basierend auf Selbstauskunft also primär von weiblichen Personen genutzt. Die Altersinformationen dienen lediglich der demographischen Vertiefung und sind nicht Fokus dieses Beitrags. Sie zeigen aber eine durchschnittlich eher junge Nutzer*innen-Gruppe auf (etwa 27 Jahre). In Tabelle 7 werden die beiden Analysen in ein Verhältnis zueinander gesetzt und der Anteil weiblicher und männlicher Figurennamen in den einzelnen Autor*innen-Geschlechtsgruppen untersucht. Es zeigt sich kein wesentlicher Unterschied im Vergleich zu männlichen und weiblichen Autor*innen in der Nutzung von weiblichen oder männlichen Figurennamen. Weibliche Autor*innen nutzen männliche Figuren in einem Verhältnis von 64% zu 36% weiblichen Figuren. Der Anteil von weiblichen Figuren verringert sich lediglich um 1% für männliche Autoren. In diesem Beitrag wurden die Ergebnisse der Korpusakquise einer Sammlung von deutschsprachigen Fanfictions aufgezeigt. Es ist zu beachten, dass wir dabei einige wesentliche Bestandteile noch nicht vertieft präsentieren konnten, wie z.B. weitere Metadaten und Review-Analysen. Das Korpus ist eine relevante Ressource für den Bereich der Dennoch konnten bereits durch allgemeine Metadatenanalysen nationale Besonderheiten eines deutschsprachigen Fanfiction-Korpus wie z.B. die Bedeutung von Fandoms wie Tatort oder Die drei ??? in AO3 sowie die Bedeutung von Anime in FF.de (Cuntz-Leng und Meintzinger, 2015) herausgearbeitet werden. Dies verdeutlicht die Notwendigkeit der Analyse nicht-englischer Texte nicht nur für die lokalen Wissenschafts-Communities, sondern auch für ein angemessenes Verständnis des Phänomens an sich. Im Kontext der geschlechtsspezifischen Fragestellungen konnten Analysen und Behauptungen, wonach Frauen Fanfictions nutzen, um Geschichten über unterrepräsentierte weibliche Figuren zu schreiben (Busse, 2009; Leow, 2011; Handley, 2012; Duggan, 2017; 2020; 2022; Milli und Bamman, 2016) nicht bestätigt werden. Im Gegensatz bestätigen sich bisherige Annahmen (Jung, 2002; Busse und Lothian, 2017; Tosenberger, 2008; Rossdal, 2015; Fast et al., 2016), die Fanfictions als Literatur von Frauen primär über männliche Figuren mit Fokus auf homo-romantischen Beziehungen verstehen auch für die deutschsprachige Fanfiction-Community. Auch eine Reduktion auf das Harry Potter-Fandom analog zu Duggan (2017; 2020; 2022) zeigt dieselben Verhältnisse auf. Es sei hier jedoch auch zu beachten, dass dieses Phänomen auch als Spiegelung der allgemeinen √úberrepräsentation von Männern in kulturellen Medien betrachtet werden kann was jedoch bisher in mehrheitlich qualitativen Studien untersucht wurde (Collins, 2011; Bretthauer et al., 2007; Garcia et al., 2015; Jia et al., 2015; Neville und Anastasio, 2019; Schmidt et al., 2020b). Wir halten es auch für eine sehr spannende Idee, die hier vorliegende binäre Geschlechtsauffassung durch weitere Geschlechtsgruppen wie non-binär oder androgyn zu erweitern, wie dies teilweise schon in Computer Vision-Projekten in den DH gemacht wurde (Schmidt et al. 2021a; Schmidt und Kurek, 2022). Annotation und Akquise von non-binären Namen wäre hier für weitere Studien notwendig. Insgesamt ist mehr Forschung die",de,Fanfictions literarisch Text erstellen Fan Figur Geschichte bestehend Medium Film Büchern nutzen Geschichte schreiben veröffentlichen dym et speziell literarisch Genre letzter Dekad Aufstieg internets populär vielseitig Kulturwissenschaft Hinblick Geschichte kulturell einfluss untersuchen sehen Hellekson Bus Jamison Verfügbarkeit narrativ Textmeng Detailliert metadaten Fanfiction beliebt Quelle verschieden Aufgabe geschlechtsspezifisch Fragestellung spielen wichtig Rolle Kontext Fanfictions bisherig Analyse mehrheitlich englischsprachig Text deuten erhöht weiblich Autorschaft Genre barn Duggan Motivation Bedeutung Popularität Fanfictions Fokus Beziehung Männer Dominanz männlich Vernachlässigung weiblich Figur vielfach anhand Begrenzter menge Text diskutieren jung Tosenberger Rossdal busse lothian entgegen argumentieren Forschung ähnlich methodisch Zugang clos Reading Bus leow Handley Duggan Autorschaft wesentlich diverser weiblich charaktere wichtig Rolle spielen derartig Analyse computergestützt groß Rahmen Untersuchung Milli Bamman tragen fast et Stereotype negativ Repräsentation weiblich Figur identifizieren präsentieren folgend Ergebnis Projekt vorliegend computergestützt Analyse Fokus deutschsprachig Bereich weiterführen forschungsbeiträge Akquis Bereitstellung strukturiert Korpus speziell Analyse deutschsprachig Text Communite allgemein Metadatenanalyse Analyse Verteilung geschlechtern bezüglich Figur Korpus Plattform korpusakquise Inhalt beide plattformen Text Metadat Kommentar reviews mittels Tabelle illustrieren Statistik korpus Summe aufgeteilen Plattform einzeln Fanfiction liefern Vergleich deutlich deutschsprachigen Inhalt plattformen mittels eindeutig nutzernam anmelden Text Posten kommentaren Bewertung interagier hierauf beziehen nutzerstatistiken Reviews bezeichnen folgend Kommentar mittels fandom medial Referenz fiktional universum Thematik bezeichnen Fanfiction spielen Tabell illustrieren top Fandom respektive grundsätzlich verhaln konform Analyse groß Englischsprachig plattformen Fandoms harry Potter Marvel Supernatural häufig Fandoms Fall besonderer historisch Bedeutung Anime naruto one piece deutsch deutlich sehen Meintzinger kristallisieren speziell national Besonderheit heraus Häufigkeit wichtig Metadatum Kontext Forschung beziehungstypen Plattforme äquivalent vorliegen markieren romantisch erotisch Beziehung Figur wichtig Rolle spielen Geschlechternatur Tabell zeigen kumuliert Verteilung plattformen beachten Fanfiction Fall mehrere Angabe bezüglich beziehungstypen obwohl Großteil geschichte generisch Generic kennzeichnen definitionsgemäß spezifisch Beziehungstyp fokussieren spielen m m bedeutend Rolle groß verbleibend Fanfiction ausmachen Vergleich Beziehung weiblich Figur beinhalen eher selten vertieft Analyse Geschlechtsklassifikation genannt Personennamen Fanfiction durchführen Experiment vortrainiert modellen geschlechtsbasiert Namenserkennung insbesondere Liste besonderer Mehrwert fiktional Name Kunst Kultur enthalten analyse verwenden korpus beschränken Profil freiwillig geschlechtsangaben Geschlechtserkennung nutzernamer aufgrund Beliebigkeit And Abstraktheit Kontext sinnvoll Tabell zeigen Verhältnis männlichen weiblich Name bezüglich fiktional Charaktere Fanfiction Name Erkennungssicherheit mindestens erreichen unsicher markieren insgesamt zeigen Nennung männlich Eigennam überwiegen Schnitt Verhältnis namen eindeutig klassifizieren Tabelle Verteilung freiwillig Selbstangaben aufzeigen wobei Gesamtübersicht Unterteilung geben letzter geschlechtsangaben reviews kommentaren verstehen zählen unabhängig Zahl veröffentlicht Geschicht reviews Interpretation Zahl beachten gleichzeitig aktiv Kategori Duplikate enthalten beziehen tatsächlich Gesamtzahl eindeutig differenzierbaer hoch Geschlecht beziehen Geschlecht angeben zeigen deutlich Dominanz weiblich Person abstrahieren n Verhältnis sogar wesentlich Unterschied Vergleich Plattform basierend Selbstauskunft primär weiblich Person nutzen Altersinformatione dienen lediglich demographisch Vertiefung Fokus Beitrag zeigen durchschnittlich eher jung Tabelle analysen Verhältnis zueinander setzen Anteil weiblich männlich Figurennam einzeln untersuchen zeigen wesentlich Unterschied Vergleich männlich weiblich Nutzung weiblich männlich Figurenname weiblich nutzen männlich Figur Verhältnis weiblich Figur Anteil weiblich Figur verringern lediglich männlich Autor Beitrag Ergebnis Korpusakquise Sammlung deutschsprachig Fanfiction aufzeigen beachten wesentlich Bestandteil vertiefen präsentieren metadaten korpus relevant Ressource Bereich dennoch allgemein Metadatenanalyse national Besonderheit deutschsprachig Bedeutung Fandoms Tatort Bedeutung Anime Meintzinger herausarbeiten verdeutlichen Notwendigkeit Analyse Text lokal angemessen Verständnis Phänomen Kontext geschlechtsspezifisch Fragestellung analysen Behauptung wonach Frau Fanfiction nutzen Geschicht unterrepräsentiert weiblich Figur schreiben Bus leow Handley Duggan Milli Bamman bestätigen Gegensatz bestätigen bisherig annahmen jung busse lothian Tosenberger Rossdal fast et Fanfiction Literatur Frau primär männlich Figur Fokus Beziehung verstehen deutschsprachig Reduktion Harry analog Duggan zeigen Verhältnis beachten Phänom Spiegelung mann kulturell Medium betrachten mehrheitlich qualitativ Studie untersuchen Collins bretthauer et Garcia et jia et Neville anastasio schmidt et halten spannend Idee vorliegend binär Geschlechtsauffassung Geschlechtsgruppe androgyn erweitern teilweise Computer dh Schmidt et Schmidt Kurek Annotation Akquise Name Studie notwendig insgesamt Forschung,"[('weiblich', 0.36036947758652393), ('fanfiction', 0.342486522134353), ('männlich', 0.19503522193014813), ('figur', 0.16912256794096453), ('duggan', 0.1551874025504945), ('reviews', 0.1551874025504945), ('plattformen', 0.14454530045453762), ('verhältnis', 0.12009631276156142), ('fanfictions', 0.11639055191287088), ('bus', 0.11639055191287088)]"
2024,DHd2024,PICHLER_Axel__LLMs_for_everything___Potentiale_und_Probleme_.xml,"""LLMs for everything?"" Potentiale und Probleme der Anwendung von In-Context-Learning für die Computational Literary Studies","Axel Pichler (Universität Stuttgart, Deutschland); Nils Reiter (Universität zu Köln, Deutschland)","Large Language Models, In-Context-Learning, Computational Literary Studies","Modellierung, Annotieren, Theoretisierung","Große Sprachmodelle, sogenannte Large Language Models (LLMs), haben das Natural Language Processing (NLP) seit dem Aufkommen der Transformer-Architektur in den letzten Jahren revolutioniert. Spätestens seit der Veröffentlichung von ChatGPT ist das Potential dieser Modelle auch der nicht akademischen Öffentlichkeit bekannt. Ein noch nicht vollständig erklärtes Merkmal dieser Modelle ist, dass sie mit zunehmender Größe 'als Schwellenwert werden hier um die 10 Milliarden Parameter genannt, 'auch Problemlösungskompetenzen entwickeln, für die sie nicht trainiert wurden (Wei et. al. 2022). Zu diesen sogenannten ""Emergent Abilities"" zählt auch eine Trainingsmethode, bei der es sich im strengen Sinne gar nicht um eine ""klassische"" Form des Fine-Tunings handelt, da dabei keine Anpassungen der Gewichte durchgeführt werden: das In-Context-Learning (ICL, Dong et al. 2023). Darunter versteht man die Praxis, einem LLM durch die Eingabe von natürlichsprachlich verfassten Beispielen, das in diesen Beispielen inkorporierte und implizierte ""Wissen"" zu vermitteln. Wie bereits Brown et. al. (2020) für GPT-3 zeigten, können LLMs eine Vielzahl komplexer Aufgaben mithilfe von ICL lösen. Im Detail noch nicht geklärt sind die Gründe, warum sie das tun. Jüngere Untersuchungen lassen vermuten, dass dabei die Tatsache, dass die verwendeten Beispiele plausibel bzw. wahr für die Aufgabe sind, weniger wichtig ist, als andere Faktoren wie zum Beispiel die zugrundeliegende Verteilung der Beispiele bzw. deren Format (Min et al. 2022) oder die über die Trainingsdaten implizit vermittelten semantischen Relationen von Begriffen (Xie et al. 2021). Clav√≠e et. al. 2023 zeigen zum Beispiel, dass bei der binären Klassifikation der Qualifikationsvoraussetzungen für eine Stellenausschreibung große LLMs wie OpenAIs text-davinci-003-Model klassische ML-Ansätze wie SVM aber auch kleinere ""foundational models"" wie DeBERTaV3 klar übertreffen. Für die Digital Humanities im Allgemeinen und die Computational Literary Studies (CLS) im Besonderen ist das ICL auf den ersten Blick sehr attraktiv, da es Wir wollen im Folgenden das Potential von ICL an einem konkreten Beispiel aus den CLS überprüfen. Dabei handelt es sich um den Versuch, die Resultate der Operationalisierung und Modellierung von generischen Aussagen aus Andrew Pipers Cambridge Element Die von Piper und seinem Team auf den annähernd ausgeglichenen Daten trainierten Modelle erzielten F1-Scores zwischen 0.591 und 0.769 sowie Accuracy-Werte zwischen 0.638 und 0.745, wobei es sich bei dem am besten performenden Modell um ein CNN mit ELMo-Embeddings handelt, bei dem der Recall die Precision deutlich übersteigt (Piper 2020, 34). Für unsere Experimente haben wir mit OpenAIs kostenpflichtigem Das beste ICL-Verfahren erzielt somit eine um 5-7 Prozentpunkte niedrigere Performance als das beste von Piper beschriebene Modell. Im Gegensatz zu Beispielen aus anderen Feldern zeigt sich also hier keine wesentlich bessere Performance als bei der Arbeit mit kleineren ""foundational models"" wie z.B. BERT. In diesem konkreten Fall erachten wir unter anderem folgende Möglichkeiten als plausible Ursachen dafür: Erstens ist die Explikation der Unterscheidung zwischen ""generalization"" und ""neutral"" im allgemeinen Sprachgebrauch nicht üblich 'man spricht zwar von generalisierenden Aussagen, bezeichnet aber gemeinhin nicht sämtliche Aussagen, die nicht unter diese Klasse fallen als ""neutral"". Pipers theoretisch durchweg gerechtfertigtes Klassifikationsschema wird somit vom Sprachgebrauch nicht gestützt. Ergänzend zu diesen konkreten Fragen zur verhältnismäßig schwachen Performance von ICL in Hinblick auf Pipers Daten wollen wir auch noch auf weitere potentielle Problemfelder und offene Fragen in Hinblick auf den Einsatz von In-Context-Learning in den CLS hinweisen. Dazu zählt, erstens, die prinzipielle Gefahr, dass das ICL durch seinen Fokus auf Beispiele dazu einlädt, Begriffe undefiniert und unreflektiert zu verwenden. Wenn, wie in unserem Fall, die besten Resultate mit jenem Prompt erzielt werden, der keine Definition der verwendeten Begriffe beinhaltet, lädt dies dazu ein, auf die Bestimmung dieser Begriffe von Anfang an zu verzichten. Die problematischen Konsequenzen eines solchen Vorgehens liegen auf der Hand: Ohne die Begriffe definiert zu haben, läuft ein re-import der Resultate in den fachspezifischen Diskurs Gefahr, deren Umfang zu verunklaren, da die bloße Nennung von Beispielen unterschiedliche Interpretationen von der Extension dieser Begriffe zulassen. Eine ähnliche Gefahr besteht jedoch, zweitens, auch wenn der Begriff vor und für das ICL definiert wird, da die Mechanismen hinter selbigen noch nicht geklärt sind. Bei einem Prompt, der sich aus Definition, Instruktion und Beispiel zusammensetzt, wissen die Nutzenden nicht, welche der drei Komponenten für die Klassifikation letztendlich ausschlaggebend ist. Ob es tatsächlich die dabei verwendete Definition ist, bleibt unklar. Dies führt, drittens, zu einem weiteren prinzipiellen Problem beim Einsatz von kommerziellen LLMs, das hinlänglich bekannt ist: Kommerzielle Anbieter wie OpenAI stellen ihre Modelle nicht öffentlich zur Verfügung. Die per se bereits breit diskutierte vermeintliche Opazität von LLMs wird so noch zusätzlich verstärkt. Viertens sind LLMs wie das hier verwendete text-davinci-003-Model von OpenAI nicht deterministisch. Die Resultate sind dementsprechend nicht stabil. In den CLS wird die Pflicht, dass man sich im Zuge des Operationalisierungs- bzw. Annotationsprozesses festlegt (welche Kategorien man wann vergibt, was diese bedeuten, wo Annotationen anfangen und aufhören, etc.) oft als Vorteil von computergestützten Verfahren gegenüber der ""traditionellen"" Literaturwissenschaft genannt (z.B. Meister 1995), da deren Begriffe ""in der Regel zu vage oder zu abstrakt [seien], als dass man sie eindeutig formalisieren könnte"" (Meister 2012, 294). Die insbesondere von Harald Fricke seit mehreren Jahrzehnten propagierte Auffassung, dass literaturwissenschaftliche Begriffe ausgehend vom standardsprachlichen Gebrauch zu präzisieren seien, um durch die solcherart hergestellte Exaktheit Vagheiten und Mehrdeutigkeiten aus dem literaturwissenschaftlichen Sprachgebrauch zu tilgen (Fricke 1989), bildet zwar mittlerweile das sprachtheoretische Fundament des Für den Einsatz von ICL in den CLS bedeutet das unseres Erachtens Folgendes: Erstens sollte man, unabhängig davon auf welches Sprachmodell man bei der Textanalyse zurückgreift, die für die Analyse zentralen Begriffe definieren und 'idealerweise 'manuell einen Referenzdatensatz erstellen. Dies erlaubt es, auch opake Modelle auf eine Art und Weise empirisch zu verankern, die den Nachvollzug sowie die Überprüfung der Validität der Analysen erleichtert bzw. in manchen Fällen überhaupt erst ermöglicht. Zweitens sollte man, falls man sich für den Einsatz von ICL entscheidet, zuerst mit kleineren Samples arbeiten, um zu überprüfen, ob das ICL überhaupt traditionelle Verfahren übertrifft: Bei Begriffen, deren Definitionen sich vom Alltagsgebrauch unterscheiden, ist das Fine-Tuning eines",de,sprachmodelle sogenannter large language models llms natural Language Processing nlp aufkommen letzter revolutionieren spätestens Veröffentlichung Chatgpt Potential Modell akademisch Öffentlichkeit vollständig erklärt Merkmal Modell zunehmend Größe schwellenwern Milliarde parameter nennen problemlösungskompetenzen entwickeln trainieren wei et sogenannter Emergent Abilities zählen Trainingsmethode streng Sinn klassisch Form handeln Anpassunge Gewichte durchführen icl Dong et verstehen Praxis llm Eingabe natürlichsprachlich verfasst Beispiel beispiel Inkorporiert impliziert wissen vermitteln Brown et zeigen llm Vielzahl komplex Aufgabe Mithilfe icl lösen Detail klären Gründ jung Untersuchung lassen vermuten Tatsache verwendet Beispiel plausibel Aufgabe wichtig Faktor zugrundeliegend Verteilung Beispiel Format min et Trainingsdat implizit vermittelt semantisch Relation begriffen xie et et zeigen binären Klassifikation qualifikationsvoraussetzungen Stellenausschreibung llms openai klassisch svm klein Foundational models klar übertreffen Digital Humanitie Computational literary Studies cls besonderer icl Blick attraktiv folgend Potential icl konkret Cls überprüfen handeln Versuch Resultate Operationalisierung Modellierung generisch Aussage Andrew piper Cambridge Element Piper Team annähernd ausgeglichen daten trainierten Modell erzielt wobei performend Modell cnn handeln Recall Precision deutlich übersteigen Piper experiment openai kostenpflichtig gut erzielen somit prozentpunkt niedrig Performance gut Piper beschrieben Modell Gegensatz Beispiel feldern zeigen wesentlich gut Performance Arbeit klein foundational models beren konkret Fall erachen folgend Möglichkeit plausibel Ursache erstens Explikation Unterscheidung Generalization neutral Sprachgebrauch üblich sprechen generalisierend Aussage bezeichnen gemeinhin sämtlicher aussagen Klasse fallen neutral Piper theoretisch durchweg gerechtfertigt Klassifikationsschema somit Sprachgebrauch stützen ergänzend konkret Frage verhältnismäßig schwach Performance icl Hinblick piper daten potentiell Problemfelder offen Frage Hinblick Einsatz Cls hinweisen zählen erstens prinzipiell Gefahr icl Fokus Beispiel einladen begriffe undefiniert unreflektiert verwenden unser Fall Resultat prompt erzielen Definition verwendet begriffe beinhalten laden Bestimmung Begriff Anfang verzichten problematisch Konsequenz vorgehens liegen Hand begriffe definieren laufen Resultate fachspezifisch Diskurs Gefahr Umfang verunklaren bloß Nennung Beispiel unterschiedlich Interpretation Extension begriffe zulassen ähnlich Gefahr bestehen zweitens Begriff icl definieren mechanismen selbigen klären prompt Definition instruktion zusammensetzen wissen Nutzend Komponente Klassifikation letztendlich ausschlaggebend tatsächlich verwendet Definition bleiben unklar führen drittens prinzipiell Problem Einsatz kommerziell llms hinlänglich kommerziell Anbieter Openai stellen Modell öffentlich Verfügung per se breit diskutiert vermeintlich Opazität llms zusätzlich verstärken viertens Llm verwendet Openai deterministisch Resultat stabil Cls Pflicht Zug annotationsprozesses festlegen kategorie vergiben bedeuten annotation anfangen aufhören Vorteil computergestützt Verfahren traditionell Literaturwissenschaft nennen Meister begriffe Regel vage abstrakt eindeutig formalisieren Meister insbesondere Harald Fricke mehrere Jahrzehnt propagiert Auffassung literaturwissenschaftlich begriffe ausgehend standardsprachlich Gebrauch präzisieren solcherart hergestellt Exaktheit vagheit Mehrdeutigkeit literaturwissenschaftlich Sprachgebrauch Tilg Fricke bilden mittlerweile sprachtheoretisch Fundament Einsatz icl Cls bedeuten unser erachtens folgend erstens unabhängig Sprachmodell Textanalyse zurückgreift Analyse zentral Begriff definieren idealerweise manuell Referenzdatensatz erstellen erlauben Opake Modell Art Weise empirisch verankern Nachvollzug Überprüfung Validität Analyse erleichtern Fall ermöglichen zweitens falls Einsatz icl entscheiden klein Samples arbeiten überprüfen icl traditionell Verfahren übertreffen begreifen definitionen Alltagsgebrauch unterscheiden,"[('icl', 0.475608733893686), ('piper', 0.24114187029817483), ('begriffe', 0.18500417635155156), ('openai', 0.1679410348942166), ('llms', 0.16076124686544987), ('cls', 0.15417014695962628), ('beispiel', 0.15397822459696905), ('modell', 0.13604692890127815), ('llm', 0.12595577617066248), ('gefahr', 0.11617119967674533)]"
2024,DHd2024,GIOVANNINI_Luca_Doctoral_Consortium__Luca_Giovannini_final.xml,Quantitative Ansätze zur Untersuchung der frühneuzeitlichen Dramengeschichte,"Luca Giovannini (Universität Potsdam, Deutschland / Universität Padua, Italien)","Theathergeschichte, Drama, Vergleichende Literaturwissenschaft, Formalismus, Operationalisierung","Theathergeschichte, Drama, Vergleichende Literaturwissenschaft, Formalismus, Operationalisierung","In den letzten Jahren hat sich die quantitative Forschung zum Drama als ein wichtiger Teil der computergestützten Literaturwissenschaft etabliert. Zum frühneuzeitlichen Drama gibt es allerdings noch wenig umfassende Studien zu verzeichnen, die über die Grenzen der nationalen Philologien hinausgehen und quantitative Beiträge zur Komparatistik liefern. Davon ausgehend ist Ziel des Promotionsvorhabens, eine quantitative Geschichte des frühneuzeitlichen europäischen Theaters zu skizzieren, die die Evolution der verschiedenen Nationalliteraturen vergleichend rekonstruiert. Als Ausgangspunkt der Dissertation dient die u. a. von Moretti (1994) verbreitete These, dass die Entwicklung des europäischen Theaters in der frühen Neuzeit als ein Prozess der biologischen Artbildung interpretiert werden kann. Im Laufe des 17. Jahrhunderts, so Moretti, wurde ein europaweites Modell der Tragödie, das aus der Antike und dem Mittelalter übernommen wurde, durch nationale Varianten wie das deutsche Trauerspiel oder die französische Dank der steigenden Textverfügbarkeit und den Fortschritten in den computationellen Methoden lässt sich diese bisher unhinterfragte literaturgeschichtliche These nun empirisch überprüfen. Daher lauten die konkreten Fragestellungen, wie eine solche Entwicklung der dramatischen Formen mit quantitativen Methoden nachzuvollziehen ist und ob der von Moretti beschriebene ""Verzweigungsprozess"" nicht nur für die Tragödie, sondern auch für die Komödie und andere Gattungen stattgefunden hat. Das Promotionsvorhaben erfolgt im Umfeld des Die Methodologisch inspiriert sich das Promotionsprojekt an den Forschungsansätzen des quantitativen Formalismus (Allison et al. 2011): Im Fokus steht die Struktur dramatischer Texte, d.h. eine der Komponenten, anhand derer die Entwicklung der Gattung Drama gezeigt werden kann. Da sich diese Dimension auf nicht sprachbedingte Elemente bezieht, etwa Figurenkonstellationen oder Redeverteilung, kann man sie produktiv für eine komparative Studie verschiedener Nationaltraditionen einsetzen. Als zentrale analytische Praxis für die Untersuchung der Variation des europäischen Dramas wird dann die Vektorisierung von Theaterstücken nach ihren strukturellen Merkmalen eingesetzt. Öhnlich wie bei Im Rahmen des Promotionsvorhabens sind zwei Anwendungsmöglichkeiten für die vektorisierten Stücke vorgesehen. Zum einen können mithilfe verschiedener Abstandsmessungen (z. B. euklidischer Abstand oder Kosinus-Öhnlichkeit) die Distanzen zwischen den Vektoren berechnet werden, wobei ein größerer Abstand auf eine größere strukturelle Unterschiedlichkeit hinweisen soll. Zum anderen ist es möglich, die Vektoren durch Techniken wie die Hauptkomponentenanalyse (PCA) auf einer niedrigdimensionalen Ebene zu visualisieren, um Cluster zu identifizieren. Erste Ergebnisse zeigen, dass ein Narrativ von kontinuierlicher Verzweigung zwischen literarischen Traditionen nicht ohne Einschränkungen vertretbar ist. Obwohl eine Tendenz zur Diversifizierung bemerkbar ist, ist die Gattungsevolution scheinbar durch komplexe und mehrschichtige Dynamiken geprägt. Auch wenn die Arbeit mit einem kleinen, aber sorgfältig kuratierten Korpus wie",de,letzter quantitativ Forschung Drama wichtig computergestützt Literaturwissenschaft etablieren frühneuzeitlich Drama umfassend Studie verzeichnen Grenze national Philologie hinausgehen quantitativ Beitrag Komparatistik liefern ausgehend Ziel Promotionsvorhaben quantitativ Geschichte frühneuzeitlich europäisch Theater skizzieren Evolution verschieden Nationalliteratur vergleichend rekonstruieren Ausgangspunkt Dissertation dienen moretti verbreitet These Entwicklung europäisch Theater früh Neuzeit Prozess biologisch artbildung interpretieren Lauf Jahrhundert moretti europaweit Modell Tragödie Antike Mittelalter übernehmen national Variant deutsch Trauerspiel französisch steigend Textverfügbarkeit Fortschritt computationell Methode lässt unhinterfragt literaturgeschichtlich These empirisch überprüfen lauten konkret Fragestellung Entwicklung dramatisch Form quantitativ Methode nachvollziehen moretti beschrieben verzweigungsprozess Tragödie komödi Gattung stattfinden Promotionsvorhaben erfolgen Umfeld Methodologisch inspirieren Promotionsprojekt Forschungsansätz quantitativ Formalismus Allison et Fokus stehen Struktur dramatisch Text Komponente anhand der Entwicklung Gattung Drama zeigen Dimension sprachbedingen Elemente beziehen Figurenkonstellation Redeverteilung produktiv komparativ Studie verschieden nationaltraditionen einsetzen zentral analytisch Praxis Untersuchung Variation europäisch Drama Vektorisierung theaterstücken strukturell Merkmale einsetzen öhnlich Rahmen Promotionsvorhaben Anwendungsmöglichkeit vektorisiert Stück vorsehen Mithilfe verschieden Abstandsmessung euklidisch Abstand Distanz Vektor berechnen wobei groß Abstand groß strukturell Unterschiedlichkeit hinweisen vektoren Technik Hauptkomponentenanalyse Pca niedrigdimensional Ebene visualisieren Cluster identifizieren Ergebnis zeigen narrativ kontinuierlich Verzweigung literarisch Tradition Einschränkung vertretbar obwohl Tendenz Diversifizierung bemerkbar Gattungsevolution scheinbar komplex mehrschichtig Dynamik prägen Arbeit sorgfältig kuratiert Korpus,"[('promotionsvorhaben', 0.26486250701073427), ('europäisch', 0.212025903087713), ('drama', 0.20992031403036973), ('quantitativ', 0.19127720079648053), ('frühneuzeitlich', 0.16019659017457896), ('moretti', 0.15918929916469182), ('theater', 0.13491657119702372), ('tragödie', 0.12497218755923147), ('abstand', 0.12497218755923147), ('national', 0.12095912764219817)]"
2024,DHd2024,KONLE_Leonard_Modellierung_von_Gattungsunterschieden__Emotio.xml,"Modellierung von Gattungsunterschieden. Emotionen in Lyrik, Prosa und Drama","Merten Kröncke (Universität Würzburg, Deutschland); Leonard Konle (Universität Göttingen, Deutschland); Simone Winko (Universität Würzburg, Deutschland); Fotis Jannidis (Universität Göttingen, Deutschland)","Emotion, Gattung, Domäne, Fehleranalyse, Machine Learning","Modellierung, Annotieren, Literatur, Methoden, Forschungsprozess, Text","Literaturwissenschaftliche Untersuchungen zielen häufig darauf ab, verschiedene Textgruppen (zum Beispiel Gedichte, Romane und Dramen) hinsichtlich verschiedener Texteigenschaften (zum Beispiel Themen oder Emotionen) miteinander zu vergleichen. In der Computerlinguistik (oder allgemeiner im gesamten Feld Machine Learning) wird die erläuterte Problemstellung unter dem Schlagwort Domain Adaptation intensiv beforscht (z.B. Ramponi und Plank 2020). Machine Learning-Probleme lassen sich als Versuch beschreiben, eine automatische Zuweisung von Datenpunkten x zu Labeln y zu lernen. Dabei wird unterstellt, dass alle Punkte x aus der gleichen Verteilung stammen, die zu lernende Zuweisung x‚Üíy also für jeden Datenpunkt ähnlich funktioniert. Diese Annahme ist in angewandter Forschung (darunter Computational Literary Studies) jedoch selten zu halten. Die Gründe für einen Domain Shift, also die Veränderung von x, während y stabil bleibt, können vielfältig sein (z.B. historischer Sprachwandel, Übersetzungen). Die Bereitstellung von Datensätzen, die dezidiert mehrere Domänen enthalten, ist also sowohl für Machine Learning Forschung als auch für die CLS hoch relevant. Als Untersuchungsbeispiel dient die Gestaltung von Emotionen in deutschsprachigen Lyrik-, Prosa- und Dramentexten der zweiten Hälfte des 19. und des beginnenden 20. Jahrhunderts. In früheren Studien hat sich unsere Forschungsgruppe auf Lyrik konzentriert und für diese Gattung umfangreiche manuelle Annotationen erstellt. Die Studie verwendet drei Korpora: ein vergleichsweise großes Lyrikkorpus, für das umfangreiche manuelle Annotationen vorliegen, und zwei deutlich kleinere, zu Testzwecken zusammengestellte Korpora mit einerseits Prosa- und andererseits Dramentexten. Das Lyrikkorpus besteht aus Texten in Anthologien aus dem Untersuchungszeitraum, die sich auf Gedichte von Zeitgenoss:innen konzentrieren. Die Anthologien stammen aus der Zeit von 1859 bis 1919 und enthalten mehr als 6000 Gedichte, von denen 1412 (270k Token) annotiert wurden. Die Emotionsannotation zielt darauf ab, die im Text gestalteten Emotionen (und nicht die Emotionen der Leser:innen) zu erfassen. Genutzt wurde ein Set von 40 diskreten Emotionen, darunter zum Beispiel Liebe, Trauer, Hoffnung, Sehnsucht oder Hass. Einerseits handelt es sich um Emotionen, die in gängigen Emotionstheorien Tabelle 2 zeigt die Qualität der Emotionsklassifikation in den drei Gattungen. Verwendet wird ein Modell, welches lediglich mit den Annotationen für Lyrik trainiert ist. Basis ist das deutsche Bert-Modell gbert-large (Chan et al. 2020). Dieses wird zusätzlich auf Lyrik angepasst Die folgenden Abschnitte beschäftigen sich mit der Suche nach möglichen Erklärungen für die großen Qualitätsunterschiede (siehe Tab. 2). Es werden sowohl Eigenschaften des Modells als auch die Verteilung der annotierten Emotionen, die Zusammensetzung einzelner Emotionen und das zugrundeliegende Textmaterial untersucht. Zunächst lässt sich danach fragen, wie sicher sich das Modell bei den Klassifikationen ist (Abb. 1). Blickt man auf die Vorhersage von (vorhandenen) Emotionen, ist die Sicherheit erwartungsgemäß bei Lyrik am größten, gefolgt von Prosa und danach Drama. Bei der Vorhersage ""keine Emotion"" ist sich das Modell hingegen im Fall von Dramentexten besonders sicher (sogar noch sicherer als im Fall von Gedichten) und im Fall von Prosa besonders unsicher. Zum einen scheint die Klassifikationsperformance also mit der Sicherheit des Modells zusammenzuhängen; zum anderen weisen die Differenzen zwischen Prosa- und Dramentexten in puncto ""Emotion""/""Keine Emotion"" auf klassifikationsrelevante Gattungsunterschiede hin. Um einen Eindruck von der Emotionsverteilung innerhalb der Gattungen zu erhalten, werden jeweils 50 Segmente (Verse bzw. Sätze) zu einer Einheit zusammengefasst, die als Vektor über die Anzahl der enthaltenen Emotionen repräsentiert wird. Um diese Vektoren zu visualisieren, werden sie in den 2-dimensionalen Raum projiziert (siehe McInnes 2018). Das Resultat (Abb. 2) zeigt, dass die annotierten Gedichte stärker streuen als die annotierten Texte der übrigen Gattungen, also vielfältigere Mischungen an Emotionen enthalten. Auffällig ist zusätzlich die Häufung von Dramen und Prosa im oberen rechten Bereich der Grafik. Die Emotionsverteilung innerhalb der beiden Gattungen ähnelt sich nach diesem Befund und weicht zugleich von der Verteilung in den meisten lyrischen Texten ab. Abbildung 3 macht deutlich, dass die Gattungen Emotionen in stark unterschiedlicher Häufigkeit gestalten. Lyrik enthält mit Abstand die meisten Emotionen, beinahe das Dreifache im Vergleich zu Dramen. Diese enthalten wiederum das Doppelte an Emotionen, gemessen an Prosa. Abbildung 4 ermöglicht einen Einblick in die einzelnen Emotionsgruppen nach Shaver ( Abbildung 5 zeigt beispielhaft für die Emotionsgruppe Erregung/Überraschung, wie sich die Gruppe je nach Gattung anteilig zusammensetzt. Es zeigen sich erhebliche Unterschiede: Während in lyrischen Texten die Kategorie ""Emotionalität"" dominiert, die vor allem für unspezifische Emotionen eingesetzt wird (""Er war ein grundsätzlich emotionaler Mensch"" usw.), kommt in den annotierten Dramen ""Aufregung"" am häufigsten vor; in den annotierten Prosatexten ist wiederum die Einzelemotion ""Spannung"", verglichen mit den anderen Gattungen, besonders verbreitet. Diese Unterschiede erzeugen ein großes Fehlerpotential, da sich mit der Zusammensetzung auch die Repräsentation der Gruppe im Modell ändert. Während in Lyrik bereits gute Ergebnisse erzielt werden können, wenn lediglich die Einzelemotion ""Emotionalität"" erkannt wird, ist diese für Prosatexte nutzlos. Umgekehrtes gilt für Spannung. Nachdem die Verteilungsunterschiede in den Emotionen dargestellt sind, werden Differenzen in der sprachlichen Gestaltung der annotierten Texte untersucht. Neben der bislang betrachteten Emotionsannotation wurde separat festgehalten, welche Wörter im Text über ihre lexikalische Bedeutung markieren, dass eine Emotion gestaltet wird, zum Beispiel Ausdrücke wie ""Angst"", ""lachen"" oder ""jauchzen"". Eine Emotionsannotation wird meist, aber nicht immer, von der Annotation entsprechender Emotionswörter begleitet; umgekehrt kommen Emotionswörter nie ohne zugehörige Emotionsannotation vor. Tabelle 3 zeigt, wie viele Emotionswörter pro Emotionsannotation je nach Gattung vorkommen. In Dramen werden etwas mehr Emotionswörter verwendet als in Prosa und Lyrik, eine explizitere Nennung von Emotionen in Lyrik als mögliche Fehlerquelle in der anschließenden Klassifikation der anderen Gattungen kann damit ausgeschlossen werden. Abschließend lässt sich danach fragen, wie groß der Abstand zwischen den Gattungen hinsichtlich des Textmaterials ist und ob die Unterschiede in der Klassifikationsperformance dazu ""passen"". Abb. 6 zeigt, dass der Abstand zwischen Lyrik und Prosa am größten, der Abstand zwischen Lyrik und Drama bereits deutlich geringer und der Abstand zwischen Drama und Prosa am geringsten ausfällt. Dass Lyrik also, was das Textmaterial angeht, eher den einbezogenen Dramen- als den Prosatexten ähnelt, schlägt sich jedoch nicht unmittelbar in der Klassifikationsperformance nieder, die nämlich im Fall von Dramen nicht besser als im Fall von Prosa ist. Die Studie ist von der Frage ausgegangen, wie sich etwaige Unterschiede zwischen den literarischen Großgattungen Prosa, Drama und Lyrik in puncto Emotionsgestaltung mit Fragen der Domain Adaptation verknüpfen und aus dieser Perspektive modellieren lassen. Der (zu erwartende) Befund, dass ausschließlich auf Lyrikannotationen trainierte Modelle deutlich schlechter performen, wenn sie auf Prosa- und Dramentexte angewendet werden, kann mit einer ganzen Reihe von Faktoren zusammenhängen, von denen einige näher untersucht wurden. Neben pragmatischen Gesichtspunkten, zum Beispiel den etwas niedrigeren Inter-Annotator-Agreement-Werten, scheinen vor allem Spezifika der Domänen eine Rolle zu spielen. Erhebliche Unterschiede zwischen den Gattungen zeigen sich unter anderem, wenn man die Häufigkeit und Verteilung der gestalteten Emotionen betrachtet und wenn man danach fragt, aus welchen Einzelemotionen sich die Emotionsgruppen zusammensetzen. Demgegenüber deuten weitere Ergebnisse darauf hin, dass sich manche Gattungsunterschiede Die Ergebnisse deuten indizienhaft an, dass Gedichte, verglichen mit Dramen und Prosatexten, besonders häufig Emotionen gestalten. Dieser Befund passt zu den in der Einleitung erwähnten gattungstypologischen Vermutungen, wenngleich berücksichtigt werden muss, dass an dieser Stelle nur sehr wenige Prosa- und Dramentexte einbezogen werden konnten. Um noch besser abgesicherte Schlüsse über die Gattungen zu ermöglichen, werden wir weitere Texte annotieren und verschiedene Verfahren der Domain Adaptation testen, um letztlich auch für Prosa und Dramen zuverlässige Klassifikatoren trainieren zu können.",de,literaturwissenschaftlich Untersuchung Ziel häufig verschieden Textgruppe gedicht roman Dram hinsichtlich verschieden texteigenschaft Thema Emotion miteinander vergleichen Computerlinguistik allgemein gesamt Feld Machine Learning erläutert Problemstellung Schlagwort Domain Adaptation intensiv Beforscht Ramponi Plank machin lassen Versuch beschreiben automatisch Zuweisung Datenpunkt x labeln y lernen unterstellen Punkt x gleich Verteilung stammen lernend Zuweisung x üíy datenpunken ähnlich funktionieren Annahme angewandt Forschung Computational literary studies selten halten gründe domain shifen Veränderung x Y stabil bleiben vielfältig historisch Sprachwandel übersetzungen Bereitstellung datensätzen dezidiert mehrere Domän enthalten sowohl Machine Learning Forschung Cls relevant Untersuchungsbeispiel dienen Gestaltung Emotion deutschsprachig dramentexten Hälfte beginnend Jahrhundert früh Studie Forschungsgruppe Lyrik konzentrieren Gattung umfangreich Manuell annotationen erstellen Studie verwenden Korpora vergleichsweise Lyrikkorpus umfangreich Manuelle annotationen vorliegen deutlich klein testzwecken zusammengestellt Korpora einerseits andererseits dramentexn Lyrikkorpus bestehen Text Anthologie Untersuchungszeitraum Gedicht Zeitgenoss innen konzentrieren Anthologie stammen enthalten gedichen token annotiert Emotionsannotation zielen Text gestaltet Emotion Emotion Leser innen erfassen nutzen set Diskret Emotion lieb Trauer Hoffnung Sehnsucht Hass einerseits handeln Emotion gängig Emotionstheorie Tabell zeigen Qualität Emotionsklassifikation Gattung verwenden Modell lediglich Annotation Lyrik trainieren Basis deutsch chan et zusätzlich Lyrik angepasst folgend Abschnitt beschäftigen Suche möglich Erklärung Qualitätsunterschiede sehen tab sowohl Eigenschaft Modell Verteilung Annotierte Emotion Zusammensetzung einzeln Emotion zugrundeliegend Textmaterial untersuchen lässen fragen sicher Modell Klassifikation abb blicken Vorhersage vorhanden Emotion Sicherheit erwartungsgemäß Lyrik groß folgen Prosa Drama Vorhersage emotion Modell hingegen Fall dramentexten sicher sogar sicher Fall Gedicht Fall Prosa unsicher scheinen Klassifikationsperformance Sicherheit Modell Zusammenzuhäng weise Differenz dramentexten puncto Emotion klassifikationsrelevant Gattungsunterschiede Eindruck Emotionsverteilung innerhalb Gattung erhalten jeweils Segment vers Sätz Einheit zusammengefassen Vektor Anzahl enthalten emotionen repräsentieren vektoren visualisieren Raum projizieren sehen mcinnes Resultat abb zeigen Annotiert gedichte stark streuen annotiert Text übrig Gattung vielfältiger Mischung Emotion enthalten auffällig zusätzlich Häufung Dram Prosa oberer Bereich Grafik Emotionsverteilung innerhalb Gattung ähneln Befund weichen Verteilung meister lyrisch Text Abbildung deutlich gattung Emotion stark unterschiedlich Häufigkeit gestalten Lyrik enthalten Abstand meister Emotion beinahe Dreifache Vergleich dramen enthalten wiederum Doppelte Emotion messen Prosa Abbildung ermöglichen Einblick einzelner Emotionsgruppe Shaver Abbildung zeigen Beispielhaft Emotionsgruppe Erregung überraschung Gruppe Gattung anteilig zusammensetzen zeigen erheblich Unterschied lyrisch Text Kategorie Emotionalität dominieren unspezifisch Emotion einsetzen grundsätzlich emotional Mensch annotierter Dram Aufregung häufig annotierter Prosatext wiederum Einzelemotion spannung vergleichen gattung verbreiten Unterschied erzeugen Fehlerpotential Zusammensetzung Repräsentation Gruppe Modell ändern Lyrik Ergebnis erzielen lediglich Einzelemotion Emotionalität erkennen Prosatext nutzlos umgekehrt gelten Spannung Verteilungsunterschiede Emotion darstellen differenzen sprachlich Gestaltung annotiert Text untersuchen bislang betrachtet Emotionsannotation Separat festhalten Wörter Text lexikalisch Bedeutung markieren Emotion gestalten ausdrücken Angst lachen jauchzen Emotionsannotation meist Annotation entsprechend Emotionswörter begleiten umgekehrt Emotionswörter zugehörig Emotionsannotation Tabelle zeigen emotionswört pro Emotionsannotation Gattung vorkommen Dram emotionswört verwenden Prosa Lyrik expliziter Nennung Emotion Lyrik möglich Fehlerquelle anschließend Klassifikation gattungen ausschließen abschließend lässt fragen Abstand gattung hinsichtlich Textmaterial Unterschied Klassifikationsperformance passen abb zeigen Abstand Lyrik Prosa groß Abstand Lyrik Drama deutlich gering Abstand Drama Prosa gering ausfallen Lyrik textmaterial angehen eher einbezogen Prosatext ähneln schlagen unmittelbar Klassifikationsperformance nieder nämlich Fall Dram Fall Prosa Studie Frage ausgehen etwaig Unterschied literarisch Großgattung Prosa Drama Lyrik puncto Emotionsgestaltung Frage Domain Adaptation verknüpfen Perspektive Modelliere lassen erwartend Befund ausschließlich lyrikannotationen trainiert Modell deutlich schlecht performen dramentext anwenden Reihe Faktor zusammenhängen nah untersuchen pragmatisch Gesichtspunkt niedrig scheinen Spezifika Domänen Rolle spielen erheblich Unterschied Gattung zeigen Häufigkeit Verteilung gestaltet Emotion betrachten fragen einzelemotionen Emotionsgruppe zusammensetzen deuten Ergebnis Gattungsunterschiede Ergebnis deut indizienhaft gedicht vergleichen Dram Prosatext häufig emotion gestalten Befund passen Einleitung erwähnt Gattungstypologischen vermutungen wenngleich berücksichtigen Stelle dramentext einbeziehen abgesichert Schlüsse Gattung ermöglichen Text annotieren verschieden Verfahren Domain Adaptation testen letztlich Prosa dramen zuverlässig klassifikatoren Trainier,"[('emotion', 0.4877417598161499), ('lyrik', 0.3355888880852935), ('prosa', 0.2393135612091913), ('gattung', 0.21653624282175749), ('emotionsannotation', 0.15847177982983982), ('abstand', 0.12362663256538337), ('dram', 0.11951375641699437), ('klassifikationsperformance', 0.11252049463263634), ('modell', 0.1072875429059774), ('emotionsgruppe', 0.10480431037999727)]"
2024,DHd2024,20240108_LEMKE_Marc_CANSpiN__Zur_computergest_tzten_Analyse_narrative.xml,CANSpiN: Zur computergestützten Analyse narrativen Raums im Roman des 19. und 20. Jahrhunderts,"Marc Lemke (Universität Rostock, Deutschland); Ulrike Henny-Krahmer (Universität Rostock, Deutschland); Nils Kellner (Universität Rostock, Deutschland)","Raum, Literatur, Roman, Deep Learning, BERT","Entdeckung, Programmierung, Räumliche Analyse, Modellierung, Annotieren, Methoden","Mit unserer Einreichung stellen wir den aktuellen Arbeitsstand des Projekts ""Computational Approaches to Narrative Space in 19th and 20th Century Novels"" (CANSpiN) vor, das im Rahmen des DFG-Schwerpunktprogramms ""Computational Literary Studies"" (SPP 2207) von April 2023 bis März 2026 gefördert wird. Ziel des Vorhabens ist es, computergestützte Methoden zur Erkennung und Analyse narrativen Raums in literarischen Texten zu entwickeln und diese Methoden für die Untersuchung literaturhistorischer Fragen zum Verhältnis von Raum und nationaler Identität in deutsch- und spanischsprachigen Romanen des 19. und 20. Jahrhunderts zur Anwendung zu bringen. Dieser Zielstellung entspricht die Zusammensetzung der Projektgruppe, die aus Wissenschaftler:innen der Germanistik, Romanistik, den Digital Humanities und der Mathematik besteht. Ausgangspunkt der Überlegungen ist die Definition des narrativen Raums: Darunter verstehen wir im weiteren Sinne die Räumlichkeit eines narrativen Textes nach Schumacher (2022b), die durch raumreferentielle sprachliche Ausdrücke bestimmt ist. In einem engeren narratologischen Sinne begreifen wir narrativen Raum als den Raum der erzählten Welt, wie er durch die Erzählung konstituiert ist (Genette, 2010). Wie dieser Raum strukturell und funktional zu beschreiben ist, dazu existieren bereits zahlreiche literaturwissenschaftliche Vorschläge (Dennerlein, 2009; Piatti, 2008; Ryan, 2014; Lotman, 1977; Renner, 2004). Wie raumanalytische Zugänge formalisiert und auf konkrete Textmerkmale abgebildet werden können, dazu bieten Arbeiten der Computational Literary Studies schon erste Antworten (Viehhauser und Barth, 2017; Barth, 2021, 2022; Viehhauser, 2020; Schumacher, 2022a, 2022b). Auf diesem Forschungsstand setzen wir auf: In einem offenen explorativen Verfahren erproben wir verschiedene raumanalytische Kategorien-Sets hinsichtlich ihrer literaturwissenschaftlichen Aussagekraft und ihrer computergestützten Operationalisierbarkeit (Arbeitspaket 1). Für jedes dieser Sets entwickeln wir Annotationsrichtlinien, um die verwendeten Korpora computergestützt zu annotieren und mit quantitativen Methoden vergleichend zu untersuchen. So haben wir ein erstes Kategoriensystem CANSpiN.CS1 für die Annotation von Raumreferenzen in Erzähltexten definiert und mit Hilfe des Tools CATMA (Gius et al., 2023) erprobt. Es ist geplant, auf diesem Wege eine Ground Truth aufzubauen, mit der im Tool NTEE (Lemke et al., 2023) Modelle für die Erkennung von Raumentitäten trainiert werden können, welche die semi-automatisierte, vollständige Annotation der Textkorpora im Projekt erlauben. Das Interesse des Projekts richtet sich dabei insbesondere auch auf die Möglichkeiten und Grenzen des Deep Learnings mit Sprachmodellen der BERT-Architektur (Devlin et al., 2019) für die computergestützte Annotation literaturwissenschaftlich definierter Textphänomene: Um diese auszuloten, werden die im Arbeitspaket 1 erzeugten Modelle mit Methoden eines Explainable AI-Ansatzes (XAI) untersucht (Arbeitspaket 2), was konkrete Maßnahmen und Vorschläge zur Optimierung der Annotationsprozesse erwarten lässt, aber auch Erkenntnisse über die technischen Grenzen dieses Vorgehens (Rogers et al., 2021). Die Korpora werden derzeit ausgehend von verschiedenen Vorarbeiten zusammengestellt, aus deutschsprachigen Romanen des 19. Jahrhunderts (Zeit: 1790-1910, Ziel: etwa 200 Romane, Quellen: ELTeC-deu (Konle et al., 2021), DTA (Berlin-Brandenburgischen Akademie der Wissenschaften, 2023), TextGrid Repository (TextGrid Konsortium, 2014)), spanischsprachigen Romanen des 19. Jahrhunderts (Zeit: 1790-1870, Ziel: etwa 100 Romane, Quellen: ELTeC-spa (Navarro-Colorado et al., 2021), Biblioteca Virtual Miguel de Cervantes (Centro de Humanidades Digitales en la Universidad de Alicante, 2021)), spanischsprachigen Romanen aus Lateinamerika des 19. Jahrhunderts (Zeit: 1830-1910, Ziel: etwa 200 Romane, Quellen: Corpus de novelas hispanoamericanas del siglo XIX (Conha19) (Henny-Krahmer, 2021)) und deutschsprachigen Romanen des 20. Jahrhunderts (Zeit: 1950-2000, Ziel: etwa 100 Romane, Quellen: TEI-Dateien der Uwe Johnson-Werkausgabe, E-Books (u.a. zu den Autor:innen Heinrich Böll, Christine Brückner, Uwe Johnson, Walter Kempowski, Christa Wolf)). Für ein einheitliches Format der Texte haben wir uns für das Schema ELTeC Level 0 (Burnard, 2012) entschieden, das wir für unsere Zwecke im CANSpiN-Projekt anpassen. Die Auswahl der Korpora gründet auf unserer Arbeitshypothese, dass narrativer Raum in Romanen sprach- und zeitübergreifend ein für die Analyse zugängliches Phänomen ist, das unterschiedlich ausgeformt, also dargestellt und funktionalisiert sein kann. Durch die quantitativen Analysen aus Arbeitspaket 1 erwarten wir in dieser Hinsicht neue Erkenntnisse zu literaturhistorischen Fragestellungen (Arbeitspaket 3), etwa zur Darstellung und Semantik von Raum in Romanen im Kontext der Nationenbildung im 19. Jahrhundert in Deutschland und Lateinamerika (Sommer, 1993; Pe√±aranda Medina, 1994; Hanway, 2003; Viel, 2009; Ferrer, 2018) und der Herausbildung zweier deutscher Identitäten in der Nachkriegsliteratur des 20. Jahrhunderts (Bond, 1996; Erll et al., 2003; Helbig et al., 2007; Westphal, 2007; Nies, 2018).",de,Einreichung stellen aktuell Arbeitsstand Projekt computational approach to narrativ Space and Century Novels Canspin Rahmen Computational literary studies spp April März fördern Ziel vorhaben computergestützt Methode Erkennung Analyse narrativ Raum literarisch Text entwickeln Methode Untersuchung literaturhistorisch Frage Verhältnis Raum national Identität spanischsprachig Roman Jahrhundert Anwendung bringen Zielstellung entsprechen Zusammensetzung Projektgruppe Wissenschaftler innen Germanistik Romanistik Digital Humanitie Mathematik bestehen Ausgangspunkt Überlegung Definition narrativ Raum verstehen Sinn Räumlichkeit narrativ Text Schumacher raumreferentiell sprachlich Ausdrücke bestimmen eng narratologisch Sinn begreifen narrativ Raum Raum erzählt Welt Erzählung konstituieren genett Raum strukturell funktional beschreiben existieren zahlreich literaturwissenschaftlich Vorschlag dennerlein Piatti Ryan lotman Renner raumanalytisch Zugänge formalisieren konkret Textmerkmal abbilden bieten arbeiten computational literary studies Antwort viehhauser Barth Barth viehhauser schumacher Forschungsstand setzen offen explorativ Verfahren erproben verschieden raumanalytisch hinsichtlich literaturwissenschaftlich Aussagekraft computergestützt Operationalisierbarkeit Arbeitspaket jeder Set entwickeln Annotationsrichtlinie verwendet Korpora computergestützen annotieren quantitativ Methode vergleichend untersuchen kategoriensystem Annotation Raumreferenze Erzähltext definieren Hilfe Tool Catma Gius et erproben planen Weg Ground Truth aufbauen Tool Ntee Lemke et Modell Erkennung Raumentität trainieren vollständig Annotation Textkorpora Projekt erlauben Interesse Projekt richten insbesondere Möglichkeit Grenze Deep Learning Sprachmodelle devlin et computergestützt Annotation Literaturwissenschaftlich definiert Textphänomene auszuloten Arbeitspaket erzeugt Modell Methode explainabel xai untersuchen Arbeitspaket konkret Maßnahme vorschläge Optimierung annotationsprozesse erwarten lässn erkenntnis technisch Grenze Vorgehen Roger et Korpora derzeit ausgehend verschieden vorarbeiten zusammenstellen deutschsprachig Roman Jahrhundert Ziel Roman quellen Konle Et Dta Akademie Wissenschaft Textgrid Repository Textgrid Konsortium spanischsprachig Roman Jahrhundert Ziel Roman quellen et Biblioteca virtual Miguel -- cervantes centro de humanidade digitales -- universidad de alicanen spanischsprachig Roman Lateinamerika Jahrhundert Ziel Roman quellen Corpus de Novelas hispanoamericanas del siglo xix deutschsprachig Roman Jahrhundert Ziel Roman quellen Uwe Autor innen Heinrich Böll Christine Brückner Uwe Johnson Walter Kempowski Christa Wolf einheitlich Format Text Schema eltec Level Burnard entscheiden Zweck anpassen Auswahl Korpora gründen Arbeitshypothese narrativ Raum Roman zeitübergreifend Analyse zugänglich Phänomen unterschiedlich ausgeformen darstellen funktionalisieren quantitativ Analyse Arbeitspaket erwarten Hinsicht erkenntnis literaturhistorisch Fragestellung Arbeitspaket Darstellung semantik Raum Roman Kontext Nationenbildung Jahrhundert Deutschland Lateinamerika Sommer medina hanway Ferrer Herausbildung zwei deutsch Identität Nachkriegsliteratur Jahrhundert Bond erll et Helbig et westphal nies,"[('arbeitspaket', 0.30785060372617756), ('roman', 0.291560854090451), ('raum', 0.23893329078073805), ('narrativ', 0.1924024904682242), ('jahrhundert', 0.1707970839341562), ('spanischsprachig', 0.16305655877886097), ('quellen', 0.1445491255616395), ('et', 0.1301557277151637), ('uwe', 0.12314024149047102), ('raumanalytisch', 0.12314024149047102)]"
2024,DHd2024,G_GGELMANN_Michael_Co_Kreativit_t_digital_erschlie_en___ber_.xml,Co-Kreativität digital erschließen: Über die Annotation komplexer ästhetischer Phänomene,"Matthias Bauer (Universität Tübingen, Deutschland); Michael Göggelmann (Universität Tübingen, Universität zu Köln, Deutschland); Sara Rogalski (Universität Tübingen, Deutschland); Sandra Wetzel (Universität Tübingen, Deutschland); Angelika Zirker (Universität Tübingen, Deutschland)","Annotieren, Co-Kreativität, Akte, Artefakte, Shakespeare, Spenser, Donne, Herbert, Vaughan, Lyrik, Frühe Neuzeit, Gemeinschaftliche Autorschaft, Gedichte","Annotieren, Kollaboration, Artefakte, Literatur, Forschungsprozess, Text","Obwohl gemeinschaftliche Autorschaft in der frühen Neuzeit häufig der Normalfall war, geht die Forschungsliteratur weiterhin und gerade dann von einem Konzept der Einzelautorschaft aus, wenn sie sich lediglich auf die Identifizierung der Anteile individueller Autoren (insbesondere Shakespeares) an Gemeinschaftswerken fokussiert (z.B. Vickers, 2002). Das Aufkommen und die Verbesserung digitaler Methoden hat leider nur zu einer Bagatellisierung des Konzepts gemeinschaftlicher Autorschaft geführt; stilometrische Untersuchungen von dramatischen Texten befördern die Vorstellung von gemeinschaftlicher Autorschaft als Summe von Einzelautorschaften und reduzieren die Autor- und Urheberschaft auf den Stil. Gemeinschaftliche Autorschaft ist aber mehr als die Summe ihrer Teile; um sie zu erforschen, braucht man eine Idee davon, was an ihr anders ist. Um dies herauszufinden, sollte man wissen, wie in der Frühen Neuzeit selbst darüber gedacht wurde. Im Projekt zur Östhetik gemeinschaftlicher Autorschaft Wir entschieden uns zu Beginn der Arbeit für kurze, aber reflexionsdichte Texte, d.h. Gedichte, um möglichst schnell einen Einblick in verschiedene Autoren und Werke zu erlangen. Bei der Aufbereitung der unserer Arbeit zugrunde liegenden Gedichtkorpora griffen wir überwiegend auf editierte, in digitaler Form vorliegende Werke bekannter frühneuzeitlicher Autoren zurück und bereiteten diese auf Basis intern entwickelter Richtlinien für die Weiterarbeit im Aufgrund der Komplexität des Phänomens, das wir untersuchen, nutzten wir den Der erste Versuch, co-kreative Reflexionen in Gedichten zu annotieren, basierte auf dem Expertenvorwissen aller Annotator:innen. Wir suchten zunächst nach Bausteinen, die kombiniert, so lautete unsere Hypothese, ein bestimmtes Konzept der Co-Kreativität bildeten. Wenn also z.B. Herbert von einem Austausch der Gedichte zwischen Gott und ihm spricht, könnte man dieses Konzept als aus den Bausteinen des Gebens und Nehmens gebildet analysieren. Die Herausforderung dieser Herangehensweise lag darin, die Bausteine ohne Kenntnis des im Gedicht anzutreffenden Konzepts zu identifizieren. Ein erster Versuch bestand darin, Synonyme bereits bekannter Bausteine in unseren Fallbeispielkorpora ausfindig zu machen und manuell 50 Dieses Modell geht davon aus, dass die Reflexion über co-kreative Prozesse einen Sonderfall der Reflexion über Produktionsvorgänge bildet, an denen mehrere Akteure beteiligt sind. Dementsprechend erfolgt die Annotation co-kreativer Reflexionen in mehreren Schritten: zunächst muss der im Text erwähnte Akt der Produktion oder das produzierte Artefakt (A) in den Blick genommen werden, dann die Akteure, auf denen die Co-Kreativität (CO) beruht, sowie im letzten Schritt die damit verbundene Prädikation (P), d.h. das, was über die Produktion gesagt wird. Das Zusammenspiel aller drei Komponenten bilden eine co-kreative Konstellation. Verdeutlicht werden kann dies am Beispiel von George Herberts Gedicht ""A True Hymne"": Hier heißt es in Z.17-18: ""Although the verse be somewhat scant, / God doth supplie the want"" (Herbert, 2008, 574). ""Verse"" ist das geschaffene Artefakt (A), das auf dem (hier implizierten) Akt des Schreibens basiert; die Co-Kreativität (CO) besteht in der Zusammenarbeit des Sprechers mit Gott, und die Prädikation (P) ist ""supplie the want,"" d.h. die Beschreibung der Leistung des Co-Autors. Co-Kreativität wird hier demnach als Aktivität beschrieben, in der ein Beteiligte:r die Mängel des oder der anderen ausgleicht. Die Annotation sieht wie folgt aus: ""Although the [verse] Ein erstes zentrales Ergebnis des Entwicklungsprozesses der Annotationsrichtlinien für die erste Komponente, die Akte und Artefakte (A), war die Feststellung, dass wir einen ganz offenen und neuen Minimalkonsens dafür schaffen mussten, worin ""Gemachtheit"" besteht. Als Grundlage für die Annotationsrichtlinien gilt daher: Die Annotation der Akte und Artefakte erforderte zudem ausführliche Angaben zu ihrer Verankerung im Text, wie z.B. die Regelung, dass wir maximal annotieren, d.h. dass wir alle syntaktischen Elemente, die einen Akt oder ein Artefakt spezifizieren, mitannotieren. Darüber hinaus führen die Annotationsrichtlinien eine Reihe von Spezialfällen auf, wie z.B. die Annotation von hypothetischen oder destruktiven Akten und Artefakten. Wenn alle Akte und Artefakte in einem Korpus erfasst und nummeriert wurden, erfolgt im zweiten Schritt die Annotation der Komponente ""CO"". Die Annotationsrichtlinien sehen hier vor: Da in einem Gedicht mehrere Akte und Artefakte mit unterschiedlichen COs auftreten können, gilt es an dieser Stelle im Annotationsprozess, die einzelnen Komponenten und ihre Zugehörigkeit zu einer Konstellation über verbindende Marker kenntlich zu machen. Die dritte Komponente, die Annotation der Prädikation (P), befindet sich momentan noch in der Erprobung. Grundsätzlich soll P eine Verhältnisbestimmung der einzelnen Komponenten beitragen. Über die Erfragung des Verhältnisses zwischen ""A"" und ""CO"" kommt man zur Prädikation. In Donnes ""A Valediction of Weeping"" (2008, 112) stehen der Sprecher und die Geliebte z.B. im wechselseitigen Abhängigkeitsverhältnis (""Since thou and I sigh one anothers breath,"" Z. 26). Diese wechselseitige Abhängigkeit zwischen den Beteiligten bildet also eine Aussage über einen co-kreativen Prozess, der nach erfolgter Annotation mit anderen Aussagen über ähnliche Prozesse bzw. mit ähnlichen Aussagen über andere co-kreative Prozesse verglichen werden kann. Das Ziel der P-Annotationen ist es, möglichst viele Aussagen über die Verhältnisse der annotierten Bislang wurden in unserem ersten Korpus mit fünfzig Gedichten alle Akte und Artefakte sowie aller CO-Akteure annotiert. Während die Erprobung der P-Annotationsrichtlinien läuft und das zweite 50er Korpus auf A und CO hin annotiert wird, können uns erste Auswertungen der vorhandenen A- und CO-Annotationen bereits Erkenntnisse liefern, die ohne diesen digitalen Zugang nicht ersichtlich wären und zudem das Potential dieser Annotationsmethode und des Die im Die Anzahl an Akten und Artefakten, die im Rahmen einer einzigen co-kreativen Aktivität erschaffen werden, bezeugt die Komplexität des untersuchten Phänomens. In manchen Fällen werden sogar bis zu neun verschiedene Akte und Artefakte in einer co-kreativen Konstellation kreiert. Die Vielschichtigkeit der Reflexionen, die wir zu erfassen suchen, wird auch in der folgenden Abbildung deutlich, die unterschiedliche CO-Konstellationen gemäß der Häufigkeit ihres Auftretens sortiert: Die Abbildung zeigt auf, welche CO-Akteure besonders häufig miteinander produktiv tätig sind: der Sprecher des Gedichts etwa tritt 41x als CO im Korpus auf, Gott 33x; gemeinsam sind sie in dieser Konstellation 12x co-kreativ tätig. Das Diagramm verschafft also einen ersten Eindruck in die unterschiedlichen CO-Konstellationen zwischen zwei oder mehr Akteuren, die wir in Reflexionen über Co-Kreativität antreffen. Die Datenlage zeigt insgesamt, dass diese Reflexionen in doppelter Hinsicht hochkomplex sind: sie involvieren mehrere COs in unterschiedlichen Konstellationen, und es werden meist die Schöpfungsprozesse von mehr als zwei Akten und Artefakten reflektiert. Unsere Annotationsmethode ermöglicht uns aber nicht nur die Erfassung der beteiligten COs und geschaffenen As, obgleich allein diese Daten bereits erkenntnisreich sind, wenn beispielsweise Untersuchungen zu den am häufigsten auftretenden Kollaborationspartnern Gottes erwünscht sind. Unser komplexes Annotationssystem bietet darüber hinaus die Möglichkeit, über die Annotation zusätzlicher Eigenschaften den Konstellationen bestimmte Kategorisierungen zuzuteilen. In diesem Korpus führten wir z.B. den sog. Marker ""enabling"" ein, da der heuristische Prozess des Annotierens und Revidierens der Annotationsrichtlinien bereits zur Aufstellung einer Hypothese führte: es schien, als würde Co-Kreativität zwischen Gott und Mensch auf einer Abhängigkeit des Menschen von Gott beruhen. Diese Abhängigkeit konnten wir weiter als eine Befähigung des Menschen zum Kreieren durch Gott spezifizieren. Das ganze Korpus wurde dahingehend untersucht und alle co-kreativen Reflexionen in denen ein CO das andere CO zum schöpferischen Prozess befähigt, erhielten den Marker ""enabling."" Da es sich nicht um eine spezifische Aussage über den Produktionsprozess handelt, wurde diese Abhängigkeit zwischen den Personen nicht als P annotiert. Die Kreisdiagramme zeigen, dass Gott in über 52% aller ""enabling""-Konstellationen, die fast ein Drittel aller co-kreativen Konstellationen ausmachen, partizipiert. Damit können wir eine gängige Annahme, dass Gott und Mensch in der frühen Neuzeit nicht als kreative Partner gedacht wurden, auf Grundlage unserer Annotationsmethode und der Auswertung widerlegen. Erkenntnisse dieser Art legen den Baustein für ein breiteres Verständnis eines vormodernen Konzepts der Co-Kreativität. Die Grundlage unserer digitalen Arbeit bilden die Konstellationen aus As und COs, die über Marker um Eigenschaften ergänzt werden, bspw. durch den ""enabling""-Marker oder Marker, die Selbstreferenzialität oder Metaphorik kennzeichnen. Die Einführung unserer dritten Komponente, der Prädikation P, wird zusätzliche Eigenschaften der co-kreativen Konstellationen aufzeigen. Der größte Gewinn dieser Annotationsarbeit ist also der große Datensatz an bereits erfassten co-kreativen Reflexionen, die mit Eigenschaftsmarkern versehen und deren Verhältnisse zueinander bestimmt wurden. Diese aufbereiteten Daten lassen uns erkennen, wie über co-kreative Prozesse gesprochen und gedacht wurde; das so entstandene Bild wird helfen, auch die Praxis der kreativen Zusammenarbeit neu zu betrachen. Die vorgestellte komplexe Annotationsmethode erlaubt es nicht nur, komplizierte Reflexionen in Einzelkomponenten herunterzubrechen und zu systematisieren. Sie bietet darüber hinaus methodische Ansätze für die Annotation weiterer komplexer literarischer Phänomene.",de,obwohl gemeinschaftlich Autorschaft früh neuzeit häufig Normalfall Forschungsliteratur weiterhin Konzept Einzelautorschaft lediglich Identifizierung Anteil individuell Autor insbesondere shakespear Gemeinschaftswerke fokussieren vicker aufkommen Verbesserung Digitaler Methode Bagatellisierung Konzept gemeinschaftlich Autorschaft führen stilometrisch Untersuchung dramatisch Text befördern Vorstellung gemeinschaftlich Autorschaft Summe Einzelautorschaft reduzieren Urheberschaft Stil gemeinschaftlich Autorschaft Summe Teil erforschen brauchen Idee herausfinden wissen früh Neuzeit denken Projekt östhetik gemeinschaftlich Autorschaft entschieden Beginn Arbeit kurz reflexionsdicht Text gedichen möglichst schnell Einblick verschieden Autor Werk erlangen Aufbereitung Arbeit zugrunde liegend Gedichtkorpora greifen überwiegend editiert Digitaler Form vorliegend werke bekannt frühneuzeitlich Autor bereiten Basis intern entwickelt Richtlinie Weiterarbeit aufgrund Komplexität Phänomen untersuchen nutzen Versuch Reflexion Gedicht annotieren basieren Expertenvorwissen Annotator innen suchen bausteinen kombinieren lauten Hypothese bestimmt Konzept bilden herberen Austausch gedichte Gott sprechen Konzept Baustein Geben Nehmen bilden analysieren Herausforderung Herangehensweise liegen Baustein Kenntnis Gedicht anzutreffend Konzept identifizieren Versuch bestehen synonyme bekannt Baustein unser Fallbeispielkorpora ausfindig manuell Modell Reflexion Prozesse Sonderfall Reflexion Produktionsvorgänge bilden mehrere Akteur beteiligen erfolgen Annotation Reflexion mehrere Schritt Text erwähnt Akt Produktion produziert Artefakt Blick nehmen Akteur co beruhen letzter Schritt verbunden Prädikation p Produktion Zusammenspiel Komponente bilden Konstellation verdeutlichen George Herberts dichen True Hymne although The verse be somewhaen scant god Doth Supplie The Want herbern vers geschaffen Artefakt impliziert Akt Schreiben basieren co bestehen Zusammenarbeit Sprecher gott Prädikation p supplie The Want Beschreibung Leistung demnach Aktivität beschreiben beteiligen r mängel ausgleichen Annotation sehen folgen although The verse zentral Ergebnis entwicklungsprozesses Annotationsrichtlinien Komponente akte artefakt Feststellung offen Minimalkonsen schaffen worin Gemachtheit bestehen Grundlage Annotationsrichtlinien gelten Annotation akte artefakt Erforderte zudem ausführlich Angabe Verankerung Text Regelung maximal annotieren syntaktisch elemenen Akt Artefakt Spezifizier mitannotieren hinaus fahren Annotationsrichtlinien Reihe Spezialfäll Annotation hypothetische destruktiv Akte artefakten Akt artefaken Korpus erfassen nummeriert erfolgen Schritt Annotation Komponente co Annotationsrichtlinien sehen dichen mehrere Akt artefaken unterschiedlich Co auftreten gelten Stelle Annotationsprozess einzeln Komponente Zugehörigkeit Konstellation verbindend Marker kenntlich Komponente Annotation Prädikation p befinden momentan Erprobung grundsätzlich p Verhältnisbestimmung einzeln Komponente beitragen Erfragung verhältnisses co Prädikation donnes valediction -- weeping stehen Sprecher geliebt wechselseitig Abhängigkeitsverhältnis since thou and i Sigh One Another Breath wechselseitig Abhängigkeit beteiligter bilden Aussage prozess erfolgt Annotation Aussage ähnlich prozeß ähnlich Aussage prozeß vergleichen Ziel möglichst Aussage Verhältnis Annotierte bislang unser Korpus fünfzig Gedicht Akt artefakt annotiert Erprobung laufen Korpus co annotiert Auswertung vorhanden Erkenntnis liefern digital Zugang ersichtlich sein zudem Potential Annotationsmethode Anzahl akte Artefakt Rahmen einzig Aktivität erschaffen bezeugen Komplexität untersucht Phänomen Fall sogar verschieden Akt artefaken Konstellation kreieren Vielschichtigkeit Reflexion erfassen suchen folgend Abbildung deutlich unterschiedlich gemäß Häufigkeit Auftreten sortieren Abbildung zeigen häufig miteinander produktiv tätig Sprecher Gedicht treten co Korpus gott gemeinsam Konstellation tätig Diagramm verschaffen Eindruck unterschiedlich akteuren Reflexion antreffen Datenlage zeigen insgesamt Reflexion Doppelter Hinsicht Hochkomplex involvieren mehrere Co unterschiedlich Konstellatione meist Schöpfungsprozess Akte Artefakt reflektieren Annotationsmethode ermöglichen Erfassung beteiligt cos geschaffen As obgleich daten erkenntnisreich beispielsweise Untersuchung häufig auftretende Kollaborationspartner gott erwünscht komplex Annotationssystem bieten hinaus Möglichkeit Annotation zusätzlich eigenschaften Konstellatione bestimmt Kategorisierung zuzuteilen Korpus führen Marker Enabling heuristisch Prozess Annotieren Revidieren Annotationsrichtlinie Aufstellung Hypothese führen scheinen gott Mensch Abhängigkeit Mensch Gott beruhen Abhängigkeit Befähigung Mensch Kreieren Gott spezifizieren Korpus dahingehend untersuchen Reflexion Co co schöpferischen Prozess befähigen erhalten Marker Enabling spezifisch Aussage Produktionsprozess handeln Abhängigkeit Person p annotieren Kreisdiagramm zeigen Gott fast Drittel konstellationen ausmachen partizipieren gängig Annahme Gott Mensch früh Neuzeit kreativ Partner denken Grundlage Annotationsmethode Auswertung widerlegen erkenntnis Art legen Baustein breiter Verständnis vormodern Konzept Grundlage digital Arbeit bilden Konstellatione As Cos Marker Eigenschafte ergänzen Marker Selbstreferenzialität Metaphorik Kennzeichn Einführung Komponente Prädikation p zusätzlich eigenschaften konstellationen aufzeigen groß Gewinn Annotationsarbeit Datensatz erfasst reflexionen eigenschaftsmarkern versehen Verhältnis Zueinander bestimmen Aufbereitet daten lassen erkennen prozesse sprechen denken entstanden Bild helfen Praxis kreativ Zusammenarbeit neu betrachen vorgestellt komplex Annotationsmethode erlauben kompliziert Reflexion einzelkomponent Herunterzubrech systematisieren bieten hinaus methodisch Ansatz Annotation weit Komplexer literarisch Phänomen,"[('co', 0.32452857066177226), ('gott', 0.25328825147765377), ('artefakt', 0.23945466911862845), ('reflexion', 0.20790982980337774), ('prädikation', 0.1838128753964091), ('akt', 0.16964287422084426), ('gemeinschaftlich', 0.16226428533088613), ('komponente', 0.1491867832953357), ('konstellation', 0.14705030031712726), ('annotationsmethode', 0.14705030031712726)]"
2024,DHd2024,20240108_HORSTMANN_Jan_InterAnnotator__Interfaces_f_r_die_Annotation_.xml,InterAnnotator: Interfaces für die Annotation intertextueller Relationen,"Jan Horstmann (Universität Münster, Deutschland); Christian Lück (Universität Münster, Deutschland); Immanuel Normann (Universität Münster, Deutschland); Jan-Erik Stange (Universität Münster, Deutschland)","Intertextualität, Annotation, Interface","Annotieren, Theoretisierung, Kollaboration, Text, Visualisierung, virtuelle Forschungsumgebungen","Theoriegetriebene Überlegungen zur Annotation von Intertextualität fortführend, wird ein Entwurf zum Ausgangspunkt war eine Erhebung über verschiedene Theorien der Intertextualität seit Einführung des Begriffs 1967 durch Julia Kristeva (Kristeva 1972) und das Herausarbeiten ihres gemeinsamen konzeptionellen Kerns (Horstmann, Lück, Normann 2023 und dies. angenommen). Dieser konzeptionelle Kern und einflussreiche Ausprägungen der Theorie (etwa Hypertextualität, Genette 1993), wurden in RDF/OWL formalisiert, was zu einer modularen Ontologie geführt hat. Die Kern-Ontologie beschreibt ein Datenmodell, dessen zentrale Relation die intertextuelle Relation ist: Sie ist eine gerichtete Relation von einem späteren Text bzw. einer Textstelle auf einen Avant-Text bzw. eine Stelle darin; sie verfügt über weitere Properties, insbesondere ist sie entsprechend einer theoretischen Ausprägung klassifizierbar (z.B. als Travestie im Sinne Genettes) und die Technisch gesehen ist der Während in unseren Vorarbeiten formale Methoden und das Datenmodell im Vordergrund standen, folgt die weitere Entwicklung des Tools einem offenen und iterativen Co-Kreationsprozess zur Visualisierung intertextueller Relationen nebst Konzentration auf Prinzipien der User-Experience. In einem Intertextualitätsnetzwerk sind Knoten ganze Texte und Textstellen. Es ist wünschenswert, in das Netzwerk Ausgehend von dieser Grundidee soll der  In Fig. 3 ist der Screen in zwei Bereiche unterteilt, links das Netzwerk, rechts die Textansicht. Aus dem Netzwerk lassen sich Texte in die Textansicht herüberziehen, um mit ihnen zu arbeiten. Durch das Markieren von Textstellen in expandierten Texten können Annotationen erzeugt werden, zwischen denen wiederum Verbindungen gezogen werden können. Rechts neben den lesbaren Texten werden diese als Balken sowie ihre Verbindungen als Bogendiagramm repräsentiert, was die Navigation intertextueller Relationen zweier Texte ermöglicht. Diese Konfiguration ist besonders nützlich bei Annotation und Analyse intertextueller Relationen einzelner Textstellen. Für hermeneutische Zugänge und Close-Reading-Ansätze ist sie besonders wertvoll. Netzwerk und der als Synopse organisierte Textbereich sind in Fig. 4 visuell stärker voneinander getrennt. In der Skizze sieht man fünf ausgewählte Texte, die jeweils eine eigene scrollbare Spalte zum Lesen haben und eine schmale, detailarme Balken-Repräsentation zur Navigation rechts daneben. Öhnlich wie in den anderen Varianten lassen sich parallel in mehreren Texten Stellen markieren, die dann miteinander verbunden werden können, um intertextuelle Relationen zu erfassen. Auch diese synoptische Konfiguration ist insbesondere für Close-Reading-Methoden geeignet. Öhnlich wie in der ersten Konfiguration startet Fig. 5 mit einer bildschirmfüllenden Ansicht des globalen Netzwerks. In diesem lassen sich Nodes auswählen (oder neue Nodes hinzufügen), um ein Korpus zusammenzustellen. Die ausgewählten Texte werden in einem weiteren Bereich als Balken radial angeordnet und bereits existierende Verbindungen zwischen ihnen angezeigt. Die Selektion eines Balkens öffnet eine lesbare Textansicht rechts daneben, mit der wiederum Querverbindungen zwischen Textstellen erzeugt werden können, welche im Radialdiagramm links daneben sichtbar werden.",de,theoriegetrieben Überlegung Annotation Intertextualität fortführend Entwurf Ausgangspunkt Erhebung verschieden Theorie Intertextualität Einführung Begriff Julia Kristeva Kristeva Herausarbeite gemeinsam konzeptionell Kerns Horstmann Lück Normann annehmen konzeptionell Kern einflussreich Ausprägung Theorie Hypertextualität Genette rdf owl formalisieren modular Ontologie führen beschreiben Datenmodell zentral Relation intertextuell Relation gerichtet Relation spät Text Textstelle Stelle verfügen properties insbesondere entsprechend theoretisch Ausprägung klassifizierbar Travestie Sinn genett technisch sehen unser vorarbeit formal Methode Datenmodell vordergrund stehen folgen Entwicklung Tool offen iterativ Visualisierung intertextuell Relation nebst Konzentration prinzipien Intertextualitätsnetzwerk knoten Text Textstelle wünschenswert Netzwerk ausgehend Grundidee fig Screen Bereich unterteilt links Netzwerk rechts textansichen Netzwerk lassen Text Textansicht herüberziehen arbeiten markieren Textstell expandiert Text Annotation erzeugen wiederum Verbindung ziehen rechts lesbaren texten Balke Verbindung Bogendiagramm repräsentieren Navigation Intertextueller relationen zwei Text ermöglichen Konfiguration nützlich Annotation Analyse Intertextueller Relation einzeln Textstelle hermeneutisch Zugäng wertvoll netzwerk Synopse Organisiert textbereich fig Visuell stark voneinander trennen Skizze sehen ausgewählt Text jeweils Scrollbare spalen lesen schmal detailarm Navigation rechts öhnlich Variant lassen parallel mehrere Text Stelle markieren miteinander verbinden intertextuell Relation erfassen synoptisch Konfiguration insbesondere geeignet öhnlich Konfiguration starten fig bildschirmfüllend Ansicht global Netzwerks lassen node auswählen node hinzufügen Korpus zusammenzustellen ausgewählt Text Bereich Balken Radial anordnen existierend Verbindung anzeigen Selektion Balken öffnen lesbar Textansicht rechts wiederum querverbindung Textstell erzeugen Radialdiagramm links sichtbar,"[('relation', 0.2480289613963464), ('rechts', 0.2332597999881194), ('konfiguration', 0.18944526792262184), ('netzwerk', 0.18580421877109188), ('intertextuell', 0.17107006858640078), ('fig', 0.17107006858640078), ('textansicht', 0.16025042481035884), ('textstelle', 0.15104871587653196), ('kristeva', 0.14926112185326212), ('navigation', 0.1414640873211201)]"
2024,DHd2024,BOENIG_Matthias_Edierst_Du_noch_oder_trainierst_Du_schon__Fo.xml,Edierst Du noch oder trainierst Du schon? Forschungsdaten als Grundlage von Trainingsdaten für die automatische Texterkennung,"Matthias Boenig (Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland); Konstantin Baierer (Staatsbibliothek zu Berlin 'Preußischer Kulturbesitz, Deutschland); Lena Hinrichsen (Herzog August Bibliothek Wolfenbüttel, Deutschland); Kay-Michael Würzner (Sächsische Landesbibliothek ‚Äî Staats- und Universitätsbibliothek Dresden (SLUB), Deutschland); Christian Reul (Zentrum für Philologie und Digitalität der Universität Würzburg, Deutschland)","Standard, Ground-Truth, OCR","Datenerkennung, Transkription, Inhaltsanalyse, Strukturanalyse, Annotieren, Archivierung","Wichtigste Grundlage der textorientierten Forschung in den Digital Humanities ist eine ausreichende Verfügbarkeit von hochwertigem maschinenlesbarem Text. Diese Anforderung kann bei grundständig digitalen Texten häufig einfacher erfüllt werden als bei historischen Texten, wo zunächst die Transformation vom gedruckten oder geschriebenen Wort auf Papier in eine geeignete digitale Repräsentation zu realisieren ist. Mit der Anwendung von Verfahren des maschinellen Lernens in der automatischen Texterkennung ist in den letzten zehn Jahren ein enormer Fortschritt vollzogen worden. Dies betrifft vor allem die Zeichenerkennung und deren Genauigkeit. Hierbei kommen Methoden zum Einsatz, die dem Paradigma Aber GT dient nicht nur dem Training der Zeichenerkennung (sowohl dem Training eines neuen Modells ""from scratch"", als auch dem ""Finetuning"" eines bestehenden Modells auf einen spezifischen Anwendungsfall hin), sondern wird auch zur Datenvalidierung, -evaluation und -referenzierung eingesetzt. Neben der Zeichenerkennung können aber weitere Teilprozesse der automatischen Texterkennung vom Einsatz maschinellen Lernens profitieren. Dies gilt insbesondere für die Erkennung und Auszeichnung der Seitenstruktur bzw. des Seitenlayouts. Diese unterschiedlichen Anwendungen setzen differenzierte GT-Typen voraus. Allgemein kann zwischen Struktur-GT und Text-GT unterschieden werden. Die Erstellung von GT erfolgt zu einem Großteil manuell, was einen hohen zeitlichen und finanziellen Aufwand erfordert. Um brauchbaren GT zu erstellen, sind abgestimmte Konventionen und Richtlinien notwendig. Aus diesem Grund entwickelt, pflegt, vermittelt und diskutiert das Projekt OCR-D Im Rahmen des vorgeschlagenen Workshops soll eine solche offene Datenkultur am Beispiel von Forschungsdaten des Deutschen Textarchivs (DTA) Das DTA wurde im Rahmen eines sprachwissenschaftlich orientierten DFG-Projektes erstellt. Der Kernbestand besteht aus 1500 Druckpublikationen mit einem Gesamtumfang von 540.000 Seiten. Die Text- und Textsortenauswahl, die zeitliche Spanne des Publikationszeitraumes vom frühen 17. bis frühen 20. Jahrhundert, die Verwendung von Erstausgaben und die vorlagengetreue Transkription kennzeichnen diesen Bestand als Grundlage eines Referenzkorpus der frühneuhochdeutschen Sprache. Die Bereitstellung der digitalen Texte erfolgt sowohl in einem XML-basierten Format als auch als unannotierter Rohtext. Für die Einschätzung der Nutzbarkeit des DTA als Quelle für GT sind nicht nur die Ergebnisdaten relevant. Ein genauerer Blick auf die einzelnen Etappen des ursprünglichen Datenerfassungsworkflows im DTA zeigt bisher ungenutzte Potenziale der einzelnen Datenstände als Trainingsmaterialien für Text- und Strukturerkennung. Die folgende Abbildung illustriert die beiden grundsätzlichen Wege der Volltexterstellung, die im DTA zur Anwendung kamen: Automatische Texterkennung mit anschließender Nachkorrektur (""OCR way"") und manuelle Transkription im Vier-Augen-Prinzip (""Double Keying way"").  Letzterer kam für den Großteil des Bestands zur Anwendung. Das Double-Keying-Verfahren wurde von Nicht-Muttersprachlern vorgenommen und ist sehr genau. Die Zeichengenauigkeit kann mit 99,99 % angesetzt werden (Haaf, 2013; Geyken, 2012). Mit OCR wurden hauptsächlich Titel des 19. und Mitte des 18. Jahrhunderts erfasst. Beiden Wegen gemein ist ein manueller Segmentierungsschritt. In diesem wurden Textzonen und Abbildungen lokalisiert und klassifiziert. Diese Segmentierung diente zwar ""nur"" der nachträglichen Auszeichnung der Volltexte im XML (und nicht etwa der Unterstützung der automatischen Texterfassung). Sie bilden aber dennoch eine der größten bekannten Sammlungen an Strukturdaten für historische deutschsprachige Drucke. Aus der Untersuchung des Datenerfassungsworkflows können somit Segmentierungsdaten und Textdaten identifiziert werden, die für die Verwendung als GT in Frage kämen. Größtes Manko der Datensammlung ist jedoch die fehlende Verknüpfung zwischen Text und Bild, die die Einsatzszenarien als Trainingsdaten massiv einschränkt. An dieser Stelle setzt der vorgeschlagene Workshop an. Die Teilnehmenden des Workshops werden mit Verfahren und Methoden der Erstellung, Erschließung und Speicherung von GT für die automatische Texterkennung vertraut gemacht. Der Workshop ist in zwei Teile geteilt: einen theoretischen und einen praktischen. Ziel des theoretischen Teils ist, dass die Teilnehmenden in die Lage versetzt werden, anhand einer Liste von Kriterien sowie einer Validierung der Daten, Forschungsdaten für die Erstellung von GT einzuschätzen. Mit den OCR-D-GT-Richtlinien bekommen die Teilnehmenden eine in der Praxis erprobte Anleitung für die Erstellung von GT zur Verfügung gestellt. Inhalt und Aufbau, aber auch die Möglichkeiten der praktischen Anwendung dieser Richtlinien im jeweiligen Projekt bilden in diesem ersten Workshopteil den Schwerpunkt. Im praktischen Teil sollen nun die Teilnehmenden in verschiedenen Szenarien GT-Daten erstellen. Dabei werden Forschungsdaten des DTA und vorhandener GT geprüft und eingeschätzt. Dazu werden die im theoretischen Teil vorgestellten Metriken und Validierungsmethoden angewendet. Mit Transformations- und Konvertierungsprogrammen kann in der Folge nun der GT automatisiert erstellt werden. Ebenfalls können spezielle Softwareprogramme für die manuelle Erstellung von GT verwendet werden. Um sich sowohl mit dem Funktionsumfang als auch mit der Leistungsfähigkeit der Tools vertraut zu machen, ist es notwendig, diese im theoretischen Teil kennenzulernen. Der unmittelbare Umgang und die Handhabung des Tools für die GT-Erstellung stehen nicht im Mittelpunkt, sondern die Entscheidung, welches Tool für das jeweilige Vorhaben am geeignetsten scheint. Zum Abschluss steht die Speicherung des GT in einem Repositorium. So können die Daten entsprechend der FAIR-Prinzipien zugänglich gemacht werden. Erklärungen zum Aufbau des Repositoriums sowie die Erschließung mit Metadaten, die Nutzung des OCR-D-GT-Repo-Template Den Teilnehmenden des Workshops sollen verschiedene Methoden und Verfahren der GT-Erstellung vorgestellt werden. die jeweiligen Teilnehmenden verfügen über: Der Raum verfügt über:   ",de,wichtig Grundlage textorientierten Forschung Digital Humanitie ausreichend Verfügbarkeit hochwertig Maschinenlesbarem Text Anforderung grundständig digital Text häufig einfach erfüllen historisch Text Transformation gedruckten geschrieben Wort Papier geeignet digital Repräsentation realisieren Anwendung Verfahren maschinell Lernen automatisch Texterkennung letzter enorm Fortschritt vollziehen betreffen Zeichenerkennung Genauigkeit hierbei Methode Einsatz Paradigma gt dienen Training Zeichenerkennung sowohl Training Modell from Scratch Finetuning bestehend Modell spezifisch Anwendungsfall Datenvalidierung einsetzen Zeichenerkennung Teilprozesse automatisch Texterkennung Einsatz maschinell Lernen profitieren gelten insbesondere Erkennung Auszeichnung Seitenstruktur Seitenlayout unterschiedlich Anwendung setzen differenziert voraus allgemein unterscheiden Erstellung Gt erfolgen Großteil manuell hoch zeitlich finanziell Aufwand erfordern Brauchbar gt erstellen abgestimmt Konvention Richtlinie notwendig Grund entwickeln pflegen vermitteln diskutieren Projekt Rahmen vorgeschlagen Workshop offen Datenkultur Forschungsdat deutsch Textarchivs dta Dta Rahmen sprachwissenschaftlich orientiert erstellen Kernbestand bestehen Druckpublikation Gesamtumfang Seite Textsortenauswahl zeitlich Spanne Publikationszeitraume früh früh Jahrhundert Verwendung Erstausgabe vorlagengetreu Transkription Kennzeichn Bestand Grundlage Referenzkorpus frühneuhochdeutsch Sprache Bereitstellung digital Text erfolgen sowohl Format unannotiert Rohtext Einschätzung Nutzbarkeit dta Quelle gt ergebnisdat relevant genauer Blick einzeln etappen ursprünglich Datenerfassungsworkflow Dta zeigen ungenutzt Potenziale einzeln Datenstände Trainingsmateriali Strukturerkennung folgend Abbildung illustrieren grundsätzlich Weg Volltexterstellung Dta Anwendung kommen automatisch Texterkennung anschließend Nachkorrektur ocr way Manuelle Transkription double Keying way letzterer Großteil Bestand Anwendung vornehmen genau Zeichengenauigkeit ansetzen haaf geyken ocr hauptsächlich Titel Mitte Jahrhundert erfassen gemein manuell Segmentierungsschritt Textzon Abbildung lokalisiern klassifizieren Segmentierung dienen nachträglich Auszeichnung Volltexte xml Unterstützung automatisch Texterfassung bilden dennoch groß bekannt Sammlung Strukturdat historisch deutschsprachig Druck Untersuchung Datenerfassungsworkflow somit segmentierungsdat Textdat identifizieren Verwendung gt Frage kommen groß Manko Datensammlung fehlend Verknüpfung Text Bild einsatzszenarium Trainingsdaten massiv einschränken Stelle setzen vorgeschlagen Workshop Teilnehmende Workshop Verfahren Methode Erstellung Erschließung Speicherung Gt automatisch texterkennung vertraut Workshop Teil teilen theoretisch praktisch Ziel theoretisch Teil Teilnehmende Lage versetzen anhand Liste kriterien Validierung daten Forschungsdat Erstellung gt einschätzen bekommen Teilnehmende Praxis erprobt Anleitung Erstellung Gt Verfügung stellen Inhalt Aufbau Möglichkeit praktisch Anwendung Richtlinie jeweilig Projekt bilden Workshopteil schwerpunken praktisch Teilnehmend verschieden Szenarie erstellen Forschungsdat dta vorhanden gt prüfen einschätzen theoretisch vorgestellt metriken validierungsmethoden anwenden konvertierungsprogrammen Folge Gt automatisiert erstellen ebenfalls speziell Softwareprogramm Manuelle Erstellung Gt verwenden sowohl Funktionsumfang Leistungsfähigkeit Tools vertraut notwendig theoretisch kennenlernen unmittelbar Umgang Handhabung Tools stehen Mittelpunkt Entscheidung Tool jeweilig Vorhaben eignetsten scheinen Abschluss stehen Speicherung gt Repositorium daten entsprechend zugänglich Erklärung Aufbau Repositorium Erschließung Metadat Nutzung Teilnehmende Workshop verschieden Methode Verfahren vorstellen jeweilig Teilnehmende verfügen Raum verfügen,"[('gt', 0.6093472305046093), ('dta', 0.20084753381934797), ('teilnehmende', 0.1619983277503294), ('texterkennung', 0.16007993527417155), ('zeichenerkennung', 0.13447819237764447), ('erstellung', 0.1335158818920033), ('workshop', 0.12977953568376757), ('anwendung', 0.10784422735443047), ('forschungsdat', 0.10617295689395251), ('datenerfassungsworkflow', 0.1015578717507682)]"
2024,DHd2024,JACKE_Janina_Agreement_und_Kookkurrenz_bei_unzuverl_ssigem_E.xml,"Agreement und Kookkurrenz bei unzuverlässigem Erzählen. Ziele, Herausforderungen und erste Ergebnisse aus dem Projekt CAUTION","André Blessing (Universität Stuttgart, Deutschland); Janina Jacke (Georg-August-Universität Göttingen, Deutschland); Jonas Kuhn (Universität Stuttgart, Deutschland)","Annotation, Inter-Annotator Agreement, Literaturwissenschaft","Beziehungsanalyse, Modellierung, Annotieren, Theoretisierung, Literatur","Ziel des Projekts CAUTION (""Computer-aided Analysis of Unreliability and Truth in Fiction 'Interconnecting and Operationalizing Narratology"") ist die computergestützte Auseinandersetzung mit dem erzähltheoretischen Konzept des unzuverlässigen Erzählens. Unzuverlässiges Erzählen liegt dann vor, wenn die fiktive Erzählinstanz eines fiktionalen Textes unwahre Öußerungen über die fiktive Welt des Textes tätigt (vgl. Shen 2011). Ziel des Projekts ist es, in einem experimentellen Mixed-Methods-Setting Erkenntnisziele auf unterschiedlichen Ebenen zu erreichen. Zum einen soll geprüft werden, ob eine computationelle Modellierung des Konzepts (oder zumindest eine literaturwissenschaftlich nützliche Annäherung, vgl. Jacke 2023) möglich ist. Zum anderen geht es um die Erforschung bestimmter theoretischer und methodologischer Fragen, die mit dem Konzept verbunden sind und deren Klärung eine potenzielle reflektierte computationelle Modellierung informieren kann: Wie interpretationsabhängig ist die Feststellung von unzuverlässigem Erzählen in literarischen Texten? Und welche Rolle spielen formale bzw. sprachliche Textmerkmale (textuelle Indikatoren) bei der Feststellung? Nach einer genaueren Vorstellung der Erkenntnisziele (Abschnitt 2) und der sich daraus ergebenden Projektkonzeption (Abschnitt 3) sollen erste Ergebnisse der Auswertung im Projekt erstellter Annotationen 'mit besonderem Fokus auf Inter-Annotator Agreement und Annotationskookkurrenz 'präsentiert und diskutiert werden (Abschnitt 4). Abschließend wird ein vorläufiges Fazit zu den im Projekt zutage tretenden Herausforderungen bezüglich adäquater Standards und Methoden bei der Evaluation statistischer Ergebnisse in der computationellen Literaturwissenschaft gezogen (Abschnitt 5). Das Konzept des unzuverlässigen Erzählens stellt eine besondere Herausforderung für die computationelle Modellierung dar, denn es wird in der Literaturwissenschaft gemeinhin als stark interpretationsabhängiges Konzept verstanden (vgl. Yacobi 1981; Kindt 2008: 53-67). Gleichzeitig werden aber auch Listen von (teilweise sprachlichen) Indikatoren zusammengestellt, die auf unzuverlässiges Erzählen hindeuten können (vgl. Nünning 1998). Dabei lassen sich in der Regel aber keine genauere Angaben darüber finden, wie relevant oder verlässlich die einzelnen Indikatoren sind. Eine weitere Herausforderung besteht darin, dass das Konzept meist als Kategorie zur Einordnung von Texten oder Erzählfiguren genutzt wird, zugleich aber sowohl eine Verwendung als Analysekategorie zur Einordnung von Textsegmenten als auch eine graduelle Anwendung des Konzepts naheliegen (vgl. Jacke 2020: 173-183). Vor diesem Hintergrund stehen im hier vorgestellten Projektteil von CAUTION folgende Fragen und Hypothesen im Zentrum: Um im Hinblick auf die im vorangegangenen Abschnitt genannten Fragen zumindest erste Einblicke zu erzielen, wird eine Variante der Methode der kollaborativen Annotation (vgl. Gius und Jacke 2017) eingesetzt. Das Kernkorpus für diesen Projektteil setzt sich aus neun Im Rahmen des annotationsbasierten Projektteils gibt es zwei Annotationsaufgaben: Die erste zielt auf die Feststellung ausgewählter Indikatoren, die andere auf die Identifikation von unzuverlässigem Erzählen. Jeder Korpustext wird von mindestens zwei Annotator:innen auf der Basis gemeinsamer Annotationsrichtlinien bearbeitet. Allgemeine Annotationsprobleme werden regelmäßig gemeinsam diskutiert und die Richtlinien werden entsprechend überarbeitet. Sobald jeder Text in einer ersten Runde bearbeitet worden ist, erfolgt eine Überarbeitungsrunde, die der finalen Version der Richtlinien folgt. Individuelle Annotationsentscheidungen werden nicht diskutiert oder von den Annotator:innen verglichen, aber es besteht die Möglichkeit, Gründe oder Unsicherheiten bei der Annotation als Kommentar im Text zu vermerken. Zum Verfassungszeitpunkt dieses Beitrags ist die erste Annotationsaufgabe abgeschlossen. Für die zweite Aufgabe liegen Ergebnisse für einige der Korpustexte vor.  Während sich das so errechnete Agreement auf diese Weise noch nicht uneingeschränkt mit üblicherweise in den Feldern der Computerlinguistik und der computationellen Literaturwissenschaft erzielten Werten vergleichen lässt, lassen sich zumindest projektintern interessante Beobachtungen treffen: So kann beispielsweise festgestellt werden, dass 'entgegen unserer ursprünglichen Hypothese 'unzuverlässiges Erzählen anscheinend mit einem höheren Agreement festgestellt wird als die sprachnahen Indikatoren (Abb 2). Mögliche Erklärungen könnten sein, dass Erzähler:inneneigeschaften wie Emotionalität sich schwieriger an genauen Textstellen festmachen lassen als eine inkorrekte Öußerung (also Unzuverlässigkeit). Außerdem ist anzunehmen, dass die Eigenschaften aus der ersten Annotationsaufgabe öfter in Abstufungen vorliegen und die Annotator:innen möglicherweise unterschiedliche intuitive Schwellenwerte haben, ab denen sie eine Annotation vornehmen. Auffällig ist weiterhin, dass 'innerhalb der ersten Annotationsaufgabe (Abb. 3) 'für die Eigenschaft Das hohe Agreement ließe sich dadurch erklären, dass das Bewusstsein einer Erzählinstanz, sich in einer kommunikativen Situation zu befinden, sich in den meisten Fällen tatsächlich an linguistischen Texteigenschaften festmachen lässt 'beispielsweise an direkter Adressat:innenansprache unter Verwendung der zweiten Person bei Personalpronomen und Verbformen. Diese Hypothese ließe sich zukünftig durch Anwendung geeigneter Computermodelle auf das Korpus überprüfen, die auf die automatische Feststellung potenziell relevanter sprachlicher Eigenschaften zielen.  Um die Indikationskraft der Erzähler:inneneigenschaften weiter zu prüfen, könnte zukünftig genauer untersucht werden, ob eine bestimmte Kombination von Indikatorkategorien besonders häufig mit dem Auftreten von Unzuverlässigkeit korreliert. Zudem sollen neben der hier in einem ersten Zugang skizzierten textstellenbasierten Analyse der Kookkurrenzen auch die Gesamttexte untersucht werden, um festzustellen, ob die Indikatorphänomene gehäuft in Texten mit vielen Vorkommnissen von Unzuverlässigkeit auftreten 'auch wenn die genauen Textstellen nicht notwendigerweise übereinstimmen. Die hier präsentierten ersten Auswertungen der im Projekt CAUTION durchgeführten Annotationen zeigen, dass sich besondere Herausforderungen aus dem doppelten Erkenntnisinteresse ergeben: Es sollen sowohl computerlinguistische Erkenntnisse hinsichtlich der computationellen Modellierbarkeit von unzuverlässigem Erzählen als auch literaturtheoretische Einsichten in die Interpretationsabhängigkeit des Konzepts und seine Relation zu bestimmten sprachnahen Indikatoren gewonnen werden. Obwohl die Erkenntnisinteressen in weiten Teilen Schnittflächen aufweisen, ergeben sich bei der genaueren Konzeption der Annotationsaufgaben sowie bei der Auswertung tendenziell Diskrepanzen u.a. im Zusammenhang mit Korpusgröße, Annotationstools, annotierten (oder nicht-annotierten bzw. nicht-segmentierten) Einheiten, der Wahl eines Agreement-Maßes und der Beurteilung der Agreementwerte. Dennoch zeichnen sich erste interessante Tendenzen ab. So ist aus literaturtheoretischer Perspektive bemerkenswert, dass bei der Feststellung des vermeintlich stark interpretationsabhängigen unzuverlässigen Erzählens tendenziell ein höheres Agreement erzielt werden konnte als bei der Identifikation vermeintlich sprachnaher Indikatoren. Darüber hinaus scheinen die Indikatoren insgesamt eine schwache positive Indikationskraft für Unzuverlässigkeit zu haben, wobei insbesondere Emotionalität und ein betont sicheres Auftreten der Erzählinstanz wichtige Rollen spielen könnten. Nächste Schritte könnten sich der Analyse von textbasierten (im Gegensatz zu textstellenbasierten) Kookkurrenzen zuwenden. Außerdem ist die genauere Analyse von besonders auffälligen Einzeltexten und Textstellen interessant 'beispielsweise dort, wo unzuverlässiges Erzählen ohne die untersuchten Indikatoren auftritt. Hierfür eignen sich besonders interaktive Visualisierungen, die Annotationen im Textverlauf abbilden und ein Close Reading von Text und Annotator:innenkommentaren zulassen (vgl. Münz-Manor und Marienberg-Milikowsky 2023). Weitere Auswertungen könnten sich der Frage zuwenden, ob die literaturwissenschaftlichen Einschätzungen zur Unzuverlässigkeit der Korpustexte mit der Häufigkeit von Unzuverlässigkeitsannotationen in den jeweiligen Texten im Einklang stehen und welche weiteren Faktoren zusätzlich zur Frequenz ggf. einzubeziehen wären. Im Zusammenhang mit der computationellen Modellierbarkeit ist interessant, dass sich die Unzuverlässigkeitsannotationen durch ihr höheres Agreement tendenziell eher für ein Trainieren von Modellen eignen als 'wie ursprünglich angenommen 'die annotierten Indikatoren. Potenziell schwierig ist bei diesem Szenario aber die Tatsache, dass die Unzuverlässigkeitsannotationen auf dem Verstehen der ausgedrückten Proposition und einer Vorstellung von der erzählten Welt basieren. Vor diesem Hintergrund erscheint ein paralleles Experimentieren mit Modellen zur automatischen Feststellung relevanter Indikatoren sinnvoll.",de,Ziel Projekt Caution Analysis of unreliability and Truth Fiction interconnecting and Operationalizing narratology computergestützt Auseinandersetzung erzähltheoretisch Konzept unzuverlässig Erzählen unzuverlässig erzählen liegen fiktiv Erzählinstanz fiktional Text unwahr öußerungen fiktiv Welt Text tätigen sh Ziel Projekt experimentell erkenntniszieln unterschiedlich Ebene erreichen prüfen computationelle Modellierung Konzept zumindest literaturwissenschaftlich nützlich Annäherung jack Erforschung bestimmt Theoretischer methodologisch Frage Konzept verbinden Klärung Potenzielle reflektiert computationell Modellierung informieren interpretationsabhängig Feststellung unzuverlässig erzählen literarisch Text Rolle spielen formal sprachlich Textmerkmal Textuelle indikatoren Feststellung genau Vorstellung Erkenntnisziele abschneten ergebend Projektkonzeption Abschnitt Ergebnis Auswertung Projekt erstellt annotation besonder Fokus agreement Annotationskookkurrenz präsentieren diskutieren abschneten abschließend vorläufig Fazit Projekt zutage tretend Herausforderung bezüglich adäquat Standard Methode Evaluation statistisch Ergebnis computationell Literaturwissenschaft ziehen abschneten Konzept unzuverlässig erzählens stellen besonderer Herausforderung computationelle Modellierung dar Literaturwissenschaft gemeinhin stark interpretationsabhängig Konzept verstehen Yacobi kindt gleichzeitig liste teilweise sprachlich indikatoren zusammenstellen unzuverlässig erzähl Hindeute nünning lassen Regel genau Angabe finden relevant Verlässlich einzeln indikatoren Herausforderung bestehen Konzept meist Kategorie Einordnung Text erzählfiguren nutzen sowohl Verwendung Analysekategorie Einordnung Textsegment graduell Anwendung Konzept nahelieg Jacke Hintergrund stehen vorgestellt Projektteil Caution folgend Frage Hypothesen Zentrum Hinblick vorangegangen Abschnitt genannt Frage zumindest einblicke erzielen Variante Methode kollaborativ Annotation Gius Jacke einsetzen Kernkorpus Projektteil setzen Rahmen annotationsbasiert Projektteil Annotationsaufgabe zielen Feststellung Ausgewählter indikatoren Identifikation unzuverlässig erzählen Korpustext mindestens Annotator innen Basis gemeinsam Annotationsrichtlini bearbeiten allgemein annotationsprobleme regelmäßig gemeinsam diskutieren Richtlinie entsprechend überarbeiten sobald Text Runde bearbeiten erfolgen überarbeitungsrund final Version Richtlinie folgen individuell Annotationsentscheidung diskutieren Annotator innen vergleichen bestehen Möglichkeit gründe Unsicherheit Annotation Kommentar Text vermerken Verfassungszeitpunkt Beitrag Annotationsaufgabe abschließen Aufgabe liegen Ergebnis Korpustexte errechnet Agreement Weise uneingeschränkt üblicherweise feldern Computerlinguistik computationell Literaturwissenschaft erzielt wert Vergleich lässen lassen zumindest Projektinter interessant Beobachtung treffen beispielsweise feststellen entgegen ursprünglich Hypothese unzuverlässiges erzählen anscheinend hoch agreement feststellen sprachnah indikatoren abb möglich erklärungen können erzähl inneneigeschaften Emotionalität schwierig genau textstell festmachen lassen inkorrekt Öußerung unzuverlässigkeit annehmen eigenschaften Annotationsaufgabe öfter Abstufung vorliegen Annotator innen möglicherweise unterschiedlich intuitiv schwellenwerte Annotation vornehmen auffällig weiterhin innerhalb Annotationsaufgabe abb Eigenschaft hoch agreement lassen erklären Bewusstsein erzählinstanz kommunikativ Situation befinden meister Fall tatsächlich linguistisch texteigenschaft festmach Lässt beispielsweise direkt Adressat innenansprache Verwendung Person Personalpronome verbformen Hypothese lassen zukünftig Anwendung geeignet Computermodelle Korpus überprüfen automatisch Feststellung Potenziell Relevanter Sprachlicher eigenschaft Ziel Indikationskraft Erzähler Inneneigenschaft prüfen zukünftig genau untersuchen bestimmt Kombination Indikatorkategorien häufig auftret Unzuverlässigkeit korrelieren zudem Zugang skizziert textstellenbasiert Analyse kookkurrenzen gesamttext untersuchen feststellen indikatorphänomen häufen Text Vorkommnisse Unzuverlässigkeit auftreten genau Textstelle notwendigerweise übereinstimmen Präsentiert Auswertung Projekt Caution durchgeführen annotationen zeigen besonderer Herausforderung doppelt Erkenntnisinteresse ergeben sowohl computerlinguistisch erkenntnis hinsichtlich computationell Modellierbarkeit unzuverlässig erzählen literaturtheoretisch Einsicht Interpretationsabhängigkeit Konzept Relation bestimmt Sprachnahe indikatoren gewinnen obwohl Erkenntnisinteresse weit Teil schnittflächen Aufweis ergeben genaueren Konzeption Annotationsaufgabe Auswertung tendenziell diskrepanz Zusammenhang Korpusgröße annotationstools annotierten einheiten Wahl Beurteilung Agreementwerte dennoch zeichnen interessant Tendenz literaturtheoretisch Perspektive bemerkenswern Feststellung vermeintlich stark interpretationsabhängig unzuverlässig erzählens tendenziell hoch agreement erzielen Identifikation vermeintlich sprachnah indikatoren hinaus scheinen indikatoren insgesamt schwach positiv Indikationskraft Unzuverlässigkeit wobei insbesondere Emotionalität Betont sicher auftreten Erzählinstanz wichtig Rolle spielen können nächster Schritt können Analyse Textbasiert Gegensatz textstellenbasierten kookkurrenzen zuwenden genau Analyse auffällig einzeltext Textstellen interessant beispielsweise unzuverlässiges erzählen untersucht indikator auftreten hierfür eignen interaktiv visualisierung Annotation Textverlauf abbilden clos Reading Text Annotator innenkommentar zulassen Auswertung können Frage zuwenden literaturwissenschaftlich Einschätzung Unzuverlässigkeit Korpustexte Häufigkeit unzuverlässigkeitsannotationen jeweilig Text einklang stehen Faktor zusätzlich Frequenz einbeziehen sein Zusammenhang computationell Modellierbarkeit interessant unzuverlässigkeitsannotationen hoch agreement tendenziell eher Trainier Modell eignen ursprünglich annehmen Annotierte indikatoren potenziell schwierig Szenario Tatsache unzuverlässigkeitsannotationen verstehen ausgedrückt Proposition Vorstellung erzählt Welt basieren Hintergrund erscheinen parallel Experimentieren Modell automatisch Feststellung Relevanter indikatoren sinnvoll,"[('unzuverlässig', 0.29671979334187576), ('indikatoren', 0.2871441734584683), ('unzuverlässigkeit', 0.19567125687587988), ('feststellung', 0.19298670037258092), ('erzählen', 0.19067248169521422), ('agreement', 0.18347271282268268), ('annotationsaufgabe', 0.17104363634769168), ('computationell', 0.14641601581950345), ('konzept', 0.13735370752108209), ('unzuverlässigkeitsannotationen', 0.12604649482011634)]"
2025,DHd2025,TUMFART_Barbara_Die_Insel___eine_digitale_Zeitschriftenediti.xml,Die Insel 'eine digitale Zeitschriftenedition,"Barbara Tumfart (Österreichische Akademie der Wissenschaften, Österreich); Silvia Waltl (Österreichische Akademie der Wissenschaften, Österreich)","Zeitschrift, Edition, Jahrhundertwende, Literatur, Buchkunst, Belletristik, Normdaten","Transkription, Annotieren, Veröffentlichung, Visualisierung, Literatur, Text"," Das in der Abteilung ""Die Insel"" wurde als ""ästhetisch-belletristische Monatsschrift"" zwischen Oktober 1899 und September 1902 im Berliner Verlag Schuster & Löffler als Teil des Insel-Verlags, ab Jahrgang 3 (1901/02) im Leipziger Insel-Verlag herausgegeben und gilt als eine der wichtigsten Zeitschriften der Jahrhundertwende und der Literatur der Moderne. Unter wechselnder Redaktion (Otto Julius Bierbaum, Alfred Walter Heymel, Rudolf Alexander Schröder) publizierten in der ""Insel"" unter anderem namhafte Autoren wie Franz Blei, Richard Dehmel, Heinrich Mann, Rainer Maria Rilke, Felix Salten, Robert Walser und Frank Wedekind. In der ""Insel"" finden sich diverse belletristische Textsorten in Prosa, Lyrik und in dramatischer Form als Einzelveröffentlichungen oder in Fortsetzungen, darunter Novellen, Erzählungen, Skizzen, dramatische Werke, Gedichte, Aufsätze und Aphorismen, außerdem kunst- und kulturtheoretische und -historische Abhandlungen, Rezensionen und Kritiken. Die Zeitschrift beinhaltet darüber hinaus zahlreiche Übersetzungen. Besondere Bedeutung kommt der Buchkunst in Form von Illustrationen, Drucken, Holzschnitten und dergleichen zu. Aus der Zeitschrift mit dem Signet des Segelschiffs ging später der ""Insel-Verlag"" hervor. Die Herausgeber der Insel sahen sich vor dem Hintergrund der Jahrhundertwende einem neuen Kunstverständnis verpflichtet. Die formenreiche Ausgestaltung des Buchschmucks sowie typographische Besonderheiten zeichnen die ""Insel"" zusätzlich aus und stellen zugleich eine besondere Herausforderung bei der Erstellung der Edition dar; mit Jahrgang 2 (1900/01) wechselt die Drucktype zudem von Fraktur zu Antiqua. Als Grundlage für die Datengenerierung zur Editionserstellung dienten die digitalen Bestände des Schwerpunkts zu Zeitungen und Zeitschriften im Austrian Academy Corpus (AAC). Diese Digitalisate basieren auf der Produktion hochauflösender Scans mit Zeutschel¬Æ-Buchscannern zur Herstellung von Faksimiles im unkomprimierten TIFF-Format, optimiert zur OCR-Erkennung. Neben den Schwarz-Weiß-Scans wurden auch Aufnahmen der Umschläge in Vollfarbe angefertigt. Zur Volltext-Gewinnung aus den Bilddateien wurde Texterkennung mit ABBYY¬Æ OCR (Optical Character Recognition) Software in den Versionen FineReader 7.0 Corporate Edition für Antiqua-Druck und FineReader 7.0 Scripting Edition für Fraktur durchgeführt mit anschließender manueller Nachkorrektur. Die OCR-Textdateien wurden schließlich mit Makros in Unicode-basierte XML-1.0-Dateien transformiert und anschließend gemäß des damals gültigen ""AAC Markup"" -Standards und einem page-per-page-Prinzip als Einzeldateien annotiert. Der weitere Workflow beinhaltete zunächst die Transformation der einzelnen XML-Dateien in TEI (Text Encoding Initiative) P5-konforme Formate und die Zusammenführung der Einzeldateien in eine Gesamtdatei pro ""Insel""-Heft. Insgesamt lagen am Ende dieses Konvertierungsprozesses 46 Dateien inklusive separater Inhaltsverzeichnisse pro Quartalsband vor. Für die Entwicklung und Bereitstellung des Transformations-Tools XMLJoiner war Andreas Basch (ACDH-CH) verantwortlich. Das Tool war zuvor schon an der Erstellung der digitalen ""Schaubühne""-Edition erprobt worden ( Mit der Umwandlung der ""alten"" Annotationen in valide TEI P5-Elemente, -Attribute und Das Projektziel besteht in der Erstellung einer digitalen als Volltext durchsuchbaren Zeitschriftenedition mit Faksimile- und Transkriptionsansicht als Grundlage für weiterführende Forschungsfragen, textwissenschaftliche Analysen und diverse Nachnutzungs-szenarien. Neben der Integration von Bild und Text liegt ein weiterer Fokus auf der Erstellung eines Personenverzeichnisses, das auf Autor:innen, Übersetzer:innen und Illustrator:innen der ""Insel"" verweist. In diesem Zusammenhang ist eine Verlinkung von Personennamen mit der Normdatei ""PMB 'Personen der Moderne Basis"" ( Die Gleichzeitig soll eine Verknüpfung dieser Entitäten mit Normdatensätzen wie der GND oder WikiData erfolgen. Durch die automatische Zuweisung unikaler IDs in der PMB und die Rückverlinkung der Datensätze in die annotierten XML-Dateien soll ein umfassendes externes Register erstellt werden, das die digitale Edition der ""Insel"" begleitet. Das Poster soll einen Überblick über den geplanten Aufbau der Edition bieten, sowie die bislang erfolgten Schritte der Datengenerierung und den Workflow illustrieren, der beispielhaft für die Erstellung weiterer Zeitschriften-Editionsprojekte innerhalb des ACDH-CH sein könnte. Zusätzlich sollen diverse Herausforderungen im editorischen Prozess vor dem Hintergrund der besonderen druckgraphischen Ausgestaltung der Zeitschrift thematisiert und Fragen des Umgangs mit Normdaten und ihrer Integration in Editionsprojekte erörtert werden.",de,Abteilung Insel Monatsschrift Oktober September Berliner Verlag Schuster Löffler Jahrgang leipzig herausgeben gelten wichtig Zeitschrift Jahrhundertwende Literatur Moderne wechselnd Redaktion Otto Julius Bierbaum Alfred Walter Heymel Rudolf Alexander Schröder publizieren Insel namhaft Autor Franz Blei Richard Dehmel heinrich Mann Rainer Maria Rilke Felix Salt Robert wals Frank wedekind Insel finden diverser belletristisch Textsort Prosa Lyrik dramatisch Form einzelveröffentlichungen Fortsetzunge novellen Erzählung Skizze dramatisch werk gedicht aufsätzen aphorismen kulturtheoretisch Abhandlung Rezension kritiken Zeitschrift beinhalten hinaus zahlreich übersetzungen besonderer Bedeutung Buchkunst Form illustration drucken Holzschnitt dergleichen Zeitschrift Signet Segelschiff hervor Herausgeber Insel sehen Hintergrund Jahrhundertwende Kunstverständnis verpflichten formenreich Ausgestaltung Buchschmuck typographisch Besonderheit zeichnen Insel zusätzlich stellen besonderer Herausforderung Erstellung Edition dar Jahrgang wechseln Drucktype zudem Fraktur Antiqua Grundlage Datengenerierung Editionserstellung dienen digital Beständ Schwerpunkt Zeitung Zeitschrift Austrian Academy Corpus aac Digitalisat basieren Produktion hochauflösend Scan Herstellung faksimile unkomprimiert optimieren Aufnahme umschläge Vollfarbe anfertigen Bilddateie Texterkennung ocr Optical character Recognition Software version fineread Corporate Edition fineread Scripting Edition Fraktur durchführen anschließend manuell Nachkorrektur schließlich Makros transformieren anschließend gemäß gültig aac markup Einzeldateien annotiert Workflow beinhalten Transformation einzeln Tei Text encoding Initiative formate Zusammenführung Einzeldateie Gesamtdatei pro insgesamt liegen Konvertierungsprozess Dateie inklusive separat Inhaltsverzeichnisse pro Quartalsband Entwicklung Bereitstellung xmljoiner Andreas Basch verantwortlich Tool zuvor Erstellung digital erproben Umwandlung alt Annotation valid tei Projektziel bestehen Erstellung digital Volltext durchsuchbar Zeitschriftenedition Transkriptionsansicht Grundlage weiterführend forschungsfrag textwissenschaftlich Analyse diverser Integration Bild Text liegen weit Fokus Erstellung personenverzeichnisses Autor innen Übersetzer innen Illustrator innen Insel verweisen Zusammenhang Verlinkung Personenname Normdatei pmb Person modern Basis gleichzeitig Verknüpfung Entität Normdatensätz gnd Wikidata erfolgen automatisch Zuweisung unikal ids Pmb Rückverlinkung Datensätz annotierter umfassend extern Register erstellen digital Edition Insel begleiten Poster Überblick geplant Aufbau Edition bieten bislang erfolgt Schritt Datengenerierung Workflow illustrieren Beispielhaft Erstellung weit innerhalb zusätzlich diverser Herausforderung editorisch Prozess Hintergrund besonderer druckgraphisch Ausgestaltung Zeitschrift thematisieren Frage umgangs normdat Integration Editionsprojekte erörtern,"[('insel', 0.4358803462301467), ('zeitschrift', 0.208877170196202), ('erstellung', 0.16372635008998365), ('edition', 0.15770183578882954), ('datengenerierung', 0.12453724178004191), ('fraktur', 0.12453724178004191), ('pmb', 0.12453724178004191), ('fineread', 0.12453724178004191), ('jahrhundertwende', 0.1159969993377416), ('jahrgang', 0.1159969993377416)]"
2025,DHd2025,SPIELBERG_Marina_Wie_Shakespeare__bersetzungen_digital_edier.xml,Wie Shakespeare-Übersetzungen digital edieren?,"Marina Spielberg (Trier Center for Digital Humanities, Universität Trier, Deutschland); Claudia Bamberg (Trier Center for Digital Humanities, Universität Trier, Deutschland)","digitale Edition, Übersetzung, Shakespeare, Datenmodell","Transkription, Modellierung, Annotieren, Literatur, Manuskript","Auf dem Gebiet der digitalen Edition von literarischen Übersetzungen sind bislang noch sehr wenige Ansätze einer spezifischen Konzeptionalisierung vorhanden. Gab es schon für Druckeditionen kaum eine Theoriebildung zu dieser Editionsform (Plachta und Woesler 2002) oder Beispiele für eine adäquate Umsetzung Das Poster möchte das geplante Projekt einer digitalen Edition der Shakespeare-Übersetzungen August Wilhelm Schlegels und des Tieck-Kreises 'unter dem Namen ""Schlegel-Tieck"" bekannt geworden 'vorstellen. Dieses wird derzeit u.a. am Trier Center for Digital Humanities und an der Shakespeare Forschungsbibliothek der LMU München vorbereitet und konzipiert. Ziel ist es, am Beispiel dieser romantischen Übertragungen, die von 1798 bis 1833 erstmals erschienen sind, auch ein nachhaltiges Konzept für die digitale Edition von Übersetzungen zu entwickeln. Die Text- und Publikationsgeschichte des ""Schlegel-Tieck"", der neben dem Werk Goethes und Schillers lange als ""dritter deutscher Klassiker"" galt und die deutsche Shakespeare-Rezeption bis weit ins 20. Jahrhundert maßgeblich geprägt hat (Jansohn 2023), ist überaus komplex. Darin mag ein Grund liegen, dass es bis heute weder eine historisch-kritische Edition noch eine Studienausgabe gibt. Bei den heutigen Drucken, die auf die Übersetzungen August Wilhelm Schlegels und des Tieck-Kreises zurückgehen, wie sie etwa noch bei Reclam zu finden sind (Jansohn 2014), handelt es sich in der Regel um nicht weiter ausgewiesene Kompilationen verschiedener Übersetzer:innen aus verschiedenen Epochen. Da in den letzten gut zehn Jahren zahlreiche Quellen zu Schlegel und Tieck erstmals erschlossen wurden (Strobel und Bamberg 2014–2021; Latifi 2018; Hölter 2014–2023, Knödler 2018), existiert inzwischen eine wichtige, ja unabdingbare Voraussetzung für eine erfolgreiche Durchführung des Projekts. Die geplante Übersetzungsedition möchte die Probleme der Autorschaftszuweisung des ""Schlegel-Tieck"" endgültig aufklären. Während August Wilhelm Schlegel die erste Ausgabe der Übersetzung von siebzehn Shakespeare-Dramen von 1797 bis 1809 allein unternahm, wurde der erste ""Schlegel-Tieck"" erst von 1825 bis 1833 (Shakespeare 1825–1833) gedruckt. Der Berliner Verleger Georg Andreas Reimer hatte die beiden ""berühmten"" romantischen Namen auf den Titel gesetzt, um sich gegen die inzwischen zahlreiche Konkurrenz der deutschen Shakespeare-Übersetzer auf dem Buchmarkt durchzusetzen. An dieser Ausgabe hatte Schlegel jedoch so gut wie keinen aktiven Anteil mehr, und auch Tieck fungierte nicht als Übersetzer. Vielmehr ließ er seine Tochter Dorothea Tieck und den Schriftsteller Wolf von Baudissin die von Schlegel noch nicht übersetzten Dramen übertragen 'Ludwig Tieck selbst brachte sich nur als Berater ein und ""korrigierte"" zudem Schlegels Übersetzungen, die in der Ausgabe wiederabgedruckt wurden. Als Schlegel davon erfuhr, war er darüber sehr empört und forderte von Reimer, die Önderungen in den folgenden Auflagen wieder zurückzunehmen (August Wilhelm Schlegel an Georg Andreas Reimer 1825). In späteren Nachdrucken zu Lebzeiten der Übersetzer:innen wurde sodann immer wieder in die Texte eingegriffen. Bei der Konzeption einer digitalen Edition der romantischen Shakespeare-Übersetzungen geht es zunächst darum zu erarbeiten, wie mit der Fülle an Texten 'Ausgangstexten, Übersetzungen, Revisionen, mit Manuskripten und mit Drucken –, aber auch mit den Co-Autorschaften umgegangen werden kann. Sodann stellt sich die Frage, wie eine digitale Kommentierung aussehen könnte: Wie etwa lassen sich Kommentare zum interlingualen Transfer und zu den jeweiligen Übersetzungsprinzipien modellieren und sodann in einem entsprechenden UX-Design darstellen und für die Nutzer:innen abrufen? Und wie lässt sich eine konsistente Datenmodellierung für die verschiedenen Übersetzungsfassungen entwickeln 'wie muss diese im Hinblick auf die Textsorte Übersetzung gegenüber konventionellen Werkausgaben modifiziert werden (vgl. ausführlich zum Datenmodell und einem darauf aufbauenden möglichen UX-Design für die romantischen Shakespeare-Übersetzungen Schlegels und des Tieck-Kreises: Bamberg und Burch 2023: 313-323)? Die Transkription und Annotation der digitalen Faksimiles erfolgt mit dem Werkzeug Das Poster möchte ein erstes Datenmodell für die digitale Edition der Shakespeare-Übersetzungen August Wilhelm Schlegels und des Tieck-Kreises präsentieren und vorstellen, welche Tools dabei zum Einsatz kommen. Da sich das Projekt in einer frühen Phase befindet, liegt der Fokus zunächst auf Vorüberlegungen und offenen Fragen. So soll das Poster die grundlegenden Ideen, theoretischen Ansätze und Herausforderungen des Vorhabens skizzieren, die die Datenmodellierung, Kommentierung und das UX-Design betreffen. Es möchte folglich auch schon beispielhaft demonstrieren, wie die Edition im Frontend aussehen und genutzt werde könnte. Das entworfene Konzept soll dabei auch modellbildend für künftige digitale Übersetzungseditionen sein.",de,Gebiet digital Edition literarisch übersetzungen bislang Ansatz spezifisch Konzeptionalisierung vorhanden druckedition Theoriebildung Editionsform plachta woesl Beispiel adäquat Umsetzung Poster geplant Projekt digital Edition August Wilhelm Schlegels Name vorstellen derzeit Trier Center for Digital Humanitie shakespear Forschungsbibliothek Lmu München vorbereiten konzipieren Ziel romantisch übertragungen erstmals erscheinen nachhaltig Konzept digital Edition übersetzungen entwickeln publikationsgeschichen Werk goeth Schiller deutsch Klassiker gelten deutsch Jahrhundert maßgeblich prägen Jansohn überaus komplex Grund liegen weder Edition Studienausgabe heutig drucken übersetzung August Wilhelm Schlegels zurückgehen Reclam finden jansohn handeln Regel ausgewiesen Kompilation verschieden Übersetzer innen verschieden Epoche letzter zahlreich quellen Schlegel Tieck erstmals erschließen strobel bamberg Latifi hölter Knödler existieren inzwischen wichtig unabdingbar Voraussetzung erfolgreich Durchführung Projekt geplant Übersetzungsedition Problem Autorschaftszuweisung endgültig aufklären August Wilhelm Schlegel Ausgabe Übersetzung siebzehn unternehmen shakespear drucken Berliner Verleger Georg Andreas Reimer berühmt romantisch Name Titel setzen inzwischen zahlreich Konkurrenz deutsch Buchmarkt durchsetzen Ausgabe schlegel aktiv Anteil Tieck fungieren Übersetzer vielmehr lassen Tochter Dorothea Tieck Schriftsteller Wolf Baudissin Schlegel übersetzt Dram übertragen Ludwig Tieck bringen Berater korrigiert zudem Schlegel übersetzungen Ausgabe wiederabgedrucken Schlegel erfahren empört fordern Reimer önderungen folgend auflage zurücknehmen August Wilhelm Schlegel Georg Andreas Reimer spät Nachdruck lebzeiten Übersetzer innen sodann Text eingreifen Konzeption digital Edition romantisch erarbeiten Fülle Text ausgangstexten übersetzungen revisionen Manuskript drucken umgehen Sodann stellen Frage digital Kommentierung aussehen lassen Kommentar interlingual Transfer jeweilig übersetzungsprinzipien Modelliere Sodann entsprechend darstellen Nutzer innen abrufen lässen konsistent Datenmodellierung verschieden übersetzungsfassungen entwickeln Hinblick textsort Übersetzung Konventionell werkausgaben modifizieren ausführlich Datenmodell aufbauend möglich romantisch Schlegel Bamberg Burch Transkription Annotation digital faksimile erfolgen Werkzeug Poster datenmodell digital Edition August Wilhelm Schlegels präsentieren vorstellen Tools Einsatz Projekt früh Phase befinden liegen Fokus vorüberlegung offen Frage Poster grundlegend Idee theoretisch Ansatz Herausforderung vorhaben skizzieren Datenmodellierung Kommentierung betreffen folglich beispielhaft demonstrieren Edition Frontend aussehen nutzen entworfen Konzept modellbildend künftig digital übersetzungseditionen,"[('schlegel', 0.40018515756917145), ('wilhelm', 0.23942281642329724), ('august', 0.21690052037403781), ('tieck', 0.21112102265293875), ('edition', 0.20091830931186402), ('romantisch', 0.1845488601675833), ('schlegels', 0.16999855595245586), ('reimer', 0.15834076698970406), ('übersetzer', 0.15006943408843929), ('übersetzungen', 0.1494223720362799)]"
2025,DHd2025,DUDAR_Julia_Exploring_Measures_of_Distinctiveness__An_Evalua.xml,Exploring Measures of Distinctiveness: An Evaluation Using Synthetic Texts,"Julia Dudar (Universität Trier, Deutschland); Christof Schöch (Universität Trier, Deutschland)","measures of disrinctiveness, evaluation, synthetic texts, French novels, literature","Programmierung, Inhaltsanalyse, Visualisierung, Literatur, Methoden, Forschungsergebnis","Comparing groups of texts with each other in order to investigate what is characteristic about each group is a fundamental approach used in many research contexts, and measures of distinctiveness (also known as keyness measures) support such research in a quantitative perspective. The research we report on here is a further step in our fundamental work on measures of distinctiveness used for comparison of groups of texts. In the research reported on here, we focus on evaluating measures of distinctiveness through an analysis based on synthetic texts. Recent work has shown the importance of both frequency and dispersion of words for keyness analysis (Gries, 2021). By conducting keyness analysis using synthetically created datasets and through inserting an artificial word with precisely-manipulated frequency and dispersion into the synthetic dataset, we aim to systematically uncover the characteristics of different measures. Our goal is to determine the sensitivity of each measure to variations in frequency and dispersion. Evaluating measures of distinctiveness is challenging due to the fact that generating a gold-standard annotation is not possible. Distinctiveness is not an inherent characteristic of a word but can only be detected in the context of the entire target corpus and in comparison to another corpus. To tackle this challenge, several studies have attempted to evaluate distinctiveness measures using various methods. Kilgarriff (2001) examined corpus similarity by reviewing the mathematical characteristics of various distinctiveness measures. Paquot & Bestgen (2009) compared three different measures in their ability to identify words distinctive of academic prose as opposed to fictional prose. Lijffijt et al. (2014) explored a broad array of measures, focusing on the statistical characteristics of these measures. Within the framework of our project ""Zeta and Company,"" we conducted an in-depth analysis of the qualitative characteristics of these measures (Schröter et al., 2021). To enhance accessibility and usability, we implemented nine measures of distinctiveness in the Python package Our research proposes a new method for evaluating measures of distinctiveness, utilizing synthetically created text collections that reflect word frequencies as they occur in a real corpus, but within an artificially homogeneous corpus design. Studies based on naturally-occurring language must work around the fact that frequency and dispersion of any word will both vary and correlate to some extent. Our approach allows for precise, independent manipulation of word frequency and dispersion by inserting an artificial word. Our method enables us to uncover new advantages and limitations of distinctiveness measures and to compare their sensitivity to frequency and dispersion variations under consistent conditions. Our research is conducted on a synthetic text collection generated through random sampling from a corpus of French contemporary novels. The foundation for this corpus is a balanced subset from our larger collection of French contemporary popular novels and consists of 320 novels first published during the time period 1980 to 1999. This custom-built corpus maintains equal representation, per decade, across four subgroups: highbrow novels and lowbrow novels with three subgenres (sentimental novels, crime fiction, and science fiction). The original text corpus comprises approximately 19 million words. We load the entire corpus as a single dataset and randomly sample synthetic ""novels,"" each with a consistent length of 40,000 words. Our newly-generated corpus contains 320 synthetic ""novels,"" matching the number of novels in the original corpus. This approach addresses two main objectives. Firstly, it ensures that the generated corpus reflects real-life word occurrences and frequencies. Secondly, it results in a homogeneous corpus, eliminating subgenre differences, since each text is sampled from the entire corpus. To conduct our evaluation we used nine measures of distinctiveness implemented in our Python package The original French corpus was annotated with spaCy to create the input required by Our experiment had two primary settings to assess the impact of frequency and dispersion on distinctiveness scores. In the first setting, an artificial word was added to one segment of both the target and comparison corpus. This setting enables us to analyze the influence of only one parameter, namely the frequency. To maintain a constant total word count while adding an artificial word, other words in the corpus that occupied the same position as the artificial word were replaced. The frequency of the artificial word was constant (10 words) in the comparison corpus, but varied in the target corpus (10 to 2000 words). In the second setting, the artificial word's frequency was fixed at 1000 occurrences, but its dispersion varied. The idea was again to isolate one parameter, in this case dispersion, and analyze its influence on the performance of measures. The dispersion experiment involved different numbers of segments receiving the artificial word with specified frequencies, leading to 20 parameter settings. In the comparison corpus, scenarios included 1 segment with 1000 words and 1000 segments with 1 word. For each of these parameters, we conducted distinctiveness analyses with variations in the target corpus (the first number refers to the number of segments that receive the artificial word, and the second to how many times the artificial word is included in each of the selected segments): 1/1000, 2/500, 5/200, 10/100, 20/50, 50/20, 100/10, 200/5, 500/2, 1000/1. The results were compiled into a single dataframe, with all words in the corpus sorted and ranked by their distinctiveness scores. Each measure's performance was evaluated based on the rank of the artificial word (where a rank of 1 indicates the highest distinctiveness score). Since our corpus is based on naturally occurring word frequencies, we conducted an additional analysis to evaluate the potential artifacts caused by random sampling effects in the synthetic texts without the artificial word. This analysis aimed to identify the frequency differences of words in the corpus across multiple runs. Figure 1 illustrates the relationship between rank and the Ratio of Relative Frequencies (RRF) scores, based on 100 runs of randomly sampled synthetic corpora. As shown, the first rank is typically achieved with scores ranging from 10 to 18. This suggests that, due to the natural variations in the frequencies of existing words, an RRF score below 10 for the artificial word is unlikely to secure the first rank. In describing the results, our main focus lies on unexpected interesting observations, rather than on a description of all findings. Generally speaking, our expectations formulated in Hypotheses 1-4 are confirmed: Frequency-based measures are sensitive to differences in frequency but not to dispersion, and dispersion-based measures are sensitive to differences in dispersion, but not in frequency. However, this result comes with many nuances. Analyzing the frequency-based measures such as the chi-square test, LLR, and RRF, we observe a tendency for the score to increase with increasing frequency, but there are notable differences among the various measures (Fig. 2). When the artificial word reaches a frequency of 200 or higher in the target corpus, its RRF rank is consistently 1, indicating that it achieves the highest score among all words in the corpus. This observation aligns with our earlier analysis conducted without the artificial word (Fig. 1). Notably, RRF scores of 10 or below (corresponding to a frequency of 100 words in the target corpus) fail to achieve the first rank. This also explains the wide distribution of ranks observed for RRF scores based on a frequency of 100 words in the target corpus. Regarding the LLR and chi-squared tests, both measures are even more sensitive to frequency variation compared to RRF. At a frequency of 40 and higher, we observe the artificial word achieving the first rank. TF-IDF shows moderate sensitivity to frequency variation, partially supporting Hypothesis 5. Regarding the performance of dispersion-based measures, such as both variants of Zeta and the rank-sum test, when the artificial word is inserted into only one segment of the comparison corpus and the number of segments containing the artificial word in the target corpus increases, the word moves up in the ranking. Specifically, starting with 10 words in 100 segments, the artificial word almost consistently receives the first rank according to these three measures (Fig. 3). Eta shows interesting results in these settings. As a dispersion-based measure, we expected Eta to effectively identify an artificial word as distinctive, especially when the word is evenly spread across a high number of segments. However, as the number of segments containing the artificial word increases, its scores remain consistently low compared to randomly assigned words. Only in a scenario with one occurrence in 1000 segments does the artificial word receive the first rank (Fig. 3). This indicates that Hypothesis 4 is supported solely by both variants of Zeta and the rank-sum test. Unexpected results are also observed with TF-IDF. Similarly to the results of dispersion-based measures, as the number of segments increases, the rank of the artificial word moves up. However, in contrast to the moderate movement with respect to rank seen with dispersion-based measures, in the scenario with 1000 words in one segment of the comparison corpus, the artificial word achieves the first rank starting with a dispersion of just 100 words in 10 segments (Fig. 3). This result partially contradicts our expectation in Hypothesis 5 regarding the moderate sensitivity of TF-IDF to variations in dispersion. Conducting analyses based on synthetic texts, we created ideal conditions to uncover the hidden properties of a range of distinctiveness measures. Through our experiment, we tested the sensitivity of these measures to variations in the frequency and dispersion of a specific word. We found that LLR and chi-square tests are even more sensitive to frequency variation than RRF, which is simple and relies only on word frequency. Both Zeta variations and the rank-sum test demonstrated similar scores and abilities to detect distinctive words. Moreover, we discovered that TF-IDF is more sensitive to slight dispersion differences of the target word compared to other dispersion-based measures. Finally, we found that Eta does not detect a word with a clear contrast in dispersion when its frequency is the same in both the target and comparison corpora. Despite the interesting observations derived from these analyses, there is significant potential for future work. One key step is to extend our framework by implementing new measures of distinctiveness, particularly those that rely purely on dispersion rather than doing so only primarily, in a combination of frequency and dispersion. Another crucial step is to explore practical applications of this newfound knowledge about distinctiveness measures. Understanding the specific contexts and scenarios where these measures can be effectively utilized will open up new possibilities and enhance our ability to analyze and compare textual corpora more predictably and more accurately. Additionally, this approach can easily be applied to corpora in languages other than French. While we assume that the method will work similarly with other languages, we encourage other researchers to test our method on further corpora to validate the robustness of our results. Data and Code Repository:",en,compare group text order investigate characteristic group fundamental approach research context measure distinctiveness know keyness measure support research quantitative perspective research report step fundamental work measure distinctiveness comparison group text research report focus evaluate measure distinctiveness analysis base synthetic text recent work show importance frequency dispersion word keyness analysis grie conduct keyness analysis synthetically create dataset insert artificial word precisely manipulate frequency dispersion synthetic dataset aim systematically uncover characteristic different measure goal determine sensitivity measure variation frequency dispersion evaluate measure distinctiveness challenge fact generate gold standard annotation possible distinctiveness inherent characteristic word detect context entire target corpus comparison corpus tackle challenge study attempt evaluate distinctiveness measure method kilgarriff examine corpus similarity review mathematical characteristic distinctiveness measure paquot bestgen compare different measure ability identify word distinctive academic prose oppose fictional prose lijffijt et al explore broad array measure focus statistical characteristic measure framework project zeta company conduct depth analysis qualitative characteristic measure schröter et al enhance accessibility usability implement measure distinctiveness python package research propose new method evaluate measure distinctiveness utilize synthetically create text collection reflect word frequency occur real corpus artificially homogeneous corpus design study base naturally occur language work fact frequency dispersion word vary correlate extent approach allow precise independent manipulation word frequency dispersion insert artificial word method enable uncover new advantage limitation distinctiveness measure compare sensitivity frequency dispersion variation consistent condition research conduct synthetic text collection generate random sampling corpus french contemporary novel foundation corpus balanced subset large collection french contemporary popular novel consist novel publish time period custom build corpus maintain equal representation decade subgroup highbrow novel lowbrow novel subgenre sentimental novel crime fiction science fiction original text corpus comprise approximately million word load entire corpus single dataset randomly sample synthetic novel consistent length word newly generate corpus contain synthetic novel match number novel original corpus approach address main objective firstly ensure generate corpus reflect real life word occurrence frequency secondly result homogeneous corpus eliminate subgenre difference text sample entire corpus conduct evaluation measure distinctiveness implement python package original french corpus annotate spacy create input require experiment primary setting assess impact frequency dispersion distinctiveness score setting artificial word add segment target comparison corpus setting enable analyze influence parameter frequency maintain constant total word count add artificial word word corpus occupy position artificial word replace frequency artificial word constant word comparison corpus vary target corpus word second setting artificial word frequency fix occurrence dispersion vary idea isolate parameter case dispersion analyze influence performance measure dispersion experiment involve different number segment receive artificial word specify frequency lead parameter setting comparison corpus scenario include segment word segment word parameter conduct distinctiveness analysis variation target corpus number refer number segment receive artificial word second time artificial word include select segment result compile single dataframe word corpus sort rank distinctiveness score measure performance evaluate base rank artificial word rank indicate high distinctiveness score corpus base naturally occur word frequency conduct additional analysis evaluate potential artifact cause random sampling effect synthetic text artificial word analysis aim identify frequency difference word corpus multiple run figure illustrate relationship rank ratio relative frequency rrf score base run randomly sample synthetic corpora show rank typically achieve score range suggest natural variation frequency exist word rrf score artificial word unlikely secure rank describe result main focus lie unexpected interesting observation description finding generally speak expectation formulate hypothesis confirm frequency base measure sensitive difference frequency dispersion dispersion base measure sensitive difference dispersion frequency result come nuance analyze frequency base measure chi square test llr rrf observe tendency score increase increase frequency notable difference measure fig artificial word reach frequency high target corpus rrf rank consistently indicate achieve high score word corpus observation align early analysis conduct artificial word fig notably rrf score correspond frequency word target corpus fail achieve rank explain wide distribution rank observe rrf score base frequency word target corpus llr chi squared test measure sensitive frequency variation compare rrf frequency high observe artificial word achieve rank tf idf show moderate sensitivity frequency variation partially support hypothesis performance dispersion base measure variant zeta rank sum test artificial word insert segment comparison corpus number segment contain artificial word target corpus increase word move ranking specifically start word segment artificial word consistently receive rank accord measure fig eta show interesting result setting dispersion base measure expect eta effectively identify artificial word distinctive especially word evenly spread high number segment number segment contain artificial word increase score remain consistently low compare randomly assign word scenario occurrence segment artificial word receive rank fig indicate hypothesis support solely variant zeta rank sum test unexpected result observe tf idf similarly result dispersion base measure number segment increase rank artificial word move contrast moderate movement respect rank see dispersion base measure scenario word segment comparison corpus artificial word achieve rank start dispersion word segment fig result partially contradict expectation hypothesis moderate sensitivity tf idf variation dispersion conduct analysis base synthetic text create ideal condition uncover hidden property range distinctiveness measure experiment test sensitivity measure variation frequency dispersion specific word find llr chi square test sensitive frequency variation rrf simple rely word frequency zeta variation rank sum test demonstrate similar score ability detect distinctive word discover tf idf sensitive slight dispersion difference target word compare dispersion base measure finally find eta detect word clear contrast dispersion frequency target comparison corpora despite interesting observation derive analysis significant potential future work key step extend framework implement new measure distinctiveness particularly rely purely dispersion primarily combination frequency dispersion crucial step explore practical application newfound knowledge distinctiveness measure understand specific context scenario measure effectively utilize open new possibility enhance ability analyze compare textual corpora predictably accurately additionally approach easily apply corpora language french assume method work similarly language encourage researcher test method corpora validate robustness result datum code repository,"[('word', 0.436570720357702), ('frequency', 0.35925091211960947), ('measure', 0.3261622150814377), ('dispersion', 0.29508257354789585), ('rank', 0.24067392817264946), ('artificial', 0.23926751065890647), ('distinctiveness', 0.22416951819249686), ('corpus', 0.2075502170619488), ('segment', 0.12245622521910478), ('base', 0.12080122692959219)]"
2025,DHd2025,FL_H_Marie_Das_Projekt_CompAnno__Comparative_Annotation_to_E.xml,Das Projekt CompAnno: Comparative Annotation to Explore and Explain Text Similarities,"Marie Flüh (Universität Hamburg, Deutschland); Julia Nantke (Universität Hamburg, Deutschland); Janis Pagel (Universität zu Köln, Deutschland); Nils Reiter (Universität zu Köln, Deutschland)","Computational Literary Studies, Intertextualität, Figurenanalyse","Beziehungsanalyse, Annotieren, Literatur","Das DFG-Projekt CompAnno entwickelt einen vergleichenden Annotationsworkflow zur computergestützten Detektion und Klassifizierung von literarischen Textähnlichkeiten am Beispiel von Figureneigenschaften als einer Kategorie, die für die Gestaltung literarischer Erzähltexte und für die Interpretation intertextueller Beziehungen zentral ist (Müller 1991:101). Der Workflow für eine computergestützte Untersuchung von Textähnlichkeit soll so gestaltet sein, dass er über die Erkennung von text-reuse hinausgeht und nicht auf ein festes Korpus bezogen ist. Gleichzeitig greifen wir mit der vergleichenden Annotation eine literaturwissenschaftliche Basismethode auf (Unsworth 2000, Epple et al. 2020:7) und entwickeln einen neuen Weg für die Arbeit mit interpretativen Kategorien. Wir arbeiten mit vier Annotatorinnen, wobei die Annotationsaufgaben zur Annotation von Figureneigenschaften ineinandergreifen. Die Annotatorinnen sind alle Germanistik- und Linguistikstudentinnen. Da Figuren in der Regel zu Beginn eines Textes eingeführt werden, ist hier mit besonders zahlreichen Eigenschaften zu rechnen. Deshalb werden in unterschiedlichen Annotationsphasen jeweils die Anfangspassagen (circa 20.000 Tokens) aus verschiedenen Prosatexten aus Um einen differenzierten und spezifischen Blick auf das Phänomen zu erlangen Figureneigenschaften werden erst manuell ermittelt und kategorisiert. Auf Grundlage der Guidelines ist jede Annotatorin für die Analyse einer ausgewählten Kategorie zuständig. In Diskussionsrunden werden die Annotationsdaten besprochen und angepasst. Darauf aufbauend wird ein Ranking der Öhnlichkeiten erstellt, das zum Trainieren oder Prompten eines maschinellen Lernsystems bzw. großen Sprachmodells verwendet wird. Ziel dieses Modells ist die automatische Erkennung von Stellen, an denen eine Figureneigenschaft vorkommt, sowie die Kategorie der Figureneigenschaft, so dass eine vergleichende Annotation Sinn ergibt. Im Gegensatz zu etablierten Annotationsansätzen beruht die vergleichende Annotation auf der gleichzeitigen Betrachtung mehrerer Textausschnitte: Auf Grundlage von Richtlinien für die vergleichende Annotation von Figureneigenschaften werden den Annotatorinnen jeweils zwei Textabschnitte vorgelegt, zu denen dann die ihnen enthaltenen Figureneigenschaften vergleichend zu annotieren sind. Benutzt wird hierzu die Webanwendung Im weiteren Projektverlauf sollen die vergleichenden Annotationen dazu benutzt werden, die zuvor genannten intertextuellen Beziehungen zu beleuchten. Bisher entstandene Teilergebnisse sind Annotationsrichtlinien, Einblicke in die Annotationspraxis, qualitative und quantitative Einblicke in die Darstellung und Verteilung von Figureneigenschaften sowie erste Automatisierungsversuche. Aus der induktiven Auswertung der explorativen Annotationsphase, in der unter Einbezug etablierter Figurenkonzepte (Forster 1949, Hansen 2000, Jannidis 2004) vor allem konzeptuelle Fragestellungen im Fokus standen, ergeben sich fünf bzw. sechs Oberkategorien, die häufig für die Beschreibung literarischer Figuren verwendet werden und deshalb als Analysekategorien für die automatisierungsorientierte und vergleichende Annotation in Frage kommen (s. Abb. 2). Bisher zeigt sich, dass Figuren vor allem über Rollen und Charaktereigenschaften näher beschrieben werden (s. Tabelle 1). Für jede Annotation legen die Annotatorinnen den Interpretationsaufwand auf einer Skala von sehr niedrig bis sehr hoch fest. Ausgehend von der Annahme, dass ein geringer Interpretationsgrad auf explizit im Text thematisierte Figureneigenschaften und ein hoher Interpretationsgrad auf implizite Figureneigenschaften hindeutet, fungiert er als Marker für den Grad der Explizitheit. Bei der Auswertung eines Teils der Annotationsdaten zeigt sich, dass der Interpretationsgrad meistens als gering eingeschätzt wird. Dieser Befund ist als individuelle Annotationsentscheidung aufzufassen. Außerdem lässt sich schlussfolgern, dass Eigenschaften vor allem explizit erwähnt werden (s. Abb. 3) und implizite Eigenschaften eher ein Randphänomen darstellen. Ein niedriger Interpretationsgrad findet sich vor allem in den Kategorien ""Kleidung"", ""Alter"" und ""Physiognomie"" und ein höherer in den Kategorien ""Rolle"" und ""Charakter"". Eigenschaften, für die ein niedriger Interpretationsgrad angegeben wurde (explizite Eigenschaften), sind in allen Texten häufig. Sie sind ""hochgradig intertextuell"" und relativ generisch (bspw. ""jung"", ""schön"" oder ""groß""). Eigenschaften, die mit einem hohen Interpretationsgrad ausgezeichnet wurden, kommen in einzelnen Texten und in geringerer Anzahl vor. Implizite Eigenschaften scheinen individuell zu sein, können aber gerade deshalb eine spezifischere Verbindung zwischen zwei Texten markieren als die explizit-generischen Eigenschaften. Erste Pilotexperimente zur automatischen Klassifizierung von Figureneigenschaften zeigen, dass zwar schon moderat gute Ergebnisse mit relativ simplen Methoden zu erreichen sind, aber die Performanz noch ausbaufähig ist. Tabelle 2 zeigt die Durchschnittswerte für Precision, Recall und F1-Score für die Klassifikation von drei Modellen Die Ergebnisse zeigen, dass die Klassifikation von Charaktereigenschaft, Physiognomie und Rolle mit ca. 60 % F1-Score im ersten Anlauf akzeptabel funktioniert, die Modelle jedoch Probleme haben, ""Alter"" und ""Kleidung"" richtig zu klassifizieren (16 bzw. 36 % F1-Score), außerdem ist die Precision für Rolle mit 6 % deutlich niedriger als für die anderen Kategorien. Tabelle 2: Klassifikationsergebnisse für das Erkennen von Figureneigenschaften Geplante Arbeitspakete betreffen vor allem die Automatisierung der vergleichenden Annotation und die Verbesserung der automatischen Erkennung der Kategorien. Eine weitere offene Frage ist der ideale Kotext, den es braucht, um automatisiert Entscheidungen bezüglich der Figureneigenschaften zu treffen.",de,Companno entwickeln vergleichend Annotationsworkflow computergestützten Detektion Klassifizierung literarisch Textähnlichkeite Figureneigenschaft Kategorie Gestaltung literarisch erzähltexen Interpretation intertextuell Beziehung zentral müller Workflow computergestützt Untersuchung Textähnlichkeit gestalten Erkennung hinausgehen fest Korpus beziehen gleichzeitig greifen vergleichend Annotation literaturwissenschaftlich Basismethode unsworth epplen et entwickeln Weg Arbeit interpretativ kategorien arbeiten Annotatorinn wobei Annotationsaufgabe Annotation Figureneigenschaft ineinandergreifen Annotatorinn linguistikstudentinnen Figur Regel Beginn Text einführen zahlreich Eigenschaft rechnen unterschiedlich Annotationsphase jeweils anfangspassagen circa Token verschieden Prosatext differenziert spezifisch Blick Phänomen erlangen figureneigenschaften manuell ermitteln kategorisieren Grundlage guidelin Annotatorin Analyse ausgewählt Kategorie zuständig diskussionsrund Annotationsdat Besproch anpassen aufbauend Ranking öhnlichkeiten erstellen Trainieren Prompt maschinell Lernsystem Sprachmodell verwenden Ziel Modell automatisch Erkennung Stelle Figureneigenschaft vorkommen Kategorie Figureneigenschaft vergleichend Annotation Sinn ergeben Gegensatz etabliert Annotationsansätz beruhen vergleichend Annotation gleichzeitig Betrachtung mehrere Textausschnitt Grundlage Richtlinie vergleichend Annotation Figureneigenschaft Annotatorinn jeweils Textabschnitt vorlegen enthalten Figureneigenschaft vergleichend annotieren benutzen hierzu Webanwendung Projektverlauf vergleichend Annotation benutzen zuvor genannt intertextuell Beziehung beleuchten entstanden teilergebnisse Annotationsrichtlini einblicken Annotationspraxis qualitativ quantitativ einblicke Darstellung Verteilung Figureneigenschaft Automatisierungsversuche induktiv Auswertung Explorative Annotationsphase Einbezug etabliert Figurenkonzept forster Hansen Jannidis konzeptuell Fragestellung Fokus stehen ergeben oberkategorien häufig Beschreibung literarisch Figur verwenden Analysekategorie automatisierungsorientiert vergleichend Annotation Frage abb zeigen Figur Rolle Charaktereigenschaft nah beschreiben Tabelle Annotation legen Annotatorinn Interpretationsaufwand Skala niedrig fest ausgehend Annahme gering Interpretationsgrad Explizit Text thematisiert Figureneigenschafte hoch Interpretationsgrad implizit Figureneigenschaft hindeuten fungieren Marker Grad Explizitheit Auswertung Teil Annotationsdat zeigen Interpretationsgrad meistens gering einschätzen Befund individuell Annotationsentscheidung aufzufassen lässt Schlussfolger eigenschaften explizit erwähnen abb implizit eigenschaften eher Randphänomen darstellen niedrig Interpretationsgrad finden kategori Kleidung alt Physiognomie hoch kategori Rolle Charakter eigenschaften niedrig interpretationsgrad angeben Explizite eigenschaften Text häufig hochgradig intertextuell relativ generisch jung schön eigenschaften hoch interpretationsgrad auszeichnen einzeln Text gering Anzahl implizit eigenschaften scheinen individuell spezifischer Verbindung Text markieren eigenschaften pilotexperimente automatisch Klassifizierung Figureneigenschaft zeigen moderat Ergebnis relativ simpel Methode erreichen Performanz ausbaufähig Tabell zeigen durchschnittswerter Precision Recall Klassifikation Modell Ergebnis zeigen Klassifikation Charaktereigenschaft Physiognomie Rolle Anlauf akzeptabel funktionieren Modell Problem alt Kleidung klassifizieren Precision Rolle deutlich niedrig Kategorie Tabelle klassifikationsergebnisse erkennen figureneigenschaften geplant arbeitspakete betreffen Automatisierung vergleichend Annotation Verbesserung automatisch Erkennung kategorien offen Frage ideal Kotext brauchen automatisiert Entscheidung bezüglich Figureneigenschafte treffen,"[('figureneigenschaft', 0.4758842539319526), ('interpretationsgrad', 0.3172561692879684), ('vergleichend', 0.29055904718975356), ('eigenschaften', 0.20273049129976314), ('annotatorinn', 0.1787270842461399), ('annotation', 0.15262279621577549), ('implizit', 0.11827583658998181), ('niedrig', 0.11710126167415673), ('intertextuell', 0.11289212847907376), ('charaktereigenschaft', 0.1057520564293228)]"
2025,DHd2025,LEMKE_Marc_Raumreferentielle_Ausdr_cke_in_deutschsprachigen_.xml,Raumreferentielle Ausdrücke in deutschsprachigen Romanen des 19. und 20. Jahrhundert. Ein Werkstattbericht des Projekts CANSpiN,"Marc Lemke (Universität Rostock, Deutschland); Nils Kellner (Universität Rostock, Deutschland); Ulrike Henny-Krahmer (Universität Rostock, Deutschland)","Computational Literary Studies, Raum, Annotation, Roman","Entdeckung, Räumliche Analyse, Annotieren, Text","Das Poster präsentiert erste Daten aus dem Projekt ""Computational Approaches to Narrative Space in 19th and 20th Century Novels"" (CANSpiN), die aus der Annotation raumreferentieller Ausdrücke mithilfe der Annotationsrichtlinie CANSpiN.CS1 (Category Set 1) in deutschsprachigen Romanen des 19. und 20. Jahrhunderts hervorgehen. Ziel des Posters ist es, Konzeption und Potentiale offenzulegen, die die mit der Richtlinie erzeugten Daten für quantitative und qualitative Auswertungen im literaturwissenschaftlichen Kontext bieten. Grundlage der Annotation ist ein epistemologisches Raumverständnis: Raum, wie er sich anhand punktueller raumreferentieller Ausdrücke in unserer Sprache und in den von uns untersuchten Romanen als kulturell verankertes Konzept darstellt. In dieser Perspektive ist Raum hierarchisch in Bezugnehmend auf diese Phänomene und in Anlehnung an die Systematiken von Raumindikatoren bei Mareike Schumacher (2023) und von raumreferentiellen Bezeichnungen bei Katrin Dennerlein (2009) haben wir fünf Kategorien raumreferentieller Ausdrücke gebildet, die in 21 Klassen ausdifferenziert sind (siehe Abbildung 1). Die Annotationsrichtlinie CANSpiN.CS1 zielt grundsätzlich darauf ab, über die raumreferentiellen Ausdrücke, deren Menge, Verteilung, Auswahl und Korrelation die Die damit erzeugten Annotationen können einerseits für rein quantitative Analysen im Sinne eines Distant Reading auf ganzen Textkorpora verwendet werden. Auf die Relevanz dieses Ansatzes für literaturhistorische Fragestellungen wurde im Allgemeinen bereits hingewiesen. (Moretti 2013, 48f, 53f; Underwood 2019, 3) Speziell anhand räumlichen Vokabulars sind entsprechende Untersuchungen bislang von Schumacher (2023) unternommen worden, deren Ansatz jedoch keine semantische Subklassifikation beinhaltet. Andererseits bieten die Annotationen auch das Potential, Ausgangspunkt für Mixed-Methods-Ansätze und textimmanente Interpretationen zu sein. Um dies nachzuvollziehen, werden im Folgenden exemplarisch die Annotationen des 1. Kapitels von Gustav Freytags ""Die verlorene Handschrift"" diskutiert. (Freytag 2021) Die Definitionen der hier besprochenen Annotationsklassen werden dafür in aller Kürze dargelegt. Für eine umfassende Einsicht in das Kategoriesystem verweisen wir auf die publizierte Annotationsrichtlinie. (Henny-Krahmer et al. 2024) Abbildung 2 zeigt die Annotationsmengen in allen Kategorien. Das Gros der Annotationen bilden Bewegungen und Orte. Unter ersterem verstehen wir Verben, die eine räumliche Distanz dadurch produzieren, dass sie eine gerichtete Bewegung von Subjekten und Objekten ausdrücken, aber auch Wahrnehmungen und die Produktion von Licht, Schall und Gerüchen. Die hohe Anzahl von BEWEGUNG-SUBJEKT- und BEWEGUNG-SCHALL-Annotationen ist in der Exposition von Freytags Text Ausdruck dessen, dass hier häufig Figurenbewegungen und Gespräche dargestellt sind. Die hohe Anzahl von BEWEGUNG-ALT-Fällen ist zu einem wesentlichen Teil Ergebnis der häufig bildlichen Sprache, mit der die Welt beschrieben wird: Nicht-räumliche Sachverhalte, die mittels eines räumlichen Vokabulars ausgedrückt werden, erfassen wir generell mit den ALT-Klassen (siehe Abbildung 1). Abbildung 3 lenkt den Fokus auf die Ortsannotationen, unter anderem auf Container: Bereiche und Räume, in denen sich Figuren prototypischerweise aufhalten könnten. Sind diese Container im Satzzusammenhang ein Ziel- oder Ausgangspunkt von Bewegungen, stehen sie in einem Bewegungskontext (ORT-CONTAINER-BK). Ein Aspekt, der bei der Betrachtung der Daten auffällt ist, dass Container hier viel häufiger erwähnt werden, als dass sie in einem Bewegungskontext stehen: Figuren-Bewegungen und Wahrnehmungen finden eher innerhalb von Containern und nicht zwischen ihnen statt. Mit Blick auf die Analyse ganzer Texte legen die dargestellten Beziehungen zwischen der Räumlichkeit eines Erzähltextes und seiner Diegese Möglichkeiten nahe, die Annotationen als Einstiegspunkt für textimmanente Interpretationen und Textvergleiche nutzen zu können. Vor dem Hintergrund eines raumtheoretisch fundierten Gattungsbegriffs des Reiseromans beispielsweise ist zu erproben, ob die Menge an ORT-CONTAINER-BK- im Verhältnis zu ORT-CONTAINER-Annotationen als ein gattungsspezifisches Muster funktioniert. (Vgl. Sicks 2009, 342f) Und in einer umfassenderen Perspektive bietet das Verhältnis von Räumlichkeit zum erzähltem Raum grundsätzlich das Potential, ein Kennzeichnen spezifischer Schreibweisen zu sein: Wird im Text viel oder wenig räumliches Vokabular für die Konstruktion der erzählten Welt verwendet? Ist der Text in seiner Sprache sozusagen ""räumlicher"" als die erzählte Welt oder nutzt er all sein räumliches Vokabular zur Darstellung der Diegese?",de,Poster präsentieren daten Projekt computational approach to narrativ Space and Century Novels Canspin Annotation raumreferentieller ausdrück Mithilfe Annotationsrichtlinie Category set deutschsprachig Roman Jahrhundert hervorgehen Ziel Poster Konzeption potential Offenzuleg Richtlinie erzeugt daten quantitativ qualitativ Auswertung literaturwissenschaftlich Kontext bieten Grundlage Annotation epistemologisch Raumverständnis Raum anhand punktuell Raumreferentieller Ausdrück Sprache untersucht Roman kulturell verankert Konzept darstellen Perspektive Raum hierarchisch Bezugnehmend phänomen Anlehnung Systematike Raumindikator Mareike Schumacher Raumreferentielle Bezeichnung Katrin Dennerlein kategorie Raumreferentieller Ausdrück bilden Klasse ausdifferenzieren sehen Abbildung Annotationsrichtlinie zielen grundsätzlich Raumreferentiell ausdrücken Menge Verteilung Auswahl Korrelation erzeugten annotation einerseits rein quantitativ Analyse Sinn distant Reading Textkorpora verwenden Relevanz Ansatzes literaturhistorisch Fragestellung hinweisen moretti underwood speziell anhand räumlich Vokabular entsprechend Untersuchung bislang Schumacher unternehmen Ansatz semantisch Subklassifikation beinhalten andererseits bieten annotatio Potential Ausgangspunkt textimmanent Interpretation nachvollziehen folgend Exemplarisch annotatio Kapitels Gustav freytags verloren Handschrift diskutieren Freytag definitionen Besprochen annotationsklassen Kürze darlegen umfassend Einsicht Kategoriesystem verweisen publiziert Annotationsrichtlinie et Abbildung zeigen Annotationsmeng Kategorie Gros annotatio bilden Bewegung Ort erster verstehen verben räumlich Distanz produzieren gerichtet Bewegung Subjekt objekt Ausdrücken wahrnehmung Produktion Licht Schall gerüchen hoch Anzahl Exposition freytags Text Ausdruck häufig figurenbewegung Gespräch darstellen hoch Anzahl wesentlich Ergebnis häufig bildlich Sprache Welt beschreiben sachverhalen mittels räumlich Vokabular ausdrücken erfassen generell sehen Abbildung Abbildung lenken Fokus Ortsannotation Container Bereich räume figur Prototypischerweise aufhalten können Container Satzzusammenhang Ausgangspunkt Bewegung stehen Bewegungskontext Aspekt Betrachtung daten auffällen Container häufig erwähnen Bewegungskontext stehen Wahrnehmung finden eher innerhalb containern Blick Analyse Text legen dargestellt Beziehung Räumlichkeit erzähltextes Diegese Möglichkeit nahe annotatio Einstiegspunkt textimmanent Interpretation Textvergleiche nutzen Hintergrund raumtheoretisch fundiert Gattungsbegriff Reiseroman beispielsweise erproben Menge Verhältnis gattungsspezifisch Muster funktionieren Sicks umfassender Perspektive bieten Verhältnis Räumlichkeit erzählt Raum grundsätzlich Potential kennzeichn spezifisch Schreibweise Text räumlich Vokabular Konstruktion erzählt Welt verwenden Text Sprache sozusagen räumlich erzählt Welt nutzen all räumlich Vokabular Darstellung Diegese,"[('räumlich', 0.28636229816738273), ('raumreferentieller', 0.20950546374138362), ('container', 0.20950546374138362), ('vokabular', 0.17823419203559507), ('bewegung', 0.16038434156847461), ('annotationsrichtlinie', 0.15621088819043483), ('ausdrück', 0.1491004432338846), ('annotatio', 0.14689716655357735), ('erzählt', 0.14055525168682914), ('freytags', 0.1396703091609224)]"
2025,DHd2025,SLUYTER_G_THJE_Henny_QUADRIGA_Fallstudien_zur_Datenkompetenz.xml,"QUADRIGA-Fallstudien zur Datenkompetenz in den Humanities: Jupyter Books als ""scalable Open Educational Resources""","Henny Sluyter-Gäthje (Universität Potsdam, Deutschland); Daniil Skorinkin (Universität Potsdam, Deutschland); Peer Trilcke (Universität Potsdam, Deutschland); Maria Chlastak (Gesellschaft für Informatik, Deutschland); Evgenia Samoilova (Universität Potsdam, Deutschland); Melanie Seltmann (Humboldt-Universität zu Berlin, Deutschland)","Open Educational Resource, Jupyter Book, OER, Lehrmaterial, Fallstudie, Datenkompetenz","Veröffentlichung, Einführung, Lehre, Text","Datenkompetenz, die sowohl den kritischen Umgang mit Daten (von Sammeln über Verwalten bis hin zu Bewerten) als auch die Analyse und Interpretation von Daten (vgl.¬† Ridsdale et al., 2015) umfasst, wird auch für traditionell ausgebildete Geisteswissenschaftler:innen zunehmend wichtiger. Im Berlin-Brandenburgischen Datenkompetenzzentrum QUADRIGA Um den verschiedenen Dimensionen der Kompetenzvermittlung gerecht zu werden, die sich von basalen technischen Fähigkeiten über methodenspezifisches Wissen bis hin zur Daten- und Methodenkritik erstrecken, orientieren sich die von QUADRIGA entwickelten OER an Fallstudien (Foran, 2001) und setzen so problembasiertes Lernen (Kay et al., 2000) ein. Das heißt, QUADRIGA geht von einer konkreten Fragestellung aus Jede dieser Aufgaben ist in strukturell identische Komponenten unterteilt: Jupyter-Book Die Struktur des Books wird über ein Inhaltsverzeichnis hierarchisch festgelegt. Mit Hilfe dieser Struktur lassen sich die Aufgaben und ihre Komponenten gut abbilden. Wie auf der Landing-Page (Abb. 02) zu sehen, besteht die Website aus drei vertikalen Abschnitten. Dadurch kann die Fallstudie zum einen chronologisch durchlaufen werden, zum anderen können einzelne Aufgaben und Komponenten direkt angesteuert werden. So kann selbst ein Lernpfad im Sinne des aktiven Lernens (Markant, 2016) gewählt werden. Aufgaben, Komponenten und ihre Bestandteile können über URLs referenziert werden, was die Modularität des Books weiter erhöht. Die interaktiven Komponenten werden in Jupyter Notebooks erstellt, in denen sowohl Markdown als auch Python-Code geschrieben werden kann. So können kleine UI-Elemente erzeugt werden wie die Worteingabe in Abb. 03. Zusätzlich kann das gesamte Jupyter Notebook über Google Colab Markdown ermöglicht eine multimodale Gestaltung (durch Tabellen, Abbildungen, GIFs, Querverweise etc.), bibliographische Angaben werden über BibTeX erzeugt. Mittels der MyST (Markedly Structured Text) Markdown-Erweiterung lassen sich weitere UI-Elemente hinzufügen, z.B. Informationsboxen (Abb. 04). Diese Strukturierungsmöglichkeit macht Informationen leicht auffindbar (scannable) und Inhalte schnell überfliegbar (skimmable). Die Realisation als Jupyter Book ermöglicht es, eine Modularisierung der Fallstudie vorzunehmen und so atomisierte Einheiten zu spezifizierten Lernzielen aus dem Feld des Datenkomptenzerwerbs zu erstellen und referenzierbar zu machen. Darüber hinaus birgt das technische Konzept von Jupyter Books die Möglichkeit, etwas zu entwickeln, was wir ""scalable OER"" nennen. Die dargestellte Fallstudie kann nämlich auf verschiedenen ""levels of expertise"" durchgeführt werden: Die Modularisierbarkeit sowie die flexible Skalierbarkeit von Jupyter Books bieten das Potential, eine Vielzahl von Lernenden mit unterschiedlichem Vorwissen zu erreichen. Fördervermerk QUADRIGA wird im Rahmen der Richtlinie Förderung von Projekten zum Aufbau von Datenkompetenzzentren in der Wissenschaft des Bundesministeriums für Bildung und Forschung unter Kennzeichen 16DKZ2034A, 16DKZ2034G und 16DKZ2034H gefördert.",de,Datenkompetenz sowohl kritisch Umgang daten Sammel verwalter bewerten Analyse Interpretation daten ridsdal et umfassen traditionell ausgebildet Geisteswissenschaftl innen zunehmend wichtig Datenkompetenzzentrum Quadriga verschieden Dimension Kompetenzvermittlung gerecht basal technisch Fähigkeit methodenspezifisch wissen Methodenkritik erstrecken orientieren Quadriga entwickelt oer Fallstudi foran setzen problembasiertes lernen kay Et quadriga konkret Fragestellung Aufgabe strukturell identisch Komponent unterteilen Struktur Book inhaltsverzeichnis hierarchisch festlegen Hilfe Struktur lassen Aufgabe Komponente abbilden abb sehen bestehen Website Vertikale abschnitten Fallstudie chronologisch durchlaufen einzeln Aufgabe Komponente direkt angesteueren Lernpfad Sinn aktiv Lernen markant wählen Aufgabe Komponente Bestandteil urls referenzieren Modularität Book erhöhen interaktiv Komponente Jupyter Notebooks erstellen sowohl Markdown schreiben erzeugen Worteingabe abb zusätzlich gesamt Jupyter Notebook Google Colab Markdown ermöglichen multimodal Gestaltung Tabell abbildungen gif querverweise bibliographisch Angabe Bibtex erzeugen mittels Myst Markedly structured Text lassen hinzufügen informationsbox abb Strukturierungsmöglichkeit Information auffindbar scannabel inhalen schnell überfliegbar skimmable Realisation Jupyter Book ermöglichen Modularisierung Fallstudie vornehmen atomisiert einheiten spezifizierten Lernziele Feld Datenkomptenzerwerb erstellen referenzierbar hinaus bergen technisch Konzept Jupyter Books Möglichkeit entwickeln scalabel oer nennen dargestellt Fallstudie nämlich verschieden levels -- expertise durchführen Modularisierbarkeit flexibel Skalierbarkeit Jupyter Books bieten Potential Vielzahl lernende unterschiedlich Vorwissen erreichen Fördervermerk Quadriga Rahmen Richtlinie Förderung Projekt Aufbau datenkompetenzzentren Wissenschaft Bundesministerium Bildung Forschung Kennzeichen g fördern,"[('quadriga', 0.32927305773512894), ('jupyter', 0.3243840732203587), ('komponente', 0.19088973813094529), ('book', 0.17575230966489533), ('fallstudie', 0.1721180916977385), ('aufgabe', 0.16529859199664002), ('oer', 0.16463652886756447), ('markdown', 0.15334644526453173), ('books', 0.1297536292881435), ('abb', 0.10489644344031014)]"
2025,DHd2025,NANTKE_Julia_Projekt_MuMokA.xml,Projekt MuMokA - Multimodale Modellierung kultureller Artefakte im digitalen Raum,"Julia Nantke (Universität Hamburg, Deutschland); Frank Steinicke (Universität Hamburg, Deutschland); Vanessa Klomfaß (Universität Hamburg, Deutschland); Qianqi Huang (Universität Hamburg, Deutschland)","Multimodalität, born digital, Datenrestaurierung, Modellierung","Inhaltsanalyse, Modellierung, Annotieren, Visualisierung, Literatur, Multimodale Kommunikation","Den Ausgangspunkt für das Projekt MuMokA ( Die Multimodalität, die konzeptionelle Unvollständigkeit und der fragmentarische Charakter der  Das Ziel des Projekts ist die Entwicklung von prototypischen Szenarien zur teilautomatisierten Exploration und Strukturierung sowie zur digitalen Repräsentation multimodaler born-digital-Korpora. Die Exploration und Modellierung der Strukturen und Inhalte des Korpus erfolgt anhand zweier komplementärer Zugriffe, die tendenziell einem Close und einem Distant Reading-Ansatz zuzuordnen sind. 1. Manuelle Modellierung der Strukturen und Inhalte des Korpus in einer Tabelle mit vier hierarchisch angeordneten und untereinander dynamisch verknüpften Ebenen, welche die überlieferte Ordnerstruktur mit der von Kempowski angelegten Ordnung verbinden und jedes Dokument in seiner Position im Korpus verortbar machen (siehe Abbildung 1). 2.1 Nutzung von gKI zur Datenextraktion und -kategorisierung, um die Zuordnung von Dokumenten zu den verschiedenen Ebenen der Tabelle soweit wie möglich zu automatisieren (vgl. Abb. 2). Wir folgen dem Prompting-Ansatz von Marvin et al. 2024 und nutzen aktuell die von der Universität Hamburg lizensierte Version UHHgpt 4omni (vgl. 2.2 Einsatz von maschinellen Lernverfahren zur Korpusanalyse mittels u.a. Named Entity Recognition, Topic Modeling und Multidimensional Scaling (vgl. Abb. 3).  Während wir bereits Konzepte zur Wiederherstellung und Strukturierung der Daten entwickelt haben, liegt unser aktueller Fokus auf der Exploration und Repräsentation der Inhalte des Korpus mittels BERTopics (Grootendorst 2022, vgl. Abb. 2). Hierbei planen wir ebenfalls, künftig verstärkt die anderen Modalitäten wie Bilder und Tondateien einzubeziehen. Darauf aufbauend werden wir an Konzepten arbeiten, um das Korpus für andere Wissenschaftler:innen zugänglich zu machen und das fragmentierte Kunstwerk",de,Ausgangspunkt Projekt Mumoka Multimodalität konzeptionell Unvollständigkeit fragmentarisch Charakter Ziel Projekt Entwicklung prototypisch Szenarie teilautomatisierten Exploration Strukturierung digital Repräsentation Multimodaler Exploration Modellierung Struktur Inhalt Korpus erfolgen anhand zwei komplementär zugriffe tendenziell clos distant zuzuordnen Manuelle Modellierung Struktur Inhalt korpus Tabelle hierarchisch angeordnet untereinander dynamisch verknüpft Ebene überliefert Ordnerstruktur Kempowski angelegt Ordnung verbinden jeder Dokument Position Korpus verortbar sehen Abbildung Nutzung gki Datenextraktion Zuordnung dokumenten verschieden Ebene Tabelle soweit automatisieren abb folgen Marvin Et nutzen aktuell Universität Hamburg lizensiert Version Uhhgpt Einsatz Maschinelle Lernverfahr Korpusanalyse mittels named entity Recognition Topic Modeling Multidimensional scaling abb Konzept Wiederherstellung Strukturierung daten entwickeln liegen aktuell Fokus Exploration Repräsentation Inhalt Korpus mittels Bertopics Grootendorst abb hierbei planen ebenfalls künftig verstärkt modalitäten Bild Tondateien einbeziehen aufbauend konzept arbeiten korpus Wissenschaftler innen zugänglich fragmentiert Kunstwerk,"[('exploration', 0.207769048322691), ('inhalt', 0.2031696763894495), ('korpus', 0.19430041326314898), ('strukturierung', 0.19280495267669093), ('abb', 0.15586888724745862), ('tabelle', 0.1325918423735014), ('repräsentation', 0.12504821265978602), ('lizensiert', 0.12231926895345034), ('lernverfahr', 0.12231926895345034), ('ordnerstruktur', 0.12231926895345034)]"
2025,DHd2025,ACHMANN_Michael_Aspektbasierte_Sentimentanalyse_von_Bookstag.xml,Aspektbasierte Sentimentanalyse von Bookstagram-Posts,"Emma Sophie Reichert (Universität Regensburg, Deutschland); Anna-Lena Babl (Universität Regensburg, Deutschland); Kyuhee Kim (Universität Regensburg, Deutschland); Michael Achmann-Denkler (Universität Regensburg, Deutschland); Christian Wolff (Universität Regensburg, Deutschland)","Aspektbasierte Sentimentanalyse, Social Media Analyse, Bookstagram, GPT, LLM","Inhaltsanalyse, Multimodale Kommunikation, Text","Der Buchmarkt befindet sich in einem Wandel, der unter anderem durch die zunehmende Präsenz von Büchern auf Online-Plattformen geprägt ist. Sogenannte ""Buchblogger"" verbreiten auf Social Media Inhalte über Bücher (Giacomuzzi 2021) und werden dabei nicht selten von Verlagen durch besondere Aktionen oder kostenlose Exemplare, sogenannte Rezensionsexemplare, unterstützt. Aufkleber wie ""#TikTokMadeMeBuyIt"" auf Büchern in Buchgeschäften (Sahner 2023) und Bestseller-Listen auf TikTok, verdeutlichen die wachsende Rolle sozialer Medien bei Kaufentscheidungen: Laut Angaben des Plattformbetreibers wurden 2023 in Deutschland über 12 Millionen '#BookTok'-Bücher verkauft (TikTok Technology Limited 2024). Auch auf Instagram hat sich eine aktive Buch-Community etabliert, die durch ihre Reichweite und Interaktionen zum Erfolg von Büchern beiträgt. Diese Arbeit befasst sich mit der aspektbasierten Stimmungsanalyse von Buchrezensionen auf Instagram. Dabei wurden folgende Forschungsfragen untersucht: Die Ergebnisse dieser Analyse ergaben neue Einblicke in die Möglichkeiten der aspektbasierten Stimmungsanalyse und lieferten Aufschlüsse über die Buch-Community auf Instagram, das sogenannte Bookstagram. Untersuchungsgegenstand dieser Arbeit waren 3745 deutschsprachige Buchrezensionen der Bookstagram-Community (siehe Abbildung 1 für ein Beispiel). Diese wurden von 144 Accounts mit unterschiedlichen Reichweiten über die Suche nach Hashtags mithilfe des Analysetools CrowdTangle gesammelt. Alle Posts enthielten dabei das Wort ""Rezension"" oder die Kurzform ""Rezi(e)"" sowie mindestens einen der folgenden Hashtags: #bookstagramgermany, #bookstagramdeutschland, #buchrezension, #buch, #bookstagram, #bücherliebe, #leseliebe, #buchblogger, #buchcommunity. Die aspektbasierte Stimmungsanalyse liefert ein detaillierteres Verständnis der Stimmung, indem sie den Text in verschiedene Aspekte zerlegt und deren Stimmungen einzeln analysiert (Kim and Song 2022). Die betrachteten Aspekte dieser Arbeit wurden anhand vorheriger Forschungsarbeiten (Zhang et al. 2019, Stollfuß 2023) unter Einbeziehung weiterführender Überlegungen festgelegt. Zhang et al. zeigten, dass auf sozialen Medien inhaltsbezogene Aspekte öfter vorkommen als externe Faktoren (2019). Die externen Faktoren (z.B. Cover, Schriftart, Illustrationen) wurden daher zu einem Aspekt zusammengefasst, während inhaltliche Aspekte weiter aufgeteilt wurden. Für unsere Analyse ergaben sich folgende Aspekte: Autor, Schreibstil, externe Faktoren, Charaktere, Logik und Handlung/Spannung. Darüber hinaus wurde das allgemeine Sentiment jeder Rezension bestimmt. Das Sentiment wurde als ""Positiv"", ""Neutral"", ""Negativ"" oder ""Nicht vorhanden"" bewertet. Nach der iterativen Entwicklung eines Prompts wurde die aspektbasierte Stimmungsanalyse mit den GPT-Modellen ""gpt-3.5-turbo-0613"" und ""gpt-4-1106-preview"" durchgeführt und verglichen. Die Reliabilität der Ergebnisse wurde durch die Bewertung von 250 zufällig ausgewählten Beiträge des Datensatzes von sechs menschlichen Annotatoren sichergestellt. Die Ergebnisse lassen auf die Verlässlichkeit des methodischen Vorgehens für künftige Arbeiten schließen. Für sechs von sieben Aspekten ergaben sich mit GPT-3.5 gewichtete F1-Scores zwischen 77% und 94% und mit GPT-4 zwischen 79% und 100%. Die davon abweichenden gewichteten F1-Scores bei dem Aspekt Der Vergleich der GPT-Modelle ergab, dass die Erkennung von Rezensionsexemplaren mit GPT-3.5 (F1-Score = 0,965) zuverlässiger war, als mit GPT-4 (F1-Score = 0,905). Diese Erkennung war zentral für die Beantwortung unserer Forschungsfragen. Außerdem waren die von GPT-4 zurückgegebenen Daten teilweise unvollständig oder nicht richtig formatiert. Für die Untersuchung der weiteren Fragestellungen wurde deshalb das GPT-3.5 Modell gewählt. Bei der Betrachtung der Verteilung der Sentiments fällt auf, dass für jeden Aspekt, außer für Für die Kategorie Es wurde untersucht, ob es Unterschiede zwischen Foto-Posts (nur ein Bild) und Album-Posts (mehrere Bilder) bezüglich der Verteilung des Sentiments gibt. Dabei konnten in der Kategorie Anhand des von CrowdTangle bestimmten Overperforming Scores konnte mittels Spearman""s Korrelationskoeffizienten gezeigt werden, dass Beiträge umso besser performten, je negativer das allgemeine Sentiment war ( Die analysierten Bookstagram-Beiträge zeigen überwiegend positives Sentiment. Eine ähnliche Verteilung konnte bereits bei Posts über das Lesen auf Instagram (Zhan et al., 2018) und bei den Plattformen Goodreads und Amazon nachgewiesen werden (Dimitrov et al., 2015). Weiter konnten wir einen negativen Zusammenhang zwischen den betrachteten Sentiments und der Performance des Beitrags feststellen. Diese Ergebnisse stimmen mit den Erkenntnissen von Hsu et al. (2019) überein. Unsere Studienergebnisse legen nahe, dass die Veröffentlichung von Rezensionen auf Instagram, unterstützt durch das überwiegend positive Sentiment, potenziell förderlich für den Buchmarkt sein könnte. Sie geben einen Einblick in den Literaturdiskurs auf sozialen Plattformen und vertiefen das Verständnis für den Einfluss auf die Bookstagram-Community. Unsere Arbeit ist durch die begrenzten Erhebungsmöglichkeiten limitiert. Zukünftige Arbeiten könnten andere Plattformen, Buchgenres oder nicht-textuelle Bestandteile von Bookstagram-Beiträgen berücksichtigen.",de,Buchmarkt befinden Wandel zunehmend Präsenz büchern prägen sogenannter buchblogger Verbreit Social Media Inhalt büch Giacomuzzi selten Verlag besonderer aktion kostenlos exemplar sogenannter Rezensionsexemplare unterstützen Aufkleber tiktokmademebuyit büchern Buchgeschäft Sahner Tiktok verdeutlichen wachsend Rolle sozial Medium Kaufentscheidunge laut Angabe Plattformbetreiber Deutschland Million verkaufen Tiktok Technology Limited instagram aktiv etablieren Reichweit interaktionen Erfolg büchern beitragen Arbeit befassen aspektbasiert Stimmungsanalyse Buchrezensione Instagram folgend Forschungsfrag untersuchen Ergebnis Analyse ergaben einblicke Möglichkeit aspektbasiert Stimmungsanalyse liefert aufschlüsse Instagram sogenannter Bookstagram Untersuchungsgegenstand Arbeit deutschsprachig Buchrezensione sehen Abbildung Accounts unterschiedlich Reichweit Suche Hashtag Mithilfe Analysetools Crowdtangle sammeln Post enthalten Wort Rezension Kurzform Rezi e mindestens folgend Hashtag bookstagramgermany Bookstagramdeutschland Buchrezension Buch Bookstagram bücherlieben Leseliebe Buchblogger Buchcommunity aspektbasiert Stimmungsanalyse liefern detaillierter Verständnis Stimmung Text verschieden Aspekt zerlegen stimmung einzeln analysieren kim and Song betrachtet Aspekt Arbeit anhand vorherig Forschungsarbeit zhang Et Stollfuß Einbeziehung weiterführend Überlegung festlegen zhang Et zeigen sozial Medium inhaltsbezogen Aspekt öfter vorkommen extern Faktor extern Faktor cov Schriftart illustrationen Aspekt zusammengefassen inhaltlich Aspekt aufteilen Analyse ergaben folgend Aspekt Autor Schreibstil extern Faktor charaktere logik Handlung Spannung hinaus allgemein Sentiment Rezension bestimmen Sentiment positiv neutral negativ vorhanden bewerten iterativ Entwicklung Prompt aspektbasiert Stimmungsanalyse durchführen vergleichen Reliabilität Ergebnis Bewertung zufällig ausgewählt Beiträge datensatzes menschlich Annotator sicherstellen Ergebnis lassen Verlässlichkeit methodisch Vorgehen künftig Arbeit schließen Aspekt ergeben gewichtet abweichend gewichtet Aspekt Vergleich ergeben Erkennung Rezensionsexemplar zuverlässiger Erkennung zentral Beantwortung Forschungsfrag zurückgegeben daten teilweise unvollständig formatiern Untersuchung Fragestellung Modell wählen Betrachtung Verteilung Sentiment fallen Aspekt Kategorie untersuchen Unterschied Bild mehrere Bild bezüglich Verteilung Sentiment Kategorie anhand Crowdtangle bestimmt Overperforming Scores mittels Spearman s korrelationskoeffizienen zeigen beitrag umso performen negativ allgemein Sentiment analysiert zeigen überwiegend positiv Sentiment ähnlich Verteilung Post lesen Instagram zhan Et plattform Goodread Amazon nachweisen Dimitrov et negativ Zusammenhang betrachtet Sentiment Performance Beitrag feststellen Ergebnis Stimme Erkenntnis Hsu et überein Studienergebnisse legen nahe Veröffentlichung Rezension Instagram unterstützen überwiegend positiv Sentiment Potenziell Förderlich buchmarkt geben Einblick Literaturdiskurs sozial Plattforme vertiefen Verständnis Einfluss Arbeit begrenzt Erhebungsmöglichkeit limitieren zukünftig arbeiten können plattformen buchgenr Bestandteil berücksichtigen,"[('instagram', 0.29031280024064965), ('sentiment', 0.28763811615972806), ('stimmungsanalyse', 0.23225024019251972), ('aspektbasiert', 0.23225024019251972), ('aspekt', 0.21354016364978448), ('rezension', 0.1537674625410194), ('büchern', 0.1298773394319439), ('extern', 0.11686088174608054), ('tiktok', 0.11612512009625986), ('crowdtangle', 0.11612512009625986)]"
2025,DHd2025,DENNERLEIN_Katrin_Zum_Aufbau_digitaler_Dramenkorpora__OCR4al.xml,Zum Aufbau digitaler Dramenkorpora. OCR4alltoDraCorTEI als Baustein für die Edition von maschinenlesbaren Versionen historischer Dramendrucke,"Katrin Dennerlein (Julius-Maximilians-Universität Würzburg, Deutschland); Martin Rupnig (Julius-Maximilians-Universität Würzburg, Deutschland); Christian Reul (Julius-Maximilians-Universität Würzburg, Deutschland)","digitale Edition, TEI, quantitative Dramenanalyse","Transkription, Programmierung, Annotieren, Bearbeitung, Software","Die Computational Literary Studies (CLS) können nur so gut sein, wie die Korpora, die ihnen zur Verfügung stehen. Insbesondere für die Geschichte des deutschsprachigen Dramas vom 17. bis 19. Jahrhundert repräsentieren diese bislang jedoch fast nur die hochkanonischen Texte. Die Präferenz liegt, wie bereits in den kodifizierten Literaturgeschichten der Germanistik auf original deutschsprachigen Sprechtheaterwerken und dabei vorwiegend auf Tragödien (Alt, 1994, Meid, 2009: 327-501, Schulz, 2007). Hingegen bleiben Libretti, populäre Komödien und generell Übersetzungen und Dramen von Frauen zumeist gänzlich unberücksichtigt, obwohl sie die Mehrheit der gedruckten und gespielten Werke ausmachen (Jahn, 1996, Krämer, 1998, Dennerlein, 2021, Kord, 1992). Dadurch ist die Geschichte des deutschsprachigen Dramas nicht nur äußerst lückenhaft, sondern entbehrt auch zahlreicher populärer und wegweisender Werke. Die Textauswahl für einzelne Autor:innen, Genres, Textgruppen wie Repertoires oder Sammlungen ist jeweils so klein, dass quantitative, genre- und periodenvergleichende Studien nur sehr eingeschränkt durchgeführt werden können. Für eine Erforschung der Gesetzmäßigkeiten der literarischen Evolution ist die gezielte Korpuserweiterung deshalb unabdingbar. Auf edierte und normalisierte Neuausgaben, wie sie der Digitalen Bibliothek und den auf sie aufsetzenden Projekten Textgrid und GerDracor zu Grunde lagen, kann für diese Erweiterungen allerdings nicht zurückgegriffen werden, weil die fehlenden Dramentexte nicht neu ediert wurden. Dabei stellen sich sowohl editorische Fragen der Transkription und Normalisierung als auch die Fragen der Automatisierung des TEI-Taggings. Im Folgenden sollen einige Vorgehensweisen zur Edition, Volltextdigitalisierung und Textauszeichnung historischer Dramentexte mit und im Anschluss an die Open Access-Software OCR4all Das Layout von Dramendrucken um 1800 ist nicht normiert und variiert von Drucker zu Drucker, die prototypische Struktur ist jedoch wie folgt aufgebaut: Titelseiten enthalten Angaben zu Titel, Untertitel, Verfasser:in, Druckerei und zumeist auch zum Erscheinungsjahr. Es folgt das Personenverzeichnis inklusive der Figurenauflistung, gefolgt von dem Beginn des Drameninhaltes mit folgender Struktur: Akt/Aufzug > Szene/Auftritt > Ortsangabe > Figurenaufzählung > Figurenname > Dialogtext > Regieanweisung. Je früher ein Dramentext erschienen ist, desto wahrscheinlicher ist es auch, dass er eine Vorrede vor dem Personenverzeichnis enthält. Üblicherweise entspricht die Reihung der Dramenelemente im Druck der tatsächlichen Lesereihenfolge. Jedoch gibt es Fälle, die dieser Logik nicht folgen. Ein Beispiel sind am Ende der Seite abgedruckte Fußnoten im Drama ""Dido"" (1794) von Charlotte von Stein, die bestimmte Textstellen kommentieren (vgl. Abb. 1). Eine weitere Besonderheit stellt die uneinheitliche Gestaltung von Figurenaufzählungen dar. Üblicherweise beginnen Szenen mit einer Aufzählung aller in der Szene auftretenden Figuren gefolgt von den Dialogen. Einige Dramen verzichten jedoch in einzelnen Szenen auf die Aufzählung ganz oder integrieren die Nennung der Figuren in die anfängliche Regieanweisung. Um alle diese Elemente berücksichtigen zu können, sollte eine Digitalisierungsumgebung gewählt werden, die eine differenzierte semantische Auszeichnung von Segmenten erlaubt. Da es bei den knappen Ressourcen im wissenschaftlichen Bereich unabdingbar ist, eine kostenfrei nutzbare Digitalisierungsumgebung zu verwenden, die dennoch bestmögliche Ergebnisse liefert und stetig gewartet und aktualisiert wird, bietet sich OCR4all an. Einige Elemente werden nicht als Layoutregionen ausgezeichnet und werden deshalb bei der späteren Texterkennung nicht berücksichtigt. Dazu zählen insbesondere Seitenzahlen, Seitentitel und Kustoden. Auch die Seitenumbrüche gehen verloren, nicht jedoch die Zeilenumbrüche, die automatisch bei der Zeilensegmentierung in LAREX erkannt werden. Zentrale Eigenschaften des Drucks gehen auf diese Weise verloren, dafür wird der Segmentierungsprozess bzw. die händische Nachkorrektur der Segmente etwas beschleunigt und das Hauptziel 'maschinenlesbare, für die Zwecke der CLS verwendbare Dramentexte zur Verfügung zu stellen 'erreicht. Repliken werden nicht durch Seitenumbrüche, Seitenzahlen oder Kustoden unterbrochen und dadurch unbrauchbar für Stilometrie, Topic Modelling oder Sentiment bzw. Emotion Analysis (vgl. Dennerlein et al., 2023). Ziel ist es nicht, eine diplomatische Transkription zu erstellen, sondern die Datenpublikation zu gewährleisten (vgl. Sahle 2013, Teil 2: 256-266). Daher ist auch ein bestimmter Umgang in OCR4all mit druckspezifischen Zeichen wie Superskripte oder die Verwendung von Schaft-S zu gewährleisten. In OCR4all können unterschiedliche Ergebnisse erzielt werden, je nachdem welches Modell für die Texterkennung angewendet wird. Mit dieser Methodik ist es unerheblich, ob das Modell auf die exakte Erkennung der konkreten Zeichen trainiert ist oder bereits eine Normalisierung ausgewählter Zeichen berücksichtigt. Die Normalisierung ausgewählter Zeichen für die DraCorTEI-Datei kann in einem weiteren Schritt über das Konvertierungsskript vorgenommen werden. LAREX bietet zudem die Möglichkeit, die Lesereihenfolge der Textregionen individuell anzupassen, sodass Sonderfälle wie Fußnoten in der Weiterverarbeitung an der entsprechenden Stelle platziert werden können. Abb. 1 zeigt die angepasste ""Reading Order"" in LAREX: Nach Akt, Szene, Regieanweisung und Figurenangaben folgt der erste Satz der Replik, dann ist die Fußnote eingegliedert. Sie ist damit hinterher als zugehörig zu diesem Satz identifizierbar und kann 'etwa als Endnote 'die spätere digitale Edition des Dramas ergänzen. Um später alle Elemente automatisch mit XML-Elementen auszeichnen zu können, lohnt sich die akribische Vorarbeit der Regionsauszeichnung, die bei entsprechender Übung nur etwa 5 Minuten pro Seite dauert. Mittelfristig sollen diese Auszeichnungen als Trainingsdaten verwendet werden, um den Automatisierungsgrad kontinuierlich zu steigern. Anschließend werden die ausgezeichneten Regionen vollautomatisch in Zeilen zerlegt. Es folgt die eigentliche Texterkennung, bei der aus den Bildzeilen der maschinenlesbare Text mittels Modellen extrahiert wird. Hierbei können entweder existierende, ""gemischte Modelle"" direkt angewendet werden oder ""werksspezifische Modelle"", durch gezieltes Training auf die Erkennung einer bestimmten Drucktype hin optimiert werden. Derzeit kann bei Erkennung mit einem gemischten Modell zuverlässig eine Zeichengenauigkeit von über 98 % erreicht werden, meist sogar über 99 %. Durch das Training gemischter Modelle kann die Genauigkeit noch deutlich weiter gesteigert werden. Die dazu benötigten Trainingsdaten können ebenfalls in LAREX erstellt werden und bestehen aus Bildzeilen sowie der korrekten Transkription des darauf zu sehenden Texts. Das Ergebnis des Digitalisierungsprozesses in OCR4all, wie auch von fast allen anderen OCR-Programmen, sind PAGE-XMLs, die neben dem gesamten Textinhalt der einzelnen Seiten weitere Informationen wie Erstellungsdatum, Metadaten, Layoutregionen und Koordinaten enthalten. Die Angabe der Reading-Order ist für jene Fälle wichtig, in denen die Koordinaten von Textregionen als Information nicht ausreichen, um die gewünschte Struktur in DraCorTEI abzubilden. Mit EzDrama existiert bereits ein Konvertierungsskript, das Dramen, die im Plaintext in lateinischer Schrift vorliegen, teilautomatisch mit den Tags von DraCorTEI auszeichnet (Skorinkin et al., 2022). Da auch diese Dramen in der Regel nicht vollständig automatisch in TEI konvertiert werden können, wurde eine Markup-Sprache mit sehr wenigen Markups und Regeln entworfen, bei der Markierungen für bspw. Szenen, Sprecher:in oder Regieanweisungen direkt in den Text hineingeschrieben werden. In einem Colab oder mithilfe eines Jupyter-Notebooks können die so ausgezeichneten Dramen dann mit TEI-Tags ausgezeichnet werden, wie sie in DraCor verwendet werden. Dazu müssen jedoch zwei Voraussetzungen erfüllt sein: Zum einen müssen die Texte bereits maschinenlesbar in Form von lateinischer Druckschrift vorliegen, zum anderen muss Zeit für eine händische Auszeichnung der Dramen in einem proprietären Format aufgewendet werden. Das im Folgenden vorgestellte Skript versteht sich ausdrücklich als Ergänzung dieser verdienstvollen Arbeit und ist für diejenigen Fälle vorgesehen, in denen beide Voraussetzungen nicht gegeben sind. Das im Folgenden kurz charakterisierte Skript wandelt die PAGE-XMLs in eine gültige DraCorTEI-Datei um. Das Skript spielt eine zentrale Rolle in der automatisierten Erfassung und Verarbeitung der Dramendrucke, indem es spezifische Merkmale und Besonderheiten der Dramendrucke adressiert. Es beinhaltet vier Klassen, die folgenden Zwecken dienen: Das Skript verarbeitet der Reihe nach alle in einem Ordner liegenden PAGE-XML-Dateien und arbeitet sich von der äußersten zur innersten Ebene vor: Seite > TextRegion > TextLine > Wort > Glyphe. Bei Initialisierungsfehlern bei der Verarbeitung werden Fehlermeldungen mit Angaben zur entsprechenden Stelle und dem Dateinamen ausgegeben, um Fehler in der Vorbearbeitung oder Beschädigungen in den Dateien zu finden. Für den Fall, dass das Skript Textregionen verarbeitet, die falsch platziert sein sollten oder deren Inhalt einer falschen Textregion entsprechen, wird in der DraCorTEI-Datei an entsprechender Stelle ein Tag mit dem Inhalt ""WARNING"" ausgegeben, die eine notfalls mögliche händische Nachbearbeitung ermöglicht. Auch in diesem Fall soll gewährleistet werden, dass Fehler in der Vorbearbeitung in OCR4all ausfindig gemacht und korrigiert werden können. Eine der Besonderheiten, die in der Bearbeitung durch das Skript berücksichtigt werden, sind spezielle Zeichen, die für die endgültige DraCorTEI-Datei normalisiert werden müssen. Folgende regelmäßig vorkommende Zeichen werden dementsprechend normalisiert: ≈ø ‚Üí s,  í ‚Üí z, aÕ§ oÕ§ uÕ§ ‚Üí ä ö ü, etc. Sollten in etwaigen Projekten, die dieser Methodik folgen, weitere spezielle Sonderheiten in den Drucken auftreten, können diese durch kleine Anpassungen im Skript mit aufgenommen werden. Mit dem hier beschriebenen Verfahren der Auszeichnung, der Transkription mit OCR4all und der XML-Kodierung, benötigt die Digitalisierung eines Dramas derzeit noch immer acht bis zehn Stunden. Verzichtet man auf die Korrektur des automatisch erfassten OCR-Textes, weil man bspw. sehr große Textmengen erfassen möchte, bei denen Fehler nicht mehr ins Gewicht fallen, verkürzt sich die Bearbeitungszeit um die Hälfte. Für andere Zeitabschnitte müsste das Verfahren zudem angepasst werden, wenn das Layout signifikant abweicht. Ein besonderer Fall sind bspw. Drucke wie die Libretti der Insgesamt ist zu bedenken, dass der Prozess der Volltextdigitalisierung historischer Dramentexte verhältnismäßig komplex ist und dass man für ideale Ergebnisse deutlich mehr Zeit investieren muss als etwa für Prosatexte. Die händisch segmentierten und korrigierten Daten können jedoch als Trainingsdaten genutzt werden, so dass der Automatisierungsgrad in Zukunft sukzessive gesteigert werden kann.",de,Computational literary Studies cls Korpora Verfügung stehen insbesondere Geschichte deutschsprachig Dramas Jahrhundert repräsentieren bislang fast hochkanonisch Text Präferenz liegen Kodifizierte literaturgeschichten Germanistik Original deutschsprachigen sprechtheaterwerken vorwiegend Tragödie alt Meid Schulz hingegen bleiben libretti populär Komödie generell übersetzungen dramen Frau zumeist gänzlich unberücksichtigt obwohl Mehrheit gedruckt gespielt werk ausmachen Jahn Krämer dennerlein Kord Geschichte deutschsprachig Drama äußerst lückenhaft entbehren zahlreich Populärer wegweisender Werk Textauswahl einzeln Autor innen genre textgruppen Repertoires Sammlung jeweils klein quantitativ periodenvergleichend Studie einschränken durchführen Erforschung Gesetzmäßigkeit literarisch Evolution gezielt Korpuserweiterung unabdingbar Edierte normalisiert neuausgaben digital Bibliothek aufsetzend Projekt Textgrid Gerdracor Grund liegen Erweiterunge zurückgegriffen fehlend Dramentexte neu edieren stellen sowohl editorisch Frage Transkription Normalisierung Frage Automatisierung folgend vorgehensweisen Edition Volltextdigitalisierung Textauszeichnung historisch Dramentexte Anschluss op Layout dramendrucken normiert variieren Drucker Drucker prototypisch Struktur folgen aufbauen Titelseit enthalt Angabe Titel Untertitel Verfasser druckerei zumeist Erscheinungsjahr folgen Personenverzeichnis inklusive Figurenauflistung folgen Beginn drameninhaltes folgend Struktur akt Aufzug Szene Auftritt ortsangabe Figurenaufzählung figurenname Dialogtext Regieanweisung Dramentext erscheinen desto wahrscheinlich Vorrede Personenverzeichnis enthalten üblicherweise entsprechen Reihung Dramenelemente Druck tatsächlich lesereihenfolge Fall Logik folgen Seite Abgedruckt fußnoten Drama Dido Charlotte Stein bestimmt Textstelle kommentieren abb Besonderheit stellen uneinheitlich Gestaltung figurenaufzählung dar üblicherweise beginn Szene Aufzählung Szene Auftretende Figur folgen Dialog Dram verzichten einzeln Szene Aufzählung integrieren Nennung Figur anfänglich Regieanweisung element berücksichtigen Digitalisierungsumgebung wählen differenziert semantisch Auszeichnung Segment erlauben knapp Ressource wissenschaftlich Bereich unabdingbar Kostenfrei nutzbar Digitalisierungsumgebung verwenden dennoch bestmöglich Ergebnis liefern stetig warten aktualisieren bieten element Layoutregione auszeichnen spät Texterkennung berücksichtigen zählen insbesondere seitenzahlen Seitentitel Kustode seitenumbrüch verlieren zeilenumbrüch automatisch Zeilensegmentierung Larex erkennen Zentrale eigenschaften Druck Weise verlieren segmentierungsprozess händisch Nachkorrektur Segment beschleunigen Hauptziel maschinenlesbare Zweck Cls verwendbar dramentexte Verfügung stellen erreichen Replike seitenumbrüch seitenzahlen Kustode unterbrechen unbrauchbar Stilometrie Topic Modelling Sentiment Emotion Analysis dennerlein et Ziel diplomatisch Transkription erstellen Datenpublikation gewährleisten sahlen bestimmt Umgang druckspezifisch Zeichen Superskript Verwendung gewährleisten unterschiedlich Ergebnis erzielen Modell Texterkennung anwenden Methodik unerheblich Modell exakter Erkennung konkret Zeichen trainieren Normalisierung ausgewählt Zeichen berücksichtigen Normalisierung ausgewählt Zeichen Schritt Konvertierungsskript vornehmen larex bieten zudem Möglichkeit lesereihenfolge Textregion individuell anpassen Sodas sonderfäll fußnoten Weiterverarbeitung entsprechend Stelle platzieren abb zeigen angepas Reading order larex Akt Szene Regieanweisung Figurenangaben folgen Satz Replik fußnot eingegliederen hinterher zugehörig Satz identifizierbar endnot spät digital Edition Drama ergänzen element automatisch auszeichnen lohnen Akribische Vorarbeit Regionsauszeichnung entsprechend Übung Minute pro Seite dauern mittelfristig Auszeichnung Trainingsdaten verwenden Automatisierungsgrad kontinuierlich steigern anschließend ausgezeichnet Region vollautomatisch zeilen zerlegt folgen eigentlich texterkennung Bildzeile maschinenlesbare Text mittels modellen extrahiern hierbei existierend gemischt Modell direkt anwenden werksspezifisch Modell gezielt training Erkennung bestimmt Drucktype optimieren derzeit Erkennung gemischt Modell zuverlässig Zeichengenauigkeit erreichen meist sogar Training Gemischter Modell Genauigkeit deutlich steigern benötigten trainingsdaten ebenfalls larex erstellen bestehen Bildzeile korrekt Transkription sehend texts Ergebnis Digitalisierungsprozesse fast gesamt Textinhalt einzeln Seite Information erstellungsdatum Metadat layoutregionen koordinaten enthalten Angabe Fall wichtig Koordinate Textregion Information ausreichen gewünscht Struktur Dracortei abzubilden Ezdrama existieren Konvertierungsskript dramen Plaintext lateinisch Schrift vorliegen teilautomatisch tags dracortei auszeichnen Skorinkin et dramen Regel vollständig automatisch Tei konvertieren weniger Markups Regel entwerfen Markierung Szene Sprecher Regieanweisung direkt Text hineingeschrieben Colab Mithilfe ausgezeichnet dramen auszeichnen Dracor verwenden Voraussetzung erfüllen Text maschinenlesbar Form lateinisch Druckschrift vorliegen händisch Auszeichnung Dram proprietär Format aufwenden folgend vorgestellt Skript verstehen ausdrücklich Ergänzung verdienstvoll Arbeit Fall vorsehen Voraussetzung geben folgend charakterisiert Skript wandeln gültig Skript spielen zentral Rolle automatisiert Erfassung Verarbeitung Dramendruck spezifisch merkmal Besonderheit Dramendrucke adressieren beinhalten Klasse folgend Zwecke dienen Skript verarbeiten Reihe ordn liegend arbeiten äußerer innerst Ebene Seite Textregion Textline Wort Glyphe initialisierungsfehlern Verarbeitung Fehlermeldung Angabe entsprechend Stelle dateinamen ausgeben Fehler Vorbearbeitung Beschädigungen dateien finden Fall Skript textregionen verarbeiten falsch platzieren Inhalt falsch Textregion entsprechen entsprechend Stelle Inhalt Warning ausgeben notfalls möglich händisch Nachbearbeitung ermöglichen Fall gewährleisten Fehler Vorbearbeitung ausfindig korrigieren Besonderheit Bearbeitung Skript berücksichtigen speziell Zeichen endgültig normalisieren folgend regelmäßig vorkommend Zeichen normalisieren üí s í üí z üí ä ö ü etwaig Projekt methodik Folge speziell Sonderheite drucken auftreten Anpassunge Skript aufnehmen beschrieben Verfahren Auszeichnung Transkription benötigen Digitalisierung Drama derzeit Stunde verzichten Korrektur automatisch erfasst Textmeng erfassen Fehler Gewicht fallen verkürzen Bearbeitungszeit Hälfte Zeitabschnitte müsste Verfahren zudem angepasst Layout signifikant abweichen besonderer Fall drucken Libretti insgesamt bedenken Prozess Volltextdigitalisierung historisch Dramentexte verhältnismäßig komplex ideal Ergebnis deutlich investieren Prosatext Händisch segmentiert korrigiert daten Trainingsdaten nutzen Automatisierungsgrad Zukunft sukzessive steigern,"[('skript', 0.20298337652751203), ('zeichen', 0.16707850933465887), ('textregion', 0.16298169977164875), ('larex', 0.16298169977164875), ('szene', 0.15138762306112352), ('dramentexte', 0.1377242437130973), ('regieanweisung', 0.13269856949304806), ('händisch', 0.12844942248468613), ('folgen', 0.12362341558375953), ('auszeichnen', 0.11861771578904742)]"
2025,DHd2025,PICHLER_Axel_Empirische_Evaluation_des_Verhaltens_von_LLMs_a.xml,Empirische Evaluation des Verhaltens von LLMs auf Basis sprachphilosophischer Theorien: Methode und Pilotannotationen,"Axel Pichler (Universität Stuttgart, Deutschland); Dominik Gerstorfer (Technische Universität, Darmstadt, Deutschland); Jonas Kuhn (Universität Stuttgart, Deutschland); Janis Pagel (Universität zu Köln, Deutschland)","Large Language Model, behavioral analysis, Bedeutung, Verstehen","Modellierung, Theoretisierung, Methoden, Text","Die Fähigkeit großer Sprachmodelle (LLMs), Bedeutung und Verstehen zu simulieren, hat in den letzten Jahren zu einer regen Debatte darüber geführt, inwiefern LLMs tatsächlich bedeutungsvolle Sprache generieren und diese verstehen. Bedeutungs- und Verstehensbegriff sind zugleich zentrale Konzepte der Kultur- und Geisteswissenschaften. Dementsprechend beteiligen sich auch viele traditionelle und digitale Geisteswissenschaftler:innen an der laufenden Debatte. So wurde 2023 auf einem Panel der DHd-Konferenz ein Zugang zu dieser Debatte präsentiert, der das Ziel hatte, durch theoretische Impulse eine präzisere Beschreibungssprache für die Digital Humanities (DH) zu schaffen, die dabei helfen sollte, die Unterschiede zwischen den Bedeutungsprozessen von Maschinen und Menschen herauszuarbeiten, um so die Anwendung von Künstlicher Intelligenz kritisch zu hinterfragen (Gengnagel et al. 2024). Während das Panel derartig eine geisteswissenschaftliche Kernkompetenz 'die Begriffsanalyse 'reaktivierte, um ordnend in die Diskussion einzugreifen, inwiefern LLMs bedeutungsvolle Sprache generieren und diese verstehen, folgte es zugleich der auch in der Natural-Language-Processing-Community weitverbreiteten Tendenz, besagte Fragen primär theoretisch zu verhandeln. Wir wollen dieser theoretischen Debatte keine weitere theoretische Position hinzufügen, sondern im Folgenden einen Vorschlag machen, wie ergänzend überprüft werden kann, inwiefern das Verhalten eines LLMs existierenden Sprachtheorien entspricht. Dabei knüpfen wir an die NLP-Tradition des ""behavioral testings"" an, die sich mit der Prüfung verschiedener Fähigkeiten eines Systems durch Validierung des Eingabe-Ausgabe-Verhaltens ohne Kenntnis der internen Struktur befasst (Beizer 1995). Hierbei verfolgen wir einen theoriegeleiteten Top-Down-Ansatz, der von einer gegebenen Sprachtheorie ausgeht und diese so modelliert, dass ihre zentralen Begriffe in einer Form operationalisiert werden können (Krautter/Pichler/Reiter 2023; Gerstorfer/Gius 2023), welche die Erstellung eines Testdatensatzes erlaubt, der die zentralen sprachtheoretischen Annahmen der Referenztheorie in einem angemessenen Grad repräsentiert. Im Falle von Bedeutungstheorien sollte ein derartiger Testdatensatz das Output eines kompetenten Sprechers auf eine Art und Weise abbilden, wie es von der untersuchten Sprachtheorie impliziert wird. Mit Hilfe eines solchen Testdatensatzes könnte dann der Grad bestimmt werden, in dem das Sprachverhalten eines LLMs dem einer Sprachtheorie entspricht. Ein derartiges Wissen über das Verhalten von LLMs ist insbesondere für jene Zweige der DH relevant, deren Theorien und Analysen auf bestimmten sprachphilosophischen Vorannahmen aufbauen. Sie könnten dann für ihre Analysen jene LLMs verwenden, die diesen entsprechen. Wir werden daher im Folgenden eine Methode präsentieren, die eine derartige Evaluation erlaubt, sie anhand eines Beispiels 'der wahrheitskonditionalen Semantik von Donald Davidson 'vorführen, und die Resultate der ersten Pilotannotationen sowie erster Experimente mit LLMs präsentieren. An diesem Punkt sei darauf hingewiesen, dass die Auswahl von Davidsons Sprachtheorie nicht daher rührt, dass wir glauben, dass sie in höherem Grad als alternative Sprachtheorien dem Textgenerierungsverhalten von LLMs entspricht, sondern daher, dass Ausgangsszene und Kernkonzept seiner Sprachphilosophie sehr treffend die Situation beschreiben, mit der Benutzer von großen Sprachmodellen konfrontiert sind: die radikale Interpretation. Bei dieser steht der radikale Interpret einer Sprecher:in einer ihm unbekannten Sprache gegenüber und versucht auf Basis einer spezifischen Form des Abgesehen von diesen Parallelen ist die im Folgenden vorgestellte Methode zur Entwicklung eines Testdatensatzes zur Überprüfung, inwiefern das Verhalten eines LLMs den Erwartungen einer Sprachtheorie bezüglich des Verhaltens eines kompetenten Sprechers entspricht, sprachtheorie-agnostisch. Mit ihrer Hilfe können auch alternative Sprachtheorien getestet werden, was in Anbetracht der Vielfalt des Theorieangebots in Sprachphilosophie und Semantik sowie von deren zentraler Rolle zum Beispiel in den Interpretationstheorien der Literaturwissenschaft ein Forschungsdesiderat darstellt. Längerfristig streben wir an, weitere sprachphilosophische Theorien derartig zu überprüfen. Die Generierung eines Testdatensatzes zur Überprüfung, inwiefern das Verhalten eines LLMs den Erwartungen einer Sprachtheorie entspricht, erfolgt in drei Schritten: In einem ersten Schritt sind die zentralen Annahmen der zu testenden Sprachtheorie rational zu rekonstruieren. Ziel dieser Rekonstruktion ist, zweitens, die Rückführung besagter Sprachtheorie auf eine oder mehrere sprachtheoretische Hypothesen, die im Folgenden getestet werden. Dafür sind, drittens, die zentralen Begriffe dieser Hypothesen so zu operationalisieren, dass mit ihrer Hilfe ein Testdatensatz erzeugt werden kann. Im Zentrum von Davidsons Bedeutungstheorie steht die These, ""that a theory of truth, modified to apply to a natural language, can be used as a theory of interpretation"" (Davidson 2006, S. 189). Davidson kommt einem Ansatz wie dem hier präsentierten, der auf eine operationalisierbare Rekonstruktion einer Theorie abzielt, nun insofern entgegen, als dass Davidson selbst mit dem Konzept der radikalen Interpretation bereits eine Operationalisierung der Kernelemente seiner Sprachtheorie vorgelegt hat, die nur in Hinblick auf jene Voraussetzungen zu adaptieren ist, die LLMs im Unterschied zu kompetenten menschlichen Sprechern nicht erfüllen. Davidson schreibt: ""A theory of meaning (in my mildly perverse sense) In Hinblick auf die zu konstituierende Leithypothese heißt das, dass einem Sprachmodell eine Vielzahl von Gelegenheitssätzen in Bezug auf eine bestimmte Situation vorzulegen ist, um dann zu überprüfen, inwiefern das Model diese Sätze für wahr hält. Die absurd anmutende Formulierung des soeben artikulierten verweist bereits auf jene Elemente der Davidson""schen Theorie, die zu adaptieren sind, wenn man sie auf LLMs anwenden möchte. Dazu zählen insbesondere, dass 1.) große Sprachmodelle keine Agenten sind, 2.) keine Überzeugungen besitzen und dementsprechend 3.) auch keine Propositionen für-wahr-halten können. LLMs können jedoch das entsprechende Verhalten eines kompetenten Sprechers simulieren. Zudem entspricht die ""Kommunikationssituation"" zwischen einem LLM und einem Menschen nicht derjenigen der radikalen Interpretation: Weder besitzt eine solche einen realweltlichen Situations- und Bezugsrahmen, auf Basis dessen in Übereinstimmung mit dem Gesamtverhalten eines Sprechers einer fremden Sprache bestimmt werden kann, ob dieser einen Satz zu einem bestimmten Zeitpunkt an einem bestimmten Ort für wahr hält oder nicht, noch handelt es sich dabei um eine kausale Relation zwischen Bezugsrahmen und Verhalten besagten Sprechers (vgl. Davidson 2001). Das im Folgenden entwickelte Testset überprüft dementsprechend nur, inwiefern sich diese Simulation zum Kommunikationsverhalten eines kompetenten Sprechers im Sinne Davidsons verhält bzw. zu welchem Grade es diesem entspricht. Ausgangspunkt bei der Entwicklung des Testsets ist dabei folgende Leit-Hypothese: (H) Ein Sprachmodell verwendet eine Sprache wie ein kompetenter Sprecher im Sinne Davidsons, gdw. es die selben zum Zeitpunkt Z geäußerten Sätze im Verhältnis zum sprachlichen Kontext K als wahr bestimmt. Die Testdatenerzeugung erfolgt dementsprechend in Bezug auf einen sprachlichen Kontext K, der von jedem beliebigen Text gefüllt werden kann, der eine Situation beschreibt, die im Hinblick auf situative Aussagen auf ihren Wahrheitswert hin überprüft werden kann. Dieser Fokus auf wahrheitskonditionale situative Aussagesätze ist der Orientierung an den theoretischen Grundannahmen der Davidson""schen Theorie geschuldet. Im Falle alternativer sprachphilosophischer Theorien können andere Frage-Antwort-Typen relevant sein. Für unsere Pilotannotation haben wir auf die Texteröffnung von Franz Kafkas Erzählung Wir haben uns für unsere Experimente auf die ersten beiden der drei genannten Schritte konzentriert und dementsprechend in einem ersten Schritt aus den beiden Textstellen das Basisvokabular extrahiert, indem wir mithilfe von spaCy Wir führen eine Pilotstudie zur Annotation der Testdaten durch, um die Durchführbarkeit des Vorhabens zu demonstrieren. Die Testdaten zu Kafkas Urteil wurden von drei Annotatoren (drei der Autoren; die 25 manuellen Sätze wurden vom ersten Autor erstellt, der selber nicht annotiert hat) annotiert, und zwar bezüglich des Wahrheitswertes des Satzes im Hinblick auf den gegebenen Kontext als auch bezüglich der Angabe, ob der Satz extrinsisch oder intrinsisch wahr oder falsch ist. Die Auswertungen der Annotationen in Form einer Inter-Annotator-Agreement-Studie befinden sich in Tabelle 1 (Agreement bezüglich der Wahrheitswerte) und Tabelle 2 (Agreement bezüglich intrinsisch/extrinsisch). Für die Auswertungen wurden Sätze, die von den Annotatoren als nicht-entscheidbar eingeschätzt wurden, auf die Werte falsch, bzw. intrinsisch gesetzt. Die Tabellen zeigen das IAA für alle Sätze und für Teilmengen (subsection) der Sätze: (i) ist der Satz ein manuell oder automatisch erstellter Satz, (ii) enthält der Satz einen Junktor oder nicht und (iii) falls der Satz einen Junktor enthält, welchen? Gezeigt wird das resultierende Fleiss"" Kappa (Fleiss 1971) als Maß dafür, wie stark die Annotationen übereinstimmen Für die Wahrheitswerte gibt es mit einem Fleiss"" Kappa von 0.56 ein moderat gutes Agreement. Am höchsten ist das Agreement für Sätze ohne logische Junktoren und für die Kon- und Disjunktion (Fleiss"" Kappa von ca. 0.75). Am wenigsten Übereinstimmungen gibt es für die Implikations-Sätze, wobei das Ergebnis nicht statistisch signifikant ist (p>0.05). Für die automatisch erstellten Sätze lässt sich ein etwas höheres Agreement ablesen als für die manuell erstellten. Im Bezug auf extrinsische/intrinsische Wahrheitswertzuschreibungen ist Fleiss"" Kappa durchweg negativ, was darauf hindeutet, dass die Annotatoren die Annotationsguidelines unterschiedlich interpretiert haben. Erste Experimente mit zwei LLMs 'OpenAI""s GPT-4o und Anthropics Claude 3.5 Sonnet Model In summa haben wir gezeigt, dass es möglich ist eine Sprachtheorie so zu modellieren und anschließend in Hinblick auf einen bestimmten Testkontext zu operationalisieren, dass die Resultate als Testdaten für diese Sprachtheorie hinreichen. Im Zuge unserer ersten Experimente haben wir festgestellt, dass eine solche Modellierung und Operationalisierung jedoch zahlreiche Fallstricke besitzt: So führt zum Beispiel eine vollständig automatisierte Testdatengenerierung auf Basis eines gegebenen Vokabulars mehrheitlich zu sinnlosen Sätzen, ebenso schränkt einen die Limitierung auf das in einem bestimmten Kontext gegebene Vokabular unnötig ein. Zudem hat im konkreten Fall Davidsons formallogische Orientierung den Annotierenden Probleme gemacht. Des Weiteren haben wir darauf verzichtet, die Input-Sequenzen so zu manipulieren, dass das LLM explizit dazu aufgefordert wird, einer bestimmten Sprachtheorie entsprechend zu handeln. Bei den hier durchgeführten Experimenten ging es uns nur darum, wie die LLMs ohne zusätzliche Informationen oder Prompt Engineering Strategien auf die ""Gelegenheitssätze"" ""reagieren"".",de,Fähigkeit Sprachmodelle llms Bedeutung verstehen simulieren letzter regen Debatte führen inwiefern Llm tatsächlich bedeutungsvoll Sprache generieren verstehen Verstehensbegriff zentral Konzept Geisteswissenschaft beteiligen traditionell digital Geisteswissenschaftl innen laufend Debatte Panel Zugang Debatte präsentieren Ziel theoretisch Impuls präziser beschreibungssprachen Digital Humanitie dh schaffen helfen Unterschied Bedeutungsprozesse Maschine Mensch herauszuarbeiten Anwendung künstlich Intelligenz kritisch hinterfragen Gengnagel et Panel derartig geisteswissenschaftlich Kernkompetenz Begriffsanalyse reaktivieren ordnend Diskussion eingreifen inwiefern Llm bedeutungsvoll Sprache generieren verstehen folgen weitverbreitet Tendenz besagt Frage primär theoretisch verhandeln theoretisch Debatte theoretisch Position hinzufügen folgend Vorschlag ergänzend überprüfen inwiefern verhalten llms existierend Sprachtheorien entsprechen knüpfen behavioral Testings Prüfung verschieden Fähigkeit System Validierung Kenntnis intern Struktur befassen Beizer hierbei verfolgen theoriegeleitet gegeben Sprachtheorie ausgehen modellieren zentral begriffe Form operationalisieren Krautter Pichler Reiter Gerstorfer Gius Erstellung testdatensatzes erlauben zentral sprachtheoretisch annahmen Referenztheorie angemessen Grad repräsentieren Fall Bedeutungstheorien derartig Testdatensatz Output kompetent Sprecher Art Weise abbilden untersucht Sprachtheorie implizieren Hilfe testdatensatzes Grad bestimmen sprachverhaln llms Sprachtheorie entsprechen derartig wissen verhalten llms insbesondere zweig dh relevant Theorie Analyse bestimmt sprachphilosophisch vorannahmen aufbauen können Analyse llms verwenden entsprechen folgend Methode präsentieren derartig Evaluation erlauben anhand Beispiel wahrheitskonditional Semantik Donald Davidson vorführen Resultat Pilotannotation experimenter llms präsentieren Punkt hinweisen Auswahl Davidsons Sprachtheorie rühren glauben höherem Grad alternativ Sprachtheorien Textgenerierungsverhalten llms entsprechen ausgangsszen Kernkonzept Sprachphilosophie treffend Situation beschreiben Benutzer Sprachmodelle konfrontieren radikal Interpretation stehen radikal Interpret Sprecher unbekannt Sprache versuchen Basis spezifisch Form absehen Parallele folgend vorgestellt Methode Entwicklung Testdatensatze Überprüfung inwiefern verhalten llms Erwartung Sprachtheorie bezüglich verhaltens kompetent Sprecher entsprechen Hilfe alternativ Sprachtheorien testen Anbetracht Vielfalt Theorieangebot Sprachphilosophie Semantik zentral Rolle interpretationstheorien Literaturwissenschaft Forschungsdesiderat darstellen längerfristig streben sprachphilosophisch Theorie derartig überprüfen Generierung Testdatensatze Überprüfung inwiefern verhalten llms Erwartung Sprachtheorie entsprechen erfolgen Schritt Schritt zentral annahmen testend Sprachtheorie Rational rekonstruieren Ziel Rekonstruktion zweitens Rückführung besagt Sprachtheorie mehrere sprachtheoretisch Hypothese folgend testen drittens zentral begriffe Hypothese operationalisieren Hilfe Testdatensatz erzeugen Zentrum davidson Bedeutungstheorie stehen These that theory -- Truth Modified to apply to natural language can be used as Theory -- interpretation davidson Davidson Ansatz Präsentiert operationalisierbar Rekonstruktion Theorie abzielen insofern entgegen Davidson Konzept radikal Interpretation Operationalisierung Kernelemente Sprachtheorie vorlegen Hinblick Voraussetzung adaptieren llms Unterschied kompetent menschlich Sprecher erfüllen Davidson schreiben theory -- meaning my Mildly Pervers sense Hinblick konstituierend Leithypothese Sprachmodell Vielzahl Gelegenheitssätze Bezug bestimmt Situation vorlegen überprüfen inwiefern Model sätze halten absurd anmutend Formulierung soeben artikulierter verweisen elemente Davidson sch Theorie adaptieren llms anwenden zählen insbesondere sprachmodelle Agent überzeugung besitzen proposition Llms entsprechend verhalten kompetent Sprecher simulieren zudem entsprechen Kommunikationssituation llm Mensch radikal Interpretation weder besitzen realweltlich Bezugsrahm Basis Übereinstimmung Gesamtverhalten Sprecher fremd Sprache bestimmen Satz bestimmt Zeitpunkt bestimmt Ort halten handeln kausal Relation bezugsrahmen verhalten besagt Sprecher davidson folgend entwickelte Testset überprüfen inwiefern Simulation kommunikationsverhalten kompetent Sprecher Sinn davidsons verhalten Grade entsprechen Ausgangspunkt Entwicklung Testset folgend h Sprachmodell verwenden Sprache kompetent Sprecher Sinn davidson gdw selber Zeitpunkt z geäußert Sätz Verhältnis sprachlich Kontext k bestimmen Testdatenerzeugung erfolgen Bezug sprachlich Kontext k beliebig Text füllen Situation beschreiben Hinblick situativ Aussage Wahrheitswert überprüfen Fokus wahrheitskonditional Situative Aussagesatz Orientierung theoretisch grundannahmen Davidson sch Theorie schulden Fall alternativ Sprachphilosophischer Theorie relevant Pilotannotation Texteröffnung Franz Kafka Erzählung experiment genannt Schritt konzentrieren Schritt Textstelle Basisvokabular extrahieren Mithilfe Spacy führen Pilotstudie Annotation Testdat Durchführbarkeit vorhaben demonstrieren Testdat Kafka Urteil Annotator Autor manuell Sätz Autor erstellen selber annotiert annotiert bezüglich Wahrheitswert satzes Hinblick gegeben Kontext bezüglich Angabe Satz extrinsisch intrinsisch falsch Auswertung annotatio Form befinden Tabelle agreement bezüglich wahrheitsweren Tabelle agreemenen bezüglich intrinsisch extrinsisch Auswertung Sätz Annotator einschätzen Wert falsch intrinsisch setzen Tabell zeigen iaa Sätz teilmeng Subsection Sätz i Satz manuell automatisch erstellt Satz ii enthalten Satz Junktor iii falls Satz junktor enthalten zeigen resultierend Fleiss Kappa fleiss Maß stark annotation Übereinstimmen Wahrheitswert fleiss Kappa Moderat agreement hoch Agreement Sätz logisch junktor Disjunktion fleiss Kappa wenigster übereinstimmungen wobei Ergebnis statistisch signifikant automatisch erstellt Sätz lässen hoch agreement ablesen manuell erstellen Bezug Extrinsische intrinsisch Wahrheitswertzuschreibung fleiss Kappa durchweg negativ hindeuten annotatoren annotationsguidelin unterschiedlich interpretieren Experimente llms Openai -- Anthropics Claude sonnen Model Summa zeigen Sprachtheorie modellieren anschließend Hinblick bestimmt Testkontext operationalisieren Resultat Testdat Sprachtheorie hinreichen Zug experimente feststellen Modellierung Operationalisierung zahlreich Fallstricke besitzen führen vollständig automatisiert Testdatengenerierung Basis gegeben Vokabular mehrheitlich sinnlos sätzen schränken Limitierung bestimmt Kontext Gegebene Vokabular unnötig zudem konkret Fall Davidson formallogisch Orientierung annotierend Problem verzichten manipulieren llm explizit auffordern bestimmt Sprachtheorie entsprechend handeln durchgeführt experimenten llms zusätzlich Information prompt Engineering strategien gelegenheitssätze reagieren,"[('sprachtheorie', 0.3629735796634254), ('llms', 0.35784368502634645), ('davidson', 0.30990884082704095), ('sprecher', 0.18973407754065602), ('kompetent', 0.1814867898317127), ('verhalten', 0.13958008742028663), ('sätz', 0.13509794411318582), ('fleiss', 0.13350907449369614), ('entsprechen', 0.12147774264322668), ('inwiefern', 0.1212872130435247)]"
2025,DHd2025,STIEMER_Haimo_Pause_im_Text__Zur_Exploration_semantisch_kond.xml,Pause im Text. Zur Exploration semantisch konditionierter Sprechpausen in Hörbüchern,"Haimo Stiemer (TU Darmstadt, Deutschland); Hans Ole Hatzel (Universität Hamburg, Deutschland); Chris Biemann (Universität Hamburg, Deutschland); Evelyn Gius (TU Darmstadt, Deutschland)","Segmentierung, Narratologie, Hörbücher, Sprechpausen","Transkription, Strukturanalyse, Daten, Ton, Text","Die für die Textanalyse grundlegende Segmentierung von Prosatexten, also deren Zerlegung in diskrete Einheiten, ist in den Computational Literary Studies (CLS) weiterhin ein prominentes Problem. Obgleich viele computationelle Verfahren die vorhergehende Unterteilung von Texten voraussetzen, fehlt es bislang an standardisierten Segmenten (cf. Bartsch et al. 2023). In Abhängigkeit von der jeweiligen Forschungsfrage und der zum Einsatz bestimmten computationellen Methoden finden sich sowohl Segmentierungen von Layoutelementen bzw. Einheiten der materiellen Textgestaltung (cf. Herzog 2018), die Tokenisierung (auf Wort- oder Satzebene) oder aber das Ein in den CLS bislang nicht geprüfter Ansatz der Segmentierung, die Zergliederung von Erzähltexten mittels der in der Rezitation emergenten Sprechpausen, ist Gegenstand dieses Beitrags. Wir präsentieren erste Beobachtungen hinsichtlich der Möglichkeiten wie Bedingungen für die Identifikation und Analyse semantisch konditionierter Sprechpausen in Hörbüchern. Ein solcher Zugang würde die genannten, allein vom Text ausgehenden Zugänge zur Segmentierung um eine in der Textrezeption erzeugte Segmentierung komplementieren, auch um die bestehenden Segmentierungsoptionen überprüfen zu können. Ausgangspunkt unserer Analyse ist das in der Sprechwissenschaft und Sprecherziehung entwickelte Konzept des interpretierenden Textsprechens (cf. Geißner 1981:175; Brand 2021), mit welchem jedwede Rezitation von Texten vor Publikum als originärer Interpretationsvorgang und mithin die Sprechfassung eines Textes als dessen Interpretation verstanden werden. In der Konsequenz verstehen wir die Pausensetzung und -länge, welche der prosodischen Dimension eines literarischen Werks angehören, als die von Vortragenden ad hoc oder planmäßig vorgenommene sinnhafte Segmentierung eines Textes. Unbestimmt bleibt dabei zunächst, inwiefern es sich bei den potentiell multifunktionalen Sprechpausen um syntaktische und typographisch bedingte, auf z.B. Interpunktion oder Absätze rekurrierende, oder aber um semantisch konditionierte Unterbrechungen handelt, denen eine noch näher zu bestimmende handlungsrelevante Funktion zugewiesen werden kann. In diesem Beitrag untersuchen wir demzufolge die semantisch-interpretative Qualität von Sprechpausen in Tonaufzeichnungen von eingelesenen Erzähltexten. Der Beschreibung unseres Audio-Korpus (2.) sowie der Transkriptions-Methode (3.) folgt die Exploration der aus diesem Korpus gewonnenen, transkribierten Daten, um die mögliche Motivierung der Sprechpausen zu erfassen. Analysiert wird hierfür die Verteilung der Anzahl wie Länge der in den Daten detektierten Pausen (4.). Anschließend diskutieren wir den Zusammenhang von Sprechkompetenz und Pausensetzung (5.) und ziehen aus der Datenexploration Rückschlüsse für weitergehende Forschungsaktivitäten (6.). Für unsere Untersuchung haben wir die aufgezeichneten Lesungen von drei, aus dem EvENT-Projektkorpus (Vauth et al. 2021) entnommenen Erzähltexten analysiert. Bei der Auswahl der Texte wurde darauf geachtet, dass sich diese durch verschiedene narrative Profile bzw. Charakteristika auszeichnen. Den analysierten Vortragsaufzeichnungen zugrunde lagen somit Franz Kafkas hypotaktisch geprägte Erzählung Für die automatische Pausenerkennung verwendeten wir Whisper (Radford et al. 2023), ein neuronales Modell zur Transkription von gesprochener Sprache, welches annähernd die Fehlerraten von professionellen Transkriptor:innen erreicht (Radford et al. 2023, Abbildung 7). Prinzipiell wäre es wünschenswert, einen Alignierungsansatz zu nutzen, die Audiodaten also mit einem 'in unserem Fall verfügbaren 'Originaltext abzugleichen. Schiel et al. (2017) beschreiben ein derartiges System. In der Praxis war jedoch keine Implementation einfach auf unseren Daten anwendbar, sodass die automatische Transkription für diese explorative Arbeit passend war. Konkret setzen wir die Implementation WhisperX (Bain et al. 2023) ein, welche zahlreiche zusätzliche Funktionen im Vergleich zur ursprünglichen Whisper-Implementation bietet. Die Anwendung von WhisperX auf eine beliebige Audiodatei erzeugt ein Transkript eben dieser, in dem einzelne Segmente mit Zeitcodes versehen sind. Die Segmente entsprechen dabei linguistischen Sätzen (da wir WhisperX mit den Standardoptionen aufrufen und somit Die Verteilung der insgesamt 4.542 im Audio-Korpus mit WhisperX detektierten Pausen (an Satzenden) nach den Sprechfassungen der Texte auf die Pausenlängenwerte (x-Achse) sowie die Pausenanzahl (y-Achse) findet sich in Abbildung 1. Mit der Auswertung der Daten und in Anlehnung an die vom Grammatischen Informationssystem grammis (cf. Institut für Deutsche Sprache 2013) vorgeschlagene Differenzierung von Sprechpausen ergeben sich zunächst folgende Beobachtungen: In allen Sprechfassungen dominieren erwartungsgemäß die sehr kurze Pausen, die lediglich auf Atemeinschnitte bei der Satzbeendigung verweisen (x<0,3 Sek.), während Verzögerungspausen (0,3<x<1) deutlich seltener auftreten. Wird ab dem Längenwert 1 Sekunde den Pausen eine dann noch zu bestimmende semantische Relevanz zugemessen (Relevanzpause; cf. ebd.), weisen beide Sprechfassungen der Die dialogreiche Gestaltung der Die höchsten Anteile an Relevanzpausen weisen die beiden Sprechfassungen des Kleist-Textes auf (professionelle Lesung 44 % und Laienlesung 41 % aller Pausen der jeweiligen Sprechfassung). Es handelt sich im Korpusvergleich um den Text mit den meisten Figuren, mit einem sehr handlungsintensiven, also auch diverse Ortswechsel enthaltenden, Plot. Nachdem das Verhältnis der Segmentlänge zur durchschnittlichen Satzlänge der Texte unterschiedlich ist, nehmen wir an, dass eher bestimmte narrative Eigenschaften für Pausen relevant sind. Zumindest scheint die höhere Heterogenität, die durch vermehrte narrative Elemente wie Dialoge, Figuren oder auch weitere, handlungsbezogene Elemente entsteht, auch die Anzahl an Relevanzpausen zu steigern. Abgesehen von der Laienfassung des Zu bemerken ist, dass sich in Absätzen auch weitere, möglicherweise durch typographische Trigger erzeugte Pausen befinden. Neben den Anführungszeichen der direkten Rede zählt dazu z.B. ungewöhnliche Interpunktion am Satzende. Wir gehen zugleich davon aus, dass die Textkenntnis bei den professionellen Sprecher:innen in der Regel höher ist, nicht zuletzt aufgrund der redaktionellen Betreuung, ihrer Ausbildung und der im Vorfeld der Aufnahme mutmaßlich erfolgten, eingehenden Beschäftigung mit dem Text. 6. Fazit und Ausblick Unsere tentative Annäherung an die Segmentierungsfunktion von Sprechpausen hat ergeben, dass es lohnend erscheint, diesen Ansatz weiter zu verfolgen und durch ein größeres Audio-Korpus zu validieren. Die Exploration unserer Daten bislang deutet an, dass mittels der sprechpausenbezogenen Segmentierung Textprofile erstellt werden können und eine größere Sprecher:innenkompetenz vermutlich zu mehr nicht primär typographisch getriggerten Sprechpausen führt. Die Erstellung eines größeren Audio-Korpus für weitere Untersuchungen erscheint dabei nicht zuletzt aufgrund der spezifischen Rhetorizität von Vortragenden geboten, ihrer idiosynkratischen Realisierung der Sprechfassung eines Textes. Um die damit verbundenen Parameter der Pausensetzung und -längen weitestmöglich zu neutralisieren, wäre die Untersuchung von deutlich mehr Sprechfassungen nur eines Textes notwendig. In einem nächsten Schritt werden wir daher acht zusätzliche Audioaufnahmen von Laien-Lesungen des",de,Textanalyse grundlegend Segmentierung Prosatext Zerlegung Diskret einheien Computational literary Studies cls weiterhin prominent Problem obgleich computationelle Verfahren vorhergehend Unterteilung Text voraussetzen fehlen bislang Standardisiert segmenten cf Bartsch et Abhängigkeit jeweilig Forschungsfrage Einsatz bestimmt computationell Methode finden sowohl segmentierungen layoutelementen Einheit materiell Textgestaltung cf Herzog Tokenisierung satzeben Cls bislang geprüft Ansatz Segmentierung Zergliederung Erzähltext mittels Rezitation emergenten sprechpausen Gegenstand beitrag präsentieren Beobachtung hinsichtlich Möglichkeit Bedingung Identifikation Analyse semantisch Konditionierter sprechpausen hörbüchern Zugang Genannt Text ausgehend Zugäng Segmentierung Textrezeption erzeugt Segmentierung komplementieren bestehend Segmentierungsoption überprüfen Ausgangspunkt Analyse Sprechwissenschaft Sprecherziehung entwickelt Konzept interpretierend Textsprechen cf Geißner Brand jedweder Rezitation Text Publikum originär Interpretationsvorgang mithin Sprechfassung Text Interpretation verstehen Konsequenz verstehen Pausensetzung prosodisch Dimension literarisch Werk angehören vortragenden ad hoc planmäßig vorgenommen sinnhaft Segmentierung Text unbestimmt bleiben inwiefern potentiell multifunktional Sprechpause syntaktisch typographisch bedingt Interpunktion Absätze Rekurrierende semantisch konditioniert Unterbrechung handeln nah bestimmend handlungsrelevant Funktion zuweisen Beitrag untersuchen Qualität Sprechpaus Tonaufzeichnung eingelesenen Erzähltext Beschreibung unser folgen Exploration Korpus gewonnenen transkribierten daten möglich Motivierung Sprechpause erfassen analysieren hierfür Verteilung Anzahl Länge daten detektieren Pause anschließend diskutieren Zusammenhang sprechkompetenz Pausensetzung ziehen Datenexploration Rückschlüsse weitergehend Forschungsaktivitäte Untersuchung Aufgezeichnet lesungen vauth et entnommen Erzähltext analysieren Auswahl Text achten verschieden narrativ profil Charakteristika auszeichnen analysierter vortragsaufzeichnungen zugrunde liegen somit Franz Kafka hypotaktisch geprägt Erzählung automatisch Pausenerkennung verwenden Whisper radford et neuronal Modell Transkription gesprochen Sprache annähernd Fehlerrat professionell Transkriptor innen erreichen radford et Abbildung prinzipiell wünschenswert Alignierungsansatz nutzen audiodaen unser Fall verfügbaren Originaltext abgleichen Schiel et beschreiben derartig System Praxis Implementation einfach unser daten anwendbar sodass automatisch Transkription explorativ Arbeit passend konkret setzen Implementation Whisperx bain et zahlreich zusätzlich Funktion Vergleich ursprünglich bieten Anwendung Whisperx beliebig Audiodatei erzeugen Transkript einzeln Segmente zeitcoder versehen Segment entsprechen linguistisch Satz whisperxn Standardoption aufrufen somit Verteilung insgesamt Whisperx detektierten Pause satzenden Sprechfassung Text pausenlängenwert Pausenanzahl finden Abbildung Auswertung daten Anlehnung grammatisch informationssyst Grammis cf Institut deutsch Sprache vorgeschlagen Differenzierung Sprechpaus ergeben folgend Beobachtung sprechfassung dominieren erwartungsgemäß kurz Pause lediglich Atemeinschnitte Satzbeendigung verweisen Sek verzögerungspausen deutlich selten auftreten Längenwert Sekunde Pause bestimmend semantisch Relevanz zugemessen relevanzpause cf weisen sprechfassungen dialogreich Gestaltung hoch Anteil Relevanzpausen weisen Sprechfassunge professionell Lesung Laienlesung Pause jeweilig Sprechfassung handeln korpusvergleich Text meister Figur handlungsintensiv diverser Ortswechsel enthaltenden plot Verhältnis Segmentlänge durchschnittlich Satzlänge Text unterschiedlich nehmen eher bestimmt narrative Eigenschaft Pause relevant zumindest scheinen hoch Heterogenität vermehrt Narrative Element Dialoge Figur handlungsbezogen Elemente entstehen Anzahl Relevanzpausen steigern absehen Laienfassung bemerken Absätze möglicherweise Typographisch Trigger erzeugt Pause befinden anführungszeich direkt Rede zählen ungewöhnlich Interpunktion Satzende Textkenntnis professionell Sprecher innen Regel hoch zuletzt aufgrund redaktionell Betreuung Ausbildung Vorfeld Aufnahme mutmaßlich erfolgt eingehend Beschäftigung Text Fazit Ausblick tentativ Annäherung Segmentierungsfunktion Sprechpaus ergeben lohnend erscheinen Ansatz verfolgen größ validieren Exploration daten bislang deuten mittels sprechpausenbezogen Segmentierung Textprofil erstellen groß Sprecher Innenkompetenz vermutlich primär typographisch getriggert Sprechpaus führen Erstellung groß Untersuchung erscheinen zuletzt aufgrund spezifisch Rhetorizität vortragenden gebieten idiosynkratisch Realisierung Sprechfassung Text verbunden parameter Pausensetzung weitestmöglich neutralisieren Untersuchung deutlich sprechfassungen Text notwendig nächster Schritt zusätzlich Audioaufnahme,"[('pause', 0.2746536431881663), ('sprechfassung', 0.24892242047710048), ('segmentierung', 0.20818734354033835), ('sprechpaus', 0.19913793638168037), ('cf', 0.1771526267677756), ('whisperx', 0.14935345228626026), ('pausensetzung', 0.14935345228626026), ('professionell', 0.1391114180651073), ('text', 0.11931312314894771), ('typographisch', 0.11433570599132217)]"
2025,DHd2025,SCHR_TER_Julian_Zur_Modellierung_von_Unsicherheit__Machine_L.xml,Zur Modellierung von Unsicherheit: Machine Learning und begriffliche Vagheit am Beispiel der Novellen im 19. Jahrhundert,"Julian Schröter (LMU München, Deutschland)","Machine Learning, Unbestimmtheit, Vagheit","Programmierung, Strukturanalyse, Modellierung, Visualisierung, Literatur, Text","Die Modellierung von Vagheit und Unsicherheit in der sprachlichen Kategorisierung ist ein offenes Problem im Bereich des Die Sprachphilosophie und die literarische Gattungstheorie stellen mit der Prototypentheorie (Rosch, 1978; Taylor, 2007; Hempfer, 2010) und dem Familienähnlichkeitsbegriff nach Wittgensteins Die folgende Modellierung ist durch drei wesentliche Schritte gekennzeichnet: (1a) der Aufbau eines geeigneten Verfahrens maschinellen Lernens, das (1b) eine nutzbare Kennzahl für eine Bemessung der relativen Nähe und Distanz zum begrifflichen Zentrum bereitstellt, (2) die Erfassung eines Bereichs relativer Vagheit, und (3) ein Maß zur Bemessung des relativen Vagheitscharakters eines Gattungsbegriffs wie der Novelle. Zu (1): Gewählt wird (1a) Logistische Regression als klassisches Modell überwachten maschinellen Lernens. Auch wenn logistische Regression ein bereits in die Jahre gekommenes Lernverfahren darstellt, hat es zwei entscheidende Vorteile. Erstens die bereits von Underwood (2019a und 2019b) betonte gute Interpretierbarkeit der Features, und zweites die relative Vorhersagewahrscheinlichkeit für die Zuordnung einer Instanz zu einer der in Frage kommenden Klassen. Die grundlegende Idee zu Schritt (1b) ist es, relative Nähe einer Instanz zum begrifflichen Zentrum im Sinn von Prototypikalität durch die Vorhersagewahrscheinlichkeit für eine bestimmte Klasse auszudrücken. Der entscheidende und neue Schritt ist nun (2) die Einführung eines Bereichs relativer Vagheit. Das vorgestellte Modell verwendet den sogenannten  Im nächsten Schritt bedarf es einer Entscheidungsfunktion für Enthaltung. Hierfür wird mit logistischer Regression ein Algorithmus verwendet, der in der sigmoiden Funktion (Logistische Funktion, Gl. 2) mit der Link-Funktion (Gl. 3) über die Berechnung von Vorhersagewahrscheinlichkeiten läuft. Abbildung 1 zeigt, wie der Bereich einer Enthaltung symmetrisch um eine Vorhersagegenauigkeit von 0,5 gewählt werden kann.  Anstelle einer Entscheidungslinie bei einem Wert von y = 0,5 für die Vorhersagewahrscheinlichkeit wird eine Art Grauzone eingeführt, so dass bei Funktionswerten innerhalb dieses Bereichs anstelle einer binären Vorhersage für ei¬≠ne der beiden möglichen Klassen eine Enthaltung erfolgt. Die Enthaltung gibt lediglich eine episte¬≠mische Enthaltung des Modells wieder, ohne Impli¬≠kation für den Status der Objekte. Was der Die optimale Breite des Enthaltungsbereichs kann in Form einer Gridsuche ermittelt werden. Abbildung 2 zeigt den Graphen, der diese Gridsuche am Beispiel eines Modells zur Klassifikation von Romanen vs. Märchen als Funktion abbildet, bei der für das trainierte Modell und die verfügbaren Validierungsdaten jeweils zu einem sukzes¬≠sive ausgeweiteten Enthaltungsbereich für Vorhersagewahrscheinlichkeiten zwischen 0 und 1 der zugehörige  Zu (3): Zur Berechnung des Grads an begrifflicher Vagheit, die auf diese Weise algorithmisch modelliert wird, stehen mehrere Maße als potenzielle Kandidaten zur Verfügung: (a) die Breite des optimalen Enthaltungsbereichs und (b) die In einem Prozess interner Evalutation wurden Daten und simuliert, um das Verhalten der Berechnung optimaler Enthaltungsbreiten besser zu verstehen. Die Ergebnisse werden in den Visualisierungen aus Abbildung 3 dargestellt. Hierfür wurde Vagheit im Sinn einer Kombination von Familienähnlichkeit und potenzieller Prototypikalität dadurch simuliert, dass in einigen Fällen jede Klasse nur ein Feature-Cluster mit mehreren informativen Features und mehr oder weniger großer Varianz sowie Störrauschen (im Sinn einer Annäherung an Prototypikalität) erzeugt wurde (linke Spalte) und in einigen Fällen jede Klasse mit mehreren Feature-Clustern (2 Cluster je Klasse in der mittleren Spalte sowie 5 Cluster je Klasse in der rechten Spalte). Jedes Simulation wurde 1000-fach iteriert. Die Boxplots in der Mitte zeigen die Verteilung der optimalen Breite des optimalen Enthaltungsbereichs über alle Iterationen. Die untere Zeile zeigt die Verteilung der Verbesserungsrate. Es zeichnet sich ab, dass die Verbesserungsrate die robustere und aussagekräftigere Kennzahl ist, um komplexe Merkmalsbündel mit komplizierten Mehrfachclustern innerhalb der Klasse abzubilden. Wenn man annimmt, dass Kategorien, die durch mehrere interne Cluster gegliedert sind und dadurch vage werden, dem Familienähnlichkeitsbegriff entsprechen, dann lässt sich auf einer strukturellen Ebene sehen, dass die Verbesserungsrate anhand des C@1-scores bei einem optimalen Enthaltungsbereich eine Modellierung von Prototypikalität darstellt, die Vagheit im Sinn der Familienähnlichkeit einzufangen erlaubt. Der Vergleich von Simulation und realen Daten erlaubt Schluss¬≠folgerungen für künftige Studien. Obwohlperspektivi¬≠sche Modellierung auf Klassifikati¬≠onsaufgaben beruht, die über die Logik von ""ja oder nein"" und nicht über die von ""mehr oder weniger"" laufen, ist das ""Mehr oder Weniger"" in einige Klassifikationsal¬≠gorithmen sozusagen eingebaut. Der  Mit dem entwickelten Verfahren lassen sich aufschlussreiche Anomalien historischer Gattungen sichtbar machen, die wichtige, aber im kodifizierten ""Literaturwissen"" nach wie vor marginalisierte Positionen in der historischen Gattungsforschung stützen (Meyer, 1987; Lukas, 1998). Hierfür wurde eine Projektionsmethode entwickelt, die jeweils eine Gattung im Verhältnis zu den beiden Nachbargattungen visualisiert. Abbildung 4 zeigt die Prototypikalität nach paarweise konstruierten Gattungsunterscheidungen. Die Achsen zeigen jeweils die invertierten Vorhersagewahrscheinlichkeiten (siehe y-Achse aus Abbildung 1). Dargestellt werden die Texte einer Gattung aus dem Set der  Die im Detail in Spezialfragen historischer Gattungsforschung führenden Ergebnisse lassen sich methodologisch auf gewinnbringende Weise für Fragen der algorithmischen Modellierung von Vagheit innerhalb historischer Semantiken generalisieren, wenn es gelingt, diese Modellierung von Unbestimmtheit so zu plausibilisieren, dass die sichtbar gemachte Vagheit tatsächlich einem Ausmaß erwartbarer Vagheit in der historischen Semantik entspricht. Bis hierher lässt sich feststellen, dass das entwickelte Modell so etwas wie die Struktur der Prototypikalität im Raum algorithmischer Konzeptualisierung abbildet. Ob die Resultate indes dem entsprechen, was man in der Linguistik sowie in der Literaturwissenschaft unter Prototypen einer Gattung oder Kategorie sowie unter dem Grad an prototypikalischer begrifflicher Struktur eines bestimmten Gattungsbegriffs versteht, bedarf der Kombination (oder Triangulation) mit weiterer philologischer, evtl. auch psychologischer Evidenz. Daher wird eine Möglichkeit der Triangulation in Form eines Vergleichs der entwickelten und auf dem Konzept der Unentscheidbarkeit beruhenden Verbesserungsrate mit einer alternativen Konzeptualisierung von Vagheit diskutiert: Zumindest für die simulierten Fälle sollten die durch mehrere Feature-Cluster (s. Abb. 3 Mitte und Rechts) erzeugten Klassen mit potenzieller kategorialer Vagheit in Form von "" Zuletzt lassen sich zwei weitere wichtige Schritte für künftige Anschlussforschung präsentieren und so zur Diskussion stellen: Zum einen die Möglichkeit und vor allem die Herausforderungen einer Evaluation anhand psycholinguistischer Evidenz, indem menschliche Urteile zu mehr oder weniger prototypischen Beispielen von Kategoriezugehörigkeit herangezogen werden. Zum anderen ist der Umgang mit uneindeutigen Kategorisierungen ein offenes Problem im Arbeitsfeld des",de,Modellierung vagheit Unsicherheit sprachlich Kategorisierung offen Problem Bereich Sprachphilosophie literarisch Gattungstheorie stellen Prototypentheorie rosch taylor Hempfer Familienähnlichkeitsbegriff Wittgenstein folgend Modellierung wesentlich Schritt kennzeichnen Aufbau geeignet verfahrens maschinell Lernen nutzbar Kennzahl Bemessung relativ Nähe Distanz begrifflich Zentrum bereitstellen Erfassung Bereich relativ Vagheit Maß Bemessung relativ Vagheitscharakter Gattungsbegriff Novelle wählen logistisch Regression klassisch Modell überwacht maschinell Lernen logistisch Regression gekommenes lernverfahren darstellen entscheidend Vorteil erstens Underwood betonen Interpretierbarkeit Features relativ Vorhersagewahrscheinlichkeit Zuordnung Instanz Frage kommend Klasse grundlegend Idee Schritt relativ Nähe Instanz begrifflich Zentrum Sinn Prototypikalität Vorhersagewahrscheinlichkeit bestimmt Klasse ausdrücken entscheidend Schritt Einführung Bereich relativ Vagheit vorgestellt Modell verwenden sogenannter nächster Schritt bedürfen Entscheidungsfunktion Enthaltung hierfür logistisch Regression Algorithmus verwenden sigmoid Funktion logistisch Funktion gl gl Berechnung Vorhersagewahrscheinlichkeit laufen Abbildung zeigen Bereich Enthaltung symmetrisch Vorhersagegenauigkeit wählen anstelle Entscheidungslinie Wert Y Vorhersagewahrscheinlichkeit Art Grauzone einführen Funktionswerte innerhalb bereichs Anstelle binären Vorhersage möglich Klasse Enthaltung erfolgen Enthaltung lediglich Enthaltung Modell Status Objekt optimal breite enthaltungsbereichs Form gridsuche ermitteln Abbildung zeigen graphen Gridsuche Modell Klassifikation romanen märchen Funktion abbildet trainiert Modell verfügbar validierungsdan jeweils ausgeweitet enthaltungsbereich Vorhersagewahrscheinlichkeit zugehörig Berechnung Grad begrifflich Vagheit Weise algorithmisch modellieren stehen mehrere Maß potenziell Kandidat Verfügung Breite optimal enthaltungsbereichs -- Prozess intern Evalutation daten simulieren verhalten Berechnung Optimaler enthaltungsbreiten verstehen Ergebnis visualisierung Abbildung darstellen hierfür vagheit Sinn Kombination Familienähnlichkeit potenziell Prototypikalität simulieren Fall Klasse mehrere informativ Features Varianz Störrausch Sinn Annäherung Prototypikalität erzeugen linker spalen Fall Klasse mehrere Cluster Klasse mittlerer spalen Cluster Klasse spalen jeder Simulation iterieren Boxplot Mitte zeigen Verteilung optimal Breite optimal enthaltungsbereichs Iteration unter zeile zeigen Verteilung Verbesserungsrate zeichnen verbesserungsrate robuster aussagekräftig Kennzahl komplexe Merkmalsbündel kompliziert mehrfachclustern innerhalb Klasse abzubilden annehmen kategorie mehrere intern Cluster gliedern vage Familienähnlichkeitsbegriff entsprechen lässen strukturell Ebene sehen Verbesserungsrate anhand optimal enthaltungsbereich Modellierung Prototypikalität darstellen vagheit Sinn Familienähnlichkeit einfangen erlauben Vergleich Simulation real daten erlauben künftig Studie Modellierung beruhen Logik laufen sozusagen einbauen entwickelt Verfahren lassen aufschlussreich Anomali historisch Gattung sichtbar wichtig kodifiziert literaturwissen marginalisiert Position historisch Gattungsforschung stützen Meyer Lukas hierfür Projektionsmethode entwickeln jeweils Gattung Verhältnis Nachbargattung visualisieren Abbildung zeigen Prototypikalität paarweise Konstruiert gattungsunterscheidungen Achse zeigen jeweils Invertiert vorhersagewahrscheinlichkeien sehen Abbildung darstellen Text Gattung set Detail spezialfragen historisch Gattungsforschung führend Ergebnis lassen methodologisch gewinnbringend Weise Frage algorithmisch Modellierung vagheit innerhalb historisch Semantike generalisieren gelingen Modellierung Unbestimmtheit plausibilisieren sichtbar gemacht vagheit tatsächlich Ausmaß erwartbarer vagheit historisch Semantik entsprechen hierher lässen feststellen entwickelt Modell Struktur Prototypikalität Raum algorithmisch Konzeptualisierung abbilden Resultate indes entsprechen Linguistik Literaturwissenschaft Prototyp Gattung Kategorie Grad prototypikalisch begrifflich Struktur bestimmt Gattungsbegriff verstehen bedürfen Kombination Triangulation weit philologisch psychologisch Evidenz Möglichkeit Triangulation Form Vergleichs entwickelt Konzept Unentscheidbarkeit beruhend verbesserungsrate alternativ Konzeptualisierung vagheit diskutieren zumindest simulierter Fall mehrere abb Mitte rechts erzeugt Klasse potenziell kategorial Vagheit Form zuletzt lassen wichtig Schritte künftig Anschlussforschung präsentieren Diskussion stellen Möglichkeit Herausforderung Evaluation anhand psycholinguistisch Evidenz menschlich Urteil prototypisch Beispiel Kategoriezugehörigkeit heranziehen Umgang uneindeutig Kategorisierung offen Problem Arbeitsfeld,"[('vagheit', 0.4234857989345639), ('prototypikalität', 0.2479989857109169), ('klasse', 0.22406019447784903), ('enthaltung', 0.2066658214257641), ('vorhersagewahrscheinlichkeit', 0.2066658214257641), ('verbesserungsrate', 0.16533265714061127), ('logistisch', 0.16533265714061127), ('begrifflich', 0.15399483597620506), ('optimal', 0.1504109216159718), ('enthaltungsbereichs', 0.12399949285545846)]"
2025,DHd2025,MENDE_Jana_Katharina_Gender__under__construction__Daten_und_.xml,Gender (under) construction: Daten und Diversität im Kontext digitaler Literaturwissenschaft,"Jana-Katharina Mende (Martin-Luther-University Halle-Wittenberg, Deutschland); Claudia Resch (Österreichische Akademie der Wissenschaften Wien, Austrian Centre for Digital Humanities and Cultural Heritage, Abteilung Literatur- und Textwissenschaft, Österreich); Mareike Schumacher (Universität Stuttgart / Universität Regensburg, Deutschland); Laura Untner (FU Berlin / Österreichische Akademie der Wissenschaften, Wien, Deutschland und Österreich); Imelda Rohrbacher (Österreichische Akademie der Wissenschaften Wien, Austrian Centre for Digital Humanities and Cultural Heritage, Abteilung Literatur- und Textwissenschaft, Österreich); Elena Suarez Cronauer (Akademie der Wissenschaften und der Literatur, Mainz, Deutschland); Andrea Gruber (Österreichische Nationalbibliothek, Wien, Österreich); Frederike Neuber (Berlin-Brandenburgische Akademie der Wissenschaften, Berlin, Deutschland)","Gender, Infrastruktur, Literaturwissenschaft, Gender Data Gap","Gender, Infrastruktur, Literaturwissenschaft, Gender Data Gap","Die sogenannte Gender Data Gap wirkt sich in mehrfacher Hinsicht auf digitale literaturwissenschaftliche Forschung aus: Historische Ungleichheiten in Bezug auf Gender zeigen sich auf der Ebene von Quellen, Textkorpora, Metadaten wie auch in literarischen Texten selbst. Eine systematische Betrachtung und Erfassung von Geschlechterverhältnissen spielt in DH- und digital-literaturwissenschaftlichen Projekten jedoch kaum eine Rolle. Das Panel widmet sich daher der kritischen Auseinandersetzung von Repräsentation und Erfassbarkeit von Gender in der digitalen Literaturwissenschaft und diskutiert anhand von vielfältigen Lösungsansätzen aus der Fachgemeinschaft, wie mit Gender im Aufbau von Datensätzen, Textkorpora und Forschungsprojekten umgegangen werden kann. Die Gender Data Gap, die sich in Bezug auf Forschungsdaten zeigt, betrifft alle datenbasiert arbeitenden Disziplinen und steht besonders seit dem Erscheinen des Buches ""Data Feminism"" (D""Ignazio/Klein 2020) immer wieder in Diskussion. Von Data Science zu den Digital Humanities offenbart sich ein Bias, der Gender, Race sowie andere marginalisierte Perspektiven weniger oder gar nicht berücksichtigt (vgl. Leyrer 2021, 50). Das resultiert in einer Unterrepräsentation von FLINTA* in Zugängen, Daten und Infrastrukturen (vgl. D""Ignazio/Klein 2020, Kap. 4; Saeger 2016) (Das Akronym FLINTA* steht für Frauen, Lesben, intergeschlechtliche, nichtbinäre, transgeschlechtliche und agender Personen, das Sternchen bezieht weitere Geschlechter mit ein (vgl. Ash 2023). Wir verwenden den Ausdruck in diesem Kontext, um darauf hinzuweisen, dass die Gender Data Gap sowohl Frauen als auch andere Geschlechter betrifft.). Die Gender Data Gap ist dabei intersektional zu verstehen, d.h. sie bezieht sich auf miteinander verknüpfte Kategorien, die jeweils mit eigenen Marginalisierungserfahrungen verbunden sind (zu DH und Intersektionalität siehe Bordalejo/Risam 2019, Losh/Wernimont 2019). Die feministische Literaturwissenschaft beschäftigt sich schon seit ihrer Entstehung in der zweiten Hälfte des 20. Jahrhunderts mit den Lücken in Bezug auf Geschlecht und Literatur. Dabei stand lange Zeit, zuletzt bei Seifert (2021), die Wiederentdeckung von Autorinnen sowie die Kritik an männlich dominierten universitären Leselisten und Kanones im Vordergrund. Die feministische Literaturtheorie bietet wichtige Grundlagen zu intersektionalen Genderbegriffen (vgl. Babka 2004), die mögliche nicht-binäre Genderkonzepte enthalten. Insgesamt zeigt sich jedoch in der traditionellen wie auch der digitalen Literaturwissenschaft, dass FLINTA* in Bezug auf Textkorpora, Kanonisierung, Datensätze und generell als (indirekte) Stakeholder (Leyrer 2021, 5.1) weniger berücksichtigt werden als Männer. Angesichts der tradierten Kanonbildung und etablierter Literaturlisten, die in den traditionellen Literaturwissenschaften oft eine dominierende Rolle spielen, stellt die digitale Transformation literarischer Texte eine Chance, aber auch eine Herausforderung dar. Wenn Texte zu Daten werden, offenbaren sich auf mehreren Ebenen Gender- und Diversitätslücken, die es zu analysieren und zu adressieren gilt. Der aktuelle Stand der Forschung zeigt, dass die Gender Data Gap in den Digital Humanities immer wieder Thema ist (vgl. Lang/Borek/Probst 2023), etwa wenn Juen (2021) konkrete Ungleichheiten in Infrastrukturen und Bibliothekskatalogen vorstellt. In der literaturwissenschaftlichen Forschung mittels digitaler Methoden bilden diese Ungleichheiten oft nicht den Ausgangspunkt, sondern werden erst im Workflow oder durch die Ergebnisse sichtbar (vgl. z.B. Weitin 2021, 47f). Kanonisierungsfragen werden durch Digitalität neu belebt (vgl. Baum 2020). Das Wechselverhältnis von feministischer Literaturtheorie in Bezug auf Genderkonzeptionen und ihre technologische Verarbeitung beschreiben Caughie et al. (2018). Flüh und Schumacher (2020) zielen in ihrem vorbildhaften Projekt dezidiert auf die Modellierungen von (nicht-binären) Genderdarstellungen in der Literatur ab. In dieser Bandbreite zeigt sich eine Vielfalt von technologischen, methodologischen und theoretischen Verschränkungen, die auf die doppelte Verschränkung von literarischer Ebene und Metadaten abzielt. Ein zentrales Anliegen des Panels ist die Diskussion über die Modellierung von Gender sowohl im Text selbst als auch auf der Ebene der Metadaten. Dabei stellt sich die Frage, wie Gender in historischen Texten erfasst und dargestellt wird und wie Normdaten zur Geschlechtszuordnung beitragen. Hier werden historische Ungleichheiten bezüglich Gender durch aktuelle Analysen in DH-Projekten entweder weitergetragen oder durch Maßnahmen gezielt korrigiert. In Bezug auf das Thema der Tagung ""Under Construction. Geisteswissenschaften und Data Humanities"" will das Panel gemeinsam mit Expert*innen erörtern, durch welche Maßnahmen der beschriebene Status quo verbessert werden könnte, wobei die Erfassbarkeit, Konstruktion und Repräsentation von Gender im Mittelpunkt stehen. Es soll untersucht werden, wie feministische Kritik dazu beitragen kann, Grenzen der Computational Literary Studies zu überwinden und Zukunftsszenarien zu entwerfen, die eine angemessene Berücksichtigung von Gender ermöglichen. Durch die kritische Reflexion und Diskussion dieser Themen will das Panel einen Beitrag zur Weiterentwicklung der Digital Humanities leisten und aufzeigen, wie digitale Methoden und feministische Theorien synergetisch zusammenwirken können, um mit Diversität in literarischen Textdaten und Datensätzen umzugehen. Um über die beschriebenen Problemkonstellationen gemeinsam reflektieren zu können, lädt das Panel ausgewiesene Expert*innen aus Forschungs- und Infrastrukturprojekten zur Diskussion ein, wobei die verschiedenen Perspektiven von Doktorandinnen und Postdocs vertreten werden, um die unterschiedlichen Erfahrungen und Ressourcen im Umgang mit diesen Lücken strukturell zu demonstrieren. Das Panel beginnt mit einer kurzen Einleitung der Organisatorinnen, dann folgen 3-5 minütige Impulse der Panelist:innen, die danach in einer Paneldiskussion besprochen werden. Die letzten 30 Minuten des Panels stehen für eine Debatte im Plenum mit Publikum zur Verfügung. In diesem Impulsvortrag wird ein Überblick über DH-Ansätze gegeben, die sich mit Gender befassen. Es wird gezeigt, dass und warum derzeitige Studien meist mit einem binären Genderverständnis operieren und inwiefern es bereits erste Schritte in Richtung einer Überwindung eines solchen gibt. Neben analytischen Ansätzen werden auch aktivistische Beiträge aus Data Feminism und Queer Studies berücksichtigt. Obwohl es derzeit noch keine ""Digital Gender Studies"" gibt, wird gezeigt, dass der Boden bereitet ist, auf denen ein solches Feld aufgebaut werden könnte. Das Projekt ""Sappho Digital"" zielt darauf ab, die deutschsprachige literarische Rezeptionsgeschichte der antiken griechischen Dichterin Sappho als Linked Data zu modellieren. Durch Biases bedingte Lücken, Unsicherheiten und Falschinformationen in Metadatensätzen stellen dabei eine bedeutende Herausforderung dar, besonders da Sappho eine zentrale Figur für weibliche und queere Autor_innen war und ist. Die im digitalen Raum erkennbaren Biases verdeutlichen nicht nur die Notwendigkeit, sondern auch die Möglichkeit, diesen Verzerrungen entgegenzuwirken wie in diesem Impulsbeitrag gezeigt wird.  Für die Untersuchung von Frauen und ihrer(/n) Lebenswelt(/en) bieten Briefe als historische Quelle vielversprechende Perspektiven: Sie sind hierbei ""Ausdruck weiblichen Lebens und Erlebens"" (Barbara Becker-Cantarino). Gleichwohl sind die Vorüberlegungen und Modellierungsebenen, gerade auch in der Arbeit mit digitalen Methoden, entscheidend, um diese Zugänge über Briefe zu finden. Anhand des Netzwerks der frühromantischen Korrespondenzen soll kurz diskutiert werden, wie mit diesen Herausforderungen in Bezug auf Modellierung von Gender umgegangen werden kann und welche Chancen für die Forschung zu Frauen um 1800 dadurch entstehen. Die Überprüfung und Aktualisierung von Klassifikationssystemen und Normdateien ist entscheidend, um die Gender Data Gap zu schließen und adäquate, inklusive Repräsentationen von Gender und Diversität zu gewährleisten. Dafür müssen historische Bias und zeitgenössische Definitionen verstanden, Regelwerke analysiert und neue Modelle entwickelt werden. Der Impulsbeitrag zeigt, wie diese Modelle mit bestehenden Infrastrukturen verbunden werden sollten, um die fortlaufende Nutzung alter Klassifizierungen zu ermöglichen, Konsistenz zu gewährleisten und zukünftige Anforderungen zu antizipieren. An der Berlin-Brandenburgischen Akademie der Wissenschaften entstehen zahlreiche digitale Editionen, die fast alle um namenhafte Männer wie Aristoteles, Alexander von Humboldt und Jean Paul konzipiert wurden. Da sich hingegen kein einziges Projekt explizit einer weiblichen Protagonistin widmet, schlagen TELOTA und die Frauenvertretung der BBAW sieben Schritte zur Überwindung der Gender Data Gap (Jahnke et al. 2023) vor, die der Beitrag schlagwortartig thematisieren wird. Der Schritt der Modellierung von Geschlecht in den TEI-Daten erfolgt seit kurzem mit der Software ediarum (Dumont/Fechner 2014/15) und steigert nicht nur die Findbarkeit und Sichtbarkeit von Frauen und anderen marginalisierten Personengruppen, sondern macht die Kategorie ""Geschlecht"" auch in Zusammenhang mit anderen Faktoren aus intersektionaler Perspektive erforschbar (Neuber et al. 2024, 3-4). Jana-Katharina Mende, Martin-Luther-Universität Halle-Wittenberg, Literaturwissenschaft; Claudia Resch, √ñsterreichische Akademie der Wissenschaften Wien, Austrian Centre for Digital Humanities and Cultural Heritage, Abteilung Literatur- und Textwissenschaft.",de,sogenannter Gender data gap wirken mehrfach Hinsicht digital literaturwissenschaftlich Forschung historisch Ungleichheit Bezug gender zeigen Ebene quellen Textkorpora metadaten literarisch Text systematisch Betrachtung Erfassung Geschlechterverhältnisse spielen Projekt Rolle Panel widmen kritisch Auseinandersetzung Repräsentation Erfassbarkeit gender digital Literaturwissenschaft diskutieren anhand vielfältig lösungsansätzen Fachgemeinschaft gend Aufbau datensätzen Textkorpora Forschungsprojekt umgehen Gender data gap Bezug Forschungsdat zeigen betreffen datenbasiert arbeitend Disziplin stehen erscheinen Buch Data feminism d Ignazio klein Diskussion Data Science Digital Humanitie offenbaren Bias Gender Race marginalisiert Perspektive berücksichtigen Leyrer resultieren Unterrepräsentation flinta zugängen daten infrastruktur d Ignazio klein kap saeger akronym flinta stehen Frau lesber intergeschlechtlich nichtbinär transgeschlechtlich agender Person sternchen beziehen geschlechter ash verwenden Ausdruck Kontext hinzuweisen Gender data gap sowohl Frau geschlecht betreffen Gender data gap intersektional verstehen beziehen miteinander verknüpft kategorien jeweils Marginalisierungserfahrung verbinden dh Intersektionalität sehen Bordalejo risam Losh Wernimont Feministische Literaturwissenschaft beschäftigen Entstehung Hälfte Jahrhundert Lücke Bezug Geschlecht Literatur stehen zuletzt seiferen Wiederentdeckung Autorinne Kritik männlich dominiert universitär Leselist kanon vordergrund feministisch Literaturtheorie bieten wichtig Grundlage intersektional Genderbegriffe Babka möglich Genderkonzept enthalten insgesamt zeigen traditionell digital Literaturwissenschaft flinta Bezug Textkorpora Kanonisierung datensätz generell indirekt Stakeholder leyrer berücksichtigen Mann angesichts tradiert Kanonbildung etabliert Literaturliste traditionell Literaturwissenschaften dominierend Rolle spielen stellen digital Transformation literarisch Text Chance Herausforderung dar Text daten offenbaren mehrere eben Diversitätslück analysieren adressieren gelten Aktuelle Stand Forschung zeigen Gender Data gap Digital Humanitie Thema Borek probst juen konkret Ungleichheit infrastruktur bibliothekskatalogen vorstellen literaturwissenschaftlich Forschung mittels Digitaler Methode bilden Ungleichheit Ausgangspunkt Workflow Ergebnis sichtbar weitin Kanonisierungsfrag Digitalität neu beleben Baum Wechselverhältnis feministisch Literaturtheorie Bezug genderkonzeptionen technologisch Verarbeitung beschreiben Caughie et Flüh Schumacher zielen vorbildhaft Projekt dezidiert Modellierunge Genderdarstellung Literatur Bandbreite zeigen Vielfalt technologisch methodologisch theoretisch Verschränkung doppelt Verschränkung literarisch Ebene metadaten abzielen zentral Anliegen Panel Diskussion Modellierung Gender sowohl Text Ebene metadaten stellen Frage gender historisch Text erfasst darstellen Normdate Geschlechtszuordnung beitragen historisch Ungleichheit bezüglich gend aktuell Analyse weitergetrag Maßnahme gezielt korrigieren Bezug Thema Tagung Under Construction geisteswissenschaften data Humanitie Panel gemeinsam erörtern Maßnahme beschrieben status -- verbessern wobei Erfassbarkeit Konstruktion Repräsentation Gender Mittelpunkt stehen untersuchen feministisch Kritik beitragen Grenze Computational literary Studie überwinden Zukunftsszenarie entwerfen angemessen Berücksichtigung gender ermöglichen kritisch Reflexion Diskussion Thema Panel Beitrag Weiterentwicklung Digital Humanitie leisten aufzeigen digital Methode feministisch Theorie synergetisch zusammenwirken Diversität literarisch Textdat datensätzen umgehen beschrieben Problemkonstellation gemeinsam Reflektier laden Panel ausgewiesen infrastrukturprojeken Diskussion wobei verschieden Perspektive doktorandinnen postdocs vertreten unterschiedlich Erfahrung Ressource Umgang Lücke Strukturell demonstrieren Panel beginnen kurz Einleitung Organisatorinn Folge minütig Impuls Panelist innen Paneldiskussion besprechen letzter Minute Panel stehen Debatte Plenum Publikum Verfügung Impulsvortrag Überblick geben gender befassen zeigen derzeitig Studie meist binären Genderverständnis operieren inwiefern Schritt Richtung Überwindung analytisch Ansatz aktivistisch Beitrag Data Feminism Queer studies berücksichtigen obwohl derzeit Digital Gender studies zeigen Boden bereiten Feld aufbauen Projekt Sappho Digital zielen deutschsprachig literarisch Rezeptionsgeschicht antik griechisch Dichterin sappho Linked data modellieren Bias bedingt Lücke Unsicherheit falschinformationen metadatensätz Stelle bedeutend Herausforderung dar sappho zentral Figur weiblich queere digital Raum erkennbaren Bias verdeutlichen Notwendigkeit Möglichkeit verzerrung entgegenwirken Impulsbeitrag zeigen Untersuchung Frau bieten Brief historisch Quelle vielversprechend Perspektive hierbei Ausdruck weiblich Leben Erleben barbara gleichwohl vorüberlegungen modellierungseben Arbeit digital Methode entscheidend Zugäng Brief finden anhand Netzwerk frühromantisch korrespondenzen diskutieren Herausforderung Bezug Modellierung Gender umgehen Chance Forschung Frau entstehen Überprüfung Aktualisierung Klassifikationssysteme normdateien entscheidend Gender data gap schließen adäquat inklusive Repräsentatione Gender Diversität gewährleisten historisch Bia zeitgenössisch definitionen verstehen regelwerken analysieren Modell entwickeln Impulsbeitrag zeigen Modell bestehend infrastruktur verbinden fortlaufend Nutzung alt Klassifizierung ermöglichen Konsistenz gewährleisten zukünftig Anforderung antizipieren Akademie Wissenschaft entstehen zahlreich digital editionen fast namenhaft Mann aristotele Alexander Humboldt Jean Paul konzipieren hingegen einzig Projekt explizit weiblich Protagonistin widmen schlagen Telota Frauenvertretung Bbaw Schritt Überwindung Gender data gap Jahnke Et Beitrag schlagwortartig thematisieren Schritt Modellierung Geschlecht erfolgen kurz Software ediarum dumont Fechner steigern Findbarkeit Sichtbarkeit Frau marginalisieret Personengruppe Kategorie Geschlecht Zusammenhang Faktor intersektional Perspektive erforschbar Neuber et mend Literaturwissenschaft claudia resch Akademie Wissenschaft Wien austrian centre for digital humanities and cultural heritage Abteilung Textwissenschaft,"[('gender', 0.5164308708620016), ('data', 0.23927087088601923), ('gap', 0.20747788652215654), ('panel', 0.15779915939475797), ('ungleichheit', 0.14561508511721635), ('frau', 0.13571639955035328), ('feministisch', 0.13562941245912383), ('digital', 0.12824452872647843), ('bezug', 0.11696847087845139), ('intersektional', 0.10921131383791227)]"
2025,DHd2025,KETSCHIK_Nora_Netzwerkanalysen_narrativer_Texte___ein_Vorgeh.xml,Netzwerkanalysen narrativer Texte - ein Vorgehensmodell,"Nora Ketschik (Universität Stuttgart, Deutschland)","Netzwerkanalyse, Figuren, Methode","Annotieren, Netzwerkanalyse, Literatur, Methoden, benannte Entitäten (named entities)","Die soziale Netzwerkanalyse ist in den Computational Literary Studies (CLS) seit mehreren Jahrzehnten als eine Methode etabliert, mit der Figurenbeziehungen in verschiedenen Textgattungen exploriert und analysiert werden. Der Fokus liegt dabei auf dramatischen Untersuchungsgegenständen (vgl. z.B. Szemes und Vida, 2024; Trilcke, 2013; Trilcke et al., 2024; Krautter und Vauth, 2023; Viehhauser, 2023); die Verwendung der Netzwerkanalyse für narrative Texte ist hingegen vergleichsweise selten. Dies liegt m.E. insbesondere daran, dass es um ein Vielfaches voraussetzungsreicher ist, die für Netzwerkanalysen benötigten Daten aus narrativen Texten zu extrahieren, als dies für Dramen der Fall ist. Während Netzwerkanalysen dramatischer Texte i.d.R. auf den Nebentextangaben basieren und daraus szenenbasierte Kookkurrenzen ableiten, bedarf es mehrerer komplexer Schritte, um die gleiche Art der Information aus Erzähltexten zu extrahieren. Die Mehrarbeit resultiert nicht nur aus komplexen Tasks wie Entitätenreferenzerkennung und Koreferenzauflösung, sondern auch aus dem Umstand, dass die relevanten Informationen mit anderen Aspekten auf Die ursprünglich aus den Sozialwissenschaften stammende Methode der (sozialen) Netzwerkanalyse wird bereits seit vielen Jahren in den CLS für die Analyse von Figurenrelationen, darunter Informations- und Machtstrukturen (Vauth, 2019; Krautter und Vauth, 2023), zur Klassifikation von Figurentypen (Krautter et al., 2020; Vauth, 2023) oder zur Unterscheidung dramatischer Formen (Trilcke, 2013; Szemes und Vida, 2024; Viehhauser, 2023) eingesetzt. Dem Gros der Beiträge ist gemein, dass sie sich dabei erstens auf dramatische Texte und zweitens auf die Szenen- oder Gesprächsstrukturen konzentrieren. Dies liegt u.a. daran, dass die notwendigen Informationen über die Textstruktur leicht greifbar sind: Die szenische Einteilung im Drama gibt Einheiten für Figurenkonfigurationen vor. Eine solch direkte und intuitive Datenerhebung ist für narrative Texte nicht möglich 'weswegen es nach wie vor keinen gleichermaßen etablierten Zugang zur Erfassung von Kookkurrenzen zwischen Figuren gibt. Rochat und Kaplan (2014) verwenden beispielsweise einen Index bestehend aus Eigennamen und Seitenzahlen, um kookkurrenzbasierte Figurennetzwerke zu Rousseaus Autobiographie zu erstellen; sie nehmen immer dann eine Figurenrelation an, wenn zwei Figuren in einem Kontext von drei Seiten gemeinsam genannt werden. Argawal et al. (2012) stützen sich hingegen auf ein an Zeitungstexten entwickeltes Konzept von Die Beispiele zeigen, dass eine große Varianz dahingehend besteht, (erstens) welche Arten von Referenzausdrücken, (zweitens) welche Arten von Relationen, und (drittens) auf welche Weise die Informationen erfasst werden. Darüber hinaus werden Kontexte der Figurenvorkommen (z.B. innerhalb vs. außerhalb von Figurenrede) und Einflüsse von Es ist anzunehmen, dass die fehlende Integration verschiedener, für die Erfassung von Figurenvorkommen und -relationen relevanter Aspekte in der Komplexität dieser Teilaufgaben begründet liegt. So wäre 'analog zum Drama 'die Einteilung eines Textes in ""Szenen"", wie sie Gius et al. (2019a) vorschlagen, ein naheliegender Schritt für die Extraktion von Kookkurrenzen, der aber seinerseits extrem voraussetzungsreich ist, da er auf komplexen Kategorien wie Raum und Zeit/Chronologie aufbaut. Das hier vorgestellte Vorgehensmodell zur Extraktion von Kookkurrenznetzwerken aus narrativen Texten baut auf Analysen zu mittelhochdeutschen Romanen (Ketschik, 2024) auf und wurde im Rahmen des vorliegenden Beitrags an anderen narrativen Texten in neuhochdeutscher Sprache weiterentwickelt und erprobt. Für die Analysen und Statistiken wurden Texte aus dem Deutschen Romankorpus (DROC, Krug et al., 2018) verwendet, in denen Annotationen zu Figuren (inkl. Koreferenzresolution) und Figurenrede enthalten sind. Die Textstatistiken vermitteln einen Eindruck davon, inwiefern die Wahl des Figurenreferenzausdrucks, die Segmentierungsgröße und die Kontexte von Figurenreferenzen die resultierenden Netzwerke beeinflussen. Tabelle 1 zeigt, dass Eigennamen mit durchschnittlich 14% und Werten zwischen knapp 10% und gut 20% den geringsten Anteil der Figurenreferenzen im Textkorpus ausmachen, gefolgt von Appellativen mit durchschnittlich 18%. Die häufigsten Referenzausdrücke sind Pronomina, wobei ihr Anteil zwischen 57% und 77% liegt. Homodiegetische Erzählungen (hier Die Verteilung ist aber innerhalb eines Textes figurenspezifisch. In Die Wahl des Referenzausdrucks hat konsequenterweise gravierende Auswirkungen auf das resultierende Netzwerk. Exemplarisch seien zwei Netzwerke zu einem Auszug aus Fontanes Neben der Wahl der Referenzausdrücke hat die Segmentgröße als Grundlage für Kookkurrenzen einen entscheidenden Einfluss auf die Netzwerke. Die Größe der Segmentierung hängt (auch) mit der Wahl der Referenzausdrücke zusammen. Werden z.B. keine Pronomina berücksichtigt, ist es sinnvoll, die Segmente zu vergrößern. Welche Segmentgröße angemessen ist, lässt sich nicht pauschal beantworten. Hinweise können die Abstände zwischen Pronomen und Antezedent geben, da etwa ein Netzwerk ohne pronominale Referenzen den ""Weg"" bis zur nächsten nicht-pronominalen Referenz (Appellativ oder Eigenname) überbrücken muss. Die Abstände sind wiederum stark textabhängig (und figurenspezifisch), beispielsweise stehen in Fontanes Das Beispielnetzwerk zu Die fehlende Differenzierung zwischen Kontexten der Figurennennungen führt nicht nur dazu, dass im Netzwerk ggf. Relationen zwischen Figuren visualisiert werden, die in der Handlung nie kookkurrieren, sondern auch, dass (etwa innerhalb von Figurenrede) erwähnte Figuren die Netzwerkmetriken (z.B. die Netzwerkgröße, Zentralitätswerte, Dichtemaße) in hohem Maß beeinflussen. So umfasst das Netzwerk zum Die verschiedenen Aspekte, die für eine reflektierte Netzwerkanalyse von narrativen Texten relevant sind, werden nun in einem Vorgehensmodell zusammengefasst (Abb. 3). Das Modell kann unabhängig davon eingesetzt werden, ob die Schritte durch manuelle Annotation oder (teil-)automatisch umgesetzt werden. Bestimmte Arbeitsschritte (v.a. III–V) können je nach Texteigenschaften oder Untersuchungsfrage ggf. wegfallen. Die einzelnen Schritte werden im Folgenden kurz erläutert.   Das hier vorgestellte Vorgehensmodell umfasst relevante Schritte für die Extraktion kopräsenter Figuren aus narrativen Texten. Ein Hauptanliegen des Beitrags ist es, aufzuzeigen, welche Aspekte bei der Netzwerkanalyse narrativer Texte (potenziell) eine Rolle spielen, und diese in die Datenerhebung zu integrieren 'oder, sollte dies nicht möglich sein, zumindest das Bewusstsein für deren Einfluss auf die netzwerkanalytischen Daten zu schärfen. Zweifelsohne können nicht alle Sonderfälle in einem möglichst generischen Modell berücksichtigt werden; vielmehr dient das Modell als methodische Grundlage, die dazu befähigt, Kookkurrenznetzwerke aus Erzähltexten zu erstellen, die für Einzelfälle aber angepasst oder ergänzt werden muss. Durch seine Modularität und die ""Filter""-Schritte soll das Modell für viele Fragestellungen und Untersuchungsgegenstände einsetzbar sein.",de,sozial Netzwerkanalyse Computational literary Studies cls mehrere Jahrzehnt Methode etablieren Figurenbeziehung verschieden Textgattunge explorieren analysieren Fokus liegen dramatisch untersuchungsgegenständ szeme Vida trilcken trilcken et Krautter Vauth viehhauser Verwendung Netzwerkanalyse narrativ Text hingegen vergleichsweise selten liegen insbesondere vielfach voraussetzungsreich netzwerkanalys benötigt daten narrativ Text extrahieren Dram Fall netzwerkanalysen dramatisch texte Nebentextangabe basieren szenenbasiert Kookkurrenze ableiten bedürfen mehrere komplex Schritt gleich Art Information Erzähltext extrahieren Mehrarbeit resultieren komplex Tasks Entitätenreferenzerkennung Koreferenzauflösung Umstand relevant Information Aspekt ursprünglich Sozialwissenschaft stammend Methode sozial Netzwerkanalyse Cls Analyse figurenrelation Machtstruktur Vauth Krautter Vauth Klassifikation Figurentype krautter et vauth Unterscheidung dramatisch Form Trilcke szeme Vida viehhauser einsetzen Gros beiträge gemein erstens dramatisch Text zweitens gesprächsstrukturen konzentrieren liegen notwendig Information Textstruktur greifbar szenisch Einteilung Drama Einheit Figurenkonfiguration solch direkt intuitiv Datenerhebung narrativ Text weswegen gleichermaßen etabliert Zugang Erfassung kookkurrenzen Figur Rochat Kaplan verwenden beispielsweise Index bestehend eigennamen seitenzahlen kookkurrenzbasiert Figurennetzwerke Rousseaus Autobiographie erstellen nehmen Figurenrelation Figur Kontext Seite gemeinsam nennen argawal et stützen hingegen zeitungstext entwickelt Konzept Beispiel zeigen Varianz dahingehend bestehen erstens Art Referenzausdrücken zweitens Art relationen drittens Weise Information erfasst hinaus Kontexte Figurenvorkomm innerhalb außerhalb Figurenred einflüsse annehmen fehlend Integration verschieden Erfassung Figurenvorkomm relevant Aspekt Komplexität Teilaufgabe begründen liegen analog Drama Einteilung Text Szene Gius et vorschlagen naheliegend Schritt Extraktion Kookkurrenze seinerseits extrem voraussetzungsreich komplex kategorien Raum Chronologie aufbauen vorgestellt Vorgehensmodell Extraktion Kookkurrenznetzwerke narrativ Text bauen Analyse mittelhochdeutsch Roman Ketschik Rahmen vorliegend Beitrag narrativ Text neuhochdeutsch Sprache weiterentwickeln erproben analyse Statistik Text deutsch Romankorpus Droc kragen et verwenden annotatio Figur Koreferenzresolution Figurenrede enthalten Textstatistike vermitteln Eindruck inwiefern Wahl Figurenreferenzausdruck Segmentierungsgröß Kontexte Figurenreferenze resultierenden netzwerk beeinflussen Tabelle zeigen eigennamen durchschnittlich werten knapp gering Anteil Figurenreferenze Textkorpus ausmachen folgen Appellativ durchschnittlich häufig Referenzausdrücke Pronomina wobei Anteil liegen homodiegetisch Erzählung Verteilung innerhalb Text figurenspezifisch Wahl Referenzausdruck konsequenterweise gravierend Auswirkung resultierend Netzwerk exemplarisch Netzwerk Auszug fontan Wahl Referenzausdrücke Segmentgröße Grundlage kookkurrenzen entscheidend Einfluss netzwerke Größe Segmentierung hängen Wahl Referenzausdrücke Pronomina berücksichtigen sinnvoll Segment vergrößern segmentgröße angemessen lässen pauschal beantworten Hinweis Abständ Pronome Antezedent geben Netzwerk pronominal referenzen weg nächster Referenz appellativ Eigenname überbrücken Abständ wiederum stark textabhängig figurenspezifisch beispielsweise stehen Fontanes Beispielnetzwerk fehlend Differenzierung Kontext Figurennennung führen Netzwerk Relation Figur visualisieren Handlung kookkurrieren innerhalb Figurenrede erwähnt Figur Netzwerkmetrik Netzwerkgröße zentralitätsweren Dichtemaße hoch Maß beeinflussen umfassen Netzwerk verschieden Aspekt reflektiert Netzwerkanalyse narrativ Text relevant vorgehensmodell zusammengefasst abb Modell unabhängig einsetzen Schritt Manuelle Annotation umsetzen bestimmt Arbeitsschritte Texteigenschaft untersuchungsfrag wegfallen einzeln Schritt folgend erläutern vorgestellt Vorgehensmodell Umfasst relevant Schritt Extraktion Kopräsenter Figur narrativ Text Hauptanliegen Beitrag aufzuzeigen Aspekt Netzwerkanalyse narrativ Text Potenziell Rolle spielen Datenerhebung integrieren zumindest Bewusstsein einfluss netzwerkanalytisch daten schärfen Zweifelsohne Sonderfälle möglichst generisch Modell berücksichtigen vielmehr dienen Modell methodisch Grundlage befähigen Kookkurrenznetzwerk Erzähltext erstellen Einzelfäll angepasst ergänzen Modularität Modell Fragestellung untersuchungsgegenständen einsetzbar,"[('narrativ', 0.22686000860046734), ('netzwerk', 0.1893896142601932), ('vauth', 0.1840390862392968), ('netzwerkanalyse', 0.1737022562349237), ('vorgehensmodell', 0.16334271816107915), ('referenzausdrücke', 0.1441938676453002), ('wahl', 0.13685639754963988), ('text', 0.13048864655691372), ('dramatisch', 0.12946477230164571), ('figur', 0.12946196075124142)]"
2025,DHd2025,BROOKSHIRE_Patrick_Daniel_Warum_wird_was_wie_klassifiziert_.xml,Warum wird was wie klassifiziert?   Scalable Reading + Explainable AI am Beispiel historischer Lebensverläufe,"Patrick Daniel Brookshire (Akademie der Wissenschaften und der Literatur | Mainz / Universität zu Köln, Deutschland)","Scalable Reading, eXplainable AI, Historische Biographien, Sentimentanalyse","Entdeckung, Annotieren, Bereinigung, Visualisierung, Methoden","Eine Klassifikation von Textabschnitten ist im DH-Kontext häufig ""under construction"", da vorliegende Verfahren viele Varianten aufweisen, aber nicht domänen-spezifisch genug sind. Deshalb ist stets eine Evaluation mit dem konkreten Datensatz nötig. Zudem sind bei besonders kontextabhängigen Tasks, für die Sentimentanalysen ein verbreitetes Beispiel sind, Deep-Learning-Methoden performanter, dafür aber weniger interpretierbar als bspw. Lexikon-basierte Verfahren (Singh und Singh, 2021; Schmidt et al., 2022; Rebora et al., 2023). Eine mögliche Lösung sind Explainability-Modelle wie In einer Pilotstudie werden Daten des retrodigitalisierten biographischen Nachschlagewerks Aus technischer Sicht umfasst das vorgestellte Verfahren die fünf Module  Durch die Markierung des neutralen Bereichs mit gängigen Schwellenwerten (¬± 0,05; vgl. Hutto und Gilbert, 2014, 224) wird deutlich, dass das Korpus einen leicht positiven Trend mit nur wenigen Schwankungen aufweist. Allerdings zeichnet ein Reinzoomen im Sinne des Scalable Readings auf 36 zufällig ausgewählte Einzelbiographien ein deutlich anderes Bild:  Die Variabilität ist bei einem individuellen Genre wie Biographien nicht überraschend, aber bei der Analyse größerer Datenmengen wegen der Tendenz von Durchnittswerten zum neutralen Bereich (vgl. Jockers, 2015) nur durch Skalenwechsel zu beobachten. Aufgrund der nicht perfekten automatischen Annotationen kann auch diese Einzelansicht mitunter täuschen, weshalb im vorgelegten Verfahren eine am Close-Reading orientierte  So gehen die hier durch Farbsättigung visualisierten Ergebnisse des (aus Performance-Gründen gewählten) Explainability-Modells von Chefer et al. (2021) über die reinen Klassifikationswahrscheinlichkeiten hinaus. Denn sie illustrieren, dass die negativ-Labels auf Subword-Tokens wie ""nur"", ""muß"" und ""Aufhebung"" zurückgehen, aber den gegebenen neutral formulierten philosophischen Abschnitt nur bedingt abbilden. In diesem Fall wäre also eine manuelle Bereinigung der entsprechenden Labels sinnvoll und dank der mitvisualisierten Position im Gesamtdatensatz durch Quellen-ID und Satznummer auch möglich. Zudem ist auch ein erneutes Finetuning mit diesen Datensätzen denkbar. Das vorgestellte Verfahren zeigt, wie sich Fehlklassifikationen, die für nachgelagerte Analyseschritte besonders relevant sind, durch eine Kombination aus Scalable-Reading- und Explainability-Ansätzen gezielt identifizieren und ggf. manuell korrigieren lassen. Denn zur Steigerung der Validität ist es durch Aggregierungsschritte nicht immer nötig, alle Datensätze zu bereinigen 'und je nach personeller und technischer Ausstattung auch nicht immer umsetzbar. Auch wenn das Verfahren hier am Beispiel von Sentimentwerten illustriert wurde, ist es grundsätzlich auf beliebige Klassifikationsaufgaben anwendbar. Daher ist derzeit ein entsprechender Ausbau in Richtung nominal-skalierter Kategorien ""under construction"". Zudem ist angedacht, die Modularisierung so konsequent umzusetzen, dass künftig neben beliebigen Klassifikationsmodellen auch bei den übrigen Modulen beliebige Komponenten angedockt werden können, um dem Variantenreichtum gerecht zu werden.",de,Klassifikation Textabschnitte häufig und Construction vorliegend Verfahren Variant aufweisen stets Evaluation konkret Datensatz nötig zudem kontextabhängig Tasks Sentimentanalyse verbreitet Performanter interpretierbar Verfahren Singh Singh schmidt et Rebora et möglich Lösung Pilotstudie daten retrodigitalisiert biographisch Nachschlagewerks technisch Sicht umfassen vorgestellt Verfahren Module Markierung neutral Bereich gängig schwellenwerten Hutto gilberen deutlich Korpus positiv Trend weniger Schwankung Aufweist zeichnen Reinzoome Sinn scalable Reading zufällig ausgewählt Einzelbiographien deutlich anderer Bild Variabilität individuell Genre Biographien überraschend Analyse groß datenmenger Tendenz Durchnittswert neutral Bereich Jockers Skalenwechsel beobachten aufgrund perfekt automatisch annotationen Einzelansicht mitunter täuschen weshalb vorgelegt Verfahren orientiert Farbsättigung visualisiert Ergebnis Gewählt Chefer et rein Klassifikationswahrscheinlichkeit hinaus illustrieren Aufhebung zurückgehen gegeben neutral formuliert Philosophisch abschneten bedingt abbilden Fall Manuelle Bereinigung entsprechend Label sinnvoll mitvisualisiert Position Gesamtdatensatz Satznummer zudem erneut Finetuning Datensätz denkbar vorgestellt Verfahren zeigen fehlklassifikationen nachgelagert analyseschritte relevant Kombination gezielt identifizier manuell korrigieren lassen Steigerung Validität Aggregierungsschritte nötig Datensätz bereinigen personell technisch Ausstattung umsetzbar Verfahren Sentimentwert illustrieren grundsätzlich beliebig Klassifikationsaufgabe anwendbar derzeit entsprechend Ausbau Richtung Kategorie Under Construction zudem angedacht Modularisierung konsequent umsetzen künftig beliebig klassifikationsmodelle übrig modul beliebig Komponent andocken Variantenreichtum gerecht,"[('neutral', 0.21050351121134367), ('verfahren', 0.2038903265254925), ('beliebig', 0.19766925274158995), ('singh', 0.1928223500000605), ('construction', 0.1629404550446347), ('datensätz', 0.1519674879716542), ('illustrieren', 0.1117850581231388), ('zudem', 0.11129114753994734), ('nötig', 0.1067579261641764), ('mitvisualisiert', 0.09641117500003026)]"
2025,DHd2025,KUPIETZ_Marc_National_Library_as_Corpus__Introducing_DeLiKo_.xml,National Library as Corpus: Introducing DeLiKo@DNB 'a Large Synchronous German Fiction Corpus,"Marc Kupietz (Leibniz-Institut für Deutsche Sprache, Germany); Peter Leinen (Deutsche Nationalbibliothek, Germany); Nils Diewald (Leibniz-Institut für Deutsche Sprache, Germany); Philippe Gen√™t (Deutsche Nationalbibliothek, Germany); Rebecca Wilm (Leibniz-Institut für Deutsche Sprache, Germany); Andreas Witt (Leibniz-Institut für Deutsche Sprache, Germany); Rameela Yaddehige (Leibniz-Institut für Deutsche Sprache, Germany)","corpus, literature, fiction, contemporary, linguistic annotation, metadata, corpus analysis, IPR, library as corpus","Umwandlung, Sammlung, Annotieren, Literatur, Text, virtuelle Forschungsumgebungen","Fiction books are weakly represented in the German Reference Corpus DeReKo (Kupietz et al. 2010, 2018). This is primarily due to the tremendously higher costs associated with licensing and converting raw fiction data into TEI-encoded XML format, compared to newspaper articles (Kupietz et al. 2014, p. 2). In the following, we discuss how we addressed these challenges and successfully created a large extensible corpus of recent fiction books. Linguistics and literary studies both face the challenge that their research data is affected by third-party rights. Obtaining transferable, uniform licenses for German fiction books is particularly costly as no licensing models for non-expressive use (previously also called ""non-consumptive use"", see Kamocki 2018) of entire texts as primary research data are generally established. Moreover, individual author permissions are often required, since the use of texts as research data is not covered by standard licensing agreements between authors and publishers. Our solution to this challenge consists of two main pillars: The first leverages ¬ß 14 of the German National Library Act (DNBG) requiring submission of all digitally published media works to the DNB. The second pillar is part of the general strategy adopted for DeReKo to address legal issues through infrastructural means, following Jim Gray's (2003) famous principle: To address the second challenge, and to make conversion into high-quality TEI-XML encoded corpora feasible, we ignored PDF ebooks and limited our focus to the 273,976 books available in the XML-based EPUB format and drew a 10% random sample from these as a first step, stratified by year of publication, resulting in a sample of 26,091 ebooks. As a second step, on the occasion of the 20th anniversary of the German Book Prize in October 2024, we added all 362 digitally available longlisted titles from the past two decades to the corpus, in cooperation with the German Publishers and Booksellers Association. To convert the data to the TEI I5 format (Lüngen and Sperberg-McQueen 2012) used by DeReKo, we applied XSLT 3.0 stylesheets in three passes, via the Saxon XSLT processor and GNU Make, using the DNB SRU API to retrieve consistent metadata, a heuristic genre classifier based on this, and a MALLET (McCallum 2002) based implementation of the standard DeReKo topic domain classifier (Weiß 2005; Klosa, Lüngen, and Kupietz 2012). In subsequent steps, the TEI-XML data was converted to KorAP-XML format and annotated for POS and lemma using the TreeTagger (Schmid 1994), for POS and morphosyntactic properties using MarMoT (Mueller et al. 2013), and dependencies using MaltParser (Nivre et al. 2007). These tools were selected due to their good balance between accuracy and performance. The entire conversion and annotation process was completed in 48 hours on a Linux server with 96 cores and 1.5 TB of RAM. The composition of the resulting corpus, categorized by genres and publication years, is presented in Figure 1. Genre classifications were derived from the DNB metadata using string matching heuristics.  DeLiKo@DNB, currently comprising 2.02 billion words, is freely accessible through the website We aim to regularly expand the corpus by incorporating newly published books. Additionally, we plan to enhance the search and analysis capabilities by integrating advancements from the long-term KorAP project, including updates to the user interface and client libraries. Further additions, improvements, and extensions, concerning e.g. the addition of books, text classifications, or annotation layers, will be driven by the demands of the user communities engaging with DeLiKo@DNB. ",en,fiction book weakly represent german reference corpus dereko kupietz et al primarily tremendously high cost associate licensing convert raw fiction datum tei encode xml format compare newspaper article kupietz et al following discuss address challenge successfully create large extensible corpus recent fiction book linguistic literary study face challenge research datum affect party right obtain transferable uniform license german fiction book particularly costly licensing model non expressive use previously call non consumptive use kamocki entire text primary research datum generally establish individual author permission require use text research datum cover standard licensing agreement author publisher solution challenge consist main pillar leverage german national library act dnbg require submission digitally publish medium work dnb second pillar general strategy adopt dereko address legal issue infrastructural mean follow jim gray famous principle address second challenge conversion high quality tei xml encode corpora feasible ignore pdf ebook limit focus book available xml base epub format draw random sample step stratify year publication result sample ebook second step occasion anniversary german book prize october add digitally available longlisted title past decade corpus cooperation german publisher bookseller association convert datum tei format lüngen sperberg mcqueen dereko apply xslt stylesheet pass saxon xslt processor gnu dnb sru api retrieve consistent metadata heuristic genre classifier base mallet mccallum base implementation standard dereko topic domain classifier weiß klosa lüngen kupietz subsequent step tei xml datum convert korap xml format annotate pos lemma treetagger schmid pos morphosyntactic property marmot mueller et al dependency maltparser nivre et al tool select good balance accuracy performance entire conversion annotation process complete hour linux server core tb ram composition result corpus categorize genre publication year present figure genre classification derive dnb metadata string matching heuristic currently comprise billion word freely accessible website aim regularly expand corpus incorporate newly publish book additionally plan enhance search analysis capability integrate advancement long term korap project include update user interface client library addition improvement extension concern addition book text classification annotation layer drive demand user community engage,"[('book', 0.2625505759680747), ('dereko', 0.2108100667125438), ('datum', 0.16963204741526478), ('licensing', 0.15810755003440782), ('fiction', 0.15002890055318555), ('convert', 0.14726519645444955), ('kupietz', 0.14726519645444955), ('german', 0.1458962455386771), ('xml', 0.1458962455386771), ('challenge', 0.1345080502048199)]"
2025,DHd2025,KELLNER_Nils_Literaturgeschichte__under_construction____was_.xml,"Literaturgeschichte ""under construction"" 'was können die Computational Literary Studies beitragen? Ein Panel zur digitalen Untersuchung von Raum in der Literatur","Berenike Herrmann (Universität Bielefeld, Deutschland); Daniel Kababgi (Universität Bielefeld, Deutschland); Marc Lemke (Universität Rostock, Deutschland); Nils Kellner (Universität Rostock, Deutschland); Ulrike Henny-Krahmer (Universität Rostock, Deutschland); Fotis Jannidis (Julius-Maximilians-Universität Würzburg, Deutschland); Katrin Dennerlein (Julius-Maximilians-Universität Würzburg, Deutschland); Matthias Buschmeier (Universität Bielefeld, Deutschland)","Computational Literary Studies, Raum, Literaturgeschichte","Computational Literary Studies, Raum, Literaturgeschichte","Im aktuellen Verhältnis derjenigen Literaturwissenschaften, deren methodischer Fokus nicht auf computergestützten Ansätzen liegt, und den Computational Literary Studies (CLS) zeigt sich ein Problem, das im Forschungsdesign vieler DH-Projekte erkennbar ist. Während erstere die Fragestellungen meist anhand literaturgeschichtlicher Kontexte entwickeln, stellen letztere deutlicher die Operationalisierung eines literarischen Phänomens, die Datenmodellierung, die Implementierung und das Finetuning computationeller Verfahren in den Mittelpunkt. Obwohl die zentralen Ziele der CLS darin liegen, ""to explain, or to provide, general laws of literature, and even of history and culture"" (Bode, 2023, 14), bleibt die Frage, ob einer literaturhistorisch fundierten Kontextualisierung oft zu wenig Raum beigemessen wird. Doch auch der Status von Historisierung in den Literaturwissenschaften generell wird debattiert; nicht nur Daniel Fulda sieht Literaturgeschichtsschreibung im starken Sinne ""wissenschaftslogisch [...] allenfalls am Rande des Fachs"" (Fulda, 2014, 104). Dass eine bewusste Historisierung jedoch vorteilhaft wäre, kann auch das Modell zur Beschreibung der Komplexität computationeller Textanalysen (Gius, 2019) zeigen. Insofern stellt sich die Frage, welchen Stellenwert literaturhistorische Grundlagen in Projekten der CLS 'oder anderen literaturwissenschaftlichen Bereichen 'derzeit einnehmen und zukünftig sollten. Ziel des Panels ist es, dieses Verhältnis sowohl allgemein mit Blick auf die Planung eines Forschungsdesigns als auch ausgehend von Perspektiven der Literaturwissenschaft sowie von Praxis-Bezügen aus zwei DH-Projekten zu erörtern. Nicht zuletzt scheint eine diachrone Perspektive in CLS-Projekten oftmals naheliegend, etwa, wenn Publikationsdaten als Metadaten vorliegen und auf der Suche nach Mustern historischer Wandel explorativ modelliert wird. Thematisch soll ""Raum in literarischen Texten"" als exemplarisch für derartige Überlegungen fruchtbar gemacht werden, zumal hierfür bereits eine breite theoretische Fundierung (allein die Bezeichnung eines spatial turns zeigt dies anschaulich (Bachmann-Medick, 2006)) und erste Ergebnisse computationeller Textanalysen vorliegen. Das Panel wird durch fünf verschiedene Panel-Teilnehmer:innen und deren Perspektiven zu dem Thema diskutiert, die im Folgenden schlaglichtartig vorgestellt werden. Diese fünf Perspektiven werden unterstützt durch die Moderation von Nils Kellner und die Mitarbeit bei der Konzeption durch Ulrike Henny-Krahmer und Daniel Kababgi. Die Literaturgeschichte befindet sich in einer schwierigen Lage: aus verschiedenen Lagern werden ihr tiefgreifende theoretische Probleme unterstellt. Nach Wellek (1973) sind diese in drei große Stoßrichtungen zu teilen: Der Literaturbegriff sei im Kern autonom und würde damit als der geschichtlichen Kontextualisierung enthobenes Objekt gelesen werden. Zweitens, aus gegensätzlicher Perspektive: die Literaturgeschichte wird von Kulturwissenschaften vereinnahmt und literarische Aspekte ausgeklammert. Drittens wird einer primär an den literarischen Gegenständen orientierten Literaturgeschichte vorgeworfen, sich in einer Geschichte literarischer Formen zu erschöpfen, die andere diskursive Kontexte ausblendet. Darüber hinaus hat Mario Valdés (2002) konstatiert, dass es keine Geschichte geben kann, die annähernd alle Kontexte abbilden kann und damit auf einen zentralen Vorwurf reagiert, dem sich Literaturgeschichte ausgesetzt sieht: Sie sei in ihrem ausgewerteten Material limitiert und in der Auswahl ästhetisch-normativ oder gar ideologisch eingeengt. Die CLS bieten sich der Literaturgeschichte als neues methodisches Verfahren an, das in der Lage ist, größere Textkorpora schneller zu analysieren, und damit sowohl dem Vorwurf der Eingeschränktheit als auch der Kanonzentrierung zu entgehen. Damit antworten die CLS aber vor allem auf die quantitativen Herausforderung der Literaturgeschichte. Verfahren der Literaturgeschichte, wie jede andere Form der Historisierung, bestehen aus der kontrollierten Erhebung von Daten einerseits und der anschließenden Überführungen der Ergebnisse in eine narrativierte Darstellung andererseits. Mindestens genauso entscheidend und herausfordernd wie die kontrollierte Erhebung von Daten sind die synthetisierenden Verfahren der Literaturgeschichte. Wie wird was mit wem verknüpft? Was lässt sich daraus schließen? Erst aus diesen Verknüpfungsleistungen entsteht die geschichtliche Darstellung. Viel Energie wird in den CLS dafür aufgewendet auf Seiten der Datenerhebung und -visualierung zu möglichst kontrollierten und damit 'unter den gegebenen Prämissen 'objektiven Befunden zu kommen. Diese Befunde aber generieren noch nicht ihre Geschichte. Der Übergang zwischen objektiver Datenerhebung und geschichtlicher Darstellung dieser Daten scheint mir in den CLS strukturell analog zu jeder anderen Form der Literaturgeschichte zu sein: es stellen sich immer unmittelbar Fragen, in welche Kontexte denn nun ein spezifischer Befund, etwa zur Semantisierung literarischer Räume in einer bestimmten Zeit, eingebunden werden sollen. Da diese größeren diskursiven Kontexte i.d.R. nicht selbst Gegenstand der Datenerhebung waren, bedarf es narrativer Verknüpfungsverfahren. Damit kehren jene Probelme von Literaturgeschichte wieder, die zur ihrer prinzipiellen Infragestellung in der Disziplin geführt haben (Buschmeier, 2014). Werden die CLS von vielen noch immer als Provokation der klassischen Literaturgeschichte gesehen, die dieser die Limitiertheit der Reichweite ihrer Aussagen in der Begrenzheit ihres ausgewerteten Materials vor Augen führt, so gilt festzuhalten: Die Provokation der CLS durch die Literaturgeschichte besteht, so die These, in der Markierung ihrer methodischer Grenze bzw. ihres blinden Flecks: der Überführung von historischen Daten in Erzählungen von Geschichte. Ein guter Grund also, zusammen zu denken und zu arbeiten und zu schreiben. Das Interesse der Geistes- und Kulturwissenschaften am Raum ist seit Jahrzehnten ungebrochen (Alidou, 2002; Ryan, 2019; Ryan et al., 2016; Caracciolo et al., 2022; Leetsch et al. 2023). Der Raum ist zentral für die Orientierung des Lesers in der erzählten Geschichte und die Bedeutung eines Textes. Jede ausführliche Charakterisierung und Funktionalisierung von Raum und Bewegung erfolgt intentional und ist wichtig für die Raumanalyse. Die Gestaltung des konkreten Raums und die Bewegung der Figuren sind oftmals wesentliche Bedeutungsträger, die sich kultur- und mentalitätsgeschichtlich interpretieren lassen. Für die Computational Literary Studies (CLS) ergeben sich zwei Herausforderungen: Erstens, die Frage der Operationalisierung und Modellierung von literarischen Phänomenen und zweitens, die Einbindung literaturhistorischer und theoretischer Grundlagen. Die Analyse von Raum und Mobilität in fiktionalen Welten dient dabei als konkretes Vehikel, um über Methodik und Praxis der Literaturgeschichtsschreibung ins Gespräch zu kommen. Exemplarisch sollen diese Zusammenhänge an der literaturgeschichtlichen und computationellen Modellierung von zwei Aspekten des erzählten Raumes diskutiert werden: 1) Die Erkennung von Schauplätzen, das heißt Räumen der erzählten Welt, in denen Ereignisse und Wahrnehmungen von Figuren verortet werden (Dennerlein, 2009). 2) Die Charakterisierung von räumlichen Gegebenheiten und Bewegungen mit den Parametern Art, Umfang und Spezifikation. Vorgestellt werden synchrone und diachrone Fragestellungen, die sich untersuchen ließen, wenn man diese Aspekte aus großen Korpora literarischer Texte extrahieren könnte. Die Literaturwissenschaft hat ein zunehmend skeptisches Verhältnis zur Literaturgeschichte entwickelt: Is literary history possible, fragte David Perkins 1992 im Titel seines Buchs und bejahte die Frage. Buschmeier, Erhart und Kauffmann brechen gar über allen Literaturgeschichten der Gegenwart den Stab: ""Die gegenwärtige Praxis der Literaturgeschichtsschreibung ist demnach nicht nur widersprüchlich, sondern auch theoretisch prekär"" (Buschmeier, Erhart und Kaufmann 2014, 3). Diese Position ist, mehr oder weniger deutlich, eng mit einem höchst normativen Begriff von Literatur verbunden, der oftmals einen großen Teil der gelesenen fiktionalen Texte der Vergangenheit ausschließt - zugunsten des Interesses am (großen) Einzelwerk. Dem steht nun gerade die Arbeit der meisten im Bereich der Computational Literary Studies entgegen: Sie interessieren sich für langfristige und großflächige Entwicklungen. In dieser Perspektive verschwindet der Unterschied zwischen Goethe und Kotzebue, zwischen Kafka und Hans Dominik vorübergehend, um später dann durch eine Kontextrelationierung wieder eingeführt werden zu können. Der Literaturgeschichte, so verstanden, kann man Fragen, die lange Zeit als unbeantwortbar zur Seite geschoben wurden, etwa nach dem Verhältnis von Binnendynamik der literarischen Entwicklung (inwieweit wird wird Literaturgeschichte von literaturinternen Dynamiken geprägt) und Kontextwirkungen (inwieweit reagieren literarische Entwicklungen auf ideengeschichtliche oder sozialhistorische Dynamiken?), wieder stellen und an ihrer Beantwortung arbeiten. Durch diese neuen Langzeit-Perspektiven werden nicht zuletzt Anschlüsse an frühere Projekte der Literaturgeschichte möglich. Im DFG-Projekt ""Computational Approaches to Narrative Space in 19th and 20th Century Novels"" (CANSpiN) werden verschiedene Ansätze der computergestützten Analyse von Raum erprobt. Mit der Annotationsrichtlinie CANSpiN.CS1 ist beabsichtigt, die Räumlichkeit des Textes an der Textoberfläche sichtbar zu machen: Raumreferenzielle Ausdrücke, deren Menge, Verteilung, Auswahl und Korrelation bilden das räumliche Vokabular eines Textes (Orte, Bewegungen, Dimensionierungen, Richtungen, Positionierungen). Für deren Erkennung ist weder die Analyse der semantischen Tiefenstrukturen des Textes nötig (wie beispielsweise in der Erzähltextanalyse), noch handelt es sich um das bloße Erfassen von Formativen. Stattdessen werden kotextuelle Zusammenhänge innerhalb von Sätzen berücksichtigt. Dieser Ansatz kommt dem Anspruch nach Operationalisierung entgegen und nutzt dafür vorhandene technische Lösungen (Large Language Models). Das räumliche Vokabular verstehen wir wie auch die narrative Struktur grundsätzlich als erfassbare, historisch bedingte Eigenschaften von Texten und damit als der literaturhistorischen Analyse gleichermaßen für quantitative wie qualitative Methoden zugänglich. Der Einsatz von CANSpiN.CS1 ist einerseits explorativ: Wir erwarten Muster, die zuvor noch nicht beschrieben worden sind. Sie bedürfen der Evaluation und Interpretation unter Bezugnahme auf literaturwissenschaftliche Wissensbestände. Andererseits finden Korpusaufbau und Richtliniendefinition hinsichtlich literaturhistorischer Fragestellungen statt und setzen damit an vorhandenen Forschungsprozessen an: Nehmen Raumdarstellungen in Romanen vom 19. zum 20. Jahrhundert zu (Schumacher, 2023, 207‚Äì217)? Gibt es ähnliche Muster der Räumlichkeit in spanisch- und deutschsprachigen Romanen im Zuge des nation buildings des 19. Jahrhunderts? Beim Spatial Distant Reading werden fiktive und fiktionale Darstellungen von Raum als zentrale Kategorie der Sinngebung untersucht (Lefebvre, 1974): dies betrifft die phänomenologische Konstruktion der fiktionalen Welten - etwa als ‚Äòurban‚Äô (Bologna, 2020) - und auch die sozial-kulturelle Dimension erkennbarer nationaler und transnationaler Räume (Wilkens, 2021). Unser Projekt untersucht die affektiven Topologien deutschschweizerischer Literatur in einem Zeitraum zwischen 1854 und 1930, indem verschiedene Arten der räumlichen Darstellung auf Differenzen wie Kultur/Natur, Stadt/Land (Rehm, 2015) und die Rolle von Interieurs (Herrmann et al., 2022) sowie die Rolle der (alpinen) Berge in der Gestaltung einer spezifisch ‚ÄòSchweizer‚Äô Literatur (Zimmer, 1998) untersucht werden. Eine wichtige Ressource ist eine Liste räumlicher Begriffe (derzeit N=187.421 Entitäten), die räumliche named entities, aber auch non-named entities urbaner, ländlicher und naturbezogener Art enthält (Grisot und Herrmann, 2023). Zudem arbeiten wir an der automatisierten Erkennung räumlicher Entitäten mittels distributioneller Semantik (Herrmann et al., 2022) und maschinellen Lernens (Kababgi et al., eingereicht). Literaturhistorisch fragt das Projekt nach den sozialhistorischen Bedingungen, aber eben auch nach den textuellen Merkmalen einer ""Schweizer"" Literatur auf deutsch. Formiert sich diese über unterschiedliche Gattungen und Subgattungen hinweg als ""Nationalliteratur""? Welche Rolle spielen prestigeträchtig wahrgenommene Autor*innen, wie etwa Jeremias Gotthelf oder Gottfried Keller, aber auch Johanna Spyri? Die Gattung der Dorfgeschichte spielt seit Mitte des 19. Jahrhunderts eine wichtige Rolle, nicht nur im deutschsprachigen Raum (Twellmann, 2019). Hier kommen systematische, sozial- und literaturhistorische Aspekte zusammen, die unsere diachrone Analyse und auch Vorhersagen über die affektive Enkodierung von Landschaft und Raum in den literarischen Texten informieren. Die Panel-Teilnehmer:innen werden nach der Hinführung zur Problemstellung des Panels in 5-minütigen Beiträgen ihre fach- bzw. projektspezifische Sicht auf die aufgeworfenen Fragen darlegen. Ansätze, Erkenntnisse, Probleme und offene Fragen zur Analyse von Raum in der Literatur dienen als konkretes Vehikel dazu, über Methodik und Praxis der Literaturgeschichtsschreibung zwischen close und distant reading, nomothetischer und idiographischer Forschung, quantitativen und qualitativen Ansätzen, den Computational Literary Studies und Ansätzen einer historisch perspektivierenden Literaturwissenschaft ins Gespräch zu kommen. Die √ñffnung der Diskussion vom Raum-Thema ausgehend zu anderen thematischen Feldern der Literaturwissenschaft ist entsprechend beabsichtigt. Im Anschluss an die aufgeworfenen Fragen wird die 30-minütige Diskussion für das Publikum geöffnet. Es wird erwartet, dass das Panel die Sensibilität von CLS-Forscher:innen für einen stärkeren Einbezug bestehender literaturgeschichtlicher Forschung und umgekehrt diejenige der historisierenden Literaturwissenschaft gegenüber computationellen Zugriffen erhöht. So soll das Ziel eines Dialogs zwischen computergestützter und anderer historisierender Forschung verfolgt werden, der die literaturwissenschaftliche Theoriebildung stärkt.",de,aktuell Verhältnis Literaturwissenschaften methodischer Fokus computergestützt Ansätze liegen Computational literary Studies cls zeigen Problem Forschungsdesign vieler erkennbar erstern Fragestellung meist anhand literaturgeschichtlich Kontexte entwickeln stellen letzterer deutlich Operationalisierung literarisch Phänomen Datenmodellierung Implementierung Finetuning Computationeller Verfahren Mittelpunkt obwohl zentral Ziel Cls liegen to explain or to provide General Laws -- literature And ev -- History and culture Bode bleiben Frage literaturhistorisch fundiert Kontextualisierung Raum beigemessen Status Historisierung Literaturwissenschaften generell debattieren Daniel Fulda sehen Literaturgeschichtsschreibung stark Sinn wissenschaftslogisch allenfalls Rand Fach Fulda bewus Historisierung vorteilhaft Modell Beschreibung Komplexität Computationeller Textanalyse Gius zeigen insofern stellen Frage Stellenwert literaturhistorisch Grundlage Projekt cls literaturwissenschaftlich Bereich derzeit einnehmen zukünftig Ziel Panel Verhältnis sowohl allgemein Blick Planung Forschungsdesigns ausgehend Perspektive Literaturwissenschaft erörtern zuletzt scheinen Diachrone Perspektive oftmals naheliegend Publikationsdat Metadat vorliegen Suche Muster historisch Wandel explorativ modellieren thematisch Raum literarisch Text exemplarisch derartig Überlegung fruchtbar zumal hierfür breit theoretisch Fundierung Bezeichnung Spatial Turns zeigen anschaulich ergebniss Computationeller textanalyse vorliegen Panel verschieden innen Perspektive Thema diskutieren folgend schlaglichtartig vorstellen Perspektive unterstützen Moderation Nils kelln Mitarbeit Konzeption Ulrike Daniel Kababgi Literaturgeschichte befinden schwierig Lage verschieden Lager tiefgreifend theoretisch Problem unterstellen Wellek Stoßrichtung teilen Literaturbegriff Kern Autonom geschichtlich Kontextualisierung enthoben Objekt lesen zweitens gegensätzlich Perspektive Literaturgeschichte Kulturwissenschaft vereinnahmen literarisch Aspekt ausklammern drittens primär literarisch gegenständ orientiert Literaturgeschichte vorwerfen Geschichte literarisch Form erschöpfen diskursiv Kontexte ausblenden hinaus Mario Valdés konstatieren Geschichte geben annähernd Kontexte abbilden zentral Vorwurf reagieren literaturgeschicht aussetzen sehen ausgewertet Material limitieren Auswahl ideologisch eingeengen cls bieten Literaturgeschichte neu methodisch Verfahren Lage groß Textkorpora schnell analysieren sowohl Vorwurf Eingeschränktheit Kanonzentrierung entgehen antworn cls quantitativ Herausforderung Literaturgeschichte Verfahren Literaturgeschichte Form Historisierung bestehen kontrolliert Erhebung daten einerseits anschließend überführungen Ergebnis narrativiert Darstellung andererseits mindestens genauso entscheidend herausfordernd kontrolliert Erhebung daten synthetisierend Verfahren Literaturgeschichte verknüpfen lässen schließen Verknüpfungsleistunge entstehen geschichtlich Darstellung Energie Cls aufwenden Seite Datenerhebung möglichst kontrollieren gegeben Prämisse Objektiv befinden Befund generieren Geschichte Übergang objektiv Datenerhebung geschichtlich Darstellung daten scheinen Cls strukturell analog Form Literaturgeschichte stellen unmittelbar fragen Kontexte spezifisch Befund Semantisierung literarisch räume bestimmt einbinden groß diskursiv Kontexte Gegenstand Datenerhebung bedürfen narrativer verknüpfungsverfahren kehren Probelm Literaturgeschichte prinzipiell Infragestellung Disziplin führen buschmeier Cls Provokation klassisch Literaturgeschichte sehen Limitiertheit Reichweite Aussage Begrenzheit ausgewertet Material Auge führen gelten festhalten Provokation cls Literaturgeschichte bestehen These Markierung methodischer Grenze blind Flecks Überführung historisch daten erzählungen Geschichte Grund denken arbeiten schreiben Interesse Kulturwissenschaft Raum Jahrzehnt ungebrochen alidou Ryan ryan et caracciolo et leetsch et Raum zentral Orientierung Leser erzählt Geschichte Bedeutung Text ausführlich Charakterisierung Funktionalisierung Raum Bewegung erfolgen intentional wichtig Raumanalyse Gestaltung konkret Raum Bewegung Figur oftmals wesentlich Bedeutungsträger mentalitätsgeschichtlich interpretieren lassen Computational literary Studies cls ergeben Herausforderung erstens Frage Operationalisierung Modellierung literarisch Phänomen zweitens Einbindung literaturhistorisch theoretisch grundlagen Analyse Raum Mobilität fiktional Welt dienen konkret Vehikel Methodik Praxis Literaturgeschichtsschreibung Gespräch exemplarisch zusammenhänge literaturgeschichtlich computationell Modellierung Aspekt erzählt raumes diskutieren Erkennung Schauplätz räumen erzählt Welt Ereignis Wahrnehmung Figur verorten dennerlein Charakterisierung räumlich Gegebenheit Bewegung Parameter Art Umfang Spezifikation vorstellen Synchrone Diachrone Fragestellung untersuchen lassen Aspekt Korpora literarisch Text extrahieren Literaturwissenschaft zunehmend skeptisch Verhältnis Literaturgeschichte entwickeln -- literary History possible fragen David Perkins Titel Buch bejahen Frage buschmeier Erhart Kauffmann brechen Literaturgeschicht Gegenwart Stab gegenwärtig Praxis Literaturgeschichtsschreibung demnach widersprüchlich theoretisch prekär buschmeier Erhart Kaufmann Position deutlich eng höchst normativ Begriff Literatur verbinden oftmals Gelesen fiktional Text Vergangenheit ausschließen zugunsten Interesse Einzelwerk stehen Arbeit meister Bereich Computational literary Studies entgegen interessieren langfristig großflächig Entwicklung Perspektive verschwinden Unterschied Goethe Kotzebue Kafka Hans Dominik vorübergehend Kontextrelationierung einführen Literaturgeschichte verstehen fragen unbeantwortbar Seite schieben Verhältnis Binnendynamik literarisch Entwicklung inwieweit Literaturgeschicht literaturintern Dynamik prägen kontextwirkung inwieweit reagieren literarisch Entwicklung ideengeschichtlich sozialhistorisch Dynamik stellen Beantwortung arbeiten zuletzt Anschlüsse früh Projekt Literaturgeschichte computational approach to narrativ Space and Century Novels Canspin verschieden Ansatz computergestützt Analyse Raum erproben Annotationsrichtlinie beabsichtigen Räumlichkeit Text Textoberfläche sichtbar raumreferenziell Ausdrück Menge Verteilung Auswahl Korrelation bilden räumlich Vokabular Text ort Bewegung Dimensionierung richtung positionierungen Erkennung weder Analyse semantisch Tiefenstruktur Text nötig beispielsweise Erzähltextanalyse handeln bloß erfassen Formative stattdessen kotextuell zusammenhänge innerhalb satz berücksichtigen Ansatz Anspruch Operationalisierung entgegen nutzen vorhanden technisch Lösung large language models räumlich Vokabular verstehen narrativ Struktur grundsätzlich erfassbar historisch bedingt eigenschaften Text literaturhistorisch Analyse gleichermaßen quantitativ qualitativ Methode zugänglich Einsatz einerseits explorativ erwarten Muster zuvor beschreiben bedürfen Evaluation Interpretation Bezugnahme literaturwissenschaftlich Wissensbeständ andererseits finden Korpusaufbau Richtliniendefinition hinsichtlich literaturhistorisch Fragestellung setzen vorhanden Forschungsprozesse nehmen Raumdarstellunge Roman Jahrhundert Schumacher ähnlich Muster Räumlichkeit Deutschsprachig Roman Zug Nation Building Jahrhundert Spatial distant Reading fiktiv fiktional Darstellung Raum zentral Kategorie Sinngebung untersuchen Lefebvre betreffen phänomenologisch Konstruktion fiktional Welt äòurban äô bologna Dimension erkennbar national transnational räume Wilkens Projekt untersuchen affektiv Topologien deutschschweizerisch Literatur Zeitraum verschieden Art räumlich Darstellung Differenz Kultur Natur Stadt Land rehm Rolle Interieurs Herrmann et Rolle Alpin Berg Gestaltung spezifisch äòschweizer Äô Literatur zimmer untersuchen wichtig Ressource Liste räumlich begriffe derzeit entitäen räumlich Named entities entities urban ländlich naturbezogener Art enthalten grisot Herrmann zudem arbeiten automatisiert Erkennung räumlich Entität mittels distributionell Semantik Herrmann et maschinell Lernen Kababgi et einreichen literaturhistorisch fragen Projekt sozialhistorisch Bedingung textuell Merkmale Schweizer Literatur deutsch formieren unterschiedlich gattung Subgattung hinweg Nationalliteratur Rolle spielen prestigeträchtig wahrgenommen jeremia gotthelf Gottfried Keller johanna Spyri Gattung dorfgeschicht spielen Mitte Jahrhundert wichtig Rolle deutschsprachig Raum Twellmann systematisch literaturhistorisch Aspekt Diachrone Analyse Vorhersag affektiv Enkodierung Landschaft Raum literarisch Text informieren innen Hinführung Problemstellung Panel beiträgen projektspezifisch Sicht aufgeworfen Frage darlegen Ansatz erkenntnisse Problem offen Frage Analyse Raum Literatur dienen konkret Vehikel Methodik Praxis Literaturgeschichtsschreibung Close distant reading nomothetisch idiographisch Forschung Quantitativen qualitativ Ansatz Computational literary Studies Ansatz historisch Perspektivierend Literaturwissenschaft Gespräch Diskussion ausgehend thematisch feldern Literaturwissenschaft entsprechend beabsichtigen Anschluss aufgeworfen Frage Diskussion Publikum öffnen erwarten Panel Sensibilität innen stark Einbezug bestehend literaturgeschichtlich Forschung umgekehrt historisierend Literaturwissenschaft computationell Zugriff erhöhen Ziel Dialog Computergestützter anderer historisierend Forschung verfolgen literaturwissenschaftlich Theoriebildung stärken,"[('literaturgeschichte', 0.3345545339563504), ('cls', 0.21768863896511625), ('raum', 0.19545789694623364), ('räumlich', 0.16063635212635938), ('literaturhistorisch', 0.14769980851225176), ('kontexte', 0.12219010031651799), ('literaturgeschichtsschreibung', 0.10935601927826798), ('literarisch', 0.1066949169200437), ('bewegung', 0.10282101971673313), ('historisierung', 0.10073411098357979)]"
2025,DHd2025,PICHLER_Axel_Die_deutschsprachige_Kurzgeschichte_nach_1945__.xml,Die deutschsprachige Kurzgeschichte nach 1945. Skizze einer hypothesen-geleiteten Operationalisierung.,"Axel Pichler (Universität Stuttgart, Deutschland)","Operationalisierung, theoriegeleitet, digitale Literaturwissenschaft","Inhaltsanalyse, Modellierung, Annotieren, Text","In der Datenanalyse lassen sich 'stark vereinfacht 'zwei Forschungsansätze unterscheiden: Ein theorie- bzw. hypothesengeleiteter Ansatz, der ausgehend von domänenspezifischen Theorien und deren Grundbegriffen diese auf das Datenmaterial ""projiziert"" und anschließend quantifiziert, und ein datengetriebener Ansatz, der Theorie durch ""massive amounts of data and applied mathematics"" (Anderson 2008) ersetzt und solcherart unbekannte Muster in den Daten freizulegen versucht. Diese beiden Ansätze kennzeichnen auch das Forschungs-Design der digitalen Literaturwissenschaft bzw. Computational Literary Studies, wobei in diesem Zweig der DH 'so mein Eindruck 'aktuell der datengetriebene Ansatz dominiert (vgl. Jannidis 2022). Diesen beiden Ansätzen entsprechen unterschiedliche Leitvorstellungen und Praktiken, wie traditionelle literaturwissenschaftliche Begriffe mit digitalen Mess- und Analysemethoden zusammengeführt werden. Diesen Prozess 'also die Überführung eines fachspezifischen theoretischen Begriffs in eine Messvorschrift 'bezeichnet man gemeinhin als Operationalisierung. Im Folgenden möchte ich anhand eines konkreten Beispiels aus der literaturwissenschaftlichen Gattungsgeschichte bzw. -theorie 'der deutschsprachigen Kurzgeschichte nach 1945 'zeigen, wie auf Basis eines bestimmten Operationalisierungsverständnisses ein derartiger Gattungsbegriff ""hypothesengeleitet"" für seine quantitative Analyse aufbereitet werden kann. ""Hypothesengeleitet"" bezeichnet dabei 'wie der Name nahelegt –, dass von gattungs- bzw. literaturgeschichtlichen (Hypo-)Thesen ausgegangen wird. Dabei verfolge ich zwei Ziele: Erstens möchte ich vorführen, wie ein komplexes literaturwissenschaftliches Konzept auf eine Art und Weise operationalisiert werden kann, welche die Rückführung der auf seiner Basis erzielten Messresultate in die nicht-digitale Literaturwissenschaft erlaubt. Voraussetzung dafür ist, so meine Vermutung, dass die einzelnen Teilschritte eines solchen Prozesses auf eine Art und Weise verbalisiert werden, sodass diese Verbalisierungen nicht nur in den domänenspezifischen Diskurs re-importiert werden können, sondern den Teilnehmenden an diesem Diskurs auch als plausibel erscheinen. Zweitens soll im Zuge dessen der Arbeitsablauf für jenes Verständnis von Operationalisierung, an dessen Explikation Nils Reiter, Benjamin Krautter und ich in den letzten Jahren in unterschiedlichen Konstellationen gearbeitet haben (Pichler/Reiter 2022; Den beiden Zielen entsprechend werde ich im Folgenden immer wieder die konkrete Operationalisierungspraxis unterbrechen, um die einzelnen Arbeitsschritte explizit zu machen und methodologisch zu reflektieren. Beginnen möchte ich diesbezüglich mit einer Rechtfertigung der Fallstudien-Auswahl: der deutschsprachigen Kurzgeschichte nach 1945. Diese eignet sich für die Realisierung der oben genannten Ziele unter anderem aus folgenden Gründen: Erstens ist die Kurzgeschichte gut erforscht (Marx 2005, Meyer 2014, Wenzel 2007). Die einschlägigen Monographien, Lehrbücher und Lexikaeinträge bieten einen guten Ausgangspunkt für die Operationalisierung ihrer zentralen Merkmale. Zweitens handelt es sich bei Kurzgeschichte um, wie der Name schon nahelegt, kurze Texte. Dies erleichtert deren Vollannotation. Drittens stellen 'dank des Aufkommens großer Sprachmodelle (LLMs) 'weder die Kürze der Texte noch etwaige Korpusgrößen mittlerweile ein Problem für die computergestützte Textanalyse dar, da es im Rahmen des sogenannten Während eine dem Stand der Forschung angemessene Analyse eines literaturwissenschaftlichen Begriffs sämtliche bzw. möglichst viele seiner Verwendungsweisen begriffsanalytisch erfassen sollte (Schröter et. al 2021), beschränke ich mich hier für Demonstrationszwecke auf die jüngere synoptische Forschung zur deutschsprachigen Kurzgeschichte 'd.s. die Arbeiten von Leonie Marx und Anne-Rose Meyer sowie den Eintrag im Reallexikon von Peter Wenzel –, um aus dieser die zentralen Kennzeichen der historischen Ausprägung der Gattung zu sammeln, ohne dabei deren Verhältnis eingangs definitorisch zu bestimmen. Fasst man diese zusammen, so lassen sich folgende gattungstypologische Merkmale festhalten: Die deutschsprachige Kurzgeschichte nach 1945 sei gekennzeichnet durch narrative Strategien der Reduktion, Selektion, Verdichtung und Begrenzung im Hinblick auf Handlung, das Figurenarsenal, Zeit und Ort, einen andeutenden oder verrätselnden Titel, einen unmittelbaren Einstieg in die Handlung sowie einen (mehrheitlich) offenen Schluss. Eine derartige Gattungsbeschreibung ist nicht als Definition im strengen Sinne zu verstehen. Sie vereint bloß jene Prädikate, welche die genannten Forschungsbeiträge als charakteristisch für die Kurzgeschichte nach 1945 erachten. Sie kann, zumindest im Rahmen eines Top-Down-Ansatzes, nicht am Stück operationalisiert werden, sondern nur über die Operationalisierung ihrer einzelnen Prädikate bzw. Merkmale. Wie sich diese Merkmale zum Gesamt-Begriff verhalten, ist in der jüngeren gattungstheoretischen Forschung umstritten. In ihr finden sich nicht nur unterschiedliche Begriffe des Begriffs (Zymner 2003), sondern auch Kritiken am Einsatz von vordefinierten, klassifikatorischen Begriffen in der Gattungsgeschichte (siehe z.B. Schröter 2024). Insofern liegt es nahe, das Verhältnis der einzelnen Textmerkmale zum Gesamt-Begriff fürs Erste offen zu halten. An die Stelle einer feststellenden bzw. festlegenden Definition tritt dann in einem ersten Schritt die am Stand der Forschung oder der historisch gegebenen poetologischen Reflexion orientierte Sammlung bzw. Auswahl der begriffsrelevanten Merkmale. Inwiefern diese eine klassifikatorische oder bloß graduelle Funktion im jeweiligen Gattungskonzept innehaben, ist dann in einem nächsten Schritt empirisch zu überprüfen. Ich werde mich im Folgenden ausschließlich auf den ersten Schritt dieses Prozesses, die Teiloperationalisierung, konzentrieren. Sie wird exemplarisch anhand eines der Merkmale vorgeführt, der Charakterisierung von Kurzgeschichten durch das Verfahren der Selektion, was 'so Anne-Rose Meyer (Meyer 2014, S. 20f.) 'bei der Gestaltung der Textanfänge von Kurzgeschichten dazu führe, dass diese durch Auslassungen gekennzeichnet seien. Idealiter handelt es sich bei dem Merkmal, das operationalisiert werden soll, bereits um ein Phänomen, für das ein etablierter Begriff vorliegt. In einem solche Fall ist in einem ersten Schritt die Bedeutung und Verwendung des Begriffes in der Forschung zu rekonstruieren und zu analysieren, um so zu einer feststellenden Definition des Begriffs zu gelangen. In sehr wenigen Fällen wird eine solche feststellende Definition ausreichend beobachtungssprachliche Hinweise enthalten, sodass sie ohne großen Aufwand in eine operationale Definition überführt werden kann. In vielen Fällen ist dies jedoch nicht der Fall 'zum Bespiel bei der Kennzeichnung des Erzähleinstieges der Kurzgeschichte qua Auslassung. Diese Kennzeichnung verweist weder auf einen etablierten literaturwissenschaftlichen oder linguistischen Begriff noch auf ein direkt auszeichenbares Textoberflächenphänomen. Um zu einem solchen zu gelangen, bedarf es der präzisierenden Begriffsarbeit. In der aktuellen Methodendiskussion der digitalen Literaturwissenschaft wird dabei oft für jenes Verfahren plädiert (Pichler/Reiter 2021, Gerstorfer/Gius 2023, Jacke 2023), das bereits Harald Fricke zum Leit-Verfahren bei der Arbeit am Zur Explikation der ""Auslassung"" bietet es sich an, einen Umweg über die Literaturtheorie zu gehen. Diese spricht bezüglich des notwendigen Auslassens von Informationen über die Ontologie erzählter Welten in Anknüpfung an Roman Ingarden auch von Unbestimmtheitsstellen. Fotis Jannidis hat Unbestimmtheit 'abseits des Kontextes der Ingard""schen Literaturtheorie 'als ""Mangel an Information in einer Öußerung"" (Jannidis 2003, S. 308) definiert. Nun ist aber 'und das ist nicht unbedeutend 'nicht jeder Informationsmangel in einer fiktionalen Erzählung handlungs- oder interpretationsrelevant. Bei der hier zu operationalisierenden ""Auslassung"" handelt es sich also um keinen rein deskriptiven Begriff. Greift man auf weitere der zuvor genannten Charakteristika der Kurzgeschichte zurück, kann die Unbestimmtheit ihrer Texteröffnungen folgendermaßen bestimmt werden: Die Texteröffnung einer Kurzgeschichte ist unbestimmt, wenn sie durch einen Mangel an handlungsrelevanter Information in Bezug auf eine zentrale Figur, den Gegenstand, den Ort oder die Zeit der Handlung gekennzeichnet ist. Wie lässt sich eine derartige Bestimmung in eine operationale Definition überführen? Eine solche hat jene Indikatoren an der Textoberfläche zu nennen, die eine Beleg-Funktion für das Vorliegen des besagten Sachverhaltes besitzen. Ein solcher Indikator für Bestimmtheit bzw. Unbestimmtheit ist im Deutschen die Definitheit von Nominalphrasen. Hadumond Bußmann definiert sie im Für die Operationalisierung von ""Auslassungen"" am Anfang von Kurzgeschichten folgt aus dieser Bestimmung, dass von einer ""Auslassung"" gesprochen werden kann, wenn eine zentrale Figur, der Gegenstand, der Ort oder die Zeit der Handlung im Text durch eine definite Nominalphrase eingeführt werden, deren Referenz und Eigenschaften zuvor im Text noch nicht bestimmt wurden. Um diese Bestimmung von ""Auslassung"" in eine finale operationale Definition und im Anschluss daran in eine Messvorschrift zu überführen, ist es notwendig die Bestimmtheitsgrade von zentraler Figur, Gegenstand, Ort und Zeit der Handlung in Eingangssätzen weiter auszudifferenzieren und dabei so zu gewichten, dass denjenigen Eingangssätzen von Kurzgeschichten, die aufgrund der hohen Frequenz von definiten Nominalphrasen einen hohen Grad an Unbestimmtheit besitzen, ein niedriger Gesamtwert zugeschrieben wird. Neben definiten Nominalphrasen können auch noch indefinite Nominalphrasen sowie Pronomen und Eigennamen auf Figur, Gegenstand, Ort und Zeit der Handlung in einer Kurzgeschichte verweisen bzw. diese einführen, wobei indefinite NPs einen geringeren, Eigennamen und Pronomen im ersten Satz eines Textes einen höheren Grad an Unbestimmtheit als definite NPs aufweisen. Insofern kann man den drei genannten Kategorien unterschiedliche Werte zuschreiben, mit dem Ziel höhere Werte zu erhalten, wenn ein Verweis grammatikalisch einen geringeren Grad an Unbestimmtheit besitzt. Dementsprechend kann indefiniten Nominalphrasen der höchste, Eigenamen und Personalpronomen der niedrigste Wert in Betreff ihres Grades an Bestimmtheit zugschrieben werden, was zu folgender Messvorschrift führt: Wenn im ersten Satz einer Kurzgeschichte ein Verweis auf Person, Gegenstand, Zeit oder Ort der Handlung erfolgt, gibt es einen Punkt, wenn dies mittels Personalpronomen oder Eigennamen geschieht, 2 Punkte, wenn dies mittels definiter Nominalphrase geschieht, und 3 Punkte, wenn es über eine indefinite Nominalphrase erfolgt. Dies führt dazu, dass je geringer der Grad an Unbestimmtheit der Referenzen in einem Satz ist, desto höher sein Gesamtscore wird und vice versa. In Anbetracht der vier Kategorien Person, Gegenstand, Ort und Zeit ergibt sich so ein möglicher Maximalscore von 12 Punkten. Damit ist der eigentliche Operationalisierungsprozess abgeschlossen. Er sei hier noch einmal in seinen zentralen Schritten zusammengefasst: An seinem Anfang stand die Rekonstruktion fachspezifischer historischer Gattungsbeschreibungen. Im gegebenen Fall erfolgte diese über die Zusammenführung von drei einschlägigen Beschreibungen aus der Forschung. Im Anschluss daran habe ich mich zu Demonstrationszwecken auf die Operationalisierung eines der Merkmale dieser Deskriptionen konzentriert: die Auslassungen am Textanfang von Kurzgeschichten. Dabei habe ich in einem ersten Schritt den Begriff der ""Auslassung"" analysiert, um festzustellen, dass für ihn keine beobachtungssprachlich relevante fachspezifische Bestimmung vorliegt. Dementsprechend wurde in einem nächsten Schritt eine Explikation des Begriffes erarbeitet, im Zuge derer auf literaturtheoretische Reflexionen zurückgegriffen wurde. Diese Explikation wurde dann in einem dritten Schritt mithilfe linguistischer Indikatoren in eine operationale Definition überführt, auf deren Basis abschließend eine Messvorschrift erstellt wurde. Hier ist es wichtig darauf hinzuweisen, dass die Realisierungen dieser Messvorschrift genau dann valide sind, wenn sie den Vorgaben der operationalen Definition 'nicht jedoch denjenigen des Ursprungbegriffes 'angemessen sind. Um einen ersten Eindruck zu bekommen, wie gut die auf ihr basierende Messvorschrift von großen Sprachmodellen realisiert werden kann, habe ich die Eingangssätze von 50 Kurzgeschichten aus der Zeit nach 1945 mit einem Fokus auf Texte von Ilse Aichinger, Heinrich Böll und Wolfgang Borchert manuell annotiert und daraufhin die besagten Sätze auf Basis der entwickelten Metrik nach ihrem Grad an Unbestimmtheit mit zwei proprietären Modellen (GPT-4o gpt-4-turbo (ohne Socring)  (ohne Socring) gpt-4-turbo (mit Scoring)  (mit Scoring) gpt-4-turbo (mit Beispiel)  (mit Beispiel) Die dabei erzielten Resultate bewegen sich im gängigen Rahmen von Klassifikationstasks in den Computational Literary Studies (vgl. Bamman/Kent/Li/Zhou 2024). Die höchste Vorhersage-Genauigkeit mit einem F1-Score von 75% erzielt Anthropics claude-sonnet-3.5-Modell mit einem Prompt mit den Annotationsrichtlinien und einem handverlesenen Beispiel. Es liegt damit ganze 25% über einer Majority Baseline also einem Verfahren, das schlichtweg immer jene Kategorie vorhersagt, die im Datensatz am häufigsten vertreten ist, und 9% über GPT-4o. In zukünftigen Experimenten sollte geklärt werden, ob diese Ergebnisse auch bei einer größeren Stichprobe und anderen Beispieltexten generalisieren und ob ein Fine-Tuning der Modelle oder Prompts, z.B. durch verschiedene Prompt-Tuning-Techniken, zu einem statistisch signifikanten Effekt in der Vorhersage führt. Jedoch kann bereits jetzt konstatiert werden: Das hier gegebene Beispiel für die Operationalisierung einer literaturgeschichtlichen Gattungsbestimmung zeigt, dass eine vom Stand der Forschung ausgehende Rückführung einzelner Teilmerkmale einer literaturhistorischen Gattungsbeschreibung auf sprachliche Indikatoren insbesondere dann möglich ist, wenn diese Rückführung sich nicht allzu eng an der ursprünglichen Bestimmung dieser Merkmale hält, sondern sie mithilfe des Verfahrens der Explikation sowohl den Anforderungen der computationellen Erkennung oder Weiterverarbeitung als auch dem literaturwissenschaftlichen Forschungskontext entsprechend anpasst. Ein solcherart adaptierter Begriff eines gattungsspezifischen Textmerkmales deckt selbstverständlich nicht mehr sämtliche semantischen Dimensionen seiner fachspezifischen Verwendung ab. Gegenüber dieser Verwendung hat er jedoch den Vorteil, dass er a.) seine Einschränkung explizit macht, b.) derartig weder mehrdeutig und vage ist und sich so c.) eindeutig auf Textoberflächenphänomene zurückführen lässt. Zudem besitzt ein derartiger Ansatz, der sich primär auf die Operationalisierung von einzelnen Merkmalen einer Beschreibung oder eines Begriffes fokussiert, den Vorteil, dass er sich nicht vorweg festlegen muss, welchem Begriff des Begriffes er folgt: Eine Operationalisierung der Prädikate, welche auf jene Gattungsmerkmale referieren, die laut Forschung diese Gattung zu einem bestimmten historischen Zeitpunkt charakterisieren, lässt fürs Erste offen, in welchem Verhältnis diese Merkmale zueinander stehen. Dieses Verhältnis kann 'und soll 'erst auf Basis der Operationalisierung sämtlicher Teilmerkmale empirisch bestimmt werden. Mithilfe eines solchen Verfahrens sollte es möglich werden, sowohl eine bestimmte historische Ausprägung einer Gattung näher zu bestimmen als auch auf Basis der selben theorie-geleitete Hypothesentests in den CLS durchzuführen, wie sie in den empirischen Sozialwissenschaften bereits üblich sind. Ein solches Verfahren hat zudem das Potential, die Kommunikation zwischen digitalen und nicht-digitalen Literaturwissenschaften zu vereinfachen, da es von einem geteilten Begriffsapparat ausgeht und diesen unter Rückgriff auf traditionelle geisteswissenschaftliche Praktiken 'die Begriffsarbeit in Form von Begriffsanalyse und Begriffsexplikation 'adaptiert. Längerfristig sollte mithilfe des hier vorgestellten Verfahrens 'das ich als ergänzendes Korrektiv der datengetriebenen Exploration verstehe 'möglich werden, existierende literaturhistorische Hypothesen zu überprüfen und teilweise zu revidieren.",de,Datenanalyse lassen stark vereinfachen Forschungsansätz unterscheiden Hypothesengeleiteter Ansatz ausgehend domänenspezifisch Theorie grundbegriffen datenmaterial projizieren anschließend quantifizieren datengetrieben Ansatz Theorie massiv amount of data and Applied Mathematics Anderson ersetzen Solcherart unbekannt Muster daten freilegen versuchen Ansatz Kennzeichn digital Literaturwissenschaft Computational literary Studie wobei Zweig dh Eindruck aktuell datengetrieben Ansatz dominieren Jannidis Ansätz entsprechen unterschiedlich leitvorstellung praktiken traditionell literaturwissenschaftlich begriffe digital analysemethoden zusammenführen Prozess Überführung fachspezifisch theoretisch Begriff Messvorschrift bezeichnen gemeinhin Operationalisierung folgend anhand konkret Beispiel literaturwissenschaftlich Gattungsgeschichte deutschsprachig Kurzgeschicht zeigen Basis bestimmt operationalisierungsverständnisses derartig Gattungsbegriff hypothesengeleiten quantitativ Analyse aufbereiten hypothesengeleiten bezeichnen Name nahelegen literaturgeschichtlich ausgehen verfolgen Ziel erstens vorführen komplex literaturwissenschaftlich Konzept Art Weise operationalisieren Rückführung Basis erzielt Messresultat Literaturwissenschaft erlauben Voraussetzung Vermutung einzeln Teilschritt prozesses Art Weise verbalisiern sodass Verbalisierung domänenspezifisch Diskurs Teilnehmende Diskurs plausibel erscheinen zweitens Zug Arbeitsablauf Verständnis Operationalisierung Explikation nils Reiter Benjamin Krautter letzter unterschiedlich Konstellatione arbeiten Pichler Reiter Ziel entsprechend folgend konkret Operationalisierungspraxis unterbrechen einzeln Arbeitsschritte explizit methodologisch reflektieren beginnen diesbezüglich Rechtfertigung deutschsprachig Kurzgeschicht eignen Realisierung genannt Ziel folgend Grund erstens Kurzgeschicht erforschen Marx Meyer wenzel einschlägig Monographi lehrbüch lexikaeinträge bieten gut Ausgangspunkt Operationalisierung zentral Merkmal zweitens handeln Kurzgeschicht Name nahelegen kurz Text erleichtern Vollannotation drittens stellen Aufkommen Sprachmodelle llms weder Kürze Text etwaig korpusgrößen mittlerweile Problem computergestützt Textanalyse dar Rahmen sogenannter Stand Forschung angemessen Analyse literaturwissenschaftlich Begriff sämtlicher möglichst verwendungsweis begriffsanalytisch erfassen schröter et -- beschränken Demonstrationszwecke jung synoptisch Forschung deutschsprachig kurzgeschichen arbeiten Leonie Marx Meyer Eintrag Reallexikon Peter Wenzel zentral Kennzeichen historisch Ausprägung Gattung sammeln Verhältnis eingangs definitorisch bestimmen fasst lassen folgend gattungstypologisch merkmal festhalten deutschsprachig Kurzgeschicht kennzeichnen narrativ Strategie Reduktion Selektion Verdichtung Begrenzung Hinblick Handlung figurenarsenal Ort andeutend verrätselnd Titel unmittelbar Einstieg handlung mehrheitlich offen Schluss derartig Gattungsbeschreibung Definition streng Sinn verstehen vereinen bloß Prädikat genannt forschungsbeitrag charakteristisch Kurzgeschicht erachten zumindest Rahmen Stück operationalisiern Operationalisierung einzeln Prädikat Merkmal Merkmal verhalten jung gattungstheoretisch Forschung umstritten finden unterschiedlich Begriff Begriff zymner kritiken Einsatz vordefiniert klassifikatorisch begriffen Gattungsgeschichte sehen Schröter insofern liegen nahe Verhältnis einzeln Textmerkmal für halten Stelle feststellender festlegend Definition treten Schritt Stand Forschung historisch gegeben poetologisch Reflexion orientiert Sammlung Auswahl begriffsrelevant Merkmal inwiefern klassifikatorisch bloß graduell Funktion jeweilig Gattungskonzept innehaben nächster Schritt empirisch überprüfen folgend ausschließlich Schritt prozesses Teiloperationalisierung konzentrieren exemplarisch anhand Merkmal vorführen Charakterisierung Kurzgeschichte Verfahren Selektion Meyer Meyer Gestaltung Textanfäng Kurzgeschicht fahren Auslassung kennzeichnen idealit handeln Merkmal operationalisieren Phänomen etabliert Begriff vorliegen Fall Schritt Bedeutung Verwendung Begriff Forschung rekonstruieren analysieren feststellend Definition Begriff gelangen weniger Fall feststellend Definition ausreichend beobachtungssprachlich Hinweis enthalten sodass Aufwand operational Definition überführen Fall Fall bespiel Kennzeichnung erzähleinstieges Kurzgeschichte qua Auslassung Kennzeichnung verweisen weder etabliert literaturwissenschaftlich linguistisch Begriff direkt auszeichenbares Textoberflächenphänomen gelangen bedürfen präzisierend Begriffsarbeit aktuell Methodendiskussion digital Literaturwissenschaft Verfahren plädieren Pichler Reiter Gerstorfer Gius jack Harald Fricke Arbeit Explikation Auslassung bieten umweg Literaturtheorie sprechen bezüglich notwendig Auslassen Information Ontologie erzählt Welt Anknüpfung Roman ingarden unbestimmtheitsstellen Foti Jannidis Unbestimmtheit abseits Kontext ingard sch Literaturtheorie Mangel Information Öußerung Jannidis definieren unbedeutend Informationsmangel fiktional Erzählung interpretationsrelevant operationalisierend Auslassung handeln rein deskriptiv Begriff greifen zuvor genannt Charakteristika Kurzgeschichte Unbestimmtheit texteröffnung Folgendermaße bestimmen Texteröffnung Kurzgeschicht unbestimmt Mangel Handlungsrelevanter Information Bezug zentral Figur Gegenstand Ort Handlung kennzeichnen lässen derartig Bestimmung operational Definition überführen indikatoren Textoberfläche nennen vorliegen besagter Sachverhalt besitzen Indikator Bestimmtheit Unbestimmtheit deutschen Definitheit Nominalphrase Hadumond Bußmann definieren Operationalisierung auslassung Anfang Kurzgeschicht folgen Bestimmung Auslassung sprechen zentral Figur Gegenstand Ort Handlung Text definit nominalphrase einführen Referenz eigenschaften zuvor Text bestimmen Bestimmung Auslassung final operational Definition Anschluss Messvorschrift überführen notwendig Bestimmtheitsgrade zentral Figur Gegenstand Ort Handlung Eingangssätze auszudifferenzieren gewichten derjenige eingangssätzen Kurzgeschichte aufgrund hoch Frequenz Definit nominalphrasen hoch Grad Unbestimmtheit besitzen niedrig Gesamtwert zuschreiben Definit nominalphrasen indefinite nominalphrasen Pronome eigennamen Figur Gegenstand Ort Handlung Kurzgeschichte verweisen einführen wobei indefinit nps gering eigennamen Pronome Satz Text hoch Grad Unbestimmtheit Definite nps aufweisen insofern genannt kategorien unterschiedlich wert zuschreiben Ziel hoch Wert erhalten Verweis grammatikalisch gering Grad Unbestimmtheit besitzen indefinit nominalphrasen hoch eigenamen Personalpronome niedrig Wert Betreff Grad Bestimmtheit zugschrieben folgend Messvorschrift führen Satz Kurzgeschichte Verweis Person Gegenstand Ort Handlung erfolgen Punkt mittels Personalpronome eigennamen geschehen Punkt mittels definit nominalphrase geschehen Punkt indefinit nominalphrase erfolgen führen gering Grad Unbestimmtheit referenzen Satz desto hoch Gesamtscore Vice versa Anbetracht Kategorie Person gegenstand Ort ergeben möglich Maximalscore punken eigentlich operationalisierungsprozess abschließen zentral Schritt zusammengefasst Anfang stehen Rekonstruktion Fachspezifischer Historischer Gattungsbeschreibunge gegeben Fall erfolgen Zusammenführung einschlägig Beschreibung Forschung Anschluss demonstrationszwecken Operationalisierung Merkmal Deskription konzentrieren Auslassung Textanfang Kurzgeschichte Schritt Begriff Auslassung analysieren feststellen beobachtungssprachlich relevant fachspezifisch Bestimmung vorliegen nächster Schritt Explikation Begriff erarbeiten Zug Derer literaturtheoretisch Reflexion zurückgegriffen Explikation Schritt Mithilfe linguistisch indikatoren operational Definition überführen Basis abschließend Messvorschrift erstellen wichtig hinzuweisen Realisierung Messvorschrift genau valide Vorgabe operational Definition derjenige ursprungbegriffes angemessen Eindruck bekommen basierend Messvorschrift Sprachmodelle realisieren Eingangssätze Kurzgeschichte Fokus Text Ilse Aichinger heinrich Böll Wolfgang Borchert manuell annotiert daraufhin besagt Sätz Basis entwickelt Metrik Grad Unbestimmtheit proprietär Modell socring socring scoring scoring erzielt Resultat bewegen gängig Rahmen Klassifikationstasks Computational literary Studies bamman Kent li Zhou hoch erzielen anthropics prompt Annotationsrichtlinien handverlesen liegen Majority baselinen Verfahren schlichtweg Kategorie vorhersagen Datensatz häufig vertreten zukünftig experimenten klären Ergebnis groß Stichprobe beispieltext generalisieren Modell Prompt verschieden statistisch signifikant Effekt Vorhersage führen konstatieren gegeben Operationalisierung literaturgeschichtlich Gattungsbestimmung zeigen Stand Forschung ausgehend Rückführung einzeln Teilmerkmale literaturhistorisch Gattungsbeschreibung sprachlich indikatoren insbesondere Rückführung allzu eng ursprünglich Bestimmung Merkmal halten Mithilfe verfahren Explikation sowohl Anforderung computationell Erkennung Weiterverarbeitung literaturwissenschaftlich Forschungskontext entsprechend anpasst Solcherart adaptiert Begriff gattungsspezifisch textmerkmale decken selbstverständlich sämtlicher semantisch Dimension fachspezifisch Verwendung Verwendung Vorteil Einschränkung explizit derartig weder mehrdeutig vage eindeutig textoberflächenphänomen zurückführen lässt zudem besitzen derartig Ansatz primär Operationalisierung einzeln Merkmale Beschreibung Begriff fokussieren Vorteil vorweg festlegen Begriff Begriff folgen Operationalisierung Prädikat gattungsmerkmal referieren laut Forschung Gattung bestimmt historisch Zeitpunkt Charakterisieren lässen für Verhältnis merkmal Zueinander stehen Verhältnis Basis Operationalisierung sämtlich Teilmerkmale empirisch bestimmen Mithilfe verfahren sowohl bestimmt historisch Ausprägung Gattung nah bestimmen Basis selber Hypothesentest Cls durchführen empirisch Sozialwissenschaft üblich Verfahren zudem Potential Kommunikation digital Literaturwissenschafte vereinfachen geteilt Begriffsapparat ausgehen Rückgriff traditionell geisteswissenschaftlich praktiken Begriffsarbeit Form Begriffsanalyse Begriffsexplikation adaptieren längerfristig Mithilfe vorgestellt verfahren ergänzend Korrektiv datengetrieben Exploration verstehen existierend literaturhistorisch Hypothese überprüfen teilweise revidieren,"[('auslassung', 0.25165753722187384), ('kurzgeschicht', 0.25165753722187384), ('begriff', 0.23222097764585603), ('unbestimmtheit', 0.20835548207621687), ('kurzgeschichte', 0.20835548207621687), ('messvorschrift', 0.16777169148124924), ('operationalisierung', 0.15315445701867228), ('merkmal', 0.1500005087153914), ('definition', 0.14248680920343265), ('operational', 0.13022217629763552)]"
2025,DHd2025,BOGDANOVIC_Arsenije_Layout_und__Para__Text__Erprobung_hybrid.xml,Layout und (Para-)Text: Erprobung hybrider Ansätze und Heuristiken zur Erforschung von Werkausgaben des 18. Jahrhunderts,"Arsenije Bogdanoviƒá (Universität Stuttgart, Deutschland); Liesen-Sophie Lange (Johannes Gutenberg-Universität Mainz, Deutschland); Philip Ajouri (Johannes Gutenberg-Universität Mainz, Deutschland); Gabriel Viehhauser (Universität Wien, Österreich)","Taxonomien, Document Layout Analysis, OCR, Scalable Reading, Buchwissenschaft","Strukturanalyse, Annotieren, Bibliographie, Metadaten, Methoden, Forschungsprozess","In der digitalen Literaturwissenschaft werden meist weitumspannende Konzepte wie Genres oder Epochen in den Blick genommen. Die dort entwickelten Methoden können jedoch auch durchaus gewinnbringend zur Untersuchung von buchgeschichtlichen Fragestellungen zum Einsatz gebracht werden. Das in Kooperation von Buchwissenschaft und Digital Humanities durchgeführte DFG-Projekt ""Scalable Reading von ""Gesammelten Werken"" des 18. Jahrhunderts, exemplarisch durchgeführt an Friedrich-von-Hagedorn-Werkausgaben"" widmet sich dementsprechend der Erforschung des Buchtypus der Gesamtausgabe, der sich im 18. Jahrhundert etabliert hat und zu einem zentralen Medium des Buchmarkts geworden ist, das für das Selbstverständnis von Autor*innen sowie für Leser*innen, Verlage und Bibliotheken große Bedeutung erlangt hat. Die Untersuchung der Ausbildung dieses Typus lässt sich mit quantitativen Methoden unterstützen, im Projekt werden etwa die 'mengenmäßig durchaus umfangreichen 'Werkausgaben Friedrichs von Hagedorn exemplarisch untersucht. Dabei gilt es, die Unterschiede in der Zusammenstellung dieser Werkausgaben mit einem Scalable-Reading-Ansatz (Mueller 2014) in den Blick zu bekommen, um so Aussagen über die Ausprägung des Buchtyps treffen zu können. Dafür wird im Projekt ein zweistufiger Workflow anvisiert: Zunächst werden unterschiedliche Methoden von Layout-Erkennungen eingesetzt, um möglichst automatisch die einzelnen Textteile der Werkausgaben identifizieren zu können. Diese werden in einem zweiten Schritt mit Methoden der Text-Reuse-Detection miteinander aligniert und auf Abweichungen bzw. Parallelen hin untersucht. Der vorliegende Beitrag widmet sich dem ersten Schritt dieses Workflows, der Layouterkennung der Werkausgaben des 18. Jahrhunderts. Wie der Blick auf ein exemplarisches Faksimile einer Hagedorn-Ausgabe in Abbildung 1 verrät, ist es zunächst die ausgeprägte Materialität und Paratextualität von historischen Werkausgaben, die unweigerlich ins Auge fällt. Dies spiegelt sich u.a. in der Auswahl von Format/Papier, Druckschrift, Layout; der Ausgestaltung von Titelseiten, Kupferstichen, Vignetten; dem Umfang, der Autorschaft und Tendenz von Titeln, Vor- und Nachworten, ""Beigaben"" und ""Nachlesen"". Bei Hagedorn interessieren darüber hinaus dessen ausgiebigen Anmerkungen (Münster 1999, 10), welche mit der Zeit durch editorische Eingriffe immer weiter aus dem Blickfeld des Publikums verbannt wurden. Paratexte gliedern und framen das ""Werk"" somit immer aufs Neue, steuern die Rezeption und ermöglichen 'einmal systematisch in ihrem Wandel erfasst 'anderweitige kulturgeschichtliche Rückschlüsse und Hypothesenbildungen (Ajouri 2017). Um eine so gegebene Komplexität weitestgehend aufzufangen, wurde im Projekt zunächst eine Taxonomie gängiger Kern- und Paratextformen erstellt, wobei Umfang und Granularität anpassbar bleiben. Ankerpunkt hierfür waren die Arbeiten von McConnaughey et al. (2017) und Underwood (2014), nicht zuletzt die Erläuterungen zur formalen/inhaltlichen Erschließung von Volltexten des DTA-Basisformats ( Bei Einheiten unterhalb der Seitenebene wächst erwartungsgemäß der Ambiguitätsgrad, weshalb sie iterativ neu verhandelt werden. So etwa die genaue Einordnung von (a) rekursiven Anmerkungen; (b) Zitaten, die mal indiziert in den Apparat eingebunden, mal selbstständig stehen; oder (c) ""schwebenden Fußnoten"", die an Gedichttexte anschließen können und einem weiteren Kerntext voranstehen, aber keine Endnoten sind (s. Abb. 2). Um auch hier die Komplexität einzudämmen, wurde eine basale Typologie beobachteter Layout-Konstellationen erstellt. Daraus lassen sich tendenziell auch typographische Vorlieben der jeweiligen Verleger ablesen, was über das Hagedorn-Korpus hinaus von Interesse sein dürfte, da es sich hierbei um im gesamten deutschsprachigen Raum tätige Akteure handelte (insgesamt 18 Verleger bzw. Ko-Verleger, s.u. Abb. 5). Das Hagedorn-Korpus hat damit ein hohes exemplarisches Potential, dessen Übertragung auf andere Gesamtausgaben lohnend erscheint. Die Layouttypen lassen sich weiter auf einzelne Seitentypen zerlegen und mit anderen Eigenschaften kombinieren (Format, Druckschrift), um neue Cluster sichtbar zu machen. Seitentypen lassen sich wiederum weiter auf Komponenten runterbrechen und zur Generierung theoretisch möglicher Layouts verwenden, was in Zeiten kostspieliger Ground Truth seine Vorteile haben kann (vgl. Fleischhaker 2024). In einem weiteren Schritt gilt es nun, TEI- oder vergleichbaren XML/JSON-Dateien zu erzeugen, in denen die Texte nach den Maßgaben dieser Taxonomie ausgezeichnet sind. Dadurch wären sie leicht auf ihre Position, Inhalt, Umfang und Abfolge bzw. die daraus folgenden (para-)textuellen Konstellationen hin beforschbar. Eine manuelle Erstellung derart detailreicher Ausgaben erscheint jedoch freilich zu aufwändig; in Ausnahmefällen, wo solche vorliegen (dreibändige Bohn-Ausgabe im DTA-Archiv, Aus den genannten Gründen müssen OCR-Verfahren 'genauer die der Die DLA ist eine unentbehrliche Vorstufe der Zeichenerkennung (OCR im engeren Sinne), steht aber im Gegensatz zu dieser weiterhin vor nicht wenigen Schwierigkeiten. Defizite und Desiderata im Bereich historischer Drucke sind in der Community allgemein bekannt und betreffen Segmentierung wie  Obwohl nicht für historische Dokumente vorgesehen, beabsichtigen wir zusätzlich Probeläufe mit sog. Instance-Based-Engines wie YOLO und Detectron2 durchzuführen, die für grobe Zoneneinteilungen, ergänzende Bilderkennung (z.B. Vignetten), oder synergisch mit den o.g. Pixel-Classifiers einsetzbar sind (vgl. Najem-Mayer/Matteo 2022). Sich auf Computer Vision allein zu stützen resp. vom ""flachen"" Text ausgehend nur auf NLP-Methoden zu setzen 'dies gilt bei perfekter Transkription (vgl. Pagel et al. 2021), geschweige denn qualitativ stark schwankender OCR –, scheint angesichts der skizzierten Herausforderungen wenig erfolgversprechend. Daher empfehlen sich hybride bzw. holistische Ansätze, die entweder 1) multimodal vorgehen, also zeitgleich Positionierung und Inhalt von Textteilen auf der Seite berücksichtigen, oder 2) eine Nachbearbeitung von anderweitigen DLA-Outputs durchführen. In die erste Kategorie fällt die LayoutLM-Modellfamilie ( Da sich das Projekt aktuell in der Annotationsphase befindet, wird eine Ground Truth iterativ in PAGE XML hergestellt (Pletschacher/Antonacopoulos 2010), einem Format, das aufgrund hoher Flexibilität und Interoperabilität alle angerissenen Lösungsszenarien bedienen kann. Je nach gewähltem Ansatz muss allerdings abgewogen werden, ob und wie man mit fehlerhaften Vorergebnissen umgeht, bzw. ob man diese unkorrigiert belässt; gleiches gilt für die nachgeschaltete OCR-Erkennung, wobei hier eine ""schmutzige"" OCR bewusst in Kauf genommen wird. Das Annotationskorpus zählt aktuell ca. 200 S. und wurde in Anlehnung an die OCR-D-Richtlinien ( Obwohl die Layouterkennung für historische Drucke weiterhin vor vielen Hürden steht und v.a. jene Forschende, die aus materialitätsbezogenen Fragestellungen neue Erkenntnisse schöpfen wollen, oft enttäuscht zurücklässt, gilt es die angerissenen und sich stets weiterentwickelnden Ansätze zu erproben und auszubauen, um eine möglichst genaue Isolierung fokussierter Buchteile für nachgeschaltete Analyseschritte zu ermöglichen. Bereits eine Übersicht über bevorzugte Layouts gibt Anlass genug, um solche vermeintlich unscheinbaren Konstellationen weiter qualitativ wie quantitativ 'also skalierbar 'zu befragen. Das Wissen um Verteilungen von Seitentypen und deren Komponenten nach Verlegern u.a. Metadaten kann weiter praktisch bei der Zusammenstellung des Annotationskorpus für OCR-relevante Tasks oder auch für die automatisierte Erstellung von Trainingsdaten herangezogen werden. Darüber hinaus ist der Buchtypus ""Werkausgabe"" 'und insofern die beachtliche Menge an Hagedorn-Exemplaren 'ein interessanter Anwendungsfall für die Erprobung von layoutgerechten Methoden: Hier wird ein relativ überschaubarer (Para-)Textkorpus in relativ vielen ""Fassungen"" visuell realisiert, was zu einer weitergehenden Ergänzung von Computer-Vision- und textbasierten Ansätzen einlädt. Dies betrifft zuvorderst die zahlreichen Anmerkungen 'ein Phänomen, dessen genauere Erfassung bislang eine eher untergeordnete Rolle spielte oder auf weniger variable Erscheinungsformen fokussiert war. Alle relevanten im Projekt erarbeiteten Forschungsdaten, Tools und Modelle werden abschließend im Einklang mit den FAIR-Prinzipien u.a. auf",de,digital Literaturwissenschaft meist weitumspannend Konzept Genr epochen Blick nehmen entwickelt Methode gewinnbringend Untersuchung buchgeschichtlich Fragestellung Einsatz bringen Kooperation Buchwissenschaft Digital Humanitie durchgeführt scalabel Reading gesammelt Werk Jahrhundert exemplarisch durchführen widmen Erforschung Buchtypus Gesamtausgabe Jahrhundert etablieren zentral Medium Buchmarkt Selbstverständnis Verlag bibliotheken Bedeutung erlangen Untersuchung Ausbildung Typus lässen quantitativ Methode unterstützen Projekt mengenmäßig umfangreich Werkausgaben Friedrich Hagedorn exemplarisch untersuchen gelten Unterschied Zusammenstellung Werkausgaben Mueller Blick bekommen aussagen Ausprägung Buchtyp treffen Projekt zweistufig Workflow anvisieren unterschiedlich Methode einsetzen möglichst automatisch einzeln Textteil Werkausgaben identifizieren Schritt Methode miteinander aligniern Abweichunge Parallele untersuchen vorliegend Beitrag widmen Schritt workflows Layouterkennung Werkausgaben Jahrhundert Blick exemplarisch Faksimile Abbildung verraten ausgeprägt Materialität Paratextualität historisch Werkausgabe unweigerlich Auge fallen spiegeln Auswahl Format Papier druckschrifen Layout Ausgestaltung titelseit kupferstich vignetten Umfang Autorschaft Tendenz Titel Nachworte beigaben nachlesen Hagedorn interessieren hinaus ausgiebigen anmerkung Münster editorisch eingriffe Blickfeld Publikum verbannen Paratext gliedern framen Werk somit auf Steuer Rezeption ermöglichen systematisch Wandel erfassen anderweitig kulturgeschichtlich Rückschlüsse Hypothesenbildungen Ajouri gegeben Komplexität weitestgehend auffangen Projekt Taxonomie gängig Paratextform erstellen wobei Umfang Granularität anpassbar bleiben ankerpunkt hierfür Arbeit Mcconnaughey Et Underwood zuletzt Erläuterung formal inhaltlich Erschließung Volltexte einheit unterhalb Seitenebene wachsen erwartungsgemäß Ambiguitätsgrad weshalb iterativ neu verhandeln genau Einordnung rekursiv anmerkungen b zitaten mal indizieren Apparat eingebund mal selbstständig stehen c schwebend fußnoten gedichttext anschließen Kerntext voranstehen endnot abb Komplexität eindämmen basal Typologie Beobachteter erstellen lassen tendenziell typographisch Vorliebe jeweilig Verleger ablesen hinaus Interesse dürfen hierbei gesamt deutschsprachig Raum tätig Akteur handeln insgesamt Verleger abb hoch exemplarisch Potential Übertragung Gesamtausgabe lohnend erscheinen Layouttype lassen einzeln Seitentype zerlegen eigenschaft Kombinier Format druckschrifen Cluster sichtbar Seitentype lassen wiederum komponent runterbrechen Generierung theoretisch möglich Layouts verwenden Zeit kostspielig Ground Truth Vorteil Fleischhaker Schritt gelten vergleichbar xml erzeugen Text Maßgaben Taxonomie auszeichnen sein Position Inhalt Umfang Abfolge folgend konstellationen beforschbar Manuelle Erstellung derart detailreich Ausgabe erscheinen freilich aufwändig Ausnahmefäll vorliegen dreibändig genannt Grund genau dla unentbehrlich Vorstufe Zeichenerkennung ocr eng Sinn stehen Gegensatz weiterhin weniger Schwierigkeit Defizit Desiderata Bereich historisch Drucke Community allgemein betreffen Segmentierung obwohl historisch Dokument vorsehen beabsichtigen zusätzlich Probeläufe yolo durchführen grob Zoneneinteilung ergänzend Bilderkennung vignetten synergisch einsetzbar matteo comput Vision stützen Resp flach Text ausgehend setzen gelten perfekter Transkription Pagel Et qualitativ stark schwankend ocr scheinen angesichts skizziert Herausforderung erfolgversprechend empfehlen hybride holistisch Ansatz Multimodal vorgehen zeitgleich Positionierung Inhalt Textteile Seite berücksichtigen Nachbearbeitung anderweitig durchführen Kategorie fallen Projekt aktuell Annotationsphase befinden Ground Truth iterativ Page xml Hergestellt pletschacher antonacopoulos Format aufgrund hoch Flexibilität Interoperabilität angerissen lösungsszenarien bedienen gewählt Ansatz abgewogen fehlerhaft vorergebnissen umgehen unkorrigiert belässen gleiches gelten nachgeschaltet wobei schmutzig ocr bewussen Kauf nehmen Annotationskorpus zählen aktuell Anlehnung obwohl Layouterkennung historisch Druck weiterhin Hürde stehen forschend materialitätsbezogen Fragestellung erkenntnis schöpfen enttäuscht zurücklässt gelten angerissen stets weiterentwickelnd Ansatz erproben ausbauen möglichst genau Isolierung Fokussierter Buchteil nachgeschaltet analyseschritte ermöglichen Übersicht bevorzugt Layouts Anlass vermeintlich unscheinbaren konstellationen qualitativ quantitativ skalierbar befragen wissen verteilungen Seitentype Komponente Verleger metadaten praktisch Zusammenstellung Annotationskorpus Tasks automatisiert Erstellung Trainingsdat heranziehen hinaus Buchtypus Werkausgabe insofern beachtlich Menge interessant Anwendungsfall Erprobung layoutgerecht Methode relativ überschaubar relativ Fassung visuell realisieren weitergehend Ergänzung textbasiert Ansatz einladen betreffen zuvorderst zahlreich anmerkung Phänomen genau Erfassung bislang eher untergeordnet Rolle spielen variabel Erscheinungsform fokussieren relevann Projekt erarbeitet Forschungsdat Tools Modell abschließend Einklang,"[('werkausgaben', 0.18474472225959782), ('seitentype', 0.1569590246411906), ('verleger', 0.14619543212233355), ('ocr', 0.11423442632823448), ('gelten', 0.10826101651454899), ('nachgeschaltet', 0.10463934976079374), ('werkausgabe', 0.10463934976079374), ('hagedorn', 0.10463934976079374), ('layouterkennung', 0.10463934976079374), ('vignetten', 0.10463934976079374)]"
2025,DHd2025,YAKUPOVA_Vera_Interdisziplin_re_transkulturelle_Analyse_von_.xml,Interdisziplinäre transkulturelle Analyse von Mensch-KI Interaktionen in Science-Fiction Literatur und im politischen Diskurs   Kulturelle Wahrnehmungen von Privatsphäre und KI-Überwachungstechnologien,"Vera Yakupova (Trinity College Dublin, Republik Irland)","KI, Science Fiction, Literaturwissenschaften, Politik, Digital Humanities","KI, Science Fiction, Literaturwissenschaften, Politik, Digital Humanities","In einer Zeit, in der Künstliche Intelligenz (KI) zunehmend unseren Alltag und politische Debatten prägt, widmet sich diese Doktorarbeit der Frage, wie Science-Fiction-Literatur als kulturelle Wissensquelle zur Reflexion von Privatsphäre, Datenschutz und Überwachungssystemen dienen kann. Der Schwerpunkt liegt darauf, wie fiktionale Figuren in Science-Fiction KI-Überwachung wahrnehmen und mit diesen Systemen interagieren, um zu zeigen, wie spekulative Fiktion gesellschaftliche und kulturelle Diskurse widerspiegelt. Durch einen transkulturellen Vergleich der USA, der EU und Russlands sollen Unterschiede in der kulturellen Akzeptanz und Kritik solcher Technologien beleuchtet werden. Science-Fiction inspiriert nicht nur die technologische Entwicklung von KI (Dillon & Schaffer-Goddard, 2023), sondern regt auch ethisches Bewusstsein und kulturelle Reflexionen an (Hudson et al., 2021; Dolan, 2020; Cave et al., 2019; Cave et al., 2020; Cave & Dihal, 2023). Werke wie George Orwells Erstens wird analysiert, wie KI-Überwachungssysteme in der Science-Fiction dargestellt werden und welche sozialen und kulturellen Kontexte sie widerspiegeln. Hierfür werden Foucaults Theorie der Überwachung (1977) und Jasanoff und Kims Konzept der soziotechnischen Imaginationen (2015) als theoretische Grundlage herangezogen, um die narrativen und symbolischen Mechanismen der Texte zu untersuchen. Zweitens erfolgt ein transkultureller Vergleich, der Gemeinsamkeiten und Unterschiede in der Darstellung von KI-Überwachung zwischen den USA, Russland und der EU mithilfe von Bakhtins ""Dialogismus"" (1981) und Kristevas ""Intertextualität"" (1969), um die kulturellen und historischen Kontexte der Narrative zu analysieren. Drittens sollen diese literarischen Darstellungen mit aktuellen Datenschutzregelungen wie der DSGVO (EU), der Privatsphärendebatte in den USA (Fazlioglu, 2020) und den Überwachungspraktiken in Russland (Pallin, 2017) verglichen werden, um die Wechselwirkungen zwischen spekulativer Fiktion und politischem Kontext zu untersuchen. Die Methodik kombiniert qualitative Analysen mit computergestützten Verfahren. In der ersten Phase wird ein Korpus von maximal 30 Science-Fiction-Werken zusammengestellt, die sich auf KI-Überwachung konzentrieren und kulturell sowie politisch repräsentativ für die untersuchten Regionen sind. Für die Analyse wird eine Mischung aus ""close reading"" und ""distant reading"" angewandt. Das Distant Reading wird mithilfe des Privacy-Wörterbuchs von Vasalou et al. (2011) spezifische Textpassagen identifiziert werden, die sich mit Privatsphäre und Überwachung befassen. Diese Passagen werden anschließend im Close Reading in den Kontext des Gesamtwerks eingebettet. Ziel ist es, sprachliche Muster zu identifizieren und zu analysieren, wie Privatsphäre thematisiert wird und wie die literarische Sprache im Vergleich zu politischen Diskursen genutzt wird. Das Close Reading mithilfe von Jonathan Cullers Literaturtheorie (2007) und Faircloughs Kritischen Diskursanalyse (1992) durchgeführt. Mithilfe von Foucaults Überwachungstheorie werden diese Passagen kategorisiert, etwa danach, ob die KI direkte Kontrolle ausübt und Widerstand provoziert oder ob Kontrolle internalisiert ist, wie im metaphorischen Panoptikum. Zudem wird untersucht, ob die KI als staatlich kontrolliertes System, unternehmensgesteuerte Technologie oder metaphorische Präsenz dargestellt wird und welche gesellschaftlichen Öngste dies reflektiert. Ziel ist es, die Darstellung von Kontrolle und Privatsphärenverletzungen in der Science-Fiction zu analysieren. Dabei werden Diskurse untersucht, die etwa Charaktere als machtlos oder widerstandsfähig gegenüber Überwachungssystemen beschreiben. Ebenso werden die Sprache, Metaphern und Symbole analysiert, die zur Beschreibung von KI-Überwachung verwendet werden. Diese Diskurse werden in ihren sozio-kulturellen und historischen Kontext eingeordnet, um zu verstehen, wie verschiedene Länder, wie Russland und die USA, KI-Überwachung unterschiedlich kritisch reflektieren. Die abschließende Phase umfasst eine vergleichende Analyse, eine Kontextualisierung, der literarischen Ergebnisse mit aktuellen politischen Dokumenten und Datenschutzrichtlinien, wie sie von der OECD, der OSTP (USA), Roskomnadzor (Russland) und dem EU-Parlament zwischen 2025 und 2028 herausgegeben werden. Inspiriert von Dillon und Craigs ""Storylistening"" (2022) wird untersucht, wie literarische Darstellungen von KI-Überwachung mit zeitgenössischen Datenschutzrichtlinien in Beziehung stehen. Ziel ist es, die kognitiven und kollektiven Funktionen dieser Darstellungen zu analysieren und ihren Einfluss auf gesellschaftliche Identitäten und politische Debatten zu diskutieren. Die Ergebnisse des Projekts sollen zeigen, wie kulturelle Unterschiede die Wahrnehmung von KI-Überwachung prägen und wie spekulative Fiktion Debatten über KI-Governance, Ethik und Datenschutz bereichern kann. Das Projekt diskutiert, wie literarische Narrative als Werkzeuge für gesellschaftliche und politische Reflexion genutzt werden können und welchen Einfluss sie auf das Verständnis von Überwachung und Privatsphäre haben. Die Kritikpunkte aus den Gutachten wurden in der überarbeiteten Version berücksichtigt, insbesondere durch eine stärkere Fokussierung des Projektrahmens. Anstelle einer allgemeinen Untersuchung von Mensch-KI-Interaktionen konzentriert sich das Projekt nun explizit auf die menschliche Wahrnehmung von KI-Überwachung und die Verletzung der Privatsphäre durch KI-Technologien. Diese thematische Eingrenzung erlaubt eine präzisere Definition der Analysekategorien und der Suchbegriffe für das Distant Reading, das mithilfe von Vasalou et al.s Privacy-Wörterbuch (2011) durchgeführt wird. Dabei wurde jedoch auch die Einschränkung akzeptiert, dass die analysierten Texte für das Distan Reading in englischer Übersetzung vorliegen müssen. Die Methodologie der Analysen wurde stärker in etablierten literaturwissenschaftlichen Traditionen verankert, um eine tiefere theoretische Fundierung zu gewährleisten. Die Auswahl der Länder 'USA, EU und Russland 'ist mit Blick auf das Thema Privatsphäre besser begründet, da diese Regionen nicht nur Vorreiter in der KI-Entwicklung sind, sondern auch unterschiedliche kulturelle Wahrnehmungen von Privatsphäre (Bellman et al., 2004) und divergierende rechtliche Rahmenbedingungen aufweisen. Fiero und Beier (2022) haben dieselben Länder bereits für eine vergleichende Analyse der Regulierung von KI und Privatsphäre im rechtlichen Diskurs herangezogen, was die Relevanz dieser Auswahl unterstreicht. Die Einbeziehung Chinas wurde in Betracht gezogen, da es ebenfalls ein Vorreiter in der KI ist und eine besondere kulturelle Wahrnehmung der Privatsphäre aufzeigt (Hua und Wang, 2023), erscheint jedoch aufgrund sprachlicher und zugangstechnischer Hürden unpraktikabel. Die Methodologie und der analytische Rahmen für den politischen Diskurs müssen weiterhin präzisiert werden. Allerdings wurde durch die Eingrenzung des Projektthemas 'von KI im politischen Diskurs allgemein auf KI-Überwachung und Privatsphäre 'bereits ein zentraler Fokus geschaffen. Beim Doctoral Consortium werde ich den Analyseprozess durch konkrete Fallbeispiele visualisieren, um die Verbindung zwischen theoretischem Ansatz und praktischer Umsetzung klar darzustellen.",de,künstlich Intelligenz ki zunehmend unser Alltag politisch Debatt prägen widmen Doktorarbeit Frage kulturell Wissensquelle Reflexion Privatsphäre Datenschutz überwachungssystemen dienen Schwerpunkt liegen fiktional Figur wahrnehmen System interagieren zeigen spekulativ Fiktion gesellschaftlich kulturell Diskurse widerspiegeln transkulturell Vergleich USA EU Russland Unterschied kulturell Akzeptanz Kritik Technologie beleuchten inspirieren technologisch Entwicklung Ki dillon regen ethisch Bewusstsein kulturell Reflexion hudson Et Dolan cave et cave et cave dihal Werk George Orwells erstens analysieren darstellen sozial kulturell Kontexte widerspiegeln hierfür Foucault Theorie Überwachung Jasanoff Kims Konzept soziotechnisch Imagination theoretisch Grundlage heranziehen narrativ symbolisch mechanismen Text untersuchen zweitens erfolgen transkulturell Vergleich gemeinsamkeit Unterschiede Darstellung USA Russland EU Mithilfe Bakhtin Dialogismus kristevas Intertextualität kulturell historisch Kontexte Narrative analysieren drittens literarisch Darstellung aktuell datenschutzregelung Dsgvo EU Privatsphärendebatte USA Fazlioglu überwachungspraktiken Russland Pallin vergleichen Wechselwirkung spekulativ Fiktion politisch Kontext untersuchen Methodik Kombiniert qualitativ Analyse computergestützt Verfahren Phase Korpus maximal zusammenstellen konzentrieren kulturell politisch repräsentativ untersucht Region Analyse Mischung Close Reading distant reading anwenden distant Reading Mithilfe vasalou Et spezifisch Textpassage identifizieren privatsphär Überwachung befassen Passag anschließend clos Reading Kontext Gesamtwerk einbetten Ziel sprachlich Muster identifizieren analysieren privatsphär thematisieren literarisch Sprache Vergleich politisch diskurse nutzen clos Reading Mithilfe Jonathan Culler Literaturtheorie Faircloughs kritisch Diskursanalyse durchführen Mithilfe Foucault Überwachungstheorie Passag kategorisieren ki direkt Kontrolle ausüben Widerstand provozieren Kontrolle internalisieren metaphorisch Panoptikum zudem untersuchen ki staatlich kontrolliert System unternehmensgesteuert Technologie metaphorisch Präsenz darstellen gesellschaftlich öngste reflektieren Ziel Darstellung Kontrolle Privatsphärenverletzung analysieren Diskurse untersuchen charakt machtlos widerstandsfähig überwachungssystemen beschreiben Sprache Metapher Symbol analysieren Beschreibung verwenden Diskurse historisch Kontext eingeordnen verstehen verschieden Land Russland USA unterschiedlich kritisch reflektieren abschließend Phase umfassen vergleichend Analyse Kontextualisierung literarisch Ergebnis aktuell politisch dokumenten datenschutzrichtlinien oecd Ostp USA Roskomnadzor Russland herausgeben inspirieren Dillon Craig Storylistening untersuchen literarisch Darstellung zeitgenössisch datenschutzrichtlinien Beziehung stehen Ziel kognitiv kollektiv Funktion Darstellung analysieren Einfluss gesellschaftlich identität politisch debatten diskutieren Ergebnis Projekt zeigen kulturell Unterschied Wahrnehmung prägen spekulativ Fiktion debatten Ethik Datenschutz bereichern Projekt diskutieren literarisch Narrative Werkzeuge gesellschaftlich politisch Reflexion nutzen einfluss Verständnis Überwachung Privatsphäre Kritikpunkt Gutacht überarbeitet Version berücksichtigen insbesondere stark Fokussierung projektrahmens Anstelle Untersuchung konzentrieren Projekt explizit menschlich Wahrnehmung Verletzung Privatsphäre thematisch Eingrenzung erlauben präziser Definition Analysekategorie Suchbegriffe distant reading Mithilfe vasalou et durchführen Einschränkung akzeptieren analysiert Text distan Reading englisch Übersetzung vorliegen Methodologie analyser stark etabliert literaturwissenschaftlich Tradition verankern tief theoretisch Fundierung gewährleisten Auswahl Land USA EU Russland Blick Thema Privatsphär begründen Region vorreit unterschiedlich kulturell Wahrnehmunge Privatsphäre Bellman et divergierend rechtlich Rahmenbedingung aufweisen Fiero Beier Land vergleichend Analyse Regulierung Ki Privatsphäre rechtlich Diskurs heranziehen Relevanz Auswahl unterstreichen Einbeziehung China betracht ziehen ebenfalls Vorreiter Ki besonderer kulturell Wahrnehmung Privatsphäre aufzeigt hua Wang erscheinen aufgrund sprachlich zugangstechnisch Hürde Unpraktikabel Methodologie analytisch Rahmen politisch Diskurs weiterhin präzisieren Eingrenzung Projektthema Ki politisch Diskurs allgemein Privatsphäre zentral Fokus schaffen Doctoral Consortium Analyseprozess konkret Fallbeispiele visualisieren Verbindung theoretisch Ansatz praktisch Umsetzung klar darstellen,"[('privatsphäre', 0.27292164979447786), ('ki', 0.27292164979447786), ('politisch', 0.2681134038466461), ('kulturell', 0.25921056825838906), ('russland', 0.22171276534309992), ('usa', 0.19794156490023473), ('reading', 0.16403305491589873), ('eu', 0.1478085102287333), ('diskurse', 0.13196104326682315), ('privatsphär', 0.12557803717158228)]"
2025,DHd2025,R_TTGERMANN_Julia_Qualitative_Genre_Profile_und_distinktive_.xml,Qualitative Genre-Profile und distinktive Wörter: Eine Studie zu Keyness in Subgenres des französischen Romans,"Julia Röttgermann (Universität Trier, Deutschland); Keli Du (Universität Trier, Deutschland); Christof Schöch (Universität Trier, Deutschland)","Computational literary studies, Keyness, Distinktivität, Evaluation, Französische Literatur","Inhaltsanalyse, Literatur, Text","In In diesem Beitrag konzentrieren wir uns auf die Analyse der Untergattungen französischer Romane und versuchen, die Lücke zwischen qualitativen und quantitativen Aspekten in der Computational Literary Studies (CLS) zu schließen, indem wir ein Mapping zwischen qualitativen, expertenbasierten ""Subgenre-Profilen"" und distinktiven Wörtern erstellen, die mit verschiedenen Distinktivitätsmaßen extrahiert wurden. In Computerlinguistik und CLS existiert eine zunehmend unübersichtliche Vielzahl an statistischen Maßen, um große Textmengen hinsichtlich distinktiver Wörter zu untersuchen. Dies ist teilweise begründet in fachlichen oder nationalen Traditionen, liegt aber auch teilweise an der Implementierung in Tools.  Der französische populäre Roman hat eine lange Geschichte, die mindestens bis ins mittlere bis späte 19. Jahrhundert zurückreicht. Die Periode von 1860–1920 wird oft als ""goldenes Zeitalter"" des populären französischen Romans angesehen, als Phänomene wie der ""roman-feuilleton"" und der Fortsetzungsroman mit wiederkehrenden Protagonisten wie Rocambole und Rouletabille aufkamen. Frühere Studien zum Populärroman konzentrierten sich häufig auf das 19. und frühe 20. Jahrhundert (Angenot 1975; Olivier-Martin 2013). Auch in der zweiten Hälfte des 20. Jahrhunderts existiert in Frankreich eine vielfältige Landschaft populärer Romane (Migozzi, 2005). Diese Romane werden von etablierten Verlagen (z.B. Harlequin, Fleuve noir, éditions du Masque) in spezialisierten Sammlungen mit hohen Auflagen und einer klaren Zielgruppenorientierung herausgegeben. Einige Subgenres des Populärromanes erhielten besondere Aufmerksamkeit, wie der ""roman policier"" (Todorov 1971; Vanoncini 2002; Dubois 2005), der Science-Fiction-Roman (Slusser 1989; Thomas 1989; Millet und Labbé 2001; Baudou 2003; Mather und Rheault 2016) oder der Liebesroman / ""roman sentimental"" (Helgorsky 1985; 1987; Constans 1999).  In jüngster Zeit wurde dem zeitgenössischen französischen populären Roman aus linguistischer Perspektive im Rahmen des Projekts ""Phraséorom"" Serien von Kriminal- oder Science-Fiction-Romanen sind zudem Beispiele für serielles Erzählen, das auf eine lange Geschichte in der französischen Literatur zurückblicken kann. Um gemeinsame Merkmale von Kriminalromanen, Science-Fiction-Romanen, Sentimentalromanen und Hochliteratur zu identifizieren, haben wir zunächst eine Liste typischer Elemente wie Themen, Figuren, sprachliche Muster, Tonalität und Erzählform durch Sichtung relevanter Sekundärliteratur qualitativ zusammengestellt. Diese Profile dienten als Grundlage für die Kategorisierung distinktiver Wörter pro Subgenre, die von den Distinktivitätsmaßen identifiziert wurden. Es wurde ein Korpus französischer Romane der 1970er, 1980er und 1990er Jahre erstellt, welches vier Gattungen enthält: Science-Fiction, Sentimentalroman, Kriminalroman und Hochliteratur, die in Frankreich sogenannte ""littérature blanche"".  Das Korpus zeitgenössischer französischer Literatur enthält 33 Millionen Tokens und umfasst 600 Romane, gleichmäßig verteilt auf vier Subgenres (Sentimentalromane, Kriminalromane, Science-Fiction-Romane, Hochliteratur) und drei Zeitperioden (1970er, 1980er, 1990er Jahre). Somit besteht das Korpus aus 50 Romanen pro Jahrzehnt und Subgenre. Die Jahrzehnte sind gleichmäßig vertreten, allerdings ist ein Höhepunkt der Veröffentlichungen in den Jahren 1989–1990 zu verzeichnen (Abb. 2). Da alle Romane urheberrechtlich geschützt sind, wurde das Korpus in Form von ""abgeleiteten Textformaten"" (Schöch et al. 2020) veröffentlicht, einem Format, das für bestimmte computergestützte Analysen geeignet ist, aber für Menschen unlesbar bleibt. Diese Strategie soll Transparenz und Reproduzierbarkeit der Studie ermöglichen. Mit dem Ziel, distinktive Wörter pro Subgenre mit drei verschiedenen Distinktivitätsmaßen zu extrahieren, vergleichen wir jedes Subgenre mit allen anderen, z. B. Science-Fiction vs. den Rest. Dazu werden 150 Romane eines Subgenres mit 450 Romanen der übrigen Subgenres verglichen. Da Französisch eine flektierte Sprache ist, werden alle Texte mit SpaCy lemmatisiert (Montani et al. 2023). Anschließend werden sie mit dem Python-Paket pydistinto (Du et al. 2022) verarbeitet. Die drei angewendeten Distinktivitätsmaße sind Zeta, Welch und LLR. Für die Berechnung von Zeta werden die Romane in 5000-Wörter-Segmente unterteilt. Aus jedem Test erhalten wir drei Listen mit distinktiven Wörtern und wählen die Top 50 Wörter für den Abgleich mit den qualitativen Subgenre-Profilen aus. Unsere Erwartung ist dabei, dass die extrahierten distinktiven Wortlisten pro Subgenre in mehr oder weniger ausgeprägtem Umfang Wörter enthalten, die sich auf die thematischen Konzepte, Sprachmuster, Figuren, Schauplätze, Tonalität, Erzählform oder Erzählstruktur aus den qualitativen Subgenre-Profilen beziehen lassen und so eine qualitative Evaluation der Maße erlauben.  Das Ziel ist es, den Anteil der interpretierbaren distinktiven Wörter pro Maß zu quantifizieren. Wörter, die nicht zu den Subgenre-Profilen passen, werden als ""unerwartet"" klassifiziert. Was verstehen wir im hier skizzierten Kontext unter Interpretierbarkeit? In dieser Studie gehen wir über die Klassifizierung von Keyness-Maßnahmen allein nach ihrer Leistung hinaus. Unter dem breiteren Paradigma von explainability (Erklärbarkeit) und interpretability (Interpretierbarkeit) von algorithmischen Methoden (u.a. Benois-Pineau et al. 2023), untersuchen wir die Ergebnislisten der Extraktion mit Keyness-Maßen. Insbesondere gleichen wir die Top 50 der distinktiven Wörter aus diesen Listen mit den von Fachwissenschaftler:innen erstellten Genreprofilen ab. Wir definieren dabei Interpretierbarkeit auf der Ebene der einzelnen Wörter: Ein distinktives Wort wird als interpretierbar definiert, wenn menschliche Annotator:innen es einem deskriptiven Genreprofil zuordnen kann. In einem ähnlichen Sinne haben Chang et al. (2009) einen Ansatz entwickelt, bei dem menschliche Annotator:innen die Aufgabe haben, aus einer Reihe von Topic Modeling Ergebnissen ""the odd one out"" (das nicht passende Wort innerhalb der Wortliste) zu identifizieren, als Operationalisierung der Interpretierbarkeit von Topic Modeling Ergebnissen. Bei der Beurteilung der Interpretierbarkeit lautet die übergreifende Frage außerdem: ""Can we trust the model?"" (Mas√≠s 2023, 6)? In unserem Fall lautet diese Frage demnach analog: Können wir dem Keyness-Maß (für diese Aufgabe im genannten Setting) vertrauen? Je höher der Anteil interpretierbarer Wörter ist, als desto zuverlässiger kann das Maß für ähnliche Aufgaben angesehen werden.   Logarithmisches Zeta erwies sich beim Matching-Prozess der Wortlisten mit den Genreprofilen als am Besten geeignet zur Extraktion distinktiver Wörter, gefolgt von Welch""s t-Test. Der Log-Likelihood-Ratio-Test zeigte im untersuchten Setting die schwächste Leistung, was überrascht, da das Maß in zahlreichen Korpusanalysetools wie Antconc oder Wordsmith Tools implementiert ist. Dieses Ergebnis wirft Fragen nach den standardmäßig implementierten Keyness-Maßen in DH-Tools auf und deutet darauf hin, dass eine kritische Überprüfung in Betracht gezogen werden sollte, zumindest in Szenarien, die mit narrativer Prosa arbeiten.",de,Beitrag konzentrieren Analyse untergattung französisch Roman versuchen Lücke qualitativ quantitativ Aspekt Computational literary Studies cls schließen Mapping qualitativen expertenbasierten distinktiv wörtern erstellen verschieden distinktivitätsmaßen extrahiern Computerlinguistik Cls existieren zunehmend unübersichtlich Vielzahl statistisch maeß Textmeng hinsichtlich Distinktiver Wörter untersuchen teilweise begründen fachlich national Tradition liegen teilweise Implementierung Tools französisch Populäre Roman Geschichte mindestens mittlerer spät Jahrhundert zurückreichen Periode golden Zeitalter populär französisch Roman ansehen phänomen Fortsetzungsroman wiederkehrend Protagonist rocambole rouletabill aufkamen früh Studie Populärroman konzentriern häufig früh Jahrhundert Angenot Hälfte Jahrhundert existieren Frankreich vielfältig Landschaft populär Romane Migozzi Roman etabliert Verlag Harlequin Fleuve Noir éditions Masque spezialisiert Sammlung hoch Auflage klar Zielgruppenorientierung herausgeben Subgenres populärromanes erhalten besonderer Aufmerksamkeit Roman Policier Todorov vanoncini Dubois sluss Thomas millet Labbé baudou Mather Rheault Liebesroman Roman sentimental helgorsky Constan jung zeitgenössisch französisch populär Roman linguistisch Perspektive Rahmen Projekt Phraséorom Serie zudem Beispiel Serielle erzählen Geschichte französisch Literatur zurückblicken gemeinsam Merkmal Kriminalroman Sentimentalroman Hochliteratur identifizieren Liste typisch element Thema Figur sprachlich Muster Tonalität Erzählform Sichtung relevant Sekundärliteratur qualitativ zusammenstellen profil Dient Grundlage Kategorisierung distinktiv Wörter pro Subgenre distinktivitätsmaße identifizieren korpus französisch Roman erstellen Gattung enthalten Sentimentalroman Kriminalroman Hochliteratur Frankreich sogenannter littératurer blanch Korpus zeitgenössisch französisch Literatur enthalten Million Token umfassen Roman gleichmäßig verteilen Subgenres Sentimentalromane kriminalroman Hochliteratur zeitperioden somit bestehen korpus Romane pro Jahrzehnt Subgenre Jahrzehnt gleichmäßig vertreten Höhepunkt Veröffentlichung verzeichnen abb Roman Urheberrechtlich schützen korpus Form abgeleitet Textformat schöch et veröffentlichen Format bestimmt computergestützt Analyse eignen Mensch unlesbar bleiben Strategie Transparenz Reproduzierbarkeit Studie ermöglichen Ziel Distinktive Wörter pro Subgenre verschieden distinktivitätsmaßen extrahieren vergleichen jeder Subgenre Rest Roman Subgenr Roman übrig Subgenr vergleichen französisch flektiert Sprache Text Spacy lemmatisieren Montani et anschließend Pydistinto et verarbeiten angewendeten distinktivitätsmaß zeta welcher llr Berechnung Zeta Roman unterteilen Test erhalten Liste distinktiver wörtern wählen top Wörter abgleich qualitativ Erwartung extrahiert distinktiv Wortlist pro Subgenre ausgeprägt Umfang Wörter enthalten thematisch Konzept Sprachmuster Figur Schauplätze Tonalität Erzählform Erzählstruktur qualitativ beziehen lassen qualitativ Evaluation Maß erlauben Ziel Anteil interpretierbar distinktiv Wörter pro Maß quantifizieren Wörter passen unerwartet klassifizieren verstehen skizziert Kontext interpretierbarkeit Studie Klassifizierung Leistung hinaus breit Paradigma Explainability Erklärbarkeit interpretability Interpretierbarkeit algorithmisch Methode et untersuchen Ergebnislist Extraktion insbesondere gleichen top distinktiv Wörter Liste Fachwissenschaftler innen erstellt Genreprofil definieren Interpretierbarkeit Ebene einzeln Wörter distinktiv Wort interpretierbar definieren menschlich Annotator innen deskriptiv Genreprofil zuordnen ähnlich Sinn Chang et Ansatz entwickeln menschlich Annotator innen Aufgabe Reihe Topic Modeling Ergebnis -- odd One out passend Wort innerhalb Wortliste identifizieren Operationalisierung Interpretierbarkeit Topic Modeling Ergebnis Beurteilung Interpretierbarkeit lauten übergreifend Frage can we trust The Model unser Fall lauten Frage demnach analog Aufgabe genannt Setting vertrauen hoch Anteil interpretierbar Wörter desto zuverlässig Maß ähnlich Aufgabe ansehen logarithmisches Zeta erweisen wortli Genreprofile geeignet Extraktion distinktiv Wörter folgen welcher -- zeigen untersucht Setting schwach Leistung überraschen Maß zahlreich Korpusanalysetool Antconc Wordsmith Tools implementieren Ergebnis werfen Frage standardmäßig implementiert deuten kritisch Überprüfung betracht ziehen zumindest Szenarie narrativ Prosa arbeiten,"[('roman', 0.27118365335378736), ('distinktiv', 0.2596679107118105), ('wörter', 0.24105789598642618), ('französisch', 0.22473463248620634), ('interpretierbarkeit', 0.2047364597313489), ('subgenre', 0.1972654530310366), ('kriminalroman', 0.14537003208421337), ('hochliteratur', 0.12832813976606608), ('qualitativ', 0.12384436539147214), ('pro', 0.12272163518150349)]"
2025,DHd2025,GERSTORFER_Dominik_Operationalizing_operationalizing.xml,Operationalizing operationalizing,"Dominik Gerstorfer (TU-Darmstadt, Deutschland); Evelyn Gius (TU-Darmstadt, Deutschland)","Operationalisierung, Explikation, Validieren, Kalibrierung, Workflow, Forschungsdesign","Modellierung, Methoden, Forschung, Forschungsprozess","In this paper we suggest a comprehensive account of operationalization. The goal is to define a workflow which specifies the normative components that ensure that the results match the research question. The focus here lies on epistemic norms 'as opposed to social norms 'which can be understood as rules that govern processes, thoughts, and actions that ought to be followed in order to produce the intended result (cf. Wedgwood 2018). This workflow is based on a measurement workflow, which is based on input-process-output components (cf. Mari, Wilson, and Maul 2023). In order to make the workflow implementable in Digital Humanities research, we need to identify the components of a generalized method for getting from a research question to its answer. While some of the components have already been discussed in Digital Humanities"" research on operationalization, we propose to put more focus on explication, measurement, and validation. In our view, measurement and validation are underdeveloped in the Digital Humanities. This becomes apparent when looking at discussions on operationalization. Before doing that we briefly line out our understanding of central terms in the discussion In order to develop our account, we will make the following assumptions: The leading question for scientific workflows involving the analysis of data is this: In the case of text based digital humanities, we can specify the scope of theory and data further. Theory and text depend on the disciplinary background of the research question. For example, in Computational Literary Studies approaches, theory means literary theory and empirical data respectively text. A first approach to address this question attempted by Franco Moretti, who introduced operationalization to the discourse of the digital humanities with his seminal paper ""Operationalizing or, the Function of Measurement in Modern Literary Theory"" (Moretti 2013). In this paper, Moretti goes back to Bridgman""s ([1927] 1958) original conception of operationalization and applies it to literary concepts. His answer to question Q"" famously is: We must imagine a bridge that is built by operations, which take us from the (qualitative) concepts of literary theory to (quantitative) measurement results. This implies existence of a simple and easy way to connect a concept C to an indicator I, resulting in a straightforward measure, which allows inferences, like the more I, the more C. Finally numbers are assigned to produce a measurement metric, be it by counting words or by using more elaborate techniques, like network analysis. In his paper, Moretti gives examples to show how this can be done, but he does not provide a detailed and generalized method that can be followed in different contexts. We therefore specify our question even further: In order to tackle this question, we have to address at least three ambivalent aspects of operations that are at least implicitly underlying understandings meaning of operationalization: In digital humanities research operationalization often only means (a), i.e. that in the research design some kind of workflow or pipeline is developed, that is used to automatically extract some features from the empirical material (text). Often literary concepts are adopted uncritically, when, in fact, critical reflection on the usage of the concepts in the sense of (b) is needed to guarantee that the workflow returns results that (c) are actually valid. The last point holds even then, when statistical validity metrics are incorporated in the workflow, since those tests can only answer how good the machine learning model or the algorithm worked, but not, if the quantified results translate back to literary concepts. This objection follows the main line of argumentation of Pichler and Reiter (2022), who argue for the importance of validity while maintaining its untenability for many real-world scenarios in the digital humanities. Overall, we understand operationalization as a workflow or process in the sense of (c), specifically in terms of the scientific input-process-output workflow (Mari, Wilson, and Maul 2023, see Fig. 1). In the following, we will introduce explication, measurement, and validity as components that make this workflow consistent and appropriate for the respective scientific context. The starting point of operationalization in our account are concepts which are transformed to empirical mini-theories that fit the research task at hand. To reach this goal, we resort to Carnapian explication (cf. Carnap 1950; Brun 2016) which allows us to accommodate various epistemic goals and permits different concept variations for different purposes. Explications involve two areas, the one of the explicandum and the one of the explicatum. These domains are sometimes viewed as pre-scientific and scientific uses of the term, but we can assume that explications can occur in any area. This construes a more exact explication in a more restricted or specialized area. For instance, moving from chemistry to molecular quantum chemistry or literary theory to computational literary theory (CLS). Measurement can be thought of as a process that takes some inputs and returns some outputs, where the input is some kind of empirical data and the output a value, see Fig 1a. But not any arbitrary process is suitable for measurement, it is necessary to specify the properties of the process, which will result in a procedure which is designed for that specific task (Mari, Wilson, and Maul 2023, 27), see Fig. 1b. Validation A common conception of validation is the following: A e.g. thermometer is intended to measure the actual temperature of an object, where temperature is a property of said object. This notion seems to be sufficiently clear, but on closer inspection it turns out that the terms in (M) are underspecified. How do we determine ""measurement"", ""measure"", and ""intended""? What exactly are we referring to, when we use those terms? ""Measurement"" is ambiguous at least respective its target (Mari, Wilson, and Maul 2023; Maul, Mari, and Wilson 2019). Measurement may either denote a measuring process or the result of such process. To avoid misunderstandings, we will adopt a strict terminology which discriminates measurement processes and measurement results. Where process means a set of operations which are implemented as an instrument, that can range from physical artefacts, like thermometers, to computational devices, like machine learning algorithms. The result is an assigned value on an appropriate scale. Calibration Calibration denotes the inferential activity of assigning values to the measurement instrument indications and thus producing the measurement results, where: Calibration links the outcomes of the measurement process to the epistemic claims about the object that is measured, this step is necessary since the indications of the measurement instrument are not of the same kind as the properties of the object, see Fig. 1c. The scale of a thermometer e.g. presents its indications visually a further step is needed to ensure that those values can be used to represent temperature. The same applies to literary concepts, like, in Moretti""s case character space. Network analysis or word counts provide indicators that need to be mapped to the possible values of the intended property being measured. We can now revise Moretti""s picture of operationalization as a simple measurement process, that connects theory to world directly (see Fig. 2a) and replace it with the refined scheme developed above (Fig. 2b). For our account of operationalization we consider operations as activities that are performed on some type of input, which return some type of outputs. The kind of activity performed in operations can vary wildly, ranging from simple reasoning about concepts over doing physical activity to actual computation. In constructing an operationalization workflow we denote operations as connected boxes: We are finally able to put all the pieces together and construct a workflow that starts with concepts and ends with measurement results. The operationalization workflow has two sources: (i) The concept(s) originating in the source domain (humanities, literary theory, CLS) and (ii) the empirical data / text. Starting with the concepts, the workflow follows these steps:",en,paper suggest comprehensive account operationalization goal define workflow specify normative component ensure result match research question focus lie epistemic norm oppose social norm understand rule govern process thought action ought follow order produce intend result cf wedgwood workflow base measurement workflow base input process output component cf mari wilson maul order workflow implementable digital humanity research need identify component generalized method get research question answer component discuss digital humanity research operationalization propose focus explication measurement validation view measurement validation underdeveloped digital humanity apparent look discussion operationalization briefly line understanding central term discussion order develop account following assumption lead question scientific workflow involve analysis datum case text base digital humanity specify scope theory datum theory text depend disciplinary background research question example computational literary study approach theory mean literary theory empirical datum respectively text approach address question attempt franco moretti introduce operationalization discourse digital humanity seminal paper operationalizing function measurement modern literary theory moretti paper moretti go original conception operationalization apply literary concept answer question q famously imagine bridge build operation qualitative concept literary theory quantitative measurement result imply existence simple easy way connect concept c indicator result straightforward measure allow inference like finally number assign produce measurement metric count word elaborate technique like network analysis paper moretti give example provide detailed generalized method follow different context specify question order tackle question address ambivalent aspect operation implicitly underlying understanding mean operationalization digital humanity research operationalization mean research design kind workflow pipeline develop automatically extract feature empirical material text literary concept adopt uncritically fact critical reflection usage concept sense b need guarantee workflow return result c actually valid point hold statistical validity metric incorporate workflow test answer good machine learning model algorithm work quantify result translate literary concept objection follow main line argumentation pichler reiter argue importance validity maintain untenability real world scenario digital humanity overall understand operationalization workflow process sense c specifically term scientific input process output workflow mari wilson maul fig following introduce explication measurement validity component workflow consistent appropriate respective scientific context starting point operationalization account concept transform empirical mini theory fit research task hand reach goal resort carnapian explication cf carnap brun allow accommodate epistemic goal permit different concept variation different purpose explication involve area explicandum explicatum domain view pre scientific scientific use term assume explication occur area construe exact explication restricted specialized area instance move chemistry molecular quantum chemistry literary theory computational literary theory cls measurement think process take input return output input kind empirical datum output value fig arbitrary process suitable measurement necessary specify property process result procedure design specific task mari wilson maul fig validation common conception validation following thermometer intend measure actual temperature object temperature property say object notion sufficiently clear close inspection turn term m underspecified determine measurement measure intend exactly refer use term measurement ambiguous respective target mari wilson maul maul mari wilson measurement denote measure process result process avoid misunderstanding adopt strict terminology discriminate measurement process measurement result process mean set operation implement instrument range physical artefact like thermometer computational device like machine learning algorithm result assign value appropriate scale calibration calibration denote inferential activity assign value measurement instrument indication produce measurement result calibration link outcome measurement process epistemic claim object measure step necessary indication measurement instrument kind property object fig scale thermometer present indication visually step need ensure value represent temperature apply literary concept like case character space network analysis word count provide indicator need map possible value intend property measure revise picture operationalization simple measurement process connect theory world directly fig replace refined scheme develop fig account operationalization consider operation activity perform type input return type output kind activity perform operation vary wildly range simple reasoning concept physical activity actual computation construct operationalization workflow denote operation connect box finally able piece construct workflow start concept end measurement result operationalization workflow source originate source domain humanity literary theory cls ii empirical datum text start concept workflow follow step,"[('measurement', 0.46636909355669937), ('operationalization', 0.3031399108118546), ('process', 0.21573802542792464), ('concept', 0.20863970522216854), ('workflow', 0.18957582645137594), ('result', 0.17327714373243025), ('theory', 0.15883738175472772), ('explication', 0.1399107280670098), ('question', 0.13909313681477906), ('humanity', 0.13576895239112552)]"
2025,DHd2025,SPIELBERG_Marina_Literary_Metaphor_Detection_with_LLM_Fine_T.xml,Literary Metaphor Detection with LLM Fine-Tuning and Few-Shot Learning,"Marina Spielberg (Trier Center for Digital Humanities (TCDH), Universität Trier, Deutschland)","large language models, metaphor detection, fine-tuning","Programmierung, Daten, Literatur, Forschungsergebnis","The study of literary metaphors plays an integral part in literature-focused disciplines within the humanities. In the field of Natural Language Processing (NLP), computational metaphor detection (MD) has produced a wealth of approaches focusing on everyday metaphors (Ptiƒçek and Dob≈°a 2023). Computational literary MD, however, has received considerably less attention. In their Graph Project, Kesarwani et al. (2017) have applied rule-based and statistical machine learning approaches to an English poetry corpus. The aim of this paper is to take the field of literary MD one step further by using the NLP state-of-the-art approach of fine-tuning Large Language Models (LLMs) on the Graph project""s datasets. This paper tests two assumptions: 1. Fine-tuning the LLM DistilBERT with the Transformers approach and the LLM all-MiniLM-L6-v2 with the SetFit approach on the Graph project""s datasets will yield better results than using the combined statistical and rule-based approach from Kesarwani et al. (2017). 2. The SetFit approach will outperform the Transformers approach. The paper starts by setting out the theoretical background of MD and then explains the data, method and experimental setup used. It closes with a description of the evaluation results and a discussion. Since the current theories and methods regarding MD come from NLP, it is beneficial to understand prevalent research in this area before focusing on the state of the field in the Digital Humanities. In most of the MD research in NLP, metaphors are understood as a mapping of a source domain like WAR to a target domain like ARGUMENT that can result in the metaphorical linguistic expression ""Your claims are Since 1975, research on MD is ongoing because ""the task is not considered solved"" (Dankin, Bar, and Dershowitz 2022, 125). The methodologies applied to MD have evolved with the development of computational capabilities starting with rule-based, statistical and machine learning methods (Ptiƒçek and Dob≈°a 2023). The current state-of-the-art method for MD involves utilizing LLMs, often derivations of the BERT model (Devlin et al. 2019), which are based on the Transformer architecture by Vaswani et al. (2017) that allows to fine-tune an existing language model on a specific downstream task like MD (Babieno et al. 2022; Li et al. 2023; Z. Song et al. 2024). Recently, prompt engineering started to be utilized for MD. Instead of labelled data this method uses task-specific formulated prompts (Jia and Li 2024). Chen et al. (2024) expanded the task to include metaphor reason in addition to detection since they found that methods focusing solely on a metaphorical versus literal distinction did not generalise well. Another recent development is to detect metaphor in multi-modal settings, such as memes, where the classification task includes both texts and images (Xu et al. 2024). In the Digital Humanities, MD focusing on literary texts is understudied. To my knowledge, there are only a handful of papers that concern themselves specifically with this task. Reinig and Rehbein (2019) proposed a supervised machine learning method for MD in German expressionist poetry, while Schneider et al. (2022) developed an unsupervised approach for Middle High German. Toker et al. (2024) used LLMs on their own Early Medieval Hebrew poetry dataset. In their Graph Project, Kesarwani et al. (2017) developed models to detect poetic metaphors in English, which has neither been done in NLP nor in the Digital Humanities. Diverging from the Conceptual Metaphor Theory, the authors based their metaphor definition on observations by Neuman et al. (2013), who found three metaphor types that signify their metaphoricity by different part-of-speech (POS) sequences. They focused on detecting Type I metaphors, which are comprised of a ""Noun-Verb-Noun"" sequence and added the sequence ""Noun-Verb-Det-Noun"" to this concept. An example for Type I metaphor is the sentence ""As if the The authors trained and tested on four datasets: Their own PoFo (Poetry Foundation) dataset, comprising 680 sentences that include Type I metaphor scraped from the Poetry Foundation website, the benchmark datasets TroFi by Birke and Sarkar (2006) (6,435 sentences from the Wall Street Journal Corpus) and MOH by Mohammad, Shutova, and Turney (2016) (647 sentences from WordNet). Finally, they created a fourth concatenated dataset which combines PoFo, TroFi and MOH. See table 1 for an overview of sample sentences labelled ""metaphorical"" from each dataset. The methods of the Graph Project mirror the progression of methods in the NLP tradition as the authors experimented with rule-based and statistical approaches (Kesarwani et al. 2017). The F1 scores for their rule-based and machine learning models were 0.669 for PoFo, 0.827 for TroFi, 0.779 for MOH and 0.781 for the concatenated dataset. Tanasescu, Kesarwani, and Inkpen (2018) used deep learning with convolutional neural networks on the concatenated dataset and reached an F1 score of 0.833. Since fine-tuning LLMs proved to be a very successful approach in recent NLP research (e.g., a F1 score of 0.944 on TroFi by Ma et al. 2021), this paper tests whether this method will improve the MD performance on the Graph project""s datasets. Based on the assumption that the metaphoricity of a word stems from the context of the whole sentence rather than a single aspect word or POS sequence, I define MD as a sentence-level classification problem, where each sentence within the four datasets is labelled as either ""metaphorical"" or ""literal"" (Ma et al. 2021). This approach differs from Kesarwani et al. (2017), who annotate metaphor on a token level. For preprocessing, I reused the cleaned versions of the TroFi and MOH datasets by Su et al. (2020) due to a lack of clear preprocessing information from the Graph project and used the original version of PoFo by Kesarwani et al. In the concatenated dataset there are more literal than metaphorical sentences since it consists of 75% of sentences belonging to TroFi, which suffers from label imbalance, having about 12% more literal than metaphorical examples (figure 1). The first method to improve the MD performance is fine-tuning the four datasets on the Transformer-based pre-trained LLM DistilBERT (Sanh et al. 2019), which is an efficient variant of BERT. This model is smaller and faster while maintaining 97% of BERTs performance. The training for this Transformers approach is threefold: The model-specific tokenizer maps the dataset""s text tokens to indexes, the transformer converts these indexes to contextual embeddings and the pre-trained head is fine-tuned on the MD task (Wolf et al. 2020). The fine-tuning procedure consisted of training with 70% of the data and evaluating with the remaining 30% for each dataset. For hyperparameters, the batch size of 32, the learning rate of 2e-5 and 5 training-epochs yielded best results. Since Transformer LLMs require fine-tuning on relatively large datasets which is a challenge for literary metaphor datasets, as can be seen from the sizes of the Graph Project""s datasets, this paper also employs the SetFit framework (Tunstall et al. 2022). It is designed for few-shot learning, that is, learning with a small number of labelled examples. SetFit operates in two steps: It generates sentence pairs, thereby enlarging the dataset significantly, and then fine-tunes embeddings of these sentences to create a sentence transformer embedding model. Then the dataset is used to train a logistic regression classifier using the fine-tuned embeddings to predict the right labels. The impact of the Sentence Transformer on the dataset size is immense: From 441 samples of the PoFo train dataset, SetFit generated 98366 unique pairs. The all-MiniLM-L6-v2 Sentence Transformer model is used within this framework because it is 5 times faster than larger models while maintaining comparable performance (Reimers et al. 2019). For implementation, the same test-train split ratio, seed and hyperparameters are used as for the Transformers implementation to maintain comparability. Figure 2 presents the evaluation results of fine-tuning with the Transformers and SetFit methods on the MD task at sentence-level. All reported results are from evaluating the fine-tuned models on the test splits. At least one of the LLM fine-tuning approaches proposed in this paper outperformed the baseline on all datasets except for TroFi. For PoFo there was a significant performance increase of 12.41% with SetFit (F1 0.752) and 2.84% with Transformers (F1 0.688). The MOH dataset displayed an F1 score of 0.785 with the Transformers approach (0.77% increase). The SetFit approach demonstrated a significant improvement with an F1 score of 0.862 (10.37% increase) on the concatenated dataset. However, the TroFi dataset performed much better with the rule-based and statistical method, surpassing SetFit by 22.25%. As for the comparison of Transformers with SetFit, SetFit achieved better performance on PoFo, TroFi and the concatenated dataset, while Transformers gave better results on the MOH dataset. Overall, no method excelled across all datasets, although the LLM approaches generally performed better. The promising results indicate the potential of fine-tuning LLMs for literary MD in general and using the SetFit method for small datasets in particular. The concatenated dataset's heterogeneity and diversity in sentence length and number, complexity, and metaphor domains provided broader contextual information, explaining its high F1 score. Despite these positive outcomes fine-tuning did not achieve better performance on TroFi. TroFi""s suboptimal performance could stem from training difficulty caused by the sentence-level approach and label imbalance. However, these arguments can be refuted by TroFi""s substantial representation in the concatenated dataset (75.1%, table 1), which showed exceptionally good results. Thus, TroFi""s characteristics must have significantly contributed to these positive results. The chosen LLM models might also contribute to the low result. Being condensed models, DistilBERT and all-Mini-LM-L6-v6 are useful for initial experiments but might not generalise well on a domain-specific task like MD. Using SetFit on a state-of-the-art Sentence Transformer model like all-mpnet-base-v2 (Song et al. 2020) may help improve results. However, the core difficulty of finding concrete explanations for the poor TroFi results and the excellent concatenated dataset and PoFo results is the interpretability limitation of LLMs due to their ""black-box"" nature. Contrary to statistical machine learning approaches, no information is provided about which features contribute most to the classification task during LLM fine-tuning, making the reasoning behind the model""s classification result not entirely comprehensible (Dobson 2023, 431). This is especially concerning for the Digital Humanities, where understanding the domain is just as important as raw performance. Ablation techniques and visualisations of attention weights could help understanding how model output was created. This paper contributed to the understudied task of literary MD by applying state-of-the-art NLP methodology, like fine-tuning the Transformer model DistilBERT and few-shot learning with the Sentence Transformer approach, on four literary metaphor datasets. Metaphor was defined quite narrowly as consisting of one of two specific POS sequences. The evaluation baseline was the combined rule-based and statistical approach of Kesarwani et al. (2017). The results demonstrate performance increases in F1 score for the fine-tuning approach over the baseline and even more so for the SetFit methods, especially for PoFo (12.41% increase) and the concatenated dataset (10.37% increase). However, improvement was not observed for the TroFi dataset, which could stem from sentence complexity and label imbalance or from small model sizes. These findings emphasise that while the current practise of fine-tuning LLMs for linguistic MD can also yield good results for literary MD and that SetFit is a valuable tool for small datasets, these methods do not guarantee improved performance. Due to the black-box nature of LLMs they might not be the right tool for literary scholars, who prioritise interpretability. Further work needs to be performed to establish whether larger models could optimise the work done in this paper. Future studies on literary MD could focus on creating larger datasets with different kinds of metaphors or employ prompt engineering. The fine-tuned models could be deployed to build interactive tools for teaching and studying metaphors in educational settings.",en,study literary metaphor play integral literature focus discipline humanity field natural language processing nlp computational metaphor detection md produce wealth approach focus everyday metaphor ptiƒçek computational literary md receive considerably attention graph project kesarwani et al apply rule base statistical machine learn approach english poetry corpus aim paper field literary md step nlp state art approach fine tune large language model llm graph dataset paper test assumption fine tune llm distilbert transformer approach llm minilm setfit approach graph dataset yield well result combine statistical rule base approach kesarwani et al setfit approach outperform transformer approach paper start set theoretical background md explain datum method experimental setup close description evaluation result discussion current theory method md come nlp beneficial understand prevalent research area focus state field digital humanity md research nlp metaphor understand mapping source domain like war target domain like argument result metaphorical linguistic expression claim research md ongoing task consider solve dankin bar dershowitz methodology apply md evolve development computational capability start rule base statistical machine learning method ptiƒçek current state art method md involve utilize llm derivation bert model devlin et al base transformer architecture vaswani et al allow fine tune exist language model specific downstream task like md babieno et al li et al song et al recently prompt engineering start utilize md instead label datum method use task specific formulate prompt jia li chen et al expand task include metaphor reason addition detection find method focus solely metaphorical versus literal distinction generalise recent development detect metaphor multi modal setting meme classification task include text image xu et al digital humanity md focus literary text understudied knowledge handful paper concern specifically task reinig rehbein propose supervised machine learning method md german expressionist poetry schneider et al develop unsupervised approach middle high german toker et al llm early medieval hebrew poetry dataset graph project kesarwani et al develop model detect poetic metaphor english nlp digital humanity diverge conceptual metaphor theory author base metaphor definition observation neuman et al find metaphor type signify metaphoricity different speech pos sequence focus detect type metaphor comprise noun verb noun sequence add sequence noun verb det noun concept example type metaphor sentence author train test dataset pofo poetry foundation dataset comprise sentence include type metaphor scrape poetry foundation website benchmark dataset trofi birke sarkar sentence wall street journal corpus moh mohammad shutova turney sentence wordnet finally create fourth concatenate dataset combine pofo trofi moh table overview sample sentence label metaphorical dataset method graph project mirror progression method nlp tradition author experiment rule base statistical approach kesarwani et al score rule base machine learning model pofo trofi moh concatenate dataset tanasescu kesarwani inkpen deep learning convolutional neural network concatenate dataset reach score fine tune llm prove successful approach recent nlp research score trofi ma et al paper test method improve md performance graph dataset base assumption metaphoricity word stem context sentence single aspect word pos sequence define md sentence level classification problem sentence dataset label metaphorical literal ma et al approach differ kesarwani et al annotate metaphor token level preprocessing reuse clean version trofi moh dataset su et al lack clear preprocessing information graph project original version pofo kesarwani et al concatenate dataset literal metaphorical sentence consist sentence belong trofi suffer label imbalance have literal metaphorical example figure method improve md performance fine tune dataset transformer base pre train llm distilbert sanh et al efficient variant bert model small fast maintain bert performance training transformer approach threefold model specific tokenizer map text token index transformer convert index contextual embedding pre train head fine tune md task wolf et al fine tuning procedure consist training datum evaluate remain dataset hyperparameter batch size learning rate training epoch yield good result transformer llm require fine tuning relatively large dataset challenge literary metaphor dataset see size graph dataset paper employ setfit framework tunstall et al design shot learning learn small number label example setfit operate step generate sentence pair enlarge dataset significantly fine tune embedding sentence create sentence transformer embed model dataset train logistic regression classifier fine tune embedding predict right label impact sentence transformer dataset size immense sample pofo train dataset setfit generate unique pair minilm sentence transformer model framework time fast large model maintain comparable performance reimer et al implementation test train split ratio seed hyperparameter transformer implementation maintain comparability figure present evaluation result fine tuning transformer setfit method md task sentence level report result evaluate fine tune model test split llm fine tune approach propose paper outperform baseline dataset trofi pofo significant performance increase setfit transformer moh dataset display score transformer approach increase setfit approach demonstrate significant improvement score increase concatenate dataset trofi dataset perform well rule base statistical method surpass setfit comparison transformer setfit setfit achieve well performance pofo trofi concatenate dataset transformer give well result moh dataset overall method excel dataset llm approach generally perform well promise result indicate potential fine tune llm literary md general setfit method small dataset particular concatenate dataset heterogeneity diversity sentence length number complexity metaphor domain provide broad contextual information explain high score despite positive outcome fine tuning achieve well performance trofi suboptimal performance stem training difficulty cause sentence level approach label imbalance argument refute substantial representation concatenate dataset table show exceptionally good result characteristic significantly contribute positive result choose llm model contribute low result condense model distilbert mini lm useful initial experiment generalise domain specific task like md setfit state art sentence transformer model like mpnet base song et al help improve result core difficulty find concrete explanation poor trofi result excellent concatenate dataset pofo result interpretability limitation llm black box nature contrary statistical machine learning approach information provide feature contribute classification task llm fine tuning make reasoning classification result entirely comprehensible dobson especially concern digital humanity understand domain important raw performance ablation technique visualisation attention weight help understand model output create paper contribute understudied task literary md apply state art nlp methodology like fine tune transformer model distilbert shot learning sentence transformer approach literary metaphor dataset metaphor define narrowly consist specific pos sequence evaluation baseline combine rule base statistical approach kesarwani et al result demonstrate performance increase score fine tuning approach baseline setfit method especially pofo increase concatenate dataset increase improvement observe trofi dataset stem sentence complexity label imbalance small model size finding emphasise current practise fine tune llm linguistic md yield good result literary md setfit valuable tool small dataset method guarantee improved performance black box nature llm right tool literary scholar prioritise interpretability work need perform establish large model optimise work paper future study literary md focus create large dataset different kind metaphor employ prompt engineering fine tune model deploy build interactive tool teach study metaphor educational setting,"[('dataset', 0.38610742192370356), ('md', 0.3186682800654876), ('transformer', 0.25227905505184434), ('metaphor', 0.24734612008003834), ('fine', 0.21621458608925442), ('sentence', 0.20293530247665248), ('al', 0.20267522831705878), ('setfit', 0.19916767504092975), ('llm', 0.18754029285103127), ('approach', 0.1736454357331907)]"
2025,DHd2025,FISCHER_Frank_Wikipedia_als_Hallraum_der_Kanonizit_t___1001_.xml,"Wikipedia als Hallraum der Kanonizität: ""1001 Books You Must Read Before You Die""","Jonas Rohe (Freie Universität Berlin, Deutschland); Viktor J. Illmer (Freie Universität Berlin, Deutschland); Lisa Poggel (Freie Universität Berlin, Deutschland); Frank Fischer (Freie Universität Berlin, Deutschland)","Wikipedia, Wikidata, Weltliteratur, Kanon, QRank","Bewertung, Daten, Literatur, Werkzeuge","Wikipedia-Sitelinks (auch: Interwiki-Links) sind Verbindungen zwischen einem Wikipedia-Artikel und den entsprechenden Artikeln in anderen Sprachversionen der Wikipedia. Diese Links ermöglichen es, schnell zwischen verschiedenen Sprachversionen eines Lemmas zu wechseln. Für unsere Untersuchung ist die Anzahl verschiedener Sprachversionen zu einem Lemma von besonderem Interesse, wie anhand eines Beispiels demonstriert werden soll. Derzeit gibt es aktive Wikipedia-Versionen für 331 verschiedene Sprachen (Wikipedia statistics 2024). Zum englischen Schriftsteller Charles Dickens gibt es etwa 162 Sitelinks, das heißt, es gibt über den Autor derzeit Artikel in 162 verschiedenen Sprachen in der Wikipedia. Zu seinem Roman ""A Tale of Two Cities"" gibt es momentan 51 Sitelinks (Wikidata 2024a). Im Weltliteratur-Diskurs verbreitet sich seit einiger Zeit die Idee, die Anzahl von Wikipedia-Sitelinks als Teil der ""Metrics of World Literature"" zu verwenden (Robinson 2017), als ""a simple measure of canonicity"" (Kukkonen 2020, S. 244; vgl. auch Fischer et al. 2023). Das bloße Vorhandensein von mehreren Sprachversionen kann dabei als Kanonizitätsmarker verstanden werden. Diese Idee greifen wir auf und möchten sie anhand eines konkreten Kanonprojekts entwickeln.  Diese Tabelle ist unser Ausgangspunkt für die Anreicherung mit Normdaten mithilfe von OpenRefine, das wir in der Version 3.8.2 genutzt haben. Über den Reconciling Service konnten wir alle Autor*innen und eine Großzahl der Werke mit ihren Wikidata-Einträgen verknüpfen. Insgesamt haben von den 1318 Werken bis dato 1257 einen Wikidata-Eintrag. Die mit Wikidata-IDs angereicherte Tabelle haben wir neben anderen Materialien und dem Code in unserem GitHub-Repositorium veröffentlicht (Rohe et al. 2024 bzw. Trotz der von Boxall im Vorwort zur zweiten Auflage angekündigten Diversifizierung des Kanons durch Erweiterung um nicht-englischsprachige Werke (vgl. Boxall 2008) wird nach Betrachtung der Geocodes offenkundig, dass sich die Auswahl weiterhin stark auf englischsprachige Texte konzentriert und sich insgesamt ein eurozentristischer Blick auf Weltliteratur manifestiert, der sich auch in den Neuausgaben nicht grundlegend ändert (Abb. 2). Es dominieren Texte aus Großbritannien (347), gefolgt von den USA (266), Frankreich (106), Deutschland (57) sowie Italien und Russland (je 30). Boxall schreibt: ""Does a body of writing, a canon of essential texts, emerge from a national context, or does it in some way transcend nationality, rising above the contexts that generate it? What does it mean to try to respond to all of these different national contexts at the same time? Is it possible to produce a list that can speak at once to the readers in Turkey and in Greece, in Serbia and Croatia?"" (Boxall 2008)  Nachdem die Zusammensetzung des Korpus beschrieben wurde, wenden wir uns nun der eigentlichen Analyse zu. Dafür betrachten wir die Wikipedia-Sitelinks und den QRank der aufgeführten Werke und Autor*innen als ""simple measure of canonicity"".  Werk- und Autor*innendaten sind somit mit zwei verwandten langfristigen ""Relevanzmaßen"" angereichert. Die vollständige Tabelle mit den Relevanzmaßen kann in unserem GitHub-Repositorium eingesehen bzw. live berechnet werden.  Im Gegensatz zu den Werken tun sich bei den Autor*innen keine Wikipedia-Lücken auf, alle haben mindestens einen zugehörigen Wikipedia-Artikel (Abb. 4). Die Mittelwerte liegen hier deutlich höher: Autor*innen haben im Durchschnitt 58,6 Sitelinks mit einem Medianwert von 48. Tabelle 1 zeigt für die Autor*innen die Korrelation zwischen QRanks und Sitelinks, konkret die zehn Autor*innen mit den höchsten QRanks. Rankt man die Top-10 nach Anzahl der Sitelinks, ergeben sich interessante Önderungen (Tab. 2). Hier befindet sich nur ein englischsprachiger Autor unter den ersten zehn. Für die Werke haben wir die QRank-Sitelink-Korrelation visualisiert (Abb. 5). Erwartungsgemäß zeigt sich ein positiver Zusammenhang zwischen der Anzahl an Sitelinks und dem QRank der Werke. Werke mit einer höheren Anzahl an Sitelinks weisen im Durchschnitt einen höheren QRank auf. Es wurde eine quadratische Regression durchgeführt, um den Zusammenhang zwischen der Anzahl an Werk-Sitelinks (x) und dem Werk-QRank (y) zu untersuchen. Diese konnte einen Wert von 0,72 für das Bestimmtheitsmaß R¬≤ erreichen, was darauf hinweist, dass 72¬†% der Varianz der QRank-Variable durch die Anzahl der Sitelinks und die quadratische Komponente erklärt werden.   Insgesamt hat sich gezeigt, dass sich Wikipedia und besonders die Wikipedia-Sitelinks gut eignen als ""Hallraum für Kanonizität"". Zu ausnahmslos allen Autor*innen sowie über 90% der Werke des Beispielkorpus gibt es mindestens einen Wikipedia-Artikel. Die Korrelation mit QRank hat gezeigt, dass es andere hilfreiche Ranking-Varianten gibt, die sich ebenso leicht heranziehen lassen. ",de,Verbindung entsprechend artikeln Sprachversion wikipedia links ermöglichen schnell verschieden Sprachversion Lemma wechseln Untersuchung Anzahl verschieden sprachversion Lemma besonderem Interesse anhand Beispiel demonstrieren derzeit aktiv verschieden Sprache wikipedia statistics englisch Schriftsteller Charles Dicken sitelinks Autor derzeit Artikel verschieden Sprache wikipedia Roman tale -- two cities momentan sitelinks Wikidata verbreiten Idee Anzahl metrics of world literature verwenden Robinson simple measure -- canonicity kukkon fisch et bloß Vorhandensein mehrere sprachversion Kanonizitätsmarker verstehen Idee greifen möchten anhand konkret Kanonprojekts entwickeln Tabell Ausgangspunkt Anreicherung normdat Mithilfe Openrefine Version nutzen Reconciling Service Großzahl Werk verknüpfen insgesamt werken dato angereichert Tabell Materialien Code unser veröffentlichen roh et trotz Boxall Vorwort Auflage angekündigt Diversifizierung Kanon Erweiterung werk Boxall Betrachtung Geocod offenkundig Auswahl weiterhin stark englischsprachig Text konzentrieren insgesamt eurozentristisch Blick Weltliteratur manifestieren Neuausgab grundlegend ändern abb dominieren Text Großbritannien folgen USA Frankreich Deutschland Italien Russland Boxall schreiben Does Body of Writing Canon -- essential texts emerge from national Context or does -- Some way transcend Nationality rising Above The Contexts that generate -- what does -- mean to try to respond to all -- These different national contexts at The Same time -- -- possible to Produce List thaen can Speak at once to -- readers Turkey And Greece Serbia and Croatia Boxall Zusammensetzung Korpus beschreiben wenden eigentlich Analyse betrachten qrank aufgeführt Werk simple measure -- canonicity somit verwandt langfristig relevanzmaß angereicheren vollständig Tabelle Relevanzmaße unser eingesehen live berechnen Gegensatz Werk mindestens zugehörig abb Mittelwerte liegen deutlich hoch Durchschnitt sitelinks Medianwert Tabelle zeigen Korrelation Qranks sitelinks konkret hoch qranks rankt Anzahl Sitelink ergeben interessant Önderungen tab befinden englischsprachig Autor Werk visualisiern abb erwartungsgemäß zeigen positiv Zusammenhang Anzahl Sitelinks qrank Werk Werk hoch Anzahl Sitelink weisen Durchschnitt hoch qrank quadratisch Regression durchführen Zusammenhang Anzahl x y untersuchen Wert Bestimmtheitsmaß erreichen hinweisen Varianz Anzahl Sitelink quadratisch Komponente erklären insgesamt zeigen wikipedia eignen hallraum Kanonizität ausnahmslos Werk Beispielkorpus mindestens Korrelation qrank zeigen hilfreich heranziehen lassen,"[('sitelinks', 0.30102357341202346), ('boxall', 0.24081885872961878), ('qrank', 0.24081885872961878), ('sprachversion', 0.2243045100188815), ('werk', 0.18217071007770283), ('does', 0.18061414404721407), ('to', 0.17667942576479007), ('wikipedia', 0.17526742561560726), ('anzahl', 0.17008657152344295), ('sitelink', 0.16822838251416114)]"
2025,DHd2025,JANNIDIS_Fotis_M_glichkeiten_und_Grenzen_der_KI_gest_tzten_A.xml,Möglichkeiten und Grenzen der KI-gestützten Annotation am Beispiel von Emotionen in Lyrik,"Merten Kröncke (Universität Würzburg, Deutschland); Fotis Jannidis (Universität Göttingen, Deutschland); Leonard Konle (Universität Göttingen, Deutschland); Simone Winko (Universität Würzburg, Deutschland)","Textannotation, Sprachmodelle, ChatGPT","Annotieren, Bewertung, Literatur, Forschungsprozess, Software"," Einleitung Die rasante Entwicklung der Verarbeitung natürlicher Sprache durch neuronale Netze hat in den letzten 11 Jahren auch die Arbeitsweisen der digitalen Geisteswissenschaften deutlich verändert. Die Entwicklung neuronaler Architekturen hat zwei Ansätze ermöglicht, die auch heute das NLP bestimmen: 1. ""Finetuning"": Ein Sprachmodell wird auf vielen Daten vortrainiert und dann auf deutlich weniger Daten für eine bestimmte Aufgabe feinjustiert. 2. ""Chat"": Sehr große Sprachmodelle werden auf sehr vielen Daten vortrainiert und dann in einem zweiten Schritt auf die Kommunikation mit Anwendern hin eingerichtet. Der Finetuning-Ansatz hat sich schnell in den Digital Humanities durchgesetzt. Allerdings ist er mit vergleichsweise hohen Kosten verbunden, da die Leistungsfähigkeit für das Finetuning stark mit der Anzahl der Trainingsbeispiele korreliert. Deswegen ist die Verwendung von sehr großen Sprachmodellen ohne eine größere Anzahl von Trainingsbeispielen (zero-shot oder few-shot prompting) besonders interessant, schließlich muss in einem solchen Kontext nur eine kleine Menge von Testdaten annotiert werden. Eine Antwort auf die Frage, ob in einem Forschungsprojekt der klassische Finetuning-Ansatz durch zero-shot oder few-shot prompting in sehr großen Sprachmodellen ersetzt werden kann, ist nicht einfach, da die Antwort von der Komplexität der Aufgabenstellung ebenso abhängt wie vom Zeitpunkt, zu dem man die Frage stellt: Die Finetuning-Ansätze entwickeln sich ebenso weiter wie die sehr großen Sprachmodelle. Dazu kommen pragmatische Fragen: Hat die Arbeitsgruppe Zugriff auf die technische Infrastruktur, die man für das Finetuning von großen Sprachmodellen benötigt? Hat sie die finanziellen Ressourcen, um die kommerziellen Modelle für umfangreiche Annotationsaufgaben zu verwenden? Unser Beitrag will eine (wenn auch nur temporär gültige) Antwort für einen bestimmten Bereich liefern, die Annotation von Emotionen in literarischen Texten, und dadurch zugleich an der Diskussion darüber mitwirken, wie in den DH eine Antwort auf jene Frage gefunden werden kann. Grundlage für unsere Arbeit sind die Annotationen von lyrischen Texten im Rahmen des DFG-Projekts The Beginnings of Modern Poetry (https://uni-goettingen.de/moderne-lyrik/). Die annotierten Texte haben wir drei großen Sprachmodellen mit der Aufgabe vorgelegt, jeweils eine Strophe mit Blick auf das Vorkommen von sechs Emotionsgruppen zu annotieren. Dabei haben wir nach einigen Vorstudien systematisch zwei Aspekte variiert: kurzer vs. langer Prompt und einfach randomisiertes vs. stratifiziert randomisiertes Sampling. Forschungsstand Die Möglichkeit, ChatGPT und verwandte Dienste zur Annotation von Daten zu verwenden, wurde sehr schnell erkannt. Ding et al. 2023 beobachten bei wenigen und klar definierten Labeln gute bis sehr gute Resultate, sehen allerdings auch eine deutliche Varianz abhängig vom Prompt. Öhnlich optimistische Ergebnisse haben Gilardi et al. 2023 erzielt. Törnberg 2023 vergleicht ChatGPT4s Annotationen von Tweets 'wird eine republikanische oder eine demokratische Position vertreten? 'mit denen von Expert:innen und von Arbeitern von Mechanical Turk und kommt zu dem Ergebnis, dass die Ergebnisse von ChatGPT deutlich besser und konsistenter sind als die der beiden menschlichen Gruppen. Reiss 2023 warnt allerdings davor, ChatGPT als Annotationswerkzeug ohne manuelle Datenvalidierung zu verwenden, da das System sehr empfindlich auf die Manipulation einzelner Wörter und Einstellungen reagiert. Rebora et al. 2023 vergleichen für die Aufgabe der Sentiment Analysis ChatGPT mit einem Finetuning-Modell und kommen zu dem Ergebnis, dass letzteres immer noch bessere Ergebnisse liefert (ähnlich Wang 2023). Die Diskrepanzen zwischen den Ergebnissen lassen sich durch drei Aspekte gut erklären: Wie schwierig ist die Aufgabe? Named Entity Recognition ist einfacher als Sentiment Analyse usw. Welches Modell wurde verwendet? Alle Aufsätze, die wir gesichtet haben, verwenden (auch) ChatGPT, aber OpenAI bietet zu einem Zeitpunkt unterschiedliche Modelle mit unterschiedlichen Leistungsniveaus an 'und die Modelle werden laufend aktualisiert. Was ist der Referenzpunkt des Vergleichs? Zum einen geht es um die Frage, ob man menschliche Annotator:innen durch große Sprachmodelle ersetzen kann, zum anderen darum, ob ChatGPT & Co. die Leistungsfähigkeit von Finetuning-Modellen erreichen.  Für die Promptgestaltung haben wir uns an den Empfehlungen von https://www.promptingguide.ai/ orientiert. Nach dem Überblick von Vatsal und Dubey 2024 gibt es keine klare Empfehlung zum Prompting bei Emotionsannotationen. Ressourcen Das Untersuchungskorpus besteht aus Texten in Lyrikanthologien, die sich auf Gedichte von Zeitgenoss:innen konzentrieren. Die Anthologien stammen aus der Zeit von 1859 bis 1919 und enthalten mehr als 6000 Gedichte, von denen 1412 manuell annotiert wurden (vgl. Winko et al. 2022a, Winko et al. 2022b).  Die Emotionsannotation zielt darauf ab, die im Text gestalteten Emotionen (und nicht die Emotionen der Leser:innen) zu erfassen. Möglich war, einer Textstelle genau eine, aber auch keine oder mehrere Emotionen zuzuweisen. Genutzt wurde ein Set von 40 diskreten Emotionen, darunter zum Beispiel Hoffnung, Sehnsucht oder Hass. Die Annotationseinheiten sind Wörter bzw. Wortfolgen (vgl. Kröncke et al. 2022). Da für viele einzelne Emotionen nur eine sehr geringe Zahl von Annotationen vorliegt, werden die Emotionen nachträglich zu sechs Gruppen zusammengefasst, orientiert an Shaver et al. 1987: Liebe (Love), Freude (Joy), Trauer (Sadness), Erregung/Überraschung (Agitation), Angst (Fear) und Wut (Anger).  Für das maschinelle Lernen wurde der Task leicht angepasst. Um die Komplexität der Aufgabe und den Rechenaufwand zu reduzieren, haben wir eine bestimmte Segmentierung vorgegeben, nämlich die Einheit ""Strophe"". Die Multi-Label-Klassifikation basiert auf dem (mithilfe einer großen Zahl manueller Annotationen trainierten) Modell SauerkrautLM-7B-HerO und wurde für die sechs Emotionsgruppen nach Shaver et al. 1987 durchgeführt. Sowohl das Korpus als auch das Annotationsverfahren und das maschinelle Lernen haben wir an anderen Stellen bereits ausführlicher erläutert (Konle et al. 2022, Konle et al. 2024).  Die folgenden Experimente basieren auf zwei Samples aus dem Gesamtkorpus: Zum einen verwenden wir ein einfach randomisiertes Sample (350 Strophen), das die im Korpus de facto vorhandenen Häufigkeitsverhältnisse der Emotionsklassen widerspiegeln soll, zum anderen ein stratifiziert randomisiertes Sample (ebenfalls 350 Strophen: 50 x jede der 6 Emotionsgruppen + 50 Strophen ohne Emotion), das eine gewisse Mindesthäufigkeit pro Emotionsgruppe garantiert, aber durch die Kookkurrenz von Emotionsgruppen ebenfalls nicht zu einer Gleichverteilung führt.1  Methoden In allen Experimenten lassen wir Chat-Modelle Fragen zu den Emotionen in lyrischen Texten beantworten. Der Task ist der gleiche, den bereits das Finetuning-Modell SauerkrautLM-7B-HerO übernommen hat, also die Zuweisung von keiner, einer oder mehreren der sechs Emotionsgruppen nach Shaver et al. 1987 zu einzelnen Strophen. Wir verwenden drei (kommerzielle) Modelle: GPT4o (OpenAI), Claude (Anthropic) und Gemini (Google).  In unseren Experimenten testen wir einen kurzen und einen langen Prompt. Der kurze Prompt enthält keine Erläuterungen der Emotionskonzepte und keine Annotationsbeispiele, der lange Prompt schon. Die Gestaltung des langen Prompts ist durch verschiedene Vorexperimente informiert. Ziel ist unter anderem, die (ansonsten zu große) Häufigkeit, mit der Emotionen zugewiesen werden, zu reduzieren. Wir setzen explizites CoT-Prompting ein und weisen darauf hin, dass hinreichend starke Emotionsindikatoren vorliegen müssen. Insgesamt ergeben sich vier Experimente: 	1.	Simple random Sampling. Short Prompt 	2.	Simple random Sampling. Long Prompt 	3.	Stratified random Sampling. Short Prompt 	4.	Stratified random Sampling. Long Prompt  Ergebnisse Tabelle 2 und 3 geben Auskunft über die Performance der Emotionserkennung. Für das Modell Claude 3.5 Sonnet können wir in der Variante Stratified random Sample 'Long Prompt noch keine Ergebnisse mitteilen, da unsere langen Prompts bei Anthropic wiederholt zur Überschreitung des rate limits geführt haben. Wir planen, das Experiment nachzuholen.  Diskussion Die Performance aller Short-Prompt-Modelle bleibt hinter den Finetuning-Ergebnissen zurück, wenn auch unterschiedlich deutlich. Andere Studien sind auf Basis anderer Daten und anderer Tasks zu ähnlichen Ergebnissen gekommen (etwa Rebora et al. 2023).  Die Long-Prompt-Modelle performen besser als die Short-Prompt-Modelle. Ein besonders deutliches Beispiel ist die Emotionsgruppe Agitation, die von den Short-Prompt-Modellen gar nicht erkannt wird, möglicherweise weil der Begriff in unserem Annotationsdesign ein spezifisches Konzept bezeichnet, das mit der alltagssprachlichen Bedeutung des Worts ""Agitation"" wenig gemein hat.2 In einigen Fällen reicht die Performance der besten Long-Prompt-Modelle fast an die Finetuning-Ergebnisse heran, z. B. im Fall von ""Love"", oder zieht gleich, etwa im Fall von ""Sadness"". Im Stratified random Sample performen die Modelle entweder ungefähr gleich gut oder deutlich besser (Anger, Fear) als im Simple random Sample. Relevant dafür ist allerdings, dass Anger und Fear im Simple random Sample nur selten vorkommen, weshalb die entsprechenden Ergebnisse nicht allzu belastbar sind. Zwischen den drei Modellen GPT4o, Gemini 1.5 und Claude 3.5 Sonnet zeigen sich je nach Sample und je nach Prompt einige Unterschiede. Im Simple random Sample ist die Performance von GPT4o oder Claude 3.5 Sonnet am besten, im Stratified random Sample von GPT4o (wobei hier für Claude 3.5 Sonnet in der Long-Prompt-Version keine Daten verfügbar sind). Die bisherigen Beobachtungen haben sich am F1-Score orientiert. Unterscheidet man Precision und Recall, werden weitere Befunde sichtbar. Das gilt nicht zuletzt für die Erkennung von solchen Strophen, die keine Emotion enthalten (vgl. Tabelle 4, exemplarisch für das Simple random Sample). Für alle Modelle und für alle Prompts gilt,3 dass bei der Erkennung von Emotionslosigkeit die Precision höher als der Recall ist. Der Befund hängt damit zusammen, dass die Modelle den Strophen häufiger Emotionen (bzw. seltener keine Emotionen) als menschliche Annotator:innen zuschreiben. Erklärungsrelevant dürfte sein, dass manuell ""sparsam"" annotiert werden sollte, auch mit Rücksicht auf das Inter-Annotator-Agreement. Die Modelle sind nicht an diese Annotationspraxis gebunden, wenngleich der lange Prompt sie (wie beabsichtigt) in diese Richtung zu lenken scheint, immerhin steigt hier der Recall, besonders bei GPT4o und Gemini 1.5. Das wichtigste Ergebnis unserer Studie ist, dass die sehr großen Sprachmodelle auch bei einer komplexen Aufgabe wie der Annotation von Emotionen teilweise das Niveau von Finetuning-Modellen erreichen, aber die Ergebnisse abhängig von der Kategorie stark und in schwer zu prognostizierender Weise schwanken. Die große und nur teilweise transparente Varianz in Abhängigkeit von der Promptgestaltung gilt es ebenfalls zu berücksichtigen. Zahlreiche weitere Studien sind denkbar. Aufschlussreich wäre, die Experimente auch für die 40 Einzelemotionen (statt nur für die 6 Emotionsgruppen) durchzuführen. Zudem lassen sich die Prompts anpassen, etwa insofern als den Modellen eine bestimmte Rolle zugewiesen wird (‚ÄúDu bist Expertin für ‚Ä¶‚Äù, ‚ÄúDu bist eine Person des 19. Jahrhunderts‚Äù usw.).4 Des Weiteren wäre zu testen, ob sich die Performance ändert, wenn der Task modifiziert oder anders modelliert wird, zum Beispiel inklusive Segmentierung (die Teil der manuellen Annotation ist) und/oder als Reihe binärer Klassifikationen. Die binäre Klassifikation haben wir exemplarisch getestet. Es zeigte sich, dass das Modell nun seltener (statt wie im bisherigen Setup häufiger) als menschliche Annotator:innen Emotionsgruppen zuweist.5 Der Befund deutet abermals auf die große Varianz des Modellverhaltens hin. Schließlich wäre informativ, das Inter-Annotator-Agreement zwischen menschlichen Annotator:innen mit dem Agreement zwischen Sprachmodellen zu vergleichen. Dass die Performanz je nach Kategorien und Prompts stark variiert, veranlasst uns zu folgendem Schluss: Auch wenn die Sprachmodelle ständig verbessert werden, wird man wohl auf absehbare Zeit nicht ohne die Entwicklung von Annotationsguidelines und die Annotation von ausreichend Testdaten auskommen.  Bibliographie Fügen Sie hier die von Ihnen benutzten Quellen ein:  Ding, Bosheng, Chengwei Qin, Linlin Liu, Yew Ken Chia, Shafiq Joty, Boyang Li und Lidong Bing. 2023. ‚ÄúIs GPT-3 a Good Data Annotator?‚Äù arXiv. http://arxiv.org/abs/2212.10450. Gilardi, Fabrizio, Meysam Alizadeh und Ma√´l Kubli. 2023. ‚ÄúChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks.‚Äù In Proceedings of the National Academy of Sciences 120. 30: e2305016120. https://doi.org/10.1073/pnas.2305016120. Konle, Leonard, Merten Kröncke, Fotis Jannidis und Simone Winko. 2022. ‚ÄúEmotions and Literary Periods.‚Äù In DH2022: Responding to Asian Diversity. Conference Abstracts, July 25‚Äì29, 2022, Tokyo, Japan, 278‚Äì281. Konle, Leonard, Merten Kröncke, Fotis Jannidis und Simone Winko. 2024. ‚ÄúOn the Unity of Literary Change. The Development of Emotions in German Poetry, Prose, and Drama between 1850 and 1920 as a Test Case.‚Äù In CHR 2024: Computational Humanities Research Conference, December 4‚Äì6, 2024, Aarhus, Denmark [eingereicht]. Kröncke, Merten, Fotis Jannidis, Leonard Konle und Winko, Simone. 2022. ‚ÄúAnnotationsrichtlinien Emotionsmarker und Emotionen.‚Äù Zenodo. https://doi.org/10.5281/zenodo.6021152. Rebora, Simone, Marina Lehmann, Anne Heumann, Wei Ding und Gerhard Lauer. 2023. ‚ÄúComparing ChatGPT to Human Raters and Sentiment Analysis Tools for German Children""s Literature.‚Äù In CHR 2023: Computational Humanities Research Conference, December 6‚Äì8, 2023, Paris, France, 333‚Äì343. Reiss, Michael V. 2023. ‚ÄúTesting the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark‚Äù. arXiv. https://doi.org/10.48550/arXiv.2304.11085. Shaver, P., J. Schwartz, D. Kirson und C O""Connor. 1987. ‚ÄúEmotion Knowledge: Further Exploration of a Prototype Approach.‚Äù In Journal of Personality and Social Psychology 52.6: 1061‚Äì1086.¬† Törnberg, Petter. 2023. ‚ÄúChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning‚Äù. arXiv. https://doi.org/10.48550/arXiv.2304.06588. Vatsal, Shubham und Harsh Dubey. 2024. ‚ÄúA Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks.‚Äù arXiv. https://doi.org/10.48550/arXiv.2407.12994. Winko, Simone, Konle, Leonard, Kröncke, Merten und Fotis Jannidis. 2022a. ‚ÄúLyrik-Anthologien 1850-1910.‚Äù Zenodo. https://doi.org/10.5281/zenodo.6053952. Winko, Simone, Konle, Leonard, Kröncke, Merten und Fotis Jannidis. 2022b. ‚ÄúKorpusbeschreibung der Lyrik-Anthologien 1850-1910.‚Äù Zenodo. https://doi.org/10.5281/zenodo.6204787. ",de,Einleitung rasant Entwicklung Verarbeitung natürlich Sprache neuronal Netz letzter Arbeitsweise digital geisteswissenschaften deutlich verändern Entwicklung neuronaler architekturen Ansatz ermöglichen Nlp bestimmen Finetuning Sprachmodell daten vortrainieren deutlich daten bestimmt Aufgabe feinjustieren chat Sprachmodelle daten vortrainieren Schritt Kommunikation Anwender einrichten schnell Digital Humanitie durchsetzen vergleichsweise hoch Kosten verbinden Leistungsfähigkeit Finetuning stark Anzahl Trainingsbeispiel korrelieren Verwendung Sprachmodelle groß Anzahl Trainingsbeispiel Prompting interessant schließlich Kontext Menge Testdat annotiert Antwort Frage Forschungsprojekt klassisch Prompting sprachmodellen ersetzen einfach Antwort Komplexität Aufgabenstellung abhängen Zeitpunkt Frage stellen entwickeln sprachmodelle pragmatisch Frage Arbeitsgruppe Zugriff technisch infrastruktur Finetuning Sprachmodelle benötigen finanziell Ressource kommerziell Modell umfangreich Annotationsaufgabe verwenden Beitrag Temporär gültig Antwort bestimmt Bereich liefern Annotation Emotion literarisch Text Diskussion mitwirken dh Antwort Frage finden Grundlage Arbeit annotatio lyrisch Text Rahmen The beginning of modern Poetry annotiert Text sprachmodellen Aufgabe vorlegen jeweils Strophe Blick vorkommen Emotionsgruppe annotieren Vorstudi systematisch Aspekt variieren kurz lang prompt einfach randomisiert stratifizieren randomisiert Sampling Forschungsstand Möglichkeit Chatgpt verwandte dienste Annotation daten verwenden schnell erkennen Ding et beobachten weniger klar Definiert Label resultat sehen deutlich Varianz abhängig prompt öhnlich optimistisch Ergebnis Gilardi et erzielen törnberg vergleichen annotation Tweet republikanisch demokratisch Position vertreten experen innen Arbeiter Mechanical Turk Ergebnis Ergebnis Chatgpt deutlich Konsistenter menschlich Gruppe Reiss warnen Chatgpt Annotationswerkzeug Manuelle Datenvalidierung verwenden System empfindlich Manipulation einzeln Wörter Einstellung reagieren Rebora et Vergleich Aufgabe Sentiment Analysis Chatgpt Ergebnis letzterer gut Ergebnis liefern ähnlich wang Diskrepanz Ergebnis lassen Aspekt erklären schwierig Aufgabe named entity Recognition einfach Sentiment Analyse Modell verwenden Aufsatz sichten verwenden Chatgpt Openai bieten Zeitpunkt unterschiedlich Modell unterschiedlich Leistungsniveau Modell laufend aktualisiert Referenzpunkt vergleichs Frage menschlich Annotator innen Sprachmodelle ersetzen Chatgpt Leistungsfähigkeit erreichen Promptgestaltung Empfehlung orientieren Überblick vatsal Dubey klar Empfehlung Prompting Emotionsannotation Ressource Untersuchungskorpus bestehen Text Lyrikanthologien Gedicht Zeitgenoss innen konzentrieren Anthologie stammen enthalten gedichen manuell annotiert Winko et Winko et Emotionsannotation zielen Text gestaltet Emotion Emotion Leser innen erfassen Textstelle genau mehrere Emotion zuzuweisen nutzen set Diskret Emotion Hoffnung Sehnsucht Hass Annotationseinheit Wörter Wortfolge Kröncke et einzeln Emotion gering Zahl annotatio vorliegen Emotion nachträglich Gruppe zusammengefassen orientieren shav Et liebe love Freude joy Trauer sadness Erregung überraschung Agitation Angst Fear Wut Anger maschinell lernen Task angepasst Komplexität Aufgabe Rechenaufwand reduzieren bestimmt Segmentierung vorgeben nämlich Einheit Strophe basieren Mithilfe Zahl manuell annotation trainierten Modell Emotionsgruppe shav Et durchführen sowohl Korpus annotationsverfahren Maschinell lernen Stelle ausführlich erläutern Konle Et Konle Et folgend experiment Basier Samples Gesamtkorpus verwend einfach randomisiert Sample strophen Korpus -- -- vorhanden Häufigkeitsverhältnisse Emotionsklasse widerspiegeln Stratifiziert randomisiert Sample ebenfalls strophen x Emotionsgruppe Strophe Emotion gewiß Mindesthäufigkeit pro Emotionsgruppe garantieren Kookkurrenz Emotionsgruppe ebenfalls Gleichverteilung Methode experimenten lassen fragen Emotion lyrisch Text beantworten Task gleich übernehmen Zuweisung mehrere Emotionsgruppe shav Et einzeln strophen verwenden kommerziell Modell openai Claude Anthropic Gemini Google unser experimenten test kurz lang prompt Kurze prompt enthalten Erläuterung Emotionskonzepte Annotationsbeispiel prompt Gestaltung lang Prompt verschieden Vorexperiment informieren Ziel ansonsten häufigkeit Emotion zuweisen reduzieren setzen explizit weisen hinreichend stark emotionsindikatoren vorliegen insgesamt ergeben experiment Simple Random Sampling Short prompt Simple Random Sampling Long prompt Stratified Random Sampling Short prompt Stratified Random Sampling Long prompt ergebniss Tabell geben Auskunft Performance Emotionserkennung Modell claude sonnen variante Stratified Random Sample long prompt Ergebnis mitteilen lang Prompt Anthropic wiederholt überschreitung Rate Limit führen planen Experiment nachholen Diskussion Performance bleiben unterschiedlich deutlich Studie Basis anderer daten anderer Tasks ähnlich Ergebnis kommen Rebora et Performe deutlich Emotionsgruppe Agitation erkennen möglicherweise Begriff unser annotationsdesign spezifisch Konzept bezeichnen alltagssprachlich Bedeutung Wort Agitation gemein Fall reichen Performance fast heran Fall love ziehen Fall sadness Stratified random Sample performen Modell ungefähr deutlich Anger Fear Simple Random Sample relevant Anger Fear Simple Random Sample selten vorkommen weshalb entsprechend Ergebnis allzu belastbar Modell Gemini Claude sonnen zeigen Sample prompt Unterschied Simple Random Sample Performance Claude sonnen Stratified Random Sample wobei Claude sonnen daten verfügbar bisherig Beobachtunge orientieren unterscheiden Precision Recall befunde sichtbar gelten zuletzt Erkennung strophen Emotion enthalten Tabell exemplarisch simpel Random Sample Modell prompt Erkennung Emotionslosigkeit Precision hoch Recall Befund hängen Modell strophen häufig Emotion selten Emotion menschlich Annotator innen zuschreiben erklärungsrelevant dürfen manuell sparsam annotiert Rücksicht Modell Annotationspraxis binden wenngleich prompen beabsichtigen Richtung lenken scheinen immerhin steigen Recall Gemini wichtig Ergebnis Studie sprachmodelle komplex Aufgabe Annotation Emotion teilweise Niveau erreichen Ergebnis abhängig Kategorie stark schwer prognostizierend Weise schwanken teilweise transparent Varianz Abhängigkeit Promptgestaltung gelten ebenfalls berücksichtigen zahlreich Studie denkbar aufschlussreich experiment einzelemotion Emotionsgruppe durchführen zudem lassen prompts anpassen insofern modellen bestimmt Rolle zuweisen äúdu Expertin äúdu Person Jahrhundert äù testen Performance ändern Task modifizieren modellieren inklusive Segmentierung manuell Annotation Reihe binär klassifikationen binär Klassifikation exemplarisch testen zeigen Modell selten bisherig setup häufig menschlich Annotator innen emotionsgrupp Befund deuten abermals Varianz Modellverhalten schließlich informativ menschlich Annotator innen Agreement Sprachmodelle vergleichen Performanz kategorien Prompt stark variieren veranlassen folgend schluss Sprachmodelle ständig verbessern absehbar Entwicklung annotationsguidelin Annotation ausreichend testdaten auskommen Bibliographie fügen benutzt quellen Ding Bosheng Chengwei qin Linlin Liu yew Ken Chia Shafiq Joty Boyang Li Lidong Bing äúis Good data arxiv Gilardi Fabrizio meysam alizadeh Kubli äúchatgpt Outperform for proceedings of the national academy -- sciences Konle Leonard meren Kröncke Foti Jannidis Simone Winko äúemotions and literary responding to asian Diversity Conference abstracts july Tokyo Japan Konle Leonard meren Kröncke Foti Jannidis Simone Winko äúon the unity -- literary Change The development of emotions German Poetry Prose and Drama between and as tesen Chr computational humanities research conference december aarhus Denmark einreichen Kröncke meren Foti Jannidis Leonard Konle Winko Simone äúannotationsrichtlinien emotionsmark Zenodo Rebora Simone Marina Lehmann Anne Heumann wei Ding Gerhard Lauer äúcomparing Chatgpt to human raters and Sentiment Analysis Tools for German children -- Chr computational humanities research conference december Paris France Reiss Michael äútesting -- reliability of Chatgpt for Text Annotation and classification Cautionary remark äù arxiv shav Schwartz Kirson c o Connor äúemotion knowledge furth Exploration of prototype journal -- personality and social Psychology Törnberg Petter Outperform experts and Crowd workers Annotating political Twitter messag with Learning äù arxiv Vatsal Shubham Harsh Dubey äúa survey -- prompt engineering Methods Large Language models for different nlp arxiv Winko Simone Konle Leonard Kröncke Mert Foti Jannidis Zenodo Winko Simone Konle Leonard Kröncke Mert Foti Jannidis Äúkorpusbeschreibung Zenodo,"[('prompt', 0.3138502512722655), ('emotion', 0.22213868157752378), ('emotionsgruppe', 0.19092965375964355), ('chatgpt', 0.19092965375964355), ('random', 0.18909510854892855), ('sample', 0.17511622073694852), ('kröncke', 0.15374009191400623), ('simone', 0.15374009191400623), ('konle', 0.15156728783899615), ('sprachmodelle', 0.1491889257390102)]"
