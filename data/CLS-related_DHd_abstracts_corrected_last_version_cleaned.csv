year,conference,filename,title,authors,keywords,topics,abstract
2014,DHd2014,2014_cls_metadata_extracted.csv,A Flexible NLP Pipeline for Computational Narratology,"Thomas Bögel (Heidelberg University, Germany); Jannik Strötgen (Heidelberg University, Germany); Christoph Mayer (Heidelberg University, Germany); Michael Gertz (Heidelberg University, Germany)","NLP, Pipeline, Narratologie, Computational","NLP, Pipeline, Narratologie, Computational","1 Project Overview Temporal dependencies reveal interesting insights into the semantic discourse structure of narrative texts. The investigations of literary scientists are, as of today, mostly based on labor-intensive manual annotations. Computational Narratology, an important subtopic of the Digital Humanities, aims at facilitating annotations and supporting literary scientists with their analyses. According to Mani (2013), one aspect of Computational Narratology focuses on exploring and testing literary hypotheses through mining narrative structures from corpora. In the context of the BMBF-funded eHu¬¥ manities project heureCL EA, we address temporal phenomena in literary text, a genre whose temporal phenomena are different from others. For example, it is often not possible to anchor temporal expressions to real points in time, but literary texts tend to have their own time frame. Our project partners, as well as many other humanists, use CATMA, a comprehensive graphical tool for annotating data. Interfacing NLP with CATMA could drastically reduce the effort of manual annotation. The goal of ¬¥ heureCL EAis to provide users with a collaborative annotation environment for tagging temporal phenomena in documents, with simple annotations (e.g., temporal expressions) being added automatically, and more complex annotations (e.g., time shifts and ellipses) being suggested. Users can correct automatic annotations, and user feedback will be used to apply machine learning techniques to improve future annotation suggestions. In the following, we outline our flexible architecture for NLP in the domain of narrative texts, as well as promising first results for annotating the tense of sub-sentences to demonstrate the effectiveness of our approach. 2 Architecture and Components ¬¥ The heureCL EAcorpus currently consists of more than 20 mostly German narrative texts from various authors of the 20th century. Due to the diversity of style and text characteristics, applying NLP is challenging as most systems are optimized for factual texts characterized by stable structures. Automatically generating annotations that are related to temporal structures of texts requires information on multiple levels of the linguistic processing stack. Thus, we implemented a modular pipeline that performs annotations with increasing levels of complexity and allows for easy adaptation and exchange of different base components. We use standard off-the-shelf tools that are freely available. In order to achieve maximum flexibility and to allow for easily substitutable individual components, the pipeline is implemented as a UIMA architecture. The general pipeline architecture is shown in Fig. 1. We distinguish between general preprocessing components that are required for all subsequent narratological annotation tasks and individual machine learning modules (red background) that are tailored to one specific target annotation. We are planning to release the preprocessing stack to the research community to allow all CATMA users to perform basic NLP analyses and annotations. 2.1 Component Overview 2.1.1 CATMA Interface The texts in our corpus are annotated by literary scientists with CATMA, a web-based collaborative ¬¥ Figure 1: Overview of the NLP architecture in heureCL EA. annotation tool that offers humanists an easy way to create stand-off annotation and share their annotation with other scholars. In order to work with annotated data in CATMA, we implemented a component that interfaces CATMA with our UIMA pipeline. As CATMA is a popular tool in the humanities, we developed the interface as a stand-alone component that can easily be used by others to combine the strengths of CATMA as an annotation framework with the analytical and predictive power of UIMA pipelines. The interface is geared to literary scientists with no knowledge of programming. To achieve a simple configuration, the user only has to specify mappings between CATMA and UIMA types in a single XML file. By providing an easy-to-use interface, we want to lower the bar for other projects in the Humanities to employ simple yet effective NLP tools and thereby alleviate manual annotations. 2.1.2 Linguistic Processing Our linguistic preprocessing stack consists of state-of-the-art components for German NLP. For sentence segmentation and part-of-speech tagging, we use the TreeTagger (Schmid, 1995). While the tree tagger provides a basic analysis of tokens, it does not extract morphological information; thus, we added Morphisto (Piskorski, 2009) as a separate component for morphological analysis. Two different parsers in our pipeline provide valuable information for detecting sub-sentences and dependency relations between tokens (e.g., to extract the subject of a certain verb): the Stanford constituent parser (Rafferty and Manning, 2008) and ParZu (Sennrich et al., 2009), a dependency parser trained on the T¬® uBaD/Z. Finally, HeidelTime (Strötgen and Gertz, 2013) extracts and normalizes temporal expressions in the text which will be used in later stages of the processing pipeline. 2.1.3 Machine Learning After the data has been annotated by the above preprocessing components, it is passed to different modules that handle the annotation of specific narratological aspects of texts (e.g., the extraction of tense clusters of time shifts). Depending on the target annotation, we employ different machine learning approaches and heuristics. Verb tenses are an example for such a relevant annotation because shifts in the verb tense of a sentence can, for instance, indicate narratological order phenomena (e.g., prolepsis). To extract tense clusters, we implemented and evaluated a robust heuristic component with promising results. Detailed evaluation results of the prediction performance based on a comparison to manually annotated data will be presented in the poster. References Mani, I. (2013, October). Computational narratology. the living handbook of narratology. http://www. lhn.uni-hamburg.de/article/ computational-narratology. Piskorski, J. (2009). Morphisto-An Open Source Morphological Analyzer for German. Finitestate Methods and Natural Language Processing: Post-proceedings of the 7th International Workshop FSMNLP 191. Rafferty, A. N. and C. D. Manning (2008). Parsing three german treebanks: Lexicalized and unlexicalized baselines. In Proceedings of the Workshop on Parsing German, PaGe ""08, Stroudsburg, PA, USA, pp. 40–46. Association for Computational Linguistics. Schmid, H. (1995). Improvements in part-of-speech tagging with an application to german. In In Proceedings of the ACL SIGDAT-Workshop, pp. 47'50. Sennrich, R., G. Schneider, M. Volk, and M. Warin (2009). A new hybrid dependency parser for german. Proc. of the German Society for Computational Linguistics and Language Technology, 115–124. Strötgen, J. and M. Gertz (2013). Multilingual and cross-domain temporal tagging. Language Resources and Evaluation 47(2), 269‚Äî-298."
2014,DHd2014,2014_cls_metadata_extracted.csv,"Zwischen Ton und Textgenese: Digital gestützte Verfahren zur kritischen Edition von Operntexten in der ""Online-Edition der Libretti zu Mozarts Opern""","Iacopo Cividini (Stiftung Mozarteum Salzburg, Austria); Adriana De Feo  (Stiftung Mozarteum Salzburg, Austria); Franz Kelnreiter (Stiftung Mozarteum Salzburg, Austria)","Onlineedition, Digitale Edition","Onlineedition, Digitale Edition","Als Bestandteil einer multimedialen Gattung bezieht der Text einer Oper mehrere quellenkundliche, sprachwissenschaftliche und musikalische Ebenen mit ein, die in einem Druckmedium nur teilweise sinnvoll dargestellt werden können. Die Online-Edition der Libretti zu Mozarts Opern, die voraussichtlich im Januar 2014 im Rahmen der Digitalen Mozart-Edition erscheinen wird, bietet die Möglichkeit, all diese Dimensionen durch digital gestützte Verfahren nach wissenschaftlichen Kriterien synchron darzustellen. Am Beispiel der Texte zu Wolfgang Amadé Mozarts Le nozze di Figaro wird gezeigt, wie Fassungen und Varianten parallel ediert werden, um einen direkten Vergleich der verschiedenen Quellenstränge zu ermöglichen. Durch die diplomatische Übertragung der Quellen mit Markierung aller Abweichungen zu den edierten Textfassungen lässt sich darüber hinaus jede einzelne editorische Entscheidung zum ersten Mal direkt zurückverfolgen. Als Verbindungsglied zur geplanten digitalen Edition des Notentextes wird schließlich mit einer metrischen Analyse des vertonten Textes ein Instrumentarium zur Erforschung der komplexen Beziehung zwischen dem metrischen Duktus der Textvorlage und dessen musikalischer Umsetzung zur Verfügung gestellt. Ausstattung: Laptop mit Internet-Zugang und der Möglichkeit, Musik abzuspielen, Beamer. CURRICULUM VITAE Iacopo Cividini (Projektverantwortlicher), geboren 1975 in Bergamo (Italien), Studium der Musikwissenschaft und Geschichte an der Universit√† degli Studi di Pavia (Italien), an der Johannes Gutenberg-Universität Mainz und an der University of Oregon (USA); Promotion in Musikwissenschaft an der Ludwig-Maximilians-Universität München im Jahre 2005 mit einer Arbeit über die Solokonzerte von Anton√≠n Dvo_√°k. Wissenschaftlicher Mitarbeiter am DFG-Projekt Bayerisches Musiker-Lexikon Online an der LMU München von 2005 bis 2007 und an der Digitalen Mozart-Edition (DME) bei der Internationalen Stiftung Mozarteum Salzburg seit 2007. Hauptforschungsgebiete: Instrumentalmusik des 19. Jahrhunderts, Bayerische Musikgeschichte, Libretti-Forschung, Philosophie der Aufklärung. Wichtigste Publikationen: Die Solokonzerte von Anton√≠n Dvo_√°k. Eine Lösung der Konzertproblematik nach Beethoven, 2007; Aufsätze zur Musik des 18. und 19. Jahrhunderts (u.a. Mozart, Brahms, Dvo_√°k). Digitale Projekte: Bayerisches Musiker-Lexikon Online, Online-Katalog der Libretti zu Mozarts Opern, Online-Edition der Libretti zu Mozarts Opern (geplante Veröffentlichung: Januar 2014). Adriana De Feo, geboren 1980 in Salerno (Italien), studierte Kunst-, Musik- und Theaterwissenschaften an der Universität Bologna und schloss ihr Studium mit einer Arbeit über Mozart in Mailand ab. 2012 promovierte sie in Musikwissenschaft an der Universität Mozarteum Salzburg über das Thema Mozarts Serenate im Spiegel der Gattungsentwicklung. Zu Ihren Forschungsschwerpunkten zählen die Huldigungsoper des 17. und 18. Jahrhunderts und die Libretti-Forschung. Aufsätze zur Musikdramaturgie des Barock und der Klassik. Als Wissenschaftliche Mitarbeiterin der Stiftung Mozarteum Salzburg (seit 2009) erarbeitet sie im Rahmen der Digitalen Mozart-Edition (DME) die Online-Edition der Libretti und den Online-Katalog der Libretti zu Mozarts Opern. Franz Kelnreiter Studium der Musikwissenschaft und Romanistik (Französisch) an der Paris-Lodron-Universität sowie der Kirchenmusik an der Hochschule Mozarteum in Salzburg. Seit 1994 an der Internationalen Stiftung Mozarteum, zunächst als Leiter der Mozart Ton- und Filmsammlung, seit 2002 Mitarbeiter an der Digitalen Mozart-Edition und Leiter des IT-Bereichs. Technische Projektbetreuung."
2014,DHd2014,2014_cls_metadata_extracted.csv,AAC-FACKEL. Das Beispiel einer digitalen Musteredition.,"Hanno Biber (Österreichische Akademie der Wissenschaften, Österreich)","Digitale Edition, Korpus, XML, OCR, PoS-tagging, Lemmatisierung, Webinterface","Digitale Edition, Korpus, XML, OCR, PoS-tagging, Lemmatisierung, Webinterface","Die AAC-FACKEL wurde unter Anwendung corpuslinguistischer und texttechnologischer Methodologien im Rahmen des an der Österreichischen Akademie der Wissenschaften gestarteten Forschungsprogrammes ""AAC-Austrian Academy Corpus"" als digitale Musteredition eines literaturgeschichtlich überaus bedeutenden Textes konzipiert und online gestellt. 1 Das AAC ist ein Textcorpus zur deutschen Sprache zwischen 1848 und 1989, mit dem philologische Grundlagenforschung im noch relativ jungen Paradigma der computergestützten Textwissenschaften geleistet werden kann. Im Folgenden sollen als exemplarischer Anwendungsfall die aus der Corpusforschung resultierende digitale Musteredition, ihr Zustandekommen und die dafür notwendigen Bedingungen einer sich mit der Sprache und mit Fragen des Sprachgebrauchs auf empirischer Textbasis befassenden Forschungsrichtung, beschrieben werden. Seit der Veröffentlichung der AAC-FACKEL, der digitalen Version der von Karl Kraus vom 1. April 1899 bis Februar 1936 in Wien herausgegeben satirischen Zeitschrift ""Die Fackel"", haben sich bis Jahresende 2013 mehr als 25.000 Benutzer auf der Website registriert, wo die bereitgestellten Daten sowohl von den von spezifischen Text- und Sprachinteressen geleiteten Wissenschaftlerinnen und Wissenschaftlern erforscht, als auch von allgemein an der Sprache und Literatur interessierten Leserinnen und Lesern aus aller Welt vielfältig genutzt werden. Das nach den für diese Edition entwickelten Prinzipien zur Funktionalität digitaler Textressourcen und von erforderlichen Überlegungen zum grafischen Design bestimmte Interface der AAC-FACKEL ermöglicht den Benutzern, die digital aufbereiteten Texte 1 AAC - Austrian Academy Corpus: AAC-FACKEL. Online Version: ""Die Fackel. Herausgeber: Karl Kraus, Wien 1899-1936"" AAC Digital Edition No. 1 (Hg. Hanno Biber, Evelyn Breiteneder, Heinrich Kabas, Karlheinz Mörth), http://www.aac.ac.at/fackel sowohl lesen, als auch in komplexer Weise ihre Formen untersuchen und analysieren, sowie einfach in ihnen nach sprachlichen Einheiten und deren Eigenschaften suchen zu können. Das Werk von Karl Kraus ist als bedeutender Beitrag der deutschsprachigen Literatur zur Weltliteratur zu betrachten und seine satirischen und polemischen Texte sind von thematischer Vielfalt, sprachlicher Komplexität und historischer Relevanz, weshalb ihre Überlieferung im digitalen Medium als unerlässlich erachtet werden kann. In der digitalen Edition der AAC-FACKEL wird die einzigartige sprachliche, literarische und satirische Qualität der Texte unter Nutzung texttechnologischer Instrumente durch verschiedene Suchmöglichkeiten und Register erschließbar gemacht. Neben der Volltextsuche und den Wortformen-Registern mit ungefähr 6 Millionen Wortformen bietet die AAC-FACKEL ein vollständiges, erstmals publiziertes Inhaltsverzeichnis sämtlicher Texte der Zeitschrift, das unter Nutzung informationstechnologischer Verfahren für die digitale Edition neu erstellt wurde. Dabei wurden sowohl die Angaben von Karl Kraus in den Überschriften der einzelnen Beiträge, bzw. von den Textanfängen und den Inhaltsangaben der Hefte, als auch jene Inhaltsverzeichnisse berücksichtigt, die vom Herausgeber nachträglich für die Quartalsbände erstellt wurden. Die vollständige Bild-Beigabe aller 22.586 Textseiten als Faksimiles ermöglicht den Nutzern die quelleneditorisch korrekte Zitierung der Texte in den 415 Heften bzw. 922 Nummern der 37 Jahrgänge der Zeitschrift. Es ist geplant, neue Funktionen wie etwa ein auf einer im AAC erstellten und bearbeiteten Namendatenbank der ""Fackel"" beruhendes Personennamenregister sowie ein Verzeichnis der Varianten der Hefte der Zeitschrift in einer neuen Version der AAC-FACKEL zu implementieren. Für die im AAC konzipierten digitalen Editionen wurde im Rahmen der texttechnologischen Forschungen ein spezifisches von Anne Burdick gestaltetes Navigationsmodul für Zeitschriften und andere Textformen entworfen, mit dessen Hilfe nicht nur von Seite zu Seite, von Heft zu Heft oder von Jahrgang zu Jahrgang navigiert werden kann, sondern auch zu im jeweiligen Zusammenhang relevanten Textpassagen. Die besondere grafische Umsetzung im Webinterface, im Bereich des Inhaltsverzeichnisses, kompensiert in diesen Fällen das Fehlen der ertastbaren physischen Objekteigenschaften und visuellen Informationen, die sonst nur durch die Wahrnehmung der Präsenz der gedruckten Zeitschrift gegeben sind. Die digitale Edition bietet auf diese Weise eine optisch wahrzunehmende Repräsentation der sonst in den von Papierqualität, -volumen, Druck- und Bindungstechnik bestimmten Eigenschaften eines Druckwerkes. In der AAC-FACKEL ist ein linguistisches Suchmodul eingerichtet den an den Texten in besonderer Weise interessierten Lesern ermöglicht, corpusbasierte Abfragen vorzunehmen und die Basisfunktionalität von mit linguistischen Tags versehenen Ressourcen zu nutzen und so die mit Part-of-speech- und Lemma-Informationen versehenen Text zu untersuchen. Die im AAC erstellten Prototypen geben Antwort auf die Frage, wie komplex organisierte Texte und historische literarische Zeitschriften mit einem Inventar von literarischen Formen und sprachlichen Eigenschaften in einer adäquaten und funktionellen Form im digitalen Medium so wiedergegeben werden, dass sie unter Nutzung der texttechnologischen Möglichkeiten vom wissenschaftlichen Nutzer wie auch vom interessierten Laien im neuen Medium, in das die Texte gleichsam übersetzt werden müssen, neu gelesen, interpretiert und analysiert werden können. Die zentrale Forschungsperspektive neben der Fragestellung nach den Steuerungsvorgängen und der damit verbundenen Reinterpretation und adäquaten Präsentation der Ausgangsmaterialien derartiger digitaler Editionen (scan-images, OCR-text, xml-tags, linguistic tagging, structural tagging, database-organisation etc.) liegt in der optimalen Nutzung der durch die informationstechnologische Aufbereitung der Texte gegebenen Such- und Indizierungsverfahren sowie den verschiedenen, dadurch eröffneten Zugangsmöglichkeiten zum Text sowie zu einzelnen Elementen der sprachlich, textlich und durch beschriebene Textstrukturen der Publikationsobjekte organisierten Bestände. Eine dritte zentrale corpusrelevante Forschungsperspektive liegt in der methodisch-theoretischen Reflexion über die sich durch die Kombination von editionsphilologischen Fragestellungen mit Fragen der Informationstechnologie, der Text-Technologie und der Corpus-Forschung sowie dem Interface Design sich ergebenden Konsequenzen und Forschungsansätze."
2014,DHd2014,2014_cls_metadata_extracted.csv,Eine digitale Ausgabe des Welschen Gastes als Chance,Jakub Simek,"stylometrische Analyse, Visualisierungen, Netzwerk, XML/TEI","stylometrische Analyse, Visualisierungen, Netzwerk, XML/TEI","Eine digitale Ausgabe des ""Welschen Gastes"" als Chance für neue Analyse- und Visualisierungsmethoden Die seit 2011 im Rahmen des Heidelberger Sonderforschungsbereichs 933 ""Materiale Textkulturen"" entstehende digitale Neuausgabe des mittelhochdeutschen Text-Bild-Gedichts ""Welscher Gast"" Thomasins von Zerklaere kodiert Volltranskriptionen der Handschriften und editorisch hergestellte Texte im XML/TEI-Format und legt damit verknüpfte Bildannotationen in einer relationalen Datenbank ab. Das Ziel des Projekts sind eine Online-Edition des gesamten textuellen und bildlichen Materials, welche die komplexen Text-Bild-Beziehungen im Werk dokumentiert und mit mächtigen Visualisierungsmechanismen anschaulich präsentiert, sowie mehrere Buchausgaben mit jeweils abgestufter Komplexität und unterschiedlichem Zielpublikum. Die Text- und Bilddaten sollen leicht vergleichbar und durchsuchbar gemacht werden. Für die stemmatologische Analyse der transkribierten, lemmatisierten und alinierten Textdaten machen wir uns phylogenetische Software aus der Bioinformatik zunutze, die es erlaubt, mutmaßliche Verwandschaftsbeziehungen zwischen den überlieferten Handschriften in gewurzelten Baumgraphen und ungewurzelten Netzwerken darzustellen. Die so gewonnenen Erkenntnisse setzen wir in der digitalen Textausgabe um, um nicht nur herkömmliche Synopsen zu generieren, sondern auch neue Formen des kritischen Apparats zu erproben. Eine Funktion, die wir als ""Baumapparat"" bezeichnen, wird dem Benutzer, der entweder den kritisch hergestellten Text oder eine Handschriftentranskription liest, beim Überfahren eines Wortes mit der Maus ein dynamisch generiertes Baumdiagramm zeigen, das dem mutmaßlichen Handschriftenstemma für die jeweilige Textstelle entspricht und an dessen ""Östen"" die Lesarten fürs jeweils ausgewählte Wort sichtbar gemacht werden. Durch farbige Unterlegung wird den angezeigten Lesarten zudem deren semantische Relevanz zugewiesen. Auf diese Weise wird der Benutzer die angezeigten Lesarten unmittelbar und intuitiv ins Stemma einordnen können. Einen anderen Visualisierungsansatz erproben wir bei der stilometrischen Wortschatz- und Reimanalyse. Die im Text am häufigsten vorkommenden Wort- und Reimkombinationen stellen wir in einem Netzwerk dar, das einen schnellen Eindruck über die sprachlichen Eigenschaften des Werkes vermittelt. Die in den Handschriften enthaltenen Miniaturzeichnungen und -malereien bilden einen festen Bildzyklus mit einem Bestand von etwa 120 Motiven. Diese Illustrationen enthalten in der Regel allegorische Figuren, die mit Beischriften und Spruchbändern versehen sind. Die Auszeichnung der Bildzonen mit Figuren, Beischriften und Spruchbändern erfolgt in einem browserbasierten graphischen Bildeditor, der die gewonnenen Koordinaten in einer relationalen Datenbank speichert. Die TEI-konforme Transkription der Texte in den Bildern wird ebenfalls in dieser Datenbank abgelegt, soll aber später zu Archivierungszwecken als TEIDokument exportiert werden. Die verschiedenen Realisierungen einzelner Motive werden in der Datenbank diesen abstrakten Motiven zugeordnet. Ein entsprechendes Alignement erfolgt ebenfalls auf der Ebene der Bild- bzw. Motivkomponenten. Somit werden Visualisierungen möglich, die verschiedene Varianten desselben Motivs nebeneinander stellen und etwa beim Überfahren einer Bildkomponente (z.B. eines Spruchbands) mit der Maus deren jeweilige Pendants in anderen Handschriften graphisch hervorheben und die darin enthaltenen Texte anzeigen. Die geplante Text-Bild-Ausgabe soll neben verschiedenen Möglichkeiten der Textdarstellung (Synopsen, dynamische Apparate, flexibler Grad der sprachlichen Normalisierung, Verlinkung mit digitalen Wörterbüchern) und der skizzierten Präsentation von Illustrationen insbesondere das Zusammenspiel von Text und Bild in den Handschriften des ""Welschen Gastes"" analysieren, dokumentieren und visuell erlebbar machen. Zu diesem Zweck werden inhaltliche und physische Bezüge zwischen einzelnen Textpassagen und den dazugehörigen Illustrationen festgehalten. Dadurch soll es möglich sein, von der Textedition schnell zum entsprechenden Bildmaterial zu gelangen und umgekehrt. Selbstverständlich wird es auch möglich sein, Texte und Bilder im Hinblick auf ihre Platzierung auf einer Handschriftenseite zu betrachten. Ein für die zweite Projektphase (ab 2015) geplanter Text- und Bildkommentar soll mit der Online-Ausgabe dynamisch verknüpft werden. Es ist unser erklärtes Ziel, die digitale Text-Bild-Ausgabe der Öffentlichkeit im Open Access verfügbar zu machen und in Zusammenarbeit mit langfristig finanzierten öffentlichen Institutionen (z.B. UB Heidelberg) für deren dauerhafte Zugänglichkeit und Archivierung zu sorgen."
2014,DHd2014,2014_cls_metadata_extracted.csv,Für eine computergestützte literarische Gattungsstilistik,"Christof Schöch (Lehrstuhl für Computerphilologie);  Steffen Pielström (Universität Würzburg, Germany)","computergestützte literarische Gattungsstilistik, Principal Component Analysis, quantitative Methoden, Maschinelles Lernen, Text Mining, Support Vector Machine, computergestützte Stilistik, Clustering, Cluster Analysis, Varianzanalyse","computergestützte literarische Gattungsstilistik, Principal Component Analysis, quantitative Methoden, Maschinelles Lernen, Text Mining, Support Vector Machine, computergestützte Stilistik, Clustering, Cluster Analysis, Varianzanalyse","Einleitung Der vorliegende Beitrag plädiert für eine computergestützte literarische Gattungsstilistik, verstanden als eine Forschungsagenda für die Literaturwissenschaften, welche hermeneutische und quantitative Methoden verbindet. Diese Agenda wird im Zusammenhang mit einem in Vorbereitung befindlichen Forschungsprojekt zum gleichen Thema formuliert, das in der romanistischen Literaturwissenschaft angesiedelt ist. Aus diesem Forschungsprojekt werden zwei Zwischenergebnisse berichtet: das Erste betrifft die konzeptuelle Verknüpfung von Gattungstheorie und computergestützter Stilistik; das Zweite betrifft die methodische Erweiterung der Principal Component Analysis (PCA) für literaturwissenschaftliche Fragestellungen. 1. Die Agenda der computergesützten literarischen Gattungsstilistik Die übergeordnete Zielsetzung einer computergestützten literarischen Gattungsstilistk ist es, eine tiefgehende Konvergenz herzustellen zwischen etablierten literaturwissenschaftichen Fragestellungen einerseits und quantitativen Verfahren der Textanalyse andererseits. Eine solche Konvergenz ist Voraussetzung dafür, dass sich entsprechende Forschungsvorhaben im Kernbereich der Digital Humanities ansiedeln können, in dem computergestützte Geisteswissenschaften und Angewandte Informatik nicht nebeneinander stehen, sondern sich zu einem neuen, dritten Forschungsparadigma verbinden. Auf eine computergestützte literarische Gattungsstilistik bezogen ergeben sich daraus eine Reihe von Forschungsfragen. Einige von ihnen sind primär literaturwissenschaftlich: Wie kann die Beziehung zwischen Stil und Gattung in einer produktiven Weise konzeptualisiert werden? Wie verhalten sich Gattungsstile, Epochenstile und Autorenstile zueinander? Welche anderen Faktoren spielen für die stilistische Beschreibung literarische Texte eine Rolle? Welche automatisch identifizierbaren sprachlichen Merkmale, auf welchen Ebenen der linguistischen Beschreibung, sind Indikatoren für Gattungen? Andere Fragestellungen 1 stammen aus dem informatischen Bereich des Text Mining: Welche Verfahren der Text_Kategorisierung und des Maschinellen Lernens können eingesetzt und angepasst werden? Wie können für Verfahren wie Support Vector Machines die besten kernels definiert und geeignete features modelliert werden? Aus der Verbindung von literaturwissenschaftlicher und informatischer Fragestellungen ergeben sich aber auch ganz neue Fragen, die dem spezifischen Bereich der Digital Humanities zuzurechnen sind: Welche Besonderheiten natürlichsprachlicher und spezifisch literarischer Daten sind zu berücksichtigen, wenn es darum geht, möglichst generische Strategien zur Trennung von Autoren_ Epochen und Gattungssignal zu entwickeln? Wie können computergestützte Verfahren so weiterentwickelt werden, dass sie einerseits auch für literatursprachliche Daten statistisch signifikant, robust und verlässlich sind, dass sie andererseits aber auch aus hermeneutischer Perspektive transparent und interpretierbar, das heißt aus literaturwissenschaftlicher Sicht bedeutungsvoll sind? Und allgemeiner, wie verändert die computergestützte Herangehensweise die Weise, wie wir über literarische Interpretation und algorithmische Analyse sowie ihre wechselseitige Beziehung nachdenken? Die Bearbeitung dieser Forschungsfragen bildet den Kern der Forschungsagenda einer computergestützten literarischen Gattungsstilistik. Zu zwei dieser Teilfragen werden hier Zwischenergebnisse berichtet. 2. Die konzeptuelle Verknüpfung von Gattungstheorie und quantitativer Stilistik Das erste Zwischenergebnis bezieht sich auf die konzeptuelle Verknüpfung von Gattungstheorie und computergestützter Stilistik. Der Gattungsstilistik geht es um einen induktiven, deskriptiven Blick auf die stilistischen Merkmale literarischer Gattungen und Untergattungen sowie auf deren historische Entwicklung. In der neueren Gattungstheorie hat sich die Auffassung durchgesetzt, dass literarische Gattungen sich nicht mit einem idealistischen, deduktiven Ansatz systematisieren lassen (Schaeffer 1989). Vielmehr sind sie als historische Konventionen zu verstehen, die komplexe und sich dynamisch entwickelnde ""generic facets"" (Kessler et al. 1998) umfassen. Diese beziehen sich zu unterschiedlichen Anteilen auf Themen, Plot und diverse stilistische Merkmale (Hoffmann 2009). Die Kombination mehrerer solcher ""facets"" definiert eine Gattung oder Untergattung, und Übergangsformen oder diachrone Entwicklungen lassen sich über den Wegfall oder das Hinzutreten einzelner ""facets"" erfassen. In ähnlicher Weise wird Stil heute als ein Phänomen aufgefasst, das als ""Bündel konkurrierender Merkmale"" auf unterschiedlichen linguistischen Beschreibungsebenen (Phonologie, Morphologie, Lexik/Semantik, Syntax, Plot, etc.) verstanden werden kann 2 1 (Sandig 2006, Karlgren & Cutting 1994). Hier kann die computergestützte Stilistik ansetzen, denn sie ist in der Lage, induktiv und umfassend zahlreiche Merkmale _ in ihrer gegenseitigen Abhängigkeit, in ihrer jeweiligen Gewichtung, und unter präziser Berücksichtigung zahlreicher möglicherweise relevanter Faktoren _ zu erfassen und für die Klassifikation oder das Clustering von Texten zu nutzen. Damit wird die von Dominique Combe eingeforderte ""stylistique des genres"" (Combe 2002) computergestützt realisiert. Abb. 1 fasst das Verhältnis zwischen der Theorie literarischer Gattungen und computergestützter Stilistik / Text Mining zusammen. Abb. 1: Literarische Gattungstheorie und computergestützte Stilistik Durch die vergleichbare Konzeption von Gattungen (mit Facetten) und Stil (mit Merkmalen) können Verbindungen zwischen historisch oder theoretisch gegebenen Untergattungen einerseits und auf der Grundlage stilistischer Öhnlichkeit gruppierten Clustern von Texten andererseits entdeckt werden. Genauer gesagt: es können in ihrer Stärke statistisch charakterisierbare Korrelationen zwischen einzelnen Gattungsfacetten und stilistischen Merkmalen erhoben und eingeordnet werden. Durch die Identifikation von besonders distinktiven Merkmalen und durch Merkmalsgeneralisation können dann auch 1 Im Bereich der Corpuslinguistik geht die computergestützte Untersuchung der stilistischen Unterschiede von (literarischen) Gattungen und Untergattungen bis in die 1980er_Jahre zurück, mit Pionierarbeiten von Douglas Biber zur Modellierung des Zusammenhangs zwischen funktionalen Gattungsaspekten und stilistischen Merkmalen, die zu synthetischen Dimensionen zusammengefasst werden (Biber 1992) und der Erprobung einer breiten Auswahl von potentiellen ""style markers"" (Karlgren & Cutting 1994). Außerdem wurden bspw. die vergleichende Evaluation von token_basierten, syntaktischen und anderen Merkmalen vorgenommen (Stamatatos et al. 2000). 3 Merkmalsbündel ermittelt werden, die zugleich statistisch signifikant mit einer Facette korellieren und aus literaturwissenschaftlicher Perspektive interpretierbar sind. 3. Die methodische Erweiterung der Principal Component Analysis Die computergestützte literarische Gattungsstilistik ist Teil einer sich aktuell verstärkenden Tendenz, stilometrische Fragen über die traditionell im Vordergrund stehende 2 Autor_Attribution hinaus zu bearbeiten. Zahlreiche wohl etablierte Methoden (wie bspw. Cluster Analysis oder Principal Component Analysis), aber auch neuere informatischen Verfahren aus dem Bereich des Text Mining und Machine Learning sind für eine so konzipierte Gattungsstilistik anschlussfähig. Wir schlagen hier vor diesem Hintergrund vor, die etablierte Methode der Principal Component Analysis (PCA, siehe grundlegend Jackson 2005) auf eine Weise zu erweitern, die ihre Interpretierbarkeit erhöht. Zur verlässlichen Unterscheidung von Kategorien oder Gruppen innerhalb einer großen Menge von Texten ist die Stilometrie auf die gleichzeitige Betrachtung einer großen Zahl von Merkmalen (bspw. Worten) angewiesen. Fasst man jedes Merkmal mehrerer Texte als je eine Dimensionen auf, beruht die stilometrische Erhebung von Öhnlichkeitsrelationen zwischen Texten daher häufig auf einer Dimensionsreduktion. Anders als bspw. Burrows' Delta (Burrows 2002) erlaubt PCA die Reduktion der Dimensionen eines Datensatzes nicht auf nur eine einzige, sondern auf wenige neue und voneinander unabhängige Dimensionen, die sog. principal components (PC), die die Varianz in den Daten besonders gut beschreiben. Jede dieser PCs korreliert hierbei mit einer spezifischen Kombination bzw. Gewichtung von Merkmalsfrequenzen. Charakteristisch für die PCA ist, daß bereits die ersten 2_4 PCs oft einen großen Teil der im Datensatz enthaltenen Varianz beschreiben (Abb. 2a). Für eine graphische Exploration der Öhnlichkeit zweier oder mehrerer Gruppen kann es also ausreichen, die ersten PCs als Koordinaten zu verwenden, anstatt in einer großen Zahl von Wortfrequenzen eine Kombination zu suchen, die Unterschiede besonders deutlich macht (Abb. 2b). 2 Im Bereich der literarischen Gattungsstilistik liegen mehrere Ansätze vor, welche die diachrone Entwicklung von Gattungen allgemein (Moretti 2005, Jockers 2013) oder spezifische methodische Lösungsversuche betreffen: bspw. Cluster Analyse oder ""unmasking"" _Prozedur für die Gattungsklassifikation (Allison et al. 2011; Kestemont et al. 2012; Schöch 2013). 4 Abb. 2: Die Principal Component Analysis am Beispiel von 141 französischen Dramen (Tragödien, Komödien, Tragikomödien, Andere) aus dem siebzehnten Jahrhundert, basierend auf den relativen Häufigkeiten der 200 am meisten verwendeten Wörter. a) Scree Plot mit den Varianzanteilen für PC 1_10.; b) Scatterplot Matrix für PC 1_4 mit gattunsspezifischer Farbcodierung. Die PCA erlaubt allerdings von sich aus keine Aussagen über die Unterschiedlichkeit zweier Kategorien von Datenpunkten, oder über den Einfluß einer bestimmten Gruppierungsvariable, weshalb stilistische Unterschiede zwischen Gattungen mit dieser Methode zwar visualisiert, aber nicht analysiert werden können. Um diese Lücke zu überbrücken, haben wir ein Verfahren entwickelt, mit dem der Einfluß der Zugehörigkeit eines bestimmten Textes zu einer spezifischen Kategorie (bspw. der Gattung) auf die PCs mittels der Varianzanalyse (ANOVA) untersucht werden kann. Hierbei wird die Gattung als unabhängige faktorielle Variable betrachtet, die Werte einer bestimmten PC als abhängige Variable. Das Bestimmtheitsmaß R_ liefert hierbei eine Vergleichsgröße, anhand derer sich die Einflüße verschiedener Faktoren (bspw. Gattung, aber auch Autorschaft oder auch Publikationsdatum) auf eine bestimmte PC quantifizieren lassen (Abb 3a). 5 Abb. 3: Von der PCA zur Kategorie. a) Ergebnisse der Varianzanalysen (ANOVAs) zur Quantifizierung des Einflusses verschiedener Faktoren auf die PCs 1_4. Untersucht wurden die Faktoren Form (d.h. Vers, Prosa oder Gemischt), Gattung (Tragödie, Komödie, Tragikomödie) und Autor. Dargestellt sind die Bestimmtheitsmaße (R_) für jeden dieser Faktoren bei jeder PC. Symbole über den Balken repräsentieren das Signifikanzniveau der jeweiligen Beziehung (‚Äòn.s. ‚Äô nicht signifikant; ‚Äò*‚Äô p < 0.05; ‚Äò**‚Äô p < 0.01; ‚Äò***‚Äô p < 0.001). Den stärksten Einfluß hat die literarische Gattung in diesem Datensatz auf PC1 und PC3; b) Loading_Werte der 20 häufigsten Worte für PC 1_4. Diese Werte repräsentieren den Einfluß einzelner Wortfrequenzen auf die PCs, und erlauben so in Kombination mit den Resultaten der Varianzanalysen interpretierende Rückschlüsse. Die Kombination von PCA und ANOVA erlaubt somit die Identifikation von PCs, die durch einen bestimmten Faktor besonders stark geprägt werden, und den Abgleich mit der Gewichtung bestimmter Wortfrequenzen in dieser PC (Abb. 3b). Gemeinsam betrachtet erlaubt dies Rückschlüsse darüber, welche Merkmale und Merkmalsbündel für die Unterschiede in bestimmten Faktoren besonders relevant sind, d.h. auch welche Kombinationen charakteristisch für Gattungsunterschiede sind. 6 Fazit Mit der konzeptuellen Verbindung von Gattungstheorie und computergestützter Stilistik einerseits, und der methodischen Erweiterung der PCA zur verbesserten Interpretierbarkeit von PCs in Bezug auf relevante Kategorien andererseits, konnten wichtige Zwischenziele erreicht und Grundlagen für die eingangs beschriebene Forschungsagenda gelegt werden. Und es konnte exemplarisch gezeigt werden, so hoffen wir, wie die hier vertretene Forschungsagenda einer computergestützten literarischen Gattungsstilistik hermeneutische und quantitative Methoden zu einem eigenständigen Ansatz verbindet, der auf die Entwicklung spezifisch erweiterter informatischer Verfahren für ein erweitertes Verständnis der Natur und der Entwicklung literarischer Gattungen abzielt. Literaturangaben Allison, Sarah, Ryan Heuser, Matthew L. Jockers, Franco Moretti, and Michael Witmore (2011). Quantitative Formalism: An Experiment. Stanford: Stanford Literary Lab. Biber, Douglas (1992). Computers in the Humanities, 26.5_6, 331_347. ""The multidimensional approach to linguistic analyses of genre variation"" , in: Burrows, John (2002). Linguistic Computing 17.3, 267_287. ""Delta: A Measure of Stylistic Difference and a Guide to Likely Authorship"" . Literary and Combe, Dominique (2002). ""La stylistique des genres"" , in: Langue fran√ßaise 135, 33_49. Hoffmann, Michael (2009). ""Mikro_ und makrostilistische Einheiten im Überblick‚Äù , in: Rhetorik und Stilistik, Ein internationales Handbuch historischer und systematischer Forschung, Band 2, hg. von Ulla Fix, Andreas Gardt & Joachim Knape. Band 2, Berlin: de Gruyter, 1529_45. Jackson, Edward (2005). A User's Guide to Principal Components. New York: Wiley. Jockers, Matthew (2013). Macroanalysis. Digital Methods and Literary History. Chicago: Univ, of Illinois Press. Juola, Patrick (2006). ""Authorship Attribution. ‚Äù Foundations and Trends in Information Retrieval 1.3, 233_334. Karlgren, Jussi & Douglas Cutting (1994). analysis‚Äù . In Proceedings of COLING '94, Vol. 2, 1071_1075. ""Recognizing text genres with simple metrics using discriminant Kessler, Brett, Geoffrey Numberg, and Hinrich Schütze (1998). ""Automatic Detection of Text Genre. ‚Äù In Proceedings of ACL 1998, 32_38. doi:10.3115/976909.979622. Kestemont, Mike, Kim Luyckx, Walter Daelemans, and Thomas Crombez (2012). Verification Using Unmasking. ‚Äù English Studies 93.3, 340‚Äì356. ""Cross_Genre Authorship Moretti, Franco (2005). Graphs, Maps, Trees: Abstract Models for a Literary History. London: Verso. Sandig, Barbara (2006). Textstilistik des Deutschen. 2. Auflage. Berlin: de Gruyter. Schaeffer, Jean_Marie (1989). Qu‚Äôest_ce qu‚Äôun genre litt√©raire? Paris: Seuil, 1989. Schöch, Christof (2013). ""Fine_tuning Our Stylometric Tools: Investigating Authorship and Genre in French Classical Theater‚Äù , Digital Humanities Conference 2013, http://dh2013.unl.edu/abstracts/ab_270.html. Stamatatos, Efstathios, Nikos Fakotakis, and George Kokkinakis (2000). Terms of Genre and Author. ""Automatic Text Categorization in Computational Linguistics 26/4, 471_497."
2014,DHd2014,2014_cls_metadata_extracted.csv,Informatik und Hermeneutik. Erste Erkenntnisse,"Evelyn Gius (Universität Hamburg, Deutschland)","hermeneutisches Markup, CATMA, Annotation, computational narratology, Kategorien, Ereignis, narrative Ebene, Ereignishaftigkeit, narrative Einheiten, Digitale Narratologie","hermeneutisches Markup, CATMA, Annotation, computational narratology, Kategorien, Ereignis, narrative Ebene, Ereignishaftigkeit, narrative Einheiten, Digitale Narratologie","In unserem aktuellen BMBF-eHumanities Projekt heureCLéA arbeiten wir als Informatiker und Literaturwissenschaftlerinnen an einer digitalen Heuristik, die die Analyse von literarischen Texten unterstützen soll. Der Anwendungsfokus liegt dabei exemplarisch auf narratologischen Phänomenen der Zeit: d.h., das heuristische Modul von heureCLéA soll die Funktionalität der Textanalyse und -annotationsumgebung CATMA2 erweitern, indem es den Usern automatisch generierte Vorschläge zur Annotation narratologisch definierter Zeit-Phänomene in einem Text anbietet. Das Modul wird auf der Basis von drei Zugängen entwickelt: (1) Ausgangspunkt ist so genanntes ""hermeneutisches Markup"" (Piez 2010), das auf klassischen narratologischen Kategorien wie etwa Ordnung, Frequenz und Dauer beruht (vgl. Genette 1972, Lahn und Meister 2013) und von geschulten Annotatorinnen vergeben wird. Dieses Markup wird (2) mit regelbasierten Verfahren sowie (3) Machine-Learning-Ansätzen kombiniert.3 Aufgrund des Zusammenspiels von literaturwissenschaftlichen 'und speziell: hermeneutischen 'Verfahren und informatischen Verfahren der Information Extraction und der Statistik stehen sich non-deterministische Zugänge zu Texten und entscheidbare bzw. deterministische Verfahren gegenüber, die nicht ohne weiteres auf den jeweilig anderen Ansatz übertragen werden können. Deshalb ist die Reproduzierbarkeit von narratologischen Analysen für die Vereinbarkeit des literaturwissenschaftlichen und des informatischen Zugangs und damit für den Erfolg des heuristischen Moduls ausschlaggebend. In unserem Beitrag präsentieren wir ein methodisches Desiderat im Bereich der Narratologie, das erst durch die interdisziplinäre Zusammenarbeit zwischen Geisteswissenschaftlern und Informatikern in den Fokus gerückt ist und aus unserer Sicht exemplarisch für eine solche Zusammenarbeit ist: die Notwendigkeit, narratologische Analysekategorien eindeutiger zu konzeptionalisieren, um sie operationalisieren zu können. 1 vgl. www.heureclea.de (gesehen am 10.12.2013) 2 vgl. www.catma.de (gesehen am 10.12.2013) 3 Damit ist heureCLéA ein Beitrag zur computational narratology im Sinne von Mani, da es zu ""exploration and testing of literary hypotheses through mining of narrative structure from corpora"" (Mani, 2013, para. 1) beiträgt. 3Zu den regelbasierten Verfahren vgl. Strötgen und Gertz (2010), die Gesamtarchitektur von heureCLéA wird außerdem in einem weiteren eingereichten Beitrag vorgestellt. Die Narratologie ist eine literaturwissenschaftliche Disziplin, die eine Reihe theoretischer Konzepte und Modelle für die Analyse und Interpretation erzählender Texte zur Verfügung stellt. Diese narratologischen Kategorien dienen normalerweise der Bezeichnung und Verortung textueller Eigenschaften, die (a) als typisch für narrative Texte angesehen werden und (b) für besonders interessant und geeignet befunden werden, um die speziellen Eigenschaften eines literarischen Einzelwerkes herauszustellen. Viele der Kategorien dienen der Bezeichnung struktureller Phänomene, die hauptsächlich an der Textoberfläche zugänglich sind. Das gilt insbesondere auch für die meisten Kategorien, die der Analyse explizit markierter Zeitphänomene dienen, wie sie im Rahmen von heureCLéA untersucht werden.4 Obwohl narratologische Kategorien gemeinhin als theoretisch durchdacht und leicht operationalisierbar gelten, zeigten sich bei ihrer formalisierten Anwendung im Rahmen manueller, kollaborativer Annotation in heureCLéA einige theoretische Unzulänglichkeiten. Typischerweise wurden solche Unzulänglichkeiten dann entdeckt, wenn sich die Annotatoren hinsichtlich der korrekten narratologischen Bestimmung konkreter Textstellen nicht einig waren. In Diskussionen über die Gründe für individuelle Annotations-Entscheidungen stellte sich dann oft die uneindeutige oder unvollständige Konzeption der jeweiligen Kategorie als Ursache uneinheitlichen Markups heraus. Die festgestellten theoretischen Versäumnisse lassen sich in zwei Gruppen einteilen, die je unterschiedliche Problemlösungsstrategien erfordern: a) konzeptionelle Unvollständigkeit, die leicht durch eine Vervollständigung der Kategorie mittels funktionaler Entscheidungen behoben werden kann. Stellt sich bei der versuchten Anwendung einer Kategorie heraus, dass ihre Definition zu vage ist, um die Bestimmung einer fraglichen Textstelle vorzunehmen, müssen pragmatische Entscheidungen im Hinblick auf die Inklusion oder Exklusion bisher nicht bedachter textueller Oberflächenmerkmale getroffen werden. 5 4 Der Klarheit wegen sollte angemerkt werden, dass keine der im Feld der Narratologie interessanten Phänomene rein formale Textmerkmale sind, da die Bedeutung von Wörtern und Sätzen stets ausschlaggebend für ihre Bestimmung ist. Das bedeutet, dass solche Phänomene zwar an der Textoberfläche zugänglich sind, ihre Bestimmung jedoch trotzdem in einem weiteren Sinne des Wortes interpretativ sein kann. 5 Ein derartiger Problemfall stellte ich an folgender Textstelle in Friedrich Hebbels Erzählung Matteo im Hinblick auf die Frage, ob es sich hier um eine Prolepse - bisher konzeptionalisiert als Vorgriff in der Zeit handelt: ""Sieh, morgen feire ich meine Hochzeit; zum Zeichen, daß du mir nicht mehr böse bist, kommst du auch, meine Mutter wird dich gern sehen."" (Hebbel 1963: para. 4). Die Schwierigkeit ist hier dadurch gegeben, dass die angesprochene Figur am folgenden Tag nicht auf der Hochzeit erscheint. Um Derartige Entscheidungen haben nur für die Anwendung der jeweiligen Kategorie Konsequenzen, nicht aber für weitere Konzepte. - Die zweite Kategorie betrifft dagegen b) theoretische Unvollständigkeit, die ihrerseits auf die unzureichende Bestimmung fundamentaler narratologischer Konzepte zurückzuführen ist. Probleme dieses Typs können nicht einfach durch pragmatische Entscheidungen behoben werden, weil die für eine Problemlösung notwendigen Setzungen auf der Ebene grundlegender narratologischer Konzepte weitreichende Konsequenzen für viele erzähltheoretische Einzelkategorien nach sich zieht. Im Folgenden soll diese zweite Problemklasse anhand eines Beispiels erläutert werden. In der Erzählung Der Tod von Thomas Mann ist bei dem Vergleich der Annotationsergebnisse im Hinblick auf die Geschwindigkeit der Erzählung6 folgende Passage in den Fokus der Aufmerksamkeit gerückt: Ich habe die ganze Nacht hinausgeblickt, und mich dünkte, so müsse der Tod sein oder das Nach dem Tode: dort drüben und draußen ein unendliches, dumpf brausendes Dunkel. Wird dort ein Gedanke, eine Ahnung von mir fortleben und -weben und ewig auf das unbegreifliche Brausen horchen? Mann 2004: 76 Diese Passage wurde von einigen Annotatoren ab dem ersten Komma als zeitraffend erzählt eingeordnet, von anderen dagegen als Erzählpause. Die Diskussion über die Gründe für die individuellen Entscheidungen hat gezeigt, dass die Annotatoren unterschiedliche Auffassungen darüber vertreten, was ein Ereignis ist. Betrachtet man mentale Vorgänge als Ereignisse, so muss man die zitierte Passage als zeitraffend klassifizieren, da die Gedanken des Erzählers in der fiktiven Welt vermutlich längere Zeit anhielten als die wenigen Sekunden, die in der Erzählung für ihre Wiedergabe eingeräumt werden. Ist man jedoch der Ansicht, dass es sich bei mentalen Prozessen nicht um Ereignisse handelt, so liegt in obiger Textstelle eine Pause vor: Der Bericht von Ereignissen wird unterbrochen durch die Darstellung nicht-ereignishafter Gegebenheiten. Die Frage danach, welche Konzeption von Ereignis korrekt oder sinnvoll ist, ist Gegenstand der Debatte um Narrativität: die für erzählende Texte konstitutive Eigenschaft, von Ereignissen zu berichten. Die unterschiedlichen Intuitionen der Annotatoren in Bezug auf die Definition von ""Ereignis"" korreliert hier mit Schmids Konzeptionen von Ereignis I, das jegliche entscheiden zu können, ob hier eine Prolepse vorliegt, muss entschieden werden, ob dieses Konzept auch antizipierte Ereignisse fassen soll, die im Verlauf der Erzählung nicht eintreten. 6 Unter ""Erzählgeschwindigkeit"" versteht man in der Narratologie das Verhältnis zwischen der Menge an Ereignissen, von denen berichtet wird, und der Zeit, die für diesen Bericht notwendig ist. Form von Zustandsveränderung inkludiert, und Ereignis II, das zusätzliche Kriterien anführt, die Zustandsveränderungen aufweisen müssen, um als Ereignis zu gelten (Schmid 2003).7 Eine Entscheidung im Hinblick auf die richtige Narrativitätsdefinition, die für die Lösung von Annotationsproblemen im Bereich der Erzählgeschwindigkeit notwendig wäre, hätte nun nicht nur für die fraglichen Kategorien Konsequenzen, sondern beispielsweise auch für die Bestimmung des Gegenstandsbereich der Narratologie und potenziell für eine Reihe weiterer Kategorien.8 Angesichts insbesondere dieser zweiten Sorte von Problem stellt sich die Frage, inwieweit solche grundlegenden Fragen im Rahmen von heureCLéA geklärt werden können und sollten. Da die theoretische Arbeit an narratologischen Grundkonzepten nicht im Fokus des Projektes stehen sollte, war zunächst ein individueller Umgang der Annotatoren mit den anwendungsbezogenen Einzelproblemen vorgesehen. Es hat sich jedoch herausgestellt, dass diese Vorgehensweise weder aus narratologischer Sicht befriedigend ist, noch eine aus informationstheoretischer Perspektive verwertbare Datengrundlage liefert. Aus diesen Gründen haben wir uns dazu entschieden, den beiden geschilderten narratologischen Basisproblemen einige Aufmerksamkeit zu widmen: Für die Bestimmung von Ebenenwechsel und -zuordnung wird eine konsistente Lösung gefunden, so dass Ordnungsphänomene tatsächlich unterschiedlichen Erzählebenen zugeordnet werden können. Für die Bestimmung von ‚ÄûEreignis"" streben wir eine plausible Konzeptionalisierung an, die ein möglichst wenig interpretatives Erkennen von Ereignissen erlaubt. 7 Zu diesen Kriterien zählt neben Resultativität, Relevanz, Unvorhersehbarkeit, Effekt, Irreversibilität und Nicht-Wiederholbarkeit auch das Kriterium der Faktizität, das die Eigenschaft von Zustandsveränderungen bezeichnet, tatsächlich in der fiktiven Außenwelt stattzufinden. Wertet man Faktizität als notwendige Eigenschaft von Ereignissen, muss die oben zitierte Passage aus Der Tod als Erzählpause klassifiziert werden. 8 Eine ähnliche Verknüpfung von Annotationsproblemen und ungeklärten narratologischen Basiskonzepten findet sich bei der Annotation von Phänomenen der zeitlichen Ordnung einer Erzählung einerseits und dem grundlegenden narratologischen Konzept der Erzählebenen. Es kann nur sinnvoll das zeitliche Verhältnis von solchen Ereignissen bestimmt werden, die sich auf derselben Erzählebene befinden. Anhand welcher Faktoren ein Ebenenwechsel festzumachen ist, wird in der narratologischen Forschung noch diskutiert (vgl. Ryan 1991, Coste/Pier 2011). Die beschriebene Problematik ist ein spezifisch literaturwissenschaftliches Problem, die sie erzeugenden Rahmenbedingungen sind jedoch zugleich exemplarisch für das Zusammenspiel von Informatik und Geisteswissenschaften. Deshalb ist der entwickelte Lösungsansatz von entscheidender Bedeutung für das Gelingen des Projekts. Die Reproduzierbarkeit von Analyseergebnissen, die durch den Ansatz anvisiert wird, wird in den Geisteswissenschaften traditionell nicht thematisiert, da diese meist dem Konzept der intersubjektiven √úbereinstimmung operieren, ohne diese weiter zu bestimmen. Die Reproduzierbarkeit von Analyseergebnissen ist jedoch zugleich auch eine von mehreren, bislang wenig erforschten Gelingensbedingungen für interdisziplinäre Projekte im Bereich der Digital Humanities. Diese disziplinäre Doppelperspektive auf ein methodisches Kriterium weist insofern auf die konzeptionellen Chancen, Probleme und Bedingungen einer Kooperation zwischen Geisteswissenschaftlern und Informatikern im Kontext von DH-Projekten. Literatur Coste, D. and Pier, J. (2013). Narrative Levels. the living handbook of narratology. http://www.lhn.uni-hamburg.de/article/narrative-levels (gesehen am 06.12.2013). First published 2011. Genette, G. (1972). Discours du r√©cit. In id., Figures III. Paris: Editions Du Seuil, pp. 67-282. Lahn, S. and Meister, J. C. (2013). Einführung in die Erzähltextanalyse: 2nd, updated edition. Stuttgart: Metzler. Mani, I. (2013). Computational Narratology. the living handbook of narratology. http://www.lhn.uni-hamburg.de/article/computational-narratology (gesehen am 06.12.2013). Piez, W. (2010): Towards Hermeneutic Markup: an Architectural Outline. Digital Humanities 2010. Conference Abstracts. London: Office for Humanities Communication, Centre for Computing in the Humanities, King‚Äôs College London, pp. 202-205. Ryan, M.-L. (1991). Possible Worlds, Artificial Intelligence, and Narrative Theory. Bloomington: Indiana UP. Schmid, W. (2003): Narrativity and Eventfulness. In T. Kindt & H.-H. Müller (eds.). What Is Narratology? Questions and Answers Regarding the Status of a Theory. Berlin: de Gruyter, 17'33. Strötgen, J. and Gertz, M. (2010). HeidelTime: High Quality Rule-based Extraction and Normalization of Temporal Expressions. Proceedings of the 5th International Workshop on Semantic Evaluation (ACL 2010)."
2014,DHd2014,2014_cls_metadata_extracted.csv,New Technologies for Old Germanic Languages: Resources and Research on Parallel Gospels in Older Continental Western Germanic,"Christian Chiarcos (Goethe Universität Frankfurt, Germany); Jens Chobotsky (Goethe Universität Frankfurt, Germany); Gaye Detmold (Goethe Universität Frankfurt, Germany); Roland Mittmann (Goethe Universität Frankfurt, Germany); Maria Sukhareva (Goethe Universität Frankfurt, Germany)","Technologien, linked-lexical resources, etymological dictionaries, annotation","Technologien, linked-lexical resources, etymological dictionaries, annotation","We describe on-going e_orts at the Goethe University Frankfurt on the study of older Continental Western Germanic languages, in particular, Old High German (OHG, antecessor of German) and Old Saxon (OS, antecessor of Low German and closely related to the antecessor of Dutch) and their relation to Old English (OE), Gothic, German and other Germanic languages as well as the relation of OHG and OS religious texts to their Latin sources. This line of research is conducted in the context of two larger e_orts, the Old German Reference Corpus and the LOEWE cluster ""Digital Humanities"", in collaboration with the Applied Computational Linguistics group at the Goethe-Universit¬®at Frankfurt. The Old German Reference Corpus is a DFG-funded project that emerged from the Deutsch Diachron Digital (DDD) initiative, conducted in cooperation between HU Berlin, U Frankfurt and U Jena, and aims to provide a morphosyntactically annotatated, exhaustive reference corpus of Old High German and Old Saxon. The LOEWE cluster ""Digital Humanities""1, funded through a programme of the State of Hessen, is a collaboration between U Frankfurt, TU Darmstadt and Freies Deutsches Hochstift Frankfurt aiming to develop methodologies and infrastructures to facilitate information-technological support of research in the humanities. Here, we concentrate on biblical texts: These are available for a variety of modern and historical European languages and possess high-quality alignment (verses, segments), thus building up a valuable parallel resource for linguistic, philological and historical research questions, as well as for Natural Language Processing, whose methodologies for alignment and annotation projection can be used to support the analysis of these texts: ‚Ä¢ The Old German Reference Corpus [4] provides a lexicon and an exhaustive corpus of older continental Western Germanic, i.e., Old Saxon (OS) and Old High German (OHG), comprising 650,000 tokens automatically enriched with morphological and morphosyntactic information drawn from existing glossaries which have been digitized by the project, complemented with manual annotations[3] and metadata and published via the ANNIS database [2]. ‚Ä¢ A Historical Linguistic Database was developed in LOEWE from a collection of etymological dictionaries for all Old Germanic languages (incl. OS, OHG, Old English, Gothic, Old Norse) as a relational data base providing user-friendly means of comparing etymologically related forms between historical dialects and their daughter languages, as well as a machine-readable view on these [5]. ‚Ä¢ Major texts in the corpus are the gospel harmonies associated with the names Heliand (OS), Tatian (OHG and Latin) and Otfrid (OHG). Although not direct translations of the 1http://www.digital-humanities-hessen.de 1 Bible and hence not directly alignable with the gospel translations we have for Old English, Gothic, and later stages of English, German, Dutch and North Germanic, a section-level alignment has been manually extrapolated from the literature in a LOEWE project [5]. ‚Ä¢ This coarse-grained alignment is currently being refined to a phrase-level alignment using the linked lexical resources mentioned above as well as statistical models of systematic character correspondences like those applied by [1]. ‚Ä¢ On the basis of correspondences between historical and modern languages in parallel and quasi-parallel text, statistical annotation projection can be applied for the syntactic annotation of Older Germanic. So far, we conducted experiments on the joint projection of dependency syntax to Old English, Middle Icelandic and Early Modern High German corpora following the methodology of [6]. These indicate that projected annotations can serve as training data for mono- and cross-language parsing also for, e.g., OHG. ‚Ä¢ These annotations can be applied, for example, to compare linguistic structures in OHG gospel harmonies and their Latin sources, thereby facilitating the research of a LOEWE project that currently uses statistical word alignment and existing morphosyntactic annotations only as the basis for a qualitative, philological comparison with the TreeAligner [7]. References [1] Marcel Bollmann. POS tagging for historical texts with sparse training data. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse (LAW/ID-2013), pages 11–18, Sofia, Bulgaria, Aug 2013. [2] Christian Chiarcos, Stefanie Dipper, Michael G¬®otze, Ulf Leser, Anke L¬®udeling, Julia Ritz, and Manfred Stede. A Flexible Framework for Integrating Annotations from Di_erent Tools and Tag Sets. Traitement Automatique des Langues (TAL), 49(2), 2008. [3] Sonja Linde and Roland Mittmann. Old German Reference Corpus. Digitizing the knowledge of the 19th century. In Paul Bennett, Martin Durrell, Silke Scheible, and Richard J. Whitt, editors, New Methods in Historical Corpus Linguistics / Korpuslinguistik und interdiziplin¬®are Perspektiven auf Sprache, Korpuslinguistik und interdiziplin¬®are Perspektiven auf Sprache / Corpus linguistics and Interdisciplinary perspectives on language (CLIP): 3, T¬®ubingen, 2013. Narr. [4] Roland Mittmann. Digitalisierung historischer Glossare zur automatisierten Vorannotation von Textkorpora am Beispiel des Altdeutschen. Journal for Language Technology and Computational Linguistics (JLCL), 27(2):39–52, 2013. Special issue Alt¬®uberlieferte Sprachen als Gegenstand der Texttechnologie / Text Technological Mining of Ancient Languages. [5] Timothy Blaine Price. Multi-faceted Alignment: Toward Automatic Detection of Textual Similarity in Gospel-derived Texts. In Proceedings of Historical Corpora 2012, Frankfurt, Dec 2012. [6] Kathrin Spreyer and Jonas Kuhn. Data-Driven Dependency Parsing of New Languages Using Incomplete and Noisy Training Data. In Proceedings of CoNLL, pages 12–20, Boulder, CO, Jun 2009. [7] Martin Volk, Joakim Lundborg, and Ma¬®el Mettler. A search tool for parallel treebanks. In Proceedings of the 1st Linguistic Annotation Workshop (LAW-2007), pages 85–92, Prague, Czech Republic, Jun 2007."
2014,DHd2014,2014_cls_metadata_extracted.csv,GeoBib - Georeferenzierte Online-Bibliographie früher Holocaust- und Lagerliteratur,"Frank Binder (Justus-Liebig-Universität Gießen, Germany); Annalena Schmidt (Herder-Institut für historische Ostmitteleuropaforschung); Bastian Entrup (Justus-Liebig-Universität Gießen, Germany); Markus Roth (Justus-Liebig-Universität Gießen, Germany); Henning Lobin (Justus-Liebig-Universität Gießen, Germany)","Onlinebibliographie, Georeferenzen","Onlinebibliographie, Georeferenzen","Zielsetzung Ziel des Projekts ist es, die frühen Texte der deutsch- bzw. polnischsprachigen Holocaust- und Lagerliteratur von 1933 bis 1949 bibliographisch in einer Online-Datenbank zu erfassen. So können diese frühen Texte, die in weiten Teilen aus dem kulturellen und kollektiven Gedächtnis verdrängt wurden, für die öffentliche, wissenschaftliche und didaktische Wahrnehmung erschlossen und aufbereitet werden. Ergänzt werden die bibliographischen Einträge durch inhaltliche und biographische Annotationen, Informationen zur Werkgeschichte sowie durch Georeferenzierung (Informationen zu Orten und Plätzen anhand von Kartenmaterial). Das zu entwickelnde Web-Portal soll dabei 'neben der bibliographischen Suche 'auch über geographische Karten gezielt Texte zu einer bestimmten Region zugänglich machen. Dabei sollen Abfragemöglichkeiten nach räumlichen Kriterien und Attributen beliebig kombinierbar sein. Methoden Die frühen Texte der Holocaustliteratur werden'verbunden mit einer tiefreichenden inhaltlichen Erschließung 'in einem Online-Bibliographie-Portal repräsentiert. Eine an internationalen Annotationsstandards (TEI) ausgerichtete systematische Erfassung der bis 1949 publizierten Texte, ggf. erschienener Rezensionen, der Sekundärliteratur sowie die Anreicherung durch biographische Informationen zu den Verfassern/-innen wird dabei kombiniert mit der Georeferenzierungvon Metadaten und Textinhalten (Orte, Lager, Gettos etc.). Sämtliche Daten werden in einer OnlineDatenbank erfasst, die den zukünftigen Nutzern den Zugriff auf die bibliographischen Daten und deren Auswertung durch innovative kartenbasierte Visualisierungen ermöglicht. Dies bildet eine wesentliche Grundlage für daran anschließende literatur- und geschichtswissenschaftliche Forschungsfragestellungen sowie für eine didaktische Nutzung dieser Zeugnisse in der schulischen und außerschulischen Bildungsarbeit. Genutzte Ressourcen Die aufwändige Beschaffung und inhaltliche Erschließung der frühen Holocausttexte wird als zentraler Teil der Projektarbeiten durch ein Team von Literaturwissenschaftler/innen und Historiker/innen unter intensiver Nutzung verschiedener einschlägiger Bibliotheken, Archive sowie den Kauf antiquarischer Bücher durchgeführt. Zur Erfassung bibliographischer, literaturwissenschaftlicher und historischer Daten wird ein auf TEIP5 basierendes XML-Schema erstellt und eine angepasste Autorenumgebung in Oxygen XML verwendet (s. Entrup et al. 2013b). Historisch-biographische Informationen zu Autor/innen sowie ortsbezogene Informationen werden zeitgleich zentral in einem projektinternen Redaktionswiki (Wikimedia) zusammengetragen. Somit können sie später automatisiert ausgelesen und in die Portaldatenbank übertragen werden. Über die Verlinkung von personen-, orts- und zeitbezogenen Informationen in den TEI-Dokumenten unter Nutzung der Wiki-Einträge werden die Zusammenhänge zwischen den erschlossenen HolocaustTexten technisch erfassbar. Informationen zu den Autoren/innen werden darüber hinaus mit der Gemeinsamen Normdatei (GND) der Deutschen Nationalbibliothek verknüpft. Zurückgegriffen werden kann aber auch auf die im Bibliographieportal des Herder-Instituts und anderen Abteilungen gesammelten Daten und die Forschungsbibliothek (Warmbrunn 2012), in der für die Jahre zwischen 1954 und 1998 alle landesweiten und regionalen Zeitungen Polens und weiterer Nachbarländer gesammelt wurden und wo über eine eigene Zeitungsausschnittssammlung auf zahlreiche biographische und ortsbezogene Informationen zurückgegriffen werden kann. Für die Georeferenzierung und Bereitstellung eines geographischen Suchzugriffes wird ein geographisches Informationssystem eingesetzt. Zur Aufbereitung von Karten wird für das entstehende Online-Portal ein Map-Server benötigt, der Karten und Abfragedienste sowie GISFunktionalitäten zur Verfügung stellt. In Bezug auf Kartenmaterial werden vorhandene Grundlagenkarten recherchiert aber bei Bedarf auch digitales Kartenmaterial erstellt. Entstehende Ressourcen Die frühen Texte der Holocaust- und Lagerliteratur werden in Form einer umfangreichen Bibliographie, nicht aber in einer digitalen Volltextbibliothek erschlossen.Urheberrechtsfragen spielen hier für die Projektarbeiten eine zentrale Rolle. Die Arbeitsstelle Holocaustliteratur tritt seit geraumer Zeit dagegen auf, Opfertexte als frei verfügbar anzusehen. Für den Gesamtbestand wäre eine befriedigende Rechteklärung aufgrund der jeweils notwendigen Einzelfallprüfungen nicht möglich gewesen. Jenseits der juristischen Dimension hätte eine Volltext-Digitalisierung ohne Rechteklärung aber auch einen immensen symbolischen Schaden zur Folge: Die Rechte der Opfer würden grob missachtet.Die Ermittlung und Erschließung der Texte in einer Bibliographie, die Rückholung in das kommunikative Gedächtnis ist dagegen auch den Rechten der Opfer stark verpflichtet und will helfen, dass deren frühe Zeugnisse (wieder) sichtbar werden. Die tiefreichende qualitative Erschließung der Quelldokumente in Form eigens erstellter Annotationsdokumente (inhaltliche Zusammenfassung, Autorbiographie, Werkgeschichte, Verschlagwortung u.a.) bilden die Datengrundlage für die weiteren informationsverarbeitenden Schritte sowie das entstehende Online-Portal. Das entstehende Webportal soll ausgewählte relevante Metadatenstandards unterstützen und einschlägige Schnittstellen zum Harvesting von Metadaten bedienen können. Ortsbezogene Informationen aus der Erschließung der Quelldokumente werden darüber hinaus mit Grundlagenkarten verknüpft und in ein geographisches Informationssystem eingepflegt. Parallel zum Projekt entstehen überdies Qualifikationsarbeiten in der Literatur- und Geschichtswissenschaft, in denen die frühen Textzeugnisse sowie ihre Entstehungsbedingungen auch auf Grundlage der im Projekt erhobenen Daten und gewonnenen Erkenntnisse untersucht werden. Ferner werden zwei Konferenzen mit jeweils einem literatur- und einem geschichtswissenschaftlichen Schwerpunkt durchgeführt, deren Ergebnisse publiziert werden. In Einzelfällen, die besonders aussagekräftig sind und bei denen sich die Frage des Urheberrechts zweifelsfrei klären lässt, sollen auch frühe Textzeugnisse reediert und somit als Volltext dem Diskurs zugänglich gemacht werden. Referenzen Entrup, Bastian, Maja Bärenfänger, Frank Binder and Henning Lobin(2013a): IntroducingGeoBib: An Annotatedand Geo-referenced Online Bibliographyof Early German andPolish Holocaust and Camp Literature (1933–1949). Digital Humanities 2013, University of Nebraska–Lincoln, 16-19 July 2013. http://dh2013.unl.edu/abstracts/ab-229.html Entrup, Bastian, Frank Binder and Henning Lobin(2013b): Extendingthepossibilitiesforcollaborativeworkwith TEI/XML throughtheusageof a wiki-system. In: Proceedingsofthe 1st Workshop on Collaborative Annotations in Shared Environments: metadata, vocabulariesandtechniques in the Digital Humanities, DH CASE ""13. September 10 2013, Florence, Italy. <doi:10.1145/2517978.2517988>. Warmbrunn, Jürgen (2012). ""Das Vernetzen von Menschen, Daten und Systemen 'Die Forschungsbibliothek des Herder-Instituts in Marburg."" In: Bernhard Mittermaier (Hrsg.) Vernetztes Wissen 'Daten, Menschen, Systeme. 6. Konferenz der Zentralbibliothek, Forschungszentrum Jülich 5. - 7. November 2012 (Proceedingsband).ISBN 978-3-89336-821-1 http://hdl.handle.net/2128/4699"
2014,DHd2014,2014_cls_metadata_extracted.csv,LitSOM. Kartierung russischer Gegenwartsliteratur,"Gernot Howanitz (Universität Passau, Deutschland); Helmut Mayer (Universität Salzburg, Österreich)","Learning Vector Quantization, computergestützte Textanalyse, Distant Reading, literarische Self-Organizing Maps, Feature Extraction, Feature-Vektoren, Java, Unified Distance Matrix, Cross Validation","Learning Vector Quantization, computergestützte Textanalyse, Distant Reading, literarische Self-Organizing Maps, Feature Extraction, Feature-Vektoren, Java, Unified Distance Matrix, Cross Validation","Zusammenfassung Das literarische SOM (LitSOM) wendet selbstorganisierende Karten (Self-Organizing Maps, SOM) und Learning Vector Quantization (LVQ) auf russische Literatur an. Das SOM wird dafür eingesetzt, um eine Karte zeitgenössischer russischer Romane zu erstellen, die es Literaturwissenschaftlerinnen und Literaturwissenschaftlern erlaubt, Beziehungen zwischen den Romanen zu untersuchen. LitSOM ist eine ""Distant reading""-Technik. Die Qualität der Karten wird sowohl subjektiv aus der Sicht der Literaturwissenschaft, als auch objektiv, d.h. in einem ""klassischen"" Problem des Textmining, nämlich der Klassifizierung von Autorinnen und Autoren, bestimmt. Um dies zu erreichen, wird der SOM-Algorithmus durch den LVQ-Algorithmus ergänzt. 1. Einleitung 1.1 Überblick Ein Hauptziel dieses Beitrags ist die Implementierung eines Systems für computerunterstützte Textanalyse, das sogenannte Literary SOM (LitSOM). Darüber hinaus zeigen wir, wie Literaturwissenschaftlerinnen und Literaturwissenschaftler dieses System für ihre Forschungen einsetzen können. In unserer Beispielanwendung haben wir jeweils 15 Romane von acht verschiedenen Autorinnen und Autoren der russischen Gegenwartsliteratur kartiert, um die Nützlichkeit von quantitativen Methoden für die slavistische Literaturwissenschaft zu demonstrieren. Zwar gibt es eine lange russische Tradition quantitativer Zugänge zur Literatur, diese ist allerdings etwas in Vergessenheit geraten. So hat der bedeutende russische Mathematiker Andrej Markov 1913 ein Paper publiziert [1], in dem er das Konzept der Markovkette demonstriert. Markovketten erlauben es, Ereignisse zu modellieren, die nacheinander stattfinden. Heutzutage werden sie in verschiedensten Feldern angewandt, beispielsweise in den Wirtschaftswissenschaften oder in der Physik, und sie spielten auch eine Schlüsselrolle in Claude Shannons grundlegender Monographie zur Informationstheorie [2]. Trotz dieser vielfältigen Anwendungsmöglichkeiten hat Markov im Jahr 1913 den Versroman ""Eugen Onegin"" (1833) zu seinem Studienobjekt gemacht. Dieses Beispiel zeigt, wie stark die Verbindung zwischen Mathematik und Literatur im Russland des frühen 20. Jahrhunderts war. 1.2 Methodologie LitSOM basiert auf sogenannten selbstorganisierenden Karten (Self-Organizing Maps, SOM) [3]. Ein SOM ist perfekt geeignet für unstrukturierte Daten und unvollständige Information, weil es hochdimensionale Probleme vereinfachen und für den Menschen leicht verständlich darstellen kann. Deshalb eignen sich SOM sehr gut für Data Mining [4]. Ein SOM kann dazu verwendet werden, eine große Anzahl von Texten zu clustern und sie auf einer zweidimensionalen Karte anzuzeigen. Das sogenannte WEBSOM [5] clustert beispielsweise Newsgroup-Postings nach ihrem Inhalt. Das LitSOM funktioniert ähnlich, arbeitet aber mit Romanen anstelle kurzer Nachrichten. Es erstellt eine Karte, die die Abstände zwischen einzelnen Romanen darstellt 'je näher, desto ähnlicher. Dieser Text-Mining-Ansatz liefert Literaturwissenschaftlerinnen und Literaturwissenschaftlern eine automatische Visualisierung von Beziehungen zwischen unterschiedlichen Texten. Nach Franco Moretti ist LitSOM ein Werkzeug für ""distant reading"" [6], also für eine Mischung aus der klassischen literarischen Textanalyse (""close reading"") und dem Querlesen von Texten [7]. 2. Vorarbeiten 2.1 Implementierung des SOM Alle Bestandteile von LitSOM (SOM, Feature Extraction basierend auf Wortfrequenzen und eine Visualisierung mittels der Unified Distance Matrix [8]) wurden in Java implementiert. Für die Feature Extraction haben wir Sergej Sharovs Liste der 5000 häufigsten russischen Wörter verwendet [9]. Der Feature-Vektor wurde dann wie folgt zusammengestellt: Für jeden Roman wurden die Wörter aus der Sharov-Liste gezählt und jeweils durch die Gesamtanzahl der Wörter in diesem Roman dividiert. Dies gewährleistet, dass die einzelnen Feature-Vektoren untereinander vergleichbar bleiben. 2.2 Setup der Experimente Verschiedene Längen des Feature-Vektors wurden getestet: 5, 10, 25, 30, 40, 50, 75, 100, 125, 150, 175 und 200 Features. Um den Einfluss verschiedener Wortarten auf die resultierenden Karten zu untersuchen, wurde Sharovs ursprügliche Liste modifziert; Versionen rein mit Nomen und Verben sowie eine Kontrollgruppe mit allen anderen Wörtern wurde erstellt. Aufgrund eigener Testreihen haben wir uns für ein SOM aus 108 Neuronen in einem hexagonalen 9 _ 12 Gitter entschieden. Die Lernrate _(0) wurde auf 0.5 gesetzt und dann wie folgt verringert: _(t + 1) = _(t)/(1 + _(t)). Der anfängliche Nachbaschaftsradius wurde mit 2.5 festgelegt. Nach jedem Zyklus wurde dieser Radius um 0.0005 verringert. Nach einer unüberwachten SOM-Phase mit 3000 Zyklen folgte eine überwachte LVQ-Phase mit 1000 Zyklen. Insgesamt wurden 4800 Experimente durchgeführt und ebensoviele Karten erstellt. Tabelle 1 Rang Feature-Vektor Korrekt identifiziert LVQ-Genauigkeit 1NN-Genauigkeit 1 150 Verben 103 85,83% 92,50% 2 100 Nomen 102 85,00% 95,83% 3 100 Verben 101 84,16% 91,67% 4 200 Sharov 100 83,33% 90,00% 5 175 Nomen 99 82,50% 95,00% 6 100 Sharov 98 81,67% 90,00% 6 150 Sharov 98 81,67% 90,83% 6 125 Nomen 98 81,67% 93,34% 6 125 Verben 98 81,67% 91,67% 10 75 Nomen 97 80,83% 94,16% 10 40 Nomen 97 80,83% 89,16% 10 175 Verben 97 80,83% 92,50% 3. Resultate 3.1 Klassifizierung von Autorinnen und Autoren 40 verschiedene Konfigurationen und Leave One Out Cross Validation (LOOCV) resultierten in einer Gesamtanzahl von 4800 verschiedenen Experimenten. Die besten Resultate dieser 4800 SOM/LVQ-Läufe sind in Tabelle 1 angeführt. Das beste Resultat '86% richtig erkannt 'wurde mit einer Liste von den 150 häufigsten Verben als Feature-Vektor erzielt. Diese Resultate legen den Schluss nahe, dass die von LitSOM produzierten Karten die Verteilung der 120 Romane tatsächlich widerspiegelt. Mit einem 1NN-Klassifzierer, der als Kontrolle fungierte, wurde sogar eine Genauigkeit von 96% erreicht. 3.2 U-Matrix Die Qualität der Karten kann nur subjektiv bestimmt werden. Deshalb präsentieren wir hier eine Karte samt Interpretation als Beispiel. Im Allgemeinen ist anzumerken, dass zwischen einzelnen Karten durchaus Unterschiede festzustellen waren, allerdings glichen sich die Karten trotzdem meist in ihrer grundlegenden Struktur. Grafik 1: U-Matrix-Visualisierung des SOM für Viktor Pelevins ""Ananaswasser für eine feine Dame"" Grafik 1 zeigt die U-Matrix, die das LitSOM für Viktor Pelevins Roman ""Ananaswasser für eine feine Dame"" nach den SOM/LVQ-Läufen darstellt. Diese Karte wurde basierend auf einem PatternVektor mit den 100 häufigsten Nomen erstellt. Pelevins Roman diente als unbekannter Test-Text, d.h. nach dem Training mit den 119 anderen Romanen wurde dieser Text 'korrekt 'klassifiziert. Wie man sieht, weist LitSOM sehr gut auf Romane hin, die eher untypisch für die jeweilige Autorin oder den jeweiligen Autor sind. Beispiele dafür sind Vladimir Sorokins ""Die Schlange"" (gekennzeichnet durch ""ocher""). Auch die jeweiligen Relationen unterschiedlicher Autorinnen und Autoren zueinander sind nachvollziehbar, so liegen die beiden Fantasy-Autoren Sergej Lukjanenko und Viktor Pelevin nebeneinander. Unser letzter Test fand sozusagen unter realistischen Bedinungen statt: Pelevins neuester Roman ""Batman Apollo"" wurde am 8. März 2013 publiziert, nachdem der Großteil unserer Experimente bereits abgeschlossen war, damit hat er auch nicht Eingang in das ursprüngliche Textkorpus gefunden. In Grafik 1 findet man ""Batman Apollo"" (""betman"" in rot) gleich neben weiteren Pelevin-Romanen jüngeren Datums, vor allem auch ""Empire V"" (""ampir""). Blättert man diese Romane durch, erfährt man, dass ""Batman Apollo"" die Fortsetzung von ""Empire V"" ist. 4. Diskussion Die Ergebnisse der Klassifizierungsexperimente mit LVQ und 1NN sind sehr gut, vor allem in Anbetracht der Tatsache, dass literarische Texte sehr komplex sein können. Wortfrequenzen erlauben es, zwischen Romanen unterschiedlicher Autorinnen und Autoren zu differenzieren. Mit der Liste der 150 häufigsten Verben konnten 103 von 120 Romanen (86%) korrekt ihren jeweiligen Autorinnen und Autoren zugeordnet werden. Damit wurde empirisch belegt, dass LitSOM die Beziehungen zwischen Texten unterschiedlicher Autorinnen und Autoren sinnvoll darstellen kann. Die Visualisierung mittels U-Matrix, die LitSOM auch zur Verfügung stellt, erlaubt es, die Relationen zwischen unterschiedlichen Texten in einfacher Form darzustellen. Unsere subjektive Bewertung der Karten zeigte, dass die Wahl der Features großen Einfluss auf die visuelle Qualität der Karten hat. So waren die Cluster einzelner Autorinnen und Autoren bei auf der Nomen-Liste basierenden Karten am besten voneinander abgetrennt. Die Verben-Liste wiederum war für das Klassifizierungsexperiment besser geeignet, optisch waren die Karten allerdings weniger klar strukturiert. Im Allgemeinen eignen sich die Karten vor allem dazu, Texte zu finden, die für einen Autor oder eine Autorin untypisch sind bzw. die dem Stil einer anderen Autorin oder eines anderen Autors ähneln. Auch innerhalb eines Clusters lassen sich interessante Schlüsse hinsichtlich der Texte ziehen, so gibt es häufig Unterschiede zwischen noch in der Sowjet-√Ñra geschriebenen Texten und späteren, post-sowjetischen. LitSOM kann in vielerlei Hinsicht verbessert werden; bei den Feature-Vektoren sind noch viele weitere Kombinationen denkbar, die durch Feature-Selection-Algorithmen bestimmt werden könnten. Weiters ist es denkbar, die visuellen Karten automatisiert durch BildverarbeitungsAlgorithmen zu vergleichen, um eine objektivere Beurteilung zu erreichen. Auch der Einfluss der SOM-Parameter, etwa unterschiedlicher Kartengrößen, auf die visuelle Qualität der Karten ist noch nicht hinreichend untersucht. Weitere wertvolle Einblicke könnten durch Einbeziehung weiterer Texte aus unterschiedlichen literarischen Epochen gewonnen werden. Gleichzeitig würden all diese Experimente helfen, mehr Erfahrung im Umgang mit den Karten zu gewinnen. Diese Erfahrung ist sehr wichtig, denn schlussendlich kann nur ein Mensch die Karten interpretieren und als Ausgangspunkt für weitere Überlegungen nutzen 'ein Prozess, der ""distant reading"" und ""close reading"" verbindet. Quellen 1. Markov, A. 1913. Primer statisticheskogo issledovaniia nad tekstom ""Evgeniia Onegina"", illiustriruiushchii sviaz ispytanii v tsep‚Äô. Izvestiia Imperatorskoi Akademii Nauk 7.3. 2. Shannon, CE. and Weaver, W., 1949. The Mathematical Theory of Communication. University of Illinois Press, Illinois. 3. Kohonen, T. 2001. Self-Organizing Maps. Berlin. 4. Lagus, K. et al. 1999. WEBSOM for Textual Data Mining. Artificial Intelligence Review 13 (5/6). http://citeseerx.ist.psu.edu/ viewdoc/summary?doi=10.1.1.12.5452 [accessed 17 March 2013]. 5. Kohonen, T. et al. 2000. Self Organization of a Massive Document Collection. IEEE Transactions on Neural Networks 11 (3), 574‚Äì585. http://lib.tkk.fi/Diss/2000/isbn9512252600/article7. pdf [accessed 14 March 2013]. 6. Moretti, F. 2000. Conjectures on World Literature. New Left Review 1, 54-68. http://newleftreview.org/II/1/franco-moretti-conjectures-on-world-literature [accessed 30 November 2013]. 7. Kirschenbaum, M. 2007. The Remaking of Reading: Data Mining and the Digital Humanities. National Science Foundation Symposium on Next Generation of Data Mining and CyberEnabled Discovery for Innovation. http://citeseerx.ist.psu.edu/viewdoc/download? doi=10.1.1.111.959&rep=rep1&type=pdf [accessed 9 May 2013] 8. Kohonen, T. 2001. Self-Organizing Maps. Berlin, 165f. 9. Sharov, S. 2001. Chastotnyi slovar‚Äô. RosNII II Website. http: //www.artint.ru"
2015,DHd2015,2015_cls_metadata_extracted.csv,Das artifizielle Manuskriptkorpus TASCFE,Armin Hoenen (Goethe Universität Frankfurt am Main),"Stemma, Stammb‚âà‚Ä†ume, R, automatische Erkennung, Distanz, Autoren Average Sign Distance","Stemma, Stammb‚âà‚Ä†ume, R, automatische Erkennung, Distanz, Autoren Average Sign Distance","1 Abstrakt In diesem Paper soll das Teheran Artificial Shahname Corpus with Frankfurt Extension (TASCFE) digitalisierter handschri_licher Texte zur Evaluation automatisch generierter Stemmata vorgestellt werden. Ein Stemma codicum oder kurz Stemma ist eine Visualisierung der genealogischen Zusammenhänge innerhalb eines Manuskriptkorpus oder einfacher gesagt ein Manuskriptstammbaum. Die Generierung solcher Stammbäume verfolgt generell zwei Hauptziele: ein genaueres Verständnis der Überlieferungsgeschichte und die Rekonstruktion eines Urtextes. Bis in die 90er Jahre hinein wurden Stemmata vornehmlich manuell erstellt, sind aber seitdem zunehmend auch automatisch generiert und analysiert worden, siehe u.a. Spencer et al. (2004), Roos and Heikkilä (2009) und Roelli and Bachmann (2010)._ Technologisch ist die bio-informatische Phylogenie Donordisziplin, wie die Nutzung phylogenetischer Programme und Algorithmen zur automatischen Manuskriptstammbaumerstellung zeigt. Dabei fehlt es in der biologischen Phylogenie an Möglichkeiten, erzeugte Stammbäume zu evaluieren, da die Aufspaltungsvorgänge der Spezies, die durch die Verzweigungen symbolisiert werden nicht beobachtet und aufgezeichnet werden konnten, lagen sie doch z.T. Millionen von Jahren in der Vergangenheit. Im Gegensatz dazu ist es in der Stemmatologie durchaus möglich, sowohl die Vorlage als auch die Kopie im Korpus vorzufinden. Mehr noch, es ist möglich neue Korpora zu erzeugen und gleichzeitig die Kopiergeschichte der Manuskripte aufzuzeichnen. Diese Daten können dann in einem klassisch informatischen Evaluationsszenario der Beurteilung von stem_Für eine detaillierte Darstellung der historischen Entwicklung der Stemmatologie siehe O""Hara (1996), Robinson and O""Hara (1996), van Reenen et al. (1996) und van Reenen et al. (2004). 1 Text Sprache Anzahl Manuskripte Anzahl Worte Publikation Parzival Englisch 21 957 Spencer et al. (2004) Notre Besoin Französisch 13 1029 Ph.V. Baret (2004) Heinrichi Altfinnisch 64 1208 Roos and Heikkilä (2009) Shahname Persisch 50 107 Hoenen (hic ipsum) Abbildung 1: Die artifiziellen Traditionen magenerierenden Methoden genutzt werden. Artifizielle Korpora wurden bisher drei Mal erzeugt, siehe Ph.V. Baret (2004), Spencer et al. (2004) und Roos and Heikkilä (2009). Nur das letztgenannte Paper evaluierte mehrere stemmagenerierende Algorithmen, darunter auch die händische Rekonstruktion, mi_els einer Distanzfunktion zwischen dem echten Stemma und den erzeugten. Diese Distanz nannten die Autoren Average Sign Distance (ASD). Sie misst die Öhnlichkeit der Topologien des korrekten und des erzeugten Stammbaums anhand der Öhnlichkeit der inneren Abstände aller Knotentripel (von vorhandenen Manuskrip_exten) im erzeugten mit deren shortest-path Abständen im echten Stammbaum. Abbildung 1 fasst Kennwerte der drei artifiziellen Korpora zusammen. Alle bisher bekannten artifiziellen Traditionen sind im lateinischen Alphabet verfasst. Hier wird das TASCFE Korpus vorgestellt, welches in persischer Sprache (Farsi) im arabischen Alphabet vorliegt. Neben der Sprache besteht seine Besonderheit _ür eine Bereicherung der Landscha_ artifizieller Korpora darin, orale Variation zu approximieren. Orale Variation ist solche Variation, die nicht aufgrund von Fehlern im Kopierprozess, sondern aufgrund der Dynamik mündlicher Überlieferung entstanden ist und die zum Teil stark von erstgenannter Variation abweicht. Die Oral Formulaic _eory (OFT) wurde in den 30er bis 60er Jahren des vorigen Jahrhunderts durch Parry and Parry (1987) und Lord (1960) im Zusammenhang mit der Homerischen Frage erarbeitet. Ergebnis dieser _eorie war u.a. die Erkenntnis, dass Texte wie die Odyssee keinen Urtext, d.h. keine Originalversion besitzen. In der Zeit vor Erfindung der Schri_ wurden Texte ausschließlich oral tradiert. Dabei war zur konkreten Textmanifestation ein Au_ührender und (mindestens ein) Zuhörer notwendig. Da die Umstände jeder Au_ührung jedoch unterschiedlich waren, war es so auch der Text selbst. Z.B. nutzte ein Barde bei derselben Geschichte viele Ausschmückungen (z.B. Adjektive), wenn er viel Zeit ha_e, erzählte sie jedoch ein anderes Mal, wo die Zeit drängte, ohne Ausschmückungen. Dazu kommen Fehler des menschlichen Erinnerungsapparates, der andersartige Variationen erzeugt, als solche, die beispiels2 weise durch Buchstabenverwechslung beim Abschreiben zu Stande kommen. Zu Beginn der Schri_ein_ührung wurden Texte via ""Pseudo-Au_ührungen"" vor einem Schreiber (Diktate) erstmals in schri_liche Form über_ührt. Da derselbe Text mehrfach in solchen Diktaten aufgezeichnet worden sein kann, da weiterhin jede Au_ührung ganz wie in der rein oralen Welt dieselbe Geschichte in unterschiedlicher Textform (mehr Ausschmückungen/weniger Ausschmückungen u.a. Arten oraler Varianz) hervorbrachte, können am Beginn mancher (meist der frühesten) Manuskrip_raditionen Varianten stehen, die sich nicht mit den Arten an Variation aus rein literarisch überlieferten (d.h. als schri_liche Texte entstandenen) Texten decken. Solch eine Variation ist _ür die Stemmagenerierung wichtig, da sie determiniert, ob ein einziges oder mehrere Stemmata und ob mehrere oder nur ein Urtext angenommen werden müssen. Das TASCFE Korpus trägt dieser Art der Variation zuminndest teilweise Rechnung, da an seinem Anfang vier verscheidene Versionen stehen. Neben der Sprache ist dies die zweite stemmatologierelevante Besonderheit des TASCFE. Der Text ist ein Auszug (Strophe) aus dem persischen Nationalepos Shahname (Buch der Könige), (l Qasim Ferdoussi, 1967, p.55). Es entstand um das Jahr 1000. Die Autorenscha_ wird generell Abu l-Qasim Ferdoussi zugerechnet, wobei orale Einflüsse im Werk bereits seit längerem diskutiert werden, siehe u.a. Yamamoto (2003) und Rubanovich (2011) . Die ca. 6.500 Token des Korpus wurden 2014 in Teheran (43 Manuskripte) und in Frankfurt (7 Manuskripte) von Freiwilligen entweder von einer gedruckten oder einer handschri_lichen Vorlage durch Abschreiben produziert. Anschließend wurde das Korpus digitalisiert und aligniert. Ein des Persischen nicht mächtiger Freiwilliger kopierte zusätzlich eines der handgeschriebenen Manuskripte, um der _ese nachzugehen, dass in historischer Zeit Analphabeten oder Schreiber anderer Schri_en Manuskripte kopiert haben könnten, was sich aber deshalb als unwahrscheinlich erwies, da es einer persischen Mu_ersprachlerin aufgrund der partiellen Unlesbarkeit nicht möglich war von dem so kopierten Manuskript eine weitere Kopie anzufertigen. Kein einziges Manuskript entsprach genau der Vorlage. Eine qualitative Analyse der Phänomene, die denen historischer Korpora ähnelten (so z.B. Zeilensprünge oder Wortsprünge, aber auch synonymische Ersetzungen) konnte zeigen, dass aufgrund des Schri_systems und der daraus teils zur lateinischen Schri_ unterschiedlichen Fehler eine andere Di_erenzierung in Fehlerklassen je nach 3 Schri_sytsem notwendig sein kann. Dies knüp_ an Andrews and Macé (2013) an, die zeigen konnten, dass Variationsklassen je nach Sprache variieren können. Das digitale Zeitalter erö_net dahingehend die Möglichkeiten einer schri_systemübergreifenden Analyse von durch Abschreibefehler verursachter Variation, die dann zur Abstraktion der dort wirkenden universalen Prinzipien beitragen wird, siehe Abbildung 2. Dies setzt die Scha_ung geeigneter Ressourcen voraus. Auf die Daten wurden im Weiteren stemmatologische Algorithmen angewandt (die dann mi_els der oben angesprochenen ASD evaluiert wurden). Hierbei konnte gezeigt werden, dass ein Ansatz zur Feststellung von Oralität durch Gruppenbildung im Stemma besteht, wobei die Levenshtein Distanz, Levenshtein (1965), bei hoher Gewichtung von Lücken im Alignment eine besonders geeignete und gleichzeitig leicht zugängliche algorithmische Basis darstellt, siehe Abbildung 3. Dabei wurde ein wortpaar-basierter Vergleich aller Manuskriptpaare durchge_ührt. Die Levenshtein Distanz aller Wortpaare des jeweiligen Manuskriptpaares wurde (auch als Baseline _ür den Vergleich mit weiteren Algorithmen) zu einer Manuskriptpaargesamtdistanz aufsummiert. Die Matrix der Manuskriptpaardistanzen wurde mi_els des Neighbor Joining Algorithmus, Saitou and Nei (1987), wiederum eine geeignete Baseline, aus dem ""ape"" Packet (Paradis et al. (2004), Paradis (2012)) der Programmiersprache R in einen Stammbaum über_ührt, der dann visualisiert und evaluiert wurde, siehe Abbildung 4._ _www.r-project.org/ 4 Abbildung 2: Die durch ungewöhnliche Buchstabenform ausgelöste Fehlkopie von (oben) nach (unten) in roten Rechtecken. Die Kennstellen, die den Abschreibefehler ausgelöst haben, sind farblich markiert. Das in ist nicht so rund wie und länger als erwartet (blau). Zudem ist der einzelne Punkt auf dem versehentlich breiter (hautfarben). Dennoch ist im oberen Rechteck eindeutig nur ein Punktmuster erkennbar, unten jedoch zwei (grün). Des Weiteren hat das zwei deutliche Hacken, wobei der mit rotem Sti_ unterlegte untere entsprechende Buchstabe nur einen aufweist. Außerdem ist das in der unteren rechten Ecke rund, was auf das vorausgehende jedoch nicht zutri_ (gelb). Obgleich der Abschreibefehler höchstwahrscheinlich durch die ungewöhnliche Form des ausgelöst wurde, passte die Ersetzung gut in den Kontext, vielleicht sogar besser als das Original. Genau diese Interaktion von kontextuellem Priming und ungewöhnlichen Buchstabenformen ist ein idealer Kandidat _ür schri_systemübergreifende Prozesse beim Abschreiben. 5 Abbildung 3: Automatische Erkennung einer durch orale Variation gekennzeichneten Gruppe. Version ASD Shahname(V1) 55, 59 Shahname(V2) 55, 13 Shahname(V3) 57, 93 Shahname(V4) 55, 83 Shahname(Durchschni_ V1-V4) 56, 12 Shahname(als eine Tradition) 38, 31 Abbildung 4: Evaluation der erzeugten Stemmata (ASD). Das Stemma der Gesamttradition unter der Annahme nur einer Wurzel evaluiert mit diesen Algorithmen deutlich schlechter als der Durchschni_ der einzelnen Versionen. 6 Literatur Andrews, T. L. and Macé, C. (2013). Beyond the tree of texts: Building an empirical model of scribal variation through graph analysis of texts and stemmas. Literary and Linguistic Computing, 28(4):504‚Äì521. l Qasim Ferdoussi, A. (1966-1967). _e Shahname - the book of kings. _e Great Islamic Encyclopaedia. Levenshtein, V. I. (1965). Binary codes capable of correcting deletions, insertions, and reversals. Doklady Akademii Nauk SSSR, 163(4):845‚Äì848. english in: Soviet Physics Doklady 10 (8) (1966) 707‚Äì710. Lord, A. B. (1960). _e Singer of Tales. Harvard University Press. O""Hara, R. J. (1996). Trees of history in systematics and philology. Memorie della Societ√† Italiana di Scienze Naturali e del Museo Civico di Storia Naturale di Milano, 27(1):81‚Äì88. Paradis, E. (2012). Analysis of Phylogenetics and Evolution with R. Springer, New York, 2nd edition. Paradis, E., Claude, J., and Strimmer, K. (2004). Ape: analyses of phylogenetics and evolution in r language. Bioinformatics, 20:289‚Äì290. Parry, M. and Parry, A. (1987). _e Making of Homeric Verse: _e Collected Papers of Milman Parry. Oxford University Press. Ph.V. Baret, C.Macé, P. (2004). Testing methods on an artificially created textual tradition. In Linguistica Computationale XXIV-XXV, volume XXIV-XXV, pages 255‚Äì281, Pisa-Roma. Instituti Editoriali e Poligrafici Internationali. Robinson, P. M. and O""Hara, R. J. (1996). Cladistic analysis of an old norse manuscript tradition. Research in Humanities Computing (4). Roelli, P. and Bachmann, D. (2010). Towards generating a stemma of complicated manuscript traditions: Petrus alfonsi""s dialogus. Revue d""histoire des textes, 5(4):307‚Äì321. 7 Roos, T. and Heikkilä, T. (2009). Evaluating methods for computer-assisted stemmatology using artificial benchmark data sets. Literary and Linguistic Computing, 24:417‚Äì433. Rubanovich, J. (2011). Medieval Oral Literature, chapter Orality in Medieval Persian Literature, pages 653‚Äì680. De Gruyter. Saitou, N. and Nei, M. (1987). _e neighbor-joining method: a new method for reconstructing phylogenetic trees. Molecular biology and evolution, 4(4):406‚Äì 425. Spencer, M., Davidson, E. A., Barbrook, A., and Howe, C. J. (2004). Phylogenetics of artificial manuscripts. Journal of _eoretical Biology, 227:503‚Äì511. van Reenen, P., den Hollander, A., and van Mulken, M. (2004). Studies in Stemmatology II. Studies in Stemmatology. John Benjamins Publishing Company. van Reenen, P., van Mulken, M., and Dyk, J. (1996). Studies in Stemmatology I. Studies in Stemmatology. John Benjamins Publishing Company. Yamamoto, K. (2003). _e Oral Background of Persian Epics. Brill, Leiden."
2015,DHd2015,2015_cls_metadata_extracted.csv,Digitale Netzwerkanalyse dramatischer Texte,Peer Trilcke (Georg-August-Universität Göttingen); Frank Fischer (Göttingen Centre for Digital Humanitie); Dario Kampkaspar (Herzog August Bibliothek Wolfenbüttel),"Netzwerkanalyse, Dramen","Netzwerkanalyse, Dramen","1 Einleitung Das Projekt ""Digitale Netzwerkanalyse dramatischer Texte"" steht in der Tradition strukturanalytischer Ansätze in der Literaturwissenschaft (allgemein Titzmann 1977), die es einerseits im Sinne eines konsequent netzwerkanalytischen Relationismus (mit Rekurs auf die Social Network Analysis, siehe u. a. Wasserman/Faust 1998), andererseits unterst¬® utzt durch Verfahren der automatisierten Datenerhebung und -auswertung weiterentwickelt, um sie auf größere Textkorpora anzuwenden und so umfassende relationale Daten ¬® uber Prozesse des literaturgeschichtlichen Strukturwandels gewinnen zu können. Als theoretisches Fundament dient dabei eine netzwerkanalytische Konzeptualisierung dramatischer Interaktion (erste Ideen dazu prominent bei Moretti 2011; Kritik und literaturtheoretisch begr¬® undete Rekonzeptualisierung bei Trilcke 2013 'dort auch ein ausf¬® uhrlicher Forschungs¬® uberblick), die 'in Fortf¬® uhrung von Konzepten der dramatischen Konfiguration (Marcus 1973, Pfister 1977; problematisch hingegen, weil mit di_user Konzeptualisierung: Pohlheim 1997) 'zunächst bei einer rudimentären Operationalisierung ansetzt, nach der eine ""Interaktion"" dann vorliegt, wenn zwei Figuren innerhalb einer durch die ¬® uberlieferte Struktur des Textes vorgegebenen Subsegmentierungseinheit (in der Regel ""Szene"" oder ""Auftritt"") als Sprecher aufgef¬® uhrt werden. ""Interaktion"" wird in diesem Sinne 'und zu Zwecken einer ersten Automatisierung 'verstanden als ""szenische Kopräsenz zweier Sprecher"". Auf Grundlage der so definierten Relation werden im Rahmen des Projekts automatisiert netzwerkanalytische Daten erhoben, die sowohl global die ""Interaktions""-Netzwerke der Dramen (Density, Average Degree, Connectedness u. dergl.) als auch fokussiert einzelne Akteure charakterisieren (Degree sowie diverse weitere Centrality-Indices). Der erstellte Workflow ermöglicht auch die Datenerhebung auf Mesoebene (u. a. Identifizierung von Clustern) und beinhaltet dar¬® uber hinaus Visualisierungen der Netzwerkdaten, die wiederum zur Analyse des literaturgeschichtlichen Strukturwandels beitragen. 2 Wahl des Dramenkorpus F¬® ur die automatisierte Analyse von Dramen war ein verlässliches und gen¬® ugend großes Dramenkorpus vonnöten. Infrage kamen hier: ‚Ä¢ Deutsches Textarchiv (DTA): 49 Dramen1 ‚Ä¢ Wikisource: 50 Dramen2 ‚Ä¢ Projekt Gutenberg-DE: 641 Dramen3 ‚Ä¢ Textgrid Repository: 690 Dramen4 1http://www.deutschestextarchiv.de 2http://de.wikisource.org/wiki/Kategorie:Drama 3http://projekt.gutenberg.de 4http://www.textgridrep.de 1 Das DTA hat zwar das qualifizierteste (TEI-)Markup, besteht aber bisher nur aus vergleichsweise wenigen Texten. Letzteres gilt auch f¬® ur die Dramen im deutschsprachigen Zweig von Wikisource. Das Projekt Gutenberg-DE wiederum, das seit 2002 bei Spiegel Online gehostet wird, hat das Problem, dass es nicht mit brauchbarem Markup versehen ist, nur rudimentärem XHTML. Deshalb kam eigentlich nur das TextGrid Repository infrage, das sich aus den alten Zeno.org-Volltexten speist und basale TEI-Auszeichnungen aufweist. Aus dem TextGrid-Gesamtkorpus wurden zunächst die in den Metadaten mit dem Genre ""drama"" versehenen Texte extrahiert, insgesamt 690. Dazu gehören vor allem deutschsprachige Dra¬® men von ca. 1500 bis 1930 sowie ferner u. a. Ubersetzungen von einem Dutzend griechischer Tragödien und einiger Shakespeare-Dramen. Aus der Gesamtmenge lassen sich prinzipiell auch recht einfach zeitlich gesta_elte Teilkorpora erstellen, denn im TEI-Header stehen innerhalb von <creation></creation> rudimentäre Entstehungsdaten (Beispiele: <date when=""1802""/>, aber auch weitläufige Eingrenzungen wie <date notBefore=""1738"" notAfter=""1758""/>). 3 Erhebung der Netzwerkdaten Als Zwischenschritt wurde f¬® ur jede der 690 TEI-Dateien eine Relationsliste (CSV-Datei) erzeugt, die den gängigen Formaten der Speicherung netzwerkanalytischer Daten entspricht. Zur Extraktion der Sprecherdaten sind in der Regel zwei getrennte Schritte nötig: Das Erkennen der einzelnen Teile eines Theaterst¬® uckes und danach das Erkennen der einzelnen Sprecher. Zur Erleichterung der nachstehenden Arbeiten teilt das Skript die vorliegenden Dateien auf: F¬® ur jede erkannte Ebene (die Datei selbst ist dabei auch eine) wird ein Unterverzeichnis angelegt, in dem wiederum TEI-Dateien mit den einzelnen Teilen stehen und in das auch die jeweiligen Registerdateien geschrieben werden. Anhand der aufgeteilten Dateien werden verschiedene Arten von Ausgaben erstellt. Zum einen ist dies ein kleinteiliges Register aller <speaker>-Tags, aber auch aller Auszeichnungen <rs> und <person>. Um eindeutige Referenzziele zu erhalten, werden ggf. ID-Nummern vergeben (dies erleichtert insbesondere auch spätere Eingri_e, wenn problematische Namen manuell korrigiert werden m¬® ussen). Zum andern werden die Kookkurrenzlisten erstellt. Im untersten Verzeichnis werden die Vorkommen aller Sprecherpaare in allen Dateien gezählt. In den dar¬® uberliegenden werden die Werte aller Unterverzeichnisse addiert. Neben dem Erkennen der Struktur ist die korrekte Zuordnung von Namen die größte Herausforderung. Im Idealfall sind alle <speaker> mit einem Attribut @who versehen, ¬® uber das eine normierte Form des Namens erreicht werden kann. Ist dies nicht der Fall (oder sind stattdessen die Tags <rs> oder <person> verwendet worden), muss das Skript den Textinhalt des Tags auswerten, wobei neben möglichen Verschreibungen (bei der Transkription oder in der Vorlage) auch syntaktische änderungen auftreten können. So findet sich bei Lessing, Nathan der Weise V/1, neben Saladin ""Ein Mameluck"", der nach seinem ersten Auftreten als ""Der Mameluck"" gef¬® uhrt wird. Es folgt ein weiterer: ""Ein zweiter Mameluck"", danach ""zweiter Mameluck"". Ist es hier noch möglich, durch ein Ber¬® ucksichtigen der Artikel mit einfachen Mitteln gute Ergebnisse zu erzielen, stellt sich dies bei Emilia Galotti etwas schwieriger dar, da teils nur ""Odoardo"", teils aber ""Odoardo Galotti"" erscheint. Auch Fälle mit mehreren Sprechern (z. B. ""Alle"") sind nicht ganz trivial zu bearbeiten. Neben dem Versuch, diese Fälle automatisch zu klären, besteht in diesen Zweifelsfällen aber immer noch die Möglichkeit des manuellen Eingri_s, wozu die erstellten Indexdateien mit eindeutiger ¬® ID beitragen können. In einer weiteren Uberarbeitung des Skriptes ist es vorgesehen, eine einfache graphische Oberfläche anzubieten, ¬® uber die solche Zweifelsfälle bearbeitet werden können. 4 Datenauswertung und Visualisierung Die Datenauswertung erfolgt ¬® uber Python (3.4.x) mit dem igraph-Paket, das sowohl zum Visualisieren der Graphen als auch zum Berechnen der netzwerkanalytischen Daten genutzt wird. F¬® ur eine erste Visualisierung des Datenbestands wurden die Graphdaten an eine spring-embeddingMethode ¬® ubergeben (Fruchterman-Reingold), die versucht, a_ne Knoten näher beieinander anzuordnen und dadurch deutlich sichtbar zu clustern. Einen Eindruck des gesamten Korpus vermittelt 2 Abbildung 1, die 671 Dramen aus 2500 Jahren Dramengeschichte enthält, chronologisch links oben mit den Griechen beginnend und bis rechts unten ins zweite Viertel des 20. Jahrhunderts reichend: Abbildung 1: Netzwerkgraphen von 671 Dramen aus dem TextGrid Repository Die visualisierten Graphen haben auch deutlich gemacht, dass die meisten berechneten CSVDateien wegen des teils nicht eindeutigen Markups zumindest kleine Fehler aufwiesen. Diese Erkenntnisse konnten zur Fehlerbehandlung an den vorhergehenden Schritt (Erhebung der Netzwerkdaten) zur¬® uckgegeben werden. Erste strukturanalytische Berechnungen erfolgten auf Basis der 12 (vollendeten) Lessing-Dramen. Entsprechende Diagramme sind in Abbildung 2 zu finden. 5 Ausblick Die erhobenen und bereinigten Netzwerkdaten sollen als Grundlage f¬® ur alle statistischen Berechnungen dienen und auch ö_entlich zur Verf¬® ugung gestellt werden. Im Mittelpunkt der Forschung steht nun die Implementierung zusätzlicher netzwerkanalytischer Berechnungstools (etwa zur Bestimmung der Betweenness Centrality, mit der die Wichtigkeit einzelner Figuren f¬® ur das Netzwerk bestimmt werden kann). Dar¬® uber hinaus wird an der Qualifizierung der Netzwerkdaten gearbeitet (außer dem reinen Fakt, dass Figuren miteinander sprechen: Redeanteile quantifizieren, B¬® uhnenpr äsenz nicht sprechender Personen mit einbeziehen usw.) sowie an der Erstellung multiplexer Netzwerke, die nicht nur die oben definierten ""Interaktions""-Relationen erfassen, sondern auch u. a. Verwandschafts- oder instrumentelle Beziehungen ber¬® ucksichtigen. 3 Abbildung 2: Beispielberechnungen anhand der Lessing-Dramen (x-Achse): Damon, 1747 'Der junge Gelehrte, 1747 'Der Misogyn, 1748 'Die alte Jungfer, 1748 'Der Freigeist, 1749 'Die Juden, 1749 'Der Schatz, 1750 'Miß Sara Sampson, 1755 'Philotas, 1759 'Minna von Barnhelm, 1767 'Emilia Galotti, 1772 'Nathan der Weise, 1779 Literatur Marcus, Solomon. Mathematische Poetik. Frankfurt/M. 1973. Moretti, Franco. Network Theory, Plot Analysis. Stanford Literary Lab Pamphlets 2 (1. 5. 2011). http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf. Pfister, Manfred. Das Drama. Theorie und Analyse. M¬® unchen 1977 u. ö. Pohlheim, Karl Konrad (Hg.). Die dramatische Konfiguration. Paderborn u. a. 1997. Titzmann, Michael. Strukturale Textanalyse. Theorie und Praxis der Interpretation. M¬® unchen 1977 u. ö. Trilcke, Peer. Social Network Analysis (SNA) als Methode einer textempirischen Literaturwissenschaft. In: Philip Ajouri, Katja Mellmann u. Christoph Rauen (Hg.): Empirie in der Literaturwissenschaft. M¬® unster 2013. S. 201‚Äì247. Wasserman, Stanley; Katherine Faust. Social Network Analysis. Methods and Applications. Cambridge u. a. 1998."
2015,DHd2015,2015_cls_metadata_extracted.csv,Wann findet die deutsche Literatur statt? Zur Untersuchung von Zeitausdr¬® ucken in großen Korpora,Frank Fischer; Jannik Strötgen,"Korpus, Zeitangaben","Korpus, Zeitangaben","1 Einleitung Exakte Datumsangaben sind ein Merkmal vieler Prosatextsorten. In der Literatur finden sich dagegen bevorzugt ungefähre Datumsangaben, die Interpretationsräume ö_nen. So sind etwa alle 19 Monatsnennungen in Theodor Storms Schimmelreiter ungefährer Natur (""an einem OctoberNachmittag"", ""Ende März"" usw.). Ausnahmen von dieser Regel bilden Tagebuch-, Brief-, Abenteuerund historische Romane: In Goethes Werther oder Jules Vernes Tour du monde en quatre-vingts jour wimmelt es genretypisch von exakten Datumsangaben, im letztgenannten Roman sind sie in Form eines Countdowns gar unentbehrlicher Teil der Handlung. Ansonsten lässt sich als Hypothese formulieren: Kommt in der erzählenden Literatur ein exaktes Datum vor, ist das eine narrative Setzung, die der näheren Analyse lohnt. Davon ausgehend lassen sich dezidiert literaturwissenschaftliche Fragen ins Blickfeld nehmen: ‚Ä¢ Ist die Vermeidung exakter Datumsnennungen tatsächlich ein durchgehendes Merkmal bestimmter literarischer Genres? In welchem Verhältnis stehen exakte zu ungefähren Datumsangaben? ‚Ä¢ Kann die Frequenzanalyse von Datumsangaben zu Genreuntersuchungen genutzt werden? ‚Ä¢ Welche andere Bedeutung haben Datumsangaben neben der zeitlichen Verortung der Handlung? (""Semiotisierung"" eines Datums) ‚Ä¢ Gibt es zeitliche Konjunkturen f¬® ur bestimmte Datumsnennungen? Wenn ja, warum? ‚Ä¢ Welche Rolle spielen fiktive Daten? (vgl. etwa Erich Kästners 35. Mai und Shakespeares 80. April in The Winter""s Tale, Autolycus"" Ballade im 4. Akt) In unserem Vortrag widmen wir uns den Datumsangaben als einem isolierbaren feature literarischer Korpora im Sinne Matthew Jockers: ""Indeed, the very object of analysis shifts from looking at the individual occurrences of a feature in context to looking at the trends and patterns of that feature aggregated over an entire corpus"" (Jockers 2013, S. 24). Anhand dieses Features (der expliziten Datumsnennung) als einzeln betrachtbare Einheit soll die Praxis der literaturwissenschaftlichen Makroanalyse methodisch bereichert werden. Die Extraktion der Zeitangaben erfolgt automatisiert mithilfe eines Temporal Taggers. Indem es dabei um die Untersuchung der Repräsentanz eines außerliterarischen Phänomens (Zeit, Datumsangaben) in literarischen Texten geht, wird auch ein Beitrag zur Erzählforschung geleistet. Die mit den Methoden der Digital Humanities erzielten Erkenntnisse werden dadurch in die Fachdisziplin (in diesem Fall die Literaturwissenschaften) zur ¬® uckgespielt. 1 2 Vorgehen Unser Vorgehen bestand aus vier meist parallel ablaufenden Schritten: 1. Zusammenstellung geeigneter (deutschsprachiger) Korpora. 2. Erhebung der Daten durch Einsatz des Temporal Taggers HeidelTime (Strötgen & Gertz 2012) zur automatischen Extraktion zeitlicher Ausdr¬® ucke im Sinne der Temporal Markup Language TimeML (Pustejovsky et al. 2003). 3. Analyse der Daten (von Heatmaps zu Einzelfällen). 4. Entwicklung einer Android-App zur explorativen Analyse des ""literarischen Jahres"". Zunächst haben wir mit dem TextGrid Repository1 und Gutenberg-DE2 zwei große literarische Korpora zusammengebracht, mit HeidelTime auf Datumsstrukturen untersucht und anhand der expliziten (und damit sehr sicher richtigen) Ausdr¬® ucke eine kalendarische Heatmap erstellt (""1"" bedeutet 0–9 Vorkommen, ""2"" bedeutet 10–19 Vorkommen usw., ""+"" bedeutet 90 oder mehr Vorkommen). Dabei zeigten sich erwartete und unerwartete Konjunkturen: JAN: +566554435455576554574445555455 FEB: 65655546454635554446554666762 MAR: 84553334565364646+4644435444465 APR: +55555344664466554465555646447 MAY: +777765557566565674836454565466 JUN: 957486574646656657586656444576 JUL: 9479486554468975676654555565465 AUG: +6+565555+5665+6974755775555556 SEP: 9745735555446575+7695554546457 OCT: +375564536445665+76734555645555 NOV: +68557546656549665554645545456 DEC: 77455755464455547644554+5533457 Wie man sieht, kommen Monatserste und fixe Feiertage (Neujahr, Weihnachten) besonders häufig vor. Ansonsten fiel die Konjunktur weiterer Tage auf, etwa des ""18. März"". Die Vermutung lag nahe, dass die Nennung dieses Datums in Werken nach 1848 und damit nach der Märzrevolution ansteigt, da dieser Tag wegen der blutigen Ereignisse in Berlin eine eigene Semantik annahm. Die Erhebung und Analyse solcher Datumskonjunkturen soll systematisch ausgebaut werden. 3 Untersuchung des DTA-Korpus Dieser ersten quantitativen Analyse lagen die beiden erwähnten Korpora zugrunde, in denen aber äuch nichtliterarische Werke und (v. a. bei Gutenberg-DE) auch Ubersetzungen fremdsprachiger Literatur vorkommen. Um mit dieser Methode belastbare Ergebnisse zu gewinnen, bedurfte es eines qualifizierteren Textkorpus. Das Deutsche Textarchiv (DTA) ist zwar weit weniger umfangreich, aber es beinhaltet (nomen est omen) nur original deutschsprachige Texte und ist sowohl grob zeitlich 'nach Jahrhunderten 'als auch thematisch 'nach Genres 'sortiert, sodass dort Hinweise auf Konjunkturen in literarischen Texten zu erwarten waren. In Tabelle 1 sind Informationen zu den DTA-Teilkorpora dargestellt. Die Anzahl der enthaltenen Texte stellt zwar letztlich keine kritische Masse f¬® ur large-scale Untersuchungen wie die unsere dar, wir versprachen uns aber weitere Hinweise auf Chancen und Grenzen der Methode. Eine Analyse des einzeln abrufbaren Belletristik-Korpus ergab eine weit weniger stabile Heatmap als oben und als häufigste Daten (zwischen ca. 20 und 70 Nennungen) die folgenden: 1. 1., 1. 4., 20. 4., 1. 5., 10. 8., 3. 11. Die DTA-Stichprobe hebt wieder die Bedeutung des 1. Mai heraus, unter den unerwarteten Daten sei der ""10. August"" herausgegri_en, der neben der zeitlichen Verortung fiktionaler Handlungen wiederum auch auf ein historisches Datum zur¬® uckzuf¬® uhren ist, den Tuileriensturm am 10. August 1792 (vgl. in B¬® uchners Dantons Tod : ""Erster B¬® urger: Danton war unter uns am 10. August, 1http://www.textgridrep.de/ 2http://gutenberg.spiegel.de/ 2 TimeML explizite Dokumente Sätze Tokens Zeitausdr¬® ucke Tagausdr¬® ucke 1600–1699 124 957,950 15,779,536 34,425 635 1700–1799 341 1,866,106 30,077,557 97,219 1,531 1800–1899 559 3,565,758 60,750,795 289,697 18,644 Belletristik 401 1,774,209 29,303,975 106,988 3,366 Gebrauchsliteratur 103 624,829 11,041,708 35,079 787 Wissenschaft 532 4,103,679 68,268,553 298,195 17,170 Tabelle 1: Informationen zu den Teilkorpora des DTA-Korpus. Danton war unter uns im September.""). Das DTA-Korpus ist aber, wie gesagt, relativ klein (nur 401 belletristische Werke, wobei Belletristik hier neben Fiktionalem auch Reiseberichte und Lebenserinnerungen einschließt) und nicht sehr belastbar f¬® ur Korpusanalysen. 4 Bedeutung f¬® ur die Literaturwissenschaft Es wäre analog zu Hans Ulrich Gumbrechts Untersuchung 1926 denkbar und w¬® unschenswert, dass man die Literaturgeschichte einzelner Tage schriebe. Dass jedes Datum seine eigene Literaturgeschichte hat, dies also in Ansätzen schon untersucht wird (allerdings noch ohne Korpusanalysen ö. A.), zeigt das Beispiel Paul Celan und der ""20. Jänner"". In Celans Prosagedicht Gespräch im Gebirg, 1960 in der Neuen Rundschau (Heft 2) erschienen, wird auf Georg B¬® uchners Erzählung Lenz angespielt, in der ebenfalls ein Gang durchs Gebirge geschildert wird. B¬® uchners Text beginnt mit dem Satz: ""Den 20. [Jänner] ging Lenz durchs Gebirg."" Dass Lenz"" Wanderung durchs Gebirge am 20. Jänner stattfindet, darauf weist auch Celan in seiner B¬® uchner-Preis-Rede Der Meridian hin. Er erweitert den Anspielungsraum noch, indem er auf einen weiteren 20. Januar verweist, nämlich den des Jahres 1942, als in Berlin die Wannseekonferenz stattfand. Und er folgert: ""Vielleicht darf man sagen, daß jedem Gedicht sein ""20. Jänner"" eingeschrieben bleibt?"" (vgl. Sieber 2007, S. 114_). Die computergest¬® utzte Erhebung von Zeitangaben aus großen Korpora macht solche Gleichzeitigkeiten sichtbarer und ermöglicht so auch deren systematische Erforschung. 5 Android-App zur Exploration expliziter Zeitausdr¬® ucke in der Weltliteratur Um eine Idee f¬® ur die Jahreszeitlichkeiten der Literatur zu entwickeln, haben wir parallel zum Projekt eine Android-App entwickelt, die als Kalender funktioniert und f¬® ur jeden Tag des Jahres Passagen aus der Weltliteratur anzeigt, die an diesem Tag spielen. Beispiele sind in Abbildung 1 dargestellt. Die von uns entwickelte und mit HeidelTime belieferte App soll die einfache Exploration expliziter Zeitausdr¬® ucke in der Weltliteratur ermöglichen. Dass James Joyce"" Ulysses etwa am 16. Juni 1904 stattfindet (""Bloomsday""), ist allgemein bekannt, allerdings im Text nicht sofort ersichtlich. In der App wird die einzige Stelle des ¬® uberlangen Romans, an dem das Datum erwähnt wird, zitiert (die Sekretärin Miss Dunne tippt es auf ihrer Schreibmaschine ein). Weitere Beispiele f¬® ur Passagen sind der 12. Juni in der Blechtrommel (Oskar Matzeraths erklärter Sohn Kurt wird geboren), der 29. Februar in Thomas Manns Zauberberg (der als spezielle Variante der Walpurgisnacht eine zentrale Rolle spielt, siehe Abbildung 1) oder der 27. Juli in Stefan Zweigs Schachnovelle (der Protagonist Dr. B. eignet sich an diesem Tag das Schachbuch an). Die App stellt somit ein erweiterbares Korpus mit Datumsnennungen in der Weltliteratur dar, das der weiteren Forschung zur Verf¬® ugung steht. Um allerdings das gesamte ""literarische Jahr"" abzubilden, m¬® ussen auch die ungefähren zeitlichen Verortungen in den Blick genommen werden, was im nächsten Abschnitt versucht werden soll. 3 Fontane Storm JAN 30 5 FEB 13 3 MAR 18 7 APR 13 7 MAY 28 11 JUN 17 9 JUL 16 6 AUG 17 10 SEP 36 10 OCT 41 11 NOV 27 13 DEC 25 1 Abbildung 1: Screenshots unserer Android-App Literjahrtur. Tabelle 2: Fontane vs. Storm. 6 Die Jahreszeiten der Literatur Von dem f¬® ur literarische Texte sehr spezifischen Verhältnis zwischen exakten und ungefähren Datumsangaben war schon die Rede. Die Analyse des DTA-Korpus nur mit (auch nicht-literarischen) Texten aus dem 19. Jahrhundert hat eine besondere Konjunktur f¬® ur die Monate März bis Juli erbracht: JAN: +575544555454453554443444444667 FEB: 53553443533424444343445645352 MAR: +6667676789669+49+8+6877888699+ APR: +78878876+76597+75699+89668869 MAY: ++6+968758+899+8886+9+98987+9++ JUN: +8489768+++++978697+976++9988+ JUL: +789987+758978+7765887565767755 AUG: 6556544458455555554454455445345 SEP: 644333434433345345334322333334 OCT: 7222222222233332231242323233324 NOV: 845553454434436445534344443434 DEC: 2423222216222222112222242141123 Bei der Suche nach Abweichungen im Werk einzelner Autoren des 19. Jahrhunderts stießen wir etwa auf Theodor Fontane und Theodor Storm. Eine Erhebung nur der Monatsnennungen in ausgew ählten fiktionalen Texten beider Autoren ergab das in Tabelle 2 verzeichnete Bild. Analog zur Popularität des 1. Mai ist auch der Gesamtmonat bei beiden stark repräsentiert. Doch f¬® ur die Sommermonate gilt das nicht. Fontanes Romane und Erzählungen scheinen vor allem von September–Januar stattzufinden, Storms Texte von August–November. Auch unter der Maßgabe, dass der Monatsname als sprachlich-klangliches Zeichen einen stilistischen E_ekt hat, scheinen beide Autoren herbstlich-winterliche Settings und Stimmungen zu bevorzugen. Mit den vorgestellten Methoden zur Ermittlung von Datumskonjunkturen, zur Beschreibung des Verhältnisses zwischen ungefähren und exakten Datumsangaben, zum Aufbau eines Korpus mit exakten Datumsnennungen und zur Jahreszeitlichkeit der Literatur und bestimmter Autoren kann die im Titel gestellte Frage ""Wann findet die deutsche Literatur statt?"" tatsächlich makroanalytisch beantwortet werden. Damit die getro_enen Aussagen tatsächlich literaturhistorisch belastbar sind, soll f¬® ur die weitere Forschung ein größeres und besser (v. a. mit Entstehungs-/Verö_entlichungsdaten) ausgezeichnetes Korpus zusammengestellt werden. 4 Literatur Matthew Jockers. Macroanalysis. Digital Methods and Literary History. Chicago: University of Illinois Press, 2013. James Pustejovsky, Jos¬¥e M. CastaÀúno, Robert Ingria, Roser Sauri, Robert J. Gaizauskas, Andrea Setzer, Graham Katz and Dragomir R. Radev. TimeML: Robust Specification of Event and Temporal Expressions in Text. In New Directions in Question Answering, S. 28–34, 2003. Mirjam Sieber. Paul Celans ""Gespräch im Gebirg"". Erinnerung an eine versäumte Begegnung. T¬® ubingen: Niemeyer, 2007. Jannik Strötgen, Michael Gertz. Temporal Tagging on Di_erent Domains: Challenges, Strategies, and Gold Standards. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012), S. 3746–3753, 2012."
2015,DHd2015,2015_cls_metadata_extracted.csv,An End-To-End Integration of Automatic Annotations into CATMA,"Thomas Bögel (Institute of Computer Science, Heidelberg University); Marco Petris (Institute for German Studies, University of Hamburg); Michael Gertz (Institute of Computer Science, Heidelberg University); Jannik Strötgen (Institute of Computer Science, Heidelberg University)",Annotation,Annotation,"1 Introduction Natural Language Processing offers solutions for predicting linguistic annotations at different levels of complexity. Thus, it seems obvious and 'in general 'a good idea to apply these methods to the Humanities in order to automate laborious manual annotations and to facilitate a deeper text analysis understanding. Apart from the purely technical aspect of developing suitable models, however, additional challenges for NLP in the Humanities arise: in order to be used as part of an analysis tool, humanists often desire justifications and explanations of automatic annotations. Just implementing a black-box approach, evaluating it intrinsically and returning the presumably best results to the user is not sufficient. In this paper, we suggest a transparent way of presenting the results of a NLP pipeline in a collaborative setting. This gives the user the possibility to judge the results directly within an already existing annotation interface and potentially use them for individual analysis tasks. We will first present individual components that are combined with each other, namely the collaborative annotation tool CATMA and UIMA as a processing pipeline for Natural Language Processing. We will then show our end-to-end integration of UIMA into CATMA and its advantages. 2 CATMA integration CATMA1 is a flexible, collaborative annotation tool for literary scholars. So far, it integrates three functional and interactive modules, namely the tagger, the analyzer, and the visualizer. While the tagger module is a graphical interface to allow the easy creation of manual annotations in texts using flexible tag sets (including feature structures, overlapping annotations, etc.), the analyzer component offers a wide range of possibilities to query a document collection or single documents, e.g., for frequently occurring patterns. Finally, the visualizer module can be used to explore a document collection, e.g., by generating distribution charts of the analysis results. In this paper, we present an extension to CATMA, which was developed in the con¬¥ text of the heureCL EA project2 - the integration of a UIMA-based text processing pipeline for the automatic creation of tag annotations created by natural language processing tools. UIMA (Unstructured Information Management Architecture)3 is a wide-spread framwork for developing and using natural language processing pipelines. One of its key characteristics is that it allows the easy combination of tools that have initially not been built to be used together. All UIMA components rely on the same data structure - the Common Analysis Structure (CAS) - there are three types of components: collection readers, analysis engines, and CAS consumers. The collection readers task is to access the source of the documents that are to be processed and to initialize a CAS object for each document. Then, the analysis engines perform linguistic processing of the data and stand-off add annotations to the CAS object. The subsequently called analysis engines can access the annotation results of the earlier components, i.e., they can perform more complex tasks. Finally, a CAS consumer performs the final processing of the CAS object. 1Website: http://www.catma.de/clea 2http://heureclea.de/ 3Website: http://uima.apache.org/ Figure 1: End-To-End architecture of combining the collaborative annotation platform CATMA with the automatic text processing pipeline UIMA. In our case, the pipeline architecture is set up as depicted in Figure 1. The Collection Reader accesses the documents directly from CATMA and returns annotation information back to CATMA. However, the actual key feature of our development is that the user can directly access the automatic processing feature within the CATMA interface. That is, the user can select the types of annotations that shall be added to her document or document collection automatically. This significantly decreases the boundary for users not familiar with applying NLP tools for automatic processing of textual data, i.e., for typical CATMA users who are often literary scholars or students of the Humanities. Nevertheless, our implementation is not a black box solution that only adds annotations that the user has to accept. In contrast, we are currently working on integrating a user feedback interface that will allow the initialization of user parameters based on the users feedback in the form of accepted or rejected annotations. 3 Research Workflow within CATMA The advantage of a direct integration of UIMA into CATMA is best illustrated with an example: in order to analyse the temporal structure of documents (such as order phenomena), many linguistic aspects need to be taken into account. Temporal signals, e.g., calendrical, deictic or relational temporal expressions (Lahn and Meister, 2008), offer a hint for temporal phenomena of order. As manual annotation for these basic linguistic phenomena is laborious, we are currently developing a machine learning system for predicting temporal signals. Figure 2 shows the possibility to create and directly inspect automatic annotations directly within the CATMA interface. With one click, the prediction of our NLP pipeline for temporal signals 'or other annotations such as date and time expressions (Strötgen and Gertz, 2013) 'can be shown. Note that the system output can easily be compared to any manual annotation as the type systems are completely independent. This flexibility allows scholars to focus on complex phenomena of the text with the possibility of automating simpler annotations. All automatic annotations are, however, non-obtrusive and completely changeable and reversible to give the choice of the level of automation to the user. References Lahn, S. and J. C. Meister (2008). Einf¬® uhrung in die Erzähltextanalyse. Stuttgart: JB Metzler. Strötgen, J. and M. Gertz (2013). Multilingual and cross-domain temporal tagging. Language Resources and Evaluation 47(2), 269–298. Figure 2: Screenshot showing automatic annotations within CATMA."
2015,DHd2015,2015_cls_metadata_extracted.csv,Ausgezeichnete Mären analysieren ‚- ein Werkstattbericht,"Friedrich Michael Dimpel (FAU Erlangen, Deutschland)","Korpora, Annotation, XML, TEI, ODD","Korpora, Annotation, XML, TEI, ODD","1. Digitale Korpora können dank der Fortschritte im Bereich der Digital Humanities mit zahlreichen quantitativen Analyseverfahren untersucht werden: Im Rahmen der Stilometrie beschäftigen sich Computerphilologen etwa mit Fragen der Autorschaftsattribution oder der Werkchronologie sowie mit Fragen nach Spezifika von Gattungen oder Epochen.1 Auch wenn digitale Texte mit zahlreichen quantitativen Analyseverfahren untersucht werden können: In der Regel sind vollständige Texte oder zusammenhängende Textabschnitte wie Buchkapitel der Gegenstand etwa von stilometrischen Studien. Die Möglichkeiten für automatische Analysen enden jedoch dort, wo solche spezifische Textebenen in den Blick zu nehmen wären, die repräsentieren, welche Eigenschaft auf welche Figur bezogen wird, und welche Figuren oder Erzähler diese Zuschreibung vornehmen: Wer spricht? Zudem: Über wen wird gesprochen 'welche Terme werden auf welche Figuren bezogen? Gibt es dabei bspw. genderspezifische Distributionen? Auf welche Weise wird gesprochen? Es ist ein erheblicher Unterschied, ob eine Figur über ihre Liebe spricht und damit ihre Emotion in der erzählten Welt öffentlich macht, oder ob nur eine Gedankenrede einer Figur erzählt wird. Wenn man auf solche Informationen zugreifen möchte, dann ist es nötig, die Texte mit einer entsprechenden Annotation zu versehen. Quantitative Analysen von Textsamples, die spezifische Textdaten enthalten wie Figurenrede bestimmter Aktanten, fokalisierte Textpassagen oder Textpassagen, die sich auf bestimmte Aktanten beziehen, sind derzeit nicht möglich ohne eine aufwendige manuelle Textaufbereitung für eine konkrete Fragestellung. Wie wichtig jedoch eine narratologische Textauszeichnung in einem diachron angelegten Korpus wäre, unterstreichen Fotis Jannidis, Gerhard Lauer und Andrea Rapp: ""Wer hat, wann und wo zum ersten Mal die Form der erlebten Rede eingesetzt, wer die episodische Reihung zu psychologischer Figurenzeichnung verdichtet? Wo verknüpfen sich populäre Novellenstoffe mit hochkulturellen Erzähltechniken [‚Ä¶]? Denn nur mit Hilfe des Computers lässt sich ein hinreichend großes Korpus über einen langen Zeitraum hinweg untersuchen und damit etablierte literaturhistorische Methoden um serielle, computerbasierte Verfahren so ergänzen, dass eine Geschichte des Erzählens überhaupt erst geschrieben werden kann.""2 1 Vgl. exemplarisch JOHN F. BURROWS: ""Delta"". A Measure of Stylistic Difference and a Guide to Likely Authorship. In: Literary and Linguistic Computing 17, 2002, S. 267–287, FRIEDRICH MICHAEL DIMPEL: Computergestützte textstatistische Untersuchungen an mittelhochdeutschen Texten. Tübingen 2004, FOTIS JANNIDIS: Methoden der computergestützten Textanalyse. In: Vera Nünning / Ansgar Nünning (Hrsg.), Methoden der literatur- und kulturwissenschaftlichen Textanalyse. Ansätze 'Grundlagen 'Modellanalysen. Stuttgart 2010, S. 109–132, CHRISTOF SCHÖCH: Corneille, Moli√®re et les autres. Stilometrische Analysen zu Autorschaft und Gattungszugehörigkeit im französischen Theater der Klassik. In: Christof Schöch / Lars Schneider (Hrsg.), Literaturwissenschaft im digitalen Medienwandel. 2014, S. 130–157. 2 FOTIS JANNIDIS / GERHARD LAUER / ANDREA RAPP: Hohe Romane und blaue Bibliotheken. Zum Forschungsprogramm einer computergestützten Buch- und Narratologiegeschichte des 2. Bei narratologisch annotierten Korpora zur deutschen Literatur handelt es sich um ein Desiderat.3 Daher soll ein Korpus von 100 Kurzerzählungen narratologisch annotiert werden. Das Korpus soll historisch relativ ausgewogen angelegt werden: 30 mittelhochdeutsche und frühneuhochdeutsche Mären, 10 Boccaccio-Novellen, 10 Chaucer-Tales, 30 neuhochdeutsche Novellen und 20 neuhochdeutsche Kurzgeschichten werden aufgenommen. Als Ebenen der Auszeichnung sind vorgesehen: 1. Welche Figuren befinden sich an dem Ort, von dem erzählt wird? 2. Welche Figurenbewegungen im Raum finden statt? 3. Welche temporalen Abweichungen ereignen sich (Prolepsen etc.)? 4. Welche Figur ist wie fokalisiert? 5. Um welche Art von Redewiedergabe (direkte Rede, Bewusstseinsdarstellung etc.) handelt es sich 'und welche Figur denkt oder spricht? 6. Um welche Art von Erzählerrede (Descriptio, Bericht Figurenaktivität, Erzählerreflexion etc.) handelt es sich? 7. 8. Auf welche Figur bezieht sich eine Figuren- oder Erzählerrede? Auf welche Figur bezieht sich eine wertende Öußerung einer anderen Figur oder des Erzählers? 9. Steht eine Öußerung in Negation? 10. Liegt eine uneigentliche Rede vor (metaphorisch, ironisch etc.)? 11. Inwieweit besteht Unsicherheit hinsichtlich der Eindeutigkeit der Textdaten? Die nötigen XML-Elemente sind noch nicht im TEI-Standard enthalten, daher muss ein geeignetes Tagset entwickelt werden.4 TEI-Kompatibilität wird jedoch angestrebt: Die Elemente können über das Roma-Tool in eine ODD-Datei integriert werden.5 Romans in Deutschland (1500-1900). In: Lucas Marco Gisi / Jan Loop / Michael Stolz (Hrsg.), Literatur und Literaturwissenschaft auf dem Weg zu den neuen Medien. germanistik.ch 2006. (= online unter http://www.germanistik.ch/publikation.php?id=Hohe_Romane_und_blaue_ Bibliotheken). 3 Allerdings kann in vielfältiger Weise auf die wichtige Grundlagenstudie von ANNELEN BRUNNER: Automatische Erkennung von Redewiedergabe in literarischen Texten. Diss. masch. Würzburg 2012, aufgebaut werden. Brunner hat in ihrer Dissertation ein Annotationsverfahren für Redewiedergabe entwickelt und ein Korpus, das aus Texten von 1787 bis 1913 besteht, manuell annotiert. Auch wenn die automatische Erkennung der Redewiedergabe (regelbasiert und via Maschinelles Lernen) noch nicht eine Fehlerrate erreichen kann, die narratologische Auswertungen erlaubt, kann das vorliegende Projekt in konzeptioneller Hinsicht von Brunners Studie erheblich profitieren. 4 Zu narratologischen Desideraten von TEI-P5 vgl. FOTIS JANNIDIS: TEI in a Crystal Ball. In: Literary and Linguistic Computing 24, 2009, S. 253–265, hier S. 261f. 5 Vgl. http://www.tei-c.org/Guidelines/Customization/odds.xml. Als XML-Elemente werden vorgestellt: 1. <FigurFokusort> (@Bezeichnung, @Figurengruppe) 2. <BewegungLokal> (@Typ) 3. <Chronologie> (@Typ) 4. <Fokalisiert> (@Bezeichnung, @Typ) 5. <Redewiedergabe> (@Typ, @Bezeichnung, @non-fact, @level) 6. <Erzählerrede> (@Typ, @Bezeichnung) 7. <Figurenbezug> (@Unmittelbar, @Mittelbar) 8. <Wertung> (@BezeichnungWertende, @BezeichnungGewertete, @Typ) 9. <Negation> (@Typ) 10. <UneigentlicheRede> 11. <certainty> sowie @cert als Attribut zu allen Elementen (> TEI P5) 3. Zentral für den Workflow ist die Entwicklung von Annotationsrichtlinien. Es wird angestrebt, dass verschiedene Versuchspersonen beim gleichen Text zu homogenen Ergebnissen kommen. In ähnlicher Weise, wie man sich bei der Herstellung einer Edition über Kollationierungsregeln verständigen muss, sind hier Annotationsregeln zu erarbeiten. Solche Regeln sind nötig, weil selbst scheinbar eindeutig definierte narratologische Phänomene sich oft nicht eindeutig in Texten wiederfinden lassen. Zudem ist Ambiguität ein charakteristisches Merkmal von literarischen Texten. Mit Blick auf das Homogenitätsziel werden Annotationsregel und Bearbeitungsdatum direkt im XML-Code dokumentiert, damit bei einer Weiterentwicklung der Annotationsrichtlinien einerseits rasch auf eine einschlägige Fallsammlung zugegriffen werden kann; anderseits können bei einer Regelrevision Entscheidungen gezielt aufgesucht und revidiert werden. Dabei hilft ein eigener Projekteditor, der in Perl/TK implementiert wurde, der neben dem Annotationsfenster in einem zweiten Fenster in Kurzform Informationen zu bereits ausgezeichneten Elementen einblendet. 4. Annotiert wurden bislang sechs Texte. Vorgestellt werden einige Probleme, die sich bei der Auszeichnung von ""Sperber"" und das ""Häslein""6 etwa durch Segmentierung oder durch Ambiguitäten ergeben haben. Anhand von Auswertungsdaten soll exemplarisch aufgezeigt werden, in welch vielfältiger Weise ein entsprechend annotiertes Korpus Analysen möglich macht; etwa in Bezug auf a) multiple Methoden: i. Das Korpus kann wie andere Korpora auch mit einer Vielzahl an statistischen Methoden analysiert werden 'etwa in Hinblick auf Heterogenität oder Homogenität. ii. Eine besondere Stellung nimmt das Korpus jedoch dadurch ein, dass auf Basis der Textauszeichnung eine Sample-Erstellung für spezifische Fragestellung möglich ist, die 6 Ausgabe: KLAUS GRUBMÜLLER: Novellistik des Mittelalters. Märendichtung. Frankfurt/Main 1996 (=Bibliothek des Mittelalters 23) nicht nur chronologisch-lineare Zugriffe auf Korpussegmente erlaubt, sondern systematische Zugriffe auf gleichartig annotierte Korpussegmente. So lässt sich ein Korpussegment bspw. mit Bewusstseinsdarstellung von weiblichen Figuren mit einem Korpussegment vergleichen, das aus Erzählerrede besteht; die Figurenrede von Antagonisten lässt sich mit Figurenrede von Protagonisten vergleichen, u.v.m. b) multiple Fragestellungen, beispielsweise: i. Wie steht es um die diachrone Entwicklung von Fokalisierung, um die Eigenschaften von Erzähler- und Figurenrede, um temporale Alternationen, wie verteilt sich der Redebezug auf verschiedene Figurentypen wie Protagonist oder Antagonist, wie steht es um quantitative Parameter bei uneigentlicher Rede? ii. Korrelation kulturwissenschaftlich relevanter Terme und aktantieller Rolle. Hier werden bspw. zahlreiche gender-bezogene Auswertungen möglich, indem eine SampleAnalyse mit Figuren- oder Erzählerrede möglich wird, die jeweils auf weibliche oder männliche Figuren bezogen ist oder durch eine Sample-Analyse mit Figurenrede, die jeweils von weiblichen oder männlichen Figuren stammt. iii. Studien zur Wertungstheorie: Wie sind evaluative Öußerungen auf Erzählerrede und Figurenrede verteilt? Welche Aktanten bewerten bevorzugt, welche werden bevorzugt bewertet? iv. Lassen sich für diese oder für andere Fragestellungen epochenspezifische Verteilungen ausmachen? Es werden Studien ermöglicht, die einen Beitrag zur Gattungsgeschichte leisten. So wären beispielsweise Theoriebildungen zu überprüfen, inwieweit und inwiefern das Verhältnis vom Märe zur Novelle in Anschluss an die Unterscheidungskriterien von Hans-Jörg Neuschäfer unter dem Gesichtspunkt eines ""Noch-Nicht"" beschrieben werden kann.7 7 Vgl. HANS-JÖRG NEUSCHÖFER: Boccaccio und der Beginn der Novelle. Strukturen der Kurzerzählung auf der Schwelle zwischen Mittelalter und Neuzeit. München 1969 (=Theorie und Geschichte der Literatur und der schönen Künste 8). Kritisch dazu FRIEDRICH MICHAEL DIMPEL: Sprech- und Beißwerkzeuge, Kunsthandwerk und Kunst in Kaufringers ""Rache des Ehemanns"". In: Daphnis 42, 2013, S. 1–27 (im Erscheinen)."
2015,DHd2015,2015_cls_metadata_extracted.csv,Modellierung eines maschinell lesbaren Lexikons für das Korpus der altäthiopischen Literatur,Alessandro Bausi (Universität Hamburg); Andreas Ellwardt (Universität Hamburg); Cristina Vertan (Universität Hamburg),"Lexikonmodell, Transkription, Korpora, XML, TEI","Lexikonmodell, Transkription, Korpora, XML, TEI","1. Einführung Die Entwicklung und ständige Erweiterung des Unicode-Kodierungssytems Unicode1 sowie der Mark-upSprachen XML2, TEI3 haben in den letzten Jahren u.a. die digitale textuelle Repräsentation von historischen Dokumenten, die mit unterschiedlichen Alphabeten geschrieben wurden, ermöglicht. Diese textuelle Repräsentation eröffnet wiederum, im Kontrast zur reinen Speicherung von BildDigitalisaten, die Möglichkeit, computergestützte linguistische sowie philologische Untersuchungen auf großen Textmengen durchzuführen. Durch solche Methoden lässt sich beispielsweise eine diachrone Analyse der Sprache gleichzeitig auf mehreren Ebenen (morphologisch, syntaktisch, semantisch) realisieren, vorausgesetzt, die elektronischen Ressourcen wie Lexika oder annotierte Korpora sowie die sprachtechnologischen Prozesse (morphologische Analysierer, Wortart-Tagger, Parser) sind vorhanden. Während die sprachtechnologischen Ressourcen und Werkzeuge für moderne Sprachen sehr weit entwickelt sind, gelten viele historische Sprachen als stark ""under-ressourced"". Laut Krauwer (2003) gibt es ein minimales Set von Ressourcen, die für eine computergestützte Sprachanalyse unabdingbar sind. Dessen Weiterentwicklung stellt die Wissenschaft vor neue Forschungsprobleme, da sich häufig Modelle, die für moderne Sprachen entwickelt wurden, nicht 1:1 auf historische Sprachen übertragen lassen (VertanEtAl.2014) In diesem Beitrag werden wir die Modellierung und Entwicklung von sprachtechnologischen Ressourcen für das Altäthiopische (Ge_ez) erläutern. Die Besonderheiten des Ge_ez (s. Sektion 2), bedingen die Entwicklung von neuen Modellen, z.B. im Bereich der Lexika. In Sektion 3 werden wir exemplarisch die Entwicklung eines Lexikon-Modells für Ge_ez darstellen, während wir in Sektion 4 die Einbindung des Lexikons in einer Architektur für die diachrone Analyse des Ge_ez diskutieren werden. 2. Kurze Darstellung des Altäthiopischen (Ge_ez) Das südsemitische Ge_ez ist die Sprache des Königreichs Aksum in der heutigen nordäthiopischen Provinz Tigray, von wo aus die im 4. Jahrhundert beginnende Christianisierung Öthiopiens ihren Anfang nahm. Die in der Folge entstehende reiche Literatur ist in großem Umfang geprägt von Übersetzungen aus dem Griechischen und später, ab dem 13. Jahrhundert, aus dem Arabischen, was durch grammatische Interferenzphänomene reflektiert wird. Während seine Verdrängung als gesprochene Sprache bereits im 9./10. Jahrhundert beginnt, bleibt es als Schriftsprache sehr viel länger erhalten und ist bis in die Gegenwart hinein Liturgiesprache des äthiopischen und eriträischen Klerus. Das Altäthiopische hat aus einer südsemitischen Schrift ein eigenes Silbenalphabet entwickelt, das bis heute in mehreren modernen Sprachen Öthiopiens und Eritreas Verwendung findet. Innerhalb der semitischen Sprachen fällt es durch die verwendete Rechtsläufigkeit auf, außerdem werden die Vokale vollständig geschrieben. Beides unterscheidet das Ge_ez von den ihm nächst verwandten Sprachen Altsüdarabisch, Arabisch, Hebräisch und Syro-Aramäisch. Des weiteren sind Grapheme, die ursprünglich distinkten Phonemen zugeordnet waren, schon früh in identischer phonetischer Realisierung zusammengefallen, was sich konkret bereits in den ältesten überlieferten Handschriftzeugnissen (aber noch nicht in den aksumitischen Inschriften) niederschlägt, wo eine beliebige Austauschbarkeit der Laryngale und Sibilanten jeweils untereinander zu konstatieren ist. Mit den genannten eng verwandten semitischen Sprachen teilt das Altäthiopische die nichtkonkatenative Morphologie. Hierbei muss das einzelne Lexem als Kombination von zwei Elementen beschrieben werden, nämlich der Wurzel und dem Schema: Die konsonantische Wurzel gibt veränderliche Positionen zwischen 1 http://www.unicode.org/ 2 http://www.w3.org/XML/ 3 http://www.tei-c.org/index.xml ihren, zumeist drei, Wurzelkonsonanten vor, die durch die Vokale des Schemas aufgefüllt werden, häufig, jedoch nicht zwingend, ergänzt um (vokalische oder konsonantische) Affixe. 3. Arbeitsschritte zu einer computergestützten Analyse des Altäthiopischen Wie bereits in Sektion 2 erwähnt, sind Ge_ez-Dokumente für die gesamte Geschichte des christlichen Orients extrem wertvoll. Manche Überlieferungen von alten griechischen Texten sind in der Originalsprache verloren und nur im Altäthiopischen erhalten. In der Zeit digitaler Bibliotheken erscheint also die Entwicklung von computergestützten Tools für die Ge_ez-Sprache umso dringender. Das primäre Ziel des Projekts TraCES4 ist die Entwicklung eines digitalen Korpus der Ge_ez-Sprache, zusammen mit Annotationen auf morphologischer, syntaktischer und semantischer Ebene. Dieses annotierte Korpus soll einerseits eine diachrone Analyse des Altäthiopischen ermöglichen, anderseits soll es selbst als Ressource für weitere computergestützte Prozesse dienen. Langfristig soll eine vergleichende digitale Analyse von altäthiopischen und griechischen (z.B. die in der digitalen PERSEUS Sammlung5 verfügbaren) oder arabischen sowie anderen christlich-orientalischen Dokumenten möglich sein. Mit Ausnahme von einigen wenigen Texten gibt es zur Zeit keine verfügbare elektronische Ressource für das Altäthiopische. Daher haben wir uns als erstes der Entwicklung eines maschinell lesbaren Lexikons des Ge_ez gewidmet. Dessen Modellierung wird in der nächsten Sektion erklärt. 4. Ein Lexikon-Modell für Ge_ez Die in Sektion 2 erwähnte Austauschbarkeit der Laryngalen und Sibilanten untereinander stellt uns vor eine erste Modellierungsanforderung. Für einen Lexikon-Eintrag muss nicht nur die Grundform, sondern es müssen auch alle möglichen graphischen Varianten gespeichert werden, wobei wohlgemerkt diese graphische Variationen auch in einigen Fällen als selbständige Lexikon-Einträge mit ganz anderer Bedeutung existieren können. Das Lexikonmodell muss daher eine starke Modularisierung und Verlinkung zwischen den einzelnen Modulen unterstützen. Wir haben uns für das Lemon-Modell (McCraeEtAl.2012) entschieden. Unserer Kenntnis nach, ist dies der erste Versuch, eine semitische Sprache mit dem Lemon-Modell zu beschreiben. Die Grundkomponenten eines Lemon-Lexikon-Modells für Ge_ez wurden wie folgt angepasst. Die Zitierform eines Wortes in klassischen Lexika semitischer Sprachen ist in der Regel eine verbale Repräsentation der Wurzel in der 3. Person Perfekt Singular maskulin. Diese Form wird in unserem LemonModell als ""Lexical Entry"" gespeichert. Ein ""Lexical Entry"" ist mit den folgenden weiteren Modulen verknüpft: -Das Lexical Form-Modul beinhaltet alle möglichen graphischen Varianten des Lemmas. Jede graphische Variante wird zusammen mit ihrer Transkription gespeichert. -Das Morphologie-Modul beinhaltet eine Subkomponente für den lexikalischen Eintrag, die das Paradigma, Ausnahmen der morphologischen Realisierung (z.B. Sonderformen im Imperfekt oder Plural) sowie die jeweiligen anderen morphologischen Kategorien für das Lemma umfasst. Das Semantik-Modul setzt sich aus einer Übersetzungs-, einer Korpusevidenz- und einer semantischeMerkmale-Komponente zusammen. Unter Korpusevidenz verstehen wir Beispiele aus Korpora für dieses Lemma oder eine seiner morphologischen Realisierungen. Die Übersetzungen sind unterteilt in eine Übersetzung ins Englische und semantische Öquivalente in anderen Sprachen wie (falls vorhanden) Arabisch, Hebräisch, Syrisch, Koptisch, Griechisch oder sogar Sanskrit. -Das Syntax-Modul beinhaltet syntaktische Funktion des Lemmas, zusammen mit Beispielen von syntaktischen Bäumen. Dieses Modul wird in einer späteren Projektphase entwickelt. 4 European Union Seventh Framework Programme IDEAS (FP7/2007-2013), European Research Council, grant agreement no. 338756, project ""TraCES 'From Translation to Creation: Changes in Ethiopic Style and Lexicon from Late Antiquity to the Middle Ages"", http://www1.uni-hamburg.de/ethiostudies/traces.html 5 http://www.perseus.tufts.edu/hopper/ 4.1. Wurzel-Modellierung Da die Wurzel eine zentrale Stellung in der semitischen Morphologie hat, haben wir als ersten Schritt ein Wurzel-Sublexikon erstellt. Dieses entspricht dem Wurzel-Submodul im morphologischen Modul. Die Erstellung des Wurzel-Lexikons wurde vollständig automatisiert. Aus einer digitalen Version des trotz seiner Abfassung im Jahre 1865 unverändert als Standardwerk geltenden ""Lexicon linguae aethiopicae"" von August Dillmann (Dillmann1865) (im Unicode-Format) wurden zirka 4000 Wurzel-Einträge mit Hilfe von String-basierten Regeln extrahiert. Für jede Wurzel wurden: - die vollständige Transkription - die auf das konsonantische Gerüst zurückgeführte Transkription - das konsonantischen Wortbildungsschema - alle graphischen Varianten zusammen mit deren Transkriptionen durch regel-basierte Verfahren extrahiert. Die Automatisierung ermöglicht zum ersten Mal die Sammlung aller graphischen Varianten für alle 4000 Wurzeln (wobei hervorgehoben werden muss, dass manche Wurzeln bis zu 50 graphische Varianten haben). Jede Wurzel wird automatisch mit ihren Homophonen (Einträge mit identischer graphischer Form, aber unterschiedlicher Bedeutung) verknüpft. Erfasst werden durch automatische Prozesse auch alle Lexikoneinträge von graphischen Varianten (falls vorhanden). Das Wurzel-Lexikon wird im XML-Format gespeichert. Dafür wurde ein eigenes XML-Schema entworfen. Eine Java-basierte graphische Oberfläche wurde implementiert. Diese Oberfläche ermöglicht nicht nur die Visualisierung von den Einzeleinträgen und die Navigation durch das Wurzel-Lexikon, sondern auch manuelle Korrekturen, das Löschen oder das Einfügen von neuen Einträgen. Nach Korrekturen wird das Wurzel-Lexikon: - als eine ""Authority List"" für das Ge_ez-Lexikon und - als Generierungsquelle für Lexikoneinträge benutzt. 5. Zusammenfassung und weitere Arbeit In diesem Beitrag haben wir die Modelle für ein Wurzel- und ein Lemma-Lexikon für die Ge_ez-Sprache erklärt. Die Wurzel und Lemma-Akquisition werden weitgehend durch computergestützte Prozesse realisiert. Die erstellte Software wird bei der Präsentation des Beitrags vorgeführt. Das Projekt TraCES wurde im März 2014 begonnen und hat eine Laufzeit von fünf Jahren. Die Erstellung des Lexikons der Ge_ez Sprache ist zurzeit die zentrale Arbeit im Projekt, wobei derzeit die Erstellung von Generierungsparadigmen im Vordergrund steht. Mit deren Hilfe werden durch Computerverfahren Lexikoneinträge generiert. Ein erster Test hat mehr als 13 000 Einträge generiert. Dies zeigt, dass die Automatisierung eine erhebliche Zeitersparnis für die Lexikon-Akquisition ermöglicht. Literatur (Dillmann1865) Dillmann, August, Lexicon lingu√¶ ßthiopic√¶ cum indice Latino, Lipsiae 1865. (Krauwer2003) Krauwer, Steven, ""The Basic Language Resource Kit (BLARK) as the First Milestone for the Language Resources"", http://www.elsnet.org/dox/krauwer-specom2003.pdf (09.11.2014) (McCraeEtAl2012). McCrae, John und Aguado-de-Cea, Guadalupe und Buitelaar, Paul und, Cimiano, Philipp und Declerck, Thierry und G√≥mez P√©rez, Asunci√≥n und Gracia, Jorge und Hollink, Laura und Montiel-Ponsoda, Elena und Spohr, Dennis und Wunner, Tobias, The Lemon Cookbook, http://lemon-model.net/lemon-cookbook.pdf (09.11.2014) (VertanET.AL.2014) Vertan, Cristina und Zervanou, Kalliopi und van den Bosch, Antal und Sporeleder, Caroline (Hrsg.), Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH), Association for Computational Linguistics, Götheborg, Sweden, 2014, http://www.aclweb.org/anthology/W14-06 (09.11.2014)"
2015,DHd2015,2015_cls_metadata_extracted.csv,"Gleiche Textdaten, unterschiedliche Erkenntnisziele Zum Potential vermeintlich widersprüchlicher Zug‚âà‚Ä†nge zu Textanalyse",Thomas Bögel (Universität  Heidelberg) ;  Michael Gertz (Universität  Heidelberg) ;  Evelyn Gius (Universität  Hamburg) ;  Janina Jacke (Universität  Hamburg) ;  Jan Christoph Meister  (Universität  Hamburg) ;  Marco Petris  (Universität  Hamburg) ;  Jannik Strötgen  (Universität  Heidelberg),"Digitale Heuristik, Machine Learning, NLP-Methoden, Annotation","Digitale Heuristik, Machine Learning, NLP-Methoden, Annotation","1. Einleitung Dieser Beitrag beleuchtet disziplinäre Errungenschaften, die durch die genaue Betrachtung unterschiedlicher disziplinäre Auffassungen von Daten und Erkenntnissen bzw. Erkenntnisinteressen im Projekt heureCLéA ermöglicht wurden und die das große Potential interdisziplinärer Zusammenarbeit im Digital Humanities-_Bereich herausstellen. heureCLéA ist ein Digital Humanities-_Kooperationsprojekt zwischen Literaturwissenschaft und Informatik, in dem eine ""digitale Heuristik"" zur narratologischen Analyse literarischer 1 Texte entwickelt wird. Mit dieser Heuristik sollen (1) bislang nur manuell durchführbare Annotationsaufgaben bis zu einem bestimmten Komplexitätsniveau automatisiert durchgeführt und (2) statistisch auffällige Textphänomene als Kandidaten für eine anschließende Detailanalyse durch den menschlichen Nutzer identifiziert werden können. Dazu wird ein Korpus literarischer Erzählungen kollaborativ manuell annotiert. Anschließend wird mit regelbasierten NLP-_Methoden sowie Machine Learning-_Verfahren an der Entwicklung der Heuristik gearbeitet, die als zusätzliches Modul in die Textanalyseplattform CATMA  implementiert werden wird. 2 1 Das Projekt heureCLéA ist ein vom BMBF gefördertes eHumanities-_Projekt, das von 02/2013-_01/2016 an den Universitäten Hamburg und Heidelberg als Verbundprojekt durchgeführt wird (vgl. dazu auch www.heureclea.de). Zum aktuellen Projektstand vgl. Bögel et al. (im Erscheinen). 2 vgl. www.catma.de 1 Die gemeinsame Frage, wie diese Heuristik erstellt werden soll, und die gemeinsame Betrachtung der literarischen Texte, die als Basis dienen, hat schnell gezeigt, dass es in den beteiligten Disziplinen unterschiedliche Auffassungen über die Qualität von und den Zugang zu Textanalysedaten gibt. So wird etwa der in der Literaturwissenschaft als notwendig geltende Interpretationspluralismus in der NLP als widersprüchlicher Noise betrachtet. Die in der NLP gängige Praxis, Verfahren weniger nach ihrer Nachvollziehbarkeit, sondern vielmehr nach der Qualität ihrer Ergebnisse zu beurteilen, wird wiederum in der Literaturwissenschaft abgelehnt, da dort die Qualität von Verfahren über einen inhaltlichen Austausch über die angewendeten Verfahren ausgehandelt wird. Unser Beitrag will auf die möglichen methodischen und methodologischen Konsequenzen solcher disziplinär unterschiedlicher Zugänge zum Forschungsgegenstand 'in unserem Fall: zu Texten und zu Textanalyse 'in der Zusammenarbeit im Digital Humanities-_Bereich hinweisen. Im Fokus stehen dabei zwei exemplarische Konsequenzen in den beteiligten Disziplinen: (1) der narratologische Workflow, für den eine Erweiterung des traditionellen hermeneutischen Zugangs zur Textanalyse entwickelt wurde, sowie (2) der für das Machine Learning gewählte Zugang der NLP, der sich durch eine besonders hohe Prozesstransparenz von klassischen Machine Learning-_Ansätzen unterscheidet. Beide Beispiele sind aus unserer Sicht exemplarisch für Interferenzen, die von Digital Humanities-_Projekten erzeugt werden können. Diese Interferenzen bedeuten vorerst Störungen des geplanten Forschungsprozesses und erzeugen teilweise erheblichen Mehraufwand. Gelingt die Lösung der damit verbundenen Probleme, generieren sie aber einen Mehrwert sowohl für den Projekterfolg als auch für den von der Projektzusammenarbeit unabhängigen Fortschritt der beteiligten Disziplinen. 2. Die Erweiterung des traditionellen Zugangs zu literaturwissenschaftlicher Textanalyse Die für die Entwicklung der Heuristik in heureCLéA eingesetzten NLP-_Verfahren werden auf ein Korpus 21 deutschsprachiger Erzählungen um etwa 1900 angewendet, das mit dem Textanalysetool CATMA annotiert wird. Das oben erwähnte Noise-_Problem wird dadurch abgemildert, dass die Texte von mehreren Annotatorinnen mit Markup versehen werden. 3 Dieser Zugang verändert den traditionellen Prozess der Textanalyse in der Literaturwissenschaft zweifach. 3 Für eine ausführlichere Beschreibung der durch das Spannungsfeld von Informatik und Hermeneutik bedingten Problematik und ihre Auswirkung auf die Anforderungen an die manuelle Annotation vgl. Gius & Jacke (in Vorbereitung). Dort werden auch die methodologischen Konsequenzen für die narratologische Theorie dargelegt. 2 Erweiterte Analysegrundlage durch Annotation Eine offensichtliche Veränderung zum traditionellen Zugang ist die Erweiterung der betrachteten Datenbasis bzw. der Annahmen über diese. Zu den Vorannahmen des Textinterpreten, den Annahmen über Textteile und den Annahmen über das Textganze, die sich immer wieder gegenseitig beeinflussen und dadurch die Annahmen bestätigen oder modifizieren, kommen die in den Annotationen festgehaltenen Annahmen weiterer Interpretinnen. Der traditionelle hermeneutische Zugang zu Texten wird hier also nicht nur durch das Annotieren selbst 'wie weiter unten ausgeführt 'intensiviert, sondern auch um 4 Annahmen anderer ergänzt. Dadurch wird gewissermaßen die Grundlage für die weitere Analyse erweitert. close(r) reading durch kollaborative, computergestützte Analysen Der computergestützte Zugang an sich forciert bereits durch sein Sichtbarmachen der Analysen in den Annotationen ein intensiveres close reading als Textanalyseverfahren, in denen Interpretationen ohne eine ausführliche Dokumentation zugrundeliegender Analysen generiert werden. Durch die kollaborative Annotation derselben Texte durch mindestens zwei Annotatoren wird außerdem offensichtlich, an welchen Stellen es keine intersubjektive Übereinstimmung zwischen den Annotatorinnen gibt. Dies führte u.a. dazu, dass schnell deutlich wurde, dass die aus Gius (2013) übernommenen Beschreibungen der narratologischen Analysekategorien in der vorliegenden Form nicht als Arbeitsgrundlage für heureCLéA ausreichen. Deshalb wurden zusätzlich Annotationsguidelines erarbeitet, die die Beschreibung der narratologischen Kategorien weiter systematisieren: Neben der Beschreibung und Operationalisierung des Phänomens enthalten die Guidelines Angaben zum typischen Umfang der getaggten Textmenge (etwa Wort/Wortgruppe, Satz, Absatz etc.), zu unmarkierten Fällen, die nicht annotiert werden, zu Indikatoren auf der Textoberfläche, zur Taggingroutine sowie Textbeispiele zur betreffenden Tagkategorien (vgl. 5 Abbildung 1). Die Taggingroutine zielt dabei insbesondere darauf ab, die Analyse so zu organisieren, dass die damit verbunden Aktivitäten in einer von einfachen zu komplexeren Aktivitäten geordneten Reihenfolge ausgeführt werden können. 6 4 vgl. zu den für das hermeneutische Verfahren relevanten Aspekten z.B. Bühler (2003). 5 vgl. Gius & Jacke (2014). 6 Dasselbe Verfahren wird in heureCLéA auch auf die Reihenfolge der annotierten Phänomen angewendet. Dieses an der Komplexität der Analysekatogerien orientierte Vorgehen wurde bereits in Gius (2013) auf Ebene der narratologischen Phänomenbereiche entwickelt und erfolgreich angewendet. 3  ¬†Abbildung 1: Annotation von Ordnung, Zusammenfassung aus den Guidelines (vgl. Gius & Jacke 2014) Das close reading wird außerdem durch Diskussionen intensiviert, die zwischen den Annotatoren stattfinden, wenn sie nach ihrem ersten Annotationsdurchgang die gesetzten Annotationen mit denen der anderen vergleichen. Die der hermeneutischen Textanalyse eigene fortdauernde Bewegung zwischen Text und Analyse/Interpretation des Textes, deren Erkenntnisse wiederum in die erneute Betrachtung des Textes mit einfließen, wird durch die beiden durch die interdisziplinäre Zusammenarbeit notwendigen Erweiterungen des Zugangs sowohl in Bezug auf die Analyse bzw. Interpretation als auch in Bezug auf das zur Verfügung stehende Interpretationsmaterial wesentlich verstärkt (vgl. Abbildung 2). 4  ¬†Abbildung 2: Der erweiterte hermeneutische Zirkel 3. NLP vor dem Hintergrund besonderer Textdomänen und notwendiger Transparenz von automatischen Entscheidungsprozessen Bei der Verarbeitung deutscher literarischer Texte im Kontext einer Zusammenarbeit mit Narratologen stellen sich im Bereich der NLP zwei Hauptaspekte heraus: Zum einen bedingt der Fokus auf eine spezielle Textdomäne die Anpassung und den flexiblen Einsatz von NLP-_Komponenten, die zumeist für Zeitungstexte optimiert sind. Auf der anderen Seite ergeben sich im Bereich der Modellbildung insbesondere im Bereich des maschinellen Lernens spezifische Herausforderungen, um die Akzeptanz von automatischen Annotationen sicherzustellen. Beide Aspekte sollen im Folgenden erläutert werden. Der NLP-_Workflow Zur Erfassung und automatischen Vorhersage linguistischer Oberflächenphänomene 7 entwickelten wir eine flexible und modulare NLP-_Pipelinearchitektur auf Basis von UIMA , die Annotationen mit steigendem Komplexitätsgrad vornimmt und die Ergebnisse in einem Schichtenmodell speichert. Die modular aufgebaute Pipeline ermöglicht einen flexiblen Austausch von Komponenten. Diese Flexibilität ist im Kontext unserer Textdomäne, also literarischer Texte, besonders 7 http://uima.apache.org/ 5 hilfreich und unabdingbare Voraussetzung, wie sich im Verlauf des Projekts gezeigt hat. Da NLP-_Komponenten auf der Domäne von Zeitungstexten entwickelt werden, funktionieren viele Systeme nur auf einem Teil der Daten ähnlich qualitativ gut wie auf der Ursprungsdomäne. Details zur Architektur und den verwendeten NLP-_Komponenten sind in Bögel et al. (2014) beschrieben. Sichtbarmachung von Entscheidungsprozessen im maschinellen Lernen Neben Features, die die Grundvoraussetzung für die Modellierung maschineller Lernverfahren darstellen und aus der oben dargestellten Pipeline gewonnen werden, ergeben sich auch bei der Wahl des konkreten Lern-_Algorithmus interessante Aspekte durch das Gesamtprojekt. In der Theorie des maschinellen Lernens werden Modelle und Gesamtsysteme danach bewertet, welchen empirischen Fehler sie auf ungesehenen Testdaten produzieren (Vapnik, 1998). Ein ideales System würde auf ungesehenen Daten perfekte Ergebnisse liefern und keine Fehler bei der Vorhersage machen. Vor dem Hintergrund unseres Kollaborationsprojektes zeigt sich jedoch, dass die Minimierung des Fehlers von Annotationen nur ein Qualitätsaspekt von Algorithmen ist. Um höhere Akzeptanz von Ergebnissen solcher Systeme zu erreichen, müssen sie einerseits verlässliche Vorhersagen produzieren, aber andererseits auch transparenten, nachvollziehbaren Entscheidungsprozessen zugrundeliegen. Mit zunehmendem Komplexitätsgrad maschineller Lernverfahren sinkt jedoch die direkte Nachvollziehbarkeit. So ist bei einer Support Vector Machine (Hearst et al., 1998), einem Standardverfahren des maschinellen Lernens, nicht ohne Weiteres nachvollziehbar, weshalb eine konkrete Entscheidung getroffen wurde und welche Einzelentscheidungen und Featurekonstellationen konkret zum Endergebnis gefüht haben. Derartige Black-_Box-_Ansätze erschweren jedoch die Akzeptanz automatischer Annotationen. Ein Beispiel für nachvollziehbare Algorithmen stellen Entscheidungsbäume (decision trees) dar, wie sie in Quinlan (1986) erstmalig beschrieben sind. Durch eine Visualisierung des Modells ist es möglich (vgl. Abbildung 3), jede Teilentscheidung, die zur Klassifikation beigetragen hat, nachzuvollziehen und den Einfluss von individuellen Kriterien (Features) zu verfolgen. Abgesehen von der Nachvollziehbarkeit und Transparenz verhindern Black-_Box-_Ansätze auch direkte Eingriffsmöglichkeiten in den Vorhersageprozess. Für die Vorhersage bestimmter Phänomene (beispielsweise der Erzählgeschwindigkeit in Erzähltexten), die 6 ambigen Konzepten zugrunde liegen, können verschiedene Features als relevant erachtet werden. Bezogen auf Abbildung 3 wäre es so beispielsweise möglich, ein Feature zu entfernen und die Auswirkungen auf das neue Modell direkt zu beobachten. Abbildung 3: Schematische Visualisierung eines Decision Trees. Dieses dargestellte Transparenz-_ und damit auch Akzeptanz-_Problem stellt sich für maschinelle Lernprozesse grundsätzlich, wenn sie abseits eines reinen Selbstzwecks in einen konkreten Anwendungskontext eingebettet werden, anstatt für synthetische Benchmarks Ergebnisse zu produzieren. 4. Gemeinsame Erkenntnisse aus der interdisziplinären Arbeit Die hier beschriebenen, durch die Zusammenarbeit veränderten Bedingungen der Textanalyse sind aus unserer Sicht typisch für Ansätze im Bereich der Digital Humanities und werden von den dort häufig genutzten kollaborativen Verfahren verstärkt. Damit wird offensichtlich, dass der Einsatz neuer Methoden nicht nur die Bearbeitung neuer Fragestellungen ermöglicht, sondern auch traditionelle Methoden wie etwa die für die Literaturwissenschaft zentrale Methode der hermeneutischen Textanalyse oder den ergebnisorientierten Zugang der NLP ergänzt bzw. modifiziert 'und dadurch so weiter entwickelt, dass sowohl die interdisziplinäre als auch die disziplinäre Forschungsarbeit von der Entwicklung profitiert. In beiden Fällen hat die Erhöhung der Transparenz der genutzten Prozesse gemäß den methodischen Bedarfen der anderen Disziplin maßgeblich zum Erfolg der Weiterentwicklung 7 beigetragen. Entsprechend wäre es interessant zu prüfen, ob dies generell eine produktive Strategie zur methodischen und methodologischen Verbesserung von in interdisziplinären Digital Humanities-_Projekten genutzten Forschungsstrategien ist. Bibliographie Bögel, T. & Gertz, M., Gius, E. & Jacke, J. & Meister, J.C & Petris, M. & Strötgen, J. (im Erscheinen). Collaborative Text Annotation Meets Machine Learning. heureCLéA, a Digital Heuristics of Narrative. DHCommons Journal. Bögel, T. & Strötgen, J. & Gertz, M. (2014). Computational Narratology: Extracting Tense Clusters from Narrative Texts. Proceedings of the 9th Edition of the Language Resources and Evaluation Conference (LREC'14). Reykjavik, Iceland, S. 950-_955. Bühler, A. (2003). Grundprobleme der Hermeneutik. Hermeneutik. Basistexte zur Einführung in die wissenschaftstheoretischen Grundlagen von Verstehen und Interpretation. Hg. von Axel Bühler. Heidelberg: Synchron, S. 3-_19. Gius, E. (2013). Erzählen über Konflikte. Eine computergestützte narratologische Untersuchung von narrativen Interviews zu Arbeitskonflikten. Dissertation, Universität Hamburg. Gius, E. & Jacke, J. (in Vorbereitung). Informatik und Hermeneutik. Zum Mehrwert interdisziplinärer Textanalyse. Zeitschrift für digital Humanities. Gius, E. & Jacke, J. (2014). Zur Annotation narratologischer Kategorien der Zeit. Guidelines zur Nutzung des CATMA-_Tagsets. Hamburg. http://heureclea.de/publications/guidelines.pdf/ Hearst, M.A. & Dumais, S.T. & Osman, E. & Platt, J. & Scholkopf, B. (1998). Support Vector Machines. Intelligent Systems and their Applications 13 (4), IEEE, S. 18-_28. Quinlan, J.R. (1986). Induction of Decision Trees. Machine learning 1 (1), S. 81-_106. Vapnik, V. N. (1998). Statistical Learning Theory. Vol. 2. New York: Wiley."
2015,DHd2015,2015_cls_metadata_extracted.csv,Comedia - Comédie: Topic Modeling als Perspektive auf das spanische und französische Theater des 17. Jhdts.,Christof Schöch (Universität Würzburg); Nanette Rißler-Pipka (Universität Siegen),"Topic Modeling, Dramen","Topic Modeling, Dramen","1. Hintergrund Im Europa des 17. Jahrhunderts entwickelten sich zeitgleich verschiedene Formen des Theaters. Trotz unterschiedlicher sozialer und poetologischer Kontexte weisen das spanische und französische Theater viele (stoffliche / stilistische) Verbindungen auf, die auf eine gemeinsame europäische Theatergeschichte hindeuten (Couderc 2013). Die Frage, ob man dazu nicht Methoden der Digital Humanities nutzen sollte, stellte sich bisher in der Romanistik nicht (anders als in weiteren Philologien, vgl. Rybicki 2012, Jockers 2013, Eder 2014). Allgemein gilt, dass sprachübergreifende, quantitative Textanalysen eine Herausforderung bleiben (vgl. Steinberger 2009, Eder/Rybicki 2011). In romanistischer Tradition über Sprachgrenzen hinweg quantitative Verfahren anzuwenden, scheint mit Topic Modelling möglich zu sein: Die Topics mehrerer einheitlich strukturierter Textsammlungen können zunächst unabhängig voneinander modelliert werden, um dann auf der Grundlage von Topic-Labels und strukturellen Merkmalen Öhnlichkeiten und Unterschiede zu ermitteln. 2. Fragestellungen Anstelle von Einzelhypothesen und konkreter Passagenvergleiche sind hier zwei spanische und französische Textsammlungen durch Topic Modeling verglichen worden. Welche Arten von Topics liegen vor, und wie verhalten sie sich zueinander? Welche Relation besteht zwischen den Topics und Kategorien wie Untergattungen (Komödie / Tragödie)? Wie gestaltet sich dies im Vergleich des spanischen und französischen Theaters? Über diese Fragen hinaus soll die Eignung der Methode für Theaterstücke geprüft werden. Wie verhalten sich die ""Topics"" zu theaterwissenschaftlich relevanten ""Themen""? Welche Perspektivenverschiebung ergibt sich durch ein quantitatives Verfahren wie Topic Modeling? 3. Textsammlungen Die spanische Textsammlung enthält 145 Theaterstücke von sechs Autoren. Die Stücke sind zwischen 1585 und 1688 erschienen. Die Untergattungen sind ""drama"", ""comedia"" und ""auto sacramental"". Die Texte stammen von www . comedias . org, Wikisource und der Biblioteca Cervantes. Die französische Textsammlung enthält 143 Theaterstücke von neun Autoren. Die Stücke sind zwischen 1630 und 1708 erschienen, und stammen von www.theatre-classique.fr. Die Untergattungen sind ""comédie"", ""tragédie"" ""tragi-comédie"" und ""pastorale"". 1 4. Methode: Topic Modeling Topic Modeling ist ein quantitativer Ansatz, um in größeren Textsammlungen thematische Muster zu entdecken (Blei 2003; Anwendungen in den DH: Blevins 2010, Rhody 2012, Jockers 2013). Mathematisch gesehen sind ""Topics"" Verteilungen von Auftretenswahrscheinlichkeiten von Wörtern. Die Wörter eines Topic mit der höchsten Auftretenswahrscheinlichkeit sind sich semantisch (oder anderweitig) ähnlich (vgl. Blei 2011 und Steyvers & Griffiths 2007). Durch Verknüpfung mit Metadaten können thematische Trends über einen Zeitverlauf oder thematische Differenzen zwischen Textgattungen entdeckt werden. Wichtige Parameter sind das Präprozessieren der Texte (bspw. Lemmatisierung), die Auswahl der zu berücksichtigenden Wörter (nach Wortarten, Wortfrequenzen oder Stoplist), die Textsegmentierung sowie die Anzahl der Topics, die gefunden werden sollen. Für diese Studie wurden die Texte mit TreeTagger (Schmidt 1994) lemmatisiert und nach Wortarten annotiert. Es wurden verschiedene Textfassungen generiert, die bspw. nur Substantive und Verben enthalten, die Texte in Segmente von 40 Lemmata zerlegt und Topic Modeling mit MALLET (McCallum 2009) durchgeführt. Die Anzahl der Topics wurde auf 50 bzw. 200 festgelegt. 5. Ergebnisse und Diskussion 5.1 Die ermittelten Topics Die ermittelten 50 Topics lassen sich meist mit einem Begriff zusammenfassen, der die inhaltliche Gemeinsamkeit der wichtigsten Worte im Topic fasst. Es gibt allgemeinere und spezifische Topics mit unterschiedlichem Gewicht in der Textsammlung, was hier am Beispiel der französischen Topics gezeigt wird (Abb. 1). Abb. 1: Auswahl von Topics aus der französischen Textsammlung. Einige Topics betreffen allgemein gefasste, erwartbare Themen, wie bspw. Liebe / Intrigen (5 der 6 wichtigsten Topics gehören in diesen Themenbereich). Das Liebestopic enthält oft ein Element des Schmerzes und Hasses, das auf die Tragödie hindeutet (Topic 14). Dagegen lässt sich ein inhaltlich typisches Komödientopic nur durch selbstreferentiellen Begriffe erkennen (Topic 34). Faktisch am distinktivsten für die Komödie sind dagegen ein relativ unbestimmtes Topic (33, ""Suchen-Finden"") sowie Topic 01 (""Vergnügen""; vgl. 5.3). Andere Topics sind spezifischer (bspw. Topic 11, ""Gefahr"" oder Topic 24 ""Geheimnis"") und könnten vermuten lassen, dass sie mit bestimmten Untergattungen des Theaters verknüpft sind (bspw. Topics 30 und 38, ""Verbrechen""). Allerdings zeigt ein Vergleich von Topics und Textklassen (vgl. 5.3.), dass Topic 38 zwar der Tragödie zugeordnet werden kann, es in Topic 30 aber offenbar um ein ""Verbrechen"" geht, das sich in Komödien abspielt. 5.2 Die Topics im Vergleich Vergleicht man die Topics der französischen und der spanischen Textsammlung miteinander, stellt man einige Übereinstimmungen und Unterschiede fest (Abb. 2). 2 Abb. 2: Topics im Sprachvergleich In beiden Textsammlungen präsent sind allgemeine Topics, wie diejenigen um das Thema ""Liebe"" (bspw. Topic 41fr vs. Topic 35sp). Zwar kann man, abgesehen von einer leichten Tendenz in Richtung Lust (""celo, ver"") im spanischen und Leid (""souffrir"") im französischen Topic, kaum von einer semantischen Differenz sprechen. Dennoch kann Topic 35sp in der Topicverteilung nach Gattungen (vgl. 5.3) der (spanischen) ""Comedia"" zugeordnet werden, während Topic 41fr der (französischen) Tragödie zugeordnet wird. Die ähnlich große Wichtigkeit beider Topics in den jeweiligen Korpora belegt die stoffgeschichtliche Verwandtschaft des Theaters beider Länder. Auch bei noch spezifischeren Topics gibt es zahlreiche Übereinstimmung, bspw. Topic 4fr und Topic 2sp, die beide mit dem Titel ""Gnade-Gottes"" versehen werden könnten, oder sehr konkrete Topics wie ""Krieg"" (Topic 46fr und 13sp) oder ""Arzt"" (Topic 7fr und 17sp), die mit fast identischen Wörtern vorkommen. Topics in der spanischen Sammlung ohne Übereinstimmung in der französischen Sammlung sind bspw. Topic 45 (""Schuld-Unschuld"") oder 16 (""Gericht-König""). Umgekehrt sind Topics in der französischen Sammlung ohne Übereinstimmung in der spanischen Sammlung bspw. Topic 18 ""Gehorsam"" oder 38 ""Verbrechen""). Diese Ergebnisse bieten Ausgangspunkte für einen Abgleich mit Erkenntnissen der Literaturgeschichte. 5.3 Topics und Textklassen Mit unterschiedlicher Ausprägung zeigt sich in beiden Textsammlungen, dass die Untergattungen jeweils mindestens einen charakteristischen Topics besitzen (Abb. 3 und 4). 3 Abb. 3: Heatmap für Topic-Scores in Genres (Spanisch) (20 Topics mit größter Varianz, gemessen als Standardabweichung) Der wesentliche Kontrast bei den spanischen Stücken (Abb. 3) liegt zwischen ""Auto sacramentales"" einerseits, ""Comedias"" und ""Dramas"", andererseits. Letztere haben schwächer kontrastive Topics, bspw. Topic 35 (""Liebe-Hoffnung"") oder, auf niedrigerem Niveau, Topic 49 (unklar). Bei den französischen Stücken hat jede Untergattung zumindest einen charakteristischen Topic (Abb. 4): Topic 33 (""Suchen-Finden"") für die Komödie, Topic 41 (""Liebe-Hoffnung"") für die Tragödie, Topic 08 (""Liebe-Schönheit"") für die Pastorale. Ausnahme ist die Tragikomödie. Abb. 4: Heatmap für Topic-Scores in Genres (Französisch) (20 Topics mit größter Varianz, gemessen als Standardabweichung) Insgesamt scheint die gattungsbezogene Trennschärfe in den französischen Texten deutlicher als in den spanischen Texten. Dieser Befund entspricht den unterschiedlichen französischen und spanischen Poetiken der Zeit. 5.4 Gruppierung auf Grundlage von ""topic scores"" Es ist nicht auszuschließen, dass die verwendeten Gattungsbezeichnungen tatsächlich vorhandene Differenzierungen verdecken. Ohne vorgängige Kategorien, nur auf Grundlage der Öhnlichkeit von Stücken nach der Verteilungen von 200 Topics sollten daher mit Principal Component Analysis Strukturen in den Textsammlungen gefunden werden. Die räumliche Verteilung der Stücke zeigt für die spanischen Texte kaum Struktur und bildet eine recht einheitliche Wolke (Abb. 6). Die französischen Texten (Abb. 5) zeigen mehr Struktur: ein kompakterer, leicht separierter Bereich rechts oben sowie ein weiterer, besonders dichter Bereich links oben. Die in den ersten beiden Komponenten enthaltene Varianz der Daten ist mit zusammen 17,3% (französisch) und 9,2% (spanisch) verhältnismäßig gering. 4 Abb. 5: PCA-Plot auf Grundlage von 200 topic scores (französische Sammlung, Genre-Labels) Die Verteilung der Gattungssymbole zeigt, dass die französischen Texte nach Gattungen gruppiert sind: rechts oben die Komödien, links oben die Tragödien; die stärker verteilten Tragikomödien überlappen vor allem mit den Tragödien. Bei den spanischen Texten gibt es ebenfalls Gruppen: die ""Auto Sacramentales"" im linken unteren Quadranten, die ""Comedias"" eher in der rechten Hälfte, die Dramen breit gestreut in der Mitte. 5 Abb. 6: PCA-Plot auf Grundlage von 200 topic scores (spanische Sammlung, Genre-Labels) Einfache Korrelationstests bestätigen den Gesamteindruck (Abb. 7). In den französischen Stücken korreliert Genre sehr deutlich nur mit PC1, Autorschaft dagegen vor allem mit PC2. Bei den spanischen Stücken ist nur die Korrelation zwischen Autorschaft und PC1 stark. Abb. 7: Korrelationtests zwischen Principal Components und Autorschaft bzw. Gattungszugehörigkeit 6 Die thematische Differenzierung der Stücke ist also in der französischen Textsammlung stärker ausgeprägt und korreliert auch stärker mit den vorhandenen Gattungs-Kategorien als in der spanischen Textsammlung. Bilanz und nächste Schritte Zahlreiche Einzelergebnissen zum Verhältnis der inhaltlichen Bestimmung einzelner Topics und ihrer eventuellen Zuordnung zu Untergattungen des Theaters zeigen, dass sich spanisches und französisches Theater auf Grundlage der Topic-Verteilungen auf eine Weise unterscheiden, die gattungspoetischen Positionen der Zeit entspricht und an vorhandene literaturwissenschaftliche Erkenntnisse anschlussfähig ist. Außerdem zeigen die Ergebnisse den Unterschied zwischen ""Topics"" und ""Themen"" im literaturwissenschaftlichen Sinn. Der semantische Gehalt des Topics, der in einem Begriff wie ""Liebe-Leidenschaft"" (Topic 14) gebündelt werden kann, beschreibt nicht unbedingt das zentrale Thema der sich dahinter verbergenden Theaterstücke (vgl. die Diskussion der Tragödien-, Komödien und Pastoralentopics). Diese vermeintliche Kluft zwischen Topics und literaturwissenschaftlichen Themen ist aber eher eine Chance als ein Dilemma: so lassen sich vorschnelle Interpretationsansätze überprüfen und neue Erkenntnisse gewinnen. Methodisch wird deutlich, dass Topic Modeling selbst nur ein Schritt in der Analyse- und Interpretationskette sein kann, der durch linguistische Annotation und Metadaten vorbereitet werden muss, und dessen Ergebnisse durch weitere Verarbeitung und Kontextualisierung erst bedeutungsvoll werden. Als nächste Schritte könnte die Textsammlung erweitert und um weitere Metadaten ergänzt werden, um die Vergleichbarkeit der Textsammlungen zu erhöhen. Es könnte mit ""Multilingual Topic Modeling"" (Boyd-Graber & Blei 2009) operiert werden, das unmittelbar thematische Bezüge zwischen Dokumenten in unterschiedlichen Sprachen ermittelt. Alternativ wäre ein algorithmisches Verfahren zur Öhnlichkeitsbestimmung verschiedensprachiger Topics zu entwickeln (vgl. Pouliquen 2006). 7 Bibliographie Blei, David M. 2011. ""Introduction to Probabilistic Topic Models."" Communication of the ACM. Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. ""Latent Dirichlet Allocation."" Journal of Machine Learning Research 3,March: 993‚Äì1022. Blevins, Cameron. 2010. ""Topic Modeling Martha Ballard‚Äôs Diary."" Historying. http :// historying . org /2010/04/01/ topic - modeling - martha - ballards - diary /. Boyd-Graber, Jordan, and David M. Blei. 2009. ""Multilingual Topic Models for Unaligned Text."" In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, 75‚Äì82. UAI ‚Äô09. Arlington, Virginia, United States: AUAI Press. http :// dl . acm . org / citation . cfm ? id =1795114.1795124. Couderc, Christophe 2013: La tragédie Espagnole et son contexte Européen : XVIe - XVIIe si√®cles, Paris : Presses Sorbonne Nouvelle. Eder, M. 2014. Stylometry, network analysis and Latin literature. In: Digital Humanities 2014: Book of Abstracts, EPFL-UNIL, Lausanne, pp. 457-58. http :// dharchive . org / paper / DH 2014/ Poster 324. xml Jockers, Matthew L. 2013. Macroanalysis: Digital Methods and Literary History. Topics in the Digital Humanities. University of Illinois Press. McCallum, Andrew K. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu. Pouliquen, Bruno, Ralf Steinberger, and Camelia Ignat. 2006. ""Automatic Annotation of Multilingual Text Collections with a Conceptual Thesaurus."" arXiv:cs/0609059, September. http :// arxiv . org / abs / cs /0609059. Rhody, Lisa M. 2012. ""Topic Modeling and Figurative Language."" Journal of Digital Humanities 2,1. http :// journalofdigitalhumanities . org /2-1/ topic - modeling - and - figurative - language - by - lisa - m - rhody /. Rybicki, Jan. 2012. The great mystery of the (almost) invisible translator: stylometry in translation. In M. Oakley and M. Ji (eds.), Quantitative Methods in Corpus-Based Translation Studies. Amsterdam: John Benjamins, pp. 231-248. Rybicki, Jan, and Maciej Eder. 2011. Deeper Delta across genres and languages : do we really need the most frequent words ?Literary and Linguistic Computing 26(3), 315-21. Schmid, Helmut. 1994. ""Probabilistic Part-of-Speech Tagging Using Decision Trees."" In Proceedings of International Conference on New Methods in Language Processing. Manchester. Steyvers, Mark, and Tom Griffiths. 2006. ""Probabilistic Topic Models."" In Latent Semantic Analysis: A Road to Meaning, edited by T. Landauer, D. McNamara, S. Dennis, and W. Kintsch. Laurence Erlbaum."
2015,DHd2015,2015_cls_metadata_extracted.csv,Die explorative Visualisierung von Texten. Von den Herausforderungen der Darstellung geisteswissenschaftlicher Primär- und Annotationsdaten,Evelyn Gius (Universität Hamburg); Marco Petris (Universität Hamburg),"Annotation, Analyse, Visualisierung","Annotation, Analyse, Visualisierung","1. Zur Komplexität von Textdaten Die Visualisierung von Textdaten ist innerhalb des Bereichs der Datenvisualisierung eine besondere Herausforderung, da es sich bei ihnen um unstrukturierte Daten handelt: Bevor man Textdaten visualisieren kann, muss aus ihnen eine Struktur abgeleitet werden. Hinzu kommt, dass Textdaten eine Vielzahl an Betrachtungsmöglichkeiten eröffnen, die durch die zahlreichen Bedeutungsdimensionen von Texten bedingt werden. Die einzelnen Dimensionen von Texten können durch Annotationen herausgearbeitet werden, wobei jede Annotationsschicht eine oder mehrere Dimensionen des Textes offenlegen kann. In diesem Sinne sind Textdaten also multidimensional. Insbesondere im Bereich der geisteswissenschaftlichen Textanalyse ist aufgrund des hermeneutischen Zugangs zu Texten auch in spezifischen Analysen nicht von vornherein klar, auf welche Weise Analyse und Interpretation zusammenhängen. Entsprechend muss eine sinnvolle Visualisierung von Textdaten im geisteswissenschaftlichen Kontext als exploratives Werkzeug zum Herausarbeiten möglicher Zusammenhänge fungieren können. 1 Ein Blick in die einschlägige Literatur zur Datenvisualisierung zeigt, dass der Besonderheit von Textdaten häufig nicht Rechnung getragen wird. Zumindest scheint im traditionell mit Datenvisualisierung befassten informationswissenschaftlichen Bereich die Komplexität von annotierten Textdaten nicht immer im vollen Umfang wahrgenommen zu werden. So verweisen etwa Ward et al. (2010) in ihrer umfassenden Einführung zu Datenvisualisierung im Kapitel zu Textdaten auf drei mit Texten zusammenhängende Sucharten, die für die Anforderungen an die Visualisierung von Texten ausschlaggebend sind. 2 Die auf die Sucharten folgende Darstellung von Möglichkeiten der Textvisualisierung beschränkt sich allerdings auf die Darstellung von Texten und Korpora mit Metadaten (Erscheinungsjahr, Publikation o.ä.). Das Problem wird in der Zusammenfassung des Textvisualisierungskapitels offensichtlich: Die diskutierten Ansätze beträfen das ""transforming unstructured text into structured data suitable for visualization and analysis"" (Ward et al. 2010: 311). Die Option, dass der Text bereits mit Analysedaten in Form von 1 Vorgehen, die darauf basieren, die Komplexität der Daten automatisiert zu reduzieren, erscheinen uns deshalb auch nicht geeignet für das beschriebene Problem (vgl. zu solchen Ansätzen z.B. Yang et al. 2003; Tatu et al. 2011). 2 Typischerweise würden Zeichenketten in Form von Wörtern, Phrasen oder Themen gesucht, im Falle von partiell strukturierten Daten könnte außerdem nach Beziehungen zwischen Wörtern, Phrasen, Themen oder Dokumenten gesucht werden und schließlich ginge es in strukturierten Texten oder Textkorpora meistens um das Identifizieren von Mustern oder Auffälligkeiten innerhalb von Texten bzw. Dokumenten (vgl. Ward et al. 2010:291). Annotierte Textdaten fallen also potentiell unter die letzten beiden Fälle. 1 Annotationen angereichert sein könnte, wird nicht in Betracht gezogen. Das, obwohl in der Einleitung auf die drei Ebenen von Texten verwiesen 'die lexikalische, die syntaktische und die semantische 'und im Fall der syntaktischen Ebenen sogar explizit die Möglichkeit von Annotationen im Rahmen von named entity recognition (NER)_Prozessen erwähnt wurde (Ward et al. 2010:294). 2. Geisteswissenschaftliche Textdaten In der stark geisteswissenschaftlich orientierten Position von Drucker (2014) werden hingegen die vielfältigen Interpretationsmöglichkeiten in den Fokus gerückt. Sie schreibt über die Visualisierung geisteswissenschaftlicher Interpretation: ""The challenge is enormous, but essential, if the humanistic worldview, grounded in the recognition of the interpretive nature of knowledge, is to be part of the graphical expressions that come into play in the digital environment"" (Drucker 2014: 136). Drucker geht es v.a. darum, die mit geisteswissenschaftlichen Analysen einhergehende Unsicherheit in der Darstellung des Wissens zu verdeutlichen, wobei sie sich nicht nur auf Texte beschränkt. Was bedeutet das im Falle von Texten? Betrachten wir die Problematik an mit CATMA 3 annotierten Texten, die durch die flexiblen Annotationsmöglichkeiten des Werkzeugs exemplarisch für die große Bandbreite und gleichzeitig eingeschränkte Vorhersagbarkeit 4 geisteswissenschaftlicher Analysen sind. Für die Visualisierung von in CATMA erzeugten Text_ und Annotationsdaten ist die von Drucker angesprochene Unsicherheit geringer, da es um die Analyse von Texten geht: Sie beschränkt sich auf (Text_)Interpretationen und liegt zudem nur in Form von Annotationen vor, die diese Unsicherheit konzeptionell durch entsprechende Tags fassen. Die Tags selbst beinhalten aber keine Unsicherheit, die für 5 die weitere Analyse berücksichtigt werden muss. Trotzdem ist Druckers Beobachtung zur Besonderheit geisteswissenschaftlicher Aussagen auch für unseren Zweck gültig und muss für die Visualisierung der Text_ und Annotationsdaten berücksichtigt werden: ""[‚Ä¶] we need to conceive of every metric ""as a factor of X"" , where X is a point of view, agenda, assumption, presumption, or simply a convention. By qualifying any metric as a factor of some condition, the character of the ""information"" shifts from self_evident ""fact"" to constructed interpretation motivated by a human agenda. "" (Drucker 2014:131). Aufgrund des freien Annotationsschemas, das CATMA zur Verfügung stellt, ist die Art der ""Information"" , die die Annotationen enthalten, nämlich nicht über die vorliegenden Daten zugänglich: Man kann in CATMA genauso gut strukturelle Textmerkmale wie inhaltliche Aspekte annotieren und dafür eine eigene Annotationshierarchie entwickeln, deren Struktur zwar von der Anlage her hierarchisch ist, die aber prinzipiell überlappendes und widersprüchliches Markup zulässt. 3 CATMA = Computer Aided Text Markup and Analysis, vgl. www.catma.de (gesehen am 10.11.2014). 4 In CATMA können Texte anhand von frei gewählten Tags annotiert werden, die zu so genannten Tagsets zusammengefasst werden. Die so entstehende Taxonomie oder Systematik kann wiederverwendet werden. Die Texte und die Annotationen können außerdem mit einer umfangreichen Suchfunktionalität durchsucht und analysiert 'und ggf. weiter annotiert werden. Zum damit außerdem verbundenen Konzept des hermeneutischen Markups vgl. Bögel et al. (im Erscheinen). 5 vgl. dazu Jacke & Meister (2014). 2 3. Anforderungen an Visualisierung als Exploration Aufgrund der nicht a priori eingrenzbaren Zwecke der Annotation und der Analyse muss die Visualisierung von Textdaten so generisch wie möglich gestalten werden. Nur so kann sie ohne ein tieferes Verständnis über die jeweils vorliegenden Text_ und Annotationsdaten eingesetzt werden und einen Mehrwert bei der Analyse der Daten 6 erzeugen. Grundsätzlich konzipieren wir Visualisierungen deshalb ausgehend von der Frage, wie viele und welche Dimensionen der Daten dargestellt werden sollen. 7 Für die Auswahl der Dimensionen stellt CATMA über die Struktur der Ergebnismenge der Abfragen folgende Kategorien zur Verfügung: _ Metadaten der Dokumente (z.B. Titel, Autor, etc.), _ Tag bzw. Typ der Annotation, _ Properties der Annotation und die für den annotierten Text vergebenen Werte, _ annotierter Text, _ Position im Text (via Zeichen_Offset), _ Textkontext des annotierten Textes (variable Anzahl von Token), _ Vorkommenshäufigkeit des annotierten Textes, _ Vorkommenshäufigkeit der Annotation, _ weitere berechnete Kategorien, wie der z_Faktor oder der TF_IDF Neben dem generischen Zugang über die Dimensionen der Daten muss auch ein Mechanismus zur Verfügung gestellt werden, mit dem der Zweck einer spezifischen Analyse in der Visualisierung der Daten herausgearbeitet werden kann 'und der die erzeugten Visualisierungen als explorative Heuristik nutzbar macht. Für die damit zusammenhängenden spezifischen Erkenntnisinteressen werden deshalb zusätzliche Anpassungsmöglichkeiten in Form von wählbaren Parametern eingeführt. Diese sollen typische Varianten abfangen, wie etwa die Frage, ob die Häufigkeit einer Annotation oder aber der annotierte Textumfang dargestellt werden soll, wie mit überlappenden Annotationen verfahren werden soll (soll etwa eine zweifach annotierte Stelle zweimal oder nur einmal gezählt bzw. dargestellt werden?) oder ob die Struktur der Tagsets so ist, dass sich Tags auf derselben Hierarchieebene gegenseitig ausschließen oder ob sie sich ergänzen können. 8 4. Beispiele Die oben angestellten √úberlegungen sollen an folgenden Beispielen demonstriert werden. Datengrundlage ist das in Gius (2013) beschriebene und analysierte umfangreich 6 Dies gilt nicht für die Visualisierung zu Demonstrations_ bzw. Kommunikationszwecken 'also von Daten, die bereits analysiert und interpretiert wurden. 7 Mit ""Dimensionen"" sind also nicht räumliche Dimensionen gemeint. Dies wird allerdings von einigen gängigen Ansätzen zur Visualisierung mehrdimensionaler Daten angenommen, die Textdaten nur als einen 'eindimensionalen 'Datentyp betrachten und allgemeine Modelle entwickeln (vgl. etwa Shneiderman 1996). 8 Auch hier unterscheidet sich der vorgestellte Ansatz durch seinen Fokus auf die Spezifik von Texten wieder deutlich von Ward et al. (2010) oder Shneiderman (1996), die so genannte tasks als Basis für zusätzliche explorative Funktionalitäten betrachten. 3 annotierte Korpus. 9 Die hier nur kurz beschriebenen Visualisierungen werden ebenso wie eine Reihe weiterer Visualisierungen in CATMA zur Verfügung gestellt. Ihre Funktionen und der damit verbundene explorative Gewinn werden im Rahmen des Vortrags näher vorgestellt werden. Interaktive TreeMap Abbildung 1: Interaktive TreeMap 10 9 Das Korpus besteht aus 24 Texten mit insgesamt 86.246 Wörter, die auf etwa 150 als Tags eingeführte narratologische Konzepte untersucht und mit insgesamt 24.347 Annotationen versehen wurden. 10 Erstellt auf Basis von Google Charts: https://developers.google.com/chart/interactive/docs/gallery/treemap?hl=de (gesehen am 10.11.2014). 4 Das erste Beispiel ist eine interaktive TreeMap. Jede einzelne Sicht zeigt zwei Dimensionen: (1) Die Vorkommenshäufigkeit der Abfrageergebnisse (Tags, Wörter, o.ä.) als Größe des zugehörigen Rechtecks und (2) die durchschnittliche annotierte Textmenge als Farbintensität auf einer Skala von rot (weniger) bis grün (mehr). Die interaktive Komponente ermöglicht das Ergründen einer dritten Dimension: Durch Klicken der einzelnen Rechtecke kann man durch (3) die Hierarchie des Tagsets navigieren. Abbildung 1 zeigt zwei Ebenen: Rechts die höchste Ebene mit den beiden Top_Level Tags ""narratological _ tagset"" und ""Konfliktanalyse"" und links die Ebene 1 den Zweig entlang dem Tag ""Konfliktanalyse"" mit den Tags der darunter liegenden Ebene. Gezeigt wird also die Verteilung von Vorkommenshäufigkeit und annotierter Textmenge für die Hierarchieebene. Für die dargestellte Datenbasis ist das insofern interessant, als hier die Konflikthaftigkeit von Erzählungen bzw. die als konflikthaft oder konfliktlos annotierten Passagen dargestellt werden. Für die Analyse des Korpus ist sowohl die Frage nach der Häufigkeit, in der konflikthafte Passagen auftauchen (sie unterbrechen nämlich von den Erzählerinnen eigentlich als konfliktlos deklarierte Erzählabschnitte und deshalb ist ihre Anzahl relevant), als auch die reine Textmenge, die sie umfassen (wird ausgiebiger über konfliktlose oder über konflikthafte Situationen erzählt?), interessant. Die Visualisierung als dreidimensionale TreeMap ermöglicht es, die beiden Betrachtungsweisen 'Anzahl vs. Textmenge 'überblickshaft in Beziehung zu setzen und dabei durch die hierarchisch angeordneten Tags zu navigieren, also zusammengefasste und detailliertere Perspektiven zu wählen. Small Multiples Das zweite Beispiel (vgl. Abbildung 2) zeigt die Vorkommenshäufigkeit von zwei Annotationen (Wiedergabe von mentalen Prozessen und Wiedergabe von Rede) im Textverlauf bei neun Texten des Korpus. Die Vorkommenshäufigkeit wird auf der y_Achse und der Textverlauf in 10%_Schritten auf der x_Achse dargestellt. Für jeden ausgewählten Text wird jeweils ein Koordinatensystem als dritte Dimension erstellt, in dem die Annotationen als farbige Linien abgebildet werden. 11 Diese Darstellung ermöglicht eine explorative Betrachtung der Verteilung der beiden annotierten Phänomene in den Einzeltexten und einen ersten √úberblick über mögliche Muster im gesamten Korpus. Für eine weitere Analyse können auffällige Stellen 'wie etwa besondere Häufigkeiten in einem Textabschnitt oder der Wechsel von dominierender Redewiedergabe zu dominierender Wiedergabe von mentalen Prozessen 'genauer betrachtet werden: Das Anklicken der entsprechenden Punkte im Graphen erzeugt eine KWIC(=KeyWord In Context)_Anzeige der Annotationen im betreffenden Textabschnitt, von denen aus wiederum durch Klicken in den Volltext gesprungen werden kann. 11 Die Darstellung als Linie wurde aus Gründen der √úbersichtlichkeit gewählt, mathematisch gesehen handelt es sich natürlich um diskrete Werte. 5 Abbildung 2: Small Multiples: Distributionsgraphen 12 12 Erstellt auf Basis von Highcharts: http://www.highcharts.com/ (gesehen am 10.11.2014). 6 5. Ausblick Für das dargestellte Korpus sind oben beschriebene Visualisierungen von großem Gewinn. Sie ermöglichen einen √úberblick über die Daten, für die nicht bereits bei der Annotation festgelegt wurde, welche Dimensionen genauer betrachtet und in Zusammenhang gebracht werden müssen. Dadurch sind die für die Analyse der Daten von großem Nutzen. Inwiefern sich nach in diesem Beitrag vorgestellten √úberlegungen entwickelte Visualisierungen auch systematisch als heuristisches Werkzeug eignen und ob sie sich als Alternative gegen generelle Datenvisualisierungen durchsetzen können, ist zum momentanen Zeitpunkt allerdings noch nicht abschätzbar. Neben der Vieldimensionalität der Daten ist dafür insbesondere die Frage der explorativen Funktion der Visualisierungen zentral: Reichen (1) die dargelegte Aufschlüsselung der Analysen nach ihrer Dimensionalität und (2) die Explorationsmöglichkeit durch einstellbare Parameter aus, um Visualisierungen zu erzeugen, die systematische Rückschlüsse auf die dargestellten Daten und Strukturen zulassen 'und nicht nur assoziative Denkanstöße zu liefern? Dies wird in breit angelegten Nutzerstudien zu ergründen sein, ebenso wie untersucht werden muss, ob und auf welche Weise die in den Visualisierungen zum Einsatz kommenden visuellen Metaphern den Verstehensprozess beeinflussen. Referenzen Bögel, Thomas, Michael Gertz, Evelyn Gius, Janina Jacke, Jan Christoph Meister, Marco Petris, and Jannik Strötgen. ""Collaborative Text Annotation Meets Machine Learning: heureCL√âA, a Digital Heuristics of Narrative. "" DHCommons Journal, im Erscheinen. Drucker, Johanna. Graphesis: Visual Forms of Knowledge Production. MetaLABprojects. Cambridge, Massachusetts: Harvard University Press, 2014. Gius, Evelyn. ""Erzählen √úber Konflikte. Eine Computergestützte Narratologische Untersuchung von Narrativen Interviews Zu Arbeitskonflikten. "" Dissertation, Universität Hamburg, 2013. Jacke, Janina, und Jan Christoph Meister. ‚ÄûPushing Back the Boundary of Interpretation: Concept, Practice and Relevance of a Digital Heuristic"" . In Digital Humanities 2014 'Book of Abstracts, 264‚Äì66. Lausanne, 2014. Shneiderman, Ben. ""The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations. "" In In IEEE Symposium on Visual Languages, 336‚Äì43, 1996. Tatu, Andrada, Georgia Albuquerque, Martin Eisemann, Peter Bak, Holger Theisel, Marcus Magnor, and Daniel Keim. ""Automated Analytical Methods to Support Visual Exploration of High_Dimensional Data. "" IEEE Transactions on Visualization and Computer Graphics 17, no. 5 (May 2011): 584‚Äì97. Ward, Matthew, Georges G. Grinstein, and Daniel Keim. Interactive Data Visualization: Foundations, Techniques, and Applications. Natick, Mass: A K Peters, 2010. Yang, J., M. O. Ward, E. A. Rundensteiner, und S. Huang. ‚ÄûVisual Hierarchical Dimension Reduction for Exploration of High Dimensional Datasets"" . In Proceedings of the 7 Symposium on Data Visualisation 2003, 19‚Äì28. VISSYM ""03. Aire_la_Ville, Switzerland, Switzerland: Eurographics Association, 2003."
2015,DHd2015,2015_cls_metadata_extracted.csv,Sprachwissenschaftliche Untersuchungen zum Klagspiegel Conrad Heydens (1436) und zum Laienspiegel Ulrich Tenglers (1511),"Barbara Aehnlich (Universität Jena, Deutschland); Elisabeth Witzenhause (Universität Jena, Deutschland)","Normalisierung, automatische Annotation, Transkription, WebAnno, GATE, XML, Digitalisierung","Normalisierung, automatische Annotation, Transkription, WebAnno, GATE, XML, Digitalisierung","Das Forschungsvorhaben ist interdisziplinär angelegt und beruht auf einem Korpus von verschiedenen Textzeugen zweier frühneuhochdeutscher Rechtsbücher des 15. und 16. Jahrhunderts, Klagspiegel und Laienspiegel. Der Klagspiegel ist das mit Abstand älteste populärwissenschaftliche Rechtsbuch der Rezeptionszeit und bildet mit dem Laienspiegel zusammen die wichtigste Grundlage an rechtswissenschaftlichen populären Texten des 15. und 16. Jahrhunderts. Ziel ist die Untersuchung der sprachlichen Besonderheiten der Texte und ihrer Auswirkungen auf die Rezeptionsgeschichte des römischen Rechts in Deutschland. Neben der korpusbasierten linguistischen Analyse der Bücher, die eine völlig neue Textsorte begründen, bietet das Projekt auch aus der Perspektive der historischen Rechtssprachenforschung einen innovativen Ansatz. Das Erkenntnisinteresse liegt hierbei auf der Geschichte von Kulturtransferprozessen innerhalb der Jurisprudenz. Durch semantische und linguistische Annotationen wird eine umfassende Forschungsgrundlage geschaffen, die für die Schließung rechts- und sprachhistorischer Forschungslücken einen zentralen Beitrag leistet. Ein weiterer Schritt soll die Digitalisierung mehrerer Ausgaben des Klagspiegels sein, um Prozesse des Schreibsprachwandels im 15. und frühen 16. Jahrhundert nachzuvollziehen. Bisher gibt es kein Korpus frühneuhochdeutscher Rechtstexte. In einem ersten Schritt zur Vorbereitung des Projektes wurden verschiedene Annotationstools getestet und geeignete Formate für die Speicherung evaluiert. Aktuell werden mit der Jenaer Computerlinguistik Möglichkeiten der Normalisierung und automatischen Annotation erprobt. Ziel ist die Beantragung eines größeren Forschungsprojektes, das bestehende Werkzeuge nutzt und die Technologie auf die Besonderheiten des Rechtskorpus anpasst. Das Poster soll die bisherigen methodologischen Überlegungen und Probleme darstellen und bietet somit gleichzeitig einen Überblick und eine Evaluation der aktuell zur Verfügung stehenden Open Source Software zu Annotationszwecken. Die Untersuchungen beziehen sich zum einen auf die sprachliche Herkunft des Klagspiegels und des Laienspiegels. Es soll festgestellt werden, welche Textsorte mit welchen spezifischen 1 sprachlichen Eigenheiten vorliegt. Zudem muss geklärt werden, ob diese Rechtsbücher aufgrund ihrer Herkunft nur im südwestdeutschen Raum oder aber im gesamten hochdeutschen Sprachgebiet verständlich waren. Dabei wird nach möglichen Ausgleichstendenzen gesucht, die vom Oberdeutschen abweichen. Auf der Ebene der Syntax ist zu fragen, welche Strukturen die sprachliche Einfachheit und leichte Verständlichkeit ausmachen, die den Texten in der gesamten (bisher ausschließlich juristischen) Forschung zugeschrieben wird. Im Bereich des Wortschatzes sind besonders die Bezeichnungen juristischer Fachbegriffe oder Tatbestände für die Forschung interessant, denn für diese gab es zuvor im Deutschen keine entsprechenden Termini. Zum anderen soll untersucht werden, inwieweit Klagspiegel und Laienspiegel frühneuhochdeutschen Sprachstandard aufweisen und ob die beiden Bücher durch ihre Verbreitung eine wesentliche Rolle für die Entwicklung des neuhochdeutschen Sprachstandards im Rahmen rechtswissenschaftlicher Prozesse gespielt haben können. Ein Vergleich mehrerer Textzeugen der Rechtsbücher liefert Erkenntnisse des frühneuhochdeutschen Schreibsprachwandels. Der Einfluss der beiden Texte auf die deutsche Standardsprache sowie auf die deutsche Rechtssprache wurde bisher noch nicht analysiert; das Vorhaben soll hierfür eine nutzbare Ausgangsbasis liefern. Eine zentrale Frage ist dabei auch, inwieweit römisches Recht und deutsches Recht sprachlich unterschiedlich vermittelt wurden und ob textintern Varianz zwischen den einzelnen Passagen, die zum Teil auch literarisiert sind, festzustellen ist. Zwei Textzeugen, jeweils eine Ausgabe des Laien- und des Klagpiegels, liegen bereits in digitalisierten Abbildungen vor und wurden transkribiert. Im nächsten Schritt werden sie in ein XML-Format übertragen und sollen semantisch sowie linguistisch annotiert werden, um eine valide Datenbasis für die Untersuchung zu schaffen und das Korpus in einem standardisierten Format in einer Infrastruktur der Digital Humanities zur Verfügung stellen zu können. Im Sinne eines vielseitig nutzbaren Korpus soll die Transkription diplomatisch, mit allen Sonderzeichen und typografischen Besonderheiten, abgebildet werden. Problematisch ist die heterogene Gestalt der Texte, die Mehrfachannotationen notwendig macht. Alle Annotationen werden deshalb in einem XML-Stand-off Format vorgenommen, um eine leichte Übertragung in andere Formate und einen annotationsfreien Primärtext zu ermöglichen. Das TCF-Format bietet hierfür eine gute Möglichkeit und ist mit vielen anderen Formaten kompatibel. 1 Werkzeuge wie WebAnno2 oder GATE3 bieten geeignete Arbeitsoberflächen, deren Vor- und Nachteile es zu diskutieren gilt. 1 http://weblicht.sfs.uni-tuebingen.de/weblichtwiki/index.php/The_TCF_Format.(06.10.2014, 13.30 Uhr). 2 https://code.google.com/p/webanno/. (06.10.2014, 13.46 Uhr). 2 Die Präsentation stellt somit zum einen den Mehrwert der bisher geleisteten Forschungsarbeit im Rahmen der Digital Humanities für sprachwissenschaftliche Untersuchungen historischer Texte heraus, zum anderen werden Grenzen in der Annotation heterogener und nicht standardisierter Sprachdaten deutlich, die weiterer Forschungsarbeit bedürfen. Die interdisziplinär angelegte Forschungsfrage und die unterschiedlichen Zielgruppen des zu erstellenden Korpus sind Faktoren, die es bei der Aufarbeitung der Daten besonders zu beachten gilt. 3 https://gate.ac.uk/sale/tao/split.html. .(06.10.2014, 12.51 Uhr)."
2015,DHd2015,2015_cls_metadata_extracted.csv,eComparatio. Editionsvergleich,"Oliver Bräckel (Universität Erfurt, Max-Weber-Kolleg, Deutschland); Hannes Kahl (Universität Erfurt, Max-Weber-Kolleg, Deutschland); Friedrich Wilhelm Meins (Universität Erfurt, Max-Weber-Kolleg, Deutschland); Charlotte Schubert (Universität Leipzig\Historisches Seminar, Deutschland)",Digitale Edition,Digitale Edition,"Das von der Deutschen Forschungsgemeinschaft (DFG) geförderte Projekt eComparatio wird seit 2014 als Kooperationsprojekt des Lehrstuhls für Alte Geschichte der Universität Leipzig und des ICE (Interdisciplinary Center of E-Humanities in History and Social Sciences/ Forschungsstelle am Max-Weber-Kolleg für kultur- und sozialwissenschaftliche Studien an der Universität Erfurt) entwickelt. Das Ziel des Projektes ist es, eine modular aufgebaute Anwendung zu entwickeln, die es ermöglicht, verschiedene Versionen eines Textes (aus Handschriften, gedruckten oder digitalen Texteditionen) miteinander zu vergleichen. Das Kernstück der Anwendung ist ein Modul zum Vergleich von Textausgaben, das auch die Erstellung eines Variantenapparates für digitale Editionen antiker Autoren ermöglicht. Die Zahl der Vergleichstexte ist beliebig, ebenso das Eingabeformat (TXT, HTML, XML, JSON, PDF). Die Anwendung wird frei skalierbar sein, so dass der Umfang der zu vergleichenden Texte nicht beschränkt ist, das Ergebnis (Kollationierung) soll in Form von Listen als kritischer Apparat (positiver oder negativer Apparat) oder auch in beliebiger anderer Form ausgegeben werden können. In einem weiteren Modul soll für Autorenreferenzen bei der Abfrage von online-Datenbanken die Anbindung an das Referenzsystem CTS (Canonical Text Services) und die Referenz auf Images von Handschriften (über das Image Citation Tool der CITE Collection Services) ermöglicht werden. Die Ansprechbarkeit für weitere Adressschemata wird ebenfalls implementiert (z.B. für JSON und den im Aufbau befindlichen PID-Service von CLARIN-D). Im bisherigen Verlauf des Projektes ist es gelungen, die Grundfunktionen des Tools zu implementieren und es in die Lage zu versetzen eine beliebig große Anzahl an Texten miteinander zu vergleichen. Dabei sind drei unterschiedliche Ansichten entstanden, die es dem Benutzer ermöglichen das Ergebnis aus verschiedenen Perspektiven zu betrachten. Die Detailansicht zeigt einen Text und markiert entsprechende Unterschiede zu anderen Texten. Die Parallelansicht (siehe auch Abbildung) zeigt alle Texte nebeneinander und markiert die Unterschiede farbig. Die Buchansicht schließlich zeigt wieder nur einen Text an und visualisiert die Varianten im Stile traditioneller Printeditionen unter dem betreffenden Abschnitt. Zu betonen ist dabei, dass der Ausgangstext für den Vergleich bei jeder Ansicht frei wählbar ist und sich somit nicht auf einen zu bevorzugenden Haupttext festgelegt bzw. eine Gewichtung der Textzeugen vorgenommen wird. Die Visualisierung und Ergebnissicherung ermöglicht zum einen, einen schnellen Überblick über die Text- und Editionsgeschichte verschiedener in digitalisierter Form vorliegender Werke zu erlangen. Darüber hinaus eignet sich das Tool als Hilfsmittel zum Kollationieren bei der Erstellung beliebiger kritischer, historischer bzw. genetischer Editionen. Weitere Funktionen, die das Spektrum von eComparatio noch einmal entscheidend erweitern werden, sind in Entwicklung. So ist die Einbindung von hochauflösenden Images der Handschriften der betreffenden Editionen geplant, um auch diesen Abschnitt der Textgeschichte dem Nutzer zugänglich zu machen. Weiterhin ist ein weiteres Modul in Entwicklung, das für die Abfrage von online-Datenbanken die Anbindung an das Notationssystem CTS (Canocical Text Services) ermöglicht. Beide Erweiterungen des Tools werden in absehbarer Zeit implementiert werden. Nach seiner Fertigstellung soll das Tool als freier Webservice für Forschung und Lehre zur Verfügung gestellt werden. Davon können Handschriften-Digitalisierungsprojekte, Editionsprojekte sowie Projekte profitieren, die sich Spezialfragen einzelner Textpassagen widmen; es ist auch für Seminararbeiten, d.h. den Einsatz in der Lehre geeignet, da es sowohl von Nicht-Editionsphilologen als auch von Editionsphilologen eingesetzt werden kann. Es ist natürlich auch nicht an den Fachbereich der Alten Geschichte gebunden, sondern kann in verschiedenen Bereichen der Textwissenschaften, unabhängig von der Sprache, eingesetzt werden. In der Fachcommunity der E-Humanities im Speziellen kann das Tool darüber hinaus in einem Bereich angewandt werden, der in jüngerer Zeit vermehrt ins Zentrum der Aufmerksamkeit gerückt ist, nämlich bei der Qualitätssicherung der digitalen Datengrundlage an sich. Gerade im Falle der Altertumswissenschaften, in denen bereits früh umfangreiche, abgeschlossene Korpora (TLG, BTL u.a.) vorlagen, ist ein nächster Schritt ein Ausbau dieser Datengrundlagen in die Tiefe, d.h. hinsichtlich der zahlreichen verschiedenen Editionen und Textausgaben. Solche Varianten spielen in der herkömmlichen altertumswissenschaftlichen Diskussion oftmals eine zentrale Rolle bei der Erörterung fachwissenschaftlicher Fragestellungen; die Möglichkeit, solche Varianten im Falle auch großer Textmengen schnell zu überblicken, kann als eine wesentliche Grundlage dafür gesehen werden, auch auf ""klassischem"" Textmining basierende Untersuchungen mit einer besseren Datengrundlage zu versehen. Da es sich bei dem Tool in erster Linie um ein Mittel zur Visualisierung handelt, ist es in hohem Maße für die Präsentation in Form eines Posters geeignet. Geplant ist die Darstellung des gesamten Workflows anhand eines Beispiels, von der Eingabe unstrukturierter Textdokumente bis hin zu den drei oben genannten Visualisierungsformen. Abb. der Parallelansicht von eComparatio am Beispiel des Fragments B1 des Anaximander."
2015,DHd2015,2015_cls_metadata_extracted.csv,Automatische Erkennung von Figuren in deutschsprachigen Romanen,Fotis Jannidis (Universität Würzburg); Markus Krug (Universität Würzburg); Frank Puppe (Universität Würzburg); Isabella Reger (Universität Würzburg); Martin Töpfer (Universität Würzburg); Lukas Weimer (Universität Würzburg),"Netzwerkanalyse, Named Entity Recognition, Conditional Random Fields, maschinelles Lernen","Netzwerkanalyse, Named Entity Recognition, Conditional Random Fields, maschinelles Lernen","Eine wichtige Grundlage für die quantitative Analyse von Erzähltexten, etwa eine Netzwerkanalyse der Figurenkonstellation, ist die automatische Erkennung von Referenzen auf Figuren in Erzähltexten, ein Sonderfall des generischen NLP-Problems der Named Entity Recognition [Sharnagat 2014]. Mit dem Stanford Parser [Finkel 2005] unter Verwendung eines Modells für deutsche Sprache [Faruqui and Pado 2010] liegen inzwischen auch freie Werkzeuge für Texte in deutscher Sprache vor. Allerdings ist die Erkennungsrate des Modells, das an einem Korpus von Zeitungstexten trainiert wurde, für literarische Texte nur eingeschränkt brauchbar (Abb. 1). Eine Auswertung anhand unseres Testkorpus (265 000 Tokens) hat einen F1-Score von nur 31% ergeben, was vor allem am sehr niedrigen Recall lag. Dieser Befund deckt sich mit vergleichbaren Erfahrungen aus der Computerlinguistik: Viele NLP-Werkzeuge müssen erst für einen neuen Anwendungsbereich angepasst werden, um brauchbare Resultate zu erbringen. Im Fall des Romankorpus führt die Einbeziehung von Appellativen in die Named Entity-Definition und deren häufige Verwendung in Romantexten zu dem schlechten Ergebnis. Da die Figurenreferenzen allerdings für fast alle nachfolgenden Verarbeitungsschritte eine hohe Relevanz haben, sind wir nicht den Weg einer automatischen Domänenadaption [Qi Li 2012] gegangen, sondern haben ein umfangreiches Trainingskorpus aufgebaut, um auf diese Weise möglichst hohe Erkennungsraten zu erhalten. Im Folgenden berichten wir über unser Vorgehen, diese Aufgabe möglichst effizient zu gestalten. Zusammenfassend können wir feststellen, dass wir die Erstellung des notwendigen Trainingskorpus durch ein Werkzeug erheblich beschleunigen konnten, das den Annotatoren bereits gute Vorschläge machte. Außerdem konnten die Resultate des verwendeten Lernverfahrens dadurch deutlich verbessert werden, dass über die üblichen Standardfeatures hinaus word2vec-Informationen (s.u.) als Feature verwendet wurden. 2 Abb. 1: Ergebnisse des Stanford-Parsers mit deutschem Modell (Faruqui and Pado 2010) angewandt auf ein Zeitungskorpus (CoNLL 2003) und ein Korpus deutschsprachiger Romane. Material und Methoden Als annotierte Trainings- und Testdaten dienten das Zeitungskorpus der CoNLL 2003 [Sang 2003] (ca. 220 000 Tokens) und ein von uns aufbereitetes Romankorpus mit je 130 zusammenhängenden Sätzen aus 50 Romanen mit 140 000 Tokens für das erste Experiment, und 85 Romanen mit 265 000 Tokens für das zweite Experiment. Die Annotation geschah mittels einem eigens für diesen Zweck entwickelten Werkzeug, das über eine komfortable grafische Benutzeroberfläche dem Annotator die mit einfachen Regeln ermittelten Vorschläge zur Bearbeitung anbietet, wodurch sich die Annotation erheblich beschleunigen ließ (die vorher direkt in XML-Dateien und dann in einem Annotationswerkzeug durchgeführt wurde, das nicht spezifisch für die Aufgabe angepasst wurde). Notiert wurden folgende Eigenschaften: a) Handelt es sich um einen wirklichen Namen, z.B. ""Effi Briest"", oder um einen Appellativ, z.B. der ""Lehrer"". b) Handelt es sich um eine einzelne Person oder um eine Personengruppe bzw. um mehr als eine Person, z.B. die ""Gäste"". c) Koreferenz per Identität (ID), d.h. alle Referenzen auf die gleiche Figur erhalten die gleiche grafisch angezeigte ID. Für die Anwendung unüberwachter Lernverfahren verwendeten wir Texte aus der FAZ (ca. 15 Millionen Tokens) und unser Erweiterungskorpus deutschsprachiger Romane (ca. 60 Millionen Tokens), beide Textsammlungen nicht annotiert. In der ersten Serie von Experimenten wurde die Frage untersucht, mit welchen Features das maschinelle Lernverfahren Conditional Random Fields (CRF), das auch im Stanford Parser eingesetzt wird, die besten Ergebnisse erbringt. Folgende sechs Features, die vom StanfordTagger [Finkel 2005] verwendet werden, wurden als Basis betrachtet: 3 1) 2) 3) 4) 5) 6) Current Word: das Wort an Position i Previous Word: das Wort an Position i-1 Next Word: das Wort an Position i+1 Word Shape: für Groß/Kleinschreibung oder Zahlen Part-Of-Speech Tags (POS-Tags) an den Positionen i, i-1 und i+1, die mit Hilfe des TreeTaggers [Schmid 1995] bestimmt wurden. Präfix bzw. Suffix, das aus den ersten oder letzten 2 Zeichen besteht. Außerdem getestete Features: 7) Gazeteers: Listen bestehend aus rd. 5200 männlichen, 3400 weiblichen Vornamen, 160 Adelstiteln, Anreden und 8700 Berufen. 8) Semantische Felder, je nach Wortart 15-23, auf der Grundlage von GermaNet 9) Satzsubjekt ermittelt mit dem Mate-Dependecy Parsers [Bohnet 2010]. 10) Compound-Words: alle von SFST [Fitschen 2004] erkannten Teilworte des Eingabewortes inkl. Prä- und Suffixe. 11) Head-Lemma: Grundform des zum Subjekt gehörenden Verbes. 12) LDA-Cluster: Es wird die Zugehörigkeit aller Nicht-Stop-Wörter zu dem wahrscheinlichsten von 250 Clustern mit der Latent-Dirichlet-Allocation (LDA) [Blei 2003] in Anlehnung an [Chrupala 2011] auf der Basis der oben erwähnten nicht annotierten Korpora mit 15 Millionen bzw. 60 Millionen Token ermittelt. Das LDA wurde mit dem Framework MALLET [MALLET 2002] implementiert. 13) Word2Vec-Cluster: Es wird ebenfalls die Zugehörigkeit aller Nicht-Stop-Wörter zu einem semantischen Cluster ermittelt. Dabei wurde eine effiziente Implementierung des ""Continuous Bag-of-Words"" Modells nach [Mikolov 2013] genutzt und die resultierenden Vektoren mit einem k-means Verfahren geclustert. Ergebnisse Zum Testen der gelernten CRFs wurde eine 10-fache Kreuzvalidierung auf der Trainingsmenge des Romankorpus (120.000 Tokens) durchgeführt. Die Baseline mit den Features 1-6 erbrachte einen F1-Score von 86,66%. Die Kombination der besten Features (letzte Zeile) erzielte einen F1-Score von 89,98, d.h. eine Steigerung um 3,32 Prozentpunkte. Der mit Abstand größte Anteil an dieser Steigerung ging auf das semantische Feature ""Word2Vec-Cluster"" zurück. Dagegen erbrachte das semantische Clustering mit LDAs einen eher negativen Effekt. In [Tkachenko 2012] wird der gleiche Effekt berichtet und die Vermutung geäußert, dass die LDA-Cluster redundant zu den POS-Tagging-Features sind. Beim Trainingskorpus mit den Zeitungsartikeln war die Baseline mit 87,9% etwas besser, aber die Steigerung durch Hinzunahme des Word2Vec-Cluster mit 1,6 Prozentpunkten (auf 89,5%) etwas schlechter. 4 Verfahren Precision in % Recall in % F1-Score in % Baseline (Features 195.12 79.60 86.66 Unterschied zur Baseline (F1-Score) in % +0 6) Baseline + (Feature 95.73 79.28 86.70 +0.04 7) Baseline +(8) 94.53 81.74 87.65 Baseline + (9) 94.96 79.74 86.67 Baseline + (10) 95.07 81.00 87.45 Baseline + (11) 95.03 79.63 86.63 Baseline + (12) 96.47 77.83 86.13 Baseline + (13) 94.97 85.28 89.84 Baseline + (7),(8),(10),(13) 94.86 85.60 89.98 +0.99 +0.01 +0.79 -0.03 -0.53 +3.18 +3.32 Tab. 1. Einfluss verschiedener Features auf die NER mit CRFs; Trainingsset ca. 120 000 Tokens. Wir haben beim Feature 13 ""Word2Vec-Cluster"" untersucht, welchen Einfluss die Anzahl der vorgegeben Cluster im k-means Verfahren zwischen 100 und 1000 auf die Qualität der NER hat. Dabei stellte sich heraus, dass bei einer Clusteranzahl ab 250 (relativ konstant bis 1000) das beste Ergebnis erzielt wird, so dass in weiteren Experimenten die Clusteranzahl von 250 gewählt wurde. In unserem zweiten Experiment beschäftigten wir uns mit den Fragen, wie groß unser annotiertes Korpus für das Training eines praktisch nutzbaren NER-Modells sein muss, bzw. ab welcher Größe eine Erweiterung des Trainingsmaterials keine nennenswerte Verbesserung der Erkennungsleistung mehr bringt. Als zweiten Aspekt gilt es das für unseren Task beste Lernverfahren zu ermitteln. Für diesen Zweck haben wir die Erkennungsgenauigkeit mit immer größeren Mengen von Trainingsdaten gemessen: Für beide Domänen wurde zunächst nur eine Trainingsmenge von 30 000 Tokens genutzt, die dann in Schritten von 10 000 Tokens auf die Maximalzahl von 230 000 Tokens bei den Romanen bzw. 170 000 Tokens bei den Zeitungsartikeln gesteigert wurde. Als Features haben wir die jeweils beste FeatureMenge für das CRF verwendet. Neben dem CRF-Klassifikator wurden auch MaximumEntropy, Naive Bayes und Decision-Trees mit der gleichen Menge an Features getestet. Abb. 2 zeigt, dass die beiden besten, von uns getesteten Klassifikationsverfahren MaxEnt, sowie CRFs sind. Auf dem Zeitungskorpus sind CRFs ca. 3-5% besser als MaxEnt, die Evaluation auf dem Romankorpus zeigt genau entgegengesetzte Ergebnisse. Eine Ausnutzung der Zustandsübergangsinformation, die CRFs zusätzlich zu MaxEnt nutzen, scheint im Fall der Romane keine nützlichen Informationen zu liefern, sondern das Ergebnis zu verschlechtern. Dies könnte in einer deutlich höheren durchschnittlichen Satzlänge (24,2 Tokens vs. 16,3 Tokens) in unserer Domäne begründet liegen. Ab einer Trainingsmenge von etwa 150 000 5 Tokens zeigt sich keine signifikante Verbesserung der Ergebnisse mehr. Wenn statt dieser 10Fold Cross-Validation eine Leave-One-Out-Evaluation verwendet wird, bei der der zu testende Roman nicht in der Trainingsmenge enthalten ist, verringert sich der durchschnittliche F1-Score um ca. fünf Prozentpunkte von 88% auf 83.4%. Entgegen unserer Erwartung führte die Hinzunahme von 35 Romanen in dem Trainingskorpus zu keiner Verbesserung der Erkennungsrate, sondern sogar zu einer Verschlechterung um ca. 2%. Eine genauere Analyse zeigte, dass unter diesen zufällig ausgewählten Romanen auch solche mit Dialekten und anderen Besonderheiten waren, was die Verschlechterung erklären könnte.1 Abb. 2. Einfluss verschiedener Größen von Trainingsdaten von 30 000 bis 230 000 bzw. 170 000 Tokens auf den F1-Wert der NER mit CRFs in zwei verschiedenen Domänen (Romane und Zeitungsartikel) und verschiedenen maschinellen Lernverfahren Ausblick Es gibt eine Reihe von weiteren Optimierungsverfahren, die im Anschluss an die berichteten Experimente exploriert werden sollen. Wir haben bisher nur Lernverfahren für die NER in Romanen auf der Basis annotierter Textkorpora untersucht. Wir versprechen uns sowohl beim Erstellen eines Goldstandards, als auch bei dem erzielbaren F1-Wert der NER Verbesserungen durch die Integration von komplexeren regelbasierten Verfahren [Klügl et al. 2014] zur Information Extraction. Außerdem soll der Vermutung nachgegangen werden, dass die Erkennungsleistung durch Verwendung von Strategien der Domänenanpassung noch verbessert werden kann, wenn diese auf das vorhandene umfangreiche Korpus mit nichtannotierten Daten angewandt werden [Qi Li 2012]. Außerdem sollen Alternativen zum 1 Unsere Implementierung des MaxEnt-Modells ist unter https://github.com/MarkusKrug/NERDetection/ zu finden. Sie ist so aufbereitet, dass sie mit dem DkPro-Framework kompatibel ist. Die Eingliederung dort soll demnächst folgen. 6 word2vec-Feature erprobt werden, die in NLP-Tasks gleichwertige Ergebnisse erbracht haben [Pennington 2014]. Literatur Blei, D., Ng, A. and Jordan, M. (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research. 3, 993‚Äì1022. Bohnet, B. (2010). Very High Accuracy and Fast Dependency Parsing is not a Contradiction. The 23rd Int. Conference on Computational Linguistics (COLING 2010), Beijing, China. Chrupala, G. (2011). Efficient induction of probabilistic word classes with LDA. Proceedings of 5th International Joint Conference on Natural Language Processing, 363-372. Faruqui, M. and Pado. S. (2010) Training and Evaluating a German Named Entity Recognizer with Semantic Generalization. Proceedings of Konvens 2010, Saarbrücken, Germany. Finkel, F., Grenager, T. and Manning, C. (2005). Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370. http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf Fitschen, A., Schmid, H. and Heid, U. (2004) SMOR: A German computational morphology covering derivation, composition, and inflection. Proceedings of the IVth International Conference on Language Resources and Evaluation (LREC 2004), 1263‚Äì1266. Klügl, P., Toepfer, M., Beck, P.D., Fette, G., Puppe, F. (2014) UIMA Ruta: Rapid development of rule-based information extraction applications. Natural Language Engineering First View, 1‚Äì40 (2014). DOI 10.1017/S1351324914000114. McCallum, A. MALLET: A Machine Learning for Language Toolkit. 2002. Mikolov, T., Chen, K., Corrado, G. and Dean, J. 2013. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013. Nadeau, D. and Sekine, S. (2007) A survey of named entity recognition and classification. Lingvisticae Investigationes 30 (1), 3-26 Pennington, J, Socher, R. and Manning, C. (2014) Glove: Global Vectors for Word Representation. Conference on Empirical Methods in Natural Language Processing (EMNLP 2014). Qi Li (2012): Literature Survey. Domain Adaption Algorithms for Natural Language Processing. nlp.cs.rpi.edu/paper/qisurvey.pdf Sang E. and Meulder, F. (2003) Introduction to the CoNLL-2003 shared task: languageindependent named entity recognition. Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 (4), 142-147. Schmid, H. (1995) Improvements in Part-of-Speech Tagging with an Application to German. Proceedings of the ACL SIGDAT-Workshop. Dublin, Ireland. Sharnagat, R. (2014) Named Entity Recognition: A Literature Survey. Surveys of the Center for Indian Language Technology. http://www.cfilt.iitb.ac.in/resources/surveys/rahul-ner-survey.pdf Tkachenko, M. and Simanovsky, A. (2012) Named entity recognition: Exploring features. Proceedings of KONVENS 2012, 118-127."
2015,DHd2015,2015_cls_metadata_extracted.csv,Computerlinguistische Verfahren zur Aufdeckung struktureller Ähnlichkeiten in Narrativen,"Nils Reiter (Universität Stuttgart, Deutschland); Anette Frank (Universität Heidelberg, Deutschland)","Sequence Alignment, Domain Adaptation, connectivity score, Graph-_based predicate alignment  ‚àöä","Sequence Alignment, Domain Adaptation, connectivity score, Graph-_based predicate alignment  ‚àöä","Einführung In diesem Beitrag stellen wir eine Methode zur automatischen  Erkennung von strukturellen Öhnlichkeiten narrativer Texte auf der Handlungsebene vor. Dafür operationalisieren  wir  strukturelle Öhnlichkeiten als (intertextuelle)  Verbindungen (Alignments) zwischen Ereignissen. Die  verwendeten  Alignierungsalgorithmen bauen auf automatisch erzeugten  linguistischen Analysen der Texte auf und verwenden  als Kriterien Eigenschaften verschiedener  linguistischer  Ebenen. Ziel unseres Ansatzes ist es, materiell in Texten vorliegende Öhnlichkeiten  auffindbar zu machen und hervorzuheben, so dass sie  von  Wissenschaftlerinnen und Wissenschaftlern zielgerichtet  analysiert und interpretiert werden können. Anwendungsszenarien Die Untersuchung  struktureller  Öhnlichkeiten zwischen Narrativen spielt  in  vielen geisteswissenschaftlichen Disziplinen eine Rolle. Als Beispielszenarien  verwenden wir  die Märchen-__  und Ritualforschung. Öhnlichkeiten zwischen Märchen  sind auf verschiedenen  Granularitätsebenen untersucht worden. Propp (1958)  veröffentlichte eine Analyse, in der in russischen Märchen  prototypische Handlungen und Charaktere identifiziert  werden. Regelmäßigkeiten im  Auftreten  von Handlungen  und Charakteren werden in einer sog. ""Morphology of the  Folktale"" erfasst. Damit sollen typische Handlungsmuster (Ereignis X folgt auf Ereignis Y) beschrieben werden. Am  anderen Ende der Granularitätsskala existieren Sammlungen  wie der ATU-__Index (Uther, 2014), in dem Märchen mit gleichen  Handlungselementen (Aussetzen von Kindern) oder Charakteren  (Lebkuchenhaus) in Klassen zusammengefasst werden. Im Bereich der Ritualforschung  werden Rituale aus  diversen religiösen, kulturellen oder politischen Kontexten  untersucht. Unter dem Stichwort ""Ritualgrammatik"" (vgl. Hellwig und Michaels, 2013) wird  diskutiert, dass in verschiedenen Ritualen ähnliche Handlungen vorkommen und Teilnehmer ähnliche  Rollen übernehmen. Verschiedene Forscher vertreten die Auffassung, dass die  Zusammensetzung wiederkehrender Ereignisse zu Ritualen  Regeln folgt. Existierende Überlegungen zur Ritualgrammatik  sind nicht formalisiert  und daher für eine automatische Analyse nur begrenzt  nutzbar. Um unsere Methode entwickeln und testen zu können, haben wir für diese beiden Szenarien ein  englischsprachiges Korpus zusammengestellt, das  mehrere Beschreibungen des gleichen Typs enthält (ATU-__Märchenklasse bzw. Ritualtyp). Computerlinguistische Verarbeitung Wir wenden die gleichen computerlinguistischen Komponenten  auf beide Korpora an. Damit werden  linguistische  Repräsentationen  für Wortarten, (syntaktische) Dependenzrelationen, semantische Rollen, Wortbedeutungen und  Koreferenzketten erstellt. Verknüpft  ergeben  diese Annotationen eine Diskursrepräsentation,  die als Basis für die Alignierungsverfahren verwendet wird.  Da Ritualbeschreibungen untypische linguistische  Phänomene enthalten, wurden  sämtliche Komponenten auf die Domäne angepasst (Domain  Adaptation). Dadurch konnten deutliche Qualitätssteigerungen der computerlinguistischen Analyse erreicht  werden. Alignierungsexperimente Drei Alignierungsalgorithmen mit unterschiedlicher  Mächtigkeit wurden verglichen: Sequence  alignment  (Needleman-__Wunsch, 1970) ist der einfachste Algorithmus, der ausschließlich paarweise und nicht-__kreuzende  Alignierungen erzeugen kann. Graph-__ based predicate alignment  (GPA; Roth, 2014, Roth & Frank, 2012) kann paarweise und kreuzende Alignierungen erzeugen.  Bayesian model merging  (BMM; Stolcke & Omohundro,  1993) ist der mächtigste Algorithmus, der Alignierungen  beliebiger Länge mit Überkreuzungen  erzeugen kann.  Diese drei Algorithmen wurden in zwei Experimenten evaluiert: In einer intrinsischen Evaluation wurden die Ergebnisse mit einem von zwei Ritualwissenschaftlern parallel erzeugten Goldstandard verglichen (_=0.61). Dabei erzielte BMM die besten Ergebnisse insgesamt und GPA die besten Ergebnisse auf einem Einzeldokumentpaar. Im zweiten Experiment wurde aus den automatisch erzeugten  Alignierungen ein Maß für Dokumentenähnlichkeit berechnet  und in einem Clustering-__Verfahren eingesetzt. Das Ergebnis  des Clusterings ' eine Einteilung der Dokumente auf Basis der errechneten strukturellen Öhnlichkeit ' konnte  dann mit der Gruppierung verglichen werden, die  ""natürlicherweise""  in den Korpora vorkommt (Ritualtypen bzw.  ATU-__Klassen). Dabei zeigten sich wieder GPA und BMM als die leistungsstärksten Algorithmen. Visualisierung und Nutzung Um es Wissenschaftlerinnen und Wissenschaftlern aus der  Ritual-__  bzw. Märchenforschung zu ermöglichen die Analysen zu  nutzen, haben wir Visualisierungen entwickelt, die eine systematische Untersuchung der gefundenen Öhnlichkeiten ermöglichen.  Auf  einer Vogelperspektive stellen wir die Dokumentenähnlichkeit in  einer Heatmap dar.  Auf interessante, dicht verknüpfte Stellen  können wir hinweisen, indem  für jedes  Ereignis ein  connectivity score  in einem Diagramm anzeigt  wird.  Eine detaillierte Darstellung der Einzelereignisse (mit  Teilnehmern und Kontext-__Ereignissen)  ist ebenfalls  möglich.  Direkt aus der Diskursrepräsentation können wir außerdem  eine Visualisierung des sozialen Netzwerks erzeugen, in der wichtige  Entitäten (Charaktere, Gegenstände  undD Materialien) in einem Netzwerk angezeigt und gemeinsam auftretende Figuren verknüpft werden. Konklusion Der Posterbeitrag präsentiert eine Methode  zur Erkennung struktureller Öhnlichkeiten  zwischen narrativen Texten. Die  Öhnlichkeiten  werden basierend auf computerlinguistischen  Analysen  vollautomatisch identifiziert  und können zielgerichtet  auf unterschiedlichen Granularitätsebenen  dargestellt und manuell inspiziert werden. Damit eignet sich die Methode  auch zur Analyse von größeren Datenmengen, ohne bestimmte  Interpretationen vorwegzunehmen.  Eine ausführliche Darstellung des Verfahrens sowie des  geisteswissenschaftlichen Anwendungskontexts  findet sich in Reiter  (2014) und Reiter et al. (2014). Auf einer methodischen Ebene zeigt sich in diesem Projekt,  dass komplexe linguistische Analysen auch für nicht-__kanonische Textsorten erstellt werden können und eine  vielversprechende Ausgangsbasis für Analysen darstellen. Die Besonderheiten natürlicher Sprache (z.B. Ambiguität,  Vielseitigkeit) stellen für automatische Verarbeitung eine große Herausforderung  dar, werden aber in der Computerlinguistik bereits untersucht. Auf (computer-__)linguistische  Analysen aufzubauen erlaubt die Untersuchung komplexer  semantischer Phänomene, die vergleichsweise eng mit den  Zielkategorien vieler Geisteswissenschaften verwandt sind. Bibliographie Oliver Hellwig and Axel Michaels. Ritualgrammatik. In Christiane Brosius, Axel Michaels, and Paula Schrode, Hrsg., Ritual und Ritualdynamik, S.  144–150. Vandenhoeck & Ruprecht, Göttingen, Germany, 2013. Saul B. Needleman and Christian D. Wunsch. A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of Molecular Biology, 48(3):443–453, March 1970. Vladimir Yakovlevich Propp. Morphology of the Folktale. University of Texas Press, Austin, TX, 2nd edition, 1958.  Translated by Laurence Scott (Original work published 1928). Nils Reiter. Discovering Structural Similarities in Narrative Texts using Event Alignment Algorithms. PhD thesis, Heidelberg University, June 2014. Nils Reiter, Anette Frank, and Oliver Hellwig. An NLP-__based cross-__document approach to narrative structure discovery. Literary and Linguistic Computing, 29(4):583–605, 2014. Michael Roth. Inducing Implicit Arguments via Cross-__document Alignment ' A Framework and its Applications. PhD thesis, Heidelberg University, 2014. Michael Roth and Anette Frank. Aligning predicates across monolingual comparable texts using graph-__based clustering. In Jun""ichi Tsujii, James Henderson, and Marius Pa_ca, editors, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 171–182, Jeju Island, Korea, July 2012. Andreas Stolcke and Stephen Omohundro. Hidden markov model induction by bayesian model merging. In Steve J. Hanson, J. D. Jack D. Cowan, and C. Lee Giles, Hrsg., Advances in Neural Information Processing Systems, volume 5, pages 11–18. Morgan Kaufmann, San Mateo, California, 1993. Hans-__Jörg Uther. The Types of International Folktales: A Classification and Bibliography. Based on the system of Antti Aarne and Stith Thompson. Number 284'286 in FF Communications. Suomalainen Tiedeakatemia, Helsinki, 2004."
2015,DHd2015,2015_cls_metadata_extracted.csv,Theorie und Praxis der erklärenden Annotation im Kontext der Digital Humanities,"Angelika Zirker (Eberhard Karls Universität Tübingen, Deutschland); Fabian Schwabe (Eberhard Karls Universität Tübingen, Deutschland); Matthias Bauer (Eberhard Karls Universität Tübingen, Deutschland)",Annotation,Annotation,"Einführung In diesem Beitrag werden zwei Referenzkorpora der deutschen Sprache verwendet, um daraus frequenznormierte Verlaufskurven für Wörter und Wortverbindungen zu berechnen: das DWDS_Kernkorpus des 20. Jhs. sowie das Referenzkorpus des Deutschen Textarchivs (1600–1900). Da beide Korpora bezüglich ihrer Metadaten vereinheitlicht und auch mit denselben linguistischen Informationen annotiert wurden, können korpusübergreifende Abfragen gestellt werden. Beispiele hierfür sind schreibweisentolerante Lemmasuchen oder textsortenspezifische Suchen. Die auf dieser Grundlage generierten Verlaufskurven stehen auf der Website des Deutschen Textarchivs für die Abfrage zur Verfügung. 1 Korpusgrundlage und Annotation Die Grundlage für die zeitlichen Verlaufskurven (Histogramme) bilden die 100 Millionen Textwörter des DWDS_Kernkorpus des 20. Jhs. sowie weitere 140 Millionen Textwörter des Deutschen Textarchivs, welches Werke des 17. bis 19. Jh. als Erstausgaben umfasst. Beide Korpora sind hinsichtlich der repräsentierten Textsorten aus Belletristik, Gebrauchsliteratur, Wissenschaft und (im DWDS:) Journalistischer Prosa sowie hinsichtlich der enthaltenen Disziplinen ausgewogen (Geyken 2007, 2013; Geyken et al. 2011). Sie wurden beide gemäß den TEI/P5_Richtlinien annotiert (Geyken et al. 2012; Haaf et al., forthcoming) und sind bezüglich der Metadaten untereinander interoperabel, insbesondere bezüglich der in den Histogrammen verwendeten Angaben zum Datum und zu den Textsorten. Beide Korpora wurden für die Auswertung linguistisch annotiert, insbesondere wurden alle Texte tokenisiert, lemmatisiert, nach lexikalischen Kategorien analysiert (PoS_Tagging) und mit GermaNet_Kategorien versehen. Von besonderer Bedeutung für die Generierung der Histogramme ist die CAB_Analyse der historischen Texte: Mit CAB werden historische Varianten einer Wortform auf die wahrscheinlichste Normalform reduziert und ihrem neuhochdeutschen Lemma zugeordnet (Jurish 2013). Damit ist nicht nur die Suche in den Korpora, sondern auch die Darstellung der Wortverläufe schreibweisenübergreifend möglich. Dies geschieht, indem das Suchwort extensional zu all denjenigen Wortformen expandiert wird, die eine (möglicherweise flektierte) Variante des Suchwortes darstellen. In den folgenden Abschnitten wird gezeigt, dass diese Vorgenerierung der möglichen Formen für den Benutzer eine effektive Hilfe darstellt (s. Abschnitt 3: Beispiele). Beide Korpora, das DWDS_Kernkorpus und das DTA_Korpus, sind mit der Suchmaschine DDC (Dialing DWDS Concordancer) indiziert (Jurish et al. 2014). DDC verfügt über reichhaltige Metadatenfilter sowie die Möglichkeit, mehrere Annotationen auf einer Wortposition zu indizieren und abfragbar zu machen. Darüber hinaus können reguläre Ausdrücke, Boolesche Verknüpfungen und Abstandsoperatoren genutzt werden. Insbesondere ist es möglich, Metadatenfilter und mehrfache linguistische Annotationen miteinander zu verbinden. Die Indizes von DDC wurden so optimiert, dass Abfragen über die zeitliche Verteilung (Histogramme) ausreichend schnell für eine dynamische Berechnung zur Laufzeit sind. Dadurch ist es auch möglich, zeitliche Verläufe für die gesamte Mächtigkeit der DDC_Abfragesprache dynamisch zu berechnen. 2 Visualisierung Grundlage der Visualisierung sind die nach Messpunkten (Jahreszahlen bzw. Datumsintervallen) normierten relativen Häufigkeiten pro Million Textwörter. Da die beiden Korpora weder gleich verteilt noch gleich groß sind und zudem die Wortanzahl je Zeitintervall variiert, ist die Normierung notwendig, um die Histogramme beider Korpora einheitlich zu präsentieren. Wie im vorigen Abschnitt erwähnt, können normierte relative Frequenzen nicht nur für einzelne Wortformen oder Lemmata, sondern auch für Kombinationen aus linguistischen Annotationen und Phrasensuchen ausgegeben werden. Darüber hinaus können Histogramme textsortenspezifisch (z. B. nur für Belletristik oder nur für Wissenschaft) oder textsortenübergreifend gebildet werden. Da die Häufigkeiten von Wortformen (Types) in Textkorpora zipfsch verteilt sind, ist aufgrund der Größe der beiden Referenzkorpora bereits bei Wortformen einer mittleren Häufigkeit damit zu rechnen, dass die Histogramme bei einzelnen Messpunkten Nullstellen aufweisen können. Umgekehrt kann auch die Unausgewogenheit der Textkorpora an einem Messpunkt zu ""Ausschlägen"" mit zu hoher Frequenz führen. Aus diesem Grund wurden in die Visualisierungskomponente verschiedene Parameter zur Glättung implementiert. Aus Platzgründen soll hier nur auf die beiden wichtigsten eingegangen werden: die Parameter ""window"" und ""pruning"" . Der Parameter ""window"" gibt die Fensterbreite (als natürliche Zahl) für die Glättung nach dem gleitenden Mittelwert an. Der Parameter ""pruning"" implementiert ein zweistufiges Verfahren. Im ersten Schritt wird eine Fehlerverteilung für die normierten Datenpunkte berechnet. Die beobachteten ""Fehler"" werden unter Annahme einer Normalverteilung in p_Werte überführt, und alle Datenpunkte mit p_Werten außerhalb des angegebenen Konfidenzbereichs (p=0.05) werden als Ausreißer behandelt. Datenpunkte, die Ausreissern entsprechen, werden durch eine lineare Interpolation der nächstliegenden Datenpunkte, die innerhalb des Konfidenzbereichs liegen, ersetzt. Die Visualisierung selbst erfolgt mittels der Javascript_Bibliothek Highcharts 1 Website des DTA abfragbar. 2 und ist auf der 3 Beispiele, Ergebnisse und Diskussion Im folgenden, abschließenden Abschnitt sollen einige Vorzüge der Visualisierung auf der Grundlage der oben beschriebenen Referenzkopora und ihrer linguistischen Annotationen anhand konkreter Beispiele illustriert werden. Die Abfragemöglichkeiten und Ergebnisse werden mit den entsprechenden Ergebnissen im Google Ngram Viewer 3 in Beziehung gesetzt. Beispiel 1: Tatsache Als erstes Beispiel dient das Lemma ""Tatsache"" , ein Begriff, der laut dem Etymologischen Wörterbuch (Pfeifer) erst mit einer Publikation aus dem Jahr 1756 Eingang in die deutsche Sprache fand. 4 Die Wortverlaufskurve in Abb. _ 1 bestätigt den Beginn dieser ""Wortkarriere"" und zeichnet ihn von der Mitte des 18. Jhs. bis in das 20. Jh. nach. 5 Die Verlaufskurve im Google Ngram Viewer, Abb. _ 2, zeigt eine vergleichbare Tendenz. Der Vergleich beider Histogramme lässt dennoch Probleme bei der Arbeit mit dem Ngram Viewer sichtbar werden: Historische Schreibweisen wie ""Thatsache"" 'sowie ggf. etliche weitere möglich Varianten 6 und idealerweise auch alle möglichen Flexionsformen bzw. Expansionen 'müssen bei der Eingabe der Anfrage explizit ergänzt werden, um die historische Entwicklung des Begriffs zu visualisieren. Auf den Punkt Flexionsformen/Expansionen wird im dritten Beispiel (""billig"") noch näher eingegangen; hier sind zunächst nicht erklärbare Ausschläge der Kurve vor 1756 von Interesse, als Spalding das Wort in die deutsche Sprache brachte. Besonders deutlich zeigt sich darin ein Problem des Google_Books_Korpus. Eine Recherche in den Dokumenten daselbst zeigt, dass es sich bei sämtlichen früheren Treffern um falsch datierte Dokumente handelt, darunter so prominente Beispiele aus dem 19. Jh. wie Goethes Schriften zur Morphologie (II. Teil, datiert auf 1659) und die Fliegenden Blätter (in Google Books datiert auf 1692). Die Wortverlaufskurven werden hier also durch Metadatenfehler verfälscht. 1 http://www.deutschestextarchiv.de/search/plot. 2 http://www.highcharts.com/products/highcharts. 3 https://books.google.com/ngrams. 4 Vgl. z. B. das Etymologische Wörterbuch des Deutschen (nach W. Pfeifer), digitale Version via DWDS: ""[...] nachgebildet (1756) von dem Theologen Spalding für engl. matter of fact [...]. "" Siehe Bestätigung der natürlichen und geoffenbarten Religion, Leipzig, 1756, sowie Johann Joachim Spaldings Übersetzung von Joseph Butlers The analogy of religion, natural and revealed, to the constitution and course of nature, 1736. Die Texterfassung dieses Werks ist für das DTA in Arbeit; derzeit stammt der früheste Beleg im DTA_Korpus aus Münter 1772. 5 Aus Platzgründen kann hier auf den Abfall der Kurve ab Mitte des 20. Jh. nicht eingegangen werden. 6 Im DTA_Korpus sind für den in dieser Hinsicht einfach erscheinenden Begriff ""Tatsache"" immerhin 15 Expansionen belegt, vgl. http://kaskade.dwds.de/dstar/dta/lizard.perl?q=Tatsache. Beispiel 2: merkwürdig Das zweite Beispiel illustriert die Möglichkeiten, die eine Textsortendifferenzierung für die Interpretation der Histogramme bietet. Der Begriff ""merkwürdig"" wurde Pfeifers Etymologischem Wörterbuch zufolge ab dem 19. Jh. vorrangig in der Bedeutung ""seltsam, verwunderlich"" verwendet; bis dahin dominierte die seit dem 17. Jh. gebräuchliche Verwendung im Sinne von ""bemerkenswert, bedeutsam"" . Abb. _ 3 zeigt das nach Textsorten differenzierte Histogramm. Belegt ist die relativ hochfrequente Verwendung von ""merkwürdig"" in den Textsorten Wissenschaft und Gebrauchsliteratur bis über die Mitte des 19. Jhs. hinaus. Folgt man der These (wie die im DTA verfügbaren Belege bestätigen), dass die Verwendung in der Wissenschaft und der Gebrauchsliteratur vorrangig im Sinne von ""bemerkenswert, bedeutsam"" geschah, legt dies einen etwas länger andauernden Gebrauch des Wortes in dieser Bedeutung nahe, als bei Pfeifer angegeben. Auch hier zeigt sich der Vorteil gegenüber der Verlaufskurve des Google Ngram Viewers (Abb. _ 4), wo die Textsortendifferenzierung fehlt. 7 Beispiel 3: billig Das abschließende Beispiel (""billig"") illustriert zwei weitere Vorzüge der auf den Referenzkorpora beruhenden Histogramme gegenüber dem Google Ngram Viewer: die bessere Abdeckung der DTA_Korpora bezüglich des 17. Jhs. und die bessere Handhabung der großen graphematischen Varianz v. a. in diesen historischen Texten. Abb. _ 5 zeigt das Histogramm für ""billig"" aus DTA und DWDS; die Kurve veranschaulicht die breite Verwendung des Begriffs im 17. Jh., die dazugehörigen Belege zeigen das Bedeutungsspektrum zwischen ‚Äòangemessen, gerechtfertigt‚Äô und ‚Äòmäßig, wohlfeil, günstig‚Äô . Die Kurve aus dem Google Ngram Viewer, Abb. _ 6, zeigt dagegen keinen kontinuierlichen Verlauf im 17. Jh., was angesichts der u. a. im DTA_Korpus belegten Verbreitung des Begriffs die verhältnismäßig geringe Substanz des Google_Books_Korpus für diesen Zeitraum belegt. Daher erscheint auf dessen Grundlage quellenbasierte Forschung zumindest in diesem Zeitraum kaum möglich. Ein weiteres, bereits angesprochenes Problem, kommt erschwerend hinzu: Insbesondere bei Abfragen für Zeiträume vor 1700 liefert das Google_Books_Korpus aufgrund der sehr heterogenen Graphie schlichtweg keine befriedigende Anzahl von Belegen; hier liefert die CAB_Analyse der DTA_Korpora klare Vorteile. Allein für das in dieser Hinsicht relativ unproblematisch erscheinende Lemma ""billig"" sind im DTA_Korpus die in Abb. _ 7 gezeigten immerhin 67 Flexions_ und Expansionsformen belegt. Bei einer Abfrage via DTA/DWDS werden alle diese Formen berücksichtigt, während der Nutzer des Google Ngram Viewers sie manuell eingeben (und zu diesem Zweck selbstverständlich überhaupt erst einmal präsent haben) müsste. 4 Ausblick Wie im vorigen Abschnitt gezeigt werden konnte, liefert die Zeitverlaufskurve auf der Grundlage der beiden Referenzkorpora interessante Ergebnisse, die mit dem wesentlich 7 NB: Überraschend ist zudem, dass die Graphien ""merkwirdig"" und ""merckwirdig"" in dem der Kurve zugrundeliegenden Korpus German (das nicht identisch mit dem abfragbaren aktuellen GB_Korpus ist) nicht belegt sind. größeren Google_Books_Korpus entweder nicht oder nur mit großem Rechercheaufwand ermittelbar gewesen wären. Grund dafür sind die verlässlichen Metadaten, die Zuordnung nach Textsorten sowie die aufgrund der genauen Texterfassung möglichen Erschließungsmethoden (CAB_Software). Es ist damit zu rechnen, dass sich die Lage der auf Referenzkorpora basierten zeitlichen Verlaufskurven künftig weiter verbessern wird. Zum einen liegt dies daran, dass immer mehr historische Volltexte in hoher Qualität entstehen. Diese Bemühungen werden dadurch verstärkt, dass die DFG erst unlängst eine spezifische OCR_Förderlinie aufgelegt hat. Zum anderen hat das DTA für die sich dynamisch verändernden Korpusgrundlagen bereits die geeigneten technischen Lösungen: Das Korpus des DTA wird automatisch im Wochenrhythmus indiziert. Abbildungen 8 http://www.deutschestextarchiv.de/search/ddc/lemmata/?lemma=Tatsache&mode=extended;norm=date%2Bclass&smooth=spli ne&single=0&grand=1&slice=10&prune=1&window=3&wbase=0&logavg=0&logscale=0&xrange=1740%3A2000&totals=0 Abb. _ 1: DTA_DWDS_Histogramm ""Tatsache"" https://books.google.com/ngrams/graph?content=Thatsache%2CTatsache&year _ start=1600&year _ end=2000&corpus=20&smo othing=3&share=&direct url=t1%3B%2CThatsache%3B%2Cc0%3B.t1%3B%2CTatsache%3B%2Cc0 _ Abb. _ 2: Google_Ngram_Histogramm ""Tatsache,Thatsache"" 8 Zu den gewählten Parametern der Kurvengenerierung siehe die jeweils angegebene URL. http://www.deutschestextarchiv.de/search/ddc/lemmata/?lemma=merkw%C3%BCrdig&mode=extended;norm=date%2Bclass&s mooth=spline&single=0&grand=1&slice=10&prune=1&window=3&wbase=0&logavg=0&logscale=0&xrange=1600%3A2000&tot als=0 Abb. _ 3: DTA_DWDS_Histogramm ""merkwürdig"" https://books.google.com/ngrams/graph?content=merkw%C3%BCrdig%2Cmerckw%C3%BCrdig%2Cmerckwirdig%2Cmerkwirdi g&year _ start=1600&year _ end=2000&corpus=20&smoothing=3&share=&direct _ url=t1%3B%2Cmerkw%C3%BCrdig%3B%2Cc0 %3B.t1%3B%2Cmerckw%C3%BCrdig%3B%2Cc0 Abb. _ 4: Google_Ngram_Histogramm ""merkwürdig,merckwürdig,merckwirdig,merkwirdig"" http://www.deutschestextarchiv.de/search/ddc/lemmata/?lemma=billig&mode=extended;norm=date%2Bclass&smooth=spline&s ingle=0&grand=1&slice=10&prune=1&window=3&wbase=0&logavg=0&logscale=0&xrange=1600%3A2000&totals=0 Abb. _ 5: DTA_DWDS_Histogramm ""billig"" https://books.google.com/ngrams/graph?content=billich%2Cbillig&year _ start=1600&year _ end=2000&corpus=20&smoothing=3& share=&direct _ url=t1%3B%2Cbillich%3B%2Cc0%3B.t1%3B%2Cbillig%3B%2Cc0 Abb. _ 6: Google_Ngram_Histogramm ""billig"" http://kaskade.dwds.de/dstar/dta/dstar.perl?fmt=expand_html&q=billig&x=Token Abb. _ 7: im DTA belegte Expansions_ und Flexionsformen des Lemma ""billig"" Bibliographie Geyken 2007: Alexander Geyken: The DWDS corpus 'A reference corpus for the German language of the 20th century. In: Fellbaum, Christiane (Hg.): Idioms and Collocations: Corpus_based Linguistic, Lexicographic Studies. London: Continuum Press, 2007, S. 23–40. Geyken et al. 2011: Alexander Geyken, Susanne Haaf, Bryan Jurish, Matthias Schulz, Jakob Steinmann, Christian Thomas, Frank Wiegand: Das Deutsche Textarchiv: Vom historischen Korpus zum aktiven Archiv. In: Digitale Wissenschaft. Stand und Entwicklung digital vernetzter Forschung in Deutschland, 20./21. September 2010. Beiträge der Tagung. Hrsg. von Silke Schomburg, Claus Leggewie, Henning Lobin und Cornelius Puschmann. 2., ergänzte Fassung. hbz, 2011, S. 157–161. http://www.hbz_nrw.de/dokumentencenter/veroeffentlichungen/Tagung_ Digitale Wisse _ nschaft.pdf#page=159 Geyken et al. 2012: Alexander Geyken, Susanne Haaf, Frank Wiegand: The DTA ‚Äòbase format‚Äô: A TEI_Subset for the Compilation of Interoperable Corpora. In: 11th Conference on Natural Language Processing (KONVENS) 'Empirical Methods in Natural Language Processing, Proceedings of the Conference. Hrsg. von Jeremy Jancsary. Wien, 2012 (= Schriftenreihe der √ñsterreichischen Gesellschaft für Artificial Intelligence 5). http://www.oegai.at/konvens2012/proceedings/57 _geyken12w/57 _geyken12w.pdf Geyken 2013: Alexander Geyken: Wege zu einem historischen Referenzkorpus des Deutschen: das Projekt Deutsches Textarchiv. In: Perspektiven einer corpusbasierten historischen Linguistik und Philologie. Internationale Tagung des Akademienvorhabens ""Altägyptisches Wörterbuch"" an der Berlin_Brandenburgischen Akademie der Wissenschaften, 12. –13. Dezember 2011. Hrsg. von Ingelore Hafemann, Berlin 2013, S. 221–234. urn:nbn:de:kobv:b4_opus_24424 Haaf et al., forthcoming: Susanne Haaf, Alexander Geyken, Frank Wiegand: The DTA ‚ÄòBase Format‚Äô: A TEI Subset for the Compilation of a Large Reference Corpus of Printed Text from Multiple Sources. To appear in: Journal of the Text Encoding Initiative (jTEI), Issue 8. [Abstract des korrespondierenden Vortrags im Rahmen der Veranstaltung: TEI Conference and Members Meeting, 2. –5. Oktober 2013, Sapienza, Universit√† di Roma (IT): http://digilab2.let.uniroma1.it/teiconf2013/program/papers/abstracts_paper#C137] Jurish 2013: Bryan Jurish: Canonicalizing the Deutsches Textarchiv. In: Perspektiven einer corpusbasierten historischen Linguistik und Philologie. Internationale Tagung des Akademienvorhabens ""Altägyptisches Wörterbuch"" an der Berlin_Brandenburgischen Akademie der Wissenschaften, 12. –13. Dezember 2011. Hrsg. von Ingelore Hafemann, Berlin 2013, S. 235–244. urn:nbn:de:kobv:b4_opus_24433 Jurish et al. 2014: Bryan Jurish, Christian Thomas, Frank Wiegand: Querying the Deutsches Textarchiv. In: Proceedings of the Workshop MindTheGap 2014: Beyond Single_Shot Text Queries: Bridging the Gap(s) between Research Communities (co_located with iConference 2014, Berlin, Germany, 4th March, 2014). Hrsg. von Udo Kruschwitz, Frank Hopfgartner, Cathal Gurrin, Berlin 2014, S. 25–30. http://ceur_ws.org/Vol_1131/mindthegap14 _ 7.pdf Münter 1772: Balthasar Münter: Bekehrungsgeschichte des vormaligen Grafen [...] Johann Friederich Struensee. Kopenhagen, 1772. In: Deutsches Textarchiv, www.deutschestextarchiv.de/muenter bekehren _ _ 1772, abgerufen am 07.11.2014."
2015,DHd2015,2015_cls_metadata_extracted.csv,Zeitliche Verlaufskurven in den DTA- und DWDS-Korpora: Wörter und Wortverbindungen über 400 Jahre (1600‚-2000),Alexander Geyken (Berlin-Brandenburgische Akademie der Wissenschaften); Matthias Boenig (Berlin-Brandenburgische Akademie der Wissenschaften); Susanne Haaf (Berlin-Brandenburgische Akademie der Wissenschaften); Bryan Jurish (Berlin-Brandenburgische Akademie der Wissenschaften); Christian Thomas (Berlin-Brandenburgische Akademie der Wissenschaften); Kay-Michael Würzner (Berlin-Brandenburgische Akademie der Wissenschaften); Frank Wiegand (Berlin-Brandenburgische Akademie der Wissenschaften),"Korpora, Referenzkorpora, GermaNet","Korpora, Referenzkorpora, GermaNet","Einführung In diesem Beitrag werden zwei Referenzkorpora der deutschen Sprache verwendet, um daraus frequenznormierte Verlaufskurven für Wörter und Wortverbindungen zu berechnen: das DWDS_Kernkorpus des 20. Jhs. sowie das Referenzkorpus des Deutschen Textarchivs (1600–1900). Da beide Korpora bezüglich ihrer Metadaten vereinheitlicht und auch mit denselben linguistischen Informationen annotiert wurden, können korpusübergreifende Abfragen gestellt werden. Beispiele hierfür sind schreibweisentolerante Lemmasuchen oder textsortenspezifische Suchen. Die auf dieser Grundlage generierten Verlaufskurven stehen auf der Website des Deutschen Textarchivs für die Abfrage zur Verfügung. 1 Korpusgrundlage und Annotation Die Grundlage für die zeitlichen Verlaufskurven (Histogramme) bilden die 100 Millionen Textwörter des DWDS_Kernkorpus des 20. Jhs. sowie weitere 140 Millionen Textwörter des Deutschen Textarchivs, welches Werke des 17. bis 19. Jh. als Erstausgaben umfasst. Beide Korpora sind hinsichtlich der repräsentierten Textsorten aus Belletristik, Gebrauchsliteratur, Wissenschaft und (im DWDS:) Journalistischer Prosa sowie hinsichtlich der enthaltenen Disziplinen ausgewogen (Geyken 2007, 2013; Geyken et al. 2011). Sie wurden beide gemäß den TEI/P5_Richtlinien annotiert (Geyken et al. 2012; Haaf et al., forthcoming) und sind bezüglich der Metadaten untereinander interoperabel, insbesondere bezüglich der in den Histogrammen verwendeten Angaben zum Datum und zu den Textsorten. Beide Korpora wurden für die Auswertung linguistisch annotiert, insbesondere wurden alle Texte tokenisiert, lemmatisiert, nach lexikalischen Kategorien analysiert (PoS_Tagging) und mit GermaNet_Kategorien versehen. Von besonderer Bedeutung für die Generierung der Histogramme ist die CAB_Analyse der historischen Texte: Mit CAB werden historische Varianten einer Wortform auf die wahrscheinlichste Normalform reduziert und ihrem neuhochdeutschen Lemma zugeordnet (Jurish 2013). Damit ist nicht nur die Suche in den Korpora, sondern auch die Darstellung der Wortverläufe schreibweisenübergreifend möglich. Dies geschieht, indem das Suchwort extensional zu all denjenigen Wortformen expandiert wird, die eine (möglicherweise flektierte) Variante des Suchwortes darstellen. In den folgenden Abschnitten wird gezeigt, dass diese Vorgenerierung der möglichen Formen für den Benutzer eine effektive Hilfe darstellt (s. Abschnitt 3: Beispiele). Beide Korpora, das DWDS_Kernkorpus und das DTA_Korpus, sind mit der Suchmaschine DDC (Dialing DWDS Concordancer) indiziert (Jurish et al. 2014). DDC verfügt über reichhaltige Metadatenfilter sowie die Möglichkeit, mehrere Annotationen auf einer Wortposition zu indizieren und abfragbar zu machen. Darüber hinaus können reguläre Ausdrücke, Boolesche Verknüpfungen und Abstandsoperatoren genutzt werden. Insbesondere ist es möglich, Metadatenfilter und mehrfache linguistische Annotationen miteinander zu verbinden. Die Indizes von DDC wurden so optimiert, dass Abfragen über die zeitliche Verteilung (Histogramme) ausreichend schnell für eine dynamische Berechnung zur Laufzeit sind. Dadurch ist es auch möglich, zeitliche Verläufe für die gesamte Mächtigkeit der DDC_Abfragesprache dynamisch zu berechnen. 2 Visualisierung Grundlage der Visualisierung sind die nach Messpunkten (Jahreszahlen bzw. Datumsintervallen) normierten relativen Häufigkeiten pro Million Textwörter. Da die beiden Korpora weder gleich verteilt noch gleich groß sind und zudem die Wortanzahl je Zeitintervall variiert, ist die Normierung notwendig, um die Histogramme beider Korpora einheitlich zu präsentieren. Wie im vorigen Abschnitt erwähnt, können normierte relative Frequenzen nicht nur für einzelne Wortformen oder Lemmata, sondern auch für Kombinationen aus linguistischen Annotationen und Phrasensuchen ausgegeben werden. Darüber hinaus können Histogramme textsortenspezifisch (z. B. nur für Belletristik oder nur für Wissenschaft) oder textsortenübergreifend gebildet werden. Da die Häufigkeiten von Wortformen (Types) in Textkorpora zipfsch verteilt sind, ist aufgrund der Größe der beiden Referenzkorpora bereits bei Wortformen einer mittleren Häufigkeit damit zu rechnen, dass die Histogramme bei einzelnen Messpunkten Nullstellen aufweisen können. Umgekehrt kann auch die Unausgewogenheit der Textkorpora an einem Messpunkt zu ""Ausschlägen"" mit zu hoher Frequenz führen. Aus diesem Grund wurden in die Visualisierungskomponente verschiedene Parameter zur Glättung implementiert. Aus Platzgründen soll hier nur auf die beiden wichtigsten eingegangen werden: die Parameter ""window"" und ""pruning"" . Der Parameter ""window"" gibt die Fensterbreite (als natürliche Zahl) für die Glättung nach dem gleitenden Mittelwert an. Der Parameter ""pruning"" implementiert ein zweistufiges Verfahren. Im ersten Schritt wird eine Fehlerverteilung für die normierten Datenpunkte berechnet. Die beobachteten ""Fehler"" werden unter Annahme einer Normalverteilung in p_Werte überführt, und alle Datenpunkte mit p_Werten außerhalb des angegebenen Konfidenzbereichs (p=0.05) werden als Ausreißer behandelt. Datenpunkte, die Ausreissern entsprechen, werden durch eine lineare Interpolation der nächstliegenden Datenpunkte, die innerhalb des Konfidenzbereichs liegen, ersetzt. Die Visualisierung selbst erfolgt mittels der Javascript_Bibliothek Highcharts 1 Website des DTA abfragbar. 2 und ist auf der 3 Beispiele, Ergebnisse und Diskussion Im folgenden, abschließenden Abschnitt sollen einige Vorzüge der Visualisierung auf der Grundlage der oben beschriebenen Referenzkopora und ihrer linguistischen Annotationen anhand konkreter Beispiele illustriert werden. Die Abfragemöglichkeiten und Ergebnisse werden mit den entsprechenden Ergebnissen im Google Ngram Viewer 3 in Beziehung gesetzt. Beispiel 1: Tatsache Als erstes Beispiel dient das Lemma ""Tatsache"" , ein Begriff, der laut dem Etymologischen Wörterbuch (Pfeifer) erst mit einer Publikation aus dem Jahr 1756 Eingang in die deutsche Sprache fand. 4 Die Wortverlaufskurve in Abb. _ 1 bestätigt den Beginn dieser ""Wortkarriere"" und zeichnet ihn von der Mitte des 18. Jhs. bis in das 20. Jh. nach. 5 Die Verlaufskurve im Google Ngram Viewer, Abb. _ 2, zeigt eine vergleichbare Tendenz. Der Vergleich beider Histogramme lässt dennoch Probleme bei der Arbeit mit dem Ngram Viewer sichtbar werden: Historische Schreibweisen wie ""Thatsache"" 'sowie ggf. etliche weitere möglich Varianten 6 und idealerweise auch alle möglichen Flexionsformen bzw. Expansionen 'müssen bei der Eingabe der Anfrage explizit ergänzt werden, um die historische Entwicklung des Begriffs zu visualisieren. Auf den Punkt Flexionsformen/Expansionen wird im dritten Beispiel (""billig"") noch näher eingegangen; hier sind zunächst nicht erklärbare Ausschläge der Kurve vor 1756 von Interesse, als Spalding das Wort in die deutsche Sprache brachte. Besonders deutlich zeigt sich darin ein Problem des Google_Books_Korpus. Eine Recherche in den Dokumenten daselbst zeigt, dass es sich bei sämtlichen früheren Treffern um falsch datierte Dokumente handelt, darunter so prominente Beispiele aus dem 19. Jh. wie Goethes Schriften zur Morphologie (II. Teil, datiert auf 1659) und die Fliegenden Blätter (in Google Books datiert auf 1692). Die Wortverlaufskurven werden hier also durch Metadatenfehler verfälscht. 1 http://www.deutschestextarchiv.de/search/plot. 2 http://www.highcharts.com/products/highcharts. 3 https://books.google.com/ngrams. 4 Vgl. z. B. das Etymologische Wörterbuch des Deutschen (nach W. Pfeifer), digitale Version via DWDS: ""[...] nachgebildet (1756) von dem Theologen Spalding für engl. matter of fact [...]. "" Siehe Bestätigung der natürlichen und geoffenbarten Religion, Leipzig, 1756, sowie Johann Joachim Spaldings Übersetzung von Joseph Butlers The analogy of religion, natural and revealed, to the constitution and course of nature, 1736. Die Texterfassung dieses Werks ist für das DTA in Arbeit; derzeit stammt der früheste Beleg im DTA_Korpus aus Münter 1772. 5 Aus Platzgründen kann hier auf den Abfall der Kurve ab Mitte des 20. Jh. nicht eingegangen werden. 6 Im DTA_Korpus sind für den in dieser Hinsicht einfach erscheinenden Begriff ""Tatsache"" immerhin 15 Expansionen belegt, vgl. http://kaskade.dwds.de/dstar/dta/lizard.perl?q=Tatsache. Beispiel 2: merkwürdig Das zweite Beispiel illustriert die Möglichkeiten, die eine Textsortendifferenzierung für die Interpretation der Histogramme bietet. Der Begriff ""merkwürdig"" wurde Pfeifers Etymologischem Wörterbuch zufolge ab dem 19. Jh. vorrangig in der Bedeutung ""seltsam, verwunderlich"" verwendet; bis dahin dominierte die seit dem 17. Jh. gebräuchliche Verwendung im Sinne von ""bemerkenswert, bedeutsam"" . Abb. _ 3 zeigt das nach Textsorten differenzierte Histogramm. Belegt ist die relativ hochfrequente Verwendung von ""merkwürdig"" in den Textsorten Wissenschaft und Gebrauchsliteratur bis über die Mitte des 19. Jhs. hinaus. Folgt man der These (wie die im DTA verfügbaren Belege bestätigen), dass die Verwendung in der Wissenschaft und der Gebrauchsliteratur vorrangig im Sinne von ""bemerkenswert, bedeutsam"" geschah, legt dies einen etwas länger andauernden Gebrauch des Wortes in dieser Bedeutung nahe, als bei Pfeifer angegeben. Auch hier zeigt sich der Vorteil gegenüber der Verlaufskurve des Google Ngram Viewers (Abb. _ 4), wo die Textsortendifferenzierung fehlt. 7 Beispiel 3: billig Das abschließende Beispiel (""billig"") illustriert zwei weitere Vorzüge der auf den Referenzkorpora beruhenden Histogramme gegenüber dem Google Ngram Viewer: die bessere Abdeckung der DTA_Korpora bezüglich des 17. Jhs. und die bessere Handhabung der großen graphematischen Varianz v. a. in diesen historischen Texten. Abb. _ 5 zeigt das Histogramm für ""billig"" aus DTA und DWDS; die Kurve veranschaulicht die breite Verwendung des Begriffs im 17. Jh., die dazugehörigen Belege zeigen das Bedeutungsspektrum zwischen ‚Äòangemessen, gerechtfertigt‚Äô und ‚Äòmäßig, wohlfeil, günstig‚Äô . Die Kurve aus dem Google Ngram Viewer, Abb. _ 6, zeigt dagegen keinen kontinuierlichen Verlauf im 17. Jh., was angesichts der u. a. im DTA_Korpus belegten Verbreitung des Begriffs die verhältnismäßig geringe Substanz des Google_Books_Korpus für diesen Zeitraum belegt. Daher erscheint auf dessen Grundlage quellenbasierte Forschung zumindest in diesem Zeitraum kaum möglich. Ein weiteres, bereits angesprochenes Problem, kommt erschwerend hinzu: Insbesondere bei Abfragen für Zeiträume vor 1700 liefert das Google_Books_Korpus aufgrund der sehr heterogenen Graphie schlichtweg keine befriedigende Anzahl von Belegen; hier liefert die CAB_Analyse der DTA_Korpora klare Vorteile. Allein für das in dieser Hinsicht relativ unproblematisch erscheinende Lemma ""billig"" sind im DTA_Korpus die in Abb. _ 7 gezeigten immerhin 67 Flexions_ und Expansionsformen belegt. Bei einer Abfrage via DTA/DWDS werden alle diese Formen berücksichtigt, während der Nutzer des Google Ngram Viewers sie manuell eingeben (und zu diesem Zweck selbstverständlich überhaupt erst einmal präsent haben) müsste. 4 Ausblick Wie im vorigen Abschnitt gezeigt werden konnte, liefert die Zeitverlaufskurve auf der Grundlage der beiden Referenzkorpora interessante Ergebnisse, die mit dem wesentlich 7 NB: Überraschend ist zudem, dass die Graphien ""merkwirdig"" und ""merckwirdig"" in dem der Kurve zugrundeliegenden Korpus German (das nicht identisch mit dem abfragbaren aktuellen GB_Korpus ist) nicht belegt sind. größeren Google_Books_Korpus entweder nicht oder nur mit großem Rechercheaufwand ermittelbar gewesen wären. Grund dafür sind die verlässlichen Metadaten, die Zuordnung nach Textsorten sowie die aufgrund der genauen Texterfassung möglichen Erschließungsmethoden (CAB_Software). Es ist damit zu rechnen, dass sich die Lage der auf Referenzkorpora basierten zeitlichen Verlaufskurven künftig weiter verbessern wird. Zum einen liegt dies daran, dass immer mehr historische Volltexte in hoher Qualität entstehen. Diese Bemühungen werden dadurch verstärkt, dass die DFG erst unlängst eine spezifische OCR_Förderlinie aufgelegt hat. Zum anderen hat das DTA für die sich dynamisch verändernden Korpusgrundlagen bereits die geeigneten technischen Lösungen: Das Korpus des DTA wird automatisch im Wochenrhythmus indiziert. Abbildungen 8 DHd2025"
2015,DHd2015,2015_cls_metadata_extracted.csv,WebLicht: Bombardieren bevor die Services explodieren,"Danièl de Kok (Universität Tübingen, Deutschland); Wie Qiu (Universität Tübingen, Deutschland); Marie Hinrichs (Universität Tübingen, Deutschland)","Annotation, Web-Analyse Software","Annotation, Web-Analyse Software","Einleitung Web-Analyse Software, die Informationen über das Nutzerverhalten sammelt und auswertet, kann eingesetzt werden, um Probleme zu verhindern, bevor diese entstehen. In diesem Abstrakt zeigen wir, wie wir eine derartige Software auf ein System mit verteilten Webservices, WebLicht [1], angewandt haben und wie wir die entstandene Analyse benutzt haben, um das System zu verbessern. WebLicht ist eine Webanwendung zur automatischen Annotation von Texten und multimodalen Daten. WebLicht nutzt eine serviceorientierte Architektur. Dies ermöglicht den CLARIN-Zentren Annotationswerkzeuge hinzuzufügen durch diese als Webservice zu implementieren und die CMDI-Metadaten [2] über diesen Service an ihr Repositorium zuzufügen. WebLicht aggregiert diese Metadaten und bietet dem Nutzer die Annotationswerkzeuge an. Wenn ein Benutzer mehrere Annotationswerkzeuge ausführen will oder ein Annotationswerkzeug vom Output eines anderen Werkzeugs abhängig ist, erlaubt WebLichts Verkettungsmechanismus es den Benutzern, Annotationswerkzeuge auf eine kompatible Art und Weise zu kombinieren. WebLicht ist eine voll entwickelte Anwendung, die mehr als hundert Annotationswerkzeuge anbietet. Dadurch weitet sich die Nutzung von WebLicht in zwei Dimensionen aus: (1) Die Anzahl der WebLicht-Benutzer steigt; (2) durchschnittlich lassen die Benutzer größere Datasets annotieren. Um diese Wachstumsstrukturen zu verstehen, sammeln wir Nutzungsstatistiken. Um WebLicht und die Annotationswerkzeuge für unsere Nutzerbasis zu optimieren, müssen wir zuerst die Nutzungsmuster der derzeitigen Benutzer kennen. Zweitens müssen wir zukünftige Kapazitätsengpässe vorhersagen können, damit wir uns mit ihnen auseinandersetzen können, bevor sie die User Experience beeinträchtigen. In den folgenden Abschnitten werden wir zuerst beschreiben, wie wir Benutzeraktivität und Nutzungsmuster messen. Dann werden wir die Simulation verschiedener Nutzungsszenarien behandeln. Schließlich werden wir einen kurzen Überblick über die Veränderungen geben, die wir im vergangenen Jahr aufgrund der Messungen vorgenommen haben. Die Messung von Benutzeraktivität Das Sammeln von Nutzerstatistiken für Webseiten ist ein wohlverstandenes Feld, in dem es mehrere konkurrierende und voll entwickelte Produkte, wie etwa Google Analytics, gibt. Es sind auch viele gute Open Source Webanalyse-Tools, wie zum 1 2 3 Beispiel Piwik , Webalizer and AWStats verfügbar. Diese Tools funktionieren gewöhnlich auf eine von zwei Arten: (1) Sie analysieren die vom Webserver protokollierten Logdateien; oder (2) sie benötigen einen Maintainer, der auf jede Seite einen Javascript-Snippet einfügt, wodurch der Browser des Benutzers mit der Analysesoftware Kontakt aufnimmt. Bereits existierende Lösungen sind nicht direkt auf WebLicht anwendbar, da ihre Verwendung nur zeigen würde, wie oft, aus welchem Land, etc. WebLicht besucht wurde. Die nützlichste Information würden fehlen: welche Annotationswerkzeuge verwenden die Benutzer, und wie oft? Piwik bietet eine Programmierschnittstelle (Application Programming Interface) an, durch die jedes Annotationswerkzeug seine eigene Nutzung registrieren kann. Jedoch würde dies erfordern, dass knapp hundert Dienste upgedatet werden müssten, um die Programmierschnittstelle aufrufen zu können. Ein weiterer Nachteil dieser Lösung ist, dass interessante Informationen, wie etwa das Land des Benutzers, nicht verfügbar sind, da die Annotationswerkzeuge von WebLicht aufgerufen werden und nicht direkt vom Browser des Benutzers. Wir haben die oben genannten Probleme durch das Melden von Statistiken im WebLicht-Verkettungsmechanismus gelöst. Da der Verkettungsmechanismus jedes Annotationswerkzeug aufruft, kann er gleichzeitig die Verwendung des Werkzeugs über die Piwik-Programmierschnittstelle melden. Dies macht die Statistiken sofort erhältlich für alle Annotationswerkzeuge, die in WebLicht verfügbar sind, ohne auch nur eine von ihnen zu verändern. Da der Verkettungsmechanismus in der WebLicht-Anwendung ausgeführt wird, hat es den Zusatznutzen, dass wir einige Metadaten bereitstellen können, die es Piwik erlauben das Land des Besuchers, den Webbrowser, etc. zu ermitteln. 1 http://piwik.org/ 2 http://www.webalizer.org/ 3 http://www.awstats.org/ Abb. 1: Piwik Nutzerstatistiken für die Stuttgarter Werkzeuge Die Simulation von Nutzungsmustern Eines der Probleme, auf das wir mit der gestiegenen Nutzung von WebLicht gestoßen sind, ist, dass manche Annotationswerkzeuge nicht zur Bewältigung vieler Simultanbenutzer oder großen Inputs entwickelt wurden. Leider wurden diese Probleme oft erst entdeckt, wenn ein Benutzer eines der Annotationswerkzeuge nicht ausführen konnte. Durch das Simulieren von WebLicht-Nutzungsmustern konnten wir solche Probleme aufdecken, bevor die Benutzer ihnen begegnen. Zu diesem Zweck haben wir ein Simulationswerkzeug namens Bombard entwickelt. Bombard ermöglicht es den Entwicklern von WebLicht-Annotationswerkzeugen, Testfälle zu spezifizieren. Jeder Testfall besteht aus: der Kette von Annotationswerkzeugen, die getestet werden soll; dem Input; einem Intervall, das anzeigt, wie oft der Testfall ausgeführt werden soll; und die maximal erlaubte Bearbeitungsdauer. Eine Bombard-Konfigurierung kann aus vielen solcher Testfälle bestehen. Wenn Bombard gestartet wird, beginnt es jeden Testfall zu einem willkürlich gewählten Zeitpunkt und wiederholt den Testfall ab da in dem festgelegten Intervall. Während des ""Bombardements"" behält Bombard den Überblick über Fehlschläge und inakzeptable Bearbeitungsdauern. Danach können die Entwickler einen Bericht mit Statistiken für jedes Annotationswerkzeug abrufen. Im CLARIN-Zentrum in Tübingen wird Bombard zum Testen neuer Dienste genutzt, indem es das folgende Szenario simuliert: Zwei Gruppen von jeweils 40 Studierenden reichen innerhalb von zwei Minuten einen Text ein. In Gruppe A bearbeitet jeder Studierende zwei Textabschnitte aus Wikipedia, in Gruppe B bearbeitet jeder Studierende den Roman Alice im Wunderland (oder eine Übersetzung davon). Mithilfe dieses Testszenarios konnten wir Kapazitätsengpässe in früher entwickelten Annotationsdiensten bestimmen und sicherstellen, dass die neuen Dienste in der Lage sind, solche Szenarien zu verarbeiten. Da die Konfigurierung der Testfälle in Bombard anpassungsfähig ist, kann sie einfach auf andere Szenarien oder Annotationswerkzeuge, für die andere Erwartungen gelten, angewendet werden. Önderungen an WebLicht Wir haben unser Klassenzimmer-Szenario an Annotationsketten getestet, die unseren Feststellungen nach häufig genutzt werden, nämlich: Part-of-speech Tagging, Lemmatisierung, Konstituenz-Parsen, Dependenz-Parsen und Named-entity Recognition. Während der Simulation entdeckten wir die folgenden Probleme: (1) Manche Dienste versagten, wenn viele Instanzen des längeren Textes gesandt wurden; (2) manche versagten an dem längeren Text, da er relativ verrauscht ist; (3) bei manchen Diensten, insbesondere Parsern, kamen die Anfragen schneller herein als der Dienst sie verarbeiten konnte. Das erste Problem war am leichtesten zu lösen 'diese Dienste hatten eine ältere Version der TCF interchange format library genutzt, die sich nicht linear an die Größe des Inputs anpasste. Das zweite Problem musste von Fall zu Fall einzeln gelöst werden. Diese Dienste hatten einige Programmierfehler, die sie bei unerwartetem Input versagen ließen. Das dritte Problem jedoch war schwerer zu lösen und wird im Rest dieses Abschnitts besprochen werden. Parser für natürliche Sprachen sind oft langsam im Vergleich zu anderen Annotationsdiensten. Zum Beispiel können gebräuchliche Konstituenz-Parser normalerweise höchstens ein paar Sätze pro Sekunde parsen. Um die von uns vorhergesehenen Gebrauchsfälle bewältigen zu können, mussten wir die Fähigkeiten moderner Server und Computercluster, Prozesse parallel durchführen zu können, ausnutzen. Dafür haben wir ein Framework entwickelt [3], das auf einer verteilten Task-Warteschlange (Jesque) basiert und das folgende bietet: Parallele Prozessverarbeitung innerhalb der Anfragen, gleichzeitige Verarbeitung von Anfragen, Garantien, was die Verwendung von Ressourcen und Fairness betrifft (z.B.: Eine große Anfrage sollte keine sichtbaren Auswirkungen auf kleine Anfragen haben.) Wir benutzen dieses Framework in den upgedateten Diensten für die Malt, Stanford- und Berkeley-Parser. Dadurch können wir solche Szenarien leicht bewältigen. Was vielleicht noch wichtiger ist: Unser Framework erlaubt es uns, Webservices an noch größere simultane Benutzerzahlen oder größere Inputs anzupassen, indem wir weitere Prozessorkerne oder Geräte hinzufügen. Fazit Wir haben diesem Abstract zwei neue Vorgehensweisen zur Messung der Nutzung und Kapazität von WebLicht dargestellt. Zuerst haben wir Nutzungsmeldungen zum Verkettungswerkzeug hinzugefügt, sodass wir die Nutzungsstatistiken der Annotationswerkzeuge bekommen ohne die CLARIN-Partner darum bitten zu müssen, ihre Werkzeuge anzupassen. Zweitens haben wir das Dienstprogramm Bombard vorgestellt, das es uns ermöglicht die Auswirkung der Hochrechnung des momentanen Wachstums zu messen. Solche Messungen machen uns die Nutzung zur Bestimmung von Kapazitätsengpässen in der WebLicht-Infrastruktur und ihre frühe Behandlung möglich. Bibliografie [1] Hinrichs, E., Hinrichs, M., and Zastrow, T. (2010). WebLicht: Web-based LRT services for German. In Proceedings of the ACL 2010 System Demonstrations, pages 25'29. Association for Computational Linguistics. [2] ISO 24622-1:2014. 2014. Language resource management -- Component Metadata Infrastructure (CMDI) -- Part 1: The Component Metadata Model. Technical Report, ISO. [3] De Kok, D., De Kok, D., and Hinrichs, M. (2014). Build your own treebank. In: Proceedings of the CLARIN Annual Conference 2014. Soesterberg, Netherlands."
2015,DHd2015,2015_cls_metadata_extracted.csv,Annotationen für die automatisierte Verarbeitung von Märchen,Thierry Declerck (Universität des Saarlandes),Annotation,Annotation,"In diesem Poster- und Demobeitrag fassen wir ältere und aktuelle Arbeiten zur Entwicklung eines Annotationsschemas für Märchen zusammen, das auch die Einbettung von Märchentexten in automatisierten Verarbeitungszenarien erlaubt. Eine Entwicklung unserer Arbeit in diesem Bereich führte zur automatischen Erkennung von Charakteren in Märchen, deren Rolle in Dialogen und deren Emotionen, die als Grundlage eines TextToSpeech Szenarios dient, das Märchentexte ""vorliest"". Dieses Ergebnis basiert auf einer Zusammenarbeit mit Studenten der Computerlinguistik an der Universität des Saarlandes, die in den letzten Jahren in Form von Bachelor- oder Masterarbeiten, oder auch in Form eines Softwareprojekts erfolgten. Angefangen hat es mit der Masterarbeit von Antonia Scheidel zur Annotation von Märchen mit Proppschen1 Funktionen. Antonia Scheidel entwickelte ein neues Annotationsschemas, nach dem Märchen nach Texteigenschaften, temporalen Strukturen, Charakteren, Dialogen, und Proppschen Funktionen abfragen kann (s. [1]). Ein Annotationsschema ist insofern wichtig, als dadurch automatisierte Systeme ein Ziel haben, in das sie ihre Ergebnisse abbilden können. Wenn dazu auch Märchen mit dem Annotationsschema manuell annotiert werden, können die Ergebnisse der automatischen Verarbeitungen mit den menschlichen Annotationen verglichen werden. Darauf aufbauend hat Nikolina Koleva an einem automatisierten System gearbeitet, das in Märchentext (sie hat mit 2 Beispielen gearbeitet; ""The Magic Swan Geese"", eine englische Version eines russischen Märchens, und ""Väterchen Frost"", eine deutsche Version eines russischen Märchens). Sie hat ein Programm geschrieben, dass der Text nach linguistischen Kriterien analysiert, mit dem Ziel, die darin vorkommenden Charaktere zu erkennen, und in eine Datenbank zu speichern. Diese Datenbank ist von der Sorte ""Ontologie"": darin können logische Operationen durchgeführt werden. Als Hintergrund fungiert eine formale Beschreibung dessen, was in den genannten Märchen vorkommen kann, inklusive eine Ontologie über Familienverhältnissen. So kann das System erkennen, dass im Text ""die Tochter"" die gleiche Person wie die ""Schwester"" ist, wenn der Kontext dies suggeriert. Erkannte Charaktere im Märchen werden somit mit allgemeineren Kategorien 1 Auszug aus Wikipedia: ""Propp gilt als Begründer der morphologischen oder strukturalistischen Folkloristik. Zwischen 1914 und 1918 studierte er russische und deutsche Philologie. Danach unterrichtete er die deutsche Sprache an verschiedenen Hochschulen in Leningrad. Von 1938 bis 1969 war er Professor für Germanistik, russische Literatur und Folklore an der Staatlichen Universität Leningrad. 1928 erschien sein bahnbrechendes Werk Morphologie des Märchens. Das Buch wurde 1958 in den USA in englischer Sprache veröffentlicht, was Propp weltweite Anerkennung verschaffte. 1946 erschien das Buch Die historischen Wurzeln des Zaubermärchens."" (http://www.wikiwand.com/de/Wladimir_Jakowlewitsch_Propp. Zugriff am 2014.11.10) semantisch annotiert. Und wir wissen dann in welchen Kontexten (oder Situationen) die Tochter (zum Beispiel) involviert ist (s. hierzu [2]). Schließlich eine Gruppe von Studenten (Christian Eisenreich, Jana Ott, Tonio Süßdorf und Christian Wilms) im Rahmen eines Softwareprojekts an Erweiterungen der oben genannten Arbeiten gearbeitet. Sie haben zum einem das Annotationsschema erweitert, mit detaillierteren Dialogbeschreibungen, und mit der Kodierung von Emotionen. Die Ontologie wurde auch erweitert, und sie inkludiert jetzt auch eine Beschreibung von Dialogen (Fragen, Antworten, Monologe, etc.), inklusive der Kodierungen der Teilnehmern und der Dialogwechseln. Auch 6 Basisemotionen (Angst, Trauer, Freude, etc) sind in der Ontologie kodiert. Eine Haupterweiterung der vergangenen Arbeiten besteht darin, dass auch synthetische Stimmen eine Rolle spielen. Ist einmal ein Charakter erkannt worden, zum Beispiel die Prinzessin (im Märchen ""Froschkönig""), werden zusätzliche Merkmale kodiert (zum Bsp. Alter, usw.). Dann wird automatisch eine vorher definierte synthetische Stimme zum Charakter addiert. Wenn dann der Text von dem System analysiert wird, kann die Geschichte von den Stimmen ""erzählt"" werden. Wenn kein Charakter in einer Dialogsituation vorkommt, dann wird angenommen, dass der Erzähler/die Erzählerin ""daran"" ist. Eine Demo kann hier gehört werden: https://bytebucket.org/ceisen/apftml2repo/raw/763c5eb533f09997e757ec61652310c74223838 4/example%20output/audio_output.mp3 Im Anhang sind 2 Screenshots, die (für den ersten Teil der Audiodatei) zeigen wie das System den Text bearbeitet und kodiert, so dass die Sprachausgabe (s. Link oben) erzeugt werden kann. Unser Poster/Demo zeigt die Korrelation zwischen die Annotationen, die zum größten Teil automatisch generiert worden sind, und den verschiedenen Stufen der Verarbeitung bis hin zur Sprachausgabe. Referenzen [1] Thierry Declerck, Antonia Scheidel, Piroska Lendvai. Proppian Content Descriptors in an Integrated Annotation Schema for Fairy Tales. Language Technology for Cultural Heritage. Selected Papers from the LaTeCH Workshop Series,Theory and Applications of Natural Language Processing, Pages 155-169, Springer, Heidelberg, 2011 [2] Nikolina Koleva, Thierry Declerck, Hans-Ulrich Krieger. An Ontology-Based Iterative Text Processing Strategy for Detecting and Recognizing Characters in Folktales in: Jan Christoph Meister (ed.): Digital Humanities 2012 Conference Abstracts, Pages 467-470, Hamburg. [3] Christian Eisenreich, Jana Ott, Tonio Süßdorf, Christian Willms, Thierry Declerck. From Tale to Speech: Ontology-based Emotion and Dialogue Annotation of Fairy Tales with a TTS Output Proceedings of ISWC 2014, Riva del Garda, Italy, Springer. Anhang Abbildung 1: Wie der Text analysiert wird, Charaktere erkannt werden, sowie Dialogstrukturen und Emotionen. Die Basis für die Generierung der Sprachausgabe Abbildung 2 Wie der Text analysiert wird, Charaktere erkannt werden, sowie Dialogstrukturen und Emotionen. Die Basis für die Generierung der Sprachausgabe (Fortsetzung von Abbildung 1)"
2015,DHd2015,2015_cls_metadata_extracted.csv,Over-tagging with XML in Digital Scholarly Editions,Elise Hanrahan (BBAW),"Digitale Edition, XML","Digitale Edition, XML","This talk looks at the phenomenon of over_tagging (term created here) in XML, which consists of exaggerated and unfocused tagging that concentrates on diplomatic characteristics. These tags are used for the display of the digital edition text on the computer screen__an especially questionable result when digital facsimiles exist. To what degree computer technology affects practices and theories in scholarly editing is an open question. But whether or not we are in the midst of a revolution or simply experiencing a change of tools, there are certain influences that can already be observed. One is the availability of digital facsimiles combined with the use of XML. How could digital facsimiles change online scholarly editions? Digital facsimiles challenge one of the claimed purposes of a scholarly edition‚Äîthe recreation of the original manuscript. 1 This aim, often strived for by means of a diplomatic transcription, is particularly prevalent in recent editions. Elements of diplomatic transcribing can for instance be found in most German critical editions from the last twenty_five years. 2 Yet the question ""How could digital facsimiles change online scholarly editions?"" was posed because astonishingly the diplomatic method continues to be dominant. It is thus the argument of this talk that online editions do not reflect this significant alteration in the relationship between researcher and original source. Instead the same editorial method is being used, and the primary new development is the use of XML instead of Microsoft Word. Indeed not only do digital editions continue to be diplomatic, but the tendency has even increased. 3 1 It should be noted that this very strong focus on an ""authentic"" recreation of the original handwriting is fairly new in Editionswissenschaft, starting in the 1970s and becoming very dominant in the 1990s with editors like Hans Zeller. 2 For a very concise summary of the take_over of the material_paradigma, see: Rasch, Wolfgang, Wolfgang Lukas, and Jörg Ritter. ""Gutzkows Korrespondenz 'Probleme Und Profile Eines Editionsprojekts. "" Brief_Edition Im Digitalen Zeitalter (Beihefte Zu Editio) 34 (2013): 99. 3 Elena Pierazzo addresses the popularity of digital documentary editions in: Pierazzo, Elena. ""Digital Documentary Editions and the Others. "" Scholarly Editing: The Annual of the Association for Documentary Editing 35 (2014). Accessed November 10, 2014. http://www.scholarlyediting.org/2014/essays/essay.pierazzo.html. 1 Structure of the Talk I. Arguments for why diplomatic transcriptions are no longer necessary when digital facsimiles are available II. An examination of why diplomatic aspects in digital editions have surprisingly increased instead of decreased III. Arguments and counter_arguments for continuing to transcribe diplomatically in digital editions IV. Suggestions for alternative editorial priorities I. Digital facsimiles are a game-changer There are two main arguments for recreating the original manuscript in the edited text (thus using a diplomatic transcription), both of which online facsimiles challenge. The first is to bridge the gap between the original manuscript and the researcher. Before digital facsimiles it was quite possible that the researcher never set eyes on the original manuscript, which was carefully stored away in a library or archive. The edition thus strove to offer the researcher an objective depiction of the handwriting. Now, however, the researcher can look at the image of the handwriting online, thus the edition must no longer bridge this gap. The second argument for a diplomatic transcription is to preserve the manuscript. If anything happened to the original source, the text of the edition could be used as a replacement. Additionally the edition protects the manuscript from being over_handled, because it functions as an authoritative substitute. There is however no longer a need to create a substitute for the manuscript, because a high_quality image exists. Despite these arguments, in praxis one finds digital editions still ruled by the diplomatic trend. Why is this? II. The diplomatic-tradition and XML There are two reasons for the prevalence of diplomatically_influenced digital transcriptions. The first is that digital editions emerged at the same time diplomatic editing was the dominant method for scholarly editions. 4 It is therefore not surprising that the current method for print editions was transferred to the newly emerging digital editions. The second reason is due to the very nature of XML. In XML, unlike in Microsoft Word, specifications for the visual presentation of the edited text are completely separated from the documentation of the original source. Hence in XML the editor is no longer limited by the 4 That is, the end of the 1990s/start of the 2000s 2 space on the page for recording textual phenomena and can enter as many XML tags as desired, allowing a theoretically endless documentation of the characteristics of the manuscript. Combine this opportunity with an already diplomatic trend, and the result is a lot of diplomatic XML_tagging, sometimes to an incredibly minute degree. This very real phenomenon shall be called 'over_tagging' . Over_tagging refers to an exaggerated amount of XML tags that do not pursue a specific research question, but are in praxis only used for the display of the edited text on the screen. Over_tagging could be for example using a character in the line below an indentation to mark the length of an indentation, tagging the exact location and angle of marginalia, tagging orthographical elements like the long s in German, or tagging line breaks that are not semantically meaningful. While there is nothing inherently wrong with tagging these kinds of textual phenomena, it is important to ask, what is the purpose of these tags? Such tagging is especially problematic when it comprises a large part of the XML schema. And one must add, no matter how detailed a transcription is, it can't recreate the image of the handwriting and the vast amount of data that the image carries. And not only is the usefulness of the results debatable, over_tagging takes a great deal of time and energy. Other fundamental editorial tasks fall by the wayside__tasks such as editorial commentary, to name only one of many. There are many other gaps to be bridged between the reader and original source besides the material gap. An essential benefit to be gained from more reflective tagging practices is the time to focus on these other editorial tasks. III. Counter-arguments: machine searchable and a reader-aid One argument for over_tagging is machine readability. This means that tagged textual information can be found in automated searches. Yet does anyone truly search for aspects such as indentation size? An editor might argue: ""Perhaps not now, but someone could in the future. And what""s more, someone could discover that this seemingly insignificant textual characteristic actually carries a semantic meaning"" . Such an answer reveals the editorial tradition that still strongly underlies digital editions today and is inseparable from diplomatic transcribing. This is the philosophy that literally everything on the page could be significant. 5 There are two responses to this. First of all search masks are currently not being made to search for diplomatic characteristics. In praxis these tags are used for the display of the text only. It is true that, if desired, search masks could be made for this purpose. It is also true that 5 This of course leads back again to the authenticity/materiality movement from Hurlebusch, Zeller and others. For an example of such a perspective that does not directly lead to the solution of a diplomatic transcription (but instead to digital facsimiles), see: Richter, Elke. ""Goethes Briefhandschriften digital 'Chancen und Probleme elektronischer Faksimilierung. "" Brief_Edition Im Digitalen Zeitalter (Beihefte Zu Editio) 34 (2013): 53_75. 3 it is impossible to prove that a certain aspect of a page is not meaningful. However, the ""everything could be significant"" position is not a feasible editorial method. It is definitely not the basis on which to build a well_functioning XML schema. In reality, at the end of the project there exists a very large amount of information that is solely used for the display of the text on the computer screen. Another argument for over_tagging is that the resulting text is a kind of facsimile_reading_aid. 6 This is however a problematic stance. Firstly, critical editions are not made primarily to be reading_aids for facsimiles, although they can be helpful for this purpose. Critical editions are editorial arguments and offer a readable edited text according to that argument. A facsimile_reading tool is potentially very useful, but is something different than a critical edition. Secondly, a diplomatic transcription was not conceived for this aim and is probably not the best method. There are certainly much better ways to help guide a researcher through a facsimile than simply mirroring the facsimile in the edited text, especially in consideration of technological possibilities. IV. Different priorities for digital editions Relinquishing over_tagging means more time for editors to concentrate on other aspects of editing, such as commentary and semantic tagging (including not just person or place names, but also more abstract themes, such as concepts found in the texts). There would also be more time to tag the creative process of the author (such as capturing the layers of the text's development by tagging crossed out/added words). There would be more time to enter meta data, to link through standard IDs like VIAFs for persons and geonames for places, and to simply to think about how to use the digital space to the researcher's best advantage. In many ways, instead of reflecting on what doors technology opens for critical editions and thus shaping technology to this end, editors have let technology define them, losing sight of priorities in today""s digital world. For instance, an essential current challenge for digital editions is to avoid the 'island' problem‚Äîsingle editions floating in the internet without a real connection to one another. Minute diplomatic tagging does not address this problem (standard IDs and meta data to some degree does). Yet it isn""t a question of what is important and what is not__all editorial tasks are important__it is a questioning of appreciating what an edition has to offer and carefully considering the energy invested and the benefits gained. ""Over_tagging"" is perhaps a very small piece of the debate on digital editions, but it could point to a general direction and is therefore worthwhile to consider in this context. 6 This idea is touched on in Pierazzo(2014), 4."
2015,DHd2015,2015_cls_metadata_extracted.csv,Digitale Analyse Graphischer Literatur,"Alexander Dunst (Institut für Anglistik und Amerikanistik, Universität Paderborn); Rita Hartel (Institut für Informatik, Universität Paderborn); Sven Hohenstein (Department Psychologie, Universität Potsdam); Jochen Laubrock (Department Psychologie, Universität Potsdam)","Graphic Novels, Graphic‚àöäNarrative Markup Language, Eyetracking, Annotation","Graphic Novels, Graphic‚àöäNarrative Markup Language, Eyetracking, Annotation","Zusammenfassung Der  hier  vorgeschlagene  Vortrag  stellt  erste  Ergebnisse  der  vom  deutschen  Bundesministerium  für Bildung  und  Forschung  (BMBF)  finanzierten  Nachwuchsgruppe  ""Hybride  Narrativität:  Digitale  und Kognitive Methoden zur  Erforschung  Graphischer Literatur"" vor. Der erste Teil des Vortrages wird das Projekt  der  Nachwuchsgruppe  und  zentrale  Forschungsfragen  vorstellen.  Im  zweiten  Teil  wird  eine kurze Einführung in die wichtigsten Merkmale der Beschreibungssprache  ""Graphic Narrative Markup Language""  (GNML)  gegeben  sowie  die  wesentlichen  Funktionen  des  Editors  zur  Erfassung  und Analyse graphischer Erzählungen demonstriert.  Außerdem präsentieren wir erste Ergebnisse, die mit Hilfe von Netzwerkgraphen und Beziehungsmatrizen zu Paul Austers City of Glass  (1985) und dessen graphischer  Adaptation  durch  David  Mazzuchelli  und  Paul  Karasik  (1994)  strukturelle  Öhnlichkeiten und  Differenzen  zwischen  den  narrativen  Systemen  des  literarischen  und  graphischen  Romans darstellen.  Der  abschließende  dritte  Teil  stellt,  wiederum  am  Beispiel  von  City  of  Glass,  eines  der interdisziplinären  Anwendungsgebiete  der  digitalen  Annotation  graphischer  Literatur  vor:  anhand von  Blickbewegungsmaßen  können  Rückschlüsse  auf  das  Leseverständnis  graphischer  Literatur gewonnen werden. 1. Forschungsfragen Die digitale  Annotation  und Analyse  literarischer  Text-__Korpora kann  in den vergangenen Jahren  auf enorme  Fortschritte  verweisen  und  hat  neue  Erkenntnisprozesse  etabliert,  die  mittlerweile  Eingang in die Forschungsbestrebungen einer breiteren Literaturwissenschaft und Literaturgeschichte finden (1).  Im  Gegensatz  dazu  steckt  die  Analyse  visueller  Kultur  erst  in  den  Anfängen  und  stellt  in  den Digitalen  Geisteswissenschaften  aus  mehreren  Gründen  oft  eine  Randerscheinung  dar:  zu  der institutionellen Verortung in traditionell text-__fokussierten Disziplinen und den Forschungsinteressen der  Computerphilologie  gesellen  sich  urheberrechtliche  Fragen,  sowie  der  vergleichsweise  hohe technische Aufwand und niedrigere Entwicklungsstand von Methoden der Bildanalyse. Ziel  dieses  interdisziplinären  Projektes  ist  die  empirische  Erforschung  graphischer  Literatur, insbesondere  des  Genres  des  graphischen  Romans  (""graphic  novel"").  Durch  die  Entwicklung  von empirischen  Methoden  für  graphische Literatur  sollen  Ansätze  aus  dem  ""Distant  Reading""  für multimediale  Kulturformen  erschlossen  werden.  Die  durch  die  Nachwuchsgruppe  entwickelten Annotations-__Werkzeuge, insbesondere die XML-__Sprache GNML  und der GNML-__Web-__Editor  (siehe 2.), sind in weiterer  Folge nicht nur für die Beschäftigung mit graphischer Literatur sondern auch für die Analyse  von  Handschriften,  Film  und  Fernsehen  von  Interesse.  In  erster  Linie  zielt  die Nachwuchsgruppe  jedoch  darauf  ab,  grundlegende  Fragen  zur  spezifischen  Narrativität  und  dem formalen  Aufbau  des  graphischen  Romans  zu  beantworten,  die  im  Rahmen  qualitativer  Methoden nicht  empirisch  überprüft  werden  können  oder  vollständig  außerhalb  des  Forschungsradius hermeneutischer  Fragestellungen  in  den  Geisteswissenschaften  liegen.  Folgende  zentrale Forschungsfragen sind hier beispielshaft zu erwähnen: Beschreibt der Terminus graphischer Roman,  1 ursprünglich  ein  Begriff  aus  der  Verlagswerbung,  tatsächlich  strukturelle  Öhnlichkeiten  mit  dem literarischen  Roman?  Welche  Charakteristika  unterscheiden  den  graphischen  Roman  vom Comicbuch?  Lassen  sich  strukturelle  Innovationen  isolieren  und  historisch  verfolgen,  die  das  Genre erfolgreich haben werden lassen? Sind diese narratologischer oder thematischer Natur, oder handelt es  sich  um  eine  Kombination  beider?  Aus  welchen  Sub-__Genres  besteht  der  graphische  Roman,  und wie interagieren diese im System des Genres? Welche gesellschaftlich relevanten Fragen werden im graphischen Roman kulturell verarbeitet und tragen so zu seiner Popularität bei? 2. Editor als Erfassungs-__  und Analysewerkzeug Die  auf der ""Comic Book Markup Language  (CBML)""  (2)  und damit auf der ""Text Encoding Initiative"" (TEI)  (3)  basierende  und  im  Rahmen  dieses  Projektes  entwickelte  XML-__Sprache  GNML  erlaubt  dem Bearbeiter nicht nur das Erfassen textueller  sondern insbesondere auch visueller  Aspekte graphischer Erzählungen.  Mit  Hilfe  von  GNML können  unter  anderem  Seiten,  Panel-__Anordnungen,  Texte, Sprechblasen, Charaktere und andere Objekte erfasst werden, sowie Interpretationen,  z.B.  zu  Panel-__ Übergänge und Texttypen,  abgelegt werden.  GNML bietet somit eine abstrakte Sicht auf visuelle  und textuelle  Aspekte,  die  so  effizient  analysiert  werden  können.  Basierend  auf  GNML  können  z.B. Eyetracking-__Experimente  ausgewertet  werden,  und  Fragestellungen  wie  ""Wie  oft  wechselt  die Aufmerksamkeit  des  Lesers  vom  Text  zum  Bild""  oder  ""Was  ist  der  relative  (visuelle)  Anteil  eines Charakters an der gesamten Erzählung"" effizient beantwortet werden. Eine  zentrale  Rolle  dieses  Projektes  nimmt  der  GNML-__Editor  ein.  Er  erlaubt  ein  effizientes, benutzerfreundliches  Erfassen  der  visuellen  und  textuellen  Aspekte,  ohne  dass  der  Bearbeiter  XML oder  GNML  beherrschen  muss.  Neben  der  Erfassung  visueller  Aspekte,  bei  der  Objekte  durch  den Bearbeiter  nachgezeichnet  und annotiert  werden können, bietet der Editor auch die automatisierte Erkennung  verschiedener  Aspekte.  Derzeit  bietet  der  Editor  z.B.  eine  automatische  Erkennung  der Panels.  Hierbei  werden  nicht  nur  regelmäßige  Formen  (Rechtecke),  sondern  nahezu  beliebige Umrandungsformen  automatisch  erkannt.  Eine  weiterführende  automatisierte  Erfassung,  etwa  mit Hilfe  von  Texterkennungssystemen  und  Handschriftenerfassung  oder  das  automatische  Erkennen von  Charakter-__Objekten,  befinden  sich  derzeit  in  Entwicklung.  Verfahren  aus  dem  Bereich  des maschinellen Lernens sollen dafür sorgen, dass die Erkennung neuer Charaktere mit zunehmendem Training zuverlässiger funktioniert. Ein zusätzliches Analysetool ermöglicht  die  Analyse bereits erfasster GNML Dokumente.  So kann der Benutzer z.B. sich die Beziehungen der Charaktere untereinander in Form von Netzwerkgraphen  und Beziehungsmatrizen  anzeigen  lassen.  Auch  erweiterte  Statistiken  über  die  Objekte  und  Charaktere der Erzählung können berechnet und dem Benutzer in Form von Diagrammen und Tabellen angezeigt werden, basierend z.B. auf den folgenden Fragestellungen: ‚Ä¢ Wie oft erscheint ein Charakter? ‚Ä¢ Was ist der relative visuelle Anteil eines Charakters? ‚Ä¢ Mit wem zusammen erscheint der Charakter auf derselben  Seite oder in demselben Panel? 3. Empirische Ergebnisse: Eyetracking-__Maße für die Aufmerksamkeitszuwendung des Lesers Die  Methode  der  Blickbewegungsmessung  (Eyetracking)  liefert  Einblicke  in  die  größtenteils unbewusste  und  in  gewissem  Maße  kulturspezifische  Verteilung  der  Aufmerksamkeit  bei  der visuellen  Rezeption  von  Informationen.  Für  die  Rezeption  von  Texten  hat  die  psycholinguistische Forschung  hier  bereits  viele  grundlegende  Erkenntnisse  gewonnen,  und  die  Methode  wird  auch  2 erfolgreich  bei  der  Erforschung  der  kognitiven  Verarbeitung  von  Bildern  oder  visuellen  Szenen angewandt.  Sequenzielle  Kunst,  Comics  und  graphische  Literatur  sind  jedoch  bisher  fast  völlig vernachlässigt, obwohl sie idealtypisch textuelle und graphische Elemente kombinieren. Die Nutzung per  GNML  annotierten  Materials  eröffnet  neue  Möglichkeiten,  die  psychologische  Wirkung graphischer Literatur zu untersuchen. Wie  gelingt  es  einem  Zeichner,  die  Aufmerksamkeit  des  Lesers  von  einem  Panel  zum  nächsten  zu lenken? Wie interagieren textuelle und graphische Elemente bei der Rezeption eines Comics? Scott McCloud (4) hat dazu erste theoretische Überlegungen angestellt und visualisiert; Neil Cohn (5) hat die  theoretische  Analyse  weiterentwickelt  zu  einer  formalen  ""visuellen  Sprache"",  die  vergleichbar einer  generativen  Grammatik  die  narrativen  Elemente  einer  graphischen  Geschichte  kategorisiert. Haben  diese  Ordnungsschemata  eine  psychologische  Realität?  Erste  Eyetracking-__Studien  aus unserem  Labor  zeigen,  dass  das  von  McCloud  postulierte  Ausmaß  an  ""Closure"",  das  zwischen unterschiedlichen  Arten  von  Panel-__Übergängen  variiert,  sich  in  unterschiedlich  langen Betrachtungszeiten niederschlägt. Wir  zeigen  außerdem  erste  Ergebnisse  aus  einem  empirischen  Eyetracking-__Corpus  graphischer Literatur.  Im  Kontext  des  Projektes  wird  ein  R-__Paket  zum  Import  von  GNML-__Daten  und  zur statistischen  Analyse  und  Visualisierung  von  Blickbewegungen  und  Corpusdaten  entwickelt,  das  in diesen  Analysen zur Anwendung kommt. Literaturverzeichnis 1. Siehe etwa: Moretti, Franco. Distant Reading. London: Verso, 2013. 2.  Walsh,  John.  Comic  Book  Markup  Language:  An  Introduction  and  Rationale.  Digital  Humanities Quarterly.  2012, Volume 6, Number 1. 3.  Text  Encoding  Initiative  -__  P5:  Guidelines  for  Electronic  Text  Encoding  and  Interchange.  [Online] 2.7.0, 09 16, 2014. http://www.tei-__c.org/release/doc/tei-__p5-__doc/en/html/. 4.  McCloud,  Scott.  Understanding  Comics:  The  Invisible  Art.  Northampton,  MA:  Kitchen  Sink  Press, 1993. 5. Cohn, Neil. The Visual Language of Comics. London: Bloomsbury, 2013."
2016,DHd2016,vortraege-062.xml,Emosaic 'Visualisierung von Emotionen in Texten durch Farbumwandlung zur Analyse und Exploration,"Martin von Lupin (FH Potsdam, Deutschland); Philipp Geuder (FH Potsdam, Deutschland); Marie-Claire Leidinger (FH Potsdam, Deutschland); Tobias Schröder (FH Potsdam, Deutschland); Marian Dörk (FH Potsdam, Deutschland)","Datenvisualisierung, Emotionen, Textumwandlung, Textexploration, Interaktiv","Umwandlung, Entdeckung, Programmierung, Inhaltsanalyse, Identifizierung, Webentwicklung, Visualisierung, Daten, Literatur, Methoden, Forschung, Forschungsergebnis, Software, Text, Werkzeuge, Visualisierung","   Während beide Tools als Datengrundlage Emotionen aus Webeinträgen nutzen, ist es hingegen unser Ziel die benutzerdefinierte Eingabe eines Textes zu ermöglichen. Zusätzlich sollen sowohl differenzierte als auch nachvollziehbare farbliche Repräsentationen für Emotionen verwendet werden. Die Farbkodierung von Emotionen zur besseren Orientierung innerhalb visueller Darstellungen scheint eine etablierte Methode zu sein, jedoch lassen die bisher vorliegenden Kodierungssysteme keine fundierte Emotionsbeschreibung zu.  Während die Sättigung und die Helligkeit Minima und Maxima analog zu Dominanz und Erregung beschreiben, stellt der Farbwert einen kontinuierlichen Farbverlauf dar. Um eine Farbwertannäherung an den Rändern zu vermeiden, haben wir bei der Farbzuweisung einen Farbwertbereich bewusst ausgespart, sodass eine Grenze zwischen Minima und Maxima deutlich hervortritt. Blau entspricht dem Minimum, rot dem Maximum und grün einem mittleren neutralen Valenzwert. Für die Beurteilung der Zuordnung orientierten wir uns an den sechs Basisemotionen (Liebe, Überraschung, Freude, Wut, Trauer und Angst), wobei wir die Zuweisung der Emotionsdimensionen auf die Farbdimensionen so wählten, dass Liebe einem Rot- / Pinkton entspricht, um der tradierten Farb-Emotions-Zuweisung in der westlichen Kultur zu entsprechen (Abbildung 1a). Eine informelle Studie zeigte, dass diese Form der Zuordnung als intuitiv bewertet wurde. Daneben zeigte sich in Übereinstimmung mit unserer Farbzuweisung, dass negative Gefühle eher dunkel sind und kühlen Farbtönen wie blau oder grün zugeordnet werden (Abbildung 1b), dagegen positive Gefühle eher hell sind und mit warmen Farbtönen wie gelb oder orange in Verbindung gebracht werden (Abbildung 1c). Grundlegend für die Funktionsweise des Tools ist die Eingabe eines Textes. Der Nutzer kann aus vorgegebenen Texten verschiedenster Länge wählen oder einen eigenen Text innerhalb eines Textfeldes platzieren. Nach der serverseitigen Textanalyse sind verschiedene statische und interaktive Darstellungen verfügbar. Die Darstellung der Emotionsanalyse teilt sich in drei Bereiche auf: Makroansicht, Textansicht und Mikroansicht (Abbildung 2). Die drei Bereiche sind miteinander verlinkt. Grundlegend für die dynamische Önderung einer Ansicht  ist die Auswahl von einzelnen Emotionen bzw. Wörtern oder einem Bereich innerhalb einer Emotionsdimension. Die Die Die "
2016,DHd2016,vortraege-040.xml,Attribuierung direkter Reden in deutschen Romanen des 18.-20. Jahrhunderts. Methoden zur Bestimmung des Sprechers und des Angesprochenen,"Markus Krug (Universität Würzburg, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland); Isabella Reger (Universität Würzburg, Deutschland); Luisa Macharowsky (Universität Würzburg, Deutschland); Lukas Weimer (Universität Würzburg, Deutschland); Frank Puppe (Universität Würzburg, Deutschland)","Sprechererkennung, NLP, Quantitative Textanalyse","Programmierung, Inhaltsanalyse, Strukturanalyse, Modellierung, Annotieren, Literatur, Personen"," Eine der ersten Arbeiten auf diesem Gebiet ist das System ESPER (Zhang et al.  2003), das direkte Reden innerhalb von Kindergeschichten erkennen soll. Das  System extrahiert zunächst die direkten Reden im Text und klassifiziert diese  mit einem Entscheidungsbaum in zwei Kategorien, Sprecherwechsel bzw. kein  Sprecherwechsel. Evaluiert werden die Ergebnisse mit zwei manuell annotierten,  sehr unterschiedlichen Geschichten. Sie berichten eine Genauigkeit, gemeint ist  hier die Anzahl der korrekt bestimmten Sprecher für alle direkten Reden, von  47.6% und 86.7%. Glass und Bangay (2006), ebenfalls regelbasiert, bestimmen  zunächst für eine direkte Rede das Kommunikationsverb und anschließend eine  Menge von Akteuren, woraus letztendlich der Sprecher bestimmt wird. Sie  evaluieren ihre Techniken auf 13 englischsprachigen fiktionalen Werken und  berichten eine Genauigkeit von 79.4% (Glass / Bangay 2007). Iosif und Mishra  (2014) ¬†folgen im Prinzip dem Schema von Glass und Bangay (2007), ergänzen es  aber durch eine aufwendigere Vorverarbeitung einschließlich  Koreferenzresolution. Sie erreichen eine Genauigkeit von ca 84.5% und zählen  damit zu den besten bisher veröffentlichten Ergebnissen. Ruppenhofer und andere  (Ruppenhofer et al. 2010) berichten einen F-Score von 79% in der Zuordnung von  Politikern zu ihren Aussagen in deutschsprachigen Kabinettsprotokollen aus den  Jahren 1949-1960. Neben diesen regelbasierten Ansätzen werden auch maschinelle Lernverfahren eingesetzt. Zu den ersten erfolgreichen Systemen zählt das von Elson und McKeown (2010). Ihre Daten für die Sprecherzuordnung ließen sie über Amazons Die Zuordnung einer angesprochenen Figur wurde unserer Wissens noch in keiner anderen Arbeit untersucht. Für diese Arbeit verwenden wir Abschnitte des frei zugänglichen Korpus DROC. DROC besteht aus 89 Romanausschnitten, jeweils 130 Sätze lang, in denen alle Figurenreferenzen (mit und ohne Namen) und Koreferenzen annotiert sind. Aus dem Korpus wurden 77 Ausschnitte ausgewählt und mit einem eigens entwickelten Tool alle direkten Reden sowie die zugehörigen Sprecher und angesprochenen Figuren eingetragen. Jeder Text wurde von einem Annotator bearbeitet; eine zweite Annotation ist vorgesehen. Insgesamt wurden so 2264 direkte Reden mit Sprecher und Angesprochenen annotiert. Für die in Abschnitt 5 diskutierten Experimente wurde das Korpus in drei zufällige Mengen aufgeteilt: Wir verwenden regelbasierte Verfahren und maschinelle Lernverfahren, aber anders als in (He et al. 2013) oder (O""Keefe et al. 2012) dienen erstere nicht nur als Baseline-Verfahren, sondern wurden soweit wie möglich optimiert. Wir verwenden die Techniken 2-Way Klassifikation und N-Way Klassifikation wie in (O""Keefe et al. 2012) vorgeschlagen. Zusätzlich evaluieren wir MaxEnt2WayToMatch, bei dem Kandidaten nur bis zum ersten tatsächlichen Sprecherkandidaten erzeugt werden. Für die Sprecherzuordnung und Zuordnung eines Angesprochenen sind die in dieser Arbeit verwendeten Features in Tabelle A1 im Anhang zusammengefasst. Für diese Aufgabe haben sich regelbasierte Verfahren als konkurrenzfähig mit den   aktuellen ML-Verfahren erwiesen. Sie besitzen außerden den Vorteil, dass sie   nicht so viele Trainingsbeispiele benötigen. Die Grundstruktur des Algorithmus   ist der Idee des regelbasierten Koreferenzsystems von Stanford (Lee et al. 2011)   angelehnt. Es werden eine Reihe von Regelpässen nacheinander ausgeführt. Die   Regelpässe sind gemäß ihrer Mit Hilfe der Trainingsdaten konnte eine optimale Reihenfolge der Ausführung der Regeln empirisch ermittelt werden, bei der einige Regeln auch mehrfach angewendet werden. (1)‚Üí(2)‚Üí(3)‚Üí(4)‚Üí(5)‚Üí(6)‚Üí(7)‚Üí(5)‚Üí(6)‚Üí(8) ‚Üí(9)‚Üí(5)‚Üí(6)‚Üí(7)‚Üí(10). Die Parameter für die ML-Verfahren wurden auf dem Development-Anteil der Daten optimiert und anschließend gegen die Testmenge evaluiert. Für die regelbasierten Verfahren gibt es keine Unterscheidung zwischen Trainings- und Development-Korpus. Ein Sprecher gilt als korrekt bestimmt, wenn sich der vom System bestimmte Kandidat in der selben Koreferenzkette befindet, wie die Entität, die von unserem Annotator als korrekt markiert wurde. Tabelle 2 beschreibt die Ergebnisse bei der Anwendung der Verfahren auf das Testkorpus. Unsere Experimente bestätigen die Aussagen von O""Keefe (O""Keefe et al. 2012),  dass 2Way ML-Verfahren bessere Ergebnisse in der Sprechererkennung liefern, als  korrespondierende NWay Verfahren. Analoges gilt für die Evaluation der CRFs, die  sogar beinahe den selben Wert für die Sprechererkennung liefern wie in (O""Keefe  et al. 2012). Sowohl auf dem Developmentkorpus, als auch auf dem Testkorpus  zeigen regelbasierte Ansätze deutliche Vorteile gegenüber den in dieser Arbeit  verwendeten ML-Verfahren. Es ist weiterhin ersichtlich, dass die Bestimmung des  Sprechers einfacher ist, als die Bestimmung des Angesprochenen. Wahrscheinlich  liegt das daran, dass im Fall der Sprecherzuschreibung mehr Information  vorliegt, nämlich die direkte Rede und der Kontext, während bei der Ermittlung  des Angesprochenen die direkte Rede selbst nur hilfreich ist, wenn ein  Angesprochener direkt darin vermerkt ist. Ein direkter Vergleich mit dem besten in der Literatur zu findenden Verfahren  (Almeida et al. 2014) kann direkt nicht durchgeführt werden. Berücksichtigt man  den Unterschied, der Verfahren von O""Keefe auf den Texten des WSJ und den  literarischen Texten, könnte eine Qualität von 90% Genauigkeit erreicht werden  und damit ein mit der state of the art vergleichbares, sogar möglicherweise  besseres Ergebnis. Im Gegensatz zu ihrem Verfahren ermitteln wir zudem auch noch  eine angesprochene Entität. Die Ergebnisse zeigen, dass das regelbasierte Verfahren für diese Aufgabe  deutlich bessere Ergebnisse erzielen kann als alle ML-Verfahren, die in dieser  Arbeit getestet wurden. Es ist geplant, die hier erstellte Zuordnung in die  regelbasierte Koreferenzauflösung von (Krug et al. 2015) einzuarbeiten, um diese  damit zu verbessern. Weil unsere Hauptmotivation die Verbesserung der  Koreferenzresolution ist, diese aber im Ansatz von Almeida nicht wirksam  verbessert werden konnte, haben wir darauf verzichtet, deren komplexes  Lernverfahren nachzuvollziehen. Gerade die Ergebnisse, die in Tabelle 2 zu sehen  sind, zeigen, dass mögliche Dialogsequenzen genauer untersucht werden müssen, um  diese zuverlässig erkennen und auflösen zu können. Eine genaue Dialoganalyse  vereinfacht wiederum die Korefenzauflösung, so dass eine Extraktion von  Beziehungen zwischen Personen und Attributen zu Entitäten innerhalb der Romane  möglicher erscheint.  (1)‚Üí(2)‚Üí(3)‚Üí(4)‚Üí(5)‚Üí(6)‚Üí(7)‚Üí(5)‚Üí(6)‚Üí(8) ‚Üí(9)‚Üí(5)‚Üí(6)‚Üí(7)‚Üí(10). "
2016,DHd2016,vortraege-019.xml,Weibliches Erzählen im Expressionismus? Eine Stilometrie von Mela Hartwigs Prosa,"Melanie Mihm (Justus-Liebig-Universität Gießen, Deutschland)","Stilometrie, Log-Likelihood-Test, Bar Chart, Mela Hartwig","Inhaltsanalyse, Stilistische Analyse, Visualisierung, Literatur","""Die Stilanalyse ist eine Schlüsselqualifikation literarturwissenschaftlicher  Arbeit"" (Meyer 2007: 70). So die Einführung, die man im zweiten Band Diese vorliegende Stilometrie setzt sich zum Ziel, mithilfe statistischer  Analysetechniken sowohl eine quantitative als auch qualitative Stiluntersuchung  von Prosa der österreichischen Autorin Mela Hartwig zu realisieren. Bislang  liegen keine computergestützten Studien für diese Autorin der  Zwischenkriegsjahre vor. Dies soll zum Anlass genommen werden, exemplarisch die  Verwendung einer Herzmetaphorik als spezifischen Stil der Autorin zu  untersuchen. Es sollen mithilfe der Stilometrie intuitive Annahmen überprüft und  in Zusammenhang mit ausgewählten Prosatexten des literarischen Expressionismus  gesetzt werden. Hartwig verwendet das Herz mit einer tieferen Bedeutung, sie  meint nicht nur das Organ, das den Körper mit Blut versorgt. Bei den  expressionistischen Texten von Hartwig kann das Herz bei der Frau beispielsweise  für das Pendant des männlichen Gehirns stehen. Hierbei wird der Frau scheinbar  das Vermögen des Denkens abgesprochen und an Stelle des Gehirns, also dem  rationalen Vermögen des Mannes, arbeitet das Herz der Frau im Sinne eines  affektiven, emotionalen, spontanen Teils des weiblichen Körpers. Es scheint  auffällig, dass Hartwig ein binäres System von Herz 'Verstand / Gehirn und  damit einhergehend eine Gegenüberstellung von Frau 'Mann herausarbeitet. Diese  Dichotomie und das überwiegend weibliche konfliktgeladene Figurenrepertoire  sowie die Erzähltextperspektive aus Sicht von weiblichen Figuren könnte als  typisches ,weibliches Erzählen"" gefasst werden. Die erste Hypothese lautet, dass  in Mela Hartwigs gesamter Prosa die Herzmetaphorik Verwendung findet. Die zweite  Hypothese lautet, dass die Herzmetapher bei Mela Hartwig wesentlich mit  Weiblichkeit verbunden ist. Ziele des Vortrages sind das zielgruppenorientierte Präsentieren des methodischen  Vorgehens und der Zwischenergebnisse, die Erläuterung der interdisziplinären  Fragestellungen, Arbeitshypothesen und das in Zusammenhangsetzen der  Zwischenergebnisse der Stilometrie für Prosatexte von Mela Hartwig mit dem von  Moretti (2013) etablierten Es wurden zwei literarische Textkorpora manuell erstellt und für das dritte  Vergleichskorpus eine Kombination von zwei Korpora aus COSMAS II In dem Korpus von Mela Hartwig (KMH) kommt die Herzmetapher in neun von elf Texten vor. Die Auswertung des Korpus mit den expressionistischen Texten ergab, dass von 122 Texten 37 eine Herzmetapher aufweisen. Die Ergebnisse der Berechnung zeigen, dass Hartwig in 81,82 % ihrer publizierten Texte die Herzmetapher verwendet. Im Falle der expressionistischen Texte ist berechnet worden, dass 30,33 % der Prosatexte eine ähnliche Herzmetapher nachweisen. Da der G¬≤-Wert 5,78 beträgt und der zuvor festgelegte kritische Wert bei 3,84 oder größer liegt (bei einem Signifikanzniveau von 5 %), ist die Herzmetapher bei Hartwigs Texten gegenüber den expressionistischen Texten im Korpus signifikant. Auch gegenüber dem größeren, epochenübergreifenden COSMAS II-Korpus tritt die Herzmetapher signifikant häufig auf: Die Berechnung ergab einen G¬≤-Wert von 8,42 und übersteigt den kritischen Punkt. Dieser G¬≤-Wert bestätigt meine zu Beginn dieser Arbeit geäußerte Vermutung, dass es sich bei der Herzmetapher, wie sie Mela Hartwig verwendet, um eine Besonderheit ihres Schreibstils handelt. Betrachtet man den niedrigen G¬≤-Wert von 0,97, der bei der Berechnung der expressionistischen Texte (KEX) und COSMAS II herauskommt, fällt auf, dass hier eine ähnliche prozentuale Verteilung der Herzmetapher (KEX = 30,33 % und COSMAS II = 24,71 %) vorliegt. Der Wert unterschreitet den festgelegten kritischen Wert von 3,84. Dennoch sei darauf aufmerksam gemacht, dass in den Texten des Expressionismus ca. 6 % häufiger die Herzmetapher vorkommt. Für die zweite Hypothese, dass Mela Hartwig die Herzmetapher nur in Texten verwendet, bei der sie eine weibliche Erzähltextperspektive und einen weiblichen autodiegetischen Erzähler einsetzt, wurde ein Konkordanz-Plot im Barcode-Format für das Personalpronomen 'ich' erstellt. Je dunkler der Barcode-Streifen des Konkordanz-Plots ausfällt, desto häufiger tritt an der schwarzen Stelle das angesteuerte Wort der Suchanfrage auf. Das Ergebnis der Visualisierung zeigt, dass das Personalpronomen in den Texten, in denen die Herzmetapher vorkommt, sehr viel häufiger verwendet wird. In Texten, in denen die Herzmetapher nicht vorkommt, findet sich das Pronomen, wenn überhaupt, nur in der wörtlichen Rede. Diese Stilometrie brachte das Zwischenergebnis zu Tage, dass man durch die Kombination von Es werden weitere korpusbasierte Analysen angestrebt. Diese erfordern aber eine Voraussetzung: Im Zuge der ""digitale[n] Wende"" (Schöch 2014: 130) ist es weiterhin wünschenswert und erforderlich, dass immer mehr literarische Texte digital zur Verfügung stehen oder diese durch leichte und praktikable Verfahren der Texterkennung (OCR,"
2016,DHd2016,posters-058.xml,Romantik im Wandel der Zeit 'eine quantitative Untersuchung,"Johannes Hellrich (Jena University Language, Information Engineering Lab, Friedrich-Schiller-Universität Jena, Deutschland); Udo Hahn (Jena University Language, Information Engineering Lab, Friedrich-Schiller-Universität Jena, Deutschland)","Sprachwandel, Diachronie, Romantik, Google Books, Deutsch","Entdeckung, Inhaltsanalyse, Modellierung, Daten, Sprache, Literatur, Text","""Romantik"" und ""romantisch"" bezeichnen heute Profanes, wie ein privates Abendessen in   edlem Ambiente bei Kerzenschein oder ein idyllisch gelegenes Hotel mit   Butzenglasscheiben, meinte aber ursprünglich Östhetisches, insbesondere   Literarisches (DWB). Dieser Bedeutungswandel kann durch die automatische Analyse   eines großen Korpus' quantifiziert werden; etwa unter Verwendung des Google Books   N-Gram Korpus, das 4% aller je gedruckten Bücher enthält (Michel et al. 2011).   Methodische Grundlage für die Analyse sind Verfahren aus der Computerlinguistik   (distributionelle Semantik), mittels derer die Bedeutung von Wörtern über den für   sie typischen Kotext (also ihren direkten textuellen Kontext) approximiert wird. Ein   aktuelles und mächtiges Verfahren ist word2vec (Mikolov et al. 2013), das auf   Forschungsarbeiten zu künstlichen neuronalen Netzen basiert (LeCun et al. 2015).   Damit gewonnene Repräsentationen können sowohl synchron als auch diachron auf ihre   Öhnlichkeit hin verglichen werden, wodurch Bedeutungswandel quantifiziert werden   kann. Dies wurde bereits am Beispiel des Englischen durch Kim et al. (2014)   demonstriert, die für einen Zeitraum von 100 Jahren Wortrepräsentationen erzeugten   und verglichen. Ein Ergebnis war die Quantifizierung des Bedeutungswandels von ""gay""   hin zu einer Bezeichnung für (männliche) Homosexualität. Zwischen den   word2vec-Repräsentationen einzelner Wörter sind zudem semantisch sinnvolle   arithmetische Operationen möglich (Mikolov et al. 2013). Beim Vergleich der modernen   und der historischen Bedeutung von ""romantisch"" und ""Romantik"" könnte somit   ermittelt werden, ob sie einander ähnlicher sind, wenn beispielsweise sexuelle   Aspekte ignoriert werden. Alternative Verfahren zur Quantifizierung von Bedeutungswandel nutzen die Kookkurrenz  von Wörtern in Bi-Grammen (Gulordava / Baroni 2011), oder einen auf Nachbarwörtern  trainierten Klassifikator (Mihalcea / Nastase 2012). Dabei kann der erste Ansatz  lediglich lokale Zusammenhänge erfassen, während der zweite vordefinierte Zeiträume  erfordert, zwischen denen ein Bedeutungsunterschied gesucht werden soll. Riedl,  Steuer und Biemann (2014) entwickelten einen distributionellen Thesaurus, der für  vordefinierte Zeiträume ähnliche Wörter gruppiert. Nachteil dieser Methoden ist  wiederum die Notwendigkeit, den untersuchten Zeitraum in Abschnitte zu unterteilen.  Vorteilhaft gegenüber word2vec ist die Möglichkeit, die einzelnen Bedeutungen  polysemer Wörter getrennt zu erfassen 'statt einer veränderlichen Gesamtbedeutung,  liegen Teilbedeutungen mit unterschiedlicher Frequenz vor. Eine semantisch  schwächere Form des quantitativen Zugangs, aber nützlich für spätere qualitative  Interpretationen, ist die Visualisierung von Kollokationen im Zeitverlauf (Jurish  2015). Die bei unseren Untersuchungen erwarteten Ergebnisse umfassen nicht nur die  zunehmende Trivialisierung der Wörter ""romantisch"" und ""Romantik"" während der  letzten 200 Jahre, sondern auch eine Reflexion des deutschen Nationalismus im 19.  und 20. Jahrhundert, von dem die Epoche der Romantik instrumentalisiert wurde  (Kremer 2007: 50-58). Dem entspricht etwa die erhöhte Frequenz von ""Romantik"" in  deutschen Texten der Zwischenkriegszeit und direkten Nachkriegszeit, die mit einer  Verdrängung des Bi-Gramms ""romantische Liebe"" einhergeht, das dafür Ende des 20.  Jahrhunderts seine maximale Popularität erreicht. Wörter mit hoher Öhnlichkeit zu ""romantisch"" im Zeitverlauf (hoher Cosinus entspricht hoher Öhnlichkeit). Geplante Folgearbeiten beinhalten, neben der geisteswissenschaftlichen Einordnung der  quantitativen Ergebnisse, den Vergleich über mehrere europäische Sprachen hinweg und  die Einbeziehung von Sentiment Analysis-Technologien, um die emotionale Ladung der  Wörter im Verlauf der Zeit abzubilden (Acerbi et al. 2013). Die beschriebenen Arbeiten sind Teil eines Promotionsvorhabens am von der Deutschen Forschungsgemeinschaft finanzierten Graduiertenkolleg ""Modell 'Romantik'"" der Friedrich-Schiller-Universität Jena."
2016,DHd2016,vortraege-035.xml,Operationalisierung von Forschungsfragen in CLARIN-D - Der Anwendungsfall Ernst Jünger,"Dirk Goldhahn (Universität Leipzig, Deutschland); Thomas Eckart (Universität Leipzig, Deutschland); Gerhard Heyer (Universität Leipzig, Deutschland)","Differenzanalyse, Webapplikation, Forschungsinfrastruktur, Operationalisierung, Forschungsfrage","Inhaltsanalyse, Stilistische Analyse, Visualisierung, Sprache, Forschungsprozess, Werkzeuge","Als konkretes Beispiel für die Nutzung einer solchen Forschungsinfrastruktur wird  im Folgenden ein Usecase vorgestellt, der zur Beantwortung einer realen  Forschungsfrage der Germanistik verschiedene Bestandteile der Infrastruktur  CLARIN nutzt (Goldhahn 2015). Dabei werden verteilte Daten und Werkzeuge  genutzt, um Ressourcen zu finden, zweckmäßig aufzubereiten, zu analysieren und  die Ergebnisse zu visualisieren. Ernst Jüngers politische Publizistik der Jahre 1919 bis 1933 liegt in einer  philologisch aufbereiteten und annotierten Edition (Berggötz 2001) vor. Die  Relevanz dieser Texte liegt in der Vielzahl behandelter Themen begründet, die  relevant für die Entwicklung Deutschlands in den zwanziger und frühen dreißiger  Jahren sind. Dies umfasst unter anderem Fronterfahrungen, Konsequenzen des  verlorenen Krieges sowie das Thema der nationalen Neuorientierung. Dabei ändern  Jüngers Texte in den 15 Jahren ihrer Erstellung deutlich thematische Prioritäten  und linguistische Form (Gloning 2016). Schlüsselfragen, die aus linguistischer und diskurshistorischer Perspektive bezüglich dieses Korpus bestehen, umfassen eine mögliche Korrelation der Sprachverwendung auf Wortebene mit den konkreten Themen, die in den Texten behandelt werden. Dabei sollte das lexikalische Profil Jüngers über die Dimension Zeit charakterisiert und mit den lexikalischen Profilen zeitgenössischen Materials (wie zum Beispiel Zeitungstexte der 1920er oder Werke anderer Autoren der gleichen Zeit) abgeglichen werden. Um diese Forschungsfragen systematisch zu beantworten, müssen sie zuerst operationalisiert werden. Wichtige Aspekte dieses Prozesses sind: Fokus der Operationalisierung wird auf der Nutzung der CLARIN Infrastruktur liegen, um relevante Daten und Algorithmen zu suchen und die Analyse durchzuführen. Dabei werden zuerst Texte gesucht, die für die Forschungsfrage von Relevanz sind. Das Korpus von Ernst Jüngers politischer Publizistik der Jahre 1919 bis 1933, das unter anderem auch die Veröffentlichungsdaten aller Texte enthält, dient dabei als Startpunkt. Für den eigentlichen Vergleich wird eine konkrete Analysemethode benötigt. Eine  Möglichkeit ist hier die Nutzung einer sogenannten Differenzanalyse (Heyer et  al. 2008). Dabei können Unterschiede zwischen Jüngers Texten unterschiedlicher  Jahre oder zwischen Jüngers Texten und Referenzkorpora untersucht werden. Dies erlaubt uns die: Eine Voraussetzung für die Durchführung einer Differenzanalyse ist die Verfügbarkeit von Referenzmaterial. Für die Suche nach entsprechenden Textdaten bietet sich das bereits erwähnte CLARIN Virtual Language Observatory an. Durch die Einschränkung der vorhandenen Ressourcen des VLO über facettierte und Volltextsuche auf Korpora in deutscher Sprache des 20. Jahrhunderts stellt sich das DWDS Kernkorpus als relevante Ressource heraus (Abbildung 1). Das DWDS Korpus (Geyken 2006) wurde an der Berlin-Brandenburgischen Akademie der Wissenschaften zwischen 2000 und 2003 erstellt. Der Hauptzweck des DWDS Kernkorpus ist der Einsatz als empirische Basis eines großen monolingualen Wörterbuches des 20. Jahrhunderts. Das Kernkorpus besteht aus ungefähr 100 Millionen laufenden Wörtern und ist weitgehend über Zeit und vier Genres balanciert. Über die DWDS Webservices wurden Texte aller Genres extrahiert. Voraussetzung für die Durchführung einer Differenzanalyse ist die Aufbereitung des Rohmaterials. Dabei müssen insbesondere die Wortfrequenzen der zugrunde liegenden Texte extrahiert werden. Damit sind vor allem Satzsegmentierung und Tokenisierung wichtige Vorverarbeitungsschritte. Darüber hinaus ist die Nutzung eines POS-Taggers zur Generierung von Wortartinformationen für erweiterte Analysen hilfreich. Für derartige Verarbeitungen ist die bereits erwähnte verteilte Umgebung WebLicht (Hinrichs et al. 2010) ein wichtiges Hilfsmittel. Abbildung 2 stellt einen Überblick über eine WebLicht-basierte Prozesskette dar. Sie importiert die Plaintext-Dateien, konvertiert diese in ein internes Format (das Text Corpus Format TCF), extrahiert Sätze und Wörter, annotiert Wortarten und zählt die Häufigkeit aller vorkommenden Wörter. Diese Verarbeitung wurde auf der Basis der Ernst Jünger Texte für die Jahre 1919 bis 1933 durchgeführt. Als Resultat stehen die Worthäufigkeiten für jedes einzelne Jahr dieser Zeitspanne zur Verfügung. Darüber hinaus wurden die Referenztexte des DWDS in 15 Jahresscheiben zerlegt und jeweils für jedes Genre ein Teilkorpus erstellt. Diese 60 Einzelressourcen wurden anschließend mittels der bereits erläuterten Prozesskette aufbereitet. Die eigentliche Analyse wurde im Anschluss mithilfe der Webanwendung Corpus Diff Die Nutzung von Worthäufigkeitslisten hat verschiedenen Vorteile: Wortlisten sind verdichtete Repräsentationen des Inhalts eines Korpus, die aufgrund ihrer geringen Größe einfach zu verarbeiten sind. Darüber hinaus unterliegen diese Informationen keinen Einschränkungen durch das Urheberrecht, da kein Zugriff auf die eigentlichen Volltexte benötigt wird. Dies bedeutet, dass in den meisten Fällen selbst für Ressourcen mit sehr restriktiven Lizenzbedingungen ein Austausch dieser Daten unbedenklich ist. Über die Weboberfläche kann ein Nutzer alle relevanten Einstellungen vornehmen:   Auswählen einer Korpusmenge, des zu nutzenden Öhnlichkeitsmaßes und wie viele   der häufigsten Wörter für die Analyse genutzt werden sollen (s. Abbildung 3).   Als Resultat wird dem Benutzer eine Matrixdarstellung der paarweisen   Korpusähnlichkeit mit verschiedenen Farbschemata präsentiert. Diese Farbschemata   werden zur Betonung ähnlicher und somit zusammengeclusteter Korpora genutzt. Ein   Dendogram stellt darüber hinaus eine Visualisierung der Korpusähnlichkeiten auf   der Basis eines Single-Linkage-Clusterings für alle genutzten Wortlisten dar.   Beide Visualisierungen, Matrix und Dendogram, sind Mittel zur Identifikation   interessanter Korpuspaare mit ungewöhnlich hoher oder niedriger   Vokabularähnlichkeit. Die beschriebene Analyse kann genutzt werden, um eine   diachrone Analyse der Önderungen über die Zeit durchzuführen, aber auch um   Korpora unterschiedlichen Genres oder unterschiedlicher Herkunft miteinander zu   vergleichen. Durch die Auswahl zweier Korpora können detailliertere Informationen über die Unterschiede ihrer Vokabulare angezeigt werden. Dies beinhaltet vor allem auch Listen von Wörtern, die in einem der Korpora signifikant häufiger oder sogar exklusiv auftreten. Beides sind wertvolle Hilfsmittel um Wörter zu identifizieren, die spezifisch für die jeweilige Ressource sind. Darüber hinaus sind diese Ergebnisse Ausgangspunkt für tiefere hermeneutische Analysen durch die jeweiligen Fachwissenschaftler. Ist der Nutzer an einem konkreten Wort interessiert, kann die Entwicklung seiner Häufigkeit über den Untersuchungszeitraum durch ein Liniendiagramm angezeigt werden. Dies ist üblicherweise relevant für wichtige Schlüsselterme der jeweiligen Texte oder Wörter, die in den vorherigen Analyseschritten als relevant herausgearbeitet wurden. Dabei kann die diachrone Entwicklung der Nutzungshäufigkeit des Wortes über verschiedene Genres hinweg einfach dargestellt werden. Abbildung 4 (links) stellt die Öhnlichkeitsmatrix und das Dendogram für Ernst  Jüngers Texte der Jahre 1919 bis 1933 dar. Unter anderen ist hier auch das  Korpuspaar der Texte von 1920 und 1927 interessant, da hier eine besonders  geringe Öhnlichkeit vorliegt. Bei der Analyse hervorstechenden Vokabulars fällt  hier unter anderem die deutlich prominentere Nutzung des Wortes ""Feuer"" in den  Texten von 1920 auf (Abbildung 4, rechts). Das Beispiel ""Feuer"" (hier vor allem in seiner militärischen Bedeutung) zeigt die  Nützlichkeit dieser Visualisierung. Sowohl in der Verwendung durch Ernst Jünger  über 15 Jahre hinweg als auch im Vergleich mit Zeitungstexten der gleichen  Periode, können Unterschiede in dessen Verwendung identifiziert werden (s.  Abbildung 5) und sind damit ein idealer Einstiegspunkt für die tiefere Analyse  durch Fachwissenschaftler. Ein zweites Beispiel für diese Form der Analyse ist das Wort ""Krieg"", das ebenfalls eine interessante Häufigkeitsverteilung aufweist. Die Verwendung dieses Wortes reflektiert das Nachwirken und die Allgegenwärtigkeit der Kriegserfahrungen in Texten dieser Zeit. Dabei ist die relative Häufigkeit in der Publizistik Ernst Jüngers deutlich höher als in Zeitungstexten. Anhand eines konkreten Anwendungsfalls der Germanistik wurde dargestellt wie sich die Infrastrukturbestandteile zu einem umfangreichen Workflow kombinieren lassen. Dabei wurden auf der Basis verteilter Ressourcen mit Hilfe einer Metadatensuchmaschine relevante Daten und Werkzeuge identifiziert und anschließend über eine föderierte Prozesskette aufbereitet. Die Analyse dieser Daten erfolgte über eine benutzerfreundliche Weboberfläche, die auch erweiterte Visualisierungsmöglichkeiten anbietet."
2016,DHd2016,vortraege-007.xml,Play(s): Crowdbasierte Anreicherung eines literarischen Volltext-Korpus,"Mathias Göbel (Universität Göttingen, Seminar für Deutsche Philologie); Hanna-Lena Meiners (Universität Göttingen, Seminar für Deutsche Philologie)","Crowdsourcing, Gamification, TextGrid Repository, Korpora","Strukturanalyse, Annotieren, Bereinigung, Bearbeitung, Crowdsourcing, Kollaboration, Kommentierung, Webentwicklung, Literatur","Crowdsourcing, Social Editing und viele verwandte Begriffe sind Konzepte, die   innerhalb der Digital-Humanities-Community in den vergangenen Jahren einen kleinen   Hype erfahren haben. An Umsetzungen mangelt es, während man sich noch über die   Definitionen streitet. Dabei sind die Lösungsansätze sehr vielversprechend 'allen   voran die von Methodisch betrachtet bieten Konzepte, die auf der zunehmenden Beteiligung der  sogenannten Während Play(s) sich methodisch eher in der zweiten genannten Kategorie wiederfinden  soll, ist wichtig zu betonen, dass der Erfolg solcher Projekte eng mit der  Entwicklung und dem Design selbst zusammenhängt. Die Oberfläche sollte  beispielsweise ansprechend gestaltet bzw. angemessen bezogen auf die Zielgruppe und  einfach zu bedienen sein. Als ¬†Spielelemente können Levels, das Sammeln von Punkten  in Kombination mit einfachen Spielanweisungen dienen. Neben der tatsächlichen  Entwicklung einer neuen Anwendung hängt ein großer Teil des Erfolgs von der zu  tätigenden Handlung der Teilnehmer_innen ab. Die Aufgabe, die im Rahmen eines  Crowdsourcing oder Citizien-Science-Projektes von den Teilnehmer_innen bearbeitet  werden soll, ist im besten Fall einfach zu verstehen und in simple Teilbereiche  unterteilt. Gleichzeitig unterliegt die Einbindung von freiwilligen, fachfremden  Teilnehmer_innen gewissen eher impliziten und wenig ausgesprochenen Regeln. So  sollten die Teilnehmenden generell als Partner oder Mitarbeiter_innen betrachtet  werden und nicht als günstige Arbeitskräfte. Zudem sollten sie nicht zur Bewältigung  von Aufgaben angehalten werden, die eigentlich einfacher und besser von einem  Computer ausgeführt werden könnten. Diese Grundethik sollte bei jeder Umsetzung  einer neuen Idee zumindest mitbedacht werden, um künftige Teilnehmer_innen nicht zu  verärgern oder zu verschrecken. Die wenigen bisher gesammelten und verfügbaren Erfahrungen aus Projekten für die Geisteswissenschaft sollen nun ausgewertet werden und in die Umsetzung einer neuen Projektidee eingebracht werden. ""Play(s)"" ist der Name der Anwendung, die sich damit befassen soll, ein literaturwissenschaftliches Volltext-Korpus anzureichern. Das TextGrid-Repositorium bietet dafür optimale Voraussetzungen: alle Texte sind im TEI-Format erfasst und diese Quelle ist frei zugänglich. In diesem Projekt knüpfen wir an die von einer Projektgruppe (vgl. Trilcke et al. 2015) bereits herausgefilterten Dramen des Repositoriums an. Dabei wurden bereits in einem manuellen Durchgang allen Sprecherinstanzen im Auswahlkorpus eindeutige Namen (IDs) zugewiesen, um eine Ausgangsbasis für Netzwerkanalysen zu schaffen. In diesen Vorarbeiten wurden die genuin deutschen Texte ausgewählt und dabei aus den insgesamt 666 Dramen auf 465 Werke zurückgegriffen. Um diese Analysen mit einer quantitativ und qualitativ erweiterten Quellenbasis zu vertiefen, bedarf es einer noch genaueren Referenzierung. So sollten zum Beispiel die als Sprecher auftretenden Personengruppen aufgelöst werden und zu diesen die beteiligten Akteure genannt werden. Auch eine Klassifizierung des Geschlechtes, der sozialen Stellung und weitere Features sind denkbar, um differenzierte Analysen tätigen zu können. Hier wird deutlich, dass jeder Text einer bestimmten Aufbereitung bedarf, die aber in vielen kleinen Einzelschritten erfolgen kann, da die Informationen und einzelnen semantischen Anreicherungen in ihren Kategorien unabhängig voneinander sind. Innerhalb des TextGrid-Korpus beschränken wir uns auf die Betrachtung der Dramen und  innerhalb derer sind es die Strukturinformationen, die auf Grundlage des XML-Codes  Netzwerkanalysen auf Basis des gemeinsamen Auftretens in einer Szene ermöglichen.  Gemeinsames Auftreten heißt in diesem Fall, dass innerhalb einer Szene alle  Sprecher_innen in Verbindung gebracht werden. Dazu gilt es die einzelnen Akteure  ausfindig zu machen, da das Korpus selbst keine Information, wie man sie im  TEI-Attribut Die Ursache kann drucktechnisch bedingt in den Buchausgaben liegen, in denen Sprechernamen abgekürzt werden, um Platz und Papier zu sparen, es können auch schlicht Fehler im Satz auftauchen und eine weitere Fehlerquelle kann der Digitalisierungsprozess sein. In all diesen Fällen ist die Korrekturaufgabe denkbar simpel: man muss jene Sprecher zusammenführen, bei denen es sich offensichtlich um die gleiche Person handelt. Getreu der Buchausgaben handelt es sich dabei nicht um Fehler, das Encoding muss hier schlicht um semantische Information erweitert werden, wie es das Attribute who in den TEI Guidelines vorsieht. Dazu zählen auch Fälle, in denen das Markup innerhalb des TextGrid-Korpus fehlerhaft ist. Das betrifft leere speaker-Elemente, solche, in denen noch Teile der Bühnenanweisung mit einfließen und auch jene, die noch ein leeres Element stellvertretend für zum Beispiel einen Seitenumbruch beinhalten und dadurch als Auswertung des Inhaltes von tei:speaker ein Leerzeichen voran steht. Man findet außerdem bei gemeinsam sprechenden Personengruppen unterschiedliche Nennungen. In einem Drama Friedrich Kaisers ist eine solche Aggregation mit ""HELFER UND ROBERT""  benannt, später folgt aber ""ROBERT UND HELFER"". Eine bestimmte vom Autor intendierte Hierarchie soll das Datenmodell nicht abdecken und somit gilt es die verschiedenen Zeichenketten als eine Entität zu betrachten. Zudem soll die Tiefenauszeichnung dieser Elemente weiter gehen und jeder Gruppe die einzelnen, sofern bestimmbaren, Akteure zugewiesen werden. Diesen Beobachtungen folgt die Spielstruktur. In einem ersten Level gilt es die unterschiedlich benannten aber in der fiktiven Welt gleichen Sprecher zu identifizieren. Dazu werden alle unterschiedlichen Zeichenketten innerhalb der tei:speaker-Elemente eines Dramas zunächst in der Reihenfolge ihres ersten Auftretens gelistet. Mutmaßlich gleiche Namen sind nacheinander auswähl- und abspeicherbar. Ist dies für ein Drama vollständig geschehen, kann dieses Drama als ""gelöst"" markiert werden. Weiterhin gilt es Aggregationen ausfindig zu machen (Level 2). Diese Aggregationen sollen schließlich aufgelöst werden (Level 3). Dazu sind nicht nur die an einer Gruppe beteiligten Akteure zu nennen, sondern auch deren Vollständigkeit zu deklarieren. Es kann zum Beispiel das Volk sprechen und weiterhin einzelne Personen aus dem Volk auftreten. Diese sind Teil des Volkes, die Gruppe selbst ist aber eine weitaus größere und daher unvollständig durch die einzelnen Akteure belegt. Sprechen zwei auch näher bestimmte Einzelpersonen gemeinsam, so kann diese Gruppe vollständig aufgelöst werden. Die Geschlechter der Akteure sind in Level 4 zu bestimmen. Dabei ist zu wählen aus male, female, both, und unknown. Die letzte Gruppe umfasst dann schließlich auch metaphysische Konstrukte, die personifiziert auftreten. In Level 5 sollen diese dann genauer spezifiziert werden. Dabei stehen die Kategorien Tier, metaphysisches Wesen (z. B. Gottheit, Hexen und Magier) und Eigenschaft / Gefühl / Moral zur Auswahl. Schließlich lässt sich noch der soziale Status bestimmen, sofern Berufsbezeichnungen, Adelstitel oder andere Indikatoren ausfindig zu machen sind. Zwischen den einzelnen Levels gilt es die Eingaben anderer Spieler zu verifizieren oder auch zu falsifizieren. Diese Eingabe wirkt sich auf die eigenen Punkte immer positiv aus, die jeweils anonym bleibende begutachtete Spielerin wird bei Fehleingaben aber Punktabzüge bekommen. Da die Dramen immer zufällig gewählt werden und auch mehrfach erfasst werden, stehen damit verschiedene Qualitätskontrollen zur Auswahl, die auch kontinuierliche nicht sinnvolle Eingaben erkennen lassen. Die betreffenden Spielerinnen können weiterspielen, finden aber nur noch eine persönliche Highscoreliste vor, während sie aus den Highscorelisten anderer getilgt werden und ihre Eingaben auch nicht in den weiteren Forschungsprozess Einzug halten. Zudem stehen die Daten für 465 Dramen im Kritisch betrachtet stammen aus der Welt der Computerspiele die Levelstruktur und einzelne Elemente, wie Avatar und Highscoreliste. Tatsächlich ist das Angebot eines, das Social Editing auf einfachste Fragestellungen hin anwendet und jeder Spielerin die Möglichkeit bietet, aktiv an der Tiefenerschließung von literarischen Texten mitzuwirken. Außerdem gibt es einen didaktischen Aspekt, da komplexe Probleme im Hinblick auf Korpuserstellung implizit aufgezeigt werden."
2016,DHd2016,vortraege-051.xml,Der falsche Quijote? Autorschaftsattribution für spanische Prosa der frühen Neuzeit.,"Nanette Rißler-Pipka (Universität Siegen, Deutschland)","Stilometrie, Autorschaftsattribution, Spanien, Cervantes, Don Quijote","Kontextsetzung, Bewertung, Stilistische Analyse, Daten, Forschung, Text","Das bis heute bekannteste Werk der spanischen Literatur ist weltweit als ""Don Quijote"" von Miguel de Cervantes geläufig. Dass Cervantes aber, wie kurz zuvor Mateo Alem√°n, mit einem Fälscher zu kämpfen hatte, der den ersten Band des ""Quijote"" ungefragt weiter dichtete und ein Jahr vor der eigenen Fortsetzung durch Cervantes einen zweiten Band aus eigener Feder unter dem Namen Alonso Fern√°ndez de Avellaneda heraus brachte, wissen die wenigsten Leser des ""Quijote"". Es ist im strengen Sinne des Wortes auch keine Fälschung oder ein Plagiat, sondern die freie Fortsetzung eines erfolgreichen Romans unter eigenem Namen bzw. in diesem Fall unter Pseudonym. Die Identität Avellanedas ist bis heute unbekannt. Angesichts der historischen Publikationsbedingungen in Spanien der frühen Neuzeit  (Chartier 2006), stellt sich die Frage, ob eine Autorschaftsattribution mithilfe  stilometrischer Methoden auf der Grundlage aktuell zugänglicher digitalisierter  Buchausgaben überhaupt möglich ist. Doch auch Patrick Juola und Christopher  Coufal nahmen 2010 die Ausgabe des ""Don Quijote"", die im Project Gutenberg  zugänglich ist als Grundlage ihrer Analyse, die als Ergebnis hatte, dass  Cervantes nicht der Autor der letzten 69 (von 74) Kapitel des 2. Bandes des ""Don  Quijote"" sei (Coufal / Juola 2010). Aus dieser provokanten These entwickelte  sich aber keine wissenschaftliche Debatte innerhalb der internationalen  Hispanistik und auch die DH-Experten Juola und Coufal erweiterten ihre  Fragestellung nicht auf die sich unmittelbar anschließende Frage, wer der Autor  des ""falschen"" Quijotes von Avellaneda sei. Auch eine weitere statistische  Untersuchung der beiden Teile des ""Quijote"" von Cervantes berücksichtigt nicht  Avellaneda (López Quintero 2011). Die Tatsache, dass sich der Stil Cervantes'  innerhalb des zweiten Teils des ""Quijote"" ändert, ist in der Hispanistik  anerkannt und wird im Allgemeinen mit dem Erscheinen der apokryphen Fortsetzung  durch Avellaneda in kausalen Zusammenhang gebracht (Strosetzki 1991: 93;  Ehrlicher 2008: 42ff.; Blasco 2007: XVII; Gómez Canseco 2008). Bislang wurde  jedoch der Stil Cervantes' gerade im Vergleich zu seinem Nachahmer Avellaneda  zumeist als in jeder Hinsicht überragend dargestellt (vgl. zu einer kompakten  Darstellung dieser Missachtung Avellanedas: Alvarez Roblin 2014). Erst durch die  jüngste Reihe von neuen Ausgaben der Avellaneda-Fortsetzung wird dessen  literarische Leistung in der Fachwelt anerkannt (Gómez Canseco 2014; Alvarez  Roblin 2009; Su√°rez Figaredo 2014; López-V√°zquez 2011). Mit dem wachsenden  Interesse an Avellaneda nimmt auch die Suche nach dessen Identität mithilfe  digitalisierter Korpora zu. Zum größten Teil stützen sich diese  Autorschaftsattributionen auf schlichte Recherchemöglichkeiten von CORDE (Corpus  diacrónica del espa√±ol), CREA (Corpus de Referencia del Espa√±ol Actual) und  GoogleBooks (Su√°rez Figaredo 2011; Madrigal 2009; López-V√°zquez 2011; Blasco  2005; Jiménez 2007) oder auf philologisch-historische Recherchen (Cruz Casado  2008; S√°nchez Portero 2006). Insgesamt werden im Laufe der Recherche nach der  wahren Identität Avellanedas 39 Namen ins Spiel gebracht. Von 18 dieser  Kandidaten liegen digitalisierte Texte frei zugänglich vor. Dennoch nutzt keiner  der Autorschaftsdetektive aktuelle Methoden der DH zur Autorschaftsattribution  (wie z. B. JGAAP oder Stilometrie mit R; vgl. Juola 2012; Eder 2015). Neben der  Autorschaftsattribution bzgl. Avellanedas apokrypher Fortsetzung des ""Quijote"",  stellt die Hauptfrage dieses Papers, die stilistische und stilometrische  Unterscheidung zwischen Cervantes und Avellaneda dar. Es kann gezeigt werden,  dass nur mithilfe stilometrischer Methoden, die festgefahrene Fachdiskussion  neue Perspektiven und unerwartete Ergebnisse erhält. Mithilfe des stylo-Pakets für R (Eder / Rybicki 2011), das für spanischsprachige Texte noch vergleichsweise wenig getestet wurde, sollen zum einen die vorliegenden Autorschaftsattributionen falsifiziert und die Fragen nach stilistischer Nähe zwischen Cervantes, Avellaneda und anderen zeitgenössischen Autoren spanischer Prosa geklärt werden. Somit wird auch die Methode hinlänglich ihrer Komptabilität mit spanischsprachigem Korpus überprüft. Dazu konnte die neueste Version des stylo-package (0.6.0) und die von Jannidis et al. (2015) vorgestellte Cosine Distance genutzt werden. Zu diesem Zweck wurde zunächst ein passendes Korpus erstellt, das repräsentativ  für die Zeit von 1585-1630 spanische Prosawerke enthält und sich auch aus den  Kandidaten für die Autorschaft des apokryphen ""Quijote"" zusammensetzt. Die Texte  stammen aus digitalen Editionen von cervantesvirtual, Wikisource und Project  Gutenberg. Sie wurden einheitlich in plain-text-Format abgespeichert und von  Textteilen, die nicht von selben Autor stammen (wie z. B. einleitende  Bemerkungen des Herausgebers) befreit. Um zunächst ein sicheres Set zu haben, das sowohl die Autorschaft als auch Genre und Epoche betreffend vergleichbar ist, wurden zunächst nur 4 Autoren mit unterschiedlich vielen Texten ausgewählt (insgesamt 32 Texte). Eine Vergleichbarkeit die Textlänge betreffend hätte bedeutet entweder nur die Novellen oder nur die Romane miteinander zu vergleichen. Da es auch mit unterschiedlicher Textlänge zu guten Ergebnissen kam, wurde auf diese Angleichung verzichtet (vgl. Eder 2010). Das Korpus wurde mit zwei verschiedenen und anerkannten Distanzmaßen (Eder""s Delta und Cosine) und 100-5000 MFW als Cluster-Analyse ausgewertet. Schrittweise wurden dann weitere Kandidaten hinzugefügt und die Ergebnisse bewertet. Die Problematik, die sich dabei ergab, war, dass es wenig Sinn macht, Autoren mit ins Korpus zu nehmen, von denen nur ein Textbeispiel vorliegt, da diese keinem ""Partner"" im Dendrogramm zugeordnet werden können und somit fälschlicherweise eigentlich weiter voneinander entfernte Texte zusammen geclustert dargestellt werden. Um dieses Problem der Cluster-Analyse zu umgehen, wurde im Vergleich eine Principle Component Analysis (PCA) durchgeführt, die eine bessere Darstellung der Distanzen zwischen den einzelnen Texten im Raum zeigt. Im ersten Korpus mit 32 Texten funktioniert die Zuordnung sehr gut. Die Cervantes-Texte sind trotz ihrer starken Größenunterschiede (Novellen mit ca. 7000 und Romane mit ca. 200.000 Wörtern) klar zusammen geclustert. Ab 900 MFW und darunter ist die Autorschaftszuordnung mit Eder""s Delta einwandfrei, bei darüber liegenden Zahlen schlich sich beständig der Nachahmer-""Lazarillo"" von Cortés de Tolosa und auch der Block der Werke von Castillo Slórzano zwischen die Cervantes-Werke. Zum Vergleich wurde der gleiche Versuch mit Cosine Distance durchgeführt und brachte keine größere Veränderung in der Darstellung. Weiterhin blieb der Roman von Cortés de Tolosa hartnäckig bis zum 300 MFW unter den Cervantes-Werken, jedoch konnte Castillo Solórzano früher heraus genommen werden. Interessant ist hier, dass die PCA mit denselben Variablen zeigt, wie weit entfernt der ""Lazarillo"" von Cortés de Tolosa doch von den übrigen Werken Cervantes"" entfernt ist (und zwar bei allen 100-5000 MFW): Deutlich wird vor allem, dass der ""Lazarillo"", den Cortés Tolosa in Nachahmung des Originals (anonym 1554) geschrieben hat, sehr von seinem übrigen Werk abweicht, aber ebenso noch deutlich von den Cervantes-Werken entfernt ist. Das erklärt sich allerdings leicht durch die sprachliche Unterscheidung der vorliegenden Edition des Werkes, die eine ältere Form des Kastilischen verwendet und daher in der Vergleichbarkeit eingeschränkt bleibt. Spannender werden die Ergebnisse, wenn man Avellaneda und weitere Kandidaten hinzunimmt: Auch mit der Korpuserweiterung bleiben die Zuordnungen relativ stabil (außer Cortés de Tolosa, wie zuvor). Keiner der möglichen Kandidaten wird jedoch Avellaneda zugeordnet, sondern der apokryphe ""Quijote"" wird mit den anderen beiden Teilen von Cervantes zusammen geclustert 'dies bleibt über 100-5000 MFW konstant. Auch mit dem einer PCA bestätigt sich dieses Ergebnis (vgl. Fig. 8: der blaue Punkt bei den schwarzen zeigt Avellaneda im Umkreis der Cervantes-Werke): In einer nächsten Korpuserweiterung nehmen wir weitere Kandidaten mit Prosabeispielen hinzu, von denen nur ein Text zur Verfügung steht. Es wird schnell deutlich, dass die Einzelgänger unter den Texten, das zuvor stabile Gefüge auseinanderbringen und offensichtlich nicht für die statistische Untersuchung zu gebrauchen sind. Erstaunlich bleibt jedoch, dass selbst in diesem sehr vagen Clustering (je nach MFW schieben sich die einzelnen Texte mal dort mal dort hin), die drei ""Quijote""-Texte konsistent bei 100-5000 MFW ein Cluster bilden. Kein anderer Kandidat schafft es in die Nähe von Avellaneda. Auch die PCA bringt keine entscheidenden Vorteile gegenüber der Cluster Analysis. Espinel und √öbeda gruppieren sich zwar in die Cervantes-Gruppe, aber nicht direkt zu Avellaneda. Wenn überhaupt ein Autorname anstelle desjenigen Avellanedas genannt werden  sollte, müsste es nach den vorliegenden Ergebnissen Cervantes selbst sein, der  in einem gekonnten Spaß ganz im Sinne seines Quijote die Leser mit der eigenen  falschen Fortsetzung an der Nase herum führt. Oder aber beide Autoren haben  ihren Stil gegenseitig aufeinander abgestimmt, dass sie sich derart im  Wortgebrauch ähneln. Es würde sich als folgende Untersuchung ein rolling delta  anbieten, um eine kollaborative Autorschaft ausmachen zu können und um den  vorgeblichen Stilwechsel im zweiten Teil von Cervantes' ""Quijote"" genauer  definieren und mit Avellaneda in Zusammenhang bringen zu können."
2016,DHd2016,posters-040.xml,Entwicklung einer digitalen Brief-Edition und eines Forschungsportals zu Theodor Fontane,"Sabine Seifert (Theodor-Fontane-Archiv, Universität Potsdam, Deutschland)","digitale Edition, Forschungsportal, Datenmodellierung, Handschriften, Briefe","Sammlung, Transkription, Inhaltsanalyse, Annotieren, Archivierung, Veröffentlichung, Kommentierung, Daten, Literatur, Manuskript, Metadaten, virtuelle Forschungsumgebungen","Das Theodor-Fontane-Archiv konzipiert eine digitale, kritische Edition aller Briefe von und an Theodor Fontane. Hierbei handelt es sich um etwa 10.000 Briefe, die bislang unvollständig und nach heutigen editionswissenschaftlichen Standards unzureichend veröffentlicht sind. Ein Großteil des Briefnachlasses befindet sich im Archiv, doch sind weitere Bestände aus anderen Institutionen und Privatbesitz zu berücksichtigen. Somit wird der gesamte, stark verstreute Briefnachlass virtuell zusammengeführt und erstmalig als ein Korpus recherchierbar, wodurch die Voraussetzung für die systematische Erforschung dieses zentralen Werkbestandes geschaffen wird. Verschiedene editorische Herausforderungen stellen sich bezüglich der Briefe.  Neben den Originalhandschriften sind mitunter mehrfache Abschriften sowie  bisherige Drucke einzubeziehen. Somit werden mehrere Überlieferungsträger eines  Briefes verzeichnet und in der Edition präsentiert, wodurch sich die Frage einer  sinnvollen Darstellung von Überlieferungsvarianten stellt. Daneben muss eine  Lösung für die digitale Umsetzung materialspezifischer Besonderheiten gefunden  werden, z. B. die häufig beschriebenen Briefränder. Diese Textbestandteile sind  oft seitenübergreifend und stehen in den unterschiedlichsten Winkeln zur  regulären Beschreibrichtung. In der Online-Präsentation werden neben den  Digitalisaten und Transkriptionen die Metadaten, die Kommentierung sowie die XML  / TEI P5-Auszeichnung verfügbar gemacht. Für die individuelle Benutzbarkeit soll  die Anzeige der genannten Daten flexibel anzupassen sein. Aufgrund der  besonderen Schriftbildlichkeit müssen die Digitalisate drehbar und im Falle von  mehreren Textzeugen soll deren parallele Anzeige möglich sein. Auf die  Verwendung von Standards (z. B. XML / TEI P5) und Normdaten (z. B. GND für  Personen) und die Erstellung von projektinternen Indizes (zu Personen,  Institutionen, Orten, Werken, Periodika) wird besonderer Wert gelegt. So können  etwa personelle und institutionelle Netzwerke, an denen Fontane teilhatte,  rekonstruiert und intertextuelle Verbindungen nachvollzogen werden. Die Edition der Briefe steht im Zusammenhang mit dem ebenfalls in der  konzeptionellen Entwicklung befindlichen Fontane-Forschungsportal. Dessen Ziel  ist die Präsentation der Digitalen Sammlungen des Archivs und optional anderer  Bestandshalter. Die aufbewahrten Handschriften liegen digitalisiert vor, die  Verknüpfung der Digitalisate mit dem technischen und bibliographischen  Metadatensatz wird über den METS / MODS- bzw. METS / EAD-Standard erfolgen. Die  Handschriften- und Bibliothekskataloge des Archivs, bisher intern als allegro-C-  und allegro-HANS-Datenbanken geführt, werden ebenfalls über das Portal als OPACs  zugänglich gemacht. Somit stellt das Portal die Verknüpfung von archivalischen  Quellen- und Erschließungsdaten und von Forschungsprimärdaten her. Neben der virtuellen Zusammenführung des zerstreuten Nachlasses ist das zweite Ziel des Portals, alle Forschungsressourcen zu Fontane schnell zugänglich bereitzustellen. Die Fontane-Aktivitäten außerhalb des Archivs sollen hier gebündelt werden und das Portal als Kommunikationsplattform für die verschiedenen Akteure dienen. Dafür werden technische Schnittstellen für Datenaustausch sowie Kooperationen mit anderen Projekten geschaffen. Doch richtet sich das Portal nicht nur an die Wissenschaft, sondern auch an die breite Öffentlichkeit, der hier ein umfassender Zugang zu Fontane, seinem Leben und Werk ermöglicht werden soll. Die technische Umsetzung der digitalen Edition und des Forschungsportals wird im Rahmen der virtuellen Forschungsumgebung FuD (""Forschungsnetzwerk und Datenbanksystem"") erfolgen, die am Kompetenzzentrum für elektronische Erschließungs- und Publikationsverfahren in den Geisteswissenschaften an der Universität Trier entwickelt wurde. Die in FuD bereitgestellten Tools werden an die projektspezifischen Bedürfnisse angepasst und weiterentwickelt, etwa für die Verzeichnung unterschiedlicher Textzeugen. Diese Weiterentwicklungen sollen von anderen Institutionen nachgenutzt werden können. Im Hintergrund stehen eine Neustrukturierung der gesamten Datenstruktur des Theodor-Fontane-Archivs und die Überführung in ein Content Management System, das ebenfalls in Zusammenarbeit mit dem Kompetenzzentrum Trier erarbeitet wird. Die Datenmodellierung des CMS, auf das Edition und Portal gleichermaßen zugreifen, muss gewährleisten, dass unterschiedliche Arten von Daten in FuD zusammengeführt, angereichert und weiterverarbeitet werden können. Hierzu gehören bibliographische Metadaten von Handschriften und Bibliotheksbeständen des Archivs, bibliographische Metadaten anderer Institutionen, Digitalisate plus deren Metadaten, Normdaten, die Transkriptionen der Edition, die TEI-Textauszeichnung, editorischen Kommentare und die erstellten Indizes. Auch werden Schnittstellen zu anderen Projekten und zu Archivdatenbanken hergestellt und der Zugang per Open Access gewährleistet. Die digitale Edition und das Forschungsportal werden auf Grundlage der gemeinsamen Datenbasis so konzipiert und miteinander verbunden, dass zwischen ihnen Datenaustausch möglich wird. So wird der jeweilige Rechercherahmen erweitert, eine umfassendere Kontextualisierung der Informationen erreicht und eine digitale Arbeits- und Forschungsumgebung geschaffen, die den Erfordernissen eines Archivs in seinen Aufgaben des Sammelns, Erschließens und Langzeitarchivierens sowie den Erfordernissen einer Forschungseinrichtung gleichermaßen gerecht wird. Dies und die Vernetzung mit verschiedenen Forschungsvorhaben ermöglicht nicht nur neue Erkenntnisse für die Fontane-Forschung selbst, sondern auch zur Literatur- und Geistesgeschichte des 19. Jahrhunderts. Für weitere, interdisziplinäre und neue Fragestellungen wird die Struktur offen und flexibel angelegt. Zudem soll die entwickelte Arbeitsumgebung in ihrer Datenstruktur auch Modellcharakter für kleinere Archive, Museen und Sammlungen haben. Im Poster werden der derzeitige Konzeptionsstand von Edition und Portal vorgestellt, Probleme und offene Fragen aufgezeigt sowie Lösungsansätze zur Diskussion gestellt."
2016,DHd2016,vortraege-055.xml,"Topic, Genre, Text","Christof Schöch (Universität Würzburg, Deutschland); Ulrike Henny (Universität Würzburg, Deutschland); José Calvo (Universität Würzburg, Deutschland); Daniel Schlör (Universität Würzburg, Deutschland); Stefanie Popp (Universität Würzburg, Deutschland)","Topic Modeling, Spanische Literatur, Lateinamerikanische Literatur, Textverlauf","Programmierung, Inhaltsanalyse, Annotieren, Visualisierung, Literatur, Text","Der Beitrag möchte zeigen, wie die Berücksichtigung detaillierter,  gattungsbezogener Metadaten auf produktive Weise mit dem Verfahren des Topic  Modeling verbunden werden kann, um bisher nicht bekannte thematische Strukturen  im Textverlauf in einer Sammlung spanischer und hispanoamerikanischer Romane zu  entdecken. Ausgangshypothese ist, dass die Wichtigkeit bestimmter Topics nicht  nur im Textverlauf variiert, sondern dies auch in verschiedenen Untergattungen  auf unterschiedliche Weise tut. Eine Pilotstudie wurde im März 2015 beim  Workshop zu Computational Narratology bei der DHd-Tagung in Graz vorgestellt. Im  Rahmen der interdisziplinären Würzburger eHumanities-Nachwuchsgruppe  ""Computergestützte literarische Gattungsstilistik ( Die Frage nach dem Text- oder Handlungsverlauf in narrativen literarischen Texten hat jüngst zunehmende Aufmerksamkeit in der digitalen Literaturwissenschaft erhalten. Matthew Jockers kam durch Sentiment Analysis im Verlauf zahlreicher Romane zu dem (kontrovers diskutierten) Ergebnis, es gäbe sechs oder sieben grundlegende Plotstrukturen (Jockers 2015). Ben Schmidt hat unter anderem den Verlauf von Topic-Wahrscheinlichkeiten in der ""screen time"" amerikanischer Fernsehserien verfolgt (Schmidt 2014). Der vorliegende Beitrag verbindet die Frage nach dem Textverlauf mit der nach den Untergattungen, seine zentrale Fragestellung lautet: Können wir nach Untergattung unterschiedliche Verlaufsmuster für bestimmte Topics über den Textverlauf hinweg feststellen? Die Textsammlung enthält 150 spanische und hispanoamerikanische Romantexte aus  der Zeit von 1880 bis 1930 (für den spanischen Roman: Altisent 2008; de Nora  1963, für den hispanoamerikanischen Roman: Gallo 1981; Williams 2009). Die Texte  sind in TEI aufbereitet und mit detaillierten Metadaten versehen worden. Es  wurden vier weit gefasste Untergattungen gewählt, um die Romane miteinander  vergleichen zu können: Topic Modeling ist eine unüberwachte, nicht-deterministische Methode aus dem  Bereich des Hier wurde Topic Modeling als Teil eines umfassenden, weitgehend automatischen Arbeitsablaufes als Serie von Python-Skripten implementiert: Präprozessieren der Texte (Segmentierung, Binning, Lemmatisierung, POS-Tagging), das eigentliche Topic Modeling (mit Mallet, siehe McCallum 2002), Aufbereitung des Mallet-Outputs, zahlreiche Visualisierungen als Perspektiven auf die Ergebnisse. Die wichtigsten Parameter: Berücksichtigung ausschließlich der Substantive, Weglassung der 70 häufigsten Substantive, Romansegmente von ca. 600 Wörtern (unter Berücksichtigung von Absatzgrenzen), Anzahl von 70 Topics. Die Python-Skripte sind frei verfügbar und ausführlich dokumentiert, Begleitmaterialien (Skripte, Parameterdatei, Metadaten, Abbildungen) sind unter Es werden zunächst die Topics selbst dargestellt, dann Unterschiede in den Topic-Verteilungen nach Untergattungen, über den Textverlauf hinweg und schließlich über den Textverlauf in Abhängigkeit der Untergattung. Die Mehrheit der erhobenen Topics beinhaltet konkrete typische Themen und Motive des spanischsprachigen Romans der Epoche. Man erkennt eine klare semantische Beziehung der Wörter: ein konkreter Bereich menschlicher Tätigkeiten, wie in Topic 19 (maestro-colegi o-escuela, dt. ""Lehrer-Schule-Schule"") oder Topic 23 (sangre-golpe-arma, dt. ""Blut-Schlag-Waffe""); oder abstrakte Begriffe und Gefühle, wie bei Topic 69 (conciencia-honor-crimen, dt. ""Gewissen-Ehre-Verbrechen""). Weniger kohärent ist Topic 45 (marido-rato-chico, dt. ""Ehegatte-Weile-Junge""). Die folgenden Wordclouds (Abbildung 2) veranschaulichen die erwähnten Topics. Die folgende Heatmap (Abbildung 3) zeigt die Verteilung der durchschnittlichen Topic-Wahrscheinlichkeiten in den vier Untergattungen für diejenigen 20 Topics, deren Werte zwischen den Untergattungen besonders stark schwanken (nach Standardabweichung). Besonders distinktive Topics existieren für die Die Ausprägung der Topics variiert nicht nur hinsichtlich der Untergattungen,   sondern auch über den Textverlauf hinweg. So gibt es einige Topics, deren   Vorkommen am Anfang der Romane besonders wahrscheinlich ist (Abbildung 4a).   Dazu zählen Topic 10 (vino-plato-pan, dt. ""Wein-Teller-Brot""), Topic 17   (sombrero-ropa-bota, dt. ""Hut-Kleidung-Stiefel"") und Topic 19, welche auf   die Beschreibung von Ambiente, Situation und Personen hindeuten. Gegen Ende   der Romane sind andere Topics wahrscheinlicher (Abbildung 4b), z. B. Topic 2   (pecado-caridad-conciencia, dt. ""Sünde-Wohltätigkeit-Gewissen""), Topic 23   und Topic 69, also abstraktere Themen oder solche, die sich auf   Wertvorstellungen beziehen. Dies deutet darauf hin, dass in den Romanen am   Ende Bilanz gezogen wird, die Handlung einen drastischen Ausgang nimmt oder   das im Textverlauf Behandelte in gesellschaftliche oder religiöse Diskurse   eingebunden wird. Für einige der genannten Topics, die in bestimmten Bereichen des Textverlaufs  wahrscheinlicher sind, kann die Tendenz über alle Untergattungen hinweg  bestätigt werden (bspw. bei Topic 10 und 17, siehe oben). Es gibt aber auch  Themen, bei denen sich durch die Betrachtung des Verlaufs in den einzelnen  Untergattungen ein differenzierteres Bild ergibt. Die Wahrscheinlichkeit von  Topic 23 beispielsweise nimmt nur für die Das kann so interpretiert werden, dass die Allgemein gilt, dass die Untergattungen sich in ihrer Topicverteilung im Textverlauf auch dann deutlich unterscheiden können, wenn dies für alle Untergattungen zusammengenommen nicht der Fall ist und so leicht übersehen werden könnte. Für die Berechnung wurden die Romansegmente von 600 Wörtern bezüglich des Textverlaufs auf 15 Romanabschnitte (Bins) verteilt, um die unterschiedliche Romanlänge zu berücksichtigen. Diese Bins wurden hinsichtlich der Untergattung gruppiert und jeweils das arithmetische Mittel bestimmt. Die in den Plots eingezeichneten Kurven entsprechen der linearen Interpolation dieser gemittelten Werte. Zusätzlich wurde der Standardfehler vertikal um den jeweiligen Kurvenpunkt eingezeichnet, der deutlich macht, wie sehr die jeweiligen dem Mittelwert zugrunde liegenden Werte streuen, also wie gut der Mittelwert die Gesamtheit der Segmentwerte repräsentiert. Insgesamt zeigen sich verschiedene Zusammenhänge: Zwischen bestimmten Topics und einzelnen Roman-Untergattungen, zwischen Topics und dem Textverlauf, und dies zum Teil dann auch wieder in Abhängigkeit von den Untergattungen. Aus literaturgeschichtlicher Perspektive betrachtet erweisen sich die in die Untersuchung einbezogenen Metadaten für eine Einordnung der Topic-Resultate als nützlich. Topics sind für die Romangattungen im vorliegenden Korpus ein wichtiger Faktor, ähnlich wie dies für Gattungen wie die klassische Komödie und Tragödie bereits gezeigt werden konnte (Schöch 2015). Ein detaillierterer Blick zeigt beispielsweise Folgendes: Topic 11, welches typisch für die Die Nutzung von Topic Modeling als Methode kann für die digitale Literaturwissenschaft verbessert werden, wenn spezifisch literaturwissenschaftliche Metadaten in die Betrachtungen einbezogen werden und die Textstruktur - hier als Sequenz von Textverlaufseinheiten - berücksichtigt wird. Verschiedene Visualisierungsstrategien erweisen sich als entscheidende ""Interfaces"" zu den Daten (im Sinne von Doueihi 2012), die Muster sichtbar machen und den Blick lenken. Die Ergebnisse des Topic Modelings können differenzierter und aus verschiedenen Perspektiven betrachtet und mit literaturhistorischem Wissen in Verbindung gebracht werden. Die Ergebnisse ergänzen und erweitern etablierte hermeneutische Lektürestrategien, insofern sie einen synthetisierenden Blick auf sehr umfangreiche Textsammlungen erlauben. Nächste Schritte betreffen insbesondere die weitere Auseinandersetzung mit der  Signifikanz von Unterschieden in den Topic-Wahrscheinlichkeiten im Textverlauf,  deren Berechnung u. a. durch die mangelnde Normalverteilung der Werte nicht  trivial ist. Zusätzlich zu den Untergattungen sollen auch Kategorien wie das  Setting modelliert werden. Zudem sollen die Textverlaufs-Daten für die  automatische Klassifikation von Romanen nach Untergattungen genutzt werden.  Schließlich wird bereits an der Erweiterung der Textsammlung gearbeitet,  insbesondere mit Blick auf den Umfang und ein ausgeglicheneres Verhältnis der  Untergattungen."
2016,DHd2016,posters-017.xml,: aichinger,"Mathias Mueller (ÖAW, Österreich); Andreas Dittrich (ÖAW, Österreich); Gilbert Waltl (ÖAW, Österreich); Marlene Csillag (ÖAW, Österreich); Katharina Godler (ÖAW, Österreich); Christine Ivanovic (ÖAW, Österreich)","Raumbezüge, Textkorpus, Ortsannotationen, Wientopographie","Datenerkennung, Bilderfassung, Gestaltung, Programmierung, Räumliche Analyse, Modellierung, Annotieren, Visualisierung, Karte, Visualisierung","Der Fokus der Untersuchung liegt auf der literarischen Repräsentation von Raum.  Bisherige Untersuchungen ihres Werks haben erwiesen, dass Aichingers Bezugnahmen  auf Orte und Ereignisse in Wien zentrale Bedeutung zukommt (Fässler 2013). Dabei  fällt auf, dass Aichinger Raumbezüge in verschiedenen Phasen ihres Werks auf  ganz unterschiedliche Weise elaboriert: Der Wienbezug ihres ersten Romans, Textgrundlage ist die achtbändige Ausgabe der Werke Ilse Aichingers (S.  Fischer Verlag 1991) sowie die danach erschienenen Einzelbände. Diese Bände  wurden gescannt und mittels OCR (Optical Character Recognition) erfasst und  dadurch maschinenlesbar gemacht. Als Vergleichskorpora sollen zusätzlich die  davon abweichenden Textfassungen der Erstausgabe des Romans sowie der  zwischen 2000 und 2004 in Tageszeitungen publizierten Texte erfasst  werden. Im zweiten Arbeitsschritt wird eine TEI-konforme Datei erstellt, in der die Texte mithilfe von Standards wie RDF (Resource Description Framework), XML (Extensible Markup Language) und PoS (Part-of-Speech-Tagging) codiert und dadurch der maschinellen Abfrage durch Abfragesprachen wie SPARQL (SPARQL Protocol And RDF Query Language) zugänglich gemacht werden. Im Hinblick auf den primären Fokus der Untersuchung, die Erfassung und Analyse der literarischen Topographien Aichingers, werden vorrangig Personennamen sowie Orts- und Zeitangaben kodiert. Außerdem ist eine Analyse anhand semantischer Felder geplant, wofür eine Vernetzung mit unterschiedlichen Datenbanken (z. B. Dornseiff) vorgesehen ist. Von vornherein soll so gearbeitet werden, dass die Möglichkeit weitere bzw. speziellere Codierungen zu ergänzen offen bleibt. Ergänzend zur digitalen Erfassung und Erschließung der Texte werden weitere Metadaten eingebracht. Dies können textgenetisch relevante Daten sein wie Entstehungs- und Publikationsdaten, oder Sacherläuterungen, wie sie in Apparaten wissenschaftlicher Editionen oder Kommentaren üblich sind, sowie Hinweise auf Varianten, Querverweise, Illustrationen etc. Ein Teil dieser Daten ist durch Recherchen in Wiener Archiven oder am Aichinger-Vorlass im Deutschen Literaturarchiv (DLA) in Marbach zu erheben. Die Auszeichnung durch RDF ermöglicht aber auch die Verlinkung mit online Datenbanken und damit den Anschluss an das semantic web (Hitzler et al. 2008; Ivanovic / Frank 2015). Das Textkorpus :aichinger soll die Basis bilden für die Durchführung von Abfragen und Analysen, die eine präzise, systematische und vollständige Evaluierung der Raumbezugnahmen im Gesamtwerk der Autorin ermöglichen soll. Erfassbar werden dadurch beispielsweise Personenkonstellationen in Verbindung mit Orten sowie Frequenzen der Nennung bestimmter Orte resp. Wege in Korrelation zur beschriebenen Zeit wie zur Zeit der Textabfassung. Diese Ergebnisse verlangen unterschiedliche Darstellungsformen. So sind Diagramme möglich, oder Wordclouds, die wiederum Häufungen oder Übereinstimmungen bzw. Korrelationen darstellen können. Auf der Basis der RDF Codierung lassen sich z. B. maschinell Karten generieren, in denen die erwähnten Orte oder Wege der in den Texten genannten Figuren u. a. aufscheinen. Die kartographische Darstellung ermöglicht es darüber hinaus Leerstellen ihres Werkes (nie genannte Orte oder Zonen) oder verdeckte Strukturen (die Wientopographie, die der für Aichinger maßgebliche Film Der Dritte Mann darstellt) sichtbar zu machen. Insbesondere anhand solcher Karten wird das Poster das Konzept und die Analysemöglichkeiten unseres Projektes darstellen. Das Projekt dient der Sichtbarmachung und besseren Analyse der raumrelevanten Strukturen im Werk Aichingers und deren Relevanz für die erinnerungskulturell motivierte Schreibweise, der die Autorin verpflichtet ist. Das Projekt hat insofern paradigmatischen Charakter, als die an diesem Beispiel entwickelten Methoden den Status eines Prototyps haben und auch bei der Analyse anderer Textkorpora Anwendung finden sollen."
2016,DHd2016,posters-014.xml,Distant-Reading-Showcase: 200 Jahre deutscher Dramengeschichte auf einen Blick,Frank Fischer (Niedersächsische Staats- und Universitätsbibliothek Göttingen); Andreas Vogel (Universität Leipzig); Mathias Göbel (Niedersächsische Staats- und Universitätsbibliothek Göttingen); Peer Trilcke (Georg-August-Universität Göttingen); Dario Kampkaspar (Herzog August Bibliothek Wolfenbüttel); Christopher Kittel (Karl-Franzens-Universität Graz),"Distant Reading, Literaturgeschichte, Dramatik, Netzwerkanalyse","Datenerkennung, Gestaltung, Netzwerkanalyse, Visualisierung, Literatur, Visualisierung","Der Terminus 'Distant Reading' wurde im Jahr 2000 von Franco Moretti geprägt. Ebenfalls auf Moretti zurück geht das Konzept der ""Maps, Graphs, Trees"" (2005), die Nutzbarmachung verschiedener Visualisierungsmethoden für literaturhistorische Daten. Die Beispiele für die Zusammenführung von Distant-Reading- und Visualisierungs-Ansätzen haben als Hintergrund jedoch fast ausschließlich englischsprachige Korpora. Im Mittelpunkt unseres Distant-Reading-Showcase-Posters steht nun mit der ""Digital Bibliothek"" das größte deutschsprachige Korpus literarischer Texte. Auf einem einzigen A0-Poster werden die Figurennetzwerke von 465 deutschsprachigen Dramen aus den Jahren 1730 bis 1930 gezeigt. Dabei werden verschiedene semantischen Dimensionen vereint. Zum einen folgt die Positionierung der Dramen chronologisch. Neuralgische Punkte der deutschen Literaturgeschichte werden sofort sichtbar, etwa die Explosion des Figurennetzwerks in Goethes ""Götz von Berlichingen"" von 1773. Das bekannte Faktum der damals einsetzenden verstärkten Shakespeare-Lektüre wird so auf einen Blick erkennbar und erscheint im Kontext des zeitlichen Davor und Danach. Eine weitere semantische Ebene ist die Abbildung der Figurennetzwerke mit dem clusternden Visualisierungsverfahren Fruchterman–Reingold. Dadurch werden jenseits der Chronologie (teils bisher unbekannte) Gemeinsamkeiten bei der Konstruktion von Dramen über zwei Jahrhunderte hinweg sichtbar. Eine zusätzliche semantische Ebene bilden die Namen tausender Figuren, wie sie die Bühnen und Dramenanthologien zweiter Jahrhunderte bevölkert haben. Dabei fällt nicht nur auf, dass darunter etwa 24 Faust-Figuren sind (inklusive einer weiblichen ""Faustine""). Dieses Wimmelbild der deutschen Dramenliteratur ist zugleich ein möglicher Wiedereinsteig ins Close Reading, der zeigt, dass sich Close und Distant Reading nicht ausschließen, sondern ergänzen. Die Vorarbeiten zu diesem Poster wurden bereits in Graz präsentiert (Trilcke et al.   2015). Der derzeitige Stand des Projekts erlaubt es uns, auf einem Schaubild die   aktuellen Ergebnisse zu verschränken. Dabei möchten wir mit dem Genre 'Poster'   (unter Beachtung der Data-Ink Ratio nach Edward Tufte) versuchen, digital betriebene   Forschung so ins Zielformat zu übersetzen, dass das Ergebnis als wissenschaftlicher   Showcase für das Feld des Distant Reading dienen kann."
2016,DHd2016,vortraege-011.xml,Aufbau und Annotation des Kafka/Referenzkorpus,"Berenike Herrmann (Universität Göttingen, Deutschland); Gerhard Lauer (Universität Göttingen, Deutschland)","Korpus, Kafka, Stil, POS, Literarische Moderne","Teilen, Entdeckung, Sammlung, Programmierung, Modellierung, Annotieren, Theoretisierung, Bereinigung, Archivierung, Community-Bildung, Veröffentlichung, Stilistische Analyse, Identifizierung, Crowdsourcing, Kollaboration, Lehre, Artefakte, Computer, Daten, Sprache, Literatur, Metadaten, Personen, Forschung, Forschungsprozess, Standards, Text","Der vorgeschlagene Beitrag dokumentiert das Ineinandergreifen philologischer und informatischer Fragestellungen und Entscheidungen bei Aufbau und Aufbereitung eines digitalen Korpus für vergleichende quantitative Stilanalysen von Franz Kafkas Prosa. In den letzten Jahren haben digitale Ressourcen wie TextGrid, das Deutsche Textarchiv [DTA], und Gutenberg-DE reichhaltige digitale Korpora von historischen Texten (literarischer und nichtliterarischer Art) zur Verfügung gestellt. Kafkas Werk selbst ist zudem fast vollständig digitalisiert. Dennoch liegen derzeit weder ein vollständiges Kafka-Kernkorpus noch ein ""Kafka-Referenzkorpus"" vor, das eine sinnvolle quantitative Analyse seines Sprachgebrauchs durch den Vergleich mit ausreichend großen Stichproben anderer Texte zulässt. Unser Projekt möchte diese Lücke füllen und ein Kafka/Referenzkorpus vorstellen, das sowohl philologisch als auch informatisch solide aufbereitetet ist, und eine hypothesengetriebene aber auch explorative quantitative Stilistik ermöglicht. Bei der Konzeption des Kafka/Referenzkorpus verfolgen wir einen autororientierten Ansatz der digitalen Stilistik. Ausgehend von der Hypothese, dass der Stil eines Autors durch von ihm rezipierte Texte beeinflussbar ist, und dass Stil quantitativ beschreibbar ist (vgl. Herrmann / van Dalen-Oskam / Schöch 2015), gehen wir zunächst vom faktischen textuellen Input Kafkas aus und ergänzen diesen durch Stichproben kanonischer und populärer zeitgenössischer Texte. Der Aufbau des Kafka/Referenzkorpus wird von drei Kriterien geleitet: (a) Vollständigkeit von Kafkas Schriften in der Originalfassung (=Kafka-Kernkorpus); (b) Abbildung von Texten, die Kafkas Schreibprozess beeinflusst haben könnten / Abbildung von Texten, die eine näherungsweise Repräsentativität der Epoche der klassischen Moderne herstellen (=Kafka-Referenzkorpus); (c) eine hohe Akkuratheit bzw. Konsistenz bei informatischer Vorverarbeitung wie Normalisierung, linguistischer Annotation ( Das Kafka-Kernkorpus (je nach Zählart ca. 120 Texte) wurde dabei intern in die Dimensionen Kafka_Publikation (zu Lebzeiten/posthum) und Kafka_Genre (Literarisch, Brief, Tagebuch, Amtliche Schriften) unterteilt. Das Referenzkorpus (ca. 8.000 Texte) wurde hauptsächlich aus TextGrid, DTA, Gutenberg-DE und Gutenberg.org extrahiert, und beinhaltet Metadaten zu Autor (Name, Gender), Publikationsdatum und -Ort, sowie Gattung. Es umfasst literarische Texte der Kategorien ""kanonisch"" und ""populär"" ebenso wie Gebrauchstexte. Neben Kinder- und Jugendliteratur die Kafka rezipierte sind hier auch Sach- und Fachliteratur von Interesse, nicht zuletzt weil Kafkas Stil durch Elemente der Fachsprache, aber auch ein hochsprachliches ""Prager Deutsch"" ohne sozio- oder dialektale Einflüsse geprägt sein soll (vgl. Nekula 2003). Zur Korpuszusammensetzung wurden Aufzeichnungen zu Kafkas Lesegewohnheiten untersucht, wobei Zeugnisse über seine Bibliothek, biographische Berichte, aber auch Dokumente zur zeitgenössischen Rezeption sowie Autor- und Werk-Indices in Literaturgeschichten konsultiert wurden (Blank 2001; Born 1990; Born / Koch 1983; Born / Mühlfeit 1979). Das Ergebnis dieses Forschungsschrittes ist eine Liste von 765 Titeln, die das Metadatum ""in Kafkas Bibliothek"" tragen, und einen Schwerpunkt zu Kafkas Lebzeiten setzen, aber eben auch Werke von älteren Autoren wie Goethe und Kleist, sowie Flaubert und Dostojewski (in Übersetzung) beinhalten. Dass die von uns einbezogenen Online-Repositorien hinsichtlich der editionsphilologischen Textqualität variieren ist ein hinzunehmendes Übel, dem wir zum einen pragmatisch (Wahl der bestmöglichen verfügbaren Ausgabe; Ziel, die Fehlermarge unter 2% zu halten), zum anderen unter Hinweis auf die flexible Struktur des Korpus (Austausch durch eine qualitativ hochwertigere Version ist möglich) begegnen. Durch die nahtlose Dokumentation des Korpus wird zudem die nötige Transparenz gewährleistet um auch Nachnutzern flexible Kontrolle der Daten zu ermöglichen. Die Hauptaufgabe der informatischen Dimension des Projektes   besteht neben der Einbettung in einen praktikablen und anschlussfähigen Workflow   (eXist Datenbank) und der Homogenisierung und informatischen Aufbereitung der   Ausgangsdaten (Tokenisierung, Normalisierung, Lemmatisierung) in einer reliablen   linguistischen Annotation auf POS (STTS Tagset). Wortarten gelten als verlässliche   Indikatoren für Register und Genrevariation (vgl. z. B. Biber / Conrad 2009), und   sind im Vergleich mit anderen Variationsmarkern durch eine relativ akkurate   automatische Annotation besonders praktikabel. Obwohl bei der POS-Annotation gute   Accuracy für das gegenwärtige Standarddeutsch mithilfe von Unser Projekt dokumentiert in seinem gegenwärtigen Status Entscheidungen auf  verschiedenen konzeptionellen, analytischen und prozeduralen Ebenen. Es zeigt, dass  der Aufbau eines digitalen Autor-Korpus, das den quantitativen Vergleich mit  synchronen und diachronen Daten erlauben soll, bei Weitem keine triviale Aufgabe  darstellt. So wird zum Beispiel deutlich, wie Forschungsfragen beziehungsweise  Hypothesen zur Konstitution von Schreibweisen und Autorschaft die Korpuskompilation  steuern 'und deshalb auf einer möglichst präzisen Modellierung der  zugrundeliegenden textwissenschaftlichen Theorien fußen sollten. Gleichzeitig sind  Metadaten (u. a. Autor, Titel, Publikationsdatum, Publikationsort, Genre) und  linguistische Parameter (wie POS) gerade die Ansatzpunkte, an denen philologische  Fragestellungen in präzise und praktikable Kategorien umgewandelt werden können.  Nicht zuletzt deshalb sollten literarische Daten in flexiblen Architekturen  gespeichert werden, die zusätzliche Annotationsebenen zulassen 'denn hermeneutische  Erkenntnisprozesse stellen eine erwachsene Stärke der Geisteswissenschaften dar, die  auch im digitalen Zeitalter einen explizit modellierten Platz einnehmen muss."
2016,DHd2016,posters-063.xml,CRETA (Centrum für reflektierte Textanalyse) 'Fachübergreifende Methodenentwicklung in den Digital Humanities,"Jonas Kuhn (Universität Stuttgart, Deutschland); Artemis Alexiadou (Universität Stuttgart, Deutschland); Manuel Braun (Universität Stuttgart, Deutschland); Thomas Ertl (Universität Stuttgart, Deutschland); Sabine Holtz (Universität Stuttgart, Deutschland); Cathleen Kantner (Universität Stuttgart, Deutschland); Catrin Misselhorn (Universität Stuttgart, Deutschland); Sebastian Pado (Universität Stuttgart, Deutschland); Sandra Richter (Universität Stuttgart, Deutschland); Achim Stein (Universität Stuttgart, Deutschland); Claus Zittel (Universität Stuttgart, Deutschland)","Methodenentwicklung, Textanalyse, Modulariserung","Inhaltsanalyse, Beziehungsanalyse, Modellierung, Annotieren, Kontextsetzung, Theoretisierung, Bearbeitung, Visualisierung, Infrastruktur, Interaktion, Sprache, Literatur, Methoden, Personen, Projekte, Forschung, Forschungsprozess, Text, Werkzeuge, Visualisierung","Dieser Beitrag soll das Konzept des neu eingerichteten Stuttgarter DH-Zentrums CRETA Das methodische Konzept hinter CRETA geht einerseits aus von der strukturellen Gleichartigkeit vieler Teilfragestellungen über ganz unterschiedliche Teilgebiete der Digital Humanities hinweg (eingebettet in sehr unterschiedliche Gesamtzusammenhänge und methodische Rahmenbedingungen). Beispielsweise findet sich das Teilziel einer systematischen Kategorisierung von Relationen, die in einer Textquelle zwischen zwei realen oder fiktionalen Personen ausgedrückt ist, in geschichtswissenschaftlichen Fragestellungen ebenso wie in sprach-, literatur- oder sozialwissenschaftlichen Gesamtuntersuchungen. Abbildung 1 skizziert weitere Typen von wiederkehrenden Fragestellungen, die disziplinübergreifend bei der Auseinandersetzung mit Texten (und allgemeiner mit kulturellen Werken) auftauchen und bei deren Modellierung daher Synergien zu erwarten sind. Eine komputationelle Modellierung des Teilfrage-Typs kann so für ganz unterschiedliche Rahmenuntersuchung die Erschließung größerer Korpora per Aggregation über Aspekte des Textinhalts bzw. der –form erschließen.  Zugleich anerkennt das methodische Konzept die Unterschiedlichkeit sowohl der jeweiligen inneren Ausprägung der Fragestellung (so gehen im genannten Beispiel Texteigenschaften und Relationstypen weit auseinander) als auch der interpretatorischen Anforderungen, die sich aus dem jeweiligen Modellierungs- und Fragekontext ergeben. Das technische Ziel, eine einzige optimale Werkzeuglösung für jede der fachübergreifend identifizierten Teilfragen zu entwickeln bzw. aus dem Der zentrale CRETA-Gedanke zur Erschließung von disziplinübergreifenden Synergien ist folgender: Für eine praktisch umsetzbare und dennoch methodisch adäquate Integration in die jeweilige Gesamtfragestellung kann es vorteilhaft sein, Modellinstanzen anfänglich auch über Kontexte hinweg zu übertragen, deren Randbedingungen nicht in vollem Maße übereinstimmen, die eingebetteten Teilmodelle aber sehr bewusst als vorläufig anzusehen 'als Gegenstand eines Folgerichtig wird bei aufgedeckten Unzulänglichkeiten die Die notwendigen Anpassungen der vorläufigen Modellinstanzen lassen sich mit Techniken   aus der Informatik (insbes. maschinellen Lernverfahren) prinzipiell ohne weiteres   umsetzen 'dabei muss jedoch die Zielrichtung der Optimierung vorgegeben sein (beim   maschinellen Lernen in der Regel unterschiedliche Eingabe- /   Ausgabe-""Trainingsdatensätze"", anhand derer die Parameter für eine gegebene   Modellklasse eingestellt werden). Und hier beginnt die eigentliche Herausforderung   für eine echte fachübergreifende Methodenintegration: selbst wenn man 'rein   hypothetisch 'für eine geisteswissenschaftliche Naheliegender Weise wird man vielmehr versuchen, Modelle für relativ eng umrissene Teilfragestellungen empirisch zu optimieren, die dann in ein Geflecht von Analyseschritten einfließen. Der Annotationsaufwand für die Erzeugung von Referenzdaten hält sich damit in vertretbaren Grenzen und Erkenntnisse zu studienübergreifend gleichartigen Teilaspekten lassen sich so systematisch übertragen. Der Identifikation von sinnvollen Teilfragestellungen, die über unterschiedliche Projekt- und Fachkontexte hinweg tragen 'einer ""Modularisierung"" 'kommt also auch aus praktischen Erwägungen heraus eine zentrale Bedeutung zu. Was aus informatischer Sicht wie eine Binsenweisheit klingt, ist jedoch in der Modellierungspraxis extrem anspruchsvoll, ist bei vielen übergeordneten Fragen eine Untergliederung in effektive Teilschritte doch alles andere als klar. Eine Vorstrukturierung auf dem Reißbrett ist nur in Einzelfällen möglich (wie im Fall der Sprachwissenschaft mit ihrer etablierten Ebenenstruktur der Sprachbetrachtung möglich ist, die auch die computerlinguistische Modulstruktur prägt, selbst wenn bewusst klassische Teilschritte kombiniert werden). Für alle offenen Fragen der Modularisierung bietet die komputationelle Modellierung und die Verwendung von digitalen Arbeitsumgebungen Potenziale, die noch lange nicht ausgeschöpft sind: alternative Modularisierungen können exploriert und gegeneinander abgewogen werden. Der CRETA-Ansatz legt diese Exploration in die interdisziplinären Verantwortung: statt auf dem Reißbrett die plausibelste Untergliederung einer Projekt-Problematik festzuhalten, Softwarelösungen anhand dieser Spezifikation umzusetzen und nach zwei Jahren Entwicklung auf die inhaltliche Fragestellung anzuwenden, findet ein Dialog zwischen komputationellen Modellierungsexpertinnen und –experten und Fachwissenschaftlerinnen und –wissenschaftlern unterschiedlicher Disziplinen statt. Überlegungen aus der fachspezifischen Kultur der Fragestellung müssen herangezogen werden, um eine geeignete Einbindung eines technisch übertragbaren Teilmodells in den Erkenntnisprozess und seine methodenkritische Reflexion zu gewährleisten. Gleichzeitig fleißen aus den informatischen Disziplinen Überlegungen zur formalen Adäquatheit möglicher Modellklassen, Erfahrungswerte aus der zu erwartenden Qualität, sowie Möglichkeiten einer Visualisierung und explorativen Ergebnispräsentation ein, um die wechselseitige Optimierung von Modellierungskomponenten zu unterstützen. Konkret stellt sich das Vorgehen bei der Modellierung folgendermaßen dar: Im multidisziplinären Dialog im Rahmen von Werkstattklausuren werden für geistes- und sozialwissenschaftliche Fragestellungen mit Bezug zu ausgewählten digitalen Ressourcensammlungen Die angesprochenen methodischen Desiderate eines transparenten Zugangs zu den  Analyse-Teilergebnissen und der Adaptierbarkeit von analytischen Teilmodellen haben  wir exemplarisch anhand mehrerer Erweiterungsszenarien des  Relationsextraktionsmodells aus Blessing und Kuhn (2014) umgesetzt: Über die  ursprüngliche Zielrelation (Emigrationsbewegungen, die in Kurzbiographien textuell  beschrieben werden) können andere Relationen interaktiv trainiert werden. Eine  Erweiterung des Korpusbestands um Texte aus weiteren Quellen wurde vorgenommen,  einschließlich eines Wechsels der Sprache (Übertragung des Teilmodells als  Erweiterung einer deutschen Analysekette auf eine französische). Eine analoge  Adaptionsplattform wurde für Zeitungstexte erstellt, die in  politikwissenschaftlichen Studien zum öffentlichen Diskurs analysiert werden. Fallstudien zum Einsatz der resultierenden Analysekette zeigen, dass eine kritische  Betrachtung der übertragenen Teilmodelle vor allem durch den Wechsel des  Blickwinkels auf aggregierte Daten mit einer Verlinkung von Einzelinstanzen  unterstützt werden: Textuelle Einzelinstanzen eines Relationstyps (z. B. Emigration  einer Person X aus dem Land A in ein Land B) werden aggregiert und das  Aggregationsergebnis kann beispielsweise geographisch visualisiert werden. Das interaktive Springen zwischen unterschiedlichen Dimensionen der Aggregation bzw. zwischen aggregierter Sicht und Einzelinstanzen erlaubt es, Datenpunkte gezielt unter die Lupe zu nehmen, die von allgemeinen Tendenzen in bestimmter Weise abweichen. Für solche Beobachtungen ist zu klären, ob es sich (a) um einen aus bekannten Zusammenhängen erklärbaren, (b) einen neuartigen, validen Effekt oder (c) um einen technisch erklärbaren Scheineffekt handelt, der durch eine methodische Verbesserung eliminiert werden könnte. Ein Beispiel ist die fehlerhafte Klassifikation von UN-Resolution 1261 und 1973 in Zeitungsartikeln als Datumsangaben. Bei der Visualisierung der Extraktionsergebnisse auf einem Zeitstrahl fällt ein unerwartetes Muster beim Jahr 1261 auf (während Scheineffekte zum Jahr 1973 möglicherweise zunächst unerkannt bleiben). Die Fallstudien unterstützen die These, dass interaktive Nachforschungen und ein adaptierbares Instrumentarium gerade bei nicht perfekten Analysekomponenten die kritische Distanz zum Modellinventar unterstützen."
2016,DHd2016,vortraege-032.xml,Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts,"Matthias Boenig (Berlin-Brandenburgische Akademie der Wissenschaften - Berlin, Deutschland); Kay-Michael Würzner (Berlin-Brandenburgische Akademie der Wissenschaften - Berlin, Deutschland); Arne Binder (Berlin-Brandenburgische Akademie der Wissenschaften - Berlin, Deutschland); Uwe Springmann (Centrum für Informations- und Sprachverarbeitung - Ludwig-Maximilians-Universität München, Deutschland)","OCR, 17. Jahrhundert, Texterfassung, Ocropus, Tesseract","Umwandlung, Datenerkennung, Transkription, Programmierung, Modellierung, Annotieren, Bearbeitung, Computer, Datei, Text","Dieser Beitrag stellt eine neuartige Methode zur optischen Zeichenerkennung ( OCR bezeichnet die Gesamtheit von Verfahren, die in der Lage sind, aus  Rastergrafiken Schriftzeichen zu erkennen. Der Begriff wird sowohl für die  eigentliche Mustererkennung als auch für den gesamten Prozess der  Bildverarbeitung verwendet. Letzterer gliedert sich normalerweise in drei  Schritte: Grundsätzlich lassen sich bei OCR zwei unterschiedliche Erkennungsansätze unterscheiden: zeichenorientierte Verfahren wie Tesseract vergleichen das Bild eines Zeichens Pixel für Pixel mit einer Datenbasis (dem sog. Modell) und geben das ähnlichste Zeichen zurück. Sequenzorientierte (segmentierungsfreie) Verfahren wie OCRopus legen ein Raster fester Größe über eine Zeile und bestimmen anhand der Folgen der einzelnen Spalten, repräsentiert als Bitvektoren (0 entspricht weiß, 1 schwarz) die wahrscheinlichste Zeichensequenz. Unsere Studie beschäftigt sich mit OCR am Beispiel von Gelegenheitsgedichten  des 17. Jahrhunderts, denen durch die von Segebrecht (1977) initiierte  literaturwissenschaftliche Neubewertung eine zunehmende kulturgeschichtliche  Bedeutung zukommt (vgl. Klöker 2010: 39). Der Zugriff auf diese Drucke wurde  durch das 111 Funeralschriften Simon Dachs wurden im Verlauf des DFG-Pilotprojektes zum  Abbildung 1 gibt einen Überblick über den Arbeitsablauf der hier vorgestellten Methode. Im Unterschied zu existierenden Workflows unterteilt unser Vorschlag die Bildoptimierung in zwei Phasen: 1. Unser Vorgehen bei der OCR orientiert sich an der manuellen Texterfassung per Die Güte der hier vorgestellten Methode wird anhand der Volltexterfassung von Funeralschriften Simon Dachs (vgl. 1.2) evaluiert. Dabei konzentriert sich die Evaluation auf drei Punkte: Ein typisches Beispiel für die Untersuchungsgrundlage sowie die entsprechenden OCR-Ausgaben gibt Abbildung 2.  Voraussetzung für die Evaluation und das Modelltraining ist fehlerfreier  Volltext ( Für das Training der spezifischen OCR-Modelle wurden 30 Seiten Ground-Truth zufällig ausgewählt. Für die Evaluation der Modelle wurden 25 andere zufällig ausgewählte Seiten verwendet. Zur Vereinigung beider OCR-Versionen wurde ein Referenzlexikon gültiger historischer Schreibungen des 17. Jahrhunderts herangezogen. Dazu wurden Wortformen ( Für Beschneidung und Begradigung wurde das Programm Die einzelnen Textzonen (Abschnitte und Kustoden) wurden mit Hilfe von Die Zeichenerkennung erfolgte sowohl mit OCRopus als auch mit Tesseract. Die erste Versuchsreihe basierte auf mitgelieferten Modellen. Für die zweite Versuchsreihe wurden die OCR-Programme mit Ground-Truth-Daten trainiert. Für das Training der OCRopus-Modelle wurde OCRopus eingesetzt. Dabei wurde für das Training aus Gründen der Modellvergleichbarkeit eine feste Anzahl von Iterationsschritten ( Die Textvereinigung wurde in Die Bestimmung der Textqualität erfolgte durch Messung des Anteils falsch erkannter Zeichen (Fehlerrate in Prozent) im Vergleich zum fehlerfreien Volltext. Tabelle 1 gibt einen Überblick über die Ergebnisse der Evaluation bzgl. der  Fehlerrate auf Zeichenebene unter Berücksichtigung der Vorverarbeitung des  Trainings- und Testmaterials, der Modellklasse (standard vs. spezifisch) und der  eingesetzten OCR-Software (OCRopus, Tesseract). Das beste (  Die geringste erreichte Fehlerrate (3,89 %) liegt etwa im Bereich der Textgenauigkeit der 111 Gedichte aus der Pilotstudie von Federbusch (Federbusch / Polzin 2013). Die Fehlerrate von Tesseract ist jeweils höher als die von OCRopus. Der sequenzorientierte Ansatz hat klare Vorteile bei der Erkennung von Schriftzeichen, die die typischen Charakteristika früher Drucke aufweisen. Desweiteren zeigt sich, dass die Vorverarbeitung mit nlbin für Tesseract sowohl auf Trainings- als auch auf Testebene jeweils schlechtere Ergebnisse bringt. Für OCRopus sind die Ergebnisse bzgl. der Vorverarbeitung differenzierter: Die beste Kombination liefert eine Vorverarbeitung des Trainingsmaterials mit nlbin bei einer nachfolgenden Vorverarbeitung des Testmaterials mit Scantailor. Unterschiede im Ergebnis der Vorverarbeitung beider Programme illustriert Abbildung 3. Abb. 3: Bild einer Textzeile nach der Vorverarbeitung mit nlbin (oben) und Scantailor (unten). Die von Scantailor durchgeführte Bildvorverarbeitung ist deutlich normativer und für einen zeichenorientierten Ansatz wie Tesseract besser geeignet. Das Training sequenzorientierter Ansätze leidet unter dieser Vergröberung. Es zeigt sich erneut, dass spezifisch trainierte Modelle eine massive Textgenauigkeitsverbesserung mit sich bringen können (vgl. auch Springmann et al. 2015). Betrachtet man die Beispielausgaben in Abbildung 2, so wird der Qualitätsunterschied zwischen beiden OCR-Programmen ersichtlich. An einzelnen Stellen jedoch (z. B. Großbuchstaben am Anfang der Zeile im letzten Abschnitt) hat Tesseract Erkennungsvorteile. Ausgehend von diesem Befund wurde der jeweils genaueste Text von OCRopus und Tesseract miteinander vereinigt. Es hat sich gezeigt, dass die Konfidenzen, die die Programme für jedes Zeichen zurückliefern, kein verlässliches Kriterium sind, um Konflikte aufzulösen. Die Fehlerrate nimmt zu. Die Strategie, Wörter bzw. Sequenzen zu bevorzugen, die sich im Referenzlexikon befinden, hat dagegen eine messbare Verbesserung mit sich gebracht. Die Anzahl der falsch erkannten Zeichen konnte um 14 % reduziert werden (Fehlerrate 3,34 %). Es ist zu vermuten, dass der Effekt größer wäre, wenn zwei OCR-Ergebnisse mit vergleichbarer Qualität vorlägen. Dies bleibt jedoch zum jetzigen Zeitpunkt für Drucke des 17. Jahrhunderts ein Desiderat."
2016,DHd2016,vortraege-005.xml,Die Corpusanalyse multimodaler Erzählungen am Beispiel graphischer Romane,"Alexander Dunst (Universität Paderborn, Deutschland); Rita Hartel (Universität Paderborn, Deutschland)","Corpusanalyse, multimodale Erzählungen, Comics, Visualisierung","Datenerkennung, Aufzeichnung, Programmierung, Inhaltsanalyse, Räumliche Analyse, Annotieren, Theoretisierung, Netzwerkanalyse, Stilistische Analyse, Visualisierung, Bilder, Literatur, Methoden, Multimodale Kommunikation","Dieser Vortrag präsentiert Analysen und Visualisierungen eines derzeit im Aufbau befindlichen Corpus an graphischen Romanen (oder ""Graphic Novels"", einer Unterform des Medium Comics) und stellt den für die Annotation dieser multimodalen Erzählform entwickelten Editor vor, der zeitgerecht zur DHd-Jahrestagung in Leipzig für den Download zur Verfügung stehen wird. Während sich die Analyse literarischer Text-Corpora bereits seit mehreren Jahren im Fokus der Digitalen Geisteswissenschaften befindet, stehen Bestrebungen zur Erforschung visueller Erzählformen wie Theater, Comics, Film, Fernsehen, sowie Computerspiele, oft eine Randerscheinung in den DH dar und vor einer Reihe ungelöster Herausforderungen. Diese bestehen sowohl in technischer - etwa in Bezug auf die automatisierte Erkennung visueller Objekte und die Annotation komplexer Bild-Text- Kombinationen 'als auch in methodischer Hinsicht. Angesichts der Dominanz visueller Erzählformen seit dem frühen 20. Jahrhundert, sowie noch verstärkt in der Gegenwartskultur, stellt dies eine außerordentliche Forschungslücke dar. In einer kurzen Einleitung wird der Vortrag den derzeit im Aufbau befindlichen Corpus sowie die Zielsetzungen der vom deutschen Bundesministerium für Bildung und Forschung (BMBF) geförderten Nachwuchsgruppe ""Hybride Narrativität: Digitale und Kognitive Methoden zur Erforschung Graphischer Literatur"" erläutern. Darauf folgt die Vorstellung der für die Annotation entwickelten XML-Beschreibungssprache und des graphischen Editors. Im zweiten Teil des Vortrages stellen wir einige Methodenkombinationen vor, die es ermöglichen sollen, die Bild-¬≠Text-¬≠Verbindungen multimodaler Kulturformen, sowie deren Beitrag zur spezifischen Narrativität graphischer Romane, zu verstehen. Während die Analyse von Textcorpora oft bereits automatisiert möglich ist, bleibt eine automatische Analyse multimodaler Narrative derzeit eine Zukunftsvision. Im Fall von Comics und gezeichneter, sowie aus anderen Gründen nicht perspektivischer, Bilder gelingt die Objekt-Identifikation (etwa die Wiedererfassung eines vorab bekannten Charakters) nur mit viel Trainingsaufwand und recht hohen Fehlerzahlen. Auch bei der automatischen Erkennung Handschriften-ähnlicher Fonts versagen übliche Standard-OCRs. Daher führt der Weg über eine Annotation des Bild-¬≠Materials mit anschließender Analyse der Annotationen und Bild-Daten. Hierzu wird im Rahmen unseres Forschungsprojektes die XML-Sprache ""Graphic Narrative Markup Language"" (GNML) entwickelt, welche die visuellen und textuellen Aspekte Graphischer Literatur beschreibt. GNML baut auf der ""Text Encoding Initiative"" (TEI), und damit auf etablierten Standards, auf. Basierend auf den GNML-Annotationen können die in der graphischen Literatur enthaltenen Texte analysiert werden, Auswertungen der Bildinhalte vorgenommen, oder deren Kombination analysiert werden. Um die Fehleranfälligkeit bei der Annotation gering zu halten wird ein graphischer GNML-Editor entwickelt. Dieser unterstützt Fachwissenschaftler bei der effizienten Annotation mit Mechanismen wie Autovervollständigung von Charakter-Namen oder integrierten Rechtschreibprüfungen. Durch eine halb-automatische Erfassung wird die Annotation beschleunigt und so erst der Aufbau eines größeren Corpus ermöglicht. Teil des Editors ist eine Erkennung der Panel-Strukturen, sowie Werkzeuge, welche die Eckpunkte einer Sprechblase oder eines Textkästchens automatisch ermitteln. Ergänzt werden diese Werkzeuge um eine effiziente Charakter-Erfassung. Da sich die Konzepte des Editors (visuelle Objekte mit graphischen und textuellen Eigenschaften) nicht nur auf Comics beschränken sondern auch auf andere Bild-Text- Kombinationen anwendbar sind, lässt sich der Editor auf eine Obermenge solcher Formate erweitern. Diese Generalisierung erlaubt es, eine XML-basierte Annotationssprache zu hinterlegen und automatisch einen entsprechenden Editor zu generieren, sowie Daten in der hinterlegten Annotationssprache zu erfassen. Damit kann der Editor auch in der Annotation anderer multimodaler Medien Anwendung finden. Der zweite Teil des Vortrags stellt Ansätze vor, die exemplarisch für einige strukturelle Bestandteile von Comics (und insbesondere des graphischen Romans) Methoden der Bild- und Textanalyse miteinander verbinden. Für die digitale Literaturwissenschaft entwickelte Zugänge wie das Topic Modelling sind für solche Kulturformen aufgrund ihrer Bildlastigkeit nur von beschränkter Relevanz. Ansätzen zur computergestützten Bilderkennung und der Analyse großer Bildmengen fehlt hingegen bisher oft das narrative Erkenntnisinteresse. Erschwerend kommt noch hinzu, dass die Konzepte der Narratologie meist für literarische Texte entwickelt wurden und den Spezifika multimodalen Erzählens häufig nicht gerecht werden. In einer ersten Analyse des derzeit noch in Erstellung befindenden Gesamtkorpus  von rund 300 graphischen Romanen vergleichen wir die historische Entwicklung der  visuellen und Textebenen ihrer Buchcovers. Dazu gehören sowohl grammatische und  semantische Auswertungen der Romantitel mit Hilfe des Stanford Parser (vgl. De  Marneffe et al. 2006), als auch der farblichen und stilistischen Gestaltung. In  einem weiteren Schritt widmen wir uns detaillierteren Analysen eines ersten  Sub-Corpus, der aus den zehn meist zitierten Titeln des Gesamtcorpus besteht.  Zwar lassen sich aufgrund der geringen Zahl hier keine Genre-Vergleiche  anstellen, oder stichhaltig historische Entwicklungen nachverfolgen.  Beispielhaft können allerdings narrative Entwicklungen dargestellt werden: so  kombinieren wir Netzwerkanalysen der Figuren mit deren visueller Prominenz und  zugeordnetem Textanteil, sowie mit stilistischen Analysen dieser Figurentexte.  Weiters stellen wir, im Anschluss an Arbeiten von Lev Manovich (vgl. u. a.  Manovich 2012), explorative Visualisierungen aller Einzelseiten im Gesamtverlauf  der Erzählung vor. Abschließend wendet sich der Vortrag der Frage zu, ob die Text-Bild-Verbindungen  multimodaler Narrative mit solchen Methodenkombinationen aus der digitalen  Literatur- und Bildwissenschaft zu erfassen sind, oder sich durch die  Operationalisierung alternativer Ansätze aus der intermedialen Narratologie,  etwa Rick Altmans Konzept des ""Following"" (vgl. Altman 2008), eigenständige  Analysemethoden entwickeln lassen."
2016,DHd2016,panels-006.xml,Nachhaltigkeit technischer Lösungen für digitale Editionen. Eine kritische Evaluation bestehender Frameworks und Workflows von und für Praktiker_innen,"Peter Andorfer (ACDH, Österreich); Matej Durco (ACDH, Österreich); Thomas Stäcker (HAB, Deutschland); Christian Thomas (BBAW, Deutschland); Vera Hildenbrandt (TCDH, Deutschland); Hubert Stigler (ZIM Uni Graz, Österreich); Sibylle Söring (SUB Göttingen, Deutschland); Lukas Rosenthaler (DH Lab, Schweiz)","Digitale Editionen, Infrastruktur",Veröffentlichung,"Digitale Editionen machen in den Digital Humanities das ""Brot- und  Buttergeschäft"" aus. Doch während sich der methodisch-theoretische Hintergrund  digitaler Editionen zusehends konsolidiert und sich diese neue Form der  Publikation von Forschungsergebnissen im (fach)wissenschaftlichen Diskurs  bereits etabliert hat, fehlt es nach wie vor an umfassend dokumentierten und  selbstkritisch reflektierten Best-Practice-Beispielen von Frameworks und  Workflows zur Erstellung und / oder Publikation von digitalen Editionen, welche  als Blaupausen für künftige digitale Editionsprojekte herangezogen werden  können. Das Resultat ist so bekannt wie unerfreulich und kann 'nur geringfügig  überspitzt 'auf folgende Formel gebracht werden: So gut wie jedes Projekt  erfindet das Rad 'das technische Grundgerüst der Edition 'wieder neu. Die wichtigsten Gründe für diese Entwicklung lassen sich rasch benennen: Im Rahmen des Panels sollen einige der aktivsten Institutionen aus dem Bereich der digitalen Editionen an einen Tisch gebracht werden. Diese erhalten im Vorfeld der Tagung einen Fragebogen zur Vorbereitung einer kurzen (pro Teilnehmer ca. fünf Minuten) Vorstellung ihrer Systeme, wobei darin der Fokus auf dem Thema Reusability der in den Projekten verwendeten Technologien und Workflows liegen sollte. Konkret sollen die Teilnehmer_innen des Panels auf folgende Punkte eingehen: Ein Ziel dieser Vorstellungsrunde soll es sein, potenziell interessierten Nutzer_innen im Auditorium einen kompakten Überblick über bestehende Angebote zur Erstellung und / oder Veröffentlichung von digitalen Editionen zu vermitteln. Im Anschluss an diese Kurzvorstellung erfolgt eine moderierte Podiumsdiskussion, worin folgende Punkte weiter thematisiert werden: Es sollen gemeinsame Problemfelder identifiziert und reflektiert werden. Auf dieser Basis kann dann über mögliche (gemeinsame) Lösungen diskutiert werden. Im letzten Drittel des Panels wird die Diskussion zum Publikum hin geöffnet werden. Dabei sollen vor allem potentielle Nutzer_innen die Möglichkeit bekommen, gezielt konkrete und ggf. eigene Projekte betreffend Fragen zu stellen und direkt mit möglicherweise zukünftigen Projektpartnern ins Gespräch zu kommen. Bei der Auswahl der Teilnehmer wurde einerseits darauf geachtet, vornehmlich etablierte Institutionen anzusprechen, die sich als Dienstleister im Bereich digitaler Editionen profiliert haben, deshalb an möglichst generischen Lösungen zur Erstellung und Publikationen von digitalen Editionen interessiert sind und dafür selbst Frameworks und Workflows entwickelt haben. Außerdem wurde versucht, bei der Auswahl der Teilnehmer möglichst den gesamten deutschsprachigen Raum abzudecken. Matej Durco und Peter Andorfer Das Das ACDH übernimmt auch die Organisation und Moderation des Panels. Thomas Stäcker Die Herzog August Bibliothek (HAB) hat für ihre digitalen Editionsprojekte  die Sibylle Söring Christian Thomas Das DFG-geförderte Projekt Thomas Burch, Vera Hildenbrandt Das Hubert Stigler Das ZIM hat im Rahmen einer Vielzahl von Editionsprojekten   forschungsgetrieben ein Lukas Rosenthaler Während die anderen Teilnehmer vor allem an Lösungen für XML / TEI-basierte  digitale Editionen arbeiten, legt"
2016,DHd2016,vortraege-031.xml,"ePoetics 'Korpuserschließung und Visualisierung deutschsprachiger Poetiken (1770-1960) für den ,Algorithmic Criticism""","Stefan Alscher (Universität Stuttgart, Deutschland); Michael Bender (Technische Universität Darmstadt); Markus John (Universität Stuttgart, Deutschland); Andreas Müller (Universität Stuttgart, Deutschland); Sandra Richter (Universität Stuttgart, Deutschland); Andrea Rapp (Technische Universität Darmstadt); Thomas Ertl (Universität Stuttgart, Deutschland); Steffen Koch (Universität Stuttgart, Deutschland); Jonas Kuhn (Universität Stuttgart, Deutschland)","Projektbeschreibung, manuelle Annotation, algorithmische Verfahren, interaktive Visualisierung, Algorithmic Criticism","Inhaltsanalyse, Modellierung, Visualisierung, Literatur, Methoden, Projekte, Forschungsprozess, Text, Werkzeuge, Visualisierung","ePoetics ist ein Forschungskooperationsprojekt der Universität Stuttgart und der   Technischen Universität Darmstadt. Gefördert vom Bundesministerium für Bildung und   Forschung zielt es gleichermaßen auf einen Erkenntnisgewinn für die Informatik sowie   die Sprach- und Literaturwissenschaft dank einer wechselseitigen Anregung und   Ergänzung im Sinne des ""Algorithmic Criticism"" nach Stephen Ramsay (Ramsay 2007).   Dieser Ansatz ist explizit nicht darauf ausgerichtet, lediglich hermeneutische   Hypothesen mit algorithmischen Verfahren zu überprüfen. Vielmehr zielt er darauf,   durch den iterativen Einsatz analoger und digitaler Methoden verschiedene   Perspektiven auf Texte einnehmen und abgleichen zu können. Darüber hinaus ist ein   zentraler Aspekt dieses Forschungsparadigmas, Erschließungsentscheidungen und   -verfahren sowie Analyseschritte transparent bzw. nachvollziehbar und nachnutzbar zu   machen. Das Projekt ePoetics ist der Digitalisierung, Annotation, Analyse und   Visualisierung eines für die Geisteswissenschaften zentralen Textkorpus"" gewidmet:   Poetiken und Östhetiken von 1770 bis 1960. Diese Texte dokumentieren das Denken und   Schreiben über Literatur und andere Künste in der zentralen Periode nach der Abkehr   von der Normen- und Regelpoetik (vor 1770) und vor dem Übergang zur Literaturtheorie   und damit dem Ende der Poetik als literaturwissenschaftlicher Textgattung (nach   1960). Sie enthalten dabei grundlegendes Wissen über Sprache und Literatur   (-wissenschaft), etwa die Erläuterungen zentraler Begriffe und deren Zusammenhänge.   ePoetics betreibt die Entwicklung und Untersuchung eines Testkorpus"" von zwanzig   Poetiken, ausgewählt aus einem Gesamtkorpus von 1240 Texten (inkl. aller Auflagen),   die Sandra Richter in ihrer Studie ""A history of Poetics"" (Richter 2010) als zur   Gattung ""Deutschsprachiger Poetik"" zählbarer Werke bibliographiert hat. Die Auswahl   des Testkorpus"" enthält 'historisch und systematisch betrachtet 'die   repräsentativsten Texte des Gesamtkorpus"", d. h. die, die am häufigsten zitiert und   in den meisten Auflagen herausgegeben wurden, und stellt dennoch auf den ersten   Blick ein sehr heterogenes Korpus dar. Aus sprach- und literaturwissenschaftlicher   Sich zeigen wir auf, wie sich diese Heterogenität im Einzelnen darstellt, aber auch,   welche tiefergehenden Gemeinsamkeiten und Abhängigkeiten die Texte auf den zweiten   Blick aufweisen und auf welche Ursprünge sich diese zurückführen lassen. Für   ausführlichere Informationen zum ausgewählten Textkorpus und zum Projekt insgesamt   besuchen Sie unsere Homepage (vgl. Im Zentrum unseres Interesses steht aktuell beispielsweise der Begriff der Metapher als ein zentrales sprach- und literaturwissenschaftliches Konzept, das in unserem Textkorpus verhandelt wird. Die mit diesem zusammenhängenden Fragen lauten: Wie wird der Begriff in einzelnen Poetiken verstanden und erklärt? Wie ändert sich dieses Verständnis innerhalb unseres Testkorpus""? Welche literarischen oder theoretischen Werke werden im Zusammenhang damit genannt oder zitiert? Wie verändert sich der ""Kanon"" dieser Werke? Verändern sich die Zusammenhänge, in denen die Werke zitiert werden? Und schließlich: Wie verändert sich insgesamt der Umgang mit Zitaten und deren Nachweisen? Problemstellungen für die digitale Annotation mit dem Ziel der computergestützten  Auswertbarkeit liegen bei solchen Texten und Anforderungen auf mehreren Ebenen vor:  Das jeweilige Metaphernverständnis muss differenziert erschlossen und die  Komponenten der Begriffsbestimmung müssen trennscharf kategorisiert werden können.  Beispiele aus der Primärliteratur müssen eindeutig erkannt und den jeweiligen  theoretischen Aspekten, für die sie stehen, zugeordnet werden. Und schließlich  müssen die Textebenen und Referenzstrukturen der Poetik explizit gemacht werden ' also wo der Autor selbst theoretisiert, wo zitiert oder paraphrasiert wird,  inwiefern dies kenntlich gemacht wird oder nicht und sogar, wo bei Zitaten vom  ursprünglichen Text abgewichen wird. Dies wird durch die Annotation nach einem  komplexen Schema umgesetzt. Die Annotationen werden einerseits in TEI-konformen  XML-Dateien publiziert, andererseits aber auch als Grundlage von computergestützten  Analysen und Visualisierungen genutzt. Abbildung 1 veranschaulicht das Vorgehen im  Projekt ePoetics im Sinne eines ""Algorithmic Criticism"" nach Stephen Ramsay  (2007). Die Texte des Testkorpus"" stehen als Image-Digitalisate und als nach dem ""Double Keying""-Verfahren transkribierte und aufbereitete digitale Volltexte zur Verfügung. Die strukturellen (und auch die semantischen) Annotationen des Korpus"" erfolgen nach den Konventionen der Text Encoding Initiative (TEI). Das Korpus wird in virtuelle Forschungs-Infrastrukturen wie TextGrid und das Deutsche Textarchiv (DTA) integriert und dort mit den vorhandenen Referenztexten verlinkt. Nach der Identifikation relevanter und interessanter Begriffe und Konzepte wurden zu einzelnen ausgesuchten Begriffen wie der Metapher mithilfe des UAM CorpusTool Annotationsschemata für manuelle Annotationen erstellt. Diese wurden unter ausführlicher Dokumentation von Annotationsguidelines durch mehrere Annotatoren getestet, kontinuierlich verbessert, ausgebaut und schließlich in den Poetiken durchgeführt. Abbildung 2 zeigt eine vereinfache Version des daraus hervorgegangenen Annotationsschemas, das sich in zwei Teilbereiche gliedern lässt, die teils direkt und teils mit leichten Veränderungen auch auf andere Begriffe übertragen werden können. Das Schema resultiert aus den oben genannten sprach- und literaturwissenschaftlichen Fragen, die sich in die Aspekte der Repräsentation und des Verständnisses bzw. der Anwendung des Metaphernbegriffs in den Poetiken aufteilen lassen. Das Annotationsschema stellt eine Systematisierung des Begriffs, d. h. seines Vorkommens und Verständnisses in den Poetiken dar. Die für den Begriff relevanten Textstellen werden zunächst dahingehend klassifiziert, ob es sich um Poetikentext handelt (also Text vom Autor der Poetik selbst), oder ob andere theoretische oder literarische Texte zitiert, paraphrasiert oder genannt werden. Neben den Verweisungsformen annotieren wir hierbei auch die Quellenangaben 'beides im Übrigen nicht nur, wenn es explizit angegeben ist. So berücksichtigen wir auch die Möglichkeit von ""versteckten"" Zitaten oder solchen, bei denen die Quelle nicht oder unvollständig benannt ist. Das Auffinden bestimmter Muster sowie zum Beispiel Titel und Personennamen oder Zitate wird dabei unterstützt durch computerlinguistische Methoden und Verfahren der interaktiven Visualisierung. Darüber hinaus systematisieren wir das vorliegende Begriffsverständnis, d. h. ob die Metapher z. B. als Übertragung erklärt wird, und grenzen sie von anderen Begriffen ab, z. B. im Unterschied zum Vergleich. Zusätzlich lassen sich auch Beobachtungen zu konkreten hermeneutischen Hypothesen annotieren, z. B. ob anhand des Metapherngebrauchs zwischen poetischer und prosaischer Sprache unterschieden wird. Schon durch die Annotation von implizitem Wissen entsteht somit bereits bei den manuellen Annotationen eine Metaebene an Informationen, mit der der digitalisierte Poetikentext angereichert wird. Die Systematisierung erfordert eine andere Herangehensweise an den Gegenstand, als es bei einer rein hermeneutischen Analyse der Fall wäre. Ebenso führt diese zwangsläufig zur Problematisierung der Systematisierungs(un)möglichkeit eines per se komplexen, weil heterogenen Untersuchungsgegenstandes. Das Ziel der algorithmischen Weiterverarbeitung wird zum Paradigma für die systematisch-kategorisierende Ausdifferenzierung von theoretischen Begriffen, wobei diesbzgl. neue Erkenntnisse, aber auch Grenzen aufgezeigt werden können. Die Operationalisierung der Daten führt so bereits zu Erkenntnissen, bevor computertechnologische Auswertungen durchgeführt werden, womit sie sich über den Status bloßer Vorverarbeitung erheben und einen Eigenwert besitzen. Mit algorithmischen Verfahren können aus kleinen Mengen annotierter Daten (aus der manuellen und damit zeitaufwendigen Annotation) große Mengen gemacht werden, indem die annotierten Arten von Informationen automatisch auf größere Datenmengen übertragen werden. Im Folgenden wird anhand eines Beispiels in Anlehnung an den rechten Teil von  Abbildung 1 beschrieben, wie die manuelle Annotation, das Training von  Klassifikationsmodellen und die Analyse der Klassifikationsergebnisse ineinander  greifen. Zur Klassifizierung von Text zwischen Anführungszeichen als eine der drei  Klassen ""Hervorhebung"" (Wörter deren Bedeutung hervorgehoben wird), ""Titel""  (Werktitel) und ""Zitat"" (Zitate aus anderen Werken) wurde manuell ein Korpus  annotiert, in dem jeder Text zwischen Anführungszeichen einer dieser drei Klassen  zugewiesen wurde (Manuelles Annotieren von Konzepten). Auf der Basis dieses Korpus  wurden Klassifikationsmodelle zur automatischen Erkennung dieser drei Klassen  trainiert (Modell trainieren / entwickeln). Die automatischen Modelle wiederum  wurden benutzt, um in anderen Poetiken Text in Anführungszeichen automatisch in  diese drei Klassen einzuteilen. Es wurde durch Stichproben und formale Evaluation  auf einem für diesen Zweck annotierten separaten Korpus erkannt, dass die  Klassifikation gut funktioniert (Evaluierung). Da so unter anderem direkte Zitate  und Werktitel automatisch erkannt werden, ermöglicht dieser Schritt wiederum die  automatische Verlinkung von Werktiteln und Zitaten mit ihren Einträgen (sofern  vorhanden) im TextGridRepository-Korpus (Evaluierung). Durch diese Information kann  vom Analysten manuell die Verteilung von Werken und Zitaten in den Poetiken  untersucht und bedeutende Werke / Zitate erkannt werden. Diese Erkenntnisse können  dann wiederum als Metadaten im Dokument annotiert werden (Manuelles Annotieren von  Konzepten und Metadaten). Interaktive Visualisierung spielt eine wesentliche Rolle in der Vorgehensweise von  ePoetics, siehe Abbildung 1, da sie eine zusätzliche Interaktion zwischen Forschern  und den Untersuchungsgegenständen ermöglicht. Zum einen können interaktive Systeme  die hermeneutischen Vorgehensweise unterstützen, indem sie den  Geisteswissenschaftlern die Möglichkeiten bieten, Annotationsschemata und  -guidelines zu entwerfen, Konzepte und Metadaten in Texten manuell zu annotieren  sowie diese Ergebnisse zu analysieren und darzustellen. Zum anderen kann die  computerlinguistische Vorgehensweise unterstützt werden, so dass Forscher Einfluss  auf komplexe Prozesse nehmen können wie beispielsweise dem Trainieren maschineller  Lernmethoden durch visuelle Veränderungsparameter. Durch diese Art der Interaktion  kann unterstützt werden, dass Modelle mit Hilfe des Experten entwickelt, angepasst,  trainiert sowie die Ergebnisse evaluiert werden können. Um diese Herausforderungen  umzusetzen, wurden zwei interaktive visuelle Analysewerkzeuge konzipiert und  entwickelt. Der VarifocalReader (Ertl et al. 2014), der auf einem hierarchischen  Navigationskonzept basiert (Wörner / Ertl 2013), ermöglicht den Anwendern einen  direkten Zugang zu Details und Dokumentquellen, während sie auf unterschiedlichen  Abstraktionsebenen mit Zusammenfassungen vorhandener Annotationen interagieren  können. Des Weiteren bietet das System die Möglichkeit, computerlinguistische  Modelle anzupassen bzw. zu trainieren sowie Metadaten zu analysieren, zu annotieren  und zu korrigieren. Eine beispielhafte Analyse ist in Abbildung 3 dargestellt, in  der der Forscher einen schnellen Überblick und Zugang zur ausgewählten Annotation  ""Wallenstein"" (in der 3. Word Cloud sichtbar) erhält. Der zweite Ansatz (Heimerl et al. 2014) wurde konzipiert, um eine textvergleichende Analyse zu ermöglichen (siehe Abbildung 4). Die Visualisierung bietet einen Vergleich von mehreren Dokumenten auf einer abstrakten Ebene in Bezug auf die Verteilung der Annotationen, während die Textfelder eine flexible Navigation durch die einzelnen Texte ermöglichen. Zusätzlich unterstützt dieser focus+context Ansatz einen reibungslosen Übergang zwischen close und distant reading. Ergebnis des Projekts ePoetics ist ein digitalisiertes und annotiertes Korpus poetologischer Texte (TEI-konform und nachnutzbar), in denen zentrale Konzepte der Sprach- und Literaturtheorie durch XML-Auszeichnung explizit gemacht und systematisiert werden. Durch Korpus-übergreifende Analysen dieser Auszeichnungen können Gemeinsamkeiten und Unterschiede sowie diachrone Entwicklungen gezeigt werden. Darüber hinaus werden die Referenz- und Diskursstrukturen erschlossen (auch implizite, ""versteckte"" Verweisungen), die auf verschiedenen Ebenen der Texte bestehen 'einerseits Verweisungen auf andere Poetiken sowie die Identifikation bestimmter Denkschulen bzw. Theorielinien, die bis auf Ansätze aus der Antike zurückgehen (z. B. Aristoteles, Quintilian), andererseits die Diskussion von literarischen Beispielen, die Rückschlüsse auf die Entwicklungen des Literaturkanons erlauben. Die manuellen Annotationen werden iterativ gestützt durch automatisierte Methoden und Verfahren der interaktiven Visualisierung. Die dabei entwickelten computerlinguistischen Anwendungen und Visualisierungssysteme (siehe Abbildungen 3 und 4) stellen ebenfalls Ergebnisse des Projekts dar."
2016,DHd2016,posters-019.xml,Dissertation: Der Berner Chorherr Heinrich Wölfli (1470-1532) und die Beschreibung seiner Heiligland-Wallfahrt von 1520/21 - Erschliessung und Darstellung durch klassisch-literaturwissenschaftliche und digital-moderne Methoden,"Stephanie Habicht (Universität Zürich, Schweiz)","Reisebeschreibung, Visualisierung, lebendige Geschichte, Mapping","Umwandlung, Entdeckung, Bilderfassung, Transkription, Gestaltung, Programmierung, Inhaltsanalyse, Strukturanalyse, Übersetzung, Beziehungsanalyse, Räumliche Analyse, Modellierung, Annotieren, Kontextsetzung, Bereinigung, Bearbeitung, Netzwerkanalyse, Bewertung, Schreiben, Veröffentlichung, Stilistische Analyse, Identifizierung, Einführung, Kommentierung, Webentwicklung, Organisation, Konservierung, Visualisierung, Artefakte, Bibliographie, Computer, Curricula, Daten, Datei, Bilder, Interaktion, Link, Literatur, Manuskript, Karte, Metadaten, Methoden, Multimedia, benannte Entitäten (named entities), Personen, Projekte, Forschung, Forschungsprozess, Software, Text, Werkzeuge, Visualisierung","Gegenstand meiner Dissertation ist die Reise nach Jerusalem von 1520/21 von Heinrich Wölfli, erhalten in der deutschen Übersetzung von Johannes Haller. Einerseits soll - ganz klassisch im literaturwissenschaftlichen Sinne - eine narratologische Analyse der Beschreibung sowie ein Handschriftenvergleich, inkl. Stemma und Jahresschätzung der 6 bekannten Handschriften, anhand von inhaltlichen Abweichungen und Wasserzeichen vorgenommen werden. Zudem wird ein kodikologischer Befund, die Biographien des Autors sowie des Übersetzers, ein Stellenkommentar und ein kleines Lexikon der für diesen Text speziellen und ansonsten eher unbekannten Wörter erstellt. Andererseits wird mithilfe von Zusätzlich - und das ist der für das Poster relevanteste Punkt - ist angedacht, die   teilweise sehr detailliert beschriebenen und auf das Datum und die kleinste   Ortschaft genau festgehaltenen Reisestationen Wölflis in XML zu erfassen, und die   Reise im Das vorgestellte Teilprojekt dient sicher der Visualisierung geisteswissenschaftlicher Daten und vielleicht auch der Vernetzung in einem ganz speziellen Sinne. Zudem kann es, weiter angereichert, auch für andere Fächer nutzbar gemacht werden, so zum Beispiel für die Fächer Geschichte, Geographie und Religionswissenschaften. Dieses Teilprojekt der Dissertation ist derzeit in Arbeit. Wieviel davon im März 2016 zur DHd-Tagung bereits abgeschlossen ist, ist derzeit nicht abschätzbar. Sicher ist aber, dass bis dahin ein Teil der Daten schon aufbereitet ist und auf dem Poster vorgestellt werden kann, sowie, dass die generelle technische Umsetzung und die theoretische Überlegung dargestellt und erörtert werden kann. Dasselbe gilt für den Handschriftenvergleich, welcher sowohl mit klassisch-literaturwissenschaftlichen als auch mit digital-modernen Methoden angegangen werden soll, wobei der Fokus für den Beitrag doch primär auf der Visualisierung der Reiseroute liegt. Für die Zukunft muss (urheberrechtspolitisch) noch eruiert werden, inwiefern die  digitalisierte Version der vermuteten Originalhandschrift, welche mit aufwändigen  Bildern versehen ist, im Netz (wenigstens eingeschränkt) zugänglich gemacht werden  kann. Falls dem nichts entgegensteht, ist als Projekt nach der Dissertation eine  digitale Edition von Wölflis Reise nach Jerusalem angedacht, im Stile des"
2016,DHd2016,vortraege-014.xml,Moving around the City of Glass,"Jochen Laubrock (Universität Potsdam, Deutschland); Sven Hohenstein (Universität Potsdam, Deutschland); Alexander Thoß (Universität Potsdam, Deutschland)","Eye-Tracking, graphische Romane, Bildanalyse","Datenerkennung, Bilderfassung, Strukturanalyse, Beziehungsanalyse, Annotieren, Daten, Bilder, Literatur, Multimedia, Text","Graphische Romane und Comics vereinen als hybride Gattung Aspekte von Literatur und bildender Kunst und werden deshalb auch als sequenzielle Kunst bezeichnet. Man kann erwarten, dass sich die psychologische Wirkung graphischer Romane von der rein wortbasierter Romane unterscheidet. Einerseits sagt ein Bild mehr als tausend Worte, was die deutlich geringeren Zahl von Wörtern bei graphischen Adaptionen klassischer Romane erklärt, andererseits hat der Leser weniger Freiheitsgrade bei der visuellen Ausgestaltung der Szene und wird durch die erforderliche Integration von Bild und Text möglicherweise vor besondere Aufgaben gestellt. Wie interagieren Bild und Text beim Lesen graphischer Literatur und ermöglichen das Verstehen des Gesamtwerkes? Welche besonderen Herausforderungen stellt das multimediale Format an den Leser? Wie unterscheidet sich die Narrativität graphischer von der herkömmlicher Romane? Im Kontext der interdisziplinär ausgerichteten Nachwuchsgruppe ""Hybride Narrativität:   Digitale und Kognitive Methoden zum Leseverständnis Graphischer Literatur"" wurde die Im vorliegenden Beitrag illustrieren wir den potenziellen Nutzen einer solchen  Kombination anhand einer Analyse der Eye-tracking-Daten von (a) einer Sammlung  kürzerer Passagen aus mehreren kanonischen graphischen Romanen 'einem  repräsentativen Korpus 'und (b) Passagen der Graphic-Novel-Adaptation von Paul  Austers Roman ""City of Glass"" (Auster 1985; Karasik / Mazzucchelli / Auster 1994).  Für (a) berichten wir eine Analyse des relativen Anteils von Fixationen auf Text vs.  Bild. Effekte der Wortlänge sowie statistischen Worthäufigkeit in der geschriebenen  Sprache auf die Fixationsdauern zeigen, dass der Text auch tatsächlich gelesen und  rezipiert wird. Analysen der Fixationsmuster zeigen zudem, dass der Text meist vor  dem Bild angeschaut wird und das Bild oft entweder gar nicht oder rein im peripheren  Sehfeld analysiert wird. Interessante Objekte wie Personen oder Gesichter werden mit  höherer Aufmerksamkeit bedacht als Objekte des Hintergrundes. Ob das Bild überhaupt  betrachtet wird, ist unter anderem vom Informationsgehalt des Bildes abhängig, der  sich wiederum je nach Art des Überganges zwischen zwei Panels unterscheidet (McCloud  1993). Wenn sich die bestehende Handlung auf dem nächsten Panel fortsetzt, wird  dieses mit höherer Wahrscheinlichkeit übersprungen als ein Panel, das sich  deutlicher von seinem Vorgänger unterscheidet und damit einen entscheidenderen  Anteil an der Handlung hat, beispielsweise bei einem Szenenwechsel. Bei (b)  fokussieren wir insbesondere auf die Frage der Text-Bild-Beziehung. Unterscheidet  sich beispielsweise das Blickverhalten, wenn Bild und Text auf gemeinsame vs.  unterschiedliche Handlungsstränge fokussieren? Zudem berichten wir über deutliche  Zusammenhänge von Leser-Expertise mit graphischer Literatur und explizit gemessener  Verständnistiefe bei diesem speziellen Werk sowie implizit gemessenen Blickdauern.  Anders als beim Lesen von reinem Text drückt sich Expertise beim Lesen graphischer  Literatur nicht in geringeren, sondern in höheren Betrachtungszeiten aus, die sich  speziell auf den Bildanteil konzentrieren. Perspektivisch soll das Material anhand von aus dem computationalem Sehen abgeleiteten Deskriptoren beschrieben und klassifiziert werden. Beispielsweise sollen dazu Farb-Histogramme, lokales Fourier-Spektrum, der SURF-Algorithmus etc. genutzt und Klassifikationsverfahren aus dem Bereich des maschinellen Lernens angewandt werden. Diese werden sicherlich die stilistische Beschreibung anreichern und können als potenzielles Nebenprodukt auch die Suche in Bilddatenbanken ohne explizites verbales Tagging vorbereiten. Im Rahmen unseres Projektes erhoffen wir uns von einer derartigen Anreicherung der Daten jedoch eine Antwort auf die Frage, wie sich die Wechselwirkung solcher Bottom-up-Merkmale mit Top-down-Einflüssen von einfacher Worthäufigkeit bis hin zu narratologischen Elementen auf das Blickbewegungsverhalten und die Rezeption der Literatur auswirkt. Letztlich ist es eine empirische Frage, wie viel des Verhaltens und Verstehens sich durch simple Deskriptoren erklären lässt und welche Anteile sich durch Hinzunahme weiterer, beispielsweise konfigurationaler oder strategisch-aufgabenorientierter Merkmale aufklären lassen. Zusammenfassend berichten wir über eine von kognitiven und psychologischen Fragen geleitete Analyse graphischer Literatur und darauf erhobenen Blickbewegungsdaten. Zum einen werden dabei allgemeine Prinzipien anhand einer Sammlung verschiedener kanonischer Werke des Genres illustriert. Zum anderen beschreiben wir eine tiefere Analyse eines spezifischen Exemplars dieser Gattung."
2016,DHd2016,posters-065.xml,neonion - Kollaboratives Annotieren zur Erschließung von textuellen Quellen,"Claudia Müller-Birn (Freie Universität Berlin, Deutschland); Andre Breitenfeld (Freie Universität Berlin, Deutschland)","Annotieren, Software, Kollaboration, Textquellen","Gestaltung, Programmierung, Annotieren, Literatur, Manuskript, Software, Text","Im Rahmen einer Posterpräsentation stellen wir die kollaborative Annotationssoftware   neonion vor, dessen Entwicklung inspiriert wurde von der Vision des Memex. Vannevar   Bush führt in seinem Artikel dazu aus, dass ""[a] record if it is to be useful to   science, must be continuously extended, it must be stored, and above all it must be   consulted."" (Bush 1945: 37). Ein solcher ""record"" kann beispielsweise ein   historisches Dokument sein. Am Anfang des Forschungsprozesses, ist noch wenig   darüber bekannt, aber das Wissen um dieses Dokument wächst kontinuierlich durch die   Forschungsarbeit der Wissenschaftler_innen. Mit Hilfe von Die grundsätzlichen funktionalen Anforderungen an die Software neonion wurde  basierend auf den Erkenntnissen einer Interviewstudie durchgeführt. Ingesamt wurden  sechs Interviews durchgeführt. Diese Interviews waren semi-strukturiert und wurden  am Arbeitsplatz durchgeführt, um auch Informationen über das Arbeitsumfeld zu  erlangen und einen direkten Einblick in die genutzten Softwarewerkzeuge und Abläufe  zu erhalten. Die Interviews dauerten eine bis anderhalb Stunden und wurden  anschließend transkribiert. Alle Teilnehmer_innen waren Mitglieder der gleichen  Forschungsreinrichtung (für weitere Informationen s. Müller-Birn et al. 2015). Das  Ziel dieser Studie war es, den Kontext der Forschungsarbeit in den  Geisteswissenschaften besser zu verstehen. Solche Interviews sind zentral bei der  nutzerzentrierten Softwareentwicklung, einem Ansatz, der bei uns konsequent verfolgt  wird. Die Ergebnisse der Interviews wurden nach vier Gesichtspunkten ausgewertet:  Form, Funktion, Wert und Status (in Anlehnung an Marshall 1998). Wir nutzen diese  Ergebnisse im Folgenden, um die grundsätzlichen Funktionen von neonion vorzustellen. Der Bereich Form setzt sich mit der Struktur von Annotationen auseinander. Die Mehrzahl der Befragten gab an, vor allem basierend auf Textdokumenten zu forschen. Daher wurde entschieden, neonion zunächst für Textdokumente zu verwenden. Die Softwarearchitektur wurde mit Webframeworks umgesetzt. Hierzu findet Django im Bereich des Back-Ends und vorrangig Bootstrap und AngularJS im Front-End Anwendung. Die erzeugten Annotationen werden zur dauerhaften Aufbewahrung zum einen in den AnnotationStore, der auf Grundlage von ElasticSearch arbeitet, gespeichert, zum andern in den Sesame Triple Store eingespeist.  Im Bereich Funktion wurde der Frage der Verwendung nachgegangen. So wurde  ersichtlich, dass unterschiedliche Annotationsmodi (s. Abbildung 1) notwendig sind.  In neonion werden daher drei Arten der Annotation unterschieden: die Markierung für  Zitate, der Kommentar für Paraphrase und semantische Tags für das semantischen  Erschließen von Dokumenten (z. B. basierend auf einer zugrundeliegenden Ontologie).  Auch wenn diese drei Arten von Annotationen aus den Interviews entstanden sind,  können diese Annotationsmodi je nach Anwendungszweck sehr variabel eingesetzt  werden. Ein Einsatz von neonion im Bereich der Linguistik wäre beispielsweise  möglich, aber ein praktischer Anwendungsfall fehlt bisher. Zur Implementierung der Annotationskomponente kommt die quelloffene JavaScript Bibliothek Annotator.js zum Einsatz. Die Bibliothek der OKFN wurde zusätzlich durch eigene Plug-Ins, insbesondere zur Realisierung einer semantischen Annotation, erweitert.  Der längerfristige Wert der Annotation wurde im dritten Bereich untersucht. Hier wurde von allen Interviewteilnehmer_innen angegeben, dass eine Weiterverwendung der Annotationen in anderen Kontexten nicht möglich ist, da die verwendete Software den Export der Annotationen verhinderte. Diese Mangel sollte in neonion behoben werden. Alle Annotation werden einerseits innerhalb eines standardisierten Datenmodells 'dem Open Annotation Data Models (OADM) - gespeichert und anderseits haben Nutzende die Möglichkeit, alle ihre Annotationen nach unterschiedlichen Gesichtspunkten zu filtern und in ein Textdokument zur Weiterverarbeitung, z. B. in ein Textverarbeitungsprogramm zu exportieren (s. Abbildung 2). Mit Hinblick auf die Kollaboration besteht die Möglichkeit Annotationen innerhalb von Gruppen mit anderen Nutzer_innen zu teilen bzw. gemeinschaftlich Dokumente zu annotieren. Ebenfalls können die verwendeten strukturierten Vokabulare gemeinschaftlich erstellt werden. Es ist geplant, hier entsprechend benötigte Diskussionsfunktionen einzubauen. Darüber hinaus wurde das bestehende Open Annotation-Datenmodell um die Möglichkeit erweitert, semantische Tags über eine typisierte Verbindung in Relationen zueinander zu setzen. Die semantischen Tags stellen in diesem Zusammenhang Instanzen von vordefinierten Konzepten mit eigener URI (Unified Resource Identifier) dar und ermöglichen durch die Beziehung der Instanzen zueinander eine mehrstufige Analyse von Annotationen.  Im vierten Bereich wurde der Frage nachgegangen, wie Annotationen inhaltlich geteilt   werden. Unsere Interviewpartner führten aus, dass vor allem im Bereich der   strukturierten Annotationen (semantische Tags basieren auf einem vordefinierten   Begriffssystem) es sehr umständlich und zeitaufwändig ist, ein gemeinschaftliches   Begriffssystem zu erstellen. In neonion können solche Begriffssysteme, die zu einer   Ontologie weiterentwickelt werden können, einfach als sogenannte Concept Sets (s.   Abbildung 3) hinterlegt werden. Diese Concept Sets können dann auch wieder anderen   Personen in neuen Forschungskontexten zur Verfügung gestellt werden. Aus technischer   Sicht bietet das Backend von neonion verschiedene Dienste an, um Annotationen und   Concept Sets mit unterschiedlichen Systemen über eine spezifizierte REST API oder   SPARQL Endpoint auszutauschen."
2016,DHd2016,posters-033.xml,Stefan George Digital,"Frederike Neuber (ZIM, Universität Graz, Österreich)","Typografie, Digitale Edition, Ontologien, Literaturwissenschaft, Modellierung","Transkription, Strukturanalyse, Modellierung, Annotieren, Theoretisierung, Archivierung, Veröffentlichung, Kommentierung, Webentwicklung, Visualisierung, Artefakte, Literatur, Text, texttragende Gegenstände","In der neueren deutschen Literatur steht Stefan George (1868-1933) wie kein anderer für die außergewöhnliche Beschäftigung eines Autors mit bzw. für die Verwendung von Typografie. Ab 1904 werden seine Werke in Eine serifenlose Schrift inmitten der in Deutschland tobenden Antiqua-Fraktur  Debatte zu verwenden, ihr Design an der eigenen Handschrift zu orientieren und  gleichzeitig auf historische Vorbilder zu referieren 'lediglich ausschnitthaft  verdeutlichen diese Aspekte die große Relevanz von Typografie für Georges Werk.  Umso verwunderlicher ist es, dass bisherige Editionen Das Editionskorpus StGDs besteht aus 29 Druckausgaben der insgesamt 11 lyrischen   Werke Die Modellierung der typografischen Detailformen erfolgt in Form einer Ontologie, welche ihre eindeutige Identifikation, formalisierte Beschreibung und Zitation ermöglicht. Damit wird eine netzwerkartige Erschließung und Verknüpfung unterschiedlicher Aspekte und Charakteristika von Schrift unternommen, welche sowohl für Mensch als auch für Maschine interpretierbar sind. Die Technologien des Semantic Web zur Wissensrepräsentation in Thesauri (SKOS), Klassenmodelle (RDFs) und Ontologien (OWL) können dafür ebenso verwendet werden wie Methoden der Daten- und Softwaremodellierung (UML). Sowohl die Digitale Edition als auch die Ontologie zur Beschreibung von Typografie werden unter CC BY-SA Lizenz auf der Das Projekt ist vorrangig für die digitale Editorik relevant, die seit geraumer Zeit  verstärkt auch die Materialität von Dokumenten zu erschließen versucht. Statt den Weg der  Abbildung von Schrift mittels Faksimiles oder ihrer Rekonstruktion im Rahmen der  Transkription (z. B. Schriftfaksimile ) zu gehen, wählt das Projekt die formale  Modellierung und macht die Informationen so analysierbar. In diesem Zusammenhang trägt  StGD auch zur Bildung einer Schließlich kann das StGD auch als exemplarisch für den Einsatz und Umgang mit projektspezifischen Datenmodellen gelten. Um die Ontologie für die breitere Forschungscommunity nutzbar zu gestalten, wird die Übertragbarkeit des Modells auf verschiedene Arten von Typen, wie beispielsweise bewegliche Lettern und frühneuzeitliche Typen, getestet. Außerdem ist der Versuch eines mappings des Modells auf Handschriften in Zusammenarbeit mit Neben einer Gesamtpräsentation des Projekts StGD, wird das Poster vorrangig drei aktuelle Herausforderungen illustrieren:"
2016,DHd2016,posters-077.xml,Dramenwerkbank - Automatische Sprachverarbeitung zur Analyse von Figurenrede,"Andre Blessing (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Peggy Bockwinkel (Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland); Nils Reiter (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Marcus Willand (Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland)","Drama, Werkbank, Sprachverarbeitung, NLP, Textanalyse","Inhaltsanalyse, Strukturanalyse, Beziehungsanalyse, Annotieren, Netzwerkanalyse, Stilistische Analyse, Computer, Literatur, Metadaten, Personen, Software, Text","In diesem Beitrag stellen wir erste Einsichten aus einer quantitativen Analyse von Dramen vor, sowie unsere Konzeption für eine darauf aufbauende interaktive Werkbank, die einen Anstoß für eine Diskussion zur Tool-Unterstützung quantitativer Dramenanalyse geben soll. Die Werkbank unterstützt interessierte Forscherinnen und Forscher beim Einlesen von Dramen aus TEI-basierten Quellen und befindet sich noch in Entwicklung Um die Anwendungsgebiete der Werkbank aufzuzeigen, skizzieren wir 'anhand einer Dramentexte unterscheiden sich insbesondere durch zwei zusammenhängende  Eigenschaften von Prosatexten: a) Dramatische Texte sind im Gegensatz zu vielen  anderen Textsorten auf allen Ebenen (Akt- bis Redefolge) ausgesprochen gut  strukturiert und ermöglichen somit eine verhältnismäßig unaufwändige  Datenerhebung. Die Kehrseite der guten Strukturiertheit ist dass dramatische  Texte damit nicht dem Prototyp eines Textes entsprechen, wie er von vielen  Werkzeugen zur Sprachverarbeitung angenommen wird. Die maschinelle  Sprachverarbeitung auf dramatischen Texten ist damit nicht durch existierende  Werkzeuge ""out of the box"" zu leisten. b) Die dramatischen Figuren sprechen Eine automatisierte Erfassung der Oberflächenstruktur inklusiver aller relevanten  Metadaten dramatischer Texte ist die Grundvoraussatzung einer quantitativen  Textanalyse im oben genannten Sinne. TEI / XML ist als Standard etabliert, um  Texte und Korpora möglichst genau entsprechend der/einer gedruckten Edition  digital zu kodieren (cf. TextGrid; DTA). Insbesondere erlaubt TEI auch die  Kodierung von Seitenzahlen, Formatierungen, Zeilenumbrüchen, Kopf- und Fußzeilen  und vieles mehr, was über den reinen Textinhalt hinausgeht. Wie Trilcke et al. (2015) auch schon festgestellt haben, ist die Extraktion der inhaltlichen Textstruktur aus den TEI-Daten keineswegs trivial. Für Netzwerkanalyse ist die eindeutige Identifizierung von Figuren besonders relevant, für eine (maschinelle, computergestützte) Analyse des Inhaltes und der Häufigkeit der Figurenrede kommen o.g. Formatierungsmarkierungen noch als Herausforderung hinzu. In unserer Werkbank bieten wir einen Plausibilitätscheck an, der es erlaubt, Fehler im Importprozess (die sowohl durch Fehlannahmen im Importmodul als auch durch Fehlkodierungen in den Quelldaten verursacht werden können) direkt zu erkennen und zu beheben. Einmal identifizierte und behobene Fehler fließen in die Quelldaten zurück. In den bereits existierenden Arbeiten zur Stilometrie auf Dramen werden komplette Dramen verglichen (z. B. durch Vorverarbeitung mit DIGIVOY). Ein differenzierter Vergleich, bei dem einzelne Figuren oder Gruppen von Figuren betrachtet werden, ist so noch nicht möglich gewesen. Andere Projekte gehen genau den gegenteiligen Weg und verwerfen alle Dialoginhalte und beziehen ihre Netzwerkanalyse nur auf die Interaktion der jeweils in der Szene aktiven Figuren (cf. Trilcke et al.). Uns ist kein verfügbares System bekannt, das diese Lücke schließt und eine inhaltliche Analyse erlaubt, die sowohl die Interaktion der aktiven Figuren als auch deren Redeinhalt einbezieht. In unserer Werkbank erfolgt die Textanalyse mit computerlinguistischen Werkzeugen, welche durch die CLARIN-D Infrastruktur (Mahlow et al. 2014) bereitgestellt werden. Der Aufbau von Dramen erfordert eine spezielle Herangehensweise bei der Textanalyse, da die in der Computerlinguistik oft getroffene Annahme, dass Texte aus vollständigen und grammatikalisch wohlgeformten Sätzen bestehen, in Dramen nicht zutrifft (wie auch in Texten aus sozialen Medien oder in gesprochener Sprache). Daneben weisen Dramen die oben genannte spezifische Struktur auf, die eine adäquate Vorverarbeitung bedingt. Um eine Verarbeitung mit einer nicht modifizierten CL-Verarbeitungskette zu ermöglichen, wird das Drama vorher in passende Textsegmente zerlegt. Segmente, die zu einem Dialog gehören müssen nach der Verarbeitung wieder der jeweiligen Figur zugeordnet werden. Im Kontext der Figurenanalyse sind insbesondere Eigennamenerkennung und Koreferenzresolution von Interesse. Wenn man den stilometrischen Blick weitet und auch syntaktische Konstruktionen (verwendet eine Figur mehr oder weniger komplexe Satzstruktur?) untersuchen möchte, sind auch andere linguistische Verarbeitungsschritte möglich. Die Ergebnisse dieser Verarbeitung werden nicht fehlerfrei sein, deswegen bietet die Werkbank Möglichkeiten, die Ergebnisse zu korrigieren. Insbesondere die Zusammenführung von unterschiedlich genannten oder geschriebenen (z. B. ""Emilia"" vs. ""Emilie"" oder ""die Soldaten"" vs. ""erster Soldat"") Figuren ist nicht trivial und teilweise nur durch zusätzliches Weltwissen realisierbar. Damit dieser Schritt vereinfacht wird kommt hier ein halb-automatischer Figurenabgleich zum Einsatz. Das überarbeitete und manuell geprüfte Drama kann in einem TEI-konformen Format exportiert werden, damit die so kuratierte Ressource wieder der Community zur Verfügung gestellt werden kann. Linguistische Annotationen, die in TEI nicht direkt repräsentiert werden können, werden in einem geeigneten stand-off-Format exportiert. In einer Pilotstudie haben wir anhand eines einzelnen Dramas exploriert, wie  der Zusammenhang von (der zentralen) Dramenfigur zur dramatischen Handlung  automatisiert sichtbar gemacht werden kann. Die (zentrale) Stellung im  Figurennetzwerk wird dabei nicht (wie in der aktuellen Forschung gängig;  vgl. Moretti 2011) lediglich durch häufige Präsenz oder Interaktion auf der  Bühne repräsentiert, sondern durch differenziertere Analysen der  Figurenaktivität. Die Kombination von in Dramen vorhandenen strukturellen Informationen und durch automatische Verarbeitung ermittelte inhaltlich-semantische Information erlaubt neue, feinkörnige Analysen von Dramen. Die im Folgenden genannten sollen durch die Werkbank unterstützt werden, entweder durch Integration existierender oder durch Entwicklung neuer Tools. Möglich ist eine automatische Auswertung der Figurenreden nach inhaltlichen  Kriterien. Ohne Vorwissen bereitstellen zu müssen, lassen sich wichtige  Begriffe, durch deren Verwendung sich eine Figur von anderen unterscheidet,  mit Verfahren wie TF*IDF ermitteln und z. B. als Tabelle oder als Wortwolke  darstellen. Komplexere Verfahren wie topic modeling (Blei et al. 2003) oder  Wortfeldanalysen können natürlich auch auf den Redeinhalt einer Person (ggf.  auf Akte / Szenen o. ä. eingeschränkt) angewendet werden, erfordern aber  zumindest die Einstellung von Parametern (z. B. Anzahl der topics im topic  modelling) oder das Spezifizieren von Wortfeldern. Automatische Methoden zur  Erweiterung von Wortfeldern (angelehnt an z. B. Query Expansion, vgl.  Manning et al. 2008) können diesen Prozess unterstützen und sollen im  Rahmen der Werkbank erprobt und integriert werden. Abbildung 2 zeigt eine  visuelle Auswertung dieser Analyse. Stilometrische Analysen werden durch eine Schnittstelle ermöglicht, durch die  man Figurenrede als Datenstrukturen in R abrufen und dann nach diversen  Kriterien untersuchen kann, etwa mit Hilfe von stylo (Eder et al. 2013). Es  ließe sich z. B. untersuchen, ob Könige bei Schiller anders sprechen als bei  Lessing, oder ob Bürgerfiguren in einem bestimmten Dramenkorpus anders  sprechen als Adelsfiguren: Durch Methoden aus der Sentiment-Analyse (die zur automatisierten Analyse von Produktreviews eingesetzt wird) ließe sich z. B. analysieren, wie und ob bestimmte Figuren über andere sprechen. Neben positiv / negativ wären auch feinere, dramenspezifische Unterscheidungen denkbar (Feigling, Hahnrei, ...). Die Kombination dieser Techniken mit Netzwerkanalyseverfahren würde es erlauben, im Netzwerk auch Entitäten darzustellen über die geredet wird, ohne dass sie direkt im Drama vorkommen (z. B. Gott), Kanten zwischen Knoten können dann (z. B. durch Farben) auch inhaltliche, relationale Informationen kodieren (X spricht viel / positiv über Y). Eine Netzwerkdarstellung, in der die Position der Figuren nicht mehr zufällig (oder durch Layout-Algorithmen gesteuert) ist ist ebenfalls denkbar (Abbildung 4). Dabei werden prototypischen Figurenrollen feste Positionen in einem Raster zugewiesen, so dass große Mengen an Netzwerken schnell und direkt verglichen werden können."
2016,DHd2016,vortraege-009.xml,DH-Projekte Österreichischer Literaturarchive: Ein Problembericht,"Vanessa Hannesschläger (ACDH-OEAW Austrian Center for Digital Humanities, Österreich)","Archiv, best practices, Standards, Digitalisierung","Teilen, Sammlung, Transkription, Gestaltung, Programmierung, Modellierung, Annotieren, Kommunikation, Archivierung, Community-Bildung, Bewertung, Kollaboration, Lehre, Projektmanagement, Webentwicklung, Organisation, Visualisierung, Bibliographie, Curricula, Daten, Infrastruktur, Literatur, Manuskript, Metadaten, Methoden, Projekte, Forschung, Forschungsprozess, Forschungsergebnis, Software, Standards, Text, Visualisierung, virtuelle Forschungsumgebungen","In diesem Beitrag werden die Probleme skizziert, die sich aus Praktiken  Österreichischer Archive bei der Umsetzung von online-Projekten ergeben. Die  Beschränkung auf Österreich ergibt sich, um die Beispiel-Palette überschaubar zu  halten; die Problembereiche und Lösungsvorschläge lassen sich allerdings  allgemein anwenden. State-of-the-art Projekte von Bibliotheken und Archiven im  deutschsprachigen Raum, die den neusten Stand der Forschung umsetzen, werden in  einem ersten Schritt beschrieben. Der zweite Abschnitt skizziert Gründe dafür  und Konsequenzen daraus, dass diese Standards häufig nicht herangezogen werden.  Schließlich werden Lösungsvorschläge präsentiert und eine Agenda vorgeschlagen,  die die Situation nachhaltig verbessern könnte. Diese wird im Rahmen des  Vortrags auf der DHd 2016 im Zentrum stehen. Dort werden auch die hier allgemein  beschriebenen Schwierigkeiten anhand mehrerer Beispielprojekte illustriert. Vor allem im Bereich der digitalen Edition haben sich im deutschsprachigen Raum  auf breiter Ebene Standards und ""best practices"" entwickelt, die von einer  etablierten Community umgesetzt werden. Umfangreiche Bibliotheken publizierter /  rechtefreier Werke bieten etwa das Im Bereich der Beforschung von Archivbeständen überwiegen im digitalen Raum  ebenfalls Editionsprojekte, die die erwähnten Standards zur Anwendung bringen.  Beispiele hierfür sind das Wegweisend speziell für die Arbeit mit literarischen Nachlässen ist auch das Virtual Research Environment (VRE) Im Bereich der Archivierung und Bereitstellung von elektronischen Publikationen, Multimedia-Objekten und anderen digitalen Daten wird in Österreich das Projekt Die in Österreichischen Literaturarchiven aufbewahrten Bestände werden im Rahmen von wissenschaftlichen Projekten mit direkt an der Institution angesiedelten Mitarbeitenden erforscht und publiziert. Aufgrund der Vergabepolitik des FWF Forschungsfonds, der in den allermeisten Fällen Geldgeber dieser Unternehmen ist, haben die betreffenden Projekte mittlerweile häufig eine digitale Komponente. Projektleitende und Mitarbeitende sind zumeist literaturwissenschaftlich ausgebildet. Sie konzipieren und entwerfen, wie die digitale Repräsentation ihrer Arbeit strukturiert wird und erarbeiten das wissenschaftliche Konzept, das Inhalt und Funktionalität zugrundeliegt. Für die technische Umsetzung werden meist erst nach Abschluss der konzeptionellen Arbeit externe Auftragnehmende engagiert, oft privatwirtschaftliche IT-Unternehmen, die von den Möglichkeiten, die im Bereich der DH bereits verfügbar wären, nur eingeschränkte Kenntnis haben. Aus dieser Situation ergeben sich Probleme in mehreren Bereichen: Die im Rahmen von Projekten erstellten Scans sollten in einer digitalen Langzeitarchivierung der projekttragenden Institution abgelegt werden, was fallweise versäumt wird. Gründe: Die Projektzuständigen haben aufgrund ihrer Ausbildung meist einen editorischen oder von archivarischen Ordnungsprinzipien geprägten Zugang zur Modellierung und Strukturierung der Projektdaten. Gründe und Konsequenzen: Forschungsprojekte werden häufig in Zusammenarbeit mit Firmen umgesetzt, die nicht (primär) mit wissenschaftlicher Klientel arbeiten, deren Wünsche und Methoden daher nicht im Detail verstehen und nicht mit bereits existierenden DH-Tools und Ressourcen vertraut sind. Konsequenzen: An hostenden Institutionen werden kaum personelle Ressourcen zur Wartung abgeschlossener online-Projekte einkalkuliert. Projekt-Websites sterben daher oft nach wenigen Jahren, mit ihnen die Daten. Auch in Projektfinanzierungsplänen wird dieser Aspekt bislang nicht berücksichtigt. Konsequenzen: Der skizzierten Situation muss auf allen Ebenen begegnet werden: Seitens der hostenden Institutionen muss stärker daran gearbeitet werden, für langfristige Datensicherung Möglichkeiten zu entwickeln und anzubieten oder Kooperationen mit Langzeitdatenarchiven einzugehen. Dafür müssen sowohl substanzielle finanzielle als auch personelle Ressourcen explizit dieser Aufgabe zugeordnet werden. Die Postionen, die die Bei der Bewilligung von Projektanträgen sollten Fördergebende die skizzierten Probleme ernsthaft berücksichtigen und Projekte, die keine ausreichenden Ressourcen für die Arbeit an der digitalen Repräsentation der Projektergebnisse vorsehen, ablehnen - anstatt die Praxis, utopische Ziele in Projektanträge einzubauen, zu unterstützen. (Inter)Nationale DH-Plattformen sollten es sich zur Aufgabe machen, ein entsprechendes Empfehlungspapier zur Verfügung zu stellen. Bedenkenswert ist auch die Forderung nach einem ""offenen Lebenszyklus"" von Forschungsprojekten, die etwa von der Plattform Das größte Potential zur nachhaltigen Verbesserung der Situation liegt im  Bereich der Projektangestellten. Geisteswissenschaftlich Forschende erfahren  im Rahmen des Studiums unzureichende Ausbildung zur Arbeit im digitalen Raum  und haben in der Folge entsprechende Hemmungen, mit digital humanists in  Austausch zu treten. Deshalb werden DH von nicht primär im digitalen Raum  arbeitenden Forschenden noch immer als eigene Disziplin wahrgenommen anstatt  als Teil und Methode des geisteswissenschaftlichen Forschens an sich. Hier  muss Bewusstsein geschaffen und Skepsis abgebaut werden, indem die Lehrpläne  grundlegend überarbeitet und gegenwärtigen Standards angepasst werden.  Dadurch würden viele der umrissenen Probleme gar nicht erst entstehen. Neben  den Forschenden der Zukunft, die man so erreichen kann, müssen kurz- bis  mittelfristig auch die Forschenden der Gegenwart stärker animiert werden,  sich mit seriösen Methodiken und Frameworks für Forschungsprojekte im  digitalen Raum auseinanderzusetzen, indem sie dort, wo sie mit ihrem Wissen  stehen, abgeholt werden. Dafür kann etwa die Vorgehensweise des Die Arbeit in den Bereichen Helpdesk und Outreach zeigt, dass für eine zeitnahe Verbesserung der Situation Adaptionen in der Ausbildung der Forschenden der Zukunft alleine nicht ausreichen; um die Forschenden der Gegenwart zu erreichen, die nicht in digitaler Methodik ausgebildet wurden und sich (noch) nicht damit auseinandergesetzt haben, braucht es ""Übersetzende"", die die Kommunikation zwischen rein geisteswissenschaftlich und rein digital Denkenden erleichtern und Brücken bauen. Die Wichtigkeit solcher Bindeglieder, die ""beide Sprachen sprechen"", kann nicht hoch genug eingeschätzt werden, da sie allen Beteiligten Frustration, Zeit, überflüssige Arbeit und letztlich auch Geld ersparen können. Zentren, Institute und Verbände, die in den Digital Humanities arbeiten, sollten ihre Aufmerksamkeit vermehrt auf diesen neuen Arbeitsbereich der digital Übersetzenden richten und ihre Aktivitäten gezielt in diese Richtung lenken. Die unzureichende Vernetzung geisteswissenschaftlich Forschender mit der DH Community führt zu technischen Unzulänglichkeiten in abseits davon ambitionierten digitalen Projekten, die ihre Nachhaltigkeit gefährden. Gegenseitige Annäherung über Outreach-Programme und die Adaption der Lehrpläne geisteswissenschaftlicher Studienrichtungen, vor allem aber verbesserte interne und externe Kommunikation sind notwendig, um zu nachhaltiger Verbesserung der Situation zu gelangen."
2016,DHd2016,posters-020.xml,Mit der FinderApp durch Goethes Faust: Treffer im Faksimile visuell hervorgehoben und multimediale Ausgabe in Videoaufführung und Hörbuch.,"Maximilian Hadersbeck (Ludwig Maximilians Universität München, Deutschland); Elisabeth Eder (Ludwig Maximilians Universität München, Deutschland); Roman Capsamun (Ludwig Maximilians Universität München, Deutschland); Nora Eichfeldt (Ludwig Maximilians Universität München, Deutschland); Simeon Herteis (Ludwig Maximilians Universität München, Deutschland); Matthias Lindinger (Ludwig Maximilians Universität München, Deutschland); Raphael Höps (Ludwig Maximilians Universität München, Deutschland); Stefan Schweter (Ludwig Maximilians Universität München, Deutschland)","GoetheFind, anchor-key XML, No SQL-Database, Faksimile-Viewer, Multimedia","Bilderfassung, Transkription, Programmierung, Modellierung, Annotieren, Kontextsetzung, Bearbeitung, Webentwicklung, Visualisierung, Daten, Bilder, Sprache, Link, Metadaten, Methoden, Multimedia, Projekte, Forschung, Software, Ton, Text, Werkzeuge, Video, Visualisierung, virtuelle Forschungsumgebungen","In unserem Poster möchten wir unsere neueste FinderApp In unserer neuen FinderApp GoetheFind, setzen wir Ideen des ""Standoff-Markups"" um, damit ""overtagged""-XML vermieden wird. Wir entwickelten eine reduzierte ""XML-TEI-P5 anchor-key"" Edition und speichern die Metainformatione in einer ""NoSQL-mongo""-Datenbank. Alle relevanten Editions-, OCR- und Transkriptionsinformationen zur multimedialen Trefferausgabe sind in der Datenbank gespeichert. Grundlage unserer FinderApp GoetheFind ist die XML-TEI-P5 Textedition im  DTABf Format vom Deutschen Textarchiv (DTA) der Berlin-Brandenburgischen  Akademie der Wissenschaften (BBAW 2013; Haaf et al. 2015), dazu die  Bilddigitalisate der Staatsbibliothek zu Berlin (BBAW 2013) und dem freiem  Hochstift Frankfurt des Frankfurter Goethe-Hauses (Signatur: III B / 23).  Da wir zur multimedialen Ausgabe der Suchtreffer zahlreiche  Metainformationen benötigen und ""overtagged"" XML des Editionstexts vermeiden  wollten, verwenden wir Ideen des ""Standoff-Markups"" und lagern alle  notwendigen Meta-Informationen in der ""NoSQL"" mongo-Datenbank GoetheDB aus.  Eine eindeutige Referenz der Datenbankeinträge zum Editionstext lösen wir  über den XML-TEI-P5 Tag <anchor/>, der an Seitenanfängen in die  Edition eingefügt ist. Die Trefferpositionen werden über ein  XML-Attributtrippel (Seite, Zeile, Token) genau spezifiziert. Da unsere FinderApp die Suchanfragen regelbasiert mit Hilfe von lokalen Grammatiken im Kontext eines Satzes realisiert, verwenden wir als wichtigste Strukturierungsebene Sätze. Goethes Faustdrama bettet Sätze in Rede und Gegenrede, sogenannte Repliken ein, die die zweite Strukturierungsebene darstellt. Zur visuellen Hervorhebung und multimedialen Ausgabe der gefundenen Textstellen im Faksimile ermitteln wir für die Replike geometrische Informationen mit Hilfe eines von uns entwickelten semiautomatischem OCR-Correction Tools. Die Bühnen- und Audioaufnahme werden mit Hilfe des Clarin-Tools: ""Munich Automatic Segmentation System WebMAUS"" (CLARIN) semiautomatisch transkribiert. Mit Hilfe unseres Speziallexikons GoetheLEX, angereichert um historische Sprachvarianten, Part of Speech Tagging und lokalen Grammatiken implementierten wir eine Partikelverb- und Semantische Suche. Bei der Eingabe von Suchanfragen verwenden wir eine symmetrische Autovervollständigung mit Informationen zur Häufigkeit des Auftretens im Text. Öhnlich wie bei Google-Docs entwickelten wir einen Browser basierter Faksimile-Viewer mit dem man in einem doppelseitigen Buchlesemodus durch das Dokument blättern kann und die gefundenen Textstellen farblich hervorgehoben werden. GoetheFind vernetzt alle Treffer mit der entsprechenden Replik in der Bühnenaufführung und der Hörbuchausgabe. Sobald der Nutzer auf die Mulitmediabuttons des Treffers drückt, startet im Browser ein Videoviewer oder eine Ausdioausgabe ab dieser Stelle. Wir danken dem Deutschen Textarchiv für die gute Zusammenarbeit und die freundliche Verfügungsstellung der Editionsdaten von Goethes Faust (BBAW 2013). Der Staatsbibliothek zu Berlin 'Preußischer Kulturbesitz und dem Freien Deutschen Hochstift, in Frankfurt danken wir für die Wiedergaberechte der Bilddigitalisate der Originalausgabe Goethes Faust."
2016,DHd2016,vortraege-030.xml,"Annotation und Distant Reading: Probleme, Synergien, Perspektiven","Angelika Zirker (Eberhard Karls Universität Tübingen, Deutschland); Matthias Bauer (Eberhard Karls Universität Tübingen, Deutschland)","Kollaboration, distant reading, close reading, erklärende Annotation","Inhaltsanalyse, Strukturanalyse, Modellierung, Annotieren, Kommunikation, Kontextsetzung, Theoretisierung, Bearbeitung, Kollaboration, Kommentierung, Text","In unserem Vortrag möchten wir Methoden des Unsere Ausgangsfrage ist, inwiefern Methoden des Im zweiten Teil des Vortrags werden Synergieeffekte von Methoden der Annotation und des Aus diesen Synergieeffekten ergibt sich der Anschluss an Perspektiven zum Verhältnis von (erklärender) Annotation und quantitativen Methoden des Der Vortrag bewegt sich an der Schnittstelle von Automatisierung und individuellen hermeneutischen Akten und damit entlang des Problems, wie im Markup eines Textes Entscheidungen getroffen werden können, welche Aspekte in einem Text relevant sind und die dem individuellen Text gerecht werden können. Wir möchten verschiedene Fallstudien aus englischsprachigen literarischen Texten vorstellen, etwa anhand von automatisierten Annotationssystemen wie x-ray, die oben geschilderte Probleme exemplarisch aufzeigen, die aber zugleich auch Synergieeffekte deutlich machen. Letztlich sollten bei dem Verhältnis von qualitativen Methoden des"
2016,DHd2016,vortraege-039.xml,Kollaboratives Annotieren literarischer Texte,"Janina Jacke (Universität Hamburg, Deutschland); Evelyn Gius (Universität Hamburg, Deutschland)","Annotation, Kollaboration, Literatur, Narratologie, best practice","Strukturanalyse, Modellierung, Annotieren, Theoretisierung, Kollaboration, Literatur, Metadaten, Forschungsprozess, Text","Kollaboratives Annotieren ist eine gute Möglichkeit, um mehr als bloß eine subjektive   Perspektive auf den Untersuchungsgegenstand 'beispielsweise einen Text '  abzubilden: Sobald mehr als ein Individuum MarkUp an einem digitalen Objekt   anbringt, kann deutlich werden, welche unterschiedlichen Aspekte des Objekts im   Zentrum des individuellen Interesses stehen oder welche verschiedenen Sichtweisen in   Bezug auf denselben Aspekt möglich sind. Soll die kollaborative Annotation jedoch   nicht bloß die Pluralität von Perspektiven und Meinungen aufzeigen, sondern einem   spezifischeren Erkenntnisinteresse dienen, so sollte die Annotationspraxis mithilfe   von Guidelines strukturiert und reguliert werden. Für die Annotation linguistischer   Phänomene (bspw. in Gebrauchstexten) werden solche bereits entwickelt (vgl. bspw.   Pyysalo / Ginter 2014; Mamoouri et al. 2008); dagegen existieren für die   kollaborative Annotation semantischer Phänomene in literarischen Texten kaum Im Folgenden möchten wir einen Die Annotationsguidelines sind im Kontext des Projekts heureCLéA entstanden (vgl.  Bögel et al. im Erscheinen). Ziel des Projekts ist die Entwicklung einer digitalen  Heuristik, d. h. eines Funktionsmoduls, das automatisch semantische Phänomene in  literarischen Texten 'hier: narratologische Phänomene der Zeitgestaltung Nach einigen Anläufen hat sich folgende Annotationspraxis als Wie deutlich geworden ist, werden die eingangs genannten drei Probleme literarischer  Annotation im Kontext dieses Ablaufschemas berücksichtigt: Die Möglichkeit, einen  literarischen Text unterschiedlich zu deuten, wird zum einen durch die individuelle  Annotationsphase (vgl. Schritt 2) gewährleistet, zum anderen dadurch, dass  widersprüchliche Annotationen erlaubt sind, sofern sie durch die Polyvalenz des  Textes bedingt sind (vgl. Grund d). Dass die Berücksichtigung der Polyvalenz nicht  in eine Beliebigkeit von Annotationsentscheidungen abgleitet, wird dadurch erreicht,  dass andere Gründe (d. h. mindestens Gründe a und b) für widersprüchliche  Annotationen ausgeschlossen werden. Die Spezifikation literaturwissenschaftlicher  Annotationskategorien wird schrittweise optimiert, um aussagekräftige  Annotationsergebnisse zu gewährleisten (vgl. Schritt 1 sowie ggf. weitere  Optimierungsschritte ausgehend von Schritt 2 und Grund b). Da klare Definiertheit  nicht notwendig mit einer Einschränkung der Perspektive auf Texte einhergeht (vgl.  Schritt 1), ist sie mit der literaturwissenschaftlichen Praxis bzw. mit der  Pluralität möglicher Erkenntnisinteressen kompatibel. Die Abhängigkeit  literaturwissenschaftlicher Kategorien untereinander wird schließlich, je nach  verfügbaren Ressourcen, entweder in einem reduzierten oder in einem ausführlichen  Ansatz explizit gemacht (vgl. Grund c)."
2016,DHd2016,vortraege-013.xml,Korpusanalyse in der computergestützten Komparatistik,"Christine Ivanovic (Universität Wien, Österreich); Andrew U. Frank (Technische Universität Wien)","Korpusanalyse, RDF, Big Data, Komparatistische Literturwissenschaft, Natural Language Processing","Datenerkennung, Programmierung, Inhaltsanalyse, Strukturanalyse, Beziehungsanalyse, Räumliche Analyse, Annotieren, Kontextsetzung, Netzwerkanalyse, Stilistische Analyse, Organisation, Literatur, Methoden","Komparatistische Forschung zielt weniger auf hermeneutische Auslegung von Einzeltexten, als darauf, (a) generalisierende Aussagen über (literarische) Texte, ihre Fomen und Funktionen zu machen, (b) deren historische Entwicklung innerhalb oder (c) im Austausch von kulturellen Systemen zu rekonstruieren, und (d) literarische Repräsentationen von 'Welterfahrung' mit anderen Repräsentationssystemen zu vergleichen. Zu diesem Zweck vergleicht die Komparatistik eine Vielzahl von Texten (resp. von Texten und anderen künstlerischen Repräsentationsformen) in unterschiedlichen Sprachen. Vergleiche erfordern die Annahme einer Anzahl von Eigenschaften der verglichenen Gegenstände als gleichwertig, während andere Eigenschaften desselben Gegenstands variieren. Die Bestimmung der Eigenschaften eines Textes ist demnach eine der unabdinglichen Voraussetzungen für den komparatistischen Vergleich. Um allgemeine Aussagen machen zu können, muss die komparatistische Forschung andererseits eine größere Anzahl von Texten untersuchen: sie muss Textkorpora bilden und evaluieren. Die Anzahl und Auswahl der in Betracht gezogenen Texte sowie der evaluierten Texteigenschaften bestimmen maßgeblich die Ergebnisse einer komparatistischen Untersuchung. Eine computergestützte korpusbasierte komparatistische Untersuchung unterscheidet sich von den bisher praktizierten Ansätzen nicht in der 'die Disziplin charakterisierenden 'Operation des Vergleichens, wohl aber in der Art und Weise, wie Auswahl und Anzahl der verglichenen Texte resp. Textkorpora begründet und dokumentiert werden. Potentiell sind alle jemals verfassten und mündlich oder schriftlich tradierten Texte aller Sprachen und aller Zeiten Gegenstand der Komparatistik. Ein umfassender systematischer Zugriff auf alle diese Texte ist (bisher) jedoch nicht möglich. Die Möglichkeiten der Evaluierung sind durch verschiedene Faktoren limitiert: nicht alle Texte sind faktisch (mehr) zugänglich, und die jeweilige Forscherperspektive beschränkt grundsätzlich die Erfassung der zum Vergleich herangezogenen Texte. Bisher sind die Kriterien für die Textauswahl in wesentlichem Maße abhängig gewesen von (a) der Subjektivität und (b) der natürlicherweise begrenzten Kapazitätder Forscher, die nur die ihnen bekannten Texte berücksichtigen können und die unter dem Credo arbeiten, nur Texte zu erforschen, die ihnen in der Originalfassung zugänglich sind. Dies führt dazu, dass die Komparatistik bisher mehrheitlich Texte aus den dominanten Sprachen (Englisch, Französisch,...) bearbeitet und Texte in 'kleinen' Sprachen (Finnisch, Urdu,....) oder Textvergleiche zwischen kaum verwandten Sprachen (Chinesisch gegen Arabisch) eher selten vorkommen. Ein weiteres Problem der Textauswahl stellt (c) die Gefahr des logischen Zirkelschlusses dar: Bei der Evaluation beispielsweise ""des"" europäischen Romans werden aus der Lesepraxis resp. -tradition herrührende Vorannahmen in die Auswahl einbezogen, wenn es darum geht, dieses Genre anhand verschiedener Beispiele zu bestimmen; sie haben unweigerlich Einfluss auf das erzielte Ergebnis. Schließlich beruhen, und auch dies bedeutet eine wesentliche Einschränkung, (d) generalisierende Aussagen wie über ""den europäischen Roman"" immer auf einer im Vergleich zur Gesamtmenge der je produzierten Texte verschwindend kleinen Auswahl. Die Auswahl der evaluierten Texte kann bei einer computergestützten korpusorientierten komparatistischen Untersuchung zumindest statistisch anders begründet werden: * durch einen definitiv bestimmten Korpus, der so angelegt ist, dass er Repräsentativität beanspruchen kann * durch einen Korpus, der in seinem Umfang weit über das Lesevermögen des Einzelnen hinausreicht und der große Textmengen in einer Vielfalt von Sprachen umfaßt, die kein Einzelleser je bewältigen könnte; * durch die Möglichkeit der Überprüfung der erzielten Ergebnisse in Wiederholungs- und Vergleichsstudien sowie mittles Vergleichskorpora; * durch die Trennung der Auswahlkriterien für die Erstellung des Korpus von den fokussierten Untersuchungsergebnissen; * durch die Möglichkeit von Negativabfragen (z. B. eine bestimmte Eigenschaft ist  in einigen der Texte des Korpus nachweisbar, während andere Eigenschaften in  keinem davon nachweisbar sind). Computergestützt korpusbasiert arbeitende Komparatistik sieht sich mit folgenden Aufgaben konfrontiert: Für den Aufbau und die Pflege großer Textkorpora bedarf es entsprechend  bearbeiteter Texte: alle in den Korpus aufgenommene Texte müssen  bibliographisch genau erfasst und mit Markups (Taggern) versehen sein.  Markups können gesetzt werden u. a. zur Auszeichnung der Sprachform  (insbesondere bei mehrspachigen Texten), der Textstruktur,  nicht-literarischer Elemente (z. B. Abbildungen im Text) etc. Bevorzugt  werden treebank getaggte Versionen mit verzweiger Struktur (parse tree), die  Koreferenzen, Personen- und Orstnamen u. a. erkennen lassen. Im Textmarkup  werden einzelne Elemente der Textstruktur identifiziert: Worte oder  Wortbestandteile, Sätze und deren Teile, Abschnitte, Kapitel und andere  Texteinteilungen bis hin zum Buchlayout. Es erscheint wichtig, auch die  Elemente zu erfassen, die nicht unmittelbar textimmantent sind, die aber zur  Identifizierung und Charakterisierung des Textes gehören wie Seitenangaben,  Verfassername und weitere Angaben, die im Rahmen einer Buchpublikation  vorkommen. Der Text sollte in UTF-8 codiert sein, um auch Texte in  nicht-alphabetischen Sprachen wie Chinesisch, Arabisch u.w.m. einbeziehen zu  können. Unserer Konzeption nach sollen die Markierungen in den Text  hineingesetzt werden, so dass der mit den Annotationen versehene Text mit  der Originalstruktur verbunden bleibt. Der mit Markups versehene Text und die POS-Annotationen werden in ein einziges Format zusammengefasst. Wir bevorzugen derzeit RDF (Manola / Miller 2004), das für die von uns avisierte Datenmenge auszureichen scheint. Unseren bisherigen Beobachtungen nach erhalten wir bei einem Text mit reichhaltiger linguistischer Auszeichnung für jedes Wort etwa 10 RDF Triples; bei einem literarischen Text von 100.000 Wörtern würde das eine Million Triples ergeben, bei einem Korpus von 10.000 Büchern wäre man mit 10 Milliarden Triples noch bei weitem innerhalb des Rahmens dessen, was das heutige RDF Depot erlaubt; Untersuchungen zur derzeitigen Kapazitätsgrenze haben für 1 Billiarde Triples eine Hochladezeit von wenigen Stunden ergeben ( Es müssen Methoden sein, die Abfragen und Analysen von Texten ermöglichen Die Anwendung aller Methoden auf alle Texte generiert eine Matrix evaluierbarer Werte; jeder Text läßt sich durch einen Vektor aus diesen Werten darstellen. Diese Darstellungsweise ermöglicht Textvergleiche mittels Clusteranalyse wie sie in der konventionellen Komparatistik aufgrund der o.g. Beschränkungen bisher nicht zugänglich waren. Korpusaufbau und Abfragemethoden müssen so gestaltet sein, dass Texte umstandslos dem Korpus hinzugefügt werden und die Methoden problemlos appliziert werden können. Dies setzt kontinuierliche Pflege und Aktualisierung des bestehenden Korpus resp. der Abfrageergebnisse voraus: wenn Texte hinzugefügt werden, müssen alle bisher angewandten Methoden automatisch darauf angewandt werden können; wenn Methoden hinzugefügt werden, müssen automatisch alle Texte einer entsprechenden Evaluierung unterzogen werden. In unserem Beitrag treten wir für die Etablierung eines computergestützten korpusbasierten Forschungsansatzes in der literaturwissenschaftlichen Komparatistik ein. Dazu wollen wir darstellen, (a) welche Vorteile die Erstellung umfangreicher Korpora literarischer Texte aus verschiedenen Sprachen für die komparatistische Analyse bietet, (b) wie sie konstruiert und gepflegt werden können, und (c) welche Abfragemöglichkeiten auf dem gegenwärtigen Stand der Technik sie bieten. In Betracht gezogen werden dafür sowohl bereits vorhandene und online zugängliche Korpora wie auch von einzelnen Forschergruppen erarbeitete, intern genutzte Korpora wie das Austrian Academy Corpus am ICLTT der ÖAW. Des weiteren wollen wir einen kursorischen Überblick über die bisher erprobten Ansätze computergestützter literaturwissenschaftlicher Analyse geben, um das gegenwärtige Spektrum der Methoden der Textevaluierung darstellen und zukünftige Desiderata aufzeigen zu können."
2016,DHd2016,posters-079.xml,RuCoCo: Automatische Koreferenzannotationen fuÃàr Russisch,"Desislava Zhekova (Centrum für Informations- und Sprachverarbeitung, LMU, München); Alena Mikhaylova (Centrum für Informations- und Sprachverarbeitung, LMU, München)","Koreferenzresolution, Alignierung, Russisch, Deutsch","Sammlung, Programmierung, Strukturanalyse, Beziehungsanalyse, Modellierung, Annotieren, Veröffentlichung, Computer, Daten, Sprache, Software, Text","Koreferenzresolution beschäftigt sich mit der Aufgabe unterschiedliche sprachliche Ausdrücke, die die gleichen Entitäten im Text beschreiben, automatisch zu identifizieren. Die meisten state-of-the-art Koreferenzresolutionssysteme basieren auf statistischen Verfahren und verwenden große vorannotierte Trainingskorpora (Pradhan et al., 2011). Die Abhängigkeit von Trainingskorpora stellt ein Problem für die Sprachen dar, für die keine Korpora verfügbar sind, die mit Koreferenzinformationen annotiert sind (Recasens et al., 2010; Pradhan et al., 2012; Zhekova, 2013). Diese Arbeit beschreibt ein Verfahren zur Gewinnung automatischer Koreferenzannotationen durch parallele Korpora für das Russische. Russisch ist eine der Sprachen, für die es noch keine frei verfügbaren Koreferenzannotationen gibt und die daher nicht mit statistischen Koreferenzsystemen bearbeitet werden können. Unser Ziel ist es, Korpora die mehr als einen Zieltext haben zu benutzen (z.B. mehrere Übersetzungen des gleichen Texts/Quelltexts), um die Koreferenzketten von mehreren Zieltexten in den Quelltext zu projizieren. Unser Vorgehen basiert auf der These, dass die Verwendung mehrerer Zieltexte eine bessere Identifikation der Ketten und Grenzen der Mentions (die potentiell koreferenten Phrasen) ermöglicht. Zusätzlich stellen wir die automatisch gewonnenen Koreferenzannotationen (RuCoCo (aus dem Englischen: Russian Coreference Corpus)) für den russischen Originaltext zur freien Verfügung. Schon oft wurden parallele Korpora für die Gewinnung automatisch generierter Koreferenzannotationen benutzt (Kobdani et al., 2011; Souza and Ora≈°an, 2011; Rahman and Ng, 2012), das Sprachenpaar Russisch-Deutsch wird allerdings wenig bearbeitet (Grishina and Stede, 2015). Darüber hinaus sind parallele Korpora üblicherweise aus einem Quell- und einem Zieltext gebildet (Ganitkevitch et al., 2013; Dolan and Brockett, 2005) und konzentrieren sich hauptsächlich auf die Bearbeitung der englischen Sprache. In Bezug auf das Russische sind uns keine frei verfügbaren Datensätze bekannt, die auch mehrere Zieltexte enthalten. Ein derartiger Korpus für Russisch, der aus dem Roman Aus diesem Grund haben wir einige Ansätze für die Alignierung von Texten für das Sprachenpaar Deutsch-Russisch implementiert (Zhekova et al., 2014), die in das frei verfügbare und interaktive Online Zunächst wird der deutsche Teil mit Koreferenzinformationen versehen, um sie anschließend in den russischen Teil der Paralleltexte projizieren zu können. Dafür wird im ersten Schritt das beste frei verfügbare Koreferenzresolutionssystem (laut die CoNLL Evaluationen), IMSCoref (Björkelund and Farkas, 2012), an die deutsche Sprache angepasst. Wir verwenden die SemEval-Datensätze für Deutsch (Recasens et al., 2010), die in das CoNLL-Format umgewandelt werden. Für die Erzeugung von Syntaxbäumen (ParseBits in CoNLL-Daten), die in den SemEval-Daten fehlen, wird der Als Baseline-Features werden die Features verwendet, die in IMSCoref für Englisch entwickelt wurden. Das ursprüngliche Featureset für Englisch enthält allerdings Informationen, die in den SemEval-Daten nicht zur Verfügung stehen. Entsprechend werden diese Features ausgelassen (z.B. Speaker-, Genre-Features, usw). Agreement-Features für Deutsch sind zusätzlich integriert worden. Tabelle 1: Systemevaluation fuÃàr Englisch (en) und Deutsch (de) mit den original (Orig), reduzierter (Red) und erweiterter (Erw) Featureset. Um die Güte des für Deutsch adaptierten IMSCoref sicherzustellen, wurde das System für beide Sprachen (Englisch und Deutsch) einem Testlauf unterzogen. Dazu haben wir vier Featuresets getestet: Um das IMSCoref auf die Bearbeitung der deutschen UÃàbersetzungen der russisch-deutschen Paralleltexte (Zhekova et al., 2015) (aligniert auf Satzebene) vorzubereiten, werden diese auch in das CoNLL-Format transformiert.  Zuerst werden die Texte mit dem Stanford-Tokenizer tokenisiert. Der TreeTagger (Schmid, 1994; Schmid, 1995) wird danach für das POS-Tagging eingesetzt. Abschließend werden die Texte der Datenaufbereitung mit GIZA++ (Och and Ney, 2003) auf Wortebene aligniert. Wir untersuchen drei verschiedene Ansätze für die Übertragung der Annotationen, die im Folgenden beschrieben sind.  Tabelle 2: Ergebnisse der Projektion von Koreferenzinformationen vom Deutschen ins Russische Da keine Gold-Standard-Annotationen vorliegen, werden vorläufig die ersten 30 Sätze des russischen Textes als ein Dokument im Sinne der Koreferenz betrachtet und manuell (von nur einem Annotator) mit Koreferenzketten annotiert. Die Ergebnisse sind in Tabelle 2 dargestellt, wo alle Übersetzungen (markiert als 1924, 1956, 2010 in der Tabelle) in allen drei Settings (S1, S2 und S3) aufgeführt sind. Die Ergebnisse zeigen, dass durchaus bei allen drei Übersetzungen vergleichbare Zahlen erreicht werden, was innerhalb der drei Settings nicht der Fall ist. Als Erstes zeigt ein Vergleich zwischen der Identifikation der Mentions (IM) für das erweiterte IMSCoref-System im Deutschen ( Eine qualitative Analyse zeigt, dass die meisten Fehler durch falsche Wortalignierungen entstehen. Mentions werden nicht gefunden und übertragen, desweiteren haben die projizierten Mentions oft einen falschen Anfang oder ein falsches Ende und tragen so zu den niedrigen Genauigkeiten bei. Oft führt auch die falsche Alignierung zu einer falschen Identifikation des Kopfes im Russischen, wodurch die Ermittlung der Mentionspan im Setting  3 stark beeinflusst wird. Ein Anteil der Fehler ist auch auf Fehlerprojektion basiert 'falsche Ketten im deutschen Text werden auch falsch weitergegeben. Jedoch ist es unser Hauptziel zu untersuchen, ob Korpora, die mehr als einen Zieltext enthalten, hilfreicher für die Projektion sein können als traditionelle Korpora. Dafür haben wir zwei zusätzliche Experimente durchgeführt (jeweils 1924/1956 und 1924/1956/2010 in Tabelle 2) '1924/1956 fügt die Koreferenzketten aus den beiden Übersetzungen (1924 und 1956) zusammen, während in 1924/1956/2010 die Ketten aus alle drei Übersetzungen zusammengefügt werden (das wird manuell nur für die ersten 30 Sätze des Originalwerks gemacht womit wir entgegen den Testset evaluieren können). Die Ergebnisse zeigen, dass die Benutzung von mehreren Texten sehr hilfreich sein kann, da sich der CoNLL-Score von 20.11% für die Übersetzung 1924 auf 22.41% für 1924/1956/2010 erhöht. Das ist ein sehr positives Ergebnis. Wir vermuten, dass mit einer besseren Alignierung und qualitativ hochwertigeren Dependenzannotation für das Russische, diese Verbesserung noch größer ausfallen würde. In dieser Arbeit haben wir gezeigt, dass parallele Korpora mit mehr als einem Zieltext für die Gewinnung von automatischen Koreferenzannotationen sehr hilfreich sein können, und dass dadurch Sprachen, die bislang für state-of-the-art Koreferenzsysteme völlig unerreichbar waren, damit bearbeitet werden können. Zusätzlich werden die Koreferenzannotationen für den russischen Originaltext und das manuell annotierte Testset zur freien Verfügung"
2016,DHd2016,vortraege-049.xml,Classification of Literary Subgenres,"Lena Hettinger (Universität Würzburg, Deutschland); Isabella Reger (Universität Würzburg, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland); Andreas Hotho (Universität Würzburg, Deutschland)","Genreklassifikation, Topic Modelling","Inhaltsanalyse, Netzwerkanalyse, Stilistische Analyse, Literatur, Text","Literary scholars and common readers use labels like educational novel, crime  novel or adventure novel to organize the large domain of fiction. In both  discourses the use of these categories is well-established even though they are  evolving and tend to be inconsistent. The classification of genres is one of the  standard tasks in document classification and has been researched intensively  (cf. Biber 1989;  Santini 2004; Freund et al 2006; Sharoff et al. 2010). Some  results seem impressive, for example distinguishing clear-cut genres like poetry  from fiction (Underwood 2014), but most texts on literary genre classification  emphasize, as the literature on genre classification in general, the variability  of genre signals (Allison et al. 2011: 19; Underwood et al. 2013; Underwood  2014). The scores for genre classification over all categories are therefore  often not very high. Jockers for example reports an accuracy of 67% (Jockers  2013: 81). Genre classification in general works best with most frequent words,  all words or character tetragrams (Freund et al. 2006; Sharoff et al. 2010) and  most of the reported experiments for literary genre classification also use all  words or only the In the following we will describe the corpus and the features we use for the task of subgenre classification. Our corpus consists of 628 German novels mainly from the 19th century  (roughly 1745 to 1935) obtained from sources like the As mentioned in section 1 we use three types of features (stylometric, topic based and network) that are described in more detail in Hettinger et al. (2015). Features are extracted and normalized to a range of [0,1] based on the whole corpus consisting of 628 novels. We use word frequencies as well as character tetragrams to represent stylometric features. We tested different amounts of most frequent words and decided to work with the top 3000 (mfw3000). Additionally we use the top 1000 character tetragrams (4gram). We use Latent Dirichlet Allocation (LDA) by Blei et al. (2003) to extract topics from our data. In literary texts topics sometimes represent themes, but more often they represent topoi, often used ways of telling a story or parts of it. For each novel we derive a topic distribution, i.e. we calculate how strongly each topic is associated with each novel. We try different preprocessing approaches and topic numbers and build ten models for each setting to reduce the influence of randomness in LDA models. In every setting we first remove a set of predefined stop words from the novels and then use LDA on our corpus of 628 novels. The different forms of preprocessing we use are: We use the character recognition system described in Jannidis et al. (2015) to identify the characters of each novel. Although the NER tool may be employed with co-reference resolution we do not make use of this option here. We extract proper names to build a network where each node is a character and the number of co-occurrences of two characters in the same paragraph is the weight of the edge between these two. The network of each novel is reduced to the most central characters and the most frequent interactions in order to bring out their basic shape. The network feature set consists of the total number of characters in a novel and six network measures: maximum degree centrality, global efficiency, transitivity, average clustering coefficient, central point dominance and density. Classification is done by means of a linear Support Vector Machine (SVM) as we have already shown in Hettinger et al. (2015) that it works best in this setting (see also Yu 2008). In each experiment we apply 100 iterations of 10-fold cross validations to account for the small data sets. The depicted results are the average over 1000 classification accuracy values. We want to investigate the following subgenre constellations: Depending on the setting the label distribution is often imbalanced. To make results comparable we use undersampling where in each of the 100 iterations a new sample is drawn from the larger class while all instances of the smaller class are used. This accounts for a majority vote (MV) baseline that always yields an accuracy score of 0.50 as both classes have equal size. To determine the influence of the LDA topic parameter a) adventure/non-adventure b) educational/non-educational c) social/non-social d) adventure/educational e) educational/social  When comparing different feature sets across our subgenre constellations we can  see that semantic based features (mfw, 4grams, lda) all perform quite good while  network features perform rather poorly (see figure 2). With an accuracy score of  more than 90% adventure novels seem to be fairly easy to differentiate from  other genres. In contrast, the other genres don""t show such a distinct signal  using surface features. As the classification performance of adventure/educational is quite impressive we  take a closer look at the discriminating words of these genres (Figure 3). Some  of the most typical words of adventure novels include To test whether authorship of novels influences our results we removed the author signal by allowing only one document per author. In this way, we construct a new dataset called ""uni"". The sampling is done once so that the same novels are used in each setting. As shown in figure 4, we observe a much lower quality after removing the authorship information indicating an overemphasized focus of features and models on the hidden authorship signal. This varies for different settings as adventure/educational shows a loss of 0.09 (blue lines) and educational/social loses 0.23 (red lines). The relatively small loss in the first setting is remarkable as it contains 8 novels per author on average. One would expect the opposite given the weaker author signal of two novels per author for the other categories. In the next experiment we test whether the combination of feature sets changes our classification results. To balance the size of the feature sets we use Principal Component Analysis (PCA) and construct 100 features from the 3000 mfw and 1000 4gram features each. As shown in figure 5 some feature sets improve when combined (e.g. 4gram100 and lda100) and for others (e.g. lda100 and network) performance decreases. But classification results vary greatly in this setting as signaled by standard deviation bars so these differences should not be overrated. In this work we classified subgenres of German novels using different feature sets (mfw 3000, 4gram, lda etc.). Some subgenres, like adventure novels, are much easier to classify than others. Most of the applied feature sets showed a varying but comparable performance except the network features. The weak performance of network features might be caused by the weak link between the novel genre and character constellation. The variability of the subgenre signal could not be countered by using higher level features like topics and network characteristics. Interestingly, the author signal has a strong influence on the classification quality. The strength of influence seems to depend on category but is visible in all experiments. In the future, we would like to extend our work by using different network features, work on advanced topic models and find a reliable indicator for plot. Another challenge we have not faced yet is the development of subgenres over time."
2016,DHd2016,vortraege-060.xml,Dramen als small worlds? Netzwerkdaten zur Geschichte und Typologie deutschsprachiger Dramen 1730-1930,"Peer Trilcke (Georg-August-Universität Göttingen, Deutschland); Frank Fischer (Niedersächsische Staats- und Universitätsbibliothek Göttingen); Mathias Göbel (Niedersächsische Staats- und Universitätsbibliothek Göttingen); Dario Kampkaspar (Herzog August Bibliothek Wolfenbüttel)","Netzwerkanalyse, Dramen, Literaturgeschichte, small worlds","Strukturanalyse, Netzwerkanalyse, Visualisierung, Literatur, Text","Neben dem ""klassischen"" strukturalistischen Paradigma, das sich wesentlich an  Theoremen der Linguistik orientiert (u. a. Lotman 1972; Titzmann 1977), gibt es  in der Literaturwissenschaft bereits seit Jahrzehnten Ansätze zu einer  Strukturanalyse, die sich auf die empirische Soziologie 'insbesondere auf die Unser derzeitiges Korpus umfasst 465 deutschsprachige Dramen (Zeitraum 1730 bis  1930), die aus dem Die diachrone Erstreckung unseres Dramenkorpus über ca. 200 Jahre deutscher  Literaturgeschichte macht es möglich, größere Entwicklungen im Bereich der  strukturellen Komposition von dramatischen Texten zu beobachten (erste  Überlegungen dazu haben wir in einem Blogpost skizziert:  Die von uns bisher erhobenen Werte zeigen, dass Dramen in dem untersuchten Zeitraum auf sehr unterschiedliche Weise strukturiert wurden. In der ""traditionellen"" Literaturwissenschaft wurden für solche unterschiedlichen ""Bauformen"" diverse Typologien entwickelt, in der Germanistik am bekanntesten ist Volker Klotz"" Unterscheidung in eine ""offene"" und eine ""geschlossen"" Dramenform (Klotz 1960). Diesen typologischen Impuls wollen wir aufgreifen und einen Vorschlag unterbreiten, wie sich mittels netzwerkanalytischer Daten bestimmte Typen der strukturellen Komposition von Dramen unterscheiden (und dann wiederum historisch verorten) lassen. Unser Vorschlag greift dabei Überlegungen aus der Forschung zu sog. Small-world-Netzwerken auf. Diese Forschungen setzen bei der Beobachtung an, dass die Werte von empirisch erhobenen Netzwerken nicht selten signifikant von entsprechenden Random-Netzwerken (also z. B. nach dem Erd≈ës-Rényi-Modell erstellten Graphen) abweichen. Abweichungen sind dabei insbesondere beim Clustering Coefficient, bei der Averge Path Length sowie bei der Degree Distribution zu beobachten (Albert / Barab√°si 2002). Für den hier projektierten Vortrag werden wir diese Werte 'sowie die Werte für die entsprechenden Random-Netzwerke 'für unser Gesamtkorpus erheben (sowie einen Workflow für die automatisierte Erhebung entwickeln) und diskutieren. Erste Testläufe deuten dabei darauf hin, dass sich auf diese Weise tatsächlich unterschiedliche Typen der strukturellen Komposition von Dramen beschreiben lassen könnten. So zeigen sich z. B. auffällige Unterschiede bei der Degree Distribution (s. exemplarisch die Tabellen für vier Dramen in Abbildung 2); und mit Blick auf den Clustering Coefficient zeigt sich, dass im Vergleich zu Random-Netzwerken signifikant höhere Werte, wie sie bei Small-world-Netzwerken zu erwarten sind, zwar in mehreren Fällen vorkommen, jedoch keineswegs für alle Dramennetzwerke charakteristisch sind (siehe exemplarisch die Werte in Abbildung 3). Im Vortrag werden wir diese Werte für alle Dramen unseres Korpus präsentieren; wir werden diskutieren, inwieweit sich hier 'aufbauend auf dem Small-world-Konzept 'netzwerkanalytisch basierte Typen der strukturellen Komposition von Dramen unterscheiden lassen und wir werden literarhistorisch fundiert erörtern, welche Eigenschaften der Dramen für die unterschiedlichen Werte verantwortlich sind. "
2016,DHd2016,posters-073.xml,"Kollaboratives Schreiben gestern, heute und morgen: Nutzen und Grenzen eines Visualisierungs- und Analysemodels aus der digitalen Literaturforschung","Heiko Zimmermann (Universität Trier, Deutschland)","digitale Literatur, kollaboratives Schreiben, Autorschaft, Visualisierung, Modellbildung","Entdeckung, Modellierung, Theoretisierung, Community-Bildung, Bewertung, Visualisierung, Computer, Interaktion, Literatur, Multimedia, Multimodale Kommunikation, Personen, Werkzeuge, Visualisierung","Sieht man vom gemeinschaftlichen Schreiben aktueller 'Wirklichkeiten' in Facebook ab, ist Novalis' Prophetie weit entfernt von kreativen Schreibprozessen der Gegenwart. Dennoch hat es unterschiedlichste Formen kollaborativen Schreibens seit jeher gegeben. Dieses Schreiben hat die Literaturkritik und -wissenschaft oft vor Probleme gestellt (vgl. Ede / Lunsford 1990). In den letzten Jahren haben die Möglichkeiten des vernetzten Schreibens am Computer neue Formen und Dimensionen kollaborativer Schreibenprozesse gefördert. Werke wie die Enzyklopädie Zur selben Zeit ist in der englischsprachigen Welt das Genre der digitalen Literatur  aufgekommen, welches die Literaturwissenschaft ebenfalls vor große Herausforderungen  stellt. Ein Hauptproblem ist das der Rekonfigurationen von Autor- und Leserschaft,  das mittels poststrukturalistischer Metaphern (Landow 2006; Simanowski 2002; Winko  1999) nicht hinreichend beschrieben werden konnte. Auch Zwischenwesen wie das Modell  des Wreaders, also des schreibenden Lesers, konnten die Abweichungen von tradierten  Rollen in der Literaturproduktion und -rezeption nicht sinnvoll modellieren. Aus den  selben Gründen funktionieren auch buchgeschichtliche Modelle des Literaturmarktes  wie das von Robert Darnton (1982) nur bedingt, um die Wirklichkeit digitaler  Literatur zu beschreiben. Um das Problem der Autor- und Leserschaft und unzureichender tradierter  Modellierungen zu lösen, wurde das visuelle Beschreibungs- und Analysemodell des  textuellen Handlungsraums entwickelt (Zimmermann 2015a). Es basiert auf dem  Texton-Skripton-Modell von Espen Aarseth (1997: 62-65) und ordnet allen am Text  handelnden Akteuren einen eindeutigen Platz im Handlungsraum zu, der abhängig ist  von der Art und Weise und vom Zeitpunkt ihres Handelns am Text im Kontinuum von  Produktion und Rezeption (vgl. Abbildung 1). Anwendungen dieses Modells waren bisher  auf englische digitale Literatur beschränkt und haben in diesem Feld ergeben, dass  es bestimmte Konstellationen von Handelnden in der Literaturproduktion und  -rezeption, beispielsweise Foucaults Idee einer beherrschenden Stellung der  Autorfunktion in literarischen Diskursen, in Frage stellt (Zimmermann 2015b). Der vorgeschlagene Vortrag soll gleichsam mehrere Brücken zwischen akademischen Disziplinen und literarischen Traditionen schlagen. Das Modell des textuellen Handlungsraums, das aus dem Feld der elektronischen Literaturforschung - und damit aus einem Kerngebiet der digitalen Geisteswissenschaften, sofern diese nicht allein über Methoden und Werkzeuge definiert werden - stammt, soll nicht nur auf digitale englischsprachige Literatur angewendet werden, sondern auch auf nicht-digitale deutsche und englischsprachige literarische Texte der Gegenwart und des 20. Jahrhunderts. Damit werden Verbindungen zwischen verschiedenen Literaturen (zeitlich, sprachlich) und akademischen Feldern (digitale Literaturforschung, digitale Geisteswissenschaften, traditionelle Literaturwissenschaft) hergestellt. Nachdem das Modell im Vortrag kurz vorgestellt wurde, fragt dieser nach den Formen von Autor- und Leserschaft ausgewählter kollaborativ geschriebener Texte, nach Möglichkeiten solches Schreiben sinnvoll zu klassifizieren und danach, ob sich Rückschlüsse auf die (kommerzielle) Verwertbarkeit ebendieser Literatur ziehen lassen. Flankierend wird damit eine Fallstudie für die Permeabilität traditioneller Literaturanalyse für Modelle aus dem Bereich der digitalen Literaturwissenschaft vorgestellt, und es werden die Potentiale und Grenzen einer derartigen Visualisierung literarischen Schaffens aufgezeigt."
2016,DHd2016,sektionen-002.xml,"""Delta"" in der stilometrischen Autorschaftsattribution","Stefan Evert (Universität Erlangen-Nürnberg, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland); Friedrich Michael Dimpel (Universität Erlangen-Nürnberg, Deutschland); Christof Schöch (Universität Würzburg, Deutschland); Steffen Pielström (Universität Würzburg, Deutschland); Thorsten Vitt (Universität Würzburg, Deutschland); Isabella Reger (Universität Würzburg, Deutschland); Andreas Büttner (Universität Würzburg, Deutschland); Thomas Proisl (Universität Erlangen-Nürnberg, Deutschland)","Stilometrie, Autorschaftsattribution, Delta","Stilistische Analyse, Literatur, Text","Stilometrische Verfahren der Autorschaftsattribution haben eine lange Tradition in den digitalen Geisteswissenschaften: Mit der Analyse der Ein jüngerer Meilenstein der stilometrischen Autorschaftsattribution ist ohne Zweifel das von John Burrows (2002) vorgeschlagene ""Delta""-Maß zur Bestimmung der stilistischen Öhnlichkeit zwischen Texten. Die beeindruckend gute Performance von Delta in verschiedenen Sprachen und Gattungen sollte allerdings nicht darüber hinwegtäuschen, dass die theoretischen Hintergründe weitgehend unverstanden geblieben sind (Argamon 2008). Anders ausgedrückt: Wir wissen, dass Delta funktioniert, aber nicht, warum es funktioniert. In diesem Kontext möchte die hier vorgeschlagene Sektion den aktuellen Stand der Forschung in der stilometrischen Autorschaftsattribution mit Delta vorstellen und neueste Entwicklungen anhand konkreter, eigener Untersuchungen demonstrieren. Jeder der drei Vorträge der Sektion leistet hierzu einen Beitrag: Die drei Beiträge demonstrieren auf diese Weise verschiedene aktuelle Entwicklungen in der stilometrischen Autorschaftsattribution mit Delta und seinen Varianten. Sie zeigen, wie bei der Anwendung stilometrischer Distanzmaße auf ganz unterschiedliche Gegenstandsbereiche ähnliche methodische Fragen zu berücksichtigen sind. Und sie partizipieren direkt an aktuellsten, internationalen Entwicklungen bei der Verwendung von Distanzmaßen wie Delta für die stilometrische Autorschaftsattribution. Burrows Delta ist einer der erfolgreichsten Algorithmen der Computational Stylistics (Burrows 2002). In einer ganzen Reihe von Studien wurde seine Brauchbarkeit nachgewiesen (z. B. Hoover 2004, Rybicki / Eder 2011). Im ersten Schritt bei der Berechnung von Delta werden in einer nach Häufigkeit sortierten Token-Dokument-Matrix alle Werte normalisiert, indem ihre relative Häufigkeit im Dokument berechnet wird, um Textlängenunterschiede auszugleichen. Im zweiten Schritt werden alle Werte durch eine z-Transformation standardisiert:  wobei Trotz seiner Einfachheit und seiner praktischen Nützlichkeit mangelt es bislang allerdings an einer Erklärung für die Funktionsweise des Algorithmus. Argamon (2008) zeigt, dass der dritte Schritt in Burrows Delta sich als Berechnung des Smith und Aldrige (2011) schlagen vor, wie im Information Retrieval üblich   (Baeza-Yates / Ribeiro-Neto 1999: 27), den Cosinus des Winkels zwischen den   Dokumentenverktoren zu verwenden. Die Cosinus-Variante von Delta übertrifft   Burrows Delta fast immer an Leistungsfähigkeit und weist, im Gegensatz zu   den anderen Varianten, auch bei der Verwendung sehr vieler MFWs keine   Verschlechterung auf (Jannidis et. al. 2015). Es stellt sich die Frage,   warum Delta Entscheidend für unsere weitere Analyse war die Erkenntnis, dass man die Verwendung des Cosinus-Abstands als eine Vektor-Normalisierung verstehen kann, da für die Berechnung des Winkels 'anders als bei Manhattan- und Euklidischem Abstand 'die Länge der Vektoren keine Rolle spielt (vgl. Abb. 1). Experimente haben gezeigt, dass eine explizite Vektor-Normalisierung auch die Ergebnisse der anderen Deltamaße erheblich verbessert und Leistungsunterschiede zwischen den Delta-Varianten weitgehend neutralisiert (Evert et al. 2015). Daraus wurden zwei Hypothesen abgeleitet: Für die hier präsentierten Untersuchungen verwenden wir drei vergleichbar  aufgebaute Korpora in Deutsch, Englisch und Französisch. Jedes Korpus  enthält je 3 Romane von 25 verschiedenen Autoren, insgesamt also jeweils  75 Texte. Die deutschen Romane aus dem 19. und dem Anfang des 20.  Jahrhunderts stammen aus der Digitalen Bibliothek von Um die Rolle von Ausreißern und damit die Plausibilität von H1 näher zu untersuchen, ergänzen wir Delta  Wir bezeichnen diese Abstandsmaße allgemein als L Abbildung 2 vergleicht vier unterschiedliche L Eine Vektor-Normalisierung verbessert die Qualität aller Delta-Maße  erheblich (vgl. Abb. 3). Argamons Delta Ein anderer Ansatz zur Abmilderung von Ausreißern besteht darin, besonders extreme Insgesamt erweist sich Hypothese H1 somit als nicht haltbar. H2 wird durch das gute Ergebnis der Vektor-Normalisierung unterstützt, kann aber nicht unmittelbar erklären, warum auch das Abschneiden von Ausreißern zu einer deutlichen Verbesserung führt. Um diese Hypothese weiter zu untersuchen, wurden reine ""Schlüsselprofil""-Vektoren erstellt, die nur noch zwischen überdurchschnittlicher (+1), unauffälliger (0) und unterdurchschnittlicher (–1) Häufigkeit der Wörter unterscheiden (vgl. Abb. 4, rechts unten). Abbildung 6 zeigt, dass solche Profil-Vektoren hervorragende Ergebnisse erzielen, die der Vektor-Normalisierung praktisch ebenbürtig sind. Selbst das besonders anfällige L H1, die Ausreißerhypothese, konnte widerlegt werden, da die Vektor-Normalisierung die Anzahl von Extremwerten kaum verringert und dennoch die Qualität aller L Burrows"" Delta (Burrows 2002) hat sich in Autorschaftsfragen etabliert; viele Studien zeigen, dass Delta für germanische Sprachen ausgezeichnet funktioniert (Hoover 2004b; Eder / Rybicki 2011; Eder 2013a; Eder 2013b; für das Neuhochdeutsche zuletzt Jannidis¬†/ Lauer 2014; Evert et al. 2015). Beim Mittelhochdeutschen ist jedoch die Schreibung nicht normiert: Das Wort ""und"" kann als ""unde"", ""unt"" oder ""vnt"" verschriftet sein. Ein Teil dieser Varianz wird zwar in normalisierten Ausgaben ausgeglichen, jedoch nicht vollständig. Viehhauser (2015) hat in einer ersten Delta-Studie zum Mittelhochdeutschen diese Probleme diskutiert: Wolfram von Eschenbach benutzt zum Wort ""kommen"" die Präteritalform ""kom"", Hartmann von Aue verwendet ""kam"", eine Form, die eher in den südwestdeutschen Raum gehört. Die Bedingungen für den Einsatz von Delta auf der Basis der Normalisierte Texte sind besser für Autorschaftsstudien geeignet, da hier die Zufälligkeiten von Schreibergraphien reduziert sind; Längenzeichen stellen dort meist weitere lexikalische Informationen zur Verfügung 'etwa zur Differenzierung von ""sin"" (""Sinn"") versus ""s√Æn"" (""sein""; allerdings ohne Disambiguierung von ""s√Æn"" als verbum substantivum oder Pronomen). In diplomatischen Transkriptionen sind dagegen etwa ""u-e"" Superskripte und andere diakritische Zeichen enthalten; die gleiche Flexionsform des gleichen Wortes kann in verschiedenen Graphien erscheinen. Anlass zu vorsichtigem Optimismus bietet allerdings eine Studie von Eder (2013a), die den Einfluss von Noise (wie z. B. Schreibervarianten) analysiert 'mit dem Ergebnis (u. a.) für das Neuhochdeutsche, dass ein zufälliger Buchstabentausch von 12% bei 100-400 MFWs die Ergebnisse kaum beeinträchtigt; bei einer mäßig randomisierten Manipulation der MFWs-Frequenzen verschlechtert sich die Quote der korrekten Attributionen bei 200-400 MFWs ebenfalls kaum. Ersetzt man im Autortext Passagen durch zufällig gewählte Passagen anderer Autoren, ergibt sich bei der Quote lediglich ein ""gentle decrease of performance""; im Lateinischen bleibt die Quote gut, selbst nachdem 40% des Originalvokabulars ausgetauscht wurden. Während die 17 Texte, die Viehhauser analysiert hat, in normalisierten Ausgaben vorliegen, habe ich zunächst 37 heterogene Texte von sieben Autoren mit Stylo-R (Eder / Kestemont et al. 2015) getestet sowie drei Texte mit fraglicher Autorzuschreibung zu Konrad von Würzburg. Ein Teil ist normalisiert (Hartmann, Wolfram, Gottfried, Ulrich, Wirnt, Konrad), andere liegen zum Teil in diplomatischen Transkriptionen vor: Bei Rudolf von Ems sind ""Gerhard"", ""Alexander"" und ""Barlaam"" normalisiert, nicht normalisiert sind ""Willehalm"" und ""Weltchronik"" (hier etwa ""ubir"" statt ""über""). Beim Stricker ist lediglich der ""Pfaffe Amis"" normalisiert. Per Skript wurden Längenzeichen eliminiert, damit nicht Texte mit und ohne Längenzeichen auseinander sortiert werden. Tustep-Kodierungen etwa für Superskripte habe ich in konventionelle Buchstaben transformiert. Dennoch bleiben große Unterschiede: Die Genitivform zu ""Gott"" lautet teils ""gotes"", teils ""gotis"", so dass eigentlich eine primäre Sortierung entlang der Unterscheidung normalisiert–nicht-normalisiert zu erwarten wäre. Das Ergebnis ist jedoch frappierend: Auf der Basis von 200 MFWs (diesen Parameter verwenden auch Eder 2013b und Viehhauser 2015) gelingt stylo-R ohne Pronomina und bei Culling=50% eine fehlerfreie Sortierung nach Autorschaft; Delta ordnet Rudolf zu Rudolf 'ob normalisiert oder nicht. Dieser Befund ist Anlass für eine Serie an automatisierten Tests in Anlehnung an Eder (2013b): Bei welchem Vektor und ab welcher Textlänge liefert Delta zuverlässige Ergebnisse? Wie wirkt sich das Einbringen von Noise aus? Per Perlskript wurde ein Delta-Test implementiert, der in einer großen Zahl an Iterationen (13.425 Delta-Berechnungen) verschiedene ""Ratetexte"" mit bekannter Autorschaft gegen ein Validierungskorpus mit bekannter Autorschaft jeweils daraufhin prüft, ob für jeden Text im Ratekorpus tatsächlich der niedrigste Delta-Wert bei einem Text des gleichen Autors im Validierungskorpus herauskommt. Gegen ein heterogenes Validierungskorpus mit 18 Texten wurden 19 normalisierte Ratetexte getestet; gegen ein heterogenes Validierungskorpus mit 15 Texten wurden 13 nicht-normalisierte Ratetexte getestet. Ermittelt wurde der Prozentsatz der richtig erkannten Autoren für jeweils eine Vektorlänge; die Vektorlänge wurde in 100er Schritten bis auf 2.500 MFWs erhöht. Pronomina wurden beseitigt. Bei den normalisierten Ratetexten ist die Erkennungsquote sehr gut bis 200–900 MFWs, bei den nicht-normalisierten sehr gut für 100–600 MFWs. Interessante Fehlattributionen 'etwa Strickers ""Pfaffe Amis"" und Konrads ""Herzmäre"" 'machen weitere Validierungsläufe nötig: Der normalisierte ""Pfaffe Amis"" wurde gegen einen nicht-normalisierten Stricker-Text getestet; das ""Herzmäre"" ist kurz (2991 Wörter). Während Burrows (2002) davon ausgeht, dass Delta ab einer Textlänge von 1.500 Wörtern anwendbar ist, zeigt Eder (2013b), dass Delta im Englischen ab 5.000 Wörtern sehr gute und unter 3.000 Wörtern teils desaströse Ergebnisse liefert; nur im Lateinischen werden ab 2.500 Wörtern gute Ergebnisse erreicht. Hier wurde die Textlänge linear begrenzt, die Texte wurden nach 1000, 2000 Wörtern usw. abgeschnitten. Das Korpus ist kleiner als zuvor, da zu kurze Texte herausgenommen wurden (normalisierte: 16 Texte Validierungskorpus, 15 Ratekorpus; nicht-normalisierte 14 Validierungskorpus, 6-7 Ratekorpus; 10.056 Delta-Berechnungen). Gleiches Korpus wie zuvor; 167.600 Delta-Berechnungen. Da die bag-of-words randomisiert zusammengestellt wird, schwankt die Erkennungsquote etwas, daher wurde jeder Test pro Textlänge und Wortlistenlänge 25x durchgeführt und der Mittelwert dieser 25 Erfolgsquoten verwendet. Aus einer Noise-Datei mit >18.000 mittelhochdeutschen und altfranzösischen Wortformen ohne Duplikate werden die Ratetexte prozentual aufsteigend randomisiert: Teile der bag-of-words werden gegen fremdes Sprachmaterial ausgetauscht, um Fehler in der √úberlieferungskette zu simulieren. Die Kurve verläuft nicht konstant linear, da für jede bag-of-words-Berechnung erneut Noise randomisiert hinzugefügt wird (hier 10 Iterationen pro Einzelwert; 1.179.360 Delta-Berechnungen). Beim Test der Vektorlänge (vgl. 3.2.1.) bleiben die Erkennungsquoten bei normalisierten Ratetexten sehr gut bis 200–900 MFWs. Bei den nicht-normalisierten Texten sind die Quoten nur für einen kleineren Bereich sehr gut: für 100–600 MFWs. Bei einer Begrenzung der Textlänge (Cutoff; vgl. 3.2.2.) bleiben die Ergebnisse bei normalisierten Texten nur ab 4000 Wörtern Textlänge weitgehend gut bis sehr gut. Schlecht sieht es bei den nicht-normalisierten Texten aus: Sehr gut ist die Quote nur bei 800 MFWs und 5000 Wörtern, ansonsten weithin desaströs. Bag-of-words (vgl. 3.2.3.) bieten dagegen stabilere Ergebnisse: Bei den normalisierten Texten sind bei einer Textlänge von 5000 die Quoten sehr gut bei 400-800 MFWs. Bei den nicht-normalisierten Texten ist die Quote wiederum nur bei Textlänge 5000 und 800 MFWs sehr gut. Bei kürzeren Texten und anderen Frequenzen verschlechtern sich die Quoten massiv, allerdings bleiben sie noch deutlich besser als beim Cutoff-Test. Bei normalisierten Texten werden durch das Eliminieren von Pronomina geringfügig bessere Quoten erreicht (vgl. 3.2.4.), bei nicht-normalisierten Texten etwas schlechtere Quoten. Stabil bleiben die Quoten bei normalisierten Texten nach dem Einbringen von Noise (vgl. 3.2.5.): Solange nicht mehr als 17% des Vokabulars ausgetauscht wurden, werden die Erkennungsquoten nur etwas schlechter. 600-800 MFWs liefern sehr gute Erkennungsquoten bis 20%. Auch die Quoten bei nicht-normalisierten Texte sind einigermaßen stabil, solange nicht mehr als 20% Noise eingebracht werden: Der Bereich von 600-800 MFWs liefert bis 9% Noise noch sehr gute und bis 22% noch gute Ergebnisse. Die Stabilität der Erkennungsquoten gibt Grund zum Optimismus für eine Anwendbarkeit bei normalisierten mittelhochdeutschen Texten. Am besten geeignet ist der Vektorbereich von 400-800 MFWs bei langen Texten mittels bag-of-words. Auch wenn die Ergebnisse für nicht-normalisierte Texte etwas zurückfallen, hat mich angesichts der wilden mittelhochdeutschen Graphien doch überrascht, dass die Delta-Performanz derart robust bleibt. Während jedoch etwa Eder Validierungsstudien mit über 60 Texten durchführen konnte, ist es um die digitale Verfügbarkeit von längeren mittelhochdeutschen Texten, von denen mindestens zwei Texte vom gleichen Autor verfasst wurden, derzeit noch deutlich schlechter bestellt. Die Aussagekraft der vorliegenden Studien wird daher durch die Korpusgröße v. a. bei den nicht-normalisierten Texten limitiert. Bei einem weiteren Versuch geht es darum, die Einflüsse von Schreibergraphie und Normalisierungsart zu reduzieren, indem nicht der Wortschatz, sondern abstraktere Daten verwendet werden: Nach Hirst und Feiguina (2007) erzielen Tests auf der Basis von Part-of-Speech–Bigrammen gute Ergebnisse; für das Mittelhochdeutsche ist jedoch noch kein Part-of-Speech-Tagger in Sicht. 2014 habe ich das Metrik-Modul aus meiner Dissertation grundlegend   überarbeitet, die Fehlerquote reduziert (nun unter 2%) und es ins Internet   zur freien Benutzung eingestellt (Dimpel 2015). Dieses Modul gibt Kadenzen   aus (etwa ""weiblich klingend""). Die metrische Struktur wird mit ""0""   (unbetonte Silben) und ""1"" (betonte Silben) ausgegeben; der dritte   ""Parzival""-Vers hat das Muster ""01010011"". Anstatt mit MFWs habe ich   Metrikmuster und Kadenzinformationen verwendet und so die Metrikdaten als   ""Worte"" testen lassen. Da ein weniger variationsreiches Ausgangsmaterial   verwendet wird, habe ich wie Hirst und Feiguina (2007) mit Bigrammen   gearbeitet. Auch ein Metrik-Delta-Plot mit Stylo-R clustert Autoren hier fehlerlos. Validierungstests sind bislang nur mit einem kleineren Korpus möglich, da das Metrikmodul Längenzeichen benötigt und nur für Texte mit vierhebigen Reimpaarversen konstruiert ist. Bei 13 Ratedateien und 11 Validierungsdateien ergibt sich bei 250–300 ""MFWs"" eine Erkennungsquote von 92,3%. Ein erfreuliches Ergebnis: (1) Bei Tests auf Grundlage von Metrik-Daten ist eine etwas geringere Abhängigkeit von Schreibergraphie und von Normalisierungsgewohnheiten gegeben. Zwar hat es mitunter metrische Eingriffe der Herausgeber gegeben, aber längst nicht immer. Wenn ein Herausgeber aus metrischen Gründen lieber das Wort ""unde"" statt ""unt"" verwendet, dann geht in den Metrik-Delta ein ähnlicher Fehler wie in den konventionellen Delta-Test ein. Immerhin immunisieren Metrik-Daten gegen Graphie-Varianten wie ""und"" oder ""unt"". (2) Zudem kann Autorschaft offenbar nicht nur mit dem vergleichsweise einfachen Parameter MFWs dargestellt werden: Nicht nur eine pure Wortstatistik führt zum Ziel, vielmehr erweist sich auch die Kompetenz zum philologischen Programmieren und zur filigranen Textanalyse als fruchtbar. (3) Bei der metrischen Struktur handelt es sich um ein Stilmerkmal, das Autoren oft intentional kunstvoll gestalten. Während es als communis opinio gilt, dass vor allem die unbewussten Textmerkmale wie MFWs autorspezifisch sind, gelingt es nun auch über ein wohl oft bewusst gestaltetes Stilmerkmal, Autorschaft zu unterscheiden. Man muss also den Dichtern nicht nur einen unbewussten stilistischen Fingerabdruck zutrauen, vielmehr lässt sich Autorschaft zumindest hier über ein Merkmal erfassen, das dem bewussten künstlerischen Zugriff unterliegen kann. Stilometrie ist der Versuch, sprachliche Besonderheiten durch statistische   Methoden herauszustellen und zu vergleichen, um damit unter anderem   Rückschlüsse auf die Urheberschaft eines Textes ziehen zu können. Als   probates Mittel bei der Autorschaftsattribuierung hat sich die Analyse der   Verwendung der häufigsten Wörter bewährt. Insbesondere Varianten des von   Burrows (2002) vorgeschlagenen Deltamaßes haben sich als sehr erfolgreich   erwiesen (Hoover 2004a; Eder / Rybicki 2011). Faktoren der Zusammensetzung   des Textkorpus, die sich negativ auf die Qualität der Ergebnisse auswirken   können, sind unter anderem zu kurze Texte (Eder 2015), unterschiedliche   Genres der Texte (Schöch 2013) und eine √úberlagerung von Autor- und   √úbersetzerstilen (Rybicki 2012). Gerade inhaltliche Unterschiede zwischen   Texten stellen ein Hindernis bei der Erkennung der Autoren dar, das nur mit   erheblichem technischen Aufwand überwunden werden kann (Stamatatos et al.   2000; Kestemont et al. 2012). In unserem Beitrag verwenden wir Deltamaße zur Identifikation von √úbersetzern. Textgrundlage ist eine Sammlung von im 12. Jahrhundert entstandenen arabisch-lateinischen √úbersetzungen wissenschaftlicher Texte aus verschiedenen Disziplinen. Wir zeigen eine Möglichkeit auf, wie die aus den oben genannten Faktoren resultierenden Limitierungen durch den Einsatz maschineller Lernverfahren kompensiert werden können. Gleichzeitig eröffnet sich dadurch eine Möglichkeit, unter den häufigsten Wörtern solche zu identifizieren, die eher Informationen zum √úbersetzer oder eher zur Disziplin tragen. Die hier verwendete Textsammlung wurde mit dem philologischen Ziel angelegt,   die √úbersetzer zu identifizieren, die im 12. Jahrhundert eine Vielzahl von   Texten aus dem Arabischen ins Lateinische übertragen und damit in den   verschiedensten Disziplinen die weitere Entwicklung der europäischen   Wissenschaften nachhaltig beeinflusst haben (Hasse / Büttner in   Vorbereitung) Für die Experimente wird ein Testkorpus so zusammengestellt, dass von jedem   √úbersetzer und aus jeder Disziplin mindestens drei Texte zur Verfügung   stehen. Dieses besteht aus insgesamt 37 Texten von 5 √úbersetzern, wobei die   Texte aus 4 Disziplinen stammen (siehe Abb. 18). Das daraus resultierende   Textkorpus ist nicht balanciert: Die Anzahl der Texte pro √úbersetzer ist   ungleich verteilt, die Länge der Texte liegt zwischen 500 und fast 200000   Wörtern; insgesamt sind die Texte auch deutlich kürzer als diejenigen der   oft verwendeten Romankorpora (vgl. etwa Jannidis et al. 2015). Weitere, die Analyse erschwerende Faktoren sind Doppelübersetzungen desselben  Originaltextes durch zwei √úbersetzer und die 'historisch nicht völlig klar  belegte 'Zusammenarbeit einiger √úbersetzer. Auf der anderen Seite sind die  unterschiedlichen Disziplinen prinzipiell klarer und eindeutiger  unterscheidbar als literarische Subgenres in Romankorpora. Ausgehend von Burrows ursprünglichem Deltamaß (Burrows 2002) wurde eine  ganze Reihe von Deltamaßen für die Autorschaftszuschreibung  vorgeschlagen (bspw. Hoover 2004b, Argamon 2008, Smith / Aldridge 2011,  Eder et al. 2013). Alle Maße operieren auf einer Term-Dokument-Matrix  der Für die folgenden Experimente verwenden wir Kosinus-Delta, das sich unter anderem bei Jannidi, Pielström, Schöch und Vitt (2015) sowie Evert, Proisel und Jannidis et al. (2015) als das robusteste Mitglied der Delta-Familie erwiesen hat. Rekursive Merkmalseliminierung (recursive feature elimination, RFE) ist eine von Guyon, Weston, Barnhill und Vapnik (2002) vorgeschlagene Methode zur Selektion einer möglichst kleinen Teilmenge von Merkmalen, mit der trotzdem möglichst optimale Ergebnisse mit einem überwachten maschinellen Lernverfahren erzielt werden können. Evert, Proisel und Jannidis et al. (2015) experimentieren zur Autorschaftszuschreibung mit durch RFE ermittelten Termen als Alternative zu den üblichen Da RFE auf einem überwachten Lernverfahren (üblicherweise einem In den folgenden Experimenten kombinieren wir beide Varianten und verkleinern die Merkmalsmenge (also die Menge der verwendeten Wörter) zunächst schrittweise auf die 500 besten Merkmale, um anschließend die optimale Merkmalsmenge zu bestimmen. Zunächst führen wir mit dem Testkorpus einige Versuche zur Anpassung der stilometrischen Methoden durch. Als Maß der Qualität des Clusterings dient dabei der Da das Hauptziel eine korrekte Zuordnung der √úbersetzer ist, soll die Menge der 500 häufigsten Wörter (im Folgenden Durch RFE wählen wir aus der Gesamtmenge weniger als 500 Wörter aus. Mit 483 Wörtern ist eine perfekte Klassifikation nach √úbersetzern möglich. Wenig überraschend erzielen wir mit diesen Wörtern auch ein perfektes Clustering der Texte nach √úbersetzern (ARI Die Analyse der Die 432 Wörter aus MFW500, die in der Menge der RFE-selektierten Wörter nicht enthalten sind, unterscheiden, wie erwartet, deutlich schlechter zwischen √úbersetzern (ARI Bei den Disziplinen erzielt die Schnittmenge der dafür mit RFE ausgewählten Wörter mit MFW500 sogar perfekte Ergebnisse (Anzahl der Merkmale: 109, ARI=1,0). Die Differenzmenge zeigt hier allerdings nicht den oben beschriebenen Effekt. Zwar ist die Clusteringqualität nach Disziplinen deutlich schlechter als der mit MFW500 erzielte Wert (ARI Um die Robustheit der Ergebnisse zu prüfen und insbesondere gegen ein Overfitting durch das RFE-Verfahren abzusichern, kann das bisher Beschriebene mit einem in ein Trainingsset und ein Testset aufgeteilten Korpus wiederholt werden, wobei die RFE-selektierten Wörter aus dem Trainingsset bestimmt und im Testset getestet werden. Dabei lassen sich die mit dem Gesamtkorpus beschriebenen Effekte reproduzieren, wenn auch 'aufgrund der dann sehr kleinen Textanzahl 'in schwächerer Ausprägung. Durch die Experimente wurde gezeigt, dass sich die Menge der Weitere Experimente in diesem Kontext werden dem Versuch dienen, die unterscheidenden Wörter besser zu charakterisieren, sodass idealerweise auch ohne maschinelles Lernen eine Auswahl der Merkmale möglich wird. Zudem steht eine Anwendung der Methode auf andere Textkorpora aus."
2016,DHd2016,posters-070.xml,"Visuelle Möglichkeiten der Textkollation anhand des Beispiels eines Vergleiches von Erich Kästners ""Fabian"" und ""Der Gang vor die Hunde""","Jan-Erik Stange (Fachhochschule Potsdam, Deutschland)","Visualisierung, Text, Kollation, Literatur","Gestaltung, Programmierung, Visualisierung, Computer, Literatur, Text, Visualisierung","Verschiedene Auflagen literarischer Werke zu unterschiedlichen Zeiten werden häufig durch Leser als originäres und statisches Erzeugnis von Autoren wahrgenommen. In der Realität führen unterschiedliche Einflussfaktoren, wie etwa das Einwirken eines repressiven staatlichen Zensurapparates auf die Veröffentlichung von Werken, das Nachbearbeiten durch Autoren selbst zu späteren Zeitpunkten oder in besonderem Maße auch die Wandlung durch Übersetzungen aus anderen Sprachen zu einer Varianz in den Fassungen, die von geringen Unterschieden in der Zeichensetzung oder unterschiedlichen Schreibweisen bis zu gänzlich anderen Inhalten reicht. Beispielhaft hierfür sind etwa die Unterschiede zwischen der 1931 veröffentlichten Fassung des Romans ""Fabian"" von Erich Kästner und der erst kürzlich wiederentdeckten und veröffentlichten Originalfassung des Romans ""Der Gang vor die Hunde"". Für die Deutsche Verlags-Anstalt war Kästners Originalmanuskript an vielen Stellen zu provokativ und sie stimmte einer Veröffentlichung nur zu unter der Bedingung, dass Kästner diese Stellen entschärfte. Gut lässt sich die Varianz auch anhand der zahllosen Übersetzungen von Shakespeare-Werken in andere Sprachen feststellen, die über die Jahrhunderte entstanden sind. In der Literaturwissenschaft erlauben sogenannte Kollationstools einen Vergleich verschiedener Fassungen eines Textes. Dieses Vorgehen bezeichnet man als textkritische Methode. Sie hat zum Ziel, oben genannte Einflussfaktoren auf die Gestalt eines Textes zu identifizieren, indem man verschiedene Textfassungen Satz für Satz miteinander vergleicht. Typische Tools dieser Art sind etwa ""CollateX"" oder ""JuXta"". Hinsichtlich des Interfaces bleiben diese Tools recht nah an der Textebene. Ohne Frage ist diese Nähe zur Textebene für eine Detailanalyse unerlässlich, erschwert es aber durch das Fehlen einer Überblicksdarstellung Zusammenhänge zwischen weiter auseinanderliegenden Textfragmenten zu erkennen, Muster zu identifizieren, die sich auf der Detailebene nicht erschließen und als unterstützende Navigationsebene, das schnelle Bewegen zwischen voneinander entfernten Positionen im Text. Auch ermöglicht der Überblick eine schnelle Einschätzung, welche Stellen des Textes für die Analyse besonders interessant sein könnten. Hierfür bietet sich eine visuelle Kodierung des Textes an, eine navigierbare interaktive Datenvisualisierung, da sie durch die Komplexitätsreduktion und die Konzentration auf bestimmte Attribute der Wörter eine komprimiertere Darstellung erlaubt. Der Vortrag zeigt anhand des oben genannten Kästner-Beispiels neuartige   Visualisierungsmöglichkeiten für Textkollationen auf und baut hierbei auf   bestehenden Visualisierungsansätzen auf. Hier ist vor allem Ben Frys Projekt zu   Darwins Werk ""On the Origin of Species: The Preservation of Favoured Traces"" zu   nennen. Die explorative Datenvisualisierung erlaubt es dem Betrachter, einen   Gesamtüberblick über die Entstehung des Werkes zu erhalten. Durch eine Farbkodierung   für die unterschiedlichen Ausgaben ist es auf einen Blick ersichtlich, welche   Abschnitte in welcher Auflage hinzugefügt wurden und welche Önderungen in den   verschiedenen Ausgaben vorgenommen worden sind. Zu jedem als farbigen Strich   dargestellten Absatz lässt sich in dieser Ansicht die entsprechende Textstelle   anzeigen, wenn man sich mit der Maus darüberbewegt. In der auf dem Poster vorgestellten Visualisierung der beiden Kästner-Fassungen dient die Farbkodierung dazu, den Grad der Abweichung zwischen Originalfassung und angepasster Fassung anzuzeigen. Satz für Satz können so die beiden Ausgaben miteinander verglichen werden. Als verwandtes Projekt, das Kollationen ebenfalls auf visuellem Wege zugänglich macht, ist hier außerdem die experimentelle Visualisierung ""TransVis"" zu nennen, entstanden innerhalb des Projektes ""Version Variation Visualization"", die es erlaubt, die insgesamt 37 deutschen Übersetzungen von Shakespeares Othello in einem visuellen Interface nebeneinanderzustellen und zu vergleichen. Öhnlich wie in diesem Projekt, gestattet es auch die Kästner-Visualisierung, Satz für Satz zu vergleichen, wobei visuell hervorgehoben wird, welche Teile des Satzes geändert wurden bzw. ob der Satz gänzlich ersetzt wurde. Eine weitere Ansicht ist geplant, die für einen durch den Nutzer zu definierenden Teilbereich des Buches eine typographische Übersicht bietet über die in diesem Bereich vorgenommenen Önderungen. Auf diese Weise kann schnell eingeschätzt werden, ob die Önderungen in einem Abschnitt einen thematischen Fokus haben (In Kästners Werk sind es häufig Formulierungen mit erotischem Bezug). Außerdem soll im Verlauf des Projektes eine weitere Variante entstehen, die sich nicht nur an Literaturwissenschaftler in Form einer analytischen Datenvisualisierung richtet, sondern auch an gewöhnliche Leser, bei denen zunächst der ununterbrochene Konsum des Textes im Vordergrund steht, für die aber zusätzliche Informationen, wie die Veränderung eines Textabschnittes über mehrere Fassungen hinweg auch von Interesse sein können. Die Textdaten für die Visualisierung basieren auf den beiden E-Book-Fassungen der  Texte ""Der Gang vor die Hunde"" (2013) und Fabian (2010). Unterschiede zwischen den  beiden Fassungen wurden mithilfe des Levenshtein-Algorithmus in Java vorberechnet  und dann als Datensatz in einer Webvisualisierung verwendet, die mit HTML, CSS,  Javascript und der Visualisierungsbibliothek D3 erstellt wurde."
2016,DHd2016,vortraege-021.xml,Digitale Editionen als Web-Services,"Immanuel Normann (pagina GmbH, Deutschland)","Semantic annotation, linked open data, REST API, digitale Editionen","Programmierung, Annotieren, Kommunikation, Kollaboration, Kommentierung, Webentwicklung, Software, Text, Werkzeuge, virtuelle Forschungsumgebungen","Verstehen wir unter einer digitalen Edition eine ""erschließende Wiedergabe   historischer Dokumente"", welche dem digitalen Paradigma folgt, indem sie die   gegenwärtigen technischen Möglichkeiten berücksichtigt (cf. Sahle 2013: 138,   148), dann stellt sich die Frage, welche   technischen Möglichkeiten zu welchem Zweck eingesetzt werden sollen. In diesem   Beitrag wird die Überzeugung vertreten, dass digitale Editionen als zentraler   Bestandteil von Forschungsumgebungen der Textwissenschaft von weit größerem Nutzen   sein können, wenn sie über standardisierte semantische Web-Schnittstellen verfügen.   Digitale Editionen wären dann primär als Web-Services zu verstehen, die über ihre   Web-Schnittstellen mit anderen Web-Services oder mit Web-Anwendungen kommunizieren.   Es wäre erst die Web-Anwendung (welche im Browser ausgeführt wird), mit der der   menschliche Nutzer interagiert, wogegen alle übrige Kommunikation von Maschine zu   Maschine liefe. Herkömmliche digitale Editionen sind primär auf eine Nutzung durch   den Menschen allein ausgerichtet. Die im Folgenden zu begründende These ist, dass   Werkzeuge der Forschungsumgebungen mit diesen herkömmlichen digitalen Editionen   deshalb nur unbefriedigend ineinandergreifen, weil sie programmatisch abgeschlossen   sind. Dieser Zustand ist insofern unbefriedigend, als dadurch Textforschung weit   weniger vernetzt und kollaborativ vonstatten geht als dies möglich wäre. Eine Verbesserung dieses Zustands kann natürlich nicht allein von technischen  Neuerungen digitaler Editionen erhofft werden. Es sind ebenso technische Neuerungen  bei allen Komponenten bestehender Forschungsumgebungen nötig (und bei Initiativen  wie TextGrid auch im Gange). Dabei besteht eine wechselseitige Abhängigkeit des  Entwicklungsfortschritts: Nur wenn die eine Komponente das eine neue Feature  anbietet, besteht bei der anderen Komponente die Chance eines Entwicklungssprungs.  Mit Blick auf diese Im Folgenden wird daher das Umfeld digitaler Editionen innerhalb einer textwissenschaftlichen Forschungsumgebung in den Blick kommen und zwar in einer Weise, die auch noch nicht existierende Systeme mitdenkt. Dies ist möglich, wenn man eine solche Umgebung zu diesem Zweck nicht als eine Ansammlung bestehender Tools auffasst, sondern die textwissenschaftlichen Tätigkeiten identifiziert, für die man sich ohne Rücksicht auf bestehende Fertiglösungen technische Unterstützung überhaupt vorstellen kann. Die aus informationstechnischer Sicht relevanten Tätigkeiten lassen sich in diesem Kontext sinnvoll unterteilen in: das Fragen wir uns nun, zu welchen dieser drei Tätigkeitsfeldern (Lesen, Schreiben,  Verwalten) eine digitale Edition eine unmittelbare und eine mittelbare Unterstützung  liefern kann. Traditionell dienen digitale Editionen (wie ihre gedruckten Vorfahren)  in erster Linie dazu gelesen zu werden. Zwar sind die in ihr enthaltenen Texte und  ihre Metadaten natürlich auch Ergebnis einer Textverwaltung. Jedoch bieten sie dem  Nutzer nur in seltenen Fällen und da auch nur rudimentär die Möglichkeit selbst Text  zu verwalten (cf. z. B. Arbeitsmappen bei All diesen digitalen Editionen ist jedoch gemeinsam, dass, sofern sie eine Textverwaltung unterstützen, diese dann nur für die im System vorhandenen (oder darin erzeugten) Texte ermöglichen. Im Allgemeinen ist der Textwissenschaftler aber nicht mit einem einzelnen Textkorpus befasst, sondern mit mehreren. Eine Textverwaltung kann dann nur ihren Nutzen entfalten, wenn sie als eigenständiger Service auf mehrere digitale Editionen zugreifen kann. Nehmen wir als einfaches Beispiel die Zusammenstellung der Literatur zu einem Germanistikseminar, in dem Texte verschiedener Autoren behandelt werden. Von einer komfortablen Textverwaltung würde man jetzt nicht die URL der jeweiligen digitalen Editionen erwarten, sondern man möchte am besten die Texte selbst per Mausklick zur Verfügung gestellt bekommen ohne dabei auf die Web-Seiten der jeweiligen digitalen Editionen gehen zu müssen. Schon dieser einfache Fall zeigt den Nutzen, den eine programmatische Schnittstelle von digitalen Editionen haben könnte: Ein eigenständiger Service zur Aggregation von Semesterapparaten ließe sich mit geringem Aufwand implementieren. Tatsächlich bieten manche digitale Editionen (z. B. das Deutsche Textarchiv) ihre  Texte (sogar in verschiedenen Formaten: TEI, HTML, plain text) zum Download an, so  dass man die entsprechenden Links schon als Web-API auffassen könnte. Allerdings  beschränkt sich diese Möglichkeit entweder auf den Download einer einzelnen Seite  oder des gesamten Textdokuments. Für eine brauchbare Textverwaltung wäre es jedoch  wesentlich praktischer, wenn man Texte nicht nach Paginierungsgrenzen sondern  bezüglich semantischer Sinneinheiten beziehen könnte. Es fällt nicht schwer, sich  entsprechende Szenarien vorzustellen: Für eine Anthologie möchte man etwa Balladen  einer bestimmten Epoche zusammenstellen.; für eine Theaterprobe möchte jeder  Schauspieler eine Zusammenstellung derjenigen Szenen, in der seine Rolle vorkommt;  ein Übersetzungsforscher möchte alle deutschen Übersetzungen des Monolog der ersten  Szene im dritten Aufzug von Shakespeares Hamlet. Die Zahl weiterer Szenarien ist  unbegrenzt. Als entscheidende Anforderung an eine digitale Edition wäre  festzuhalten: die Adressierbarkeit und Auffindbarkeit von Texten in allen üblichen  Struktureinheiten (z. B. Kapitel, Absatz, Drama, Akt, Szene, Gedicht, Strophe, Vers,  etc.). Da in den meisten digitalen Editionen die Texte im TEI-XML vorliegen, welche  die Kodierung solcher Struktureinheiten erlauben, dürfte es prinzipiell nicht  schwierig sein, diese auch über eine Web-API adressierbar zu machen. Was die  Auffindbarkeit betrifft, wäre es wünschenswert, die Möglichkeitender in der  Backend-Datenbank verwendeten Anfragesprachen weitgehend in der Web-API abzubilden.  Das ganze Feld der Suchmöglichkeiten ist allerdings so umfangreich, dass es einen  eigenen Beitrag rechtfertigen würde und daher hier nicht weiter vertieft werden  soll. Allein die Adressierbarkeit aller textspezifischen Struktureinheiten (s. o.)  mittels der Web-API von digitalen Editionen wäre eine große Chance zur Entwicklung  nützlicher Textverwaltungsdienste. Allerdings sollten neben den vorgegebenen  Struktureinheiten auch vom Nutzer frei definierte Textauswahlen von einer digitalen  Edition adressierbar sein. Damit soll die verbreitete Praxis, Textausschnitte mit  einem Textmarker zu markieren, im digitalen Medium nicht nur die Funktion erhalten,  etwas farblich hervorzuheben, sondern die so ausgezeichneten Textpassagen sollen  durch eine generierte Adresse permanent referenzierbar gemacht werden. Damit wäre  beispielsweise eine Sammlung von Exzerpten referenzierbar, die ein Benutzer mit  einem virtuellen Textmarker erzeugt hat. Bis hierin wurde die Adressierbarkeit von jeglichen Textausschnittenin den oben angeführten Szenarien ausschließlich für die Erstellung von Textsammlungen verwendet. Das ist aber nur eine einfache Form der Textverwaltung. Denn eine Textsammlung ist zunächst eine in sich unstrukturierte Menge von Texten. Ziel einer Textverwaltung ist es aber meist, in eine Textsammlung eine bestimmte Ordnung zu bringen. Das ist unter anderem der Fall, wenn man die gesammelten Texte nach forschungseigenen Kriterien klassifiziert; z. B. als Linguist nach grammatischen Eigenschaften, als Literaturwissenschaftler nach Motiven, als Übersetzer nach Idiomen, etc. Textklassifikation wäre eine Relation zwischen Texten und Sammelbegriffen. Darüber hinaus wäre es wichtig, in einer Textverwaltung die Beziehung der Texte untereinander explizit machen zu können. So könnte man beispielsweise explizit erfassen, dass eine bestimmte Textpassage eine Anspielung auf einen anderen Text ist; oder dass die eine Textfassung aus jener Skizze hervorgegangen ist, etc. Soweit würde man Textausschnitte aus digitalen Editionen in Beziehung zueinander setzen. Man würde aber in einer Textverwaltung insbesondere auch die Texte der digitalen Editionen in Beziehung zu selbstverfassten Texten setzten wollen. Auch würde man Texte zu nicht textartigen Gegenständen wie Personen, Orte oder Ereignissen in Beziehung setzen wollen; beispielsweise wenn man in historischen Romanen den Bezug zu historisch belegten Sachverhalten herstellen möchte. Eine Textverwaltung, die all die skizzierten Funktionalitäten bereitstellen würde, könnte einen Textwissenschaftler bei der Arbeit am Text bzw. der Organisation der eigenen Texte erheblich unterstützen. Sie würde darüber hinaus das kollaborative Arbeiten erleichtern, indem sie eine auf Austausch von Dokumenten basierte Arbeitsweise durch eine Praxis der direkten Vernetzung von Inhalten im Netz ersetzen würde. Sie könnte aber nur funktionieren, wenn die Texte digitaler Editionen in aller Granularität über Web-APIs adressierbar wären. Abschließend soll erwähnt werden, das eine ganze Reihe von Anstrengung von verschiedenen Seiten schon unternommen wurden, die durch eine geeignete Zusammenführung ein solides Fundament zur Umsetzung dieser Visionen bilden könnten. Allgemeine technische Grundlage wären die Semantic-Web-Technologien. Darauf aufbauend wären folgende theoretische und praktische Arbeiten hervorzuheben: Von Silvio Peroni (2014) zu ""Semantic Publishing"" , Fabio Ciottis und Francesca Tomasis (2014) Entwurf zu ""Formal ontologies, Linked Data and TEI semantics"", das semantic annotation Tool"
2017,DHd2017,vortrag-REGER.xml,Analyzing Features for the Detection of Happy Endings in German Novels,"Fotis Jannidis (Universität Würzburg, Deutschland); Isabella Reger (Universität Würzburg, Deutschland); Albin Zehe (Universität Würzburg, Deutschland); Martin Becker (Universität Würzburg, Deutschland); Lena Hettinger (Universität Würzburg, Deutschland); Andreas Hotho (Universität Würzburg, Deutschland)","Happy End classification, plot representation","Inhaltsanalyse, Strukturanalyse, Literatur"," Der Plot ist ein grundlegendes Strukturelement literarischer Texte. Dementsprechend wären Methoden zur computergestützten Repräsentation von Plot oder bestimmten Plot-Elementen ein großer Gewinn für die quantitative Literaturanalyse. Dieses Paper betrachtet ein solches Plot-Element: das Ende; genauer gesagt untersuchen wir die Frage, ob ein Werk ein Happy End hat oder nicht. Dazu setzen wir Sentimentanalyse ein, wobei wir den Fokus auf die qualitative Betrachtung bestimmter Features und deren Performanz legen, um tiefere Einsicht in die Funktionsweise der automatischen Klassifikation zu erhalten. Außerdem zeigen wir, wie die beschriebene Vorgehensweise auf nachfolgende Forschungsfragen angewendet werden und dabei zu interessanten Ergebnissen hinsichtlich der Erscheinungszeit der Romane führen kann. In einer der ersten Arbeiten beschäftigt sich Mark Finlayson mit folkloristischen Erzählungen und entwickelt einen Algorithmus, der Ereignisse erkennt und daraus übergeordnete Konzepte wie Niedertracht oder Belohnung abstrahiert (Finlayson 2012). Reiter et al. identifizieren Ereignisse sowie deren Teilnehmer und Reihenfolge und nutzen maschinelle Lernverfahren, um strukturelle Öhnlichkeiten über Erzählungen hinweg aufzudecken (Reiter 2013, Reiter et al. 2014). In letzter Zeit richtet sich einige Aufmerksamkeit auf die Sentimentanalyse, insbesondere seit Matthew Jockers emotionale Erregung als Indikator für Plotstrukturen vorgeschlagen hat (Jockers 2014). Er unterteilt Romane in Segmente und bildet daraus emotionale Plot-Kurven (Jockers 2015). Obwohl die Idee, Sentimentanalyse in diesem Zusammenhang einzusetzen, gut aufgenommen wurde, wurde Jockers für seine Verwendung der Fourier-Transformation zur Glättung der resultierenden Plot-Kurven kritisiert (Swafford 2015, Schmidt 2015). Micha Elsner (Elsner 2015) verwendet, neben anderen Features, ebenfalls Sentimentkurven, um Repräsentationen des Plots romantischer Werke zu erstellen. Er verknüpft diese Kurven mit bestimmten Figuren und untersucht auch das gemeinsame Auftreten von Figuren. Die Auswertung seines Ansatzes zeigt, dass er echte Romane mit beachtlichem Erfolg von künstlich umgestellten Versionen unterscheiden kann, was darauf hindeutet, dass seine Methoden tatsächlich bestimmte Aspekte der Plotstruktur abbilden. In vorhergehenden Arbeiten haben wir Sentiment-Features verwendet, um Happy Ends, als ein wichtiges Plot-Element, in deutschsprachigen Romanen zu erkennen, wobei wir einen F1-score von 73% erreichen konnten (Zehe et al. 2016). Unser Datensatz besteht aus 212 deutschsprachigen Romanen, die hauptsächlich aus dem 19. Jahrhundert stammen. Unsere Sentimentanalyse erfordert eine Ressource, die auflistet, welche Gefühle Leser typischerweise mit bestimmten Worten oder Phrasen eines Textes assoziieren. Dieses Paper verwendet das NRC Sentiment Lexikon (Mohammad und Turney 2013), zu dem eine automatisch übersetzte deutsche Version verfügbar ist Tabelle 1: Beispieleinträge aus dem NRC Sentiment Lexikon Ziel dieses Papers ist es, Features, die zur Erkennung von Happy Ends in Romanen genutzt wurden, genauer zu untersuchen, um Einsichten in die Relevanz bestimmter Features zu erhalten. Dazu übernehmen wir die Features und Methoden, wie sie in Zehe et al. (2016) beschrieben sind. Die Parameter der linearen SVM sowie die Einteilung in 75 Segmente sind ebenfalls aus diesem Paper übernommen. Aufgrund unserer Annahme, dass die relevante Information zur Klassifikation von Happy Ends am Ende eines Romans zu finden ist, wurden zunächst die Sentiment-Werte des letzten Segments als einziges Feature-Set ( Um unserer Intuition gerecht zu werden, dass nicht nur das letzte Segment an sich, sondern auch sein Verhältnis zum Rest des Romans für die Klassifikation von Bedeutung ist, wurden sogenannte Sektionen ( Diese Beobachtung führte uns zu der Annahme, dass unser Begriff des ""Endes"" nicht differenziert genug ist, da die Anzahl an Segmenten für jeden Roman und damit auch die Grenzen des finalen Segments relativ willkürlich gewählt wurden. Daher wurde die Aufteilung in final section und main section im Folgenden variiert, sodass die final section mehr als nur das letzte Segment enthalten kann. Abbildung 1: Klassifikationsgenauigkeit für verschiedene Unterteilungen in main und final section. Die gestrichelte Linie gibt die Baseline an, die gepunktete Linie markiert die Aufteilung, bei der der maximale F1-score erreicht wird. Abbildung 1 zeigt, dass die Klassifikationsgenauigkeit steigt, wenn mindestens 75% der Segmente in der main section sind und ein Maximum bei ca. 95% erreicht (bei 75 Segmenten insgesamt bedeutet das 4 Segmente in der final section und 71 Segmente in der main section). Mit dieser Aufteilung verbessert sich der F1-Wert auf 68%, wenn nur das Feature-Set der final section ( Da sich die Ergebnisse durch die Einbeziehung des Verhältnisses zwischen der final section und der main section verbessert haben, war unser nächster Schritt, den Verlauf der Sentimentkurve gegen Ende eines Romans genauer zu modellieren. Beispielsweise könnte sich kurz vor dem Ende eine Katastrophe ereignen, die anschließend im Sinne eines Happy Ends aufgelöst wird. Um diese Intuition abzubilden, führten wir eine weitere Sektion ein, die sogenannte late-main section, die die letzten Segmente der main section umfasst. Die Differenzen zwischen den Feature-Sets für die late-main section und die final section wurden als zusätzliche Merkmale verwendet ( Tabelle 2: F1-score für die verschiedenen Feature-Sets Die beschriebenen Ergebnisse sind in Tabelle 2 zusammengefasst. Hier wird deutlich, dass die Aufnahme der einzelnen Feature-Sets jeweils zu einer kleinen Verbesserung geführt hat, bis hin zu einem F1-score von 73%. Obwohl die Aufteilung mit 4 Segmenten in der final section die besten Ergebnisse erzielte, konnten wir auch beobachten, dass einige Romane mit mehreren verschiedenen Unterteilungen korrekt klassifiziert werden konnten. Andere Romane hingegen konnten in keinem Setting korrekt vorhergesagt werden. Als Beispiel sei hier Jules Vernes Roman  Abbildung 2: F1-score für verschiedene Unterteilungen in main und final section. Die farbigen Kurven stehen für Romane aus verschiedenen Zeitperioden. Die gestrichelte Linie zeigt die Zufallsbaseline für die Zeitperiode ab 1871. Die Baselines für die anderen Zeitperioden liegen etwas darunter und werden daher nicht dargestellt. Die gepunkteten Linien zeigen jeweils den maximalen F1-Wert für die entsprechende Zeitperiode. Abbildung 2 zeigt, dass die Klassifikation erneut dann am besten funktioniert, wenn ca. 95-98% der Segmente in der Hauptsektion sind, unabhängig von der Zeitperiode. Die beste Aufteilung in Sektionen korreliert also nicht mit dem Erscheinungsjahr eines Romans. Es fällt jedoch auf, dass die Romane nach 1848 deutlich niedrigere Werte liefern als die vor diesem Jahr veröffentlichten Texte, meistens sogar unterhalb der Baseline. Das deutet auf eine Korrelation zwischen dem Erscheinungsdatum und der Klassifikationsgenauigkeit hin: Vor dem Realismus erschienene Romane sind hinsichtlich des Happy Ends leichter zu klassifizieren als realistische Romane. Eine mögliche Erklärung für diese Beobachtung könnte die stärker schematische Struktur der vor-realistischen Romane sein. Wir sind uns bewusst, dass die Anzahl der Romane für die einzelnen Zeitperioden relativ klein ist, sodass diese Beobachtungen zunächst als exploratorische Einblicke gesehen werden müssen. Nichtsdestotrotz zeigen diese vorläufigen Ergebnisse, dass die automatische Erkennung von Happy Ends, sogar mit nur einem recht einfachen Feature-Set, Zusammenhänge zu anderen Eigenschaften von Romanen aufdecken kann, die für die Literaturwissenschaft von großem Interesse sind. Die automatische Erkennung von Happy Ends als wesentlichem Plot-Element von Romanen ist ein nützlicher Schritt in Richtung einer umfassenden computergestützten Repräsentation des Plots literarischer Texte. Unsere Experimente zeigen, dass verschiedene Features auf Basis von Sentimentanalyse eine Erkennung von Happy Ends in Romanen mit unterschiedlicher, aber insgesamt solider Genauigkeit ermöglichen. Obwohl unser Ansatz relativ einfach gehalten ist, kann er zu substantiellen Erkenntnissen für die Literaturwissenschaft führen. In zukünftigen Arbeiten soll die Genauigkeit unserer Methode verbessert werden, indem die hohe Variabilität des Endes in Romanen differenzierter betrachtet wird. Außerdem könnte der Ansatz eingesetzt werden, um bestimmte Eigenschaften weiterer Romankorpora tiefergehend zu untersuchen."
2017,DHd2017,poster-PIELS.xml,"Einfaches Topic Modeling in Python - Eine Programmbibliothek für Preprocessing, Modellierung und Analyse","Fotis Jannidis (Universität Würzburg, Deutschland); Steffen Pielström (Universität Würzburg, Deutschland); Christof Schöch (Universität Würzburg, Deutschland); Thorsten Vitt (Universität Würzburg, Deutschland)","topic modeling, python","Programmierung, Inhaltsanalyse, Modellierung, Visualisierung, Literatur"," Ursprünglich entwickelt, um in größeren Sammlungen kürzerer Fachartikel schnell jene zu identifizieren, die für bestimmte Themen relevant sein könnten, kann diese Methode darüber hinaus für eine Reihe von Problem im Bereich der digitalen Literaturwissenschaft interessante neue Lösungsansätze bieten. Dazu gehört die automatische Identifikation von Romanen, die ähnliche Themen behandeln (wenngleich eine direkte Gleichsetzung probabilistischer ""Topics"" mit literarischen ""Themen"" durchaus problematisch ist), ebenso wie die Zuordnung zu bestimmten Genres anhand inhaltlicher Aspekte, oder die quantifizierende Betrachtung der zu- und abnehmenden Bedeutung einzelner Themenfelder über den Verlauf eines einzelnen Romans (vgl. Blevins 2012, Jockers 2011, Mit den Programmen ""Mallet"" (vgl. McCallum 2002) und ""Gensim"" (vgl. Rehurek 2010) stehen zur Zeit zwei State-of-the-Art Implementierungen von Topic Modeling-Algorithmen zur Verfügung. Um die Methode produktiv einzusetzen, sind aber neben der Erzeugung des Modells weitere Arbeitsschritte notwendig (Abb. 1). Im ""Preprocessing"" gilt es zunächst, die Textsammlungen in eine Form zu bringen, in der sie vom Modellierungsprogramm verarbeitet werden können. Darüber hinaus werden die Texte normalerweise durch das Herausfiltern häufiger Funktionswörter auf die potentiell inhaltsrelevanten Wörter reduziert, was in der Regel den vorhergehenden Einsatz von NLP-Tools (Natural Language Processing) erfordert. Sind die ""Topics"" dann erst einmal errechnet worden, kann sich eine Visualisierung der Ergebnisse anschließen, oder ihre statistische Evaluierung anhand interner oder externer Kriterien, ein Aspekt dem beim Einsatz von Topic Modeling-Verfahren im DH-Kontext bisher eher zu wenig Beachtung geschenkt wurde. Ziel unseres Projektes ist es, den Einstieg in aktuelle Topic Modeling-Verfahren für digital arbeitende Literaturwissenschaftler wesentlich zu vereinfachen, indem wir möglichst viele der notwendigen Arbeitsschritte in einer einheitlichen, umfangreichen und gut dokumentierten Programmbibliothek für die unter digital-quantitativ arbeitenden Geisteswissenschaftlern stark verbreitete Programmiersprache Python anbieten. Hierbei sollen Nutzerinnen und Nutzer bei allen Arbeitsschritten auf vorhandene, in einem ausführlichen Tutorial dokumentierte Funktionen zurückgreifen und so weit wie möglich wie mit einem Kommandozeilentool arbeiten können, ohne selbst programmieren zu müssen. Die Anforderungen an die Programmierkenntnisse der Forschenden, die diese Verfahren einsetzen möchten, werden damit minimiert und die Methode wird so einem größeren Nutzerkreis zugänglich gemacht. Für das NLP-Preprocessing steht mit dem DARIAH-DKPro-Wrapper (DDW) ein komfortables Einheitswerkzeug zur Verfügung, das ein großes Spektrum an NLP-Aufgaben abdeckt und linguistische Annotationen in einem Python-Pandas-kompatiblen Ausgabeformat erzeugt. Ein Ziel unserer Bibliothek ist die direkte Anbindung des DDW-Outputs an existierende Implementierungen verschiedener etablierter Varianten von Topic Modeling-Algorithmen. Für die Untersuchung der resultierenden Modelle möchten wir verschiedene Evaluierungsverfahren anbieten, sowohl interne Verfahren wie z.B. das Perplexity-Maß, als auch externe Vefahren, wie z.B. die Weglänge zwischen zwei Begriffen in einem Wörterbuch. Hieran schließen sich verschiedene Optionen zur Visualisierung der Ergebnisse an. Im Fokus der Entwicklung steht die Gestaltung schlüssig aufeinander aufbauender Programmbefehle, die einer einheitlichen Syntax folgen und deren Funktion sich schnell erschließen lässt. Sie sollen sich ohne längere Einarbeitung nutzen und zu einer Pipeline zusammenfügen lassen, die die spezifischen Arbeitsschritte eines bestimmten Topic Modeling-Projektes umsetzt. Hierbei können Nutzerinnen und Nutzer auf detaillierte Anleitungen aus einem umfangreichen Tutorial zurückgreifen, in dem alle Funktionen, alle Outputs, und potentielle Kombinationen detailliert dokumentiert und anhand von Beispielen erläutert werden.  "
2017,DHd2017,poster-HOHEN.xml,Raum und Zeit in Comics: Die Wirkung von Zwischenräumen auf Aufmerksamkeit und empfundene Zeit beim Lesen graphischer Literatur,"Sven Hohenstein (Universität Potsdam, Deutschland); Jochen Laubrock (Universität Potsdam, Deutschland)","Graphische Literatur, Zeitwahrnehmung, Blickbewegungen, Experimentalpsychologie","Visualisierung, Bilder, Literatur, Methoden, Text","Aufgrund der Kombination von Text und Bild stellen graphische Literatur und Comics komplexe Medien dar. Diese Hybridität stellt an die Aufmerksamkeit beim Lesen andere Anforderungen als bei rein textbasierten Romanen, da Informationen unterschiedlichen Typs erfasst und verarbeitet werden müssen. Wegen ihrer Konfiguration als eine Folge von Panels werden Comics auch als sequenzielle Kunst bezeichnet. Nach McCloud (1993) spielt der Raum zwischen den Panels, der als ""gutter"" bezeichnet wird, eine Rolle für die Verbindung der einzelnen Panels. Obwohl dieser Raum selbst leer ist, so vergeht doch nach McCloud Zeit zwischen zwei Panels. Diesem Postulat hinsichtlich der Empfindung, die durch den ""gutter"" ausgelöst wird, haben wir uns im Rahmen einer empirischen Studie gewidmet. Die Wirkung zusätzlichen, leeren Raums zwischen Panels für die subjektive Wahrnehmung von Zeit beim Lesen graphischer Literatur haben wir mit kognitionspychologischen Experimenten untersucht. Dieses Vorgehen erlaubt es über die reine Beschreibung des Materials hinaus den subjektiven Eindruck der Leserin bzw. des Lesers zu erfassen. Für diese Experimente stellten wir eine Sammlung von einzelnen Panels aus verschiedenen Comic-Reihen zusammen, beispielsweise ""Astérix"" und ""Donald Duck"". Die Auswahl der Panels erfolgte nach dem Kriterium, dass sie sich horizontal teilen lassen. Bei dieser Teilung wurde ein Panel per Bildbearbeitungssoftware in mehrere kleinere Unterpanels geteilt. Zusammenhängende Textabschnitte blieben dabei ungeteilt. Im ersten Experiment wurde das Material in zwei Bedingungen dargeboten. In der Kontrollbedingung wurden die Panels jeweils ohne Teilung in ihrer ursprünglichen Form auf einem Bildschirm präsentiert. In der zweiten Bedingung wurden die Subpanels hintereinander auf dem Bildschirm gezeigt. Jeder Durchgang endete damit, dass die Probanden gefragt wurde, wieviel Zeit während der Geschichte, die in dem Panel erzählt wird, vergangen ist. Die Antworten der Probanden spiegeln somit deren subjektive Einschätzung der Dauer wider. Obwohl in beiden Bedingungen letztlich dieselben Panels gezeigt wurden, gab es bedeutsame Unterschiede in den Antworten. Die Teilung der Panels führte zu längeren subjektiven Dauern als die Kontrollbedingung. Dieses Ergebnis verdeutlicht den Einfluss der Konfiguration visueller Information auf die Wahrnehmung der Leserin bzw. des Lesers. Um eine detailliertere Analyse der Aufmerksamkeit der Probanden vornehmen zu können, haben wir im zweiten Experiment zusätzlich Blickbewegungen erhoben. Für die Kontrolle der Auswirkungen der Panel-Teilung auf die wahrgenommene Dauer haben wir zudem das Material in einer Weise präsentiert, die ähnlicher zu tatsächlichen Comics ist. Die Subpanels wurden nebeneinander mit zusätzlichem, leerem Zwischenraum angeordnet, so dass das Aussehen einer kurzen Comic-Geschichte mit mehreren Panels gleicht. In der Kontrollbedingung wurden die Panels erneut ungeteilt dargeboten. Erneut wurden die Dauern länger eingeschätzt, wenn die Panels geteilt auf dem Bildschirm erschienen. Die Auswertung der Blickbewegungen ergab ein differenziertes Bild der Aufmerksamkeitsverteilung beim Betrachten der Panels. Die Blickbewegungsmuster unterschieden sich in Hinblick auf die experimentelle Bedingung. Waren die Panels geteilt, so machten die Versuchspersonen mehr Fixationen. Die höhere Anzahl an Fixationen ist somit eine mögliche Ursache für die subjektiv längere verstrichene Zeit. Außerdem zeigte sich eine leichte relative Tendenz zur Fixation nahe dem Zentrum eines jeden Subpanels, die bei geteilten Panels stärker ausgeprägt war. Diese und andere Befunde sprechen dafür, dass die Teilung von Panels die Aufmerksamkeit beim Lesen und Betrachten sowie die Wirkung graphischer Literatur beeinflussen kann."
2017,DHd2017,panel-REITE.xml,Aktuelle Herausforderungen der Digitalen Dramenanalyse,"Marcus Willand (Universität Stuttgart); Peer Trilcke (Universität Potsdam); Christof Schöch (Universität Würzburg); Nanette Rißler-Pipka (Katholische Universität Eichstätt-Ingolstadt); Nils Reiter (Universität Stuttgart); Frank Fischer (Higher School of Economics, Moskau)","Dramenanalyse, Methodenvergleich, Labor, Drama","Inhaltsanalyse, Strukturanalyse, Beziehungsanalyse, Modellierung, Annotieren, Bereinigung, Netzwerkanalyse, Literatur, Metadaten, Methoden, Forschungsprozess, Software, Standards, Text","Das hier vorgeschlagene Panel greift mit der Digitalen Dramenanalyse einen sich derzeit dynamisch entwickelnden Bereich der digitalen Literaturwissenschaften auf. Es setzt sich erstens zum Ziel, aktuelle Herausforderungen der Digitalen Dramenanalyse auf verschiedenen Ebenen vorzustellen, wobei insbesondere die Ebenen der dramatischen Gattung, der Netzwerkstrukturen und der dramatischen Figuren im Zentrum stehen werden. Zweitens möchte das Panel mit dem Publikum mögliche Lösungsansätze diskutieren, unter anderem durch Bezug auf vielfältige, vorhandene Erfahrungen mit der Analyse narrativer Texte. In der Summe wird das Panel einerseits eine Zwischenbilanz zum Stand der Forschung anbieten, andererseits auch im Sinne einer Konsolidierung des Forschungsfelds eine Agenda für die weitere Entwicklung formulieren, bei der es nicht zuletzt darum geht, Szenarien einer integrativen, mithin diverse methodische Ansätze synergetisch zusammenführenden Forschung, zu diskutieren. Dazu wird das Panel eine Art Laborsituation fingieren, in der die Erkenntnisziele, Möglichkeiten und Grenzen unterschiedlicher methodischer Zugänge zu dem titelgebenden Forschungsbereich der digitalen Dramenanalyse zu Tage treten sollen: In den In drei Kurzvorstellungen sollen die folgenden Methoden von jeweils einer Forschergruppe des Panels vorgestellt werden, wobei zur besseren Vergleichbarkeit der drei methodisch unterschiedlich aufgestellten Arbeitsgruppen zeitlich und gattungsbezogen vergleichbare Textsammlungen analysiert werden. Zwar werden diese Verfahren jeweils anhand eines individuellen Teilkorpus vorgestellt, es ist jedoch zu berücksichtigen, dass sie alle auf der statistischen Analyse größerer Textmengen basieren. Der Einsatz von Zweitens zeigt sich, dass sich einzelne dramatische Untergattungen wie Tragödie, Komödie oder Tragikomödie zwar in Bezug auf die jeweils dominanten Einzel Die in den quantitativen Sozialwissenschaften entwickelten Verfahren der Netzwerkforschung zielen auf eine formale Analyse sozialer Strukturen (Wasserman / Faust 1994). Angewandt auf literarische Texte ermöglichen sie Strukturbeschreibungen, die aus einer signifikant anderen Perspektive erfolgen als traditionelle literaturwissenschaftliche Verfahren der semantikbasierten Strukturanalyse (z.B. Titzmann 1977), insofern sie nicht die semantische Organisation literarischer Texte, sondern die ästhetische Modellierung sozialer Formationen im Medium der Literatur analysieren (Trilcke 2013). Ob ihres stark formalisierten Charakters operieren netzwerkanalytische Konzeptualisierungen dabei zunächst mit epistemischen Objekten, die sich erheblich von den Objekten der ""klassischen"" Literaturwissenschaft unterscheiden. Gerade deshalb aber bilden solche Konzeptualisierungen ein ebenso attraktives wie kontroverses Experimentierfeld für computerbasierte Zugänge zum Gegenstandsbereich ""Literatur"", die nicht nur neue Antworten auf alte Fragen finden, sondern dezidiert Computerlinguistische Methoden wie Named Entity Recognition und Koreferenzresolution (cf. Poesio et al. 2016) erlauben die Erkennung von Figurenreferenzen in der Rede dramatischer Figuren. Die erkannten Referenzen wiederum können genutzt werden, um den Stellenwert einer Figur innerhalb des Gesamttextes zu identifizieren. Neben der direkten Präsenz von Figuren (im Sinne von: Figur spricht; siehe auch das Problem der sog. Konfiguration, hierzu Ilsemann 1995, 2008) lässt sich damit auch die indirekte Präsenz (über eine Figur wird gesprochen) messen. Im Falle von Unser Beitrag zum Panel diskutiert zum einen die Herausforderungen an die maschinelle Sprachverarbeitung, wenn sie auf Dramentexte angewendet wird (Blessing et al. 2016). Zum anderen wollen wir untersuchen, inwiefern Autorinnen und Autoren sprachliche Eigenheiten der Figuren nutzen, um diese zu charakterisieren und z.B. als bestimmten Figurentypus (zärtlicher Vater, Hanswurst usw.; cf. Sørensen 1984, Aust 1989, Kord 2009) zu kennzeichnen. Die unterschiedlichen methodischen Zugänge zu dramatischen Texten erlauben zwar eine direkte Gegenüberstellung und Diskussion der drei Forschungsansätze, ihrer Prämissen, aber auch der Relevanz ihrer Ergebnisse für literaturtheoretische oder -historische Fragestellungen. Die vorgestellten Verfahren sollen letztlich aber nicht als konkurrierend oder unverbunden gedacht werden, sondern als Beiträge zu einem gemeinsamen Ziel: dem differenzierteren literaturwissenschaftlichen Verständnis dramatischer Texte. Vor dem Hintergrund der das Panel leitenden Idee einer Bilanzierung bisheriger und Konsolidierung aktueller Forschung auf dem Gebiet der ‚óè Jede der drei Methoden verfolgt spezifische Fragen und birgt spezifische Herausforderungen. In welchem Maße gibt es gemeinsame Forschungsziele, zu denen jede der Methoden einen Beitrag leisten kann? Können die verschiedenen Methoden beispielsweise einen Beitrag zu einer empirisch gesicherten Gattungsdifferenzierung oder für die literaturgeschichtliche Periodisierung leisten? ‚óè Wie können Ergebnisse, die mit unterschiedlichen methodischem Vorgehen gewonnen wurden, in Bezug zueinander gesetzt werden? ‚óè Welche Ressourcen (insbesondere Textsammlungen) liegen vor und wie kann die Verfügbarkeit geeigneter Ressourcen für die Digitale Dramenanalyse zukünftig verbessert werden? Wie können die teils unterschiedliche Anforderungen der Methoden an die Formate von Daten und Metadaten aufgefangen werden? ‚óè Welche konzeptuellen und datenbezogenen Standards für dokumentbezogene Metadaten und strukturelle oder semantische, lokale Annotationen liegen vor, wie kann die Standardisierung (bspw. durch Annotationsrichtlinien) weiter gefördert werden? ‚óè Welche Tools sind für die digitale Dramenanalyse derzeit verfügbar, wie könnte die Tool-Entwicklung zielgerichtet gefördert werden? Welche generischen Tools könnten produktiv eingesetzt werden, wie könnte der Einsatzbereich vorhandener Tools erweitert (Adaptierbarkeit, Übertragbarkeit) und so eine breitere Nutzerbasis geschaffen werden? Indem das Panel die Vielfalt digitaler Dramenanalysen vorführt und die explorative Kraft methodischer Innovation durch die Digital Humanities für die Literaturwissenschaften betont, möchten wir die fingierte ""Laborsituation"" im Sinne der theoretischen und wissenschaftspolitischen Implikationen einer auf Überprüfbarkeit und Wiederholbarkeit angelegten Wissenschaft verstanden wissen."
2017,DHd2017,poster-ZIRKE.xml,TEASys (Tübingen Explanatory Annotations System): Die erklärende Annotation literarischer Texte in den Digital Humanities,"Angelika Zirker (Eberhard Karls Universität Tübingen, Deutschland); Matthias Bauer (Eberhard Karls Universität Tübingen, Deutschland)","Annotation, Literaturwissenschaft, Hermeneutik, Lehrinstrument","Entdeckung, Gestaltung, Inhaltsanalyse, Strukturanalyse, Annotieren, Kontextsetzung, Theoretisierung, Community-Bildung, Stilistische Analyse, Kollaboration, Lehre, Text","Das Poster präsentiert das Lehr- und Forschungsprojekt TEASys (Tübingen Explanatory Annotations System) zur erklärenden Annotation literarischer Text in den Digital Humanities. Die erklärende Annotation wird dabei als Anreicherung bislang vor allem literarischer Texte um Informationen verstanden, die zum Textverständnis beitragen bzw. es überhaupt ermöglichen, d.h. sie dienen etwa der Überwindung von historischer Distanz (vgl. Hanna 1991). Eine Anwendung des Systems auf andere (nicht-literarische) Texte wird derzeit vorbereitet. TEASys arbeitet mit verschiedenen Kategorien der erklärenden Annotation sowie ihrer Präsentation auf mehreren Ebenen, die sich etwa bezüglich ihrer Komplexität unterscheiden und aufeinander aufbauen (vgl. Bauer & Zirker 2015). Die Kategorien der erklärenden Annotation sind Sprache, Form, Intratextualität, Intertextualität, Kontext und Interpretation. Die Interpretation ergibt sich dabei aus den Informationen, die aus den anderen Kategorien zum besseren Verständnis an den Text herangetragen werden. Weitere Kategorien, die auf einer Meta-Ebene angesiedelt sind, beinhalten philologische Informationen (z.B. zu Varianten) sowie Fragen oder Anmerkungen (z.B. zu Items, zu denen bislang keine Informationen gefunden werden konnten sowie zur bislang bereits stattgefundenen Recherche zu einzelnen Items). Letztere Kategorie ist vor allem auch im Hinblick auf Fragen der Nachhaltigkeit essentiell. Die Ebenen der Annotation bauen aufeinander auf, d.h. die erste von insgesamt drei Ebenen bietet Informationen an, die das Textverstehen grundsätzlich ermöglichen, und die weiteren Ebenen nennen weitere, meist komplexere und ausführliche Informationen. TEASys geht auf ein Peerlearning-Projekt zurück, das in Tübingen seit 2011 besteht und von Studierenden der englischen Literatur und weiteren geisteswissenschaftlichen Fächern getragen und von den Leitern des Forschungsprojekts (Prof. Dr. Matthias Bauer & PD Dr. Angelika Zirker) wissenschaftlich unterstützt wird. Es gibt derzeit vier Peerlearning-Gruppen, die sich mit Texten verschiedener Gattungen und Epochen beschäftigen und diese kollaborativ annotieren (zur Kollaboration in den DH s. z.B. McCarty 2012; Meister 2012; Stroud 2006). Das Forschungsprojekt widmet sich vor allem der Theoriebildung zur erklärenden Annotationen und der darauf aufbauenden Entwicklung eines best-practice-Modells, das wiederum auf die Theorie rückwirken soll (s. dazu Bauer & Zirker 2015). Die DH-Komponente liegt vor allem in der entsprechenden Aufbereitung und Visualisierung der erklärenden Annotationen für das digitale Medium sowie der darin möglichen Dynamik (s. Eggert 2009): Annotationen sind, entgegen ihrer Darstellung im Buch, ständig revidier- und erweiterbar und somit einer möglichst großen Rezipientengruppe offen, die umgekehrt für eine beständige Qualitätskontrolle sorgt. Ferner ermöglicht die digitale Repräsentation das Filtern von Informationen: je nach Bedarf können z.B. lediglich Annotationen zur Intertextualität angezeigt werden. Das Poster stellt sowohl den Aufbau von TEASys als best-practice-Modell vor wie auch seine theoretischen Grundlagen und Beispielannotationen aus dem Peerlearning-Projekt, die von Studierenden erstellt wurden. Es macht deutlich, wie grundlegende hermeneutische Fragestellungen in das digitale Medium übernommen und dort abgebildet werden können (vgl. Drucker 2012) 'und wie umgekehrt wiederum die digitale Präsentation aufgrund der theoretischen Überlegungen verbessert werden kann."
2017,DHd2017,vortrag-HERRM.xml,"Das ""Was-bisher-geschah"" von KOLIMO. Ein Update zum Korpus der literarischen Moderne","Berenike Herrmann (Universität Göttingen, Deutschland); Gerhard Lauer (Universität Göttingen, Deutschland)","Korpus, Annotation, Moderne, POS, Stil","Teilen, Sammlung, Strukturanalyse, Modellierung, Annotieren, Kontextsetzung, Theoretisierung, Bereinigung, Bearbeitung, Archivierung, Veröffentlichung, Stilistische Analyse, Projektmanagement, Webentwicklung, Konservierung, Artefakte, Daten, Infrastruktur, Sprache, Link, Literatur, Metadaten, benannte Entitäten (named entities), Text","Der vorgeschlagene Beitrag dokumentiert den Fortschritt beim Aufbau unseres digitalen Korpus der literarischen Moderne (KOLIMO), das im Herbst 2016 in der Beta-Version veröffentlicht werden soll (abrufbar unter https://kolimo.uni-goettingen.de/). Im Fokus des Beitrags stehen das Verfahren zur Aufbereitung der Texte (insb. Format und Metadaten in TEI) und das linguistische Tagging (POS). Als Teil des laufenden Projektes Q-LIMO (Quantitative Analyse der literarischen Moderne) ist KOLIMO ein repräsentatives und computerlinguistisch solide aufbereitetes Korpus von narrativen fiktionalen Erzähltexten der literarischen Epoche der Moderne. Um durch stratifiziertes Sampling Repräsentativität (verstanden als ""extent to which a sample includes the full range of variability in a population""; vgl. Biber 1994) zu ermöglichen, umfasst das Korpus ein möglichst breites Spektrum der literarischen Moderne, verteilt über kanonische und nichtkanonische Texte. So wurden in das Korpus bislang ca. 596.000.000 Wörter aus frei zugänglichen Repositorien importiert (s. Abbildung 1). Abbildung 1 Gesamtanzahl Wörter aus den drei Hauptressourcen (Zwischenstand August 2016) Die Datenbank umfasst so neben Texten aus TextGrid und Gutenberg-DE (s. Abbildung 2) und dem DTA auch eine wachsende Zahl von Retrodigitalisaten. Das Sampling ist nicht zuletzt dadurch beeinflusst, dass KOLIMO auch das Kafka/Referenzkorpus (KAREK) beinhaltet, welches zum Ziel hat, Kafkas Texte und Texte, die Kafkas Schreibprozess beeinflusst haben könnten, möglichst umfangreich abzubilden (vgl. Herrmann / Lauer 2016a,b). Abbildung 2 Screenshot KOLIMO-WebApp: Anzahl Wörter, Autoren und Einträge aus TextGrid & Gutenberg-DE (ohne DTA und andere Quellen, Stand August 2016) Um philologischen Ansprüchen an den editorischen Status literarischer Texte und die Abbildung von Epochen sowie Gattungskonzepten zu genügen, war eine hohe Genauigkeit und Konsistenz bei der informatischen Vorverarbeitung Textmarkup (XML-TEI) inklusive der Metadaten (Autor, Entstehungszeitpunkt und Gattung) besonders wichtig. Gerade die Auszeichnung der genannten Metadaten stellt eine Schnittstelle zwischen den informatischen und philologischen Dimensionen unseres Projektes dar: so sind Metadaten (a) die unabhängigen Variablen unserer stilistischen Analyse und (b) variieren in den von uns importierten Korpus-Ressourcen stark in qualitativer und quantitativer Hinsicht (Fehler, missing entries, unterschiedliche Ontologien). Der vorgeschlagene Beitrag wird so erstens einen kurzen Einblick in unsere Vorgehensweise geben, wobei Kriterien der Nachhaltigkeit berücksichtigt werden: Zweitens wird der Beitrag unser Vorgehen bezüglich der linguistischen Anreicherung zusammenfassen: Unter der Annahme, dass Stil quantitativ beschreibbar ist (vgl. Herrmann / van Dalen-Oskam / Schöch 2015), und dass Wortarten verlässliche Indikatoren für Register und Genrevariation sind (vgl. z.B. Biber / Conrad 2009), haben wir uns für die linguistische Annotation auf POS (STTS Tagset; vgl. Schiller / Teufel / Thielen 1995) entschieden. POS sind im Vergleich mit anderen Variationsmarkern durch eine relativ akkurate automatische Annotation besonders praktikabel. Das Webinterface liefert variablen Zugriff auf die annotierten Daten, u.a. eine Volltextansicht (siehe Abbildung 3); geplant sind zur Veröffentlichung die Exportierbarkeit in .csv-Files und TCF-Format. Abbildung 3 Screenshot KOLIMO WebApp Textview POS-Tagging Zwar liefern bereits trainierte Modelle von einigen Taggern (z.B. TreeTagger) eine gute Genauigkeit für das gegenwärtige Standarddeutsch, angewendet auf ältere Sprachstufen oder vom Standarddeutschen abweichende Register wie ""Literatur"" sinkt die Genauigkeit jedoch. Ein bereits auf POS annotiertes Korpus ist das Deutsche Textarchiv (DTA, Berlin-Brandenburgische Akademie der Wissenschaften 2016), ein Referenzkorpus für das Deutsche, das sowohl historische Sprachstufen als auch das Register ""Literatur"" enthält. Die POS-Annotation baut hier auf fehlertoleranten linguistischen Analyse historischer Texte auf und verwendet ein Tool zur Morphologisierung (Jurish 2012), ist allerdings hinsichtlich ihrer Qualität noch nicht umfassend evaluiert worden. Ausgehend von diesem Datensatz haben wir zwei Strategien verfolgt: (1) Ein epochensensitives POS-Tagging, das verschiedene Tagger auf dem Datensatz des DTA, aber auf unterschiedlichen literarischen Epochen trainiert (vgl. Paluch et al. in Vorbereitung); (2) eine Überprüfung der Qualität der DTA-POS-Tags durch quantitative und qualitative Verfahren. In Strategie (1) machen wir uns zunutze, dass Annotationsgenauigkeit erhöht werden kann, wenn Tagger auf verschiedene Register/Sprachstände trainiert und diese trainierten Modelle dann auf noch nicht trainierte Texte des gleichen Registers angewendet werden (vgl. Giesbrecht / Evert). Für KOLIMO haben wir u.a. den TreeTagger (vgl. Schmid 1994), Perceptron (vgl. Rosenblatt 1958) und MarMoT (vgl. Müller / Schmid / Schütze 2013) verwendet. Durch die Wahl unterschiedlicher Tagger soll gewährleistet werden, dass die Genauigkeit der POS-Annotation maximiert werden kann, indem nur derjenige Tagger mit den besten Ergebnissen pro Register verwendet wird. Die Auswahl der Tagger basierte einerseits darauf, dass sie unterschiedliche Prinzipien benutzen: So funktioniert der TreeTagger nach dem Hidden Markov Model (HMM, vgl. Baum / Petrie 1966), MarMot nach dem Prinzip der Conditional Random Fields (DRF, vgl. Hammersly / Clifford 1971) und Perceptron nach dem neuronaler Netzwerke. Der Grund für die Wahl des TreeTaggers war zudem seine Prävalenz in der Forschungsliteratur, die nicht zuletzt durch gute Ergebnisse begründet scheint (vgl. Dipper 2012; Giesbrecht / Evert 2009). In einem ersten Schritt (vgl. Paluch et al. in Vorbereitung) wurden hier bereits getaggte Texte aus dem DTA in fünf Epochen geordnet. Neben der Moderne umfassten diese zu Vergleichszwecken auch Barock, Aufklärung, Romantik, und Realismus. Für die Einteilung der Epochen in Zeitperioden sowie der Einteilung von Autoren zu bestimmten Epochen wurden einschlägige Literaturgeschichten zu Rate gezogen (u.a. Beutin 2001; Jørgensen / Bohnen / Øhrgaard 1990; Meid 2009; Schulz 2000; Sprengel 1998, 2004). Anschließend wurden die Tagger auf jeweils eine Epoche trainiert, indem die Texte randomisiert in Trainings- und Evaluationstexte getrennt wurden und eine k-fold cross validation (vgl. Witten / Elbe 2005) für jeden Tagger durchgeführt wurde. Die Ergebnisse (vgl. auch Paluch et al. in Vorbereitung) weisen auf eine gute Genauigkeit insbesondere von Perceptron hin, müssen aber unter dem Vorbehalt betrachtet werden, dass der Status des DTA als Goldstandard für POS-Tagging noch fraglich ist. Hier setzen wir mit Strategie (2) an, mit der wir zunächst für alle POS-Tags Übereinstimmung und Abweichung (Matches und Missmatches) des Outputs des Tree-Taggers und MarMots mit dem DTA-Datensatz vergleichen. Aufbauend auf diese quantitative Überprüfung der einzelnen Tag-Zuweisung evaluieren wir zudem händisch Stichproben der Nichtübereinstimmungen in der Annotation der einzelnen Tags. Unsere quantitative Überprüfung ergibt eine generelle Übereinstimmung mit dem DTA-Datensatz in POS-Tags für den TreeTagger und den Marmot Tagger von jeweils 80%. Die generelle Übereinstimmung zwischen den Tags des TreeTaggers und denen des MarMot Taggers hingegen liegt bei 0.78%. Tabelle 1 zeigt Ergebnisse aus der Analyse der Übereinstimmungen (Matches) und Abweichungen (Missmatches) bei der POS-Tagzuweisung von TreeTagger (TT) und MarMot (MM) im Vergleich mit den Tags des DTA. Abgebildet sind hier solche Fälle pro POS-Tag, in denen TT und MM übereinstimmen, aber vom DTA abweichen. Die Tabelle listet die elf POS-Tags, die (von TT und MM gemeinsam) die proportional den höchsten Anteil der Abweichung vom DTA ausmachen. Tabelle 1 Abweichung zu POS-Tags des DTA (Übereinstimmung MM und TT) *STTS Tagset Aufbauend auf diesen Daten wird im nächsten Schritt die tatsächliche Qualität der bereits vorhandenen DTA-Tags für den Datensatz der literarischen Texte evaluiert. Auf der Grundlage von randomisiertem Sampling verbessern wir die POS-Annotationen bei tatsächlichen Fehlern händisch, um in der Folge u.a. eigene Sprachmodelle für unser spezifisches Korpus narrativer Texte zu trainieren. So soll schließlich unter Nutzung vorhandener Ressourcen ein Silber- oder sogar Goldstandard für das POS-Tagging historischer literarischer Texte des Deutschen erreicht werden. KOLIMO wird in der Beta-Version zur Tagung veröffentlicht (s. Gleichzeitig planen wir eine detaillierte Dokumentation der Arbeitsschritte zu veröffentlichen, die ähnlichen Projekten als Leitfaden zur Verfügung zu stehen soll. Unser Projekt dokumentiert in seinem gegenwärtigen Status Entscheidungen auf verschiedenen konzeptionellen, analytischen und prozeduralen Ebenen. Es zeigt, dass der Aufbau eines digitalen literarischen Korpus, das den synchronen und diachronen quantitativen Vergleich einer Schwerpunktepoche erlauben soll, bei Weitem keine triviale Aufgabe darstellt. So wurde zum Beispiel deutlich, wie Hypothesen zur Konstitution von Epochen, Autorschaft und Gattungen die Korpuskompilation steuern ‚Äì und deshalb auf einer möglichst präzisen Modellierung der zugrundeliegenden textwissenschaftlichen Theorien fußen sollten. Gleichzeitig sind Metadaten (u. a. Autor, Titel, Publikationsdatum, Publikationsort, Gattung) und linguistische Parameter (wie POS) gerade die Ansatzpunkte, an denen philologische Fragestellungen in präzise und praktikable Kategorien umgewandelt werden können. Nicht zuletzt deshalb sollten literarische Daten in flexiblen Architekturen gespeichert werden, die zusätzliche Annotationsebenen zulassen ‚Äì denn hermeneutische Erkenntnisprozesse stellen eine erwachsene Stärke der Geisteswissenschaften dar, die auch im digitalen Zeitalter einen explizit modellierten Platz einnehmen muss."
2017,DHd2017,workshop-REITE.xml,CUTE: CRETA Unshared Task zu Entitätenreferenzen,"Nils Reiter (Universität Stuttgart, Deutschland); Andre Blessing (Universität Stuttgart, Deutschland); Nora Echelmeyer (Universität Stuttgart, Deutschland); Steffen Koch (Universität Stuttgart, Deutschland); Gerhard Kremer (Universität Stuttgart, Deutschland); Sandra Murr (Universität Stuttgart, Deutschland); Maximilian Overbeck (Universität Stuttgart, Deutschland); Axel Pichler (Universität Stuttgart, Deutschland)","shared task, unshared task, Entitätenreferenz, Annotation, Modularisierung von Forschungsfragen","Inhaltsanalyse, Beziehungsanalyse, Annotieren, Veröffentlichung, Kollaboration, Literatur, Methoden, Forschungsprozess, Standards, Text","Der Workshop zum CRETA Unshared Task (CUTE) verfolgt ein inhaltliches und ein methodisches Ziel. Das inhaltliche Ziel ist die Anregung eines Diskurses über Entitäten, deren Annotation und Kategorisierung entlang von geistes- und sozialwissenschaftlichen Forschungsfragen sowie deren Potential als disziplinübergreifende Textanalyseaufgabe. Methodisch möchten wir ein Workshop-Format erproben, das unseres Erachtens eine produktive Schnittstelle zwischen Geistes-/SozialwissenschaftlerInnen und InformatikerInnen bildet. Das genaue Programm des Workshops wird von den Teilnehmenden durch Beiträge gestaltet (durch Beiträge, siehe Call for Papers Das Konzept der Entität und ihrer Referenz ist ein bewusst weites, das anschlussfähig sein soll für verschiedene Forschungsfragen aus den Geistes- und Sozialwissenschaften. Wir möchten dabei explizit verschiedene Perspektiven auf Entitäten berücksichtigen. Figuren in literarischen Texten sind ""mit ihrer sinnkonstitutiven und handlungsprogressiven Funktion"" ein zentraler Bestandteil der fiktiven Welt (Platz-Waury 1997). Von besonderem Interesse dabei sind Figurenkonstellationen und Interaktionen, die Entwicklung von Figuren sowie die Funktionalisierung von Figuren als Handlungsträger. Die Erkennung von Figurenreferenzen ist grundlegend, um z.B. Figuren zu charakterisieren, ihre Relationen identifizieren und Netzwerkanalysen durchführen zu können (vgl. Jannidis 2015, Trilcke 2013). Neben der Figur rückt –- spätestens seit dem Politische Parteien, internationale Organisationen oder Institutionen sind seit jeher zentrale Analyseobjekte der empirischen sozialwissenschaftlichen Forschung und werden spätestens seit dem Im Unterschied zu den Literatur- und Sozialwissenschaften spielen Entitäten als Untersuchungsgegenstand in philosophischen Texten zunächst keine Rolle. Aufgrund ihrer metareflexiven Ausrichtung fragt Philosophie primär nicht nach individuell unterscheidbaren Objekten in der echten oder einer fiktiven Welt, sondern beschäftigt sich mit transzendentalen Fragen nach den Bedingungen und Möglichkeiten derartiger individueller Objekte. Dabei arbeitet sie mit abstrakten Konzepten, die sich ebenfalls als -- nicht-dingliche -- Objekte einer Welt auffassen lassen. Pragmatisch gesehen erfolgt die Referenz auf abstrakte Konzepte in Texten jedenfalls in ähnlicher Weise wie die Referenz auf Figuren, Organisationen und Orten (s.u.). Auch wenn die Interpretation von z.B. der Erwähnung von Organisationen in politischen und des Auftretens von Figuren in literarischen Texten anderen Regeln folgt und mit anderen Forschungsfragen zusammenhängt, gibt es Gemeinsamkeiten auf linguistisch-struktureller Ebene. Im Text realisiert werden Referenzen auf die o.g. Arten von Entitäten entweder als Eigennamen ( Abstrakt gesprochen verstehen wir unter Entitäten individuell unterscheidbare Objekte in der echten oder einer fiktiven Welt. Wir unterscheiden sechs verschiedene Typen von Entität: Personen, Orte, Ereignisse, Organisationen, kulturelle Artefakte und Konzepte. Die Bezeichnung als ""Objekt"" impliziert also Die Erstellung abstrakter Annotationsrichtlinien und deren systematische, kontrollierte Anwendung (Annotation) auf konkrete Texte verspricht im Wesentlichen zwei Ergebnisse: In diesem Sinne ist das zweite, methodische Ziel des Workshops zu verstehen: Wir möchten einen Community-Task veranstalten, der eine Beiträge zu Aufgabe 1 werden quantitativ evaluiert und im Wettbewerb mit den Evaluationsergebnissen der anderen Beiträge verglichen ( Das von uns im Rahmen des Workshops veröffentlichte Korpus umfasst vier Teilkorpora: Auch wenn jedes Teilkorpus seine eigenen Besonderheiten hat, wurden alle nach einheitlichen Annotationsrichtlinien annotiert, die wir ebenfalls veröffentlichen und zur Diskussion stellen möchten. Der Workshop wird ausgerichtet vom Centre for Reflected Text Analytics (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine ""black box"" sein, sondern auch für nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird."
2017,DHd2017,vortrag-TRILC.xml,"Netzwerkdynamik, Plotanalyse 'Zur Visualisierung und Berechnung der ""progressiven Strukturierung"" literarischer Texte","Peer Trilcke (Universität Potsdam, Deutschland); Frank Fischer (Higher School of Economics, Moskau, Russland); Mathias Göbel (Staats- und Universitätsbibliothek Göttingen, Deutschland); Dario Kampkaspar (Herzog-August-Bibliothek Wolfenbüttel, Deutschland); Christopher Kittel (Universität Graz, Österreich)","Netzwerkanalyse, Literatur, Plot, Digitale Literaturwissenschaft, Strukturanalyse","Programmierung, Strukturanalyse, Modellierung, Theoretisierung, Netzwerkanalyse, Visualisierung, Daten, Sprache, Literatur, Text, Visualisierung","Die Anwendung von Methoden der Netzwerkanalyse auf literarische Texte hat sich in den letzten Jahren zu einem eigenständigen Forschungsfeld der Darüber hinaus wird ausgelotet, inwiefern sich mittels visueller und/oder statistischer Auswertung der Netzwerkdaten genuin literaturwissenschaftliche Erkenntnisse gewinnen bzw. neue Wege der literaturwissenschaftlichen Analyse entwickeln lassen: Neben Ansätzen zur quantitativen Beschreibung und Hierarchisierung des Figurenpersonals (Jannidis et al. 2016) werden hier, im Rahmen korpusbasierter Analysen, Optionen der literaturhistorischen Periodisierung auf Basis von quantitativen Strukturdaten diskutiert (Trilcke et al. 2015) sowie Typen der ästhetischen Modellierung sozialer Formationen in und durch literarische Texte differenziert (Stiller et al. 2003; Stiller & Hudson 2005; Trilcke et al. 2016). Nahezu keine Rolle spielte bisher jedoch ein durchaus hehres Erkenntnisversprechen, das 'bereits in der prä-automatisierten Zeit formuliert (de Nooy 2006) 'auch den Fluchtpunkt des einschlägigen ""Pamphlets"" von Franco Moretti steht: dass nämlich die Netzwerkanalyse als ein Instrumentarium der quantitativen ""plot analysis"" (Moretti 2011) fungieren könne. Tatsächlich lässt sich dieses Erkenntnisversprechen mit den derzeit verfolgten Ansätzen im Bereich der literaturwissenschaftlichen Netzwerkanalyse kaum aufgreifen, geschweige denn einlösen (so auch Prado et al. 2016). Denn die sequentielle Dimension literarischer Texte, mithin ihre Temporalität, bleibt hier in der Regel ausgeblendet: Erfasst, visualisiert und analysiert werden statische Netzwerke. Plot ist jedoch wesentlich ein Konzept, das die Temporalität narrativer (wie auch dramatischer) Versuche, die Netzwerkanalyse in Richtung einer quantitativen Plotanalyse weiterzuentwickeln, stehen also zunächst vor der Aufgabe, bei ihrer Modellierung des Untersuchungsgegenstandes die Zeitdimension zu berücksichtigen. Der Text ist entsprechend nicht lediglich als ein statisches Netzwerk zu modellieren, sondern als eine sich über die Zeit verändernde Folge von Netzwerkzuständen. Erst anhand dieser Netzwerkdynamiken lassen sich die Erkenntnispotenziale, die netzwerkanalytische Zugänge für die quantitative Plotanalyse bergen, überhaupt diskutieren. Der projektierte Vortrag wird 'in Anschluss an Prado et al. 2016 'aus theoretischer und methodischer Perspektive sowie anhand exemplarischer Fallstudien eine Erweiterung der bisherigen, auf die Analyse Entsprechend der zweigleisigen Auswertungsroutinen, die auf netzwerkanalytische Daten angewendet werden, wird der Vortrag zwei Szenarien der netzwerkbasierten Analyse der progressiven Strukturierung literarischer Texte diskutieren: zum einen (3.1) sind Möglichkeiten und Erkenntnispotenziale der Während die Visualisierung dynamischer Netzwerke in anderen Domänen bereits seit längerem gang und gäbe ist (vgl. exemplarisch Pohl et al. 2008; Frederico et al. 2011), wurde erst vor Kurzem der Versuch unternommen, entsprechende Visualisierungsverfahren auch auf literarische Netzwerke anzuwenden (Xanthos et al. 2016). Während Xanthos et al. ¬†u.a. auf didaktische Anwendungsszenarien hinweisen, wird ein literaturwissenschaftliches Erkenntnispotenzial lediglich angedeutet; eine Diskussion dessen, was durch eine solche Visualisierung nicht nur Hingegen zeigen erste, im Vortrag zu vertiefende Zwischenergebnisse unserer Analysen, dass die dynamische Visualisierung insbesondere dann erkenntnisrelevant wird, wenn es darum geht, multiplexe Netzwerke zu modellieren, d.¬†h. Netzwerke, die unterschiedliche Interaktionstypen zugleich erfassen. So zeigt eine statische Visualisierung von Lessings bürgerlichem Trauerspiel Abb.¬†1: Statisches Netzwerk zu Lessing: Zerlegt man das statische Dramennetzwerk (Abb.¬†1) nun nach Akten und dynamisiert es damit, so zeigt sich, dass die Familie Galotti zu keinem Zeitpunkt des Dramas gemeinsam auf der Bühne steht (vgl. Abb.¬†2). Abb.¬†2: Dynamisches Netzwerk zu Lessings Anschaulich und Dass dynamische Visualisierungen in diesem Sinne aus literaturwissenschaftlicher Sicht v.a. für die Analyse multiplexer Netzwerke produktiv gemacht werden können, werden wir im Vortrag anhand weiterer Beispiele aus dem dlina-Korpus (philologisch kuratierte Netzwerkdaten zu 465 deutschsprachige Dramen aus der Zeit 1730–1930, siehe Mehr noch als die Visualisierung statischer Netzwerke stellt diejenige dynamischer im Grunde keine Option eines korpusbasierten Von Carley (2003: 135–136) wurden dabei mehrere rudimentäre globale Maße (i.¬†e. Der Vortrag liefert einen Beitrag zur Methodenentwicklung und -reflektion im Bereich der"
2017,DHd2017,poster-REGER.xml,Comparison of Methods for Automatic Relation Extraction in German Novels,"Markus Krug (Universität Würzburg, Deutschland); Christoph Wick (Universität Würzburg, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland); Isabella Reger (Universität Würzburg, Deutschland); Lukas Weimer (Universität Würzburg, Deutschland); Nathalie Madarasz (Universität Würzburg, Deutschland); Frank Puppe (Universität Würzburg, Deutschland)",Relation Extraction,"Beziehungsanalyse, Modellierung, Annotieren, Netzwerkanalyse, Literatur, benannte Entitäten (named entities)","Die automatische Erkennung von spezifischen Relationen ermöglicht Einsichten über die Beziehungen zwischen Entitäten. Solche Informationen können nicht nur als Kantenbezeichner in sozialen Netzwerken fungieren, sondern auch als globale Constraints für das schwierige Problem der Coreference Resolution eingesetzt werden. Darüber hinaus kann eine Relationserkennung zur Beantwortung diverser literarischer Fragestellungen eingesetzt werden, z.B. ob eine Romangattung sich mit bestimmten Relationstypen befasst, oder ob die Arten der Relationen sich über die Jahrhunderte verändern. In dieser Arbeit stellen wir ein Label-Set für die Extraktion von binären Relationen zwischen Personen-Entitäten vor und vergleichen Feature-basierte Ansätze des maschinellen Lernens mit regelbasierten Ansätzen zur automatischen Erkennung dieser Relationen. Da Trainingsmaterial zur Verfügung steht, liegt der Fokus in dieser Arbeit auf dem Einsatz überwachter Methoden, d.h. unsere regelbasierten Verfahren sind ebenfalls auf einer zuvor abgetrennten Menge entwickelt worden. Wir verwenden ein neues Korpus, das manuell mit mehr als 50 verschiedenen, hierarchisch gegliederten Relationstypen annotiert wurde. Eine Übersicht über Arbeiten zur Relationserkennung findet sich in [Jung et al. 2012] sowie [Bach und Badaskar 2007]. Sowohl für den überwachten, als auch den halb-überwachten Fall wurden erfolgreiche Methoden entwickelt. Da dieses Paper sich hauptsächlich auf überwachte Algorithmen bezieht, geben wir nur einen knappen Überblick über halb-überwachte Verfahren. Algorithmen zur Relationsextraktion erhalten typischerweise zwei (oder mehr) Referenzen zu Entitäten (sogenannte Instanzen) als Input und sollen die Klasse, und das dazugehörige Label, vorhersagen, welche die Relation zwischen den Entitäten beschreibt. Die meisten Experimente wurden anhand englischer Texte und den Datensätzen der Automatic Content Extraction (ACE) Workshops 2004 und 2006 durchgeführt. Auf dem Datensatz von 2004 wurden Experimente zur Unterscheidung von 5 und 27 verschiedenen Klassen wie Arbeitsplatz-, körperliche, soziale, Mitgliedschafts- und Diskursrelationen (wobei manche Unterklassen von anderen sein können) betrachtet. Hierfür gibt es zahlreiche Ansätze, die jedoch alle versuchen, eine diskriminative Beschreibung der Instanzen zu erhalten und diese davon ausgehend zu klassifizieren: Im Folgenden vergleichen wir die genannten Methoden anhand eines Label-Sets zur Erkennung binärer Relationen zwischen Figuren in manuell annotierten Abschnitten von deutschsprachigen Romanen. Da Textstellen, an denen Relationen zwischen Entitäten explizit benannt werden, in Romanen typischerweise rar sind, ist es nicht sinnvoll, komplette Romane zu annotieren, da der Ertrag an Daten zu gering wäre. Aus diesem Grund wurde zunächst eine kleine Teilmenge per Hand annotiert und dann genutzt, um mit einem MaxEnt Classifier in einer Active Learning-Umgebung neue Sätze zum Labeln vorschlagen zu können. (Ein Überblick hierzu findet sich in Finn und Kushmerick [Finn und Kushmerick 2003]). Diese Umgebung erhielt Sätze aus 312 verschiedenen Romanen von Projekt Gutenberg und 215 Zusammenfassungen aus dem Kindler Literatur Lexikon Online. Daraus entstand ein Korpus mit 2412 Sätzen, die insgesamt 1265 Relationen enthalten (was wiederum die Knappheit an Daten illustriert). 33 Texte wurden zufällig für die Testmenge ausgewählt, sodass es feste Test- und Trainingsdaten gibt (1988 respektive 424 Sätze mit 1070 respektive 195 Relationen). Die verwendeten Label sind ähnlich zu Massey et al. [Massey et al. 2015]. Die Relationen werden durch eine Ontologie mit momentan 57 verschiedenen Relationstypen repräsentiert, die hierarchisch geordnet sind (beispielsweise ist die Relation ""Tochter"" der Relation ""Familie"" untergeordnet). Abbildung 1 zeigt die oberste Ebene des Label-Sets, mit den gleichen Kategorien wie in Massey et al. [Massey et al. 2015] und einer zusätzlichen Relation ""Liebe"". Abbildung 1: Die ersten beiden Ebenen unseres verwendeten Label-Sets mit den vier Haupttypen, die sich weiter in insgesamt 57 Relationstypen untergliedern lassen. Eine Relation wurde von einem Annotator als ein benannter, gerichteter Bogen zwischen zwei Entitäten in einem Satz gelabelt, sofern sie explizit im Text beschrieben ist. Es wurde immer das spezifischste Label verwendet, da die übergeordneten Relationstypen (vgl. Abbildung 1) daraus abgeleitet werden können. Abbildung 2 zeigt ein Beispiel einer Relation, wie sie in unserem Korpus annotiert ist. Abbildung 2: Zwei gelabelte Instanzen von Relationen in unserem Datensatz. Die erste zeigt die Relation ""hatTochter"" und die zweite die Relation ""hatVerehrer"". Um solche Relationen automatisch erkennen zu können, müssen die Texte eine große Zahl an Vorverarbeitungsschritten durchlaufen. Wir verwenden die Figurenerkennung von Jannidis et al. [Jannidis et al. 2015] und die gleiche Vorverarbeitung wie in [Krug et al. 2016]. Wir verwenden einen regelbasierten Ansatz mit manuell erstellten Regeln und zwei Feature-basierte Lernverfahren (Maximum Entropy, MaxEnt und Support Vector Machines, SVM). Der regelbasierte Ansatz nutzt sowohl die textuelle Repräsentation, als auch den kürzesten Pfad im Dependency-Baum und formuliert die Regel auf Basis dieser Repräsentationen und der Repräsentationen aus dem reinen Text. Das folgende Beispiel zeigt Regeln, die zu den Relationen aus Abbildung 2 passen: Die erste Regel basiert auf der angepassten Text-Repräsentation, während die zweite Regel sich auf den kürzesten Dependency-Pfad zwischen ""sich"" und ""sie"" bezieht. Die Zahlen in runden Klammern geben die Richtung an (in beiden Fällen von Entität 2 auf Entität 1). Die Regeln wurden manuell auf den zuvor gewählten Trainingsdaten erzeugt. Insgesamt wurden fast 500 solcher Regeln ermittelt. Der Großteil der Relationen konnte jedoch mit 3 Regeln (ab hier sogenannte Core-Regeln) abgedeckt werden, die Possessiv- und Genitivkonstruktionen abbilden. Die Feature-basierten Ansätze wurden in zwei Szenarien evaluiert: a) nur mit bereits bekannten Features aus Related Work und b) mit zusätzlichen Booleschen Features (eines pro Regel), falls eine der 500 Regeln passt. Tabelle 1 zeigt die Evaluationsergebnisse der verschiedenen Methoden für drei hierarchische Ebenen (alle Relationen, Relationen der obersten Ebene, alle 57 Relationstypen) und Tabelle 2 die Ergebnisse für die vier Relationstypen der obersten Ebene. Während die Verwendung aller Regeln zu einem F1-Score von 71% für alle Relationen und 59% für die vier übergeordneten Relationstypen führt, erreicht der Feature-basierte Ansatz mit MaxEnt mit einem Booleschen Feature für jede Regel etwas bessere Ergebnisse (F1 von 73,6% und 61,2%). Ohne die Regel-Features liegt der Score der Lernverfahren deutlich niedriger. Die SVM erreicht teilweise eine höhere Precision als MaxEnt, aber im Allgemeinen einen signifikant geringeren F1-Wert. Tabelle 1: Ergebnisse der verschiedenen Ansätze für drei verschiedene Evaluationsszenarien: binär (das reine Vorliegen einer Relation), für die 4 Haupttypen und für alle 57 Relationstypen insgesamt. Tabelle 2: Ergebnisse für die verschiedenen Ansätze, aufgeschlüsselt nach den 4 Haupttypen. Familienrelationen erreichen sehr gute Ergebnisse mit einem F1-Wert von fast 80% und einer Precision von bis zu 95%. Liebesrelationen sind schwerer zu erkennen, liegen aber dennoch bei 56,3% F1. Die anderen Relationstypen fallen in der Qualität ab, sind aber gleichzeitig weniger relevant. Sehr auffällig ist das gute Ergebnis für die drei Core-Regeln und dabei besonders die hervorragende Precision von 96,2% für Familien-Relationen. Eine genauere Betrachtung der False Positives (FP) in Tabelle 3 zeigt, dass diese Relationen fast immer syntaktisch korrekt erkannt wurden, aber semantisch irrelevant und daher nicht im Goldstandard annotiert sind (z.B. ""mein Gott""). Hier zeigt sich eine Schwachstelle dieser Arbeit: teilweise unpräzise Richtlinien für die Annotation von Relationen. Das ist jedoch ein sehr schwieriges Problem, das eventuell umgangen werden kann, wenn die Relationserkennung kein Ziel in sich, sondern eine untergeordnete Aufgabe im Zuge der Erkennung der Hauptfiguren und deren Beziehungen in Romanen ist. Tabelle 3: Auswertung der drei Core-Regeln auf unserem Datensatz Dieses Paper hat gezeigt, dass automatische Relationserkennung eine Herausforderung darstellt. Einfache Regeln können jedoch bereits einen wesentlichen Teil der Relationen mit hoher Precision erkennen. Dennoch ist der Bedarf an weiteren Verbesserungen durch fortschrittliche Methoden hier deutlich. Zudem ist die Evaluation der Relationserkennung an sich schwierig und kann besser im Kontext eines übergeordneten Ziels wie der automatischen Erstellung eines Netzwerks der Hauptfiguren eines Romans [Krug 2016] oder der Gattungsklassifikation [Hettinger et al. 2015] eingebracht werden."
2017,DHd2017,poster-BURGH.xml,Digitale Erschließung einer Sammlung von Volksliedern aus dem deutschsprachigen Raum,"Manuel Burghardt (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland); Sebastian Spanner (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland); Thomas Schmidt (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland); Florian Fuchs (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland); Katia Buchhop (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland); Miriam Nickl (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland); Christian Wolff (Lehrstuhl Medieninformatik, Universität Regensburg, Deutschland)","crowdsourcing, optical character recognition, optical music recognition, music transcription tool","Transkription, Crowdsourcing, Webentwicklung, Notenblätter, Werkzeuge","Dieser Beitrag beschreibt ein laufendes Projekt Die Datengrundlage des Projekts stellen umfangreichen Quellen zur Volksmusikforschung dar, die seit einigen Jahren von der Universitätsbibliothek Regensburg verwaltet werden. Die Regensburger Liedblattsammlung umfasst etwa 140.000 Blätter mündlich oder handschriftlich tradierter Volkslieder aus dem gesamten deutschsprachigen Raum, und ist, was Abdeckung und Umfang angeht, in dieser Form einzigartig (Krüger, 2013). Die losen Einzelblätter enthalten einerseits handschriftliche, monophone Melodien und andererseits Liedtexte, welche zumeist mit Schreibmaschine verfasst wurden (vgl. Abb. 1). Abbildung 1: Ausschnitt aus dem Liedblatt Nr. A23: ""Klana Mann wollt"" e grouß Fraa hou"". Zu den Liedblättern existieren darüber hinaus Metadaten wie Für die Transkription der Texte und Melodien wurden Tools für die automatische Erfassung evaluiert. Neben automatischer Texterkennung (OCR, Die Evaluation der Eignung bestehender OCR-Tools für den Kontext der Regensburger Liedblattsammlung lehnt sich an Kanungo, Marton und Bulbul (1999) an. Das Testkorpus umfasst 102 Liedblätter, die möglichst viele unterschiedliche typographische und orthographische Phänomene abdecken, etwa Druckschrift (mit unterschiedlich starkem Kontrast), Frakturschrift, aufgeklebte Korrekturen, Sonderzeichen, etc. Für die Evaluation wurde die Textzone unterhalb der Notenzeilen ausgewählt, da die Noten als unbekannte Sonderzeichen das Texterkennungsergebnis negativ verfälschen würden. Für jene Textzonen wurde eine manuelle Transkription erstellt, die in der weiteren Evaluation als Mithilfe des OCR-Evaluationstools Abbildung 2: OCR-Evaluationsergebnisse für die getesteten Tools hinsichtlich der korrekt erkannten, der falsch erkannten, der gar nicht erkannten sowie der überflüssigerweise erkannten Zeichen. Anhand dieser Parameter lassen sich Kennzahlen für die Tools berechnen, etwa die Abbildung 3: Boxplot zur Erkennungsgenauigkeit der einzelnen OCR-Tools. Dass In Anlehnung an eine OMR-Evaluationsstudie (Bellini, Bruno & Nesi, 2007) wurden drei der am weitesten verbreiteten OMR-Tools hinsichtlich ihrer Eignung für die Liedblattsammlung evaluiert: Anders als bei der OCR-Evaluation ist die Erstellung eines automatisch abgleichbaren Bei der Berechnung der Erkennungsgenauigkeit wurden dieselben Parameter verwendet wie schon bei der OCR-Evaluation (vgl. Abb. 2). Die Ergebnisse der OMR-Evaluation zeigen, dass hinsichtlich der durchschnittlichen Erkennungsgenauigkeit mit 36% bei Abbildung 4: Boxplot zur Erkennungsgenauigkeit der einzelnen OMR-Tools. Als alternative Erschließungsstrategie wurde ein Transkriptionstool namens Bei der Umsetzung des Tools für die Transkription der Regensburger Liedblätter wurde besonderes Augenmerk auf die einfache Bedienbarkeit durch iteratives Als erster Schritt wird in  Nach Angabe der Liedblattnummer sowie der Auswahl von Taktart und Tonart gelangt man in den eigentlichen Transkriptionsmodus, bei dem Takt für Takt auf einer interaktiven Notenzeile mit Maus und Tastatur (Shortcuts) transkribiert wird (vgl. Abb. 6). Jeder einzelne Takt kann im Browser abgespielt werden, um so ggf. auf auditiver Ebene schnell Transkriptionsfehler zu erkennen. Abbildung 6: Taktweise Transkription der Liedblätter mit dem Allegro-Tool. Im Hintergrund werden die Eingaben auf das virtuelle Notenblatt schließlich in ein maschinenlesbares Format ( Das Transkriptionstool befindet sich aktuell in der offenen Beta-Testphase und findet guten Zuspruch bei den Anwendern: Dieser Beitrag gibt einen Einblick in ein laufendes Projekt zur digitalen Erschließung einer großen Sammlung von Liedblättern. Während OCR-Tools für die automatische Erfassung der Liedtexte annehmbare Ergebnisse mit einer Erkennungsrate von bis zu 80% liefern, so liegt die Erkennungsgenauigkeit bestehender OMR-Tools für die handschriftlichen Notensätze bei lediglich maximal 36%. Im Falle der Notenerkennung wurde von Grund auf ein neues, intuitiv bedienbares Transkriptionstool entwickelt, welches über einen Crowdsourcing-Ansatz die sukzessive Erschließung der Notensätze sicherstellen soll. Aktuell liegt der Projektfokus auf der Erschließung der Liedblätter. Parallel entstehen zudem erste Prototypen (vgl. Burghardt et al., 2016) für das angedachte Informationssystem, das die Analyse der Liedblätter anhand der verfügbaren Metadaten, der Liedtexte sowie anhand verschiedener melodischer Parameter (vgl. Mongeau & Sankoff, 1990; Orio & Rod√°, 2009; Typke, 2007) erlaubt. Im Rahmen des weiteren Projektverlaufs sollen anhand der digital erschlossenen Liedblätter u.a. die folgenden Fragestellungen untersucht werden:"
2017,DHd2017,poster-LAUBR.xml,Visuelle Elemente grafischer Literatur: Aufmerksamkeitszuwendung und objektive Beschreibung,"Jochen Laubrock (Universität Potsdam, Deutschland); Eike Richter (Universität Potsdam, Deutschland); Sven Hohenstein (Universität Potsdam, Deutschland)","grafische Literatur, Comics, Eyetracking, Korpus","Räumliche Analyse, Annotieren, Visualisierung, Bilder, Text","Graphische Romane vereinen als hybride Gattung Aspekte von Literatur und bildender Kunst (McCloud, 1993). Wie interagieren Bild und Text beim Lesen graphischer Literatur und ermöglichen das Verstehen des Gesamtwerkes? Worauf fokussiert die Aufmerksamkeit des Lesers? Als Methode zur Beantwortung dieser Fragen ist die Blickbewegungsmessung besonders geeignet. Blickbewegungen haben sich in einer Vielzahl an Studien als valides, nichtreaktives Maß für die Verarbeitung und das Verstehen von Text und Bild erwiesen, in dem sich zudem auch unbewusste Verarbeitungsprozesse niederschlagen (Findlay & Gilchrist, 2003; Wade & Tatler, 2005). In früheren Arbeiten (Laubrock, Hohenstein & Thoß, 2016; Dunst, Hartel, Hohenstein Laubrock, 2016) haben wir mit Eyetracking-Analysen gezeigt, dass beim Lesen grafischer Literatur der größte Teil der Aufmerksamkeit dem Text in Sprechblasen und Beschriftungen (Captions) zugewandt wird und nur ein relativ kleiner Teil den originär visuellen Gestaltungselementen alloziert wird. Wird der visuelle Inhalt gar nicht beachtet, oder kann er möglicherweise bereits im peripheren Sehen während der Fixationen auf dem Text verarbeitet werden? Wir hatten bereits berichtet, dass Comics-Experten den Bildanteil stärker beachten und darauf verstehensrelevante Information extrahieren. In einer neuen Serie von Studien untersuchen wir mittels blickkontingenter Präsentation, ob (a) den Bildanteilen mehr Aufmerksamkeit zugewandt wird, wenn die Vorschau verhindert wird, indem das Bild erst eingeblendet wird, wenn der Blick sich auf ein Panel bewegt und (b) die Aufmerksamkeit andere grafische Elemente auswählt, wenn zwar der visuelle Teil der Panels sichtbar ist, der Text aber erst nach Fokussierung eines Panels eingeblendet wird. Das visuelle Material wurde auf zweierlei Weise annotiert. Einerseits annotierten Menschen Personen und einzelne Objekte innerhalb der Panels. Andererseits versuchen wir eine objektiven Beschreibung des visuellen Materials mithilfe von Deskriptoren aus dem maschinellen Sehen (Computer Vision), z.B. mittels Farbhistogrammen, lokalem Fourier-Spektrum oder SIFT-Deskriptoren (Lowe, 1999). Der Vorteil dieser Beschreibung ist neben der Objektivität die skriptgesteuerte Anwendbarkeit auf große Datenmengen, etwa digitalisierte Korpora grafischer Literatur. Vergleichbare Arbeiten aus der Schnittstelle von Kunstgeschichte und Informatik ermöglichen beispielsweise eine automatisierte Klassifikation von Kunstrichtungen (Saleh & Elgammal, 2015) und zeigen das Potenzial eines solchen Ansatzes als Stilometrie visueller Merkmale. Für die Zuordnung der Blickbewegungsdaten auf das Stimulusmaterial nutzen wir die im Projekt entwickelte Graphic Novel Markup Language (GNML), eine Erweiterung der Comic Book Markup Language (CBML; Walsh, 2012). Das Material wurde mit unserem Editor annotiert, für Weiterverarbeitung und statistische Analyse der Daten nutzten wir ein in Entwicklung befindliches R-Paket. Die objektive Beschreibung des visuellen Materials mit Deskriptoren aus dem maschinellen Sehen wurde unter Nutzung von OpenCV (Bradski, 2000) und VLFEAT (Vedaldi & Fulkerson, 2008) teils in Python und teils in Matlab implementiert, da für R für diesen Anwendungsbereich keine hinreichend entwickelte Funktionsbibliothek existiert."
2017,DHd2017,workshop-KOLLA.xml,Annotieren und Publizieren mit DARIAH-DE und TextGrid,"Thomas Kollatz (Steinheim-Institut für deutsch-jüdische Geschichte Essen, Deutschland); Philipp Hegel (Technische Universität Darmstadt, Deutschland); Ubbo Veentjer (Niedersächsische Staat- und Universitätsbibliothek Göttingen, Deutschland); Sibylle Söring (Niedersächsische Staat- und Universitätsbibliothek Göttingen, Deutschland); Stefan E. Funk (Niedersächsische Staat- und Universitätsbibliothek Göttingen, Deutschland)","Annotation, Infrastruktur, Publizieren, Archivieren","Annotieren, Kommunikation, Bearbeitung, Archivierung, Veröffentlichung, Kollaboration, Kommentierung, Daten, Bilder, Infrastruktur, Interaktion, Metadaten, Text, Werkzeuge, virtuelle Forschungsumgebungen","Im Rahmen des halbtägigen Workshops werden den Teilnehmerinnen und Teilnehmern Werkzeuge zum Publizieren und Annotieren von Forschungsdaten demonstriert, die im Rahmen von Hands-On-Einheiten anhand eigener und / oder bereitgestellter Daten erprobt werden können. Vorgestellt und angewendet werden das TextGrid- und DARIAH-DE Repositorium, der DARIAH-DE Publikator und die DARIAH-DE Annotation Sandbox. Zudem wird in die Arbeit mit dem Text-Bild-Link-Editor des TextGrid Laboratoriums eingeführt und exemplarisch gezeigt, diese Text-Bild Relationen mit Hilfe des Web-Publikationstools ""SADE 'Scalable Architecture for Digital Editions"" in eine digitale Präsentation bzw. ein Web-Portal zu übernehmen. Der Workshop richtet sich an Geisteswissenschaftlerinnen und –wissenschaftler aus text- und bildbasierten Disziplinen aller Phasen des akademischen Werdegangs ebenso wie an Vertreterinnen und Vertreter von Institutionen 'etwa Bibliotheken, Forschungsverbünde oder Archive –, die im Rahmen ihrer Vorhaben digitale Forschungsinfrastruktur nutzen bzw. nutzen wollen, um ihre Forschungsdaten nachhaltig digital zu publizieren und zu annotieren. Der Workshop liefert durch Kurzvorträge und Hands-On-Einheiten Einblicke in verschiedene Verfahren, Anwendungen und Workflows liefern, um Geisteswissenschaftlerinnen und Geisteswissenschaftlern die maschinenlesbare Annotation von Text- und Bilddaten sowie die Publikation solcher Forschungsdaten in einem Repositorium zu ermöglichen. Nach einer kursorischen Einführung in die Angebote von TextGrid und DARIAH-DE liefert ein Überblick über das Annotieren in den digitalen Geisteswissenschaften verschiedene Anwendungsszenarien, -anforderungen, -modelle und -technologien. Dabei werden neben bereits bestehenden Angeboten wie dem TextGrid Text-Bild-Link-Editor auch neuere Entwicklungen wie die Annotation Sandbox und das DARIAH-DE Repositorium und seine Publish GUI (Publikator) demonstriert und in interaktiven Übungen durch die Teilnehmenden anhand eigener bzw. zur Verfügung gestellter Daten erprobt.  Digitales Annotieren ist zentrale Praxis bei der Wissensgenerierung und variiert je nach spezifischer wissenschaftlicher Zielsetzung und Forschungsgegenstand. Verfahren des fachwissenschaftlichen digitalen Annotierens bilden heute eine der Kernanwendungen der Digital Humanities. Im Zentrum steht dabei ein weites Spektrum von Daten und / oder Objekten, z.B. Texte, Bilder und Musik (Töne, Noten). Digitale Annotationen unterscheiden sich daher in Form, Funktion und Tragweite. Einführend werden die technischen Ebenen und theoretischen Dimensionen der digitalen Annotation in den Geisteswissenschaften exemplarisch erörtert. Die vermittelten Grundlagen können danach im Workshop praktisch angewandt werden. Forschungsinfrastrukturen wie TextGrid und DARIAH-DE haben zum Ziel, methodologische Fähigkeiten auf diesem Gebiet zu vermitteln, entsprechende Verfahren zu evaluieren bzw. bereitzustellen und die nachhaltige Anwendung dieser Verfahren in den Fachwissenschaften zu ermöglichen. Die DARIAH-DE Annotation Sandbox (Beta) ermoÃàglicht heute die Text- und Bildannotation der Bestände des TextGrid Repository. Darüber hinaus können beliebige Webseiten uÃàber den DARIAH-DE Annotationsdienst annotiert werden. Zudem lässt sich der DARIAH-DE Annotationsdienst in eigene Webseiten einbetten; hierzu wurden die digitalen Werkzeuge Annotator.js, Via und ein Annotation Manager über die DARIAH AAI (Authorization and Authentification Service) verfügbar gemacht. Die DARIAH-DE Annotation Sandbox gestattet die direkte Verbindung der in den Repositorien publizierten Forschungsdaten mit ihrer digitalen Annotation. Diese schließt sowohl die disziplinübergreifenden Nachnutzung als auch die Datenanreicherung oder die Analyse ein. Mittelfristig können Annotationen somit als Zwischenschritt des Forschungsprozesses, aber auch als genuines Forschungsergebnis - etwa im Sinne einer Mikropublikation - verstanden bzw. generiert, verfügbar gemacht und als solches nachgenutzt werden. Im Rahmen einer digitalen Infrastruktur fließen sie wie die Forschungsdaten, auf die sie Bezug nehmen, ebenfalls in die Archivierung ein, um ¬†weiterverarbeitet und nachgenutzt zu werden. Ein weiteres Anwendungsszenario digitaler Annotation stellt die Annotation von Bildern bzw. Bilddaten dar. Eine Vielzahl von Werkzeugen im TextGrid Laboratory erlaubt das Arbeiten mit Texten und Bildern, aber auch beispielsweise mit Noten und Digitalisaten. Eine dieser Komponenten, die auch für die Annotation von Bildbereichen dienen kann, ist der Text-Bild-Link-Editor. Er unterstützt den in TextGrid integrierten XML-Editor bei der Alignierung von Text- und Bildelementen. Ziel ist die Erstellung einer Ausgabedatei, die die Textelemente und die topographische Position von rechtwinkligen und polygonen Bildbereichen in SVG miteinander verknüpft, wie dies zum Beispiel bei der Verbindung von Faksimiles und Transkriptionen in kritischen Editionen der Fall ist. Auch können Bilder auf diese Weise im Rahmen kunsthistorischer Untersuchungen annotiert werden. Die Software SADE der Berlin-Brandenburgischen Akademie der Wissenschaften ist als ""Skalierbare Architektur für digitale Editionen"" in TextGrid eingebunden, um eigene Webportale für die Publikation gestalten zu können. Sie enthält ein Modul, mit dem die Verknüpfungen, die mit dem Text-Bild-Link-Editor erstellt wurden, in ein Web-Portal übernommen werden können. Dieses Modul basiert auf dem in DARIAH-DE integrierten Werkzeug ""Semantic Topological Notes"" (SemToNotes). Es erlaubt unter anderem, Zeilen auf einem Digitalisat auszuwählen und Transkriptionen anzuzeigen. Das DARIAH-DE Repositorium bildet eine zentrale Komponente der Infrastruktur, auf die mittels verschiedener Dienste und Anwendungen zugegriffen werden kann. Das Repositorium erlaubt es, Forschungsdaten zu speichern, diese mit Metadaten zu versehen und die Forschungsdaten durch die Generische Suche aufzufinden. Die Daten werden im DARIAH-DE Storage sicher gespeichert. Darüber hinaus ermöglicht das Repositorium die nachhaltige und sichere Archivierung von Datensammlungen bzw. Kollektionen. Abb.1: DARIAH-DE-Repositorium: Architektur Dies ist komfortabel und intuitiv über ein Web-Interface des DARIAH-DE Portals im Browser möglich, dem DARIAH-DE Publikator. Daten im Repositorium sind in Kollektionen organisiert, die zunächst vom Nutzer über den Publikator angelegt und mit Metadaten ausgezeichnet werden. Einer Kollektion können beliebig viele Dateien zugeordnet werden, die ebenfalls über den Publikator hochgeladen und mit Metadaten ausgezeichnet werden. Eine publizierte Kollektion sowie alle darin enthaltene Objekte können unmittelbar nach dem Publizieren per Persistent Identifier (PID) referenziert werden und sind damit öffentlich zugänglich und nachhaltig referenzier- und zitierbar. Im nächsten Schritt kann die Kollektion in der Collection Registry nachgewiesen und veröffentlicht werden. Sobald die Kollektion selbst ebenfalls in der Collection Registry publiziert wurde, sind die Daten auch mit der Generischen Suche recherchierbar. Abb. 2: DARIAH-DE Publikator: Übersicht über die Kollektionen Abb. 3: DARIAH-DE Publikator: Kollektion bearbeiten Im Workshop werden exemplarisch Annotationen an einem Digitalisat in TextGrid vorgenommen. Zu diesem Zweck ist ein eigener Rechner mitzubringen, auf dem im Idealfall TextGrid bereits installiert ist 'Eine Registrierung für TextGrid und DARIAH kann online beantragt werden unter  Bitte teilen Sie uns im Vorfeld des Workshops (möglichst bis zum 5. Februar 2017) mit, ob und welche eigenen Materialien Sie verwenden wollen. Für Rückfragen erreichen Sie uns unter workshop@de.dariah.eu Mirjam Blümm, Niedersächsische Staats- und Universitätsbibliothek Göttingen, Abt. Forschung und Entwicklung, Papendiek 14, 37073 Göttingen, Stefan E. Funk, Niedersächsische Staats- und Universitätsbibliothek Göttingen, Abt. Forschung und Entwicklung, Papendiek 14, 37073 Göttingen, Canan Hastik, Technische Universität Darmstadt,¬†Dolivostraße 15,¬†Institut für Sprach- und Literaturwissenschaft,¬†64293 Darmstadt, Philipp Hegel, Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Dolivostraße 15, 64293 Darmstadt, Thomas Kollatz, Salomon Ludwig Steinheim-Institut für deusch-jüdische Geschichte, Essen, Edmund-Körner-Platz 2, 42157 Essen, Sibylle Söring, Niedersächsische Staats- und Universitätsbibliothek Göttingen, Abt. Forschung und Entwicklung, Papendiek 14, 37073 Göttingen, Ubbo Veentjer, Niedersächsische Staats- und Universitätsbibliothek Göttingen, Abt. Forschung und Entwicklung, Papendiek 14, 37073 Göttingen, Aufgrund des hohen Praxisanteils soll die Zahl der Teilnehmerinnen und Teilnehmer auf möglichst 25 beschränkt bleiben. WLAN / Beamer / Stellwände / Verlängerungskabel"
2017,DHd2017,vortrag-WAGNE.xml,Ambige idiomatische Ausdrücke in kinderliterarischen Texten: Mehrwert einer Datenbankanalyse,"Wiltrud Wagner (Eberhard Karls Universität Tübingen, Deutschland)","Datenbank, Ambiguität, Idiom, interdisziplinär, Analysetool","Entdeckung, Sammlung, Gestaltung, Strukturanalyse, Annotieren, Theoretisierung, Archivierung, Community-Bildung, Veröffentlichung, Kollaboration, Organisation, Konservierung, Visualisierung, Sprache, Literatur, Metadaten, Methoden, Visualisierung","In meinem Vortrag setze ich mich mit der Frage auseinander, welchen Beitrag die Datenbank TInCAP (""Tübingen Interdisciplinary Corpus of Ambiguity Phenomena""), die bei der Tagung der Digital Humanities im deutschsprachigen Raum 2016 in Leipzig vorgestellt wurde und die der Sammlung und Annotation von Ambiguitätsbelegen dient, zur Erforschung des Phänomens ""Ambiguität"" leisten kann. Den Mehrwert, den TInCAP durch die innovative interdisziplinäre Annotation und die Zusammenführung von Belegen in einer durchsuchbaren Datenbank liefert, werde ich am Beispiel ambiger idiomatischer Ausdrücke in kinderliterarischen Texten illustrieren. Die Datenbank TInCAP entsteht im Rahmen des interdisziplinären Graduiertenkollegs Auch wenn alle an diesem Projekt beteiligten WissenschaftlerInnen das Interesse am Phänomen der Ambiguität verbindet, das hier als Doppel- oder Mehrdeutigkeit in ihren verschiedensten Formen verstanden wird, so sind die zu annotierenden Belege doch sehr divers: Durch die Vielzahl der beteiligten Disziplinen unterscheiden sich die Belege hinsichtlich Medium (aktuell: Schrift, Audio, Bild, Video) und Sprache (aktuell: Deutsch, Englisch, Französisch, Hebräisch, Italienisch, Latein, Spanisch, Griechisch), aber auch Umfang. Im Bestreben, eine gemeinsame Datenbank aufzubauen, sahen wir uns demnach zwei großen Herausforderungen gegenüber gestellt: (1) Der Erarbeitung einer disziplinenübergreifenden Terminologie, die einerseits präzise, andererseits aber nicht an das Vokabular einer der Disziplinen gebunden ist, und (2) der Entwicklung eines interdisziplinären Annotationsschemas, das 'trotz der notwendigen Komplexitätsreduktion 'den Anforderungen der einzelnen Disziplinen genügt und für alle Beteiligten profitabel ist. Das Ergebnis ist ein Annotationsschema, das die folgenden fünf Punkte fokussiert: Zusätzlich ist die Verknüpfung von Annotationen möglich, zum Beispiel, wenn ein Beleg auf verschiedenen Kommunikationsebenen (unterschiedlich) annotiert wird. Die Nachhaltigkeit der gesammelten Daten wird durch eine Kombination verschiedener Faktoren gewährleistet: Das von uns entwickelte XML-Schema ist soweit möglich TEI-konform, es wurde für die inhaltliche Annotation der Daten um ein eigenes Schema erweitert. Der gesamte Korpus bzw. Subkorpora können im XML-Format im- und exportiert werden. Diese XML-Dateien werden in Kooperation mit Clarin-D Tübingen im Rahmen der universitären Infrastruktur langfristig gespeichert, katalogisiert und mit PIDs zugänglich gemacht. Teilkorpora können dabei ebenso exportiert werden wie das Gesamtkorpus. Bei Video-, Audio- und Bilddateien halten wir uns an die üblichen Standards für nachhaltige Datenformate (nicht-proprietäre Formate, Formate mit gutem Nachnutzungswert). Nach der allgemeinen Vorstellung der Datenbank wende ich mich im zweiten Teil des Vortrags der Frage zu, was die Datenbank im Hinblick auf konkrete Fragestellungen leistet. Die von mir in die Datenbank eingebrachten Ambiguitätsbelege entstammen zum größten Teil meiner Dissertation, die einen interdisziplinären Beitrag zur Ambiguitätsforschung leistet: Der linguistische Teil der Arbeit untersucht, wie idiomatischen Ausdrücken das Potential zur Ambiguität inhärent sein kann. An der Schnittstelle zur Literaturwissenschaft zeigt die Arbeit, wann und wie idiomatische Ausdrücke in Interaktion mit unterschiedlichen Kotexten ihr Ambiguitätspotential entfalten. Am Beispiel von kinderliterarischen Texten wird schließlich dargestellt, wie die aus dieser Interaktion resultierende Bewusstmachung von Ambiguität als sprachspielerisches Potential für literarische Texte produktiv gemacht werden kann. (a)-(c) stellen typische Beispiele aus meinem Korpus dar, die jeweils annotierten Stellen sind fett markiert: (a)  (c) Die Annotation meiner Beispiele mit TInCAP ermöglicht die Sichtbarmachung von Aspekten, die bei der reinen linguistischen oder literaturwissenschaftlichen Analyse möglicherweise verborgen bleiben. Besonderes Gewicht kommt dabei der Möglichkeit zu, Ambiguitäten auf mehreren Kommunikationsebenen zu annotieren und die resultierenden Annotationen zu verknüpfen. Dies möchte ich anhand von Beispielen wie (a)-(c) illustrieren und mich dabei auf folgende Phänomene konzentrieren: Diese Phänomene, die erst durch die Annotation mit TInCAP und durch entsprechende Suchabfragen sichtbar werden, zeigen das Potential, das diese Datenbank innerhalb eines Projekts entfaltet. In einem abschließenden Ausblick möchte ich darüber hinaus auf den interdisziplinären Nutzen der Datenbank verweisen, der im Rahmen des GRK 1808 bereits zum Tragen kommt, insbesondere in der Vergleichbarkeit, die über Medien hinweg geschaffen wird."
2017,DHd2017,vortrag-GIUSE.xml,Datenvisualisierung als Aisthesis,"Evelyn Gius (Universität Hamburg, Deutschland); Rabea Kleymann (Universität Hamburg, Deutschland); Jan Christoph Meister (Universität Hamburg, Deutschland); Marco Petris (Universität Hamburg, Deutschland)","Datenvisualisierung, Hermeneutik, visuelle Grammatik, Interaktivität","Umwandlung, Gestaltung, Programmierung, Bearbeitung, Visualisierung, Daten, Bilder, Interaktion, Forschungsprozess, Werkzeuge, Visualisierung","Jede/r DH-Praktiker/in weiß: Computergestützte Forschung in den Geisteswissenschaften beginnt mit der Übersetzung relevanter Phänomene in digitale Daten. Seltener thematisiert wird dagegen, dass die digitale Derartige Expansionsverfahren systematisch zu beschreiben, ist in den Geisteswissenschaften Visualisierungen gelten heute disziplinübergreifend als probates Mittel der ""Expansion"" schwer überschaubarer Primär- und Sekundärdaten in intuitiv erfassbarer Form (Goodings 2003:281). In der disziplinspezifischen Perspektive ist allerdings zugleich zu fordern, dass den methodologischen Besonderheiten der Datenvisualisierungen, die als visuelle Expansionsverfahren Trotz der großen Vielfalt an bestehenden Visualisierungstools und Visualisierungsmetaphern gibt es allerdings bislang kein derartiges, theoretisch reflektiertes Gegenstand des Projekts ""3DH 'dreidimensionale dynamische Datenvisualisierung und Exploration fuÃàr Digital Humanities-Forschungen"" ist die Entwicklung und prototypische Implementierung eines solchen Konzepts der geisteswissenschaftlichen Datenvisualisierung. Grundlegendstes dieser Erfordernisse ist, die bildhafte Veranschaulichung von Daten konsequent bi-direktional zu denken. Die Zweites Erfordernis ist, dass hermeneutisch funktionale Visualisierungen neben generischen Anforderungen auch die Besonderheiten geisteswissenschaftlicher Praxis in den Das skizzierte Spannungsverhältnis zwischen den allen Geisteswissenschaften gemeinsamen und den disziplinspezifischen Anforderungen an eine visuelles ""Expansionskonzept"" hat Grinstein (2012) zur Formulierung einer ""grand challenge"" motiviert:. Er fordert ein Visualisierungssystem, das auf disziplinspezifische Anforderungen reagiert und die in Hinblick auf die jeweilige Forschungsfrage wie die verfügbaren Daten optimale Visualisierungslösung automatisch generieren kann. Diese Vision mag zwar in der Tat ""grand"" und unter dem Gesichtspunkt der Implementierbarkeit utopisch anmuten; als konzeptionelle Messlatte für das 3DH Projekt ist sie dennoch richtig. Denn nur Visualisierungslösungen, die den systematischen Zusammenhang zwischen den methodischen Anforderungen eines Forschungsvorhabens und den objektspezifischen Eigenschaften der in diesem Kontext erhobenen und generierten Daten konzeptionell reflektieren, haben zumindest eine theoretische Chance, die von Grinstein verlangten ""Passungen"" automatisch zu ermitteln. Das 3DH-Projekt erforscht den Phänomenbereich ""Datenvisualisierung"" vor diesem Hintergrund unter drei systematischen Aspekten, nämlich (1) einer Typologie hermeneutischer Routinen, Bedingungen und Zielsetzungen des begriffsorientierten (d.h. natürlich- bzw. fachsprachlich artikulierten) Interpretierens von Daten, die in ihrer für den geisteswissenschaftlichen Verstehensprozess kennzeichnenden Ausprägung zu definieren sind; (2) einer Syntax grafischer Strategien, die 'je nach Kontextbedingung und Prozessphase 'die ""bottom up""-definierten Grundlagen für ein erkenntnisproduktives visuelles ""mapping"" der vorgenannten hermeneutischen Operationen auf die jeweils behandelten Primär- und Sekundärdatensets bereitstellen; und (3) einer nach Designprinzipien geordneten Taxonomie konkreter Visualisierungstypen, die als ""top-down""-Determinanten und epistemologische Paradigmen aufgefasst werden können. Die Designprinzipien werden ihrerseits nicht auf die Funktion der bloßen Steuerung visueller Datenrepräsentation am Ende eines geisteswissenschaftlichen Arbeitszyklus reduziert; sie sollen vielmehr als eigenständige, komplementäre Verfahren nicht-sprachlicher, bildgebundener Verstehensoperationen aufgefasst werden. Die Bearbeitung der drei Aspekte soll neben der theoretischen Konzeptentwicklung auch zur Erarbeitung einer visuellen Grammatik für geisteswissenschaftliche Datenvisualisierung führen. Im ersten Schritt haben wir eine Reihe exemplarischer Use Cases der DH-Forschung Tabelle 1: epistemologische Gegensatzpaare Jedes dieser Gegensatzpaare markiert eine Dimension hermeneutischer Praxis, in der datenbasierte Erkenntnisprozesse in der Regel nicht auf normativ geregelte finite Auslegungen von Bedeutung und Wert, sondern auf kontextsensitive, skalierte dynamische Zuschreibungen von Informationsgehalt und Relevanz abzielen. Als epistemologische Matrix bildet diese Tabelle zugleich die Grundlage für die Entwicklung einer ""Grammar of Graphics"" in Anlehnung an Bertin (1983) und Wilkinson (2005). Wie von Satyanarayan et al. (2016) vorgeschlagen, müssen diese Ansätze allerdings um den Aspekt der Interaktivität erweitert werden. Graphische Merkmale sollen entsprechend durch sog. ""Aktivatoren"" visuell modalisierbar werden. Die so erweiterte visuelle Grammatik soll in eine Notation überführt werden, die möglichst allgemein verständlich, generisch und unabhängig von einer bestimmten Programmiersprache implementierbar sein muss; aufgrund der großen Verbreitung von XML in den Digital Humanities ist eine zusätzliche XML-Notation geplant. Daneben sollen für eine Reihe exemplarischer hermeneutischer Verstehens- und Interpretationsprozesse die systematischen Zusammenhängen zwischen Datenstrukturen und geeigneten Visualisierungsprinzipien erforscht und adäquate Vorschläge für eine (oder mehrere) Visualisierungen erarbeitet werden. Die Implementierung der entwickelten Visualisierungen wird eine webbasierte Browser-Anwendung sein, die kollaboratives Arbeiten ermöglicht und über ein Web Service Interface mit anderen Systemen verbunden werden kann. Die Spezifikation der Visualisierungen mit Hilfe einer von einer Grafik-Engine unabhängigen Grammatik erlaubt prinzipiell beliebige Ausgabeformate. Aufgrund der Interaktivität und Webfähigkeit ist zunächst SVG als Format geplant. Auch wenn die weiteren Schritte zur Erarbeitung der visuellen Grammatik und der prototypischen Implementierung geisteswissenschaftlich funktionaler Visualisierungsansätze vorgezeichnet scheinen: Die Frage nach der methodischen Adäquatheit des Vorgehens bleibt für unser Vorhaben weiterhin virulent. So stehen bei den epistemologischen Gegensatzpaaren in Tabelle 1 bislang logische Gegensätze des Typs A und non-A (z.¬†B. Reliablity vs. Unreliability) und phänomenologische Gegensätze (z.¬†B. Probability vs. Factuality) nebeneinander. Noch ist nicht geklärt, ob es sich hier um Kategorienfehler im analytischen Sinne handelt, oder ob nicht gerade dieses Nebeneinanderstehen kategorial unterschiedlicher Konzepte dem hermeneutischen Prozess gerecht wird. Welche Konsequenzen hätte es zum Beispiel für ein geisteswissenschaftliches Visualisierungskonzept, wenn sich strikt logische, binäre Modellierungen hermeneutischer Prozesse sogar als prinzipiell ungeeignet erweisen? Unter diesem kritischen Vorbehalt erscheinen zum einen konkrete, etablierte visuelle Verfahren in einem neuen Licht. Kann zum Beispiel Shneidermans (1996) bekanntes Erst das Nachdenken über die Erfordernisse eines geisteswissenschaftlichen Visualisierungs"
2017,DHd2017,vortrag-PERNE.xml,Aufbau eines historisch-literarischen Metaphernkorpus für das Deutsche,"Stefan Pernes (Universität Würzburg, Deutschland); Lennart Keller (Universität Würzburg, Deutschland); Christoph Peterek (Universität Würzburg, Deutschland)","Metapher, Annotation, Klassifikation","Datenerkennung, Entdeckung, Sammlung, Inhaltsanalyse, Annotieren, Veröffentlichung, Stilistische Analyse, Identifizierung, Daten, Infrastruktur, Software","Metaphorischer Sprachgebrauch umfasst komplexe gedankliche Würfe genauso wie alltägliche Begrifflichkeiten. Die Metapher gilt als Untersuchungsgegenstand nicht nur in den Literaturwissenschaften, Sprachwissenschaften und der Anthropologie, sondern hat auch Relevanz für so disparate Forschungsprogramme wie das der Künstlichen Intelligenz und der Kritischen Diskursanalyse. Darüber hinaus stellt die Erkennung und Auflösung von Metaphern ein wichtiges Desiderat in sprachtechnologischen Anwendungen dar, deren Gegenstand die Disambiguierung von Wortbedeutungen umfasst. Korpusuntersuchungen zeigen, dass metaphorischer Sprachgebrauch in gängigen Textsorten durchschnittlich in jedem dritten Satz zu finden ist (vgl. Steen et al. 2010; Shutova und Teufel 2010) 'ein Beleg für die Ubiquität der Metapher, die in erster Linie darin begründet liegt, dass die idealtypische Karriere eines metaphorischen Ausdrucks als kühne Betonung beginnt und als konventionelle Form endet. Eine Sprachressource zum Metapherngebrauch kann also eine wichtige Ergänzung bei der automatischen inhaltlichen Erschließung von Textbeständen darstellen. Dabei stellt das hier entwickelte Korpus annotierter Sätze, dessen Grundlage eine Sammlung deutschsprachigen Romane aus dem 19. Jahrhunderts bildet, einen spezifischen Beitrag zur Erschließung von historischen Textbeständen dar. Die große Mehrheit der heute verfügbaren Metaphernkorpora basiert auf dem Prinzip, einige wenige Zielbegriffe sowie unter Umständen ausgewählte konzeptuelle Domänen zu definieren und alle passenden sprachlichen Realisierungen aus einem großen Textbestand zu extrahieren. Diese Herangehensweise lässt vermuten, dass die damit modellierten Eigenschaften sich nicht auf arbiträren Text in realen Anwendungsszenarien übertragen lassen, denn jedes vordefinierte lexikalische oder konzeptuelle Inventar wird dabei zu kurz greifen (vgl. Shutova 2015). Im Gegensatz dazu enthält das hier entwickelte Korpus keine Einschränkungen hinsichtlich der konzeptuellen Domänen oder der erfassten sprachlichen Konstruktionen, bis auf die Tatsache, dass es sich aus literarischen Prosatexten zusammensetzt. Es sollte noch darauf hingewiesen werden, dass mit der Hamburg Metaphor Database eine weitere deutschsprachige Ressource zur Metapher existiert, diese jedoch nach wesentlich anderen Gesichtspunkten erstellt wurde und lediglich eine kleine Zahl ausgewählter Beispielsätze enthält. Grundlage für die Erstellung des Korpus bildet die Romansammlung der Digitalen Bibliothek des Projektes TextGrid. Die Sammlung umfasst insgesamt 454 Werke vom frühen 16. bis zum frühen 20. Jahrhundert, wobei der Bedarf nach orthographisch normalisiertem Text die Datengrundlage auf 383 Romane aus den Jahren 1830 bis 1940 eingeschränkt hat. Zur Ziehung der zu annotierenden Sätze wird eine balancierte Sampling-Strategie hinsichtlich zeitlicher Streuung und Gender der AutorInnen eingesetzt. Es handelt sich dabei um eine Quotenstichprobe, die aus jedem 10-Jahres-Abschnitt und zu gleichen Teilen männlicher und weiblicher Autorinnen Sätze auswählt. Darüber hinaus wird im Rahmen des Samplings eine automatische Vorauswahl getroffen, sodass die Hälfte der Sätze Metaphern enthält. Möglich wird dies durch einen Classifier, der anhand von TF-IDF Scores 'auf Grundlage einer lemmatisieren Version des gesamten Romankorpus 'feststellen kann wie ""ungewöhnlich"" ein zu klassifizierender Satz ist. Anhand eines empirisch festgestellten, von der Größe des TF-IDF Korpus abhängigen, Schwellenwerts ist es anschließend möglich, eine Vorauswahl zu treffen, die indirekt Metaphorizität erfasst. Es handelt sich dabei um eine vereinfachte Form des von Schulder & Hovy (2014) entwickelten Klassifikationsansatzes. Ziel des hier entwickelten Korpus ist es, einen Gesamtumfang von bis zu 2000 annotierten Sätzen zu erreichen. Als Grundlage dafür wurden insgesamt 3000 Sätze ausgewählt. Wir orientieren uns an der Metaphor Identification Procedure (MIP) der Pragglejaz Group (Pragglejaz Group 2007; Steen et al. 2010) und sehen zunächst jedes Wort im Text als potentielle Metapher. Gegenstand der Metaphernannotation ist es somit, jedes Wort als metaphorisch beziehungsweise nicht metaphorisch zu klassifizieren. Die Aufgabe ist dabei auf metaphorische Öußerungen auf der Wortebene beschränkt, das heißt Satzmetapher und Textmetapher sowie Phänomene grammatischer Metapher sind ausgenommen. Aufgrund der Neigung des Deutschen zur Kompositabildung wird jedoch eine automatische Kompositazerlegung durchgeführt. Da der Umfang der zu annotierenden Sätze eine Herausforderung für eine solche detallierte Herangehensweise wie das MIP darstellt, wird eine automatische Vorselektion potentieller Metaphernkandidaten durchgeführt. Auf Grundlage von Part-of-Speech-Informationen und Dependency-Bäumen werden aus den Sätzen folgende Konstruktionstypen als Kandidaten für eine metaphorische Verwendung extrahiert (zu den Typen vgl. Skirl und Schwarz-Friesel 2007): Substantivmetapher 'dazu gehören Komposita, Kopulakonstruktionen (""X ist ein Y""), Simile (""X ist wie ein Y""), Genitivmetapher 'sowie Verb-, Adjektiv-, Präpositions- und 'als'-Metapher. Wir folgen mit diesem Ansatz der automatischen Vorauswahl Gandy et al. (2013), die dadurch bei der Metaphernauszeichnung eine Übereinstimmung der Annotatoren von Kappa = 0.80 erreichen. Die extrahierten Konstruktionen werden anschließend zusammen mit den Sätzen für die Annotation in ein geeignetes Format exportiert und in die Annotationsumgebung WebAnno (Yimam et al. 2014) geladen. Für den manuellen und bei weitem umfassendsten Teil der Arbeit wurde ein Annotationsleitfaden verfasst, der die Identifikationsstrategie MIP reproduziert, Hinweise zum Umgang mit lexikalisierten metaphorischen Ausdrücken und der Abgrenzung zur Metonymie enthält. Darüber hinaus ist dargestellt, welche Konstruktionstypen vormarkiert werden und welche Ausnahmen dabei zu erwarten sind (Eigennamen und Hilfsverben sind von der Markierung ausgenommen, vgl. Shutova und Teufel 2010). Schließlich wird festgelegt wie mit Ausnahmen und fehlerhaften Sätzen zu verfahren ist. Satzfragmente, starke dialektale Formen sowie Sätze, die ohne Kontext nicht interpretierbar sind, werden als Ausschuss markiert, fehlerhafte Vormarkierungen werden gekennzeichnet und im Rahmen der Kuration der annotierten Sätze verbessert. Es kann von den folgenden vorläufigen Ergebnissen der automatischen Vorauswahl und der manuellen Annotation berichtet werden: Die automatische Extraktion der möglicher Metaphernkandidaten hat es ermöglicht, ein korpusgestütztes Bild darüber zu erlangen wie die Konstruktionen in einer relativ offenen Domäne 'einem Romankorpus, das diverse Gattungen umfasst 'verteilt sind. Des Weiteren ist ausgehend von der Annotationspraxis festzustellen, dass die erhobenen Konstruktionen 'zumindest im Rahmen der hier zugrundeliegenden Texte 'theoretisch alle Vorkommen von metaphorischen Öußerungen auf der Wortebene abdecken. In der Praxis kommt es jedoch aufgrund komplexer Hypotaxen und fehlender automatischer Koreferenz-Auflösung zu fehlerhaften Vormarkierungen. Die vorbereitende, automatische Klassifikation der Sätze in Metapher beziehungsweise Nicht-Metapher führt zu einem Anteil von 48% an Sätzen, die lebendige Metaphern enthalten. Werden lexikalisierte metaphorische Ausdrücke mit eingerechnet, steigt der Anteil der Sätze, die Metaphern enthalten, auf 61%. Ein erheblicher Vorteil, der sich aus der Klassifikation der Sätze ergibt, ist die Fülle des Materials, die sich dadurch generieren lässt. Ohne Vorauswahl liegt die durchschnittliche Anzahl von metaphorischen Ausdrücken pro Satz 'je nach Textsorte 'zwischen 0.12 und 0.54 (vgl. Shutova & Teufel 2010), während mit dem hier vorgestellten Ansatz ein Wert von 1.91 Metaphern pro Satz erreicht wird. Eine genaue Auswertung der Präzision des Classifiers steht noch aus, in Bezug auf die Struktur der so ausgewählten Sätze kann jedoch festgestellt werden, dass die Klassifizierung keine Auswirkung auf die Verteilung der erhobenen Konstruktionstypen hat. Für die Übereinstimmung zwischen zwei Annotatoren beim Aufbau des hier vorgestellten Metaphernkorpus kann ein Wert von 0.87 (Cohen‚Äôs Kappa) berichtet werden. Werden lediglich die vormarkierten Konstruktionen als Grundlage der Berechnung herangezogen, schwankt Kappa je nach Einbezug lexikalisierter Öußerungen zwischen 0.77 bis 0.80."
2017,DHd2017,vortrag-BARTH.xml,Digitale Modellierung literarischen Raums,"Florian Barth (Universität Stuttgart, Deutschland); Gabriel Viehhauser (Universität Stuttgart, Deutschland)","Raum, Narratologie, Digitale Textanalyse, Literaturkartographie","Datenerkennung, Entdeckung, Inhaltsanalyse, Strukturanalyse, Räumliche Analyse, Modellierung, Annotieren, Theoretisierung, Bereinigung, Netzwerkanalyse, Stilistische Analyse, Visualisierung, Daten, Sprache, Literatur, Karte, Methoden, benannte Entitäten (named entities), Forschung, Forschungsprozess, Forschungsergebnis, Text, Visualisierung","Problemstellung Im Anschluss an den 1990 durch den Humangeographen Edward Soja ausgerufenen ""Spatial Turn"" (Soja 1990) haben sich zahlreiche kulturwissenschaftliche Forschungsarbeiten mit einer Beschreibung des Raums beschäftigt. In der Literaturwissenschaft fanden dabei u.a. kartografische Darstellungen große Resonanz: Franco Moretti etwa untersuchte in seinem ""Atlas of the European Novel"" Orte der literarischen Produktion und Rezeption (Moretti 1998), Barbara Piattis Studie ""Die Geographie der Literatur"" richtete den Fokus auf die Illustration einer konkreten literarisch thematisierten Gegend (die Zentralschweiz, vgl. Piatti 2008). Besondere Aufmerksamkeit wurde literarischen Karten auch im Kontext der Digital Humanities zu Teil, in denen geografische Informationssysteme (GIS) zum Einsatz kommen (typische Workflows beschreiben Gregory et. al. 2015) Den meisten dieser Ansätze ist dabei gemein, dass sie für ihre Datengrundlage in erster Linie auf konkrete Nennungen von Ortsnamen (Toponymen) rekurrieren und weitere Ortsmarker weniger stark berücksichtigen. An der Konstitution literarischer Räume sind jedoch in der Regel auch komplexere Faktoren beteiligt, zu deren Beschreibung bereits erste narratologische Ansätze vorliegen (etwa von Kathrin Dennerlein [2009 und 2011] oder Gabriel Zoran [1984], vgl. auch die Überlegungen bei Piatti [2008]), die jedoch im Kontext der Digital Humanities bislang noch zu wenig Beachtung gefunden haben. In unserem Beitrag möchten wir diese Ansätze aufgreifen, um das Instrumentarium der digitalen Textanalyse hinsichtlich der Kategorie des Raums zu schärfen und zu erweitern. Dazu scheinen uns insbesondere zwei Aspekte von Bedeutung: Zum einem die Unterscheidung von Raummarkierungen hinsichtlich ihrer Handlungsrelevanz (I), zum anderen die Ausweitung der Analyse auf räumliche Begriffe, die über bloße Namensnennungen hinausgehen (II). Für beide Problemfelder präsentieren wir erste Verfahren zur automatischen Auswertung und geben Ausblicke auf die Möglichkeiten einer vergleichenden Analyse.  Im Anschluss an Dennerleins Narratologie des Raumes hat sich insbesondere der Terminus der Am Beispiel von Jules Vernes ""Reise um die Erde in 80 Tagen"" lässt sich die Wichtigkeit dieser Unterscheidung aufzeigen: So bietet z.B. Kapitel 14 eine Zugfahrt durch das Gangestal mit Aufenthalten in Allahabad und Benares sowie der Ankunft in Calcutta. Genannt werden im Text jedoch auch weitere Städte Indiens und das nächste Ziel Hongkong; eine Vielzahl der extrahierten Ortsnamen bezieht sich somit nicht auf den Handlungsort des Kapitels. Erst die kategoriale Trennung der Raummarkierungen in Ereignisregionen und erwähnte räumlichen Gegebenheiten ermöglicht die valide Rekonstruktion einer Reiseroute, die dann in einer GIS-Darstellung visualisiert werden kann (Abbildung 1). Eine automatische Unterscheidung dieser Kategorien muss daher das Fernziel computerunterstützter Raumuntersuchungen sein. Ansätze zu einer solchen Differenzierung suchen wir in der Anwendung von computerlinguistischen Methoden der Relationsextraktion, bei denen sich aus strukturellen Auffälligkeiten Regeln zur Klassifikation von Ereignisregionen und erwähnten räumlichen Gegebenheiten ableiten lassen. Hierzu zwei Beispiele: 1. Das 14. Kapitel der ""Reise um die Erde in 80 Tagen"" beginnt mit dem in Abbildung 2 dargestellten Satz: Vor allem der Hauptsatz mit dem Pfad [Figur 'SUBJ 'Bewegungsverb - OBJ 'Raumnomen] zwischen ""Phileas Fogg"" und ""Gangesthal"" kennzeichnet letzteres als Ereignisregion. Dabei stellt insbesondere die Verbkategorie ein Indiz für die Klassifikationsentscheidung dar: Statische Verben (""stehen"", ""sitzen"") oder Verben der Bewegung (""gehen"", ""fahren"") zeugen in spezifischen Satzstrukturen häufig von einer Ereignishaftigkeit im Gegensatz zu Verben der Kognition (""denken""). Zur Einteilung der Verben nutzen wir das von der Universität Tübingen entwickelte lexikalisch-semantische Netz 2. Findet ein erzähltes Ereignis innerhalb einer Bewegung im Raum statt, können mehrere Räume zu einem ""Ich fuhr [...] Im zweiten Satz findet sich zudem eine Referenz auf das Antezedens ""Gründerheim"". Eigentlich werden derartige deiktische Adverbialausdrücke (""hier"", ""da"" und ""dort"") bei der Koreferenz-Resolution nicht berücksichtigt. Deshalb planen wir eine Erweiterung der Trainingssets bestehender Koreferenz-Systeme hinsichtlich dieser Termini. Die hier an zwei Beispielfällen dargestellten Regeln zur Unterscheidung narratologischer Raumeinheiten sollen in Zukunft kontinuierlich erweitert und zunächst so gestaltet werden, dass sie eine hohe Präzision erzielen. Anschließend können sie als Features für spätere maschinelle Lernverfahren verwendet werden.  Netzwerkvisualisierung von Raumnomen Eine kartografische Darstellung, wie sie in Abbildung 1 ersichtlich ist, bleibt auf realweltliche Ortsnamen beschränkt und lässt wesentliche Aspekte der Raumdarstellung außer Acht. Einen ersten Ansatz, die Vielfältigkeit der tatsächlichen Handlungsräume und ihrer Zusammenhänge abzubilden, bietet das Netzwerk in Abbildung 3. Hier werden für das angesprochene Kapitel 14 neben den Toponymen auch unspezifische Raumnomen als Knoten berücksichtigt und Verbindungen immer dann etabliert, wenn zwei Raumbegriffe gemeinsam in einem Satz auftreten. Insbesondere landschaftliche (grün) und architektonische Raumnomen (grau) stellen relevante Klassen von Raummarkern dar, die neben den konkreten Ortsnamen zentrale Komponenten der literarischen Raumbeschreibung bilden. Als räumliche Gegebenheiten kommen aber auch bewegliche Objekte, in denen sich Figuren aufhalten können, in Frage, wie die in der Grafik blau markierten Fahrzeuge. Abbildung 3: Netzwerk Lexikon und Taxonomien für Raumbegriffe Zur lexikalischen Erfassung von realweltlichen Toponymen greifen wir auf die Named-Entitiy-Recognition von Innerhalb dieses Lexikons planen wir zudem eine Einordung der Raumbegriffe in spezifische Taxonomien:  Narratologisch wird unter dem Begriff Statt einer festen Zuschreibung nähern wir uns dem Verhältnis von Ort und Raum über eine vertikale Taxonomie von räumlichen Gegebenheiten an, die von der Planetenebene bis zu jenen Objekten reicht, in denen sich unter Annahme faktualer Gesetzmäßigkeiten keine Figur mehr aufhalten kann. Im Sinne des Abbildung 4 zeigt basierend auf den kapitelweise extrahierten Ereignisregionen in ""Reise um die Erde in 80 Tagen"" die obersten taxonomischen Stufen: 1. Kontinent, 2. Land, 3. Stadt bzw. landschaftliche Region (inklusive Transportmittel, markiert in blau). Während in den ersten beiden Ebenen dieses Beispiels ausschließlich Toponyme vorkommen, beinhaltet zumindest die dritte Stufe in der Nominalphrase ""Latenenwald bei Allahabad"" ein unspezifisches Raumnomen. Weitere allgemeine Begriffe wären vor allem auf einer hier nicht dargestellten vierten Ebene zu finden (z.B. ""Sumpf"", ""Bach"", ""Weizenfeld"" innerhalb der Landschaft ""Behar"", vgl. Abb. 3). Zur automatischen Erstellung einer solchen Hierarchie bieten sich bei Toponymen die in  Wie in Abb. 3 ersichtlich, speisen sich Raumnomen zu großen Teilen aus den Wortfeldern Architektur und Landschaft. Die folgende Analyse basiert auf semiautomatisch erstellten Wortlisten, die auf der Basis von Makroperspektive Das Potential digitaler korpusgestützter Raum-Analysen soll anhand des Vergleichs dreier ""Berlin-Romane"" exemplarisch aufgezeigt werden. Dazu werden die Texte in jeweils zehn Segmente aufgeteilt und hinsichtlich der Frequenz spezifischer Raumbegriffe aus den Wortfeldern Architektur und Landschaft untersucht: Dabei lassen sich deutlich höhere Anteile des architektonischen Vokabulars gegenüber dem Segmentmittelwerten eines Vergleichskorpus erkennen, das aus 451 im Textgrid-Repository enthaltenen Romanen besteht (Abbildung 5, oben). Während die Verteilung des architektonischen Wortschatzes in Hesses ""Heimliches Berlin"" nur temporäre Spitzen zeigt, sind die Segmentverteilungen von ""Berlin Alexanderplatz"" und Wilhelm Raabes ""Die Chronik der Sperlingsgasse"" gegenüber der mittleren Verteilung des Korpus signifikant verschieden. Dies wurde sowohl mit dem Wilcoxon-Rangsummentest (Annahme der Varianzhomogenität und Gleichverteilung zwischen den Sampleverteilungen) sowie dem Mood's Median-Test (keine Verteilungsannahme) überprüft (Abbildung 6). Das landschaftliche Vokabular hingegen liegt bei den Berlin-Romanen tendenziell etwas unter dem Mittel des Korpus, allerdings sind die Abweichungen nur im Fall von ""Berlin Alexanderplatz"" eindeutig signifikant (Abb. 5 unten, Abb. 6) Ungeachtet dieser Unterschiede sind die Zusammenhänge zwischen beiden Wortfeldern auffällig (Abbildung 7). Die Spearman-Korrelation zwischen architektonischen und landschaftlichen Begriffen bei ""Berlin Alexanderplatz"" beträgt 0.5030488 und bei ""Die Chronik der Sperlingsgasse"" sogar 0.7454545. So kann trotz abweichender Anteile der Wortfelder hinsichtlich ihrer Frequenz eine starke Verflechtung spezifischer Klassen von Räumen angenommen werden. Ausblick Die vorgestellten Ansätze verstehen sich als Anregung für die Entwicklung eines differenzierten Instrumentariums der digitalen Raumanalyse, das in Zukunft weiter ausgebaut werden soll und die Grundlage für die Behandlung weiterführender literaturwissenschaftlicher Fragestellungen bildet, die etwa Aspekte der Semantisierung von Räumen (Lotman 1972), des raumzeitlichen Entwurfs von Erzählwelten (Bachtin 1989) und der Bedeutung von Raumkonstellationen für die Gattungspoetik beinhalten (vgl. zusammenfassend Nünning 2009)."
2018,DHd2018,PIELSTRÖM_Steffen_LDA_Topic_Modeling_über_ein_graphisches_In.xml,LDA Topic Modeling über ein graphisches Interface,"Severin Simmler (Universität Würzburg, Deutschland); Thorsten Vitt (Universität Würzburg, Deutschland); Steffen Pielström (Universität Würzburg, Deutschland)","Topic Modeling, Python","Inhaltsanalyse, Modellierung, Visualisierung"," In den letzten Jahren ist das Interesse an LDA als Verfahren für die Analyse literarischer Textcorpora auf Seiten der digitalen Geisteswissenschaften stark gestiegen. Im Kontrast zu diesem gesteigerten Interesse ist die Anwendung der Methode allerdings nicht wesentlich leichter geworden. Gängige Implementierungen des LDA-Algorithmus werden entweder über ein kommandozeilenbasiertes Java-Programm (MALLET von McCallum 2002) oder über Skripte in der Programmiersprache Python (Gensim von Rehurek und Sojka 2010) angesprochen. Die Aufbereitung der Daten vor dem Topic Modeling, das sog. ""Preprocessing"" und die Analyse der Ergebnisse hinterher geschieht dann zumindest in Teilen häufig unter Verwendung weiterer Programme bzw. Arbeitsumgebungen. Alles in allem erfordert die Durchführung einer LDA-basierten Inhaltsanalyse damit zur Zeit relativ umfangreiche technische Kenntnisse. Um den Zugang zu dieser Methode zu erleichtern entwickeln wir im Rahmen von DARIAH-DE (https://de.dariah.eu/) zur Zeit eine ausführlich dokumentierte Python-Programmbibliothek, die es ermöglichen soll, den gesamten Arbeitsprozess einer LDA-basierten Analyse in einer einzigen Umgebung durchzuführen ( Um einen leichtgewichtigen Einstieg in diese Thematik zu bieten haben wir auf Basis unserer Programmbibliothek, der Python-nativen LDA-Implementierung von Allan Riddell (https://pypi.python.org/pypi/lda) und dem Python-Microframework ""Flask"" ( Der GUI-Demonstrator übernimmt und erklärt hierbei exemplarisch alle Arbeitsschritte einer einfachen Analyse. Zunächst werden Textdateien über ein Auswahlmenü eingelesen und tokenisiert. Nutzerinnen und Nutzer können zur Reduktion des Vokabulars auf die Funktionswörter vorgeben, wie viele der häufigsten Wörter aus den Texten entfernt werden sollen, oder alternativ über ein weiteres Auswahlmenü eine externe Stopwortliste einbinden. Die Anzahl der zu berechnenden Topics und die Zahl der Iterationen, über die die Berechnung durchgeführt werden soll, ein Faktor, der die Qualität der Ergebnisse entscheidend beeinflusst, können ebenfalls über das Interface gesteuert werden. In der derzeitigen Form generiert das Programm als Output eine Tabelle mit den zehn am stärksten gewichteten Wörtern in jedem Topic, sowie ein Heatmap als Übersicht über die Verteilung der Topics über die Texte. Im Fokus der gegenwärtigen Weiterentwicklung steht die Gestaltung interaktiver Outputs mit Hilfe von Bokeh ( Das Ziel dieser Entwicklung bleibt aber in erster Linie ein didaktisches: Der GUI-Demonstrator führt die ¬†grundsätzlichen Möglichkeiten der Methode vor und informiert gleichzeitig über die Abläufe im Hintergrund, so dass der Schritt hin zur Verwendung der gleichen Funktionalitäten in einem vorbereiteten Notebook mit interaktiven Codeblöcken, das schnell an die spezifischen Bedürfnisse eine bestimmten Forschungsfrage angepasst werden kann, nur noch klein ist. "
2018,DHd2018,FISCHER_Frank_Netzwerkanalytischer_Blick_auf_die_Dramen_Anto.xml,Netzwerkanalytischer Blick auf die Dramen Anton Tschechows,"Veronika Faynberg (Higher School of Economics, Moskau, Russland); Frank Fischer (Higher School of Economics, Moskau, Russland); Svetlana Lashchuk (Higher School of Economics, Moskau, Russland); Tatyana Orlova (Higher School of Economics, Moskau, Russland); German Palchikov (Higher School of Economics, Moskau, Russland); Evgenia Shlosman (Higher School of Economics, Moskau, Russland)","Netzwerkanalyse, Anton Tschechow, Russische Literatur, Drama","Netzwerkanalyse, Visualisierung, Literatur"," In diesem Kontext siedelt sich auch unser Posterprojekt an, in dessen Mittelpunkt die extrahierten Netzwerkdaten zu den Stücken des russischen Dramatikers Anton Tschechow (1860–1904) stehen. Die Datengrundlage bildet das von uns aufgebaute und betriebene Russian Drama Corpus (RusDraCor), das es sich zur Aufgabe gestellt hat, russischsprachige Stücke in der Zeitspanne zwischen den 1740er-Jahren (Sumarokow, Lomonossow u.¬†a.) und den 1930er-Jahren (mit Texten von Autoren wie Majakowski oder Gorki) im TEI-Format zur Verfügung zu stellen (Fischer u. a. 2017). Neben Large-Scale-Analysen zur strukturellen Evolution des russischen Dramas ergibt sich so auch die Möglichkeit zur Betrachtung von nach verschiedenen Kriterien portionierten Teilkorpora, etwa der Stücke einzelner Autoren. Anton Tschechow gehört zu den meistgespielten russischen Dramatikern, dessen Werke bis heute inszeniert werden, gerade auch an deutschsprachigen Bühnen, vor allem seine vier letzten Stücke, ""Die Möwe"", ""Onkel Wanja"", ""Drei Schwestern"" und ""Der Kirschgarten"". Von der Figurenkonstellation her haben diese Werke einen hohen Wiedererkennungswert: Es gibt keine wirklichen Protagonisten; die Redeanteile und Gesprächssituationen sind relativ gleichmäßig über eine Gruppe von Figuren verteilt. Dies zeigt sich sofort auch in den Netzwerkgraphen: Die Knoten (von denen jeder für eine Figur des jeweiligen Dramas steht) bilden einen Die Beschaffenheit des Russian Drama Corpus erlaubt es, quantitative Analysen auch zugeschnitten auf bestimmte Figurengruppen zu beschränken, etwa gesondert nach Geschlecht oder sozialem Status. Bereits eine simple Worthäufigkeitsanalyse kann so etwa zeigen, dass weibliche und männliche Rollen in Tschechow-Stücken von den Redeanteilen und dem Vernetzungsgrad her vergleichbar sind (anders als etwa bei allen anderen Autoren im Korpus). Diese Verteilungsdiagramme sowie netzwerktheoretische Werte wie Dichte, Diameter, Clustering-Koeffizient und Average Path Length ergänzen die chronologisch sortierten Netzwerkvisualisierungen. Die im Poster geschaffene Übersicht über alle Tschechow-Dramen hat auch enzyklopädischen Charakter, enthält sie doch etwa alle Figuren im Kontext ihres Auftretens im Tschechow""schen Dramenkosmos. Der netzwerkanalytische Blick ist somit durchaus geeignet, als Brücke zur inhaltlichen Auseinandersetzung mit den Werken Tschechows zu dienen."
2018,DHd2018,KONLE_Leonard_Analysing_Direct_Speech_in_German_Novels.xml,Analysing Direct Speech in German Novels,"Fotis Jannidis (Universität Würzburg, Deutschland); Leonard Konle (Universität Würzburg, Deutschland); Albin Zehe (Universität Würzburg, Deutschland); Andreas Hotho (Universität Würzburg, Deutschland); Markus Krug (Universität Würzburg, Deutschland)","Direct Speech, Deep Learning, Machine Learning","Datenerkennung, Programmierung, Stilistische Analyse, Text"," This paper aims to provide a detailed analysis of the use of direct speech across different time periods and domains. To create a reliable database for these analyses, we need to measure the usage of direct speech in a large and representative corpus. This task is more challenging than it may sound: While, nowadays, direct speech is often marked very explicitly by the use of quotes, this has not always been consistently the case. Many historical novels are not available in a well-edited form, meaning that there may be inconsistent use of quotation, or no quotation at all (Brunner, 2013). In this case, a more robust method for detecting direct speech is necessary. Our first contribution is therefore a deep learning-based method to detect direct speech using large amounts of rule-based, but slightly flawed, labelled data extracted from raw text. ¬†This has multiple advantages over the use of manually annotated training data: First, manually annotating large amounts of text is very time-intensive and therefore costly. Furthermore, annotations for one type of texts may not be transferable to other types, leading to the necessity of new annotated data for new corpora. Being able to learn from the already existing weakly labelled data is therefore desirable, as this data can automatically be extracted for a new corpus. Our second contribution is the application of this approach on curated texts to gain insight in trends of direct speech distribution. On one hand we try to look for development of direct speech over time, analysing a large dataset of novels from the nineteenth century, on the other hand we focus on differences in genre comparing contemporary high and low brow literature.  For example, Brunner (2013) tests rule-based and machine learning driven classification, as well as combinations of both, on German novels. She recommends using a pure machine learning approach (Random Forest), reaching an F1 score of 0.87. Scheible et al. (2016) employ a simple greedy algorithm and a semi-Markov model, showing that the latter outperforms the previous state-of-the-art by achieving a precision of 0.88. Although the results seem quite satisfying, these systems require a relatively large amount of labelled data for training. As stated above, this is problematic because of the need for expensive annotation and lack of transferability to other domains. Thus, our goal in this paper differs from that in previous work. We do not aim to set a new state-of-the-art in direct speech detection, but instead: a) present a method that can leverage large amounts of weakly labelled data extracted from raw text, and b) use this model for the analysis of different distributions of direct speech across genres or time-periods. To the best of our knowledge, the second task has never been done on a large collection of texts.  In order to train and evaluate our classifiers, we need to obtain labels specifying which parts of the texts contain direct speeches. To this end, we chose two strategies: For training our classifiers, we decided to extract weak labels using a simple rule based on quotation, implying everything written between quotation marks is direct speech. To yield high accuracy for this approach, it is necessary to use a well-edited collection of texts. Our PD corpus contains such a subset, which we refer to as our Using our quotation rule on the For further evaluation, we chose to annotate a smaller subset of the corpus  We conducted experiments on two different levels, starting with a sentence classification task, which is then refined to detect direct speech on word-level. In our first classification task, documents are split into sentences and vectorised by storing each sentence in a bag-of-words representation. To create a baseline for measuring the advantage using deep learning for direct speech recognition, we compared the performance of traditional machine learning algorithms on our labelled datasets. Training and testing some of the most common machine learning classifiers to detect sentences containing at least one word of direct speech leads to an accuracy of Since we noticed that three of our classifiers all ended up with about the same score, we decided to give the task to two human annotators to establish an upper bound. We selected 250 sentences for manual annotation and again removed all quotation marks. Both annotators ended up with an accuracy comparable to that of the best machine learning methods, 84% and 82.8% respectively. From this result we concluded that it is not expedient to further optimise the sentence classification task, as we had already reached human-level accuracy. Because of the results from the previous section, we decided to modify our task to a word-level prediction, which enables us to include more context by ignoring sentence boundaries and at the same time make more fine-grained predictions. In this second classification task, each word is to be classified separately as inside or outside a direct speech. As baseline for this task, we trained a Linear Chain Conditional Random Field (CRF) that was only given the word itself and its part-of-speech tag. This CRF stagnated at a comparably low accuracy of 0 Since our goal was to provide the classifier with more context, we chose to use an architecture based on recurrent neural networks, which are able to deal with relatively large contexts. Our assumption here is that, for a good classification, we need context from both before and after the target word itself, as markers for direct speech can be found at the beginning or the end of the direct speech. We thus designed a two-branch network, visualised in Figure 1. This network receives as input a text-segment, specifically the target word in its context. The words of the input are then passed through an embedding layer and split into two parts, where the first part contains the context up to the target word and the second part contains the context following the target word. The target word itself is contained in both parts. Each part is passed through three separate LSTM-layers. In the future-branch, the context is passed through the layers in reverse, so that the target word is the last word to be read in both branches. The LSTM-layers in the past-branch are stateful and can therefore theoretically retain the entire context of the novel up to the target word. The outputs of the final LSTM-layer of both branches are concatenated. The final prediction is made based on this concatenation by a fully connected layer. In our best setup, we used 60 words before and after the target word as context. Training on one half of the   This finding is contrary to the assumption mentioned above. We propose that, while there is no clear difference in the average use of direct speech between high and low brow literature, authors in high brow literature are far more flexible in choosing how much direct speech they use in their novels. Low brow literature, on the other hand, is expected to have a rather constant amount of dialogue.  While an accuracy of 0.9 is remarkable, there is still need for optimisation. Recent developments in the performance of neural networks by adding an attention mechanism (see Rush 2015) could improve the results. We used our neural network to analyse the distribution of direct speech over time and genres. Besides algorithmic refinements, there is a lot of potential in adding more text to our corpus and refining metadata to allow more sophisticated research questions like differences between or development of direct speech in certain genres."
2018,DHd2018,LEBHERZ_Daniel_Text_Mining_und_Computersimulation_zur_Analys.xml,Text Mining und Computersimulation zur Analyse autobiographischer Texte: Einflüsse auf das literarische Schaffen Klaus Manns,Jan Hess (Trier Center for Digital Humanities (TCDH); Universität Trier); Daniel Lebherz (Center for Informatics Research and Technology (CIRT); Universität Trier); Christian Zeyen (Center for Informatics Research and Technology (CIRT); Universität Trier),"Klaus Mann, autobiographische Texte, Text Mining, Workflows, agentenbasierte Sozialsimulation","Inhaltsanalyse, Strukturanalyse, Beziehungsanalyse, Modellierung, Annotieren, Literatur","""Ist das alles?"" (Mann 1995, S. 151) 'Die vorwurfsvoll anmutende (rhetorische) Frage, die Klaus Mann Ende Juni 1933 unter einer Auflistung seiner Werke des zurückliegenden Halbjahrs in seinem Tagebuch notiert, gibt einen Hinweis darauf, wie selbstkritisch sich der Schriftsteller mit dem eigenen Schaffen auseinandersetzt. Angesichts der persönlichen und politischen Umstände sowie der Vielzahl an Kontakten und Aktivitäten, die er in seinem Journal verzeichnet, erscheint es fast etwas überraschend, dass die gut drei Monate nach seiner Emigration erstellte Werkliste immerhin 21 'wenn auch größtenteils kürzere 'Texte umfasst. In Anbetracht seiner ständigen Rastlosigkeit stellt sich nicht nur die Frage, wann Klaus Mann überhaupt seiner eigentlichen schriftstellerischen Arbeit nachgeht, sondern auch, welche Faktoren zum Ge- oder Misslingen seines Schaffens beitragen. Wie wirken sich beispielsweise die beinahe allabendlichen Theater-, Kino- und Barbesuche oder die mehr oder weniger regelmäßig eingenommenen Rauschmittel auf seine literarische Produktivität aus? Welche Rolle spielt das tägliche Lektürepensum? Haben die zahlreichen Treffen, Telefonate und Korrespondenzen Einfluss auf seine schriftstellerische Arbeit? Oder erweisen sich die politischen Umstände, Angst, Verzweiflung und Hass gegenüber dem Nationalsozialismus als entscheidendes Vehikel literarischer Produktivität? Aufgrund des Umfangs und der Detailgenauigkeit seiner alltäglichen Schilderungen von Gedanken und Aktivitäten 'insbesondere auch der teilweise bis auf die Tageszeit genauen Protokollierung seiner Arbeitsprozesse 'erscheinen Klaus Manns Tagebücher als idealer Untersuchungsgegenstand zur Beantwortung der aufgeworfenen Fragen. Gerade die Vielzahl und Komplexität der darin enthaltenen Informationen machen es jedoch schwierig, sich potentiellen Faktoren literarischer Produktivität tiefergehend analytisch zu nähern. Um diese große Anzahl an Informationen und deren Zusammenhänge besser bzw. überhaupt untersuchen zu können, sollen im Rahmen von Methoden der Computersimulation haben sich bereits seit längerer Zeit in verschiedenen¬†Wissenschaftsdisziplinen etabliert. Seit den 1990er Jahren halten diese auch vermehrt Einzug in sozialwissenschaftliche Forschungsbereiche und bieten dort verschiedene Möglichkeiten zur Darstellung und Analyse von gesellschaftlichen Systemzusammenhängen (Gilbert 2007). Eine ähnliche Entwicklung lässt sich auch in den Digital Humanities nachvollziehen, wenngleich diese Methoden 'insbesondere im literaturwissenschaftlichen Bereich 'eine eher untergeordnete Rolle spielen (Kohle 2017). Die jedoch insgesamt zunehmende Anwendung von Computersimulation als Forschungsmethode liegt darin begründet, dass sie die Möglichkeit bietet, komplexe, unzugängliche Systeme zu untersuchen und experimentell zu analysieren. In solchen Experimenten lassen sich u.a. durch die Betrachtung verschiedener Szenarien Resultate erzielen, die Rückschlüsse auf Plausibilitäten von Handlungen, Strukturen und Zusammenhängen im zugrundeliegenden System erlauben. Agentenbasierte Computersimulation bietet darüber hinaus insbesondere bei der Analyse von individuellem Entscheidungsverhalten die Möglichkeit, sogenannte emergente Effekte aufzudecken. Diese ergeben sich aus dem Zusammenspiel individueller Handlungen und Interaktionen von einzelnen Akteuren, lassen sich jedoch nicht ausschließlich durch diese erklären (Bonabeau 2002). Ein direkter Zusammenhang zwischen Systeminput und -output muss bei agentenbasierter Simulation also nicht bestehen. Dies steht im Gegensatz zu anderen statistischen Analyseverfahren, bei denen i.d.R. eine (lineare) Abhängigkeit zwischen verschiedenen Objekten vorausgesetzt wird. Ferner ermöglicht agentenbasierte Computersimulation detaillierte Analysen von Komponenten der Makroebene. Der Weg zu verlässlichen Ergebnissen einer Simulationsstudie führt in einem ersten, wesentlichen Schritt über die Modellbildung und Sammlung dafür relevanter Daten (Law 2015). Zu diesem Zweck soll das zugrundeliegende System, also konkret Klaus Mann, seine sozialen Kontakte und sein Umfeld möglichst realitätsnah mit allen wesentlichen Eigenschaften, Aktivitäten und Zusammenhängen in einer agentenbasierten Sozialsimulation (Davidsson 2002) abgebildet werden (Abb. 2). Die dafür notwendige Generierung einer geeigneten Datengrundlage ist ein anspruchsvoller Prozess, in dem anhand von Methoden aus dem Bereich des Text Mining zunächst die wesentlichen, auf den Untersuchungsgegenstand einwirkenden Faktoren identifiziert und analysiert werden müssen. Konkret sollen dabei u.a. Methoden wie Named Entity Recognition, Worthäufigkeits- oder Kookkurrenzanalysen angewendet werden, um vorab Kontakte, Aktivitäten, Aufenthalte, Tätigkeiten, persönliche oder politische Ereignisse zu identifizieren, welche mit Klaus Manns literarischer Produktivität in Zusammenhang stehen könnten. Anders als im Falle der Computersimulation kommen Text Mining-Methoden, -Werkzeuge und Programmbibliotheken im literaturwissenschaftlichen Kontext bereits ungleich häufiger zum Einsatz. Zur Komplexitätsreduktion werden die zugrundeliegenden Text Mining-Prozesse dabei jedoch oft als Black Box betrachtet, was den Nachteil birgt, dass das Zustandekommen der erzielten Ergebnisse nur schwer nachvollzogen werden kann. Um dem entgegenzuwirken sollen die im Rahmen von Im Kontext von"
2018,DHd2018,BURGHARDT_Manuel_Digital_Dylan___Computergestützte_Analyse_d.xml,Digital Dylan 'Computergestützte Analyse der Liedtexte von Bob Dylan (1962 '2016),"Colin Sippl (Lehrstuhl für Medieninformatik, Universität Regensburg); Florian Fuchs (Lehrstuhl für Medieninformatik, Universität Regensburg); Manuel Burghardt (Lehrstuhl für Medieninformatik, Universität Regensburg)","Liedtexte, Korpusvergleich, Frequenzanalyse, Signifikanztest","Entdeckung, Inhaltsanalyse, Webentwicklung, Text, Visualisierung","Am 13. Oktober 2016 gab die Schwedische Akademie bekannt, dass sie den Nobelpreis in Literatur an Bob Dylan ""für seine poetischen Neuschöpfungen in der großen amerikanischen Songtradition"" verleihen werde. Die (welt-)politischen Entwicklungen, die das Schaffen Dylans inspirierten, sind im Kontext seines Wirkens umfassend diskutiert worden, unter anderem in ""Bob Dylan und die sechziger Jahre: Aufbruch und Abkehr"", Dieser Beitrag erprobt, inwiefern mithilfe digitaler Methoden im Sinne des Bereits vor der Auszeichnung Dylans mit dem Nobelpreis in Literatur, waren seine Texte Gegenstand wissenschaftlicher Betrachtungen im Sinne des Eine umfassende Untersuchung Dylans Werks mithilfe computerbasierter Methoden fand sich bis zum Abfassungszeitpunkt des vorliegenden Texts nicht. Allerdings sind quantitative Verfahren zur stilistischen und inhaltlichen Analyse von Liedtexten in den Digital Humanities durchaus verbreitet. So beschreiben etwa, Den analytischen Bezugsrahmen dieser Studie stellt die phasenweise Einteilung von Dylans Schaffen nach Brown (2014) In dieser Arbeit wurde ein Korpus bestehend aus 452 Liedtexten mit einem Umfang von 133.045 Tokens untersucht, die Bob Dylan zwischen den Jahren 1962 und 2016 auf Studio-Alben veröffentlicht hat. Die Liedtexte und Metainformationen wie etwa Titel, Album und Jahr stammen von der Plattform Das Korpus wurde weiterhin mit Methoden der Computerlinguistik aufbereitet, insbesondere unter Verwendung des Ein etabliertes Verfahren, um aus einem Korpus spezifische Wörter zu extrahieren, ist ein direkter Korpusvergleich mit dem Als Referenzkorpus dient das mündliche Subkorpus des Beim Korpusvergleich kann entweder das gesamte Dylan-Korpus mit dem Referenzkorpus verglichen werden, oder mit den jeweiligen Dylan-Subkorpora, also bspw. all seinen Texten aus den 1970er-Jahren oder aus der ersten Schaffensperiode ""Becoming Bob Dylan"" (1960-1964). Ein Vergleich der einzelnen Dylan-Subkorpora zum Gesamtwerk ist ebenso möglich. Letztere Option wird z.B. genutzt, um anhand jeweils signifikanter Wörter die einzelnen Schaffensperioden nach Brown (2014) zu überprüfen und damit die grundsätzliche Eignung solch quantitativer Verfahren zur Identifikation thematischer Verschiebungen zu untersuchen. Die Ergebnisse dieses Korpusvergleichs sind, zusammen mit allen anderen Ergebnissen der angewandten Analyseverfahren, in einer interaktiven Webanwendung über unterschiedliche Visualisierungen (Balkendiagramm, Im direkten Vergleich des gesamten Dylan-Korpus (1962-2016) mit dem OANC-Referenzkorpus treten einige interessante, signifikant-häufige Wörter im Werk Dylans hervor. Die von Bob Dylan verwendeten Adjektive erzeugen in der Gesamtschau tendenziell eher eine bedrückende Stimmung ( Die Analyse signifikant-häufiger Wörter für die einzelnen Schaffensphasen Dylans liefert Ergebnisse mit hoher Aussagekraft. So fällt etwa für die Phase ""The Changing of the Guard"" (1978-1981), in der sich Dylan dem Christentum hinwendet, auf, dass das Vokabular tatsächlich viele christliche Motive aufweist ( Ein differenziertes Bild ergibt sich für die N-Gramm-Analyse, was einerseits der Vielfalt an verfügbaren Methoden zur Berechnung Im Sinne einer Kritik der Digitalen Vernunft bleibt demnach festzuhalten, dass sich Methoden der computergestützten Textanalyse und des statistischen Korpusvergleichs grundsätzlich dafür eignen, einen inhaltlichen Gesamtüberblick zu einem Liedtext-Korpus zu erhalten. Es können damit diachrone Entwicklungen des Wortschatzes und Verlagerungen thematischer Schwerpunkte als grobe Tendenzen aufgezeigt werden, um das Bild des Gesamtwerks zu ergänzen. Ein solcher Ansatz eignet sich demnach gut für die initiale Thesengenerierung und kann in gewisser Weise die Funktion eines Empfehlungs- bzw. Hinweissystems für erklärungsbedürftige Stellen Die Identifikation konkreter Schaffensperioden, ausschließlich auf Basis signifikant häufiger Wörter ist aber 'zumindest für das Werk Dylans 'nicht ohne Weiteres erfassbar. Bei den N-Grammen zeigt sich, dass im Falle von Dylans Texten methodenübergreifend und mit zunehmender N-Gramm-Länge meist keine brauchbaren Ergebnisse erzielt werden konnten. Dies ist ein Hinweis darauf, dass die hier präsentierten Analysemethoden, die für andere Textsorten wie bspw. Parlamentsprotokolle bereits erfolgreich eingesetzt werden konnten (vgl. Sippl et al. 2016), auf Liedtexte nur eingeschränkt anwendbar sind. Ein möglicher Kritikpunkt am hier beschriebenen Vorgehen mag zudem das verwendete OANC-Referenzkorpus sein, welches trotz hoher Anteile mündlicher Kommunikation doch nur beschränkt vergleichbar mit der Textsorte ""Liedtext"" ist. Für künftige Vergleichsstudien böte sich ggf. ein Vergleich mehrerer unterschiedlicher Künstler und deren Liedtexte an, also bspw. Bob Dylan vs. Johnny Cash."
2018,DHd2018,BÖRNER_Ingo_Cäsar_Flaischlens__Graphische_Litteratur_Tafel__.xml,"Cäsar Flaischlens ""Graphische Litteratur-Tafel"" 'digitale Erschließung einer großformatigen Karte zur Deutschen Literatur","Ingo Börner (Universität Wien, Österreich); Frank Fischer (National Research University Higher School of Economics, Moskau, Russland); Angelika Hechtl (Wirtschaftsuniversität Wien, Österreich); Robert Jäschke (University of Sheffield, UK); Peer Trilcke (Universität Potsdam, Deutschland)","Literaturkarte, Literaturgeschichtsschreibung, TEI Encoding, Bildanalyse","Transkription, Inhaltsanalyse, Annotieren, Literatur, Karte, Visualisierung","Cäsar Flaischlens ""Graphische Litteratur-Tafel"" von 1890 stellt den Versuch dar, die Entwicklung der Deutschen Literatur mit ihren Einflüssen aus anderen Nationalliteraturen graphisch in der Form eines Flusses darzustellen. Gegenstand des Vortrags ist die digitale Edition und Bereitstellung des Vorwortes und der Karte sowie der entwickelte Workflow: Für die Edition wurde das graphische Karteninventar kodiert. Mithilfe computergestützter Bildanalyse können nicht-textuelle Informationen der Visualisierung erfasst und einer quantitativen Analyse zugeführt werden. In den letzten Jahren lässt sich ein Trend innerhalb der Literaturwissenschaft 'u.a. der Literaturgeschichtsschreibung 'ausmachen, Fragestellungen auf der Grundlage von großen Datenkorpora zu beantworten. Charakteristisch für diese Art von Literaturwissenschaft ist ein Methodenimport aus Natur- und Sozialwissenschaften, der sich nicht zuletzt in den Darstellungsformen deutlich zeigt. Auch wenn sich diese Zugänge gegenwärtig großer Beliebtheit erfreuen, sind Darstellungsweisen wie jene in Morettis einflussreichem Buch ""Kurven, Karten, Stammbäume: Abstrakte Modelle für die Literaturgeschichte"" (Moretti 2007) keineswegs ein Phänomen der Gegenwart, denn Literaturgeschichtsschreibung bedient sich bereits seit der Antike Bildmedien und anderer 'nicht rein textueller 'Präsentationsformen. Darstellungen von AutorInnen, wie etwa jene auf Raffaels berühmtem Parnassfresko in den Vatikanischen Museen lassen sich aus heutiger Perspektive wie Diagramme lesen. Information zu Relevanz sowie Verbindungen einzelner AutorInnen sind hier im Bildmedium kodiert. (vgl. Hölter/Schmitz-Emans 2013, Hölter 2005) Das Parnassfresko ist nur eine Art, wie sich Kanonbildung, Rezeption und die Zugehörigkeit zu einer AutorInnengruppe darstellen lassen. Häufig gewählte Darstellungsformen sind (Stamm-)Baum (Lima 2014) und Fluss. Die Literaturwissenschaft greift damit Formen auf, die erst mit Fortschritten in der Buchproduktion durch die Entwicklung der Lithographie möglich geworden sind und zunächst in der Geschichtsschreibung Anwendung gefunden haben (vgl. Rosenberg/Grafton 2010). Wie produktiv sich diese tradierten Denkbilder auf die Konzeptualisierung von (Literatur-)Karten auswirken können, zeigt die ""Graphische Litteratur-Tafel"" (1890) des deutschen Autors Cäsar Flaischlen (1864–1920). Flaischlens großformatige Karte (58x86,5 cm) visualisiert den 'wie es im Untertitel heißt '""Einfluss fremder Literaturen"" auf die deutsche Literatur und bedient dafür die Denkfigur geschichtlicher Prozesse als Fluss, bestehend aus einer Summe von Einflüssen. Er knüpft damit an eine Darstellungstradition von Weltgeschichte an, wie sie durch die Graphik ""Strom der Zeiten"" (1804) des österreichischen Historiographen Friedrich Strass maßgeblich geprägt wurde (vgl. Rosenberg und Grafton, 2010). In Cäsar Flaischlens ≈íuvre nimmt die aufwendig gestaltete Litteratur-Tafel eine Sonderstellung ein: Die für ihre Zeit ungewöhnliche literaturwissenschaftliche Arbeit erschien beinahe zeitgleich mit seiner Promotion 1899 und sollte Flaischlens einzige Publikation zur Literaturgeschichte bleiben. Flaischlen verließ die akademische Welt und war als Mitherausgeber und Redakteur von Literatur- und Kunstzeitschriften tätig. Heute ist der Autor hauptsächlich für seine Mundartgedichte und Erzählungen bekannt. Auf der Karte wird die deutschsprachige Literatur von ihren Anfängen bis in Flaischlens Gegenwart dargestellt. Was in der Grafik zunächst als zwei sich schlängelnde Bäche der ""Volks- und Kunstpoesie"" um 750 beginnt, entwickelt sich im Laufe der Jahrhunderte zu einem breiten Strom, in welchen über den gesamten (Zeit-)Verlauf Zuflüsse aus anderen (hauptsächlich) europäischen Nationalliteraturen einmünden. In der Legende der Karte führt Flaischlen folgende Einflüsse auf: Altes- und Neues Testament, Englisch, Französisch, Nordisch, Orientalisch, Klassisches Altertum, Spätlateinisch, Niederländisch, Italienisch, Spanisch, Schwedisch/Dänisch/Norwegisch, Russisch. Als Vertreter positivistischer Denkrichtung unternimmt Cäsar Flaischlen also den Versuch, die Darstellung der Zeit als Fluss mit den exakten Wissenschaften zu verbinden. Die Tatsache, dass er keine Quellen für die Zusammenstellung seiner Tafel nennt, spricht dafür, dass er den gängigen Kanon abbildet. Im 8-spaltigen Vorwort zur Tafel spielt Flaischlen zwar den Zusammenhang von quantitativem Befund und Visualisierung herunter 'so gibt er etwa zu bedenken, dass die Breite des Flusses ""nicht mathematisch berechnet"" sei, die Platzierung der Autoren folgt jedoch einem gewissen Prinzip: Die Autoren habe er am ""Höhepunkt"" ihres Schaffens eingezeichnet. Der Informationsgehalt der ""Litteraturtafel"" ist sehr hoch: Auswahl, Platzierung und Größe der beeinflussenden und beeinflussten Autoren ermöglichen die Rekonstruktion von Flaischlens Datengrundlage und lassen Rückschlüsse auf die hierfür verwendeten Quellen zu. So sind etwa in der Tafel Namen von Autoren, kanonischen Texten, literarischen Gruppierungen und literarischen Schulen enthalten. Ferner nutzt Cäsar Flaischlen typographische Gestaltungsmöglichkeiten wie Schriftart, Schriftgröße, Farbwahl und Unterstreichung), Symbole (Kreise in verschiedenen Größen, römische und lateinische Ziffern) und die farbliche Schraffur der (Zu-)Flüsse. Am unteren Rand der Karte ist zwar eine Legende angebracht, diese weist jedoch lediglich die Bedeutung der Schraffur aus (z.B. blau für Einflüsse aus der Englischen Literatur, rot für Einflüsse aus der Französischen Literatur, etc.). Weitere Angaben, insbesondere Erläuterungen zu verwendeten Schriftarten und -größen fehlen jedoch. Das vorgestellte Projekt ""Cäsar Flaischlens Graphische Litteratur-Tafel digital"" unternimmt den Versuch eines ""reverse engineering"" und erschließt dieses Dokument früher Visualisierung literaturgeschichtlicher Daten mit Methoden der Digital Humanities. Dazu wurden die Koordinaten von angeführten Personen, Texten, literarischen Strömungen und Schulen unter Verwendung des GIMP ImageMap-Editors ermittelt und entsprechend den in den TEI P5 Guidelines (TEI Consortium 2017) definierten Transkriptionskonventionen (""Advanced Uses of surface and zone"") erfasst, um die spatiale Dimension (Kodierung von Information über räumliche Anordnung) ebenfalls zugänglich machen zu können. Die Personen- und Werkreferenzen wurden mit den entsprechenden Normdaten (GND, VIAF, wikidata) verknüpft. Das kartographische Inventar der Karte wurde unter Rückgriff auf CSS innerhalb von @style erfasst. Dies ermöglicht nun, neben den textuellen Informationen zusätzlich die typographische Gestaltung des Textes auszuwerten. Die TEI-Daten werden über ein github-repository bereitgestellt und als Webseite aufbereitet. Das Interface fügt die drei separaten Abschnitte von Flaischlens Karte zusammen. In einer Print- Ausgabe wäre der zusammengefügte Fluss beinahe drei Meter lang und somit nur schwer lesbar. Die Web-Version erlaubt über das Scroll-Interface jedoch einen komfortablen Zugang. Die kodierten Informationen sind über Register erschlossen. Ein Prototyp der Edition ist unter http://litteratur-tafel.weltliteratur.net zugänglich. Durch eine Analyse des kartographischen Inventars lässt sich das Zeichensystem rekonstruieren, das zudem gängigen kartographischen Konventionen folgt. Beeinflusste und beeinflussende Autoren werden durch Unterstreichung unterschieden. Die Farbe der Unterstreichung entspricht der farblichen Kennzeichnung der Zuflüsse und ordnet somit die Autoren einer der beeinflussenden Nationalliteraturen zu. Durch die Verwendung unterschiedlicher Schriftgrößen wird die ""Relevanz"" eines Autors bzw. Textes für die deutsche Literatur zum Ausdruck gebracht. Besonders wichtige deutsche Autoren sind in Konturschrift ausgeführt, vergleichbar der Beschriftung von Städten in Abhängigkeit von ihrer Einwohnerzahl (vgl. Kohlstock 2004: 100). EIne Analyse der Typografie erlaubt es somit, Flaischlens ""Verständnis"" der deutschen Literatur zu rekonstruieren. Um die farblich kodierten Informationen der Karte ebenfalls berücksichtigen zu können, wurde ein Zugang über Verfahren aus dem Bereich computergestützter Bildanalyse gewählt. Mittels der Open Source Computer Vision Library (openCV) wurden die Pixel nach Farbbereichen klassifiziert und quantitativ ausgewertet, um die ""Einflüsse"" zu messen und ihre Veränderungen im Laufe der Zeit visualisieren zu können. Abbildung 1 zeigt jene Pixel, der Kategorie ""rot"" und somit dem französischen Einfluss zugeordnet wurden. Für Abbildung 2 wurde wurden die Pixel nach Jahren gruppiert und als Liniendiagramm visualisiert. Deutlich erkennbar sind Spitzen um 1620, 1665, 1715 und 1800, die sich verschiedenen Epochen der französischen Literaturgeschichte zuordnen lassen: Dem Französischen Klassizismus und der Aufklärung. Der Peak in den 1860er Jahren ist mit dem französischen Naturalismus verbunden. Das Projekt erschließt nicht nur einen inspirierenden Vorläufer gegenwärtiger Versuche von Visualisierung literaturgeschichtlicher Daten mithilfe von ""Graphen, Karten und Stammbäumen‚Äô, sondern erprobt auch auf einer methodologischen Ebene Möglichkeiten und Workflows zur Erschließung und Kodierung älterer graphischer Darstellungen von (Literatur-)geschichte. Darüber hinaus liefert es Impulse für die Arbeit mit der TEI für das Encoding von Bildmaterial, indem die Eignung von primär zur Kodierung von Manuskripten eingesetzten Tags für Grafiken und Diagramme überprüft werden."
2018,DHd2018,JACKE_Janina_Digital_vs__Humanities__Didaktische_Aufbereitun.xml,Digital vs. Humanities. Didaktische Aufbereitung digitaler Methoden für die klassischen Geisteswissenschaften im Projekt forTEXT,"Janina Jacke (Universität Hamburg, Deutschland); Jan Horstmann (Universität Hamburg, Deutschland); Jan Christoph Meister (Universität Hamburg, Deutschland)","Literatur, Interpretation, Forschungsumgebung, Annotation, Didaktik","Sammlung, Modellierung, Annotieren, Kollaboration, Lehre, Literatur","Computergestütztes Arbeiten kann geisteswissenschaftliches Forschen auf unterschiedlichste Weise befördern und bereichern. Dennoch müssen wir in unserem Arbeitsalltag und in Gesprächen mit Kolleginnen und Kollegen Das lässt sich gut am Beispiel der Literaturwissenschaft illustrieren: Digitale Methoden werden bisher vornehmlich von Literaturwissenschaftlern genutzt, die an strukturellen oder anderen formalen Aspekten literarischer Texte interessiert sind (beispielsweise an narrativen Strukturen, Figurennetzwerken etc.). Ihrem traditionellen Selbstverständnis nach ist die Literaturwissenschaft allerdings zentral an komplexen und innovativen Damit digitale Methoden eine breitere Akzeptanz finden, ist es deswegen notwendig, den Nutzen dieser Methoden auch für stärker hermeneutisch ausgerichtete geisteswissenschaftliche Forschungsfragen zu reflektieren. Unserem (weiten) Verständnis von ""Hermeneutik"" entsprechend handelt es sich bei hermeneutischen Forschungsfragen um Fragen, die auf die (holistische) Auslegung bzw. Deutung von Texten gerichtet sind (vgl. bspw. Spörl 2004: 128). In literaturwissenschaftlichen Zusammenhängen spielen dabei insbesondere Fragen nach Funktion bzw. Wirkung bestimmter Textelemente oder des Gesamttextes eine Rolle, ebenso wie die In-Beziehung-Setzung des Textes mit bestimmten Kontexten. Diese Forschungsfragen sollten exponiert im Zusammenhang mit der Entwicklung von Tools, digitaler Forschungsumgebungen und vor allem didaktischer Konzepte zur Vermittlung von DH-Methoden berücksichtigt werden. Diese Forderungen werden bisher jedoch nicht in zureichendem Maße erfüllt. Wir möchten in diesem Beitrag das aktuelle Projekt forTEXT (2017–2020) vorstellen, das der Vermittlung, Aufbereitung und Bereitstellung von Mitteln zur computergestützten Textanalyse insbesondere für hermeneutisch arbeitende Geisteswissenschaftler gewidmet ist. Im Folgenden sollen in diesem Zusammenhang zunächst die unterschiedlichen konzeptionellen Dimensionen (Abschnitt 2) sowie anschließend erste inhaltliche Ergebnisse des Projekts präsentiert werden (Abschnitt 3). Das Anfang 2017 gestartete DFG-Projekt (a) Orientierung an genuin geisteswissenschaftlichen Arbeitsweisen: Es geht, ganz im Sinne des geisteswissenschaftlichen Selbstverständnisses, um die Unterstützung der genuin (b) Niedrigschwelliger Zugang: Geisteswissenschaftler sollen die digitale Forschungsumgebung In den folgenden Unterabschnitten sollen sowohl forTEXTs Empfehlungssystem als auch die drei Komponenten des Informationsrepositoriums ( Für Geisteswissenschaftler, die noch nicht wissen, auf welche Weise digitale Methoden der Textanalyse und -interpretation ihre eigene Forschung unterstützen können, bietet forTEXT ein individualisiertes Empfehlungssystem in Form eines digitalen Fragebogens an (siehe Abb. 1). Hier können die Nutzer beispielsweise angeben, ob sie schon Vorerfahrungen mit digitalen Methoden der Textanalyse gemacht haben, in welchem Zustand sich ihr Textkorpus befindet, unter welcher Fragestellung sie ihre Texte untersuchen wollen und welcher literaturtheoretischen Schule sie sich zuordnen. forTEXTs digitale Forschungsumgebung ist in drei Bereiche gegliedert. (a) Routinen: Im Teilbereich Zum anderen werden unter (b) Ressourcen: Unter (c) Tools: Im Bereich Alle Verzeichnisse und Einträge aus den drei forTEXT-Bereichen Routinen, Ressourcen und Tools können von Nutzern eigenständig durchsucht und aufgerufen werden 'oder es erfolgt ein angeleiteter Zugriff durch die Nutzung des Empfehlungssystems. Im verbleibenden Teil dieses Beitrags möchten wir etwas genauer auf einen Lehrmodulentwurf eingehen, das dem forTEXT-Bereich Zwei von forTEXTs 90-minütigen Lehrmodulen sind der Vermittlung der digitalen Methode des Um diese Anforderungen umzusetzen, sieht forTEXT zwei Lehreinheiten zum manuellen Annotieren vor, von denen wir die erste im Folgenden kurz vorstellen möchten. In der Einheit Das verwendete Programm CATMA ist hierbei in zweifacher Hinsicht auf die forTEXT-Paradigmen abgestimmt: Es bietet eine intuitiv bedienbare Benutzeroberfläche und unterstützt den freien, undogmatischen und genuin interpretativen Zugang zu Texten, während es zugleich Optionen stärkerer Formalisierung bereithält (vgl. Abb. 2). Die im Rahmen des Lehrmoduls verfolgte didaktische Strategie hat mehrere Vorteile: Der erste Schritt, d.h. das digitale Anbringen freier Kommentare, stellt einen vollkommen explorativen und potenziell unstrukturierten Zugang zu literarischen Texten dar. Er erzwingt also kein formalistisches Umdenken und bildet die traditionellere geisteswissenschaftlich-hermeneutische Arbeitsweise gut ab. Im Vergleich zum analogen Arbeiten birgt er aber dennoch den Vorteil, dass die freien Kommentare durch digitale Unterstützung effektiver Als zusätzliches Angebot an Literaturwissenschaftler, die für eine etwas stärkere Formalisierung ihres Zugangs offen sind, zeigt das Lehrmodul, welche weiteren Vorteile und Optionen Im Lehrmodul zum manuellen digitalen Annotieren sind also die Paradigmen umgesetzt, die auch die weitere Arbeit am forTEXT-Projekt bestimmen sollen: Durch stärkere Orientierung an der traditionell-geisteswissenschaftlichen Arbeitsweise und erleichterten Zugang können digitale Methoden einer breiteren Nutzergemeinschaft nähergebracht werden."
2018,DHd2018,BRUNNER_Annelen_Projektvorstellung___Redewiedergabe__Eine_li.xml,Projektvorstellung 'Redewiedergabe. Eine literatur- und sprachwissenschaftliche Korpusanalyse,"Annelen Brunner (Institut für Deutsche Sprache, Deutschland); Stefan Engelberg (Institut für Deutsche Sprache, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland); Ngoc Duyen Tanja Tu (Institut für Deutsche Sprache, Deutschland); Lukas Weimer (Universität Würzburg, Deutschland)","Redewiedergabe, Annotation, Maschinelles Lernen, Linguistik, Narratologie","Programmierung, Strukturanalyse, Annotieren, Veröffentlichung, Stilistische Analyse","Das laufende DFG-Projekt ""Redewiedergabe"" stellt einen Anwendungsfall quantitativer Sprach- und Literaturwissenschaft dar und beschäftigt sich mit dem Phänomen ""Redewiedergabe"" auf der Grundlage großer Datenmengen. Zu diesem Zweck wird zum einen ein Korpus manuell mit Redewiedergabeformen annotiert, zum anderen werden Verfahren zur automatischen Erkennung des Phänomens entwickelt. Ziel ist es, Forschungsfragen nach der Entwicklung von Redewiedergabe vor allem im 19. Jahrhundert zu beantworten. Das Poster präsentiert einen Überblick über das Gesamtprojekt sowie erste Projektergebnisse. Sowohl aus linguistischer als auch aus literaturwissenschaftlicher Perspektive ist Redewiedergabe ein interessantes Phänomen. Die Art und Weise, wie die Figurenstimme in die Erzählung eingebunden ist, steht in engem Zusammenhang mit Erzählweise und -haltung, sowie der Konstruktion der erzählten Welt. Folglich wird dem Phänomen in der Erzählforschung viel Aufmerk¬≠samkeit geschenkt und es liegen zahlreiche systematische Analysen vor (vgl. z.B. Genette 1998; Mart√≠nez / Scheffel 2007). Zu Phänomenen wie der erlebten Rede, dem Bewusstseinsstrom usw. gibt es eine umfangreiche Spezialforschung (Überblick bei McHale 2014). Aus linguistischer Perspektive ist Redewiedergabe vor allem in Bezug auf den Funktionswandel des Konjunktivs im Zusammenhang mit seinem Auftreten in indirekter Rede untersucht worden (vgl. z.B. Übersicht in √Ågel 2000). In geringem Umfang sind auch Redewiedergabeverben und ihr Verhältnis zur wiedergegebenen Rede in das Blickfeld der Forschung gerückt (eine kurze Synopse bei Fritz 2005). Ein Vorbild für die ausführliche, manuelle Annotation von Redewiedergabe ist v.a. Semino / Short 2004. Implementierungen der automatischen Erkennung stammen vor allem aus dem Bereich der Computerlinguistik und werden oft als Vorverarbeitungsschritt für andere Anwendungen durchgeführt (z.B. Wissensextraktion, Sprechererkennung oder dem Aufbau von sozialen Netzwerken literarischer Figuren, vgl. z.B. Krestel / Bergler / Witte 2008; Elson / Dames / McKeown 2010; Iosif / Mishra 2014). Eine literaturwissenschaftlich motivierte Anwendung ist die Untersuchung von Schöch et al. 2016 zur Erkennung von direkter Wiedergabe in französischen Romantexten. Die wichtigste Vorarbeit für das vorgestellte Projekt ist die Studie Brunner 2015, auf deren Ergebnissen es aufbaut. In dieser Studie wurde ein Korpus von 13 Erzähltexten manuell annotiert und Prototypen für die automatische Erkennung (sowohl regelbasiert als auch mit Hilfe von maschinellem Lernen) wurden entwickelt und ausgewertet. Das Untersuchungskorpus umfasst die Jahre 1840-1920 und enthält sowohl fiktionale als auch nicht-fiktionale Texte. Der nicht-fiktionale Teil setzt sich zusammen aus Texten des ""Mannheimer Korpus Historischer Zeitungen und Zeitschriften"" und der Zeitschrift ""Die Grenzboten"" (digitalisiert durch die Staats- und Universitätsbibliothek Bremen), der fiktionale Teil aus Erzählungen der Sammlung der Digitalen Bibliothek (textgrid). So sind sowohl Beobachtungen von Entwicklungen über die Zeit hinweg als auch Vergleiche zwischen Textsorten möglich. Auszüge aus den Texten werden manuell annotiert. Das in Brunner 2015 vorgestellte und an Kategoriensystemen der Literaturwissenschaft orientierte Annotationssystem wurde für das Projekt erweitert und präzisiert. Es unterscheidet zwischen Wiedergabe von gesprochener Sprache, von Schrift und von Gedanken sowie den Typen direkte Wiedergabe ( Die Annotatoren arbeiten mit dem im Projekt Kallimachos ( Die zweite Projektphase, welche zum Einreichungszeitpunkt dieses Posters gerade beginnt, umfasst die Entwicklung eines automatischen Erkenners für Redewiedergabeformen. Hierbei dient das manuell annotierte Material als Test- und Trainingsmaterial. Die in Brunner 2015 implementierten Prototypen dienen als Ausgangspunkt, die Implementierung erfolgt unter Nutzung des UIMA-Frameworks sowie in Python. Geplant ist eine Verbesserung des maschinellen Lernens durch Optimierung der Attributauswahl sowie Tests mit verschiedenen Lernalgorithmen (RandomForest, SVM, eventuell Conditional Random Fields und Deep Learning) und verschiedenen Parametereinstellungen. Auch regelbasierte Ansätze sollen weiter verfolgt werden, eventuell auf Grundlage einer aufwendigeren Vorverarbeitung (z.B. Parsing). Zudem ist eine Ergänzung und Verfeinerung einer Liste von Wörtern geplant, die auf Redewiedergabe hinweisen, welche sich bereits in Brunner 2015 als wertvolles Werkzeug bei der automatischen Erkennung erwiesen hat. Der Redewiedergabe-Erkenner wird dann auf weitere Texte in unserem Untersuchungszeitraum angewendet, um größere Entwicklungslinien beobachten zu können und verschiedene offene narratologische und linguistische Forschungsfragen auf quantitativer Basis zu untersuchen, z.B.: Welche Entwicklungen in der Verwendung und Form von Redewiedergabe lassen sich im Untersuchungszeitraum beobachten? Welche Rolle spielen Textsortenunterschiede bei der Entwicklung von Redewiedergabeformen? Wie kommt die Dynamik im Bestand an Verben zustande, die als Redeeinleiter gebraucht werden? Sowohl das manuell annotierte Korpus als auch der automatische Erkenner werden am Ende des Projekts der Forschungsgemeinschaft zur Verfügung gestellt. Es werden dafür sowohl das CLARIN-D-Forschungsdatenrepositorium des Instituts für Deutsche Sprache als auch das DARIAH-DE-Repository genutzt."
2018,DHd2018,REITER_Nils_Maschinelles_Lernen_lernen__Ein_CRETA_Hackatoria.xml,Maschinelles Lernen lernen: Ein CRETA-Hackatorial zur reflektierten automatischen Textanalyse,"Nils Reiter (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Nora Ketschik (Institut für Literaturwissenschaft, Universtität Stuttgart, Deutschland); Gerhard Kremer (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Sarah Schulz (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland)","Shared task, training, maschinelles Lernverfahren","Programmierung, Modellierung, Annotieren, Methoden, Text","Das Ziel dieses Tutorials ist es, den Teilnehmerinnen und Teilnehmern konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten. Es werden dabei keinerlei Vorkenntnisse zu maschinellem Lernen oder Programmierkenntnisse vorausgesetzt. Es gibt keinen Grund, den Ergebnissen von maschinellen Lernverfahren im Allgemeinen und NLP-Tools im Besonderen blind zu vertrauen. Durch die konkreten Einblicke in den ""Maschinenraum"" von maschinellen Lernverfahren wird den Teilnehmenden ermöglicht, das Potenzial und die Grenzen statistischer Textanalysewerkzeuge realistischer einzuschätzen. Mittelfristig hoffen wir dadurch, den immer wieder auftretenden Frustrationen beim Einsatz automatischer Verfahren für die Textanalyse und deren teilweise wenig zufriedenstellender Ergebnis-Daten zu begegnen, aber auch die Nutzung und Interpretation der Ergebnisse von maschinellen Lernverfahren (d.h. in erster Linie von automatisch erzeugten Annotationen) zu fördern. Zu deren adäquater Nutzung, etwa in hermeneutischen Interpretationsschritten, ist der Einblick in die Funktionsweise der maschinellen Methoden unerlässlich. Insbesondere ist die Art und Herkunft der Trainingsdaten für die Qualität der maschinell produzierten Daten von Bedeutung, wie wir im Tutorial deutlich machen werden. Neben einem Python-Programm für die automatische Annotierung von Entitätenreferenzen, mit und an dem während des Tutorials gearbeitet werden wird, stellen wir ein heterogenes, manuell annotiertes Korpus sowie die Routinen zur Evaluation und zum Vergleich von Annotationen zu Verfügung. Das Korpus enthält Entitätenreferenzen, die im ""Center for Reflected Text Analytics"" (CRETA) Als empirisches Phänomen befassen wir uns mit dem Konzept der Entität und ihrer Referenz. Das Konzept steht für verschiedene linguistische und semantische Kategorien, die im Rahmen der Digital Humanities von Interesse sind. Es ist bewusst weit gefasst und damit anschlussfähig für verschiedene Forschungsfragen aus den geistes- und sozialwissenschaftlichen Disziplinen. Auf diese Weise können unterschiedliche Perspektiven auf Entitäten berücksichtigt werden. Insgesamt werden in den ausgewählten Texten fünf verschiedene Entitätenklassen betrachtet: PER (Personen/Figuren), LOC (Orte), ORG (Organisationen), EVT (Ereignisse) und WRK (Werke). Unter Entitätenreferenzen verstehen wir Ausdrücke, die auf eine Entität in der realen oder fiktiven Welt referieren. Das sind zum einen Eigennamen (Named Entities, z.B. ""Peter""), zum anderen Gattungsnamen (z.B. ""der Bauer""), sofern diese sich auf eine konkrete Instanz der Gattung beziehen. Dabei wird als Referenzausdruck immer die maximale Nominalphrase (inkl. Artikel, Attribut) annotiert. Pronominale Entitätenreferenzen werden hingegen nicht annotiert. In In den Die Grundlage für (überwachte) maschinelle Lernverfahren bilden Annotationen. Um die Annotierung von Entitätenreferenzen automatisieren zu können, bedarf es Textdaten, die die Vielfalt des Entitätenkonzepts abdecken. Bei diesem Tutorial werden wir auf Annotationen zurückgreifen, die im Rahmen von CRETA an der Universität Stuttgart entstanden sind (cf. Blessing et al., 2017; Reiter et al., 2017a). Das Korpus enthält literarische Texte aus zwei Sprachstufen des Deutschen (Neuhochdeutsch und Mittelhochdeutsch) sowie ein sozialwissenschaftliches Teilkorpus. Der  Das Der Ablauf des Tutorials orientiert sich an sog. Zunächst diskutieren wir kurz die zugrundeliegenden Texte und deren Annotierung. Annotationsrichtlinien werden den Teilnehmerinnen und Teilnehmern im Vorfeld zur Verfügung gestellt. Im Rahmen der Einführung wird auch auf die konkrete Organisation der Annotationsarbeit eingegangen, so dass das Tutorial als Blaupause für zukünftige Tätigkeiten der Teilnehmenden in diesem und ähnlichen Arbeitsfeldern dienen kann. Die Teilnehmerinnen und Teilnehmer versuchen selbständig und unabhängig voneinander, eine Kombination aus maschinellen Lernverfahren, Merkmalsmenge und Parametersetzungen zu finden, die auf einem neuen, vom automatischen Lernverfahren ungesehenen Datensatz zu den Ergebnissen führt, die dem Goldstandard der manuellen Annotation am Öhnlichsten sind. Das bedeutet konkret, dass der Einfluss von berücksichtigten Features (z.B. Groß- und Kleinschreibung oder Wortlänge) auf die Erkennung von Entitätenreferenzen empirisch getestet werden kann. Dabei sind Intuitionen über die Daten und das annotierte Phänomen hilfreich, da simplem Durchprobieren aller möglichen Kombinationen (``brute force'') zeitlich Grenzen gesetzt sind. Wir verzichten bewusst auf eine graphische Benutzerschnittstelle (cf. Reiter et al., 2017b) -- stattdessen editieren die Teilnehmerinnen und Teilnehmer das (Python)-Programm direkt, nach einer Einführung und unter Anleitung. Vorkenntnisse in Python sind dabei nicht nötig: Das von uns zur Verfügung gestellte Programm ist so aufgebaut, dass auch Python-Neulinge relativ schnell die zu bearbeitenden Teile davon verstehen und damit experimentieren können. Wer bereits Erfahrung im Python-Programmieren hat, kann fortgeschrittene Funktionalitäten des Programms verwenden. Wie am Ende jedes maschinellen Lernprozesses wird auch bei uns abschließend eine Evaluation der automatisch generierten Annotationen durchgeführt. Hierfür werden den Teilnehmerinnen und Teilnehmern nach Ablauf einer begrenzten Zeit des Experimentierens und Testens (etwa 60 Minuten) die finalen, vorher unbekannten Testdaten zur Verfügung gestellt. Auf diese Daten werden die erstellten Modelle angewendet, um automatisch Annotationen zu erzeugen. Diese wiederum werden dann mit dem Goldstandard verglichen, wobei die verschiedenen Entitätenklassen sowie Teilkorpora getrennt evaluiert werden. Auch das Programm zur Evaluation stellen wir bereit. Am hier verwendeten Beispiel der automatischen Annotation von Entitätenreferenzen demonstrieren wir, welche Schritte für die Automatisierung einer Textanalyseaufgabe mittels maschinellen Lernverfahren nötig sind und wie diese konkret implementiert werden können. Die Teilnehmerinnen und Teilnehmer bekommen einen zusammenhängenden Überblick von der manuellen Annotation ausgewählter Texte über die Feinjustierung der Lernverfahren bis zur Evaluation der Ergebnisse. Die vorgestellte Vorgehensweise für den gesamten Ablauf ist grundsätzlich auf ähnliche Projekte übertragbar. Das Tutorial schärft dabei das Verständnis für den Zusammenhang zwischen untersuchtem Konzept und den dafür relevanten Features, die in ein statistisches Lernverfahren einfließen. Durch Einblick in die technische Umsetzung bekommen die Teilnehmerinnen und Teilnehmer ein Verständnis für die Grenzen und Möglichkeiten der Automatisierung, das sie dazu befähigt, zum einen das Potenzial solcher Verfahren für eigene Vorhaben realistisch(er) einschätzen zu können, zum anderen aber auch Ergebnisse, die auf Basis solcher Verfahren erzielt wurden, angemessen hinterfragen und deuten zu können. Der Workshop wird ausgerichtet von Mitarbeiterinnen und Mitarbeitern des ""Center for Reflected Text Analytics"" (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine ""black box"" sein, sondern auch für Nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird.  Die Forschungsinteressen von Nils Reiter liegen generell in der Anwendung computerlinguistischer Methoden auf Fragen aus den Geistes- und Sozialwissenschaften. Insbesondere die Operationalisierung literarischer Forschungsfragen und die adäquate Interpretation von Ergebnissen ist dabei ein Schwerpunkt, neben der regelgeleiteten Annotation und damit zusammenhängenden Fragen.  Nora Ketschik ist Promotionsstudentin in der Abteilung für Germanistische Mediävistik. Im Rahmen des CRETA-Projekts nimmt sie Analysen narratologischer Kategorien (u.a. Figur, Raum) an ausgewählten mittelhochdeutschen Romanen vor und setzt sich dabei mit der Verwendung computergestützter Methoden für literaturwissenschaftliche Analysezwecke auseinander.  Der Interessenschwerpunkt Gerhard Kremers ist der reflektierte Einsatz von Werkzeugen der Computerlinguistik für geistes- und sozialwissenschaftliche Fragestellungen. Damit zusammenhängend gehören die Entwicklung übertragbarer Arbeitsmethoden und die angepasste, nutzerfreundliche Bedienbarkeit automatischer linguistischer Analysetools zu seinen Forschungsthemen.  Sarah Schulz beschäftigt sich überwiegend mit der automatischen Verarbeitung von Texten, die syntaktischen oder lexikalischen Eigenschaften aufweisen und damit vom Zwischen 15 und 25. Es wird außer einem Beamer keine besondere technische Ausstattung benötigt. Es sollte sich um einen Raum handeln, in dem es möglich ist, den Teilnehmenden über die Schulter zu blicken und durch die Reihen zu gehen."
2018,DHd2018,GIUS_Evelyn_hermA__Zur_Rolle_von_Annotationen_in_hermeneutis.xml,hermA. Zur Rolle von Annotationen in hermeneutischen Prozessen,"Benedikt Adelmann (Universität Hamburg); Melanie Andresen (Universität Hamburg); Anke Begerow (Hochschule für angewandte Wissenschaften, Hamburg); Uta Gaidys (Hochschule für angewandte Wissenschaften, Hamburg); Evelyn Gius (Universität Hamburg); Gertraud Koch (Universität Hamburg); Wolfgang Menzel (Universität Hamburg); Dominik Orth (Bergische Universität Wuppertal); Sebastian Topp (Universität Hamburg); Michael Vauth (Technische Universität Hamburg); Heike Zinsmeister (Universität Hamburg)","Hermeneutik, Automatisierung, Annotation, Codierung","Modellierung, Annotieren, Kollaboration, Methoden, Forschungsprozess, Text","Der Forschungsverbund ""Automatisierte Modellierung hermeneutischer Prozesse"" (hermA) befasst sich im Rahmen einer interdisziplinären Zusammenarbeit von Literaturwissenschaft, Pflegewissenschaft, Kulturanthropologie, Computerlinguistik und Informatik mit der Frage, ob und inwieweit hermeneutisches Arbeiten im Bereich der sozial- und geisteswissenschaftlichen Textanalyse computergestützt automatisiert werden kann. Von der Auseinandersetzung mit dieser Frage sind zum einen Erkenntnisse über die Verwendung und Funktion von Annotationen in den jeweiligen hermeneutischen Prozessen zu erwarten, zum anderen sollen erste Ansätze zur Automatisierung des Analyseprozesses entwickelt werden, die die Auswertung größerer Textmengen unterstützen. Die fünf Teilprojekte von hermA folgen in ihrer hermeneutischen Arbeit an und mit Texten unterschiedlichen Forschungslogiken (deduktiv, induktiv und/oder abduktiv); sie arbeiten außerdem jeweils eigenständig zu einem Thema im Gesundheitsbereich und stellen damit thematisch verbundene, fachdisziplinäre Forschungsszenarien zur Evaluation der automatisierten Modelle zur Verfügung: Die Teilprojekte im Projekt hermA decken die gesamte Bandbreite an hermeneutischen Vorgehensweisen ab, die sich auf deduktive, induktive und abduktive Schlussverfahren zurückführen lassen. Damit geht es um alle drei darauf basierende Forschungslogiken, die Charles Sanders Peirce folgendermaßen zusammenfasst: ""Deduction proves that something Es werden also jeweils bestimmte Strategien genutzt, um unterschiedliche Arten von Erkenntnissen zu erlangen: Hinzu kommt: In jedem hermeneutischen Erkenntnisprozess werden laufend neue Erkenntnisse generiert. Wenn diese sich nicht in die jeweilige Forschungslogik integrieren lassen, müssen die entsprechenden Hypothesen und/oder Vorhersagen revidiert oder erweitert und anschließend erneut angewendet werden. Dabei ist es zum Teil nötig, auf andere Forschungslogiken zurückzugreifen (etwa durch eine induktive Herleitung einer neuen Regel, die im deduktiven Prozess angewendet werden kann). Ein erstes Zwischenergebnis des Projekts hermA ist, dass die Rolle von Annotationen in hermeneutischen Prozessen von der jeweilig zur Anwendung kommenden Forschungslogik abhängt. Diese Zusammenhänge zwischen den Forschungslogiken und dem Einsatz von Annotationen sollen auf dem vorgeschlagenen Poster mit Blick auf die von den Teilprojekten verfolgten Forschungsfragen dargestellt werden. Dabei geht es um folgende Aspekte: Bei der Betrachtung der Rolle von Annotationen muss zusätzlich zwischen manuellen und automatischen Annotationen differenziert werden. Während manuelle Annotationen eher zur Entwicklung oder Überprüfung von Hypothesen genutzt werden, unterstützen die automatisierten Zugänge jene Aspekte der hermeneutischen Prozesse, die bereits klar definiert werden können 'etwa die Erkennung bereits operationalisierter Phänomene oder die Identifikation relevanter Texte oder Textstellen für die weitere Analyse und Interpretation."
2018,DHd2018,FISCHER_Frank__Liebe_und_Tod_in_der_Deutschen_Nationalbiblio.xml,Liebe und Tod in der Deutschen Nationalbibliothek.   Der DNB-Katalog als Forschungsobjekt der digitalen Literaturwissenschaft,"Frank Fischer (Higher School of Economics, Moskau); Robert Jäschke (Humboldt-Universität, Berlin)","Deutsche Nationalbibliothek, Metadaten, Katalogdaten, Literaturwissenschaft","Programmierung, Visualisierung, Literatur, Metadaten","Der Sammelauftrag der Deutschen Nationalbibliothek (DNB) beginnt 1913 und bezieht sich auf ""lückenlos alle deutschen und deutschsprachigen Publikationen"" (""Wir über uns"", 16.03.2017). Der DNB-Katalog ist natürlich längst digitalisiert und die Arbeit mit ihm mittlerweile sehr komfortabel, da der Datendienst der DNB unter http://www.dnb.de/datendienst vierteljährlich einen Komplettabzug der Katalogdaten im RDF-Format bereitstellt, unter der freien Lizenz CC0 1.0. Momentan (Stand vom 23.06.2017) enthält er 14¬†102¬†309 Datensätze, also Metadaten zu von der DNB gesammelten Medien. Bisher gibt es aus geisteswissenschaftlicher Sicht nur wenige Versuche, diese Quelle nutzbar zu machen (eine Ausnahme bilden etwa Häntzschel u.¬†a. 2009). Wir präsentieren ein einfaches Framework, mit dem verschiedene Aspekte des DNB-Katalogs untersucht werden können, seine Entwicklung über die knapp 105 Jahre seit Bestehen der Nationalbibliothek (vgl. auch Schmidt 2017, der für die Library of Congress einen ähnlichen Ansatz vorgestellt hat). Wir konzentrieren uns dabei auf Romane als Untersuchungsobjekt, von denen in der DNB rund 180¬†000 als solche rubriziert sind (dies entspricht nicht der Gesamtanzahl an Romanen, denn Nachauflagen und Übersetzungen zählen dort mit hinein 'außerdem fehlen auch einige Romane, da sie nicht entsprechend verschlagwortet worden sind. Dieser Vortrag ist methoden-, nicht vorderhand ergebniszentriert, wobei wir an zwei Anwendungsszenarien aus der Praxis der digitalen Literaturwissenschaft demonstrieren, wie Katalogmetadaten bei der Bearbeitung konkreter Forschungsfragen behilflich sein können bzw. diese überhaupt erst ermöglichen. Die Titeldaten der DNB werden in typischen Linked-Data-Formaten (RDF/XML, JSON-LD usw.) angeboten. Der übliche Ansatz mit solchen Daten zu arbeiten ist, diese in eine geeignete Datenbank (Triple-Store) einzuladen und Anfragen mit Hilfe der entsprechenden Anfragesprache (i.¬†A. SPARQL) zu stellen. Prinzipiell sind auch andere Systeme (z.¬†B. relationale Datenbank, Suchmaschine) geeignet. Dies ermöglicht sehr flexible Anfragen und die leichte Einbindung weiterer Datenquellen. Da die Größe der Daten (unkomprimiert ca. 21¬†GB) jedoch gewisse Anforderungen an die Hardware stellt und die Konfiguration und Optimierung der Datenbank aufwendig ist, haben wir uns für eine andere, kompakte und leichter nachzuvollziehende Lösung entschieden. Langfristiges Ziel ist jedoch die Bereitstellung einer fertig konfigurierten Arbeitsumgebung in Form eines Docker-Containers, in der die Daten in einer Datenbank ad hoc verfüg- und analysierbar sind. Der Titeldatensatz ist mit 14¬†102¬†309 Datensätzen und 227¬†212¬†707 Tripeln (""Fakten"") sehr umfangreich und enthält neben Angaben zu Büchern auch Angaben zu weiteren Medientypen wie etwa Zeitschriften. Neben den üblichen Metadatenfeldern wie Titel und Erscheinungsjahr ist bei Buchobjekten meist auch die Seitenanzahl sowie das Format vermerkt. Ganz im Sinne von Linked Data werden viele Angaben mit Hilfe von standardisierten Vokabularien (z.¬†B. Dublin Core oder Bibo) beschrieben und ermöglichen so die Verlinkung mit weiteren Datensätzen. Insbesondere ermöglicht die Angabe der Autor*innen durch die numerische Kennung aus der Gemeinsamen Normdatei (GND) die Verknüpfung der Daten mit Wikidata, der (zukünftig) hinter Wikipedia stehenden Faktendatenbank. Wikidata verwendet ein auf Linked Data basierendes Datenmodell und ermöglicht, ähnlich wie Wikipedia, jedermann das Hinzufügen und Bearbeiten von Daten. Neben Angaben zu Städten und Ländern (z.¬†B. Fläche, Einwohnerzahl) sind in Wikidata auch Daten zu zahlreichen Persönlichkeiten gespeichert, etwa deren Namen, Geburtsdaten, Berufe, Werke und, falls vorhanden, GND-Kennung (als Beispiel sei auf die Seite zu Johann Wolfgang von Goethe verwiesen: https://www.wikidata.org/wiki/Q5879). Unser Framework umfasst derzeit vier Schritte, die im Folgenden beschrieben werden: RDF/XML wird von den üblichen Softwaretools im Allgemeinen nicht als Datenstrom verarbeitet, sondern im Hauptspeicher abgelegt und dann weiterverarbeitet. Aufgrund der Größe der Daten scheidet diese Möglichkeit aus. Da jedoch alle wesentlichen Daten zu einem Medium typischerweise innerhalb eines XML-Tags ""rdf:Description"" abgelegt sind, können wir die Daten auch mit Hilfe eines SAX-Parsers als XML verarbeiten. Wir extrahieren die für die Analyse wesentlichen Metadaten (z.¬†B. dcterms:contributor, dcterms:language, dc:title, dcterms:extent, rdau:P60493) und speichern diese als JSON ab. JSON ist im Allgemeinen platzsparender als RDF/XML und kann leicht in Elasticsearch eingeladen werden, was ein geplanter nächster Schritt ist. Unser Ziel ist die Anreicherung der Autorenangaben im DNB-Datensatz mit Informationen aus Wikidata, beispielsweise Geburtsdatum- und ‚Äëort, Beruf und Verweis auf einen etwa vorhandenen Artikel in Wikipedia. Da die Python-Softwarebibliothek zur Verarbeitung von Wikidata-Datensätzen veraltet ist, greifen wir auf das Java-basierte Wikidata Toolkit zurück. Nach Herunterladen des aktuell (14.08.2017) 16¬†GB großen komprimierten Wikidata-Datensatzes extrahieren wir in zwei Durchgängen zunächst alle Elemente mit einer GND-Kennung einschließlich ausgewählter Merkmale und ergänzen im zweiten Durchlauf die Werte der Merkmale. Das Ergebnis speichern wir im JSON-Format. Unser Python-Skript implementiert eine Pipeline, die alle in den vorherigen Schritten extrahierten Daten einliest und mit Hilfe der GND-Kennung verknüpft, Metadatenangaben (wie z.¬†B. Seitenanzahlen) extrahiert, vereinfacht und normalisiert, Datensätze mit fehlenden Angaben filtert und schließlich die gewünschten Datenfelder spaltenbasiert ausgibt. Die Vereinfachung umfasst vor Allem das Entfernen von Namespace-Präfixen (etwa http://id.loc.gov/vocabulary/iso639-2/ bei der Angabe der Sprache); Seitenanzahlen werden mit Hilfe eines regulären Ausdrucks extrahiert, der die häufigsten Fälle abdeckt; Jahreszahlen ebenso; Verlagsnamen können mit Hilfe einer Normtabelle normiert werden (dies ist nötig, da die Schreibung dieser Namen innerhalb des Katalogs nicht standardisiert ist). Die entstandenen Dateien im TSV-Format können mit den üblichen Unix-Kommandozeilen-Werkzeugen wie awk, sort, uniq etc. leicht verarbeitet und analysiert werden; Visualisierungen wurden mit gnuplot erzeugt. Alle Schritte sind im GitHub-Repository dokumentiert. Abbildung¬†1 zeigt die zeitliche Verteilung einiger Subdatensätze des Katalogs. Von den etwa 14,1¬†Mio. Objekten im originalen DNB-Datensatz weisen etwa 8,3¬†Mio. extrahierbare Seitenanzahlen auf (59¬†%). Beschränken wir diese Anzahl auf ""Romane"" (über das Datenfeld ""rdau:P60493""), bleiben 353¬†498 übrig, von denen wiederum 316¬†518 Umfangsangaben aufweisen und 180¬†219 einen Verfasser, der mindestens einen Wikipedia-Eintrag (in egal welcher Sprache) besitzt. Dieses Datenset ist die Grundlage für die unten folgenden Anwendungsszenarien.  Als möglicher Plausibilitäts- bzw. Repräsentativitätstest kann das Auszählen derjenigen Romanciers dienen, die mit den meisten Romanen im Katalog vertreten sind. Da der DNB-Katalog Vollständigkeit anstrebt, kann ein entsprechendes Ranking etwas über vergangene Realitäten auf dem deutschsprachigen Buchmarkt aussagen (Tab.¬†1), und tatsächlich stehen die Verfasser*innen von Romanbestsellern im Unterhaltungsbereich ganz oben (die Anzahl der Bücher umfasst von der DNB mitgesammelte Neuauflagen, Konsalik hat also nicht über 2¬†000 Romane geschrieben). Tabelle 1: Romanautor*innen geordnet nach Anzahl der Werke (inkl. Nachauflagen) im DNB-Katalog. Die Verfügbarkeit großer digitalisierter Kataloge ermöglicht Large-Scale-Analysen bibliografischer Metadaten, etwa die Entwicklung von Romantiteln. Ein Vorläufer auf diesem Gebiet, Werner Bergengruens immer noch zu empfehlende Bibliothekarsfantasie ""Titulus"" von 1960, musste sich noch auf eine manuelle Sammlung des Autors stützen. Mittlerweile gibt es mit Franco Morettis Studie ""Style Inc."" (2009) ein prominentes datengestütztes Beispiel (wobei sich Moretti bei seiner Analyse von um die 7¬†000 Romantiteln auf Fachbibliografien stützte, nicht auf Katalogdaten). Um einen ersten Einblick in das Vokabular von Romantiteln zu bekommen, seien in Tabelle¬†2 die am häufigsten vorkommenden Substantive aufgelistet. Tabelle 2: Häufigste Substantive in Romantiteln im gesamten DNB-Katalog. Überzeitliche Konzepte 'Liebe, Tod usw. 'dominieren das Feld. Und nebenbei bemerkt: Ein wenig erinnert diese Liste an Jan Böhmermanns satirischen Song ""Menschen, Leben, Tanzen, Welt"", mit dem auf die Beliebig- und Austauschbarkeit kontemporärer deutschsprachiger Liedproduktion angespielt wird (vgl. Pandzko/Böhmermann 2017), ein Befund, der sich analog auch auf Romantitel projizieren ließe. Diese Anfragetechnik kann 'wie beim Google Ngram Viewer 'auf n-Gramme ausgedehnt werden, die Top-10 der häufigsten Trigramme findet sich in Tabelle¬†3. Tabelle 3: Häufigste Trigramme in Romantiteln im DNB-Katalog. Ebenfalls analog zum Ngram Viewer lässt sich die zeitliche Entwicklung von n-Gramm-Frequenzen darstellen. Die unterschiedlichen Darstellungen in absoluten (Abb.¬†2) und relativen Zahlen (Abb.¬†3) kann etwa zeigen, dass sich zwischen Mitte der 1970er-Jahre und Mitte der 1990er-Jahre die Zahl an Romanen mit ""Liebes""-Titeln zwar nahezu verdoppelt, dass sich diese Titel aber in relativen Zahlen nicht großartig vermehren. Für genauere Analysen auf Grundlage dieser Extraktions- und Visualisierungsmethoden stellt das von uns vorgestellte Framework eine ideale Basis dar. Unser zweites Anwendungsszenario betrifft die Erforschung des literarischen Textumfangs. Abbildung¬†4 zeigt die durchschnittliche Seitenanzahl von Romanen im Katalog der DNB. Als Zuarbeit zu einer Theorie des literarischen Textumfangs haben wir mit dem von uns hier vorgestellten Framework in einer umfangreicheren Studie untersucht, wie sich der Umfang von Romanen etwa auf die Kanonbildung auswirkt (längere Romane, speziell solche von mehr als 1¬†000 Seiten Umfang, haben es leichter, in Kanonlisten zu landen). Außerdem ist es uns gelungen zu zeigen, wie umfangreiche Romane die DNA von Verlagen bestimmen können (vgl. Fischer/Jäschke 2018).  Katalogdaten als Untersuchungsobjekt der quantifizierenden Literaturwissenschaften sind keine sich selbst erklärende Quelle, sondern ein über Jahrhunderte gewachsenes, überaus komplexes System. Die bibliothekarische Betreuung dieser Daten zielt nicht per se auf literaturwissenschaftliche Anwendungsfälle. Die Verschlagwortung kann lückenbehaftet sein, bestimmte Angaben wie etwa zum Textumfang können Fehler aufweisen. Die literaturwissenschaftliche Beschäftigung mit Katalogdaten setzt deren Explorier- und Kontrollierbarkeit voraus, wozu das hier vorgestellte Framework einen ersten Beitrag leisten soll. Zwei konkrete Anwendungsfälle sollten als Praxisbeispiele und ausdrücklich als Anreiz für weitere Szenarien dienen."
2018,DHd2018,SCHÖCH_Christof_Burrows_Zeta__Varianten_und_Evaluation.xml,Burrows Zeta: Varianten und Evaluation,"Christof Schöch (Universität Trier, Deutschland); Albin Zehe (Universität Würzburg, Deutschland); José Calvo Tello (Universität Würzburg, Deutschland); Andreas Hotho (Universität Würzburg, Deutschland)","Zeta, Distinktivität, Keyness, Stilometrie","Programmierung, Inhaltsanalyse, Beziehungsanalyse, Theoretisierung, Text","Der vorliegende Beitrag enthält methodische Überlegungen und Experimente zu ""Zeta"", einem von John Burrows (2007) vorgeschlagenen Maß für die Distinktivität oder ""keyness"" von textuellen Merkmalen (Wortformen, Lemmata, etc.). Mit solchen Maßen werden Merkmale ermittelt, die für eine bestimmte Gruppe von Texten gegenüber einer Vergleichsgruppe charakteristisch sind. Das Exposé gibt einen Überblick zu solchen Maßen, bevor die Funktionsweise von Zeta erläutert wird. Aufbauend auf einer Neu-Implementierung in Python (""pyzeta"", https://github.com/cligs/pyzeta) und Vorarbeiten (Schöch im Druck) liegt der spezifische Forschungsbeitrag dann in den folgenden Schritten: erstens werden mehrere Varianten von Zeta vorgeschlagen und implementiert; zweitens werden Verfahren zum Vergleich und der Evaluation der Ergebnisse erprobt. Ziel ist es, Zeta in seiner Funktionsweise und in seiner Beziehung zu vergleichbaren Maßen besser zu verstehen und vorhandene Nachteile des Maßes durch gezielte Modifikationen zu beheben. Die vergleichende, kontrastierende Analyse zweier Gruppen von Texten ist ein in den Sprach- und Literaturwissenschaften weit verbreitetes Verfahren. Entsprechend wurden zahlreiche Maße der Distinktivität oder ""keyness"" von Merkmalen entwickelt und für vielfältige Fragestellungen eingesetzt. Die grundlegende Annahme solcher Maße ist, dass ein Merkmal nicht schon durch seine reine Häufigkeit in einer Textgruppe für diese charakteristisch ist, sondern dass dies auch davon abhängt, wie häufig das Merkmal in einer Vergleichsgruppe ist. Diejenigen Merkmale bekommen einen besonders hohen Wert zugewiesen, die in der einen Gruppe sehr häufig sind und zugleich in der Vergleichsgruppe sehr selten sind (Scott 1997, 236). Man kann vier Arten von Verfahren unterscheiden: Die praktische Bedeutung von Distinktivitätsmaßen ist daran erkennbar, dass Korpusanalyse-Software meist eine entsprechende Funktion anbietet, so ""keyness"" in WordCruncher (Scott 1997) oder ""spécificity"" in TXM (Heiden et al. 2012). Kilgariff 2004 und Lijfijt et al. 2014 sind wichtige Arbeiten zur Evaluation von Distinktivitätsmaßen. Das von John Burrows (2007) vorgeschlagene ""Zeta"" beruht auf einem Dispersionsmaß. Vor der Berechnung werden die Texte in kleinere Segmente gesplittet, wobei die Segmentlänge ein wichtiger Parameter ist. Dann wird für jedes Merkmal der Anteile der Segmente erhoben, in denen das Merkmal mindestens einmal vorkommt (die ""document proportion""). Von diesem Anteil in der untersuchten Gruppe wird der entsprechende Anteil in der Vergleichsgruppe subtrahiert, woraus sich ein Zeta-Wert zwischen -1 und 1 ergibt. Ein Effekt dieser Berechnungsweise ist, dass Zeta Inhaltswörter als distinktive Wörter favorisiert, Funktionswörter sowie Eigennamen hingegen penalisiert. Daraus ergibt sich eine hohe Interpretierbarkeit der Ergebnisse, die Zeta im Vergleich zu anderen Maßen für die (digitalen) Literaturwissenschaften besonders attraktiv macht. Ein Nachteil ist, dass Merkmale durch die Subtraktion niemals einen Zeta-Wert bekommen können, der höher ist als ihre ""document proportion"" in der untersuchten Textgruppe, selbst wenn sie gegenüber der Vergleichsgruppe deutlich überrepräsentiert sind (Abbildung 1, Wörter in den roten Rahmen; Schöch im Druck).  Eine bekannte Implementierung von Zeta existiert im stylo-Paket für Rin der Funktion ""oppose()"" (Eder et al. 2016). Abbildung 2 zeigt für ein Beispiel die Ergebnisdarstellung in der hier verwendeten ""pyzeta""-Implementierung.  Anwendungsbeispiele von Zeta gibt es in der Shakespeare-Forschung (Craig und Kinney 2009), der modernen englischsprachigen Literatur (Hoover 2010; Weidman und O""Sullivan 2017) und der Romanistik (Schöch im Druck). In der zuletzt genannten Arbeit zum französischen Theater der Klassik und Aufklärung konnte nicht nur die erwartbare, klare Differenzierung von Komödien und Tragödien gezeigt werden. Vielmehr wurde auch die spezifische Verortung der Tragikomödien deutlich, die nicht als Mischform zwischen Komödien und Tragödien zu verstehen sind, sondern eine besondere Affinität zur Tragödie aufweisen (Abbildung 3). Ausgehend von der ursprünglichen Formulierung von Zeta durch Burrows als Subtraktion der ""document proportions"" lassen sich mehrere Faktoren identifizieren, die zur Formulierung von Varianten von Zeta geeignet erscheinen: Die Kombination dieser Faktoren ergibt 8 Varianten von Zeta (Tabelle 1). Tabelle 1: Übersicht über die getesteten Varianten von Zeta. Die Variante mit Label ""sd0"" entspricht Burrows"" Zeta. Einige der Varianten sind mathematisch gut motivierbar und versprechen, den oben genannten Nachteil der begrenzten Werte für bestimmte Wörter auszugleichen und damit Zeta zu verbessern, es wurden aber alle implementiert und auf zwei Datensätzen evaluiert. Es wurden zwei unterschiedliche Korpora verwendet. Erstens ein Korpus aus der textbox-Sammlung (Schöch et al. 2017), das Romane enthält, die zwischen 1880 und 1940 veröffentlicht wurden: jeweils 24 Texte aus Spanien und aus Lateinamerika (ca. 2,8 Millionen Tokens). Zweitens, ein Teil der Sammlung Die 8 Varianten führen zu unterschiedlichen Wortlisten, geordnet nach absteigenden Zeta-Werten. Vergleicht man den Beginn der Wortlisten für zwei Varianten, fällt auf, dass es wie erwartet zu Verschiebungen im Rang der distinktivsten Wörter kommt. Um den Grad der Abweichung der Ergebnisse für alle Varianten zueinander auf der Grundlage längerer Wortlisten zu erheben, ist ein quantifizierendes Verfahren unerlässlich. Ein Ansatz ist, ein Clustering der Maße auf Basis der Zeta-Werte ihrer Wörter vorzunehmen (Abbildung 4).  Abbildung 4 zeigt, dass der wichtigste Faktor für die Unterschiedlichkeit der Varianten ist, ob subtrahiert oder dividiert wird (zwei Haupt-Cluster). Die beiden anderen Variablen spielen eine viel kleinere Rolle. (Die Ergebnisse weiterer Analysen, u.a. auf Basis der RBO-Öhnlichkeit (""ranked biased order"", Webber et al. 2010), werden aus Platzgründen hier nicht diskutiert.) Unabhängig von den Beziehungen der Varianten zueinander stellt sich die Frage, welche der Varianten von Zeta besonders gut distinktive Wörter identifiziert. Dabei kann zur Evaluation nicht auf einen Goldstandard zurückgegriffen werden: eine händische Annotation der Wörter nach dem Grad ihrer Distinktivität ist nicht möglich, weil niemand das zugrunde liegende Korpus überblicken kann. Die Qualität eines Distinktivitätsmaßes kann aber evaluiert werden, indem es als Merkmalsselektor für einen Klassifikationstask verwendet wird. Wenn die durch Zeta am höchsten bewerteten Wörter als Features für einen Klassifikator verwendet werden, sollte dieser Klassifikator eine höhere Genauigkeit erreichen als bei einfacher Verwendung der häufigsten Wörter. Tatsächlich lässt sich dieser Effekt auf dem Korpus der spanisch-sprachigen Romane nachweisen (Tabelle 2). Zur Ermittlung einer Baseline wurde für die Klassifikation in spanische und lateinamerikanische Romane ein linearer SVM-Classifier auf den häufigsten 80, nach TF-IDF gewichteten Wörtern (ohne Stoppwörter) trainiert. Dieser Classifier erreichte lediglich eine Klassifikationsgüte (F1-Score) von 0.49, ist also nicht vom Zufall zu unterscheiden. Trainiert man stattdessen auf den 40 distinktivsten Wörtern nach Zeta (oder einer der Varianten), lassen sich Genauigkeiten von deutlich über 90% erzielen. Diese Genauigkeit kann nicht als tatsächliches Klassifikationsergebnis gesehen werden, da die distinktivsten Merkmale auf dem gesamten Korpus extrahiert wurden, ohne Aufteilung in Trainings- und Testdaten. Dennoch zeigt das Ergebnis, dass die von Zeta selektierten Merkmale tatsächlich sehr nützlich für eine Klassifikation sind. Zudem zeigen sich deutliche Unterschiede in der Performanz je nach verwendeter Variante: während mit ""sd0"" (=Burrows Zeta) 81% Genauigkeit erreicht wird, erhöht sich dieser Wert bei der Variante mit log2-Transformation, ""sd2"", auf 98%.  Wichtigste Ergebnisse dieses Beitrags sind ein differenziertes Verständnis davon, wie Zeta im Kontext anderer Distinktivitätsmaße einzuordnen ist und wie bestimmte mathematischen Parameter sich auf die Ergebnislisten auswirken: als ein auf dem Grad der Dispersion der Merkmale beruhendes Maß, dessen entscheidende Eigenschaft die Subtraktion der Werte ist. Ein weiteres wesentliches Ergebnis sind die beiden vorgeschlagenen Strategien zum Vergleich und der Evaluation von Distinktivitätsmaßen, wenn eine direkte Evaluation auf Goldstandard-Daten nicht möglich ist. Nächste Schritte: Wir möchten als weitere Evaluationsstrategie künstliche Texte generieren, in denen wir kontrolliert einzelne Wörter mit unterschiedlich stark abweichender Verteilung einfügen. So können verschieden Zeta-Varianten direkt dahingehend evaluiert werden, wie gut sie diese Wörter korrekt identifizieren. Zudem möchten wir neben der ""document proportion"" von Zeta ein weiteres Dispersionsmaß, die von Gries (2008) vorgeschlagene ""deviation of proportions"" als Grundlage für eine weitere Zeta-Variante verwenden. Schließlich möchten wir untersuchen, ob die hohe Interpretierbarkeit des Original-Zeta bei den Varianten mit noch höherer Klassifikationsgüte erhalten bleibt. Eine separate Untersuchung ist in Vorbereitung zu zwei eng zusammenhängenden Fragen: wie sich unterschiedliche Segmentlängen einerseits auf die Ergebnisse auswirken, und wie sich die Ergebnisse verändern, wenn unterschiedlich lange Texte nicht mit allen Segmenten in die Berechnung eingehen, sondern aus jedem Einzeltext zufällig eine identische Anzahl von Segmenten gesampelt wird. Übergeordnetes Ziel all dieser Arbeiten zu Zeta ist es letztlich weniger, ein perfektes Distinktivitätsmaß zu identifizieren, als ein justierbares Maß vorzuschlagen, bei dem in Abhängigkeit von Daten und Forschungsfragen dynamisch Parameter verändert und die resultierenden Verschiebungen in den Ergebnissen visualisiert werden können."
2018,DHd2018,HERRMANN_J__Berenike_Praktische_Tagger_Kritik__Zur_Evaluatio.xml,Praktische Tagger-Kritik. Zur Evaluation des POS-Tagging des Deutschen Textarchivs,"Berenike Herrmann (Universität Basel, Schweiz)","Korpus, POS, Tagger, DTA, Intercoder-Reliabilität","Annotieren, Bearbeitung, Stilistische Analyse, Kollaboration, Literatur, Text","Der vorliegende Beitrag leistet eine Tool- und Methoden-Kritik der automatischen Auszeichnung von Wortarten (Part of Speech-, bzw. POS-Taggern) an literarischen Texten des 19. und frühen 20. Jahrhunderts. Er geht über eine rein intellektuelle Reflektion hinaus, indem er erste Schritte einer empirischen Evaluation des POS-Tagging des Deutschen Textarchivs (DTA, Berlin-Brandenburgische Akademie der Wissenschaften) und seiner praktischen Verbesserung vorlegt. Aus der Perspektive der Digitalen Literaturstilistik und des Distant Reading sind Wortarten besonders interessante lexiko-grammatikalische Merkmale, ist ihre Verteilung doch ein wichtiger Indikator für Dimensionen wie Autorstil, Gattung und Register (z.B. Biber / Conrad 2009). POS sind vergleichsweise leicht und scheinbar valide zu bestimmen, gilt doch in der Computerlinguistik das Problem der automatischen Wortartenannotation als gelöst 'auch für das Deutsche, wo eine durchschnittliche Erkennungsgenauigkeit bei 95-97% liegt (vgl. Giesbrecht / Evert 2009). Für DH-Anwender scheint es also nahe zu liegen, ihre Korpora komfortabel mit out-of-the-box-Taggern zu annotieren, oder sich bereits annotierter Korpora zu bedienen, wie zum Beispiel des DTA. Ein genauerer Blick zeigt jedoch, dass Korpora der Geisteswissenschaft, historische wie literarische, von der sprachlichen Varietät abweichen, die den Sprachmodellen der verfügbaren Tagger zugrunde liegt, also Zeitungtexten der Gegenwart (der für das Deutsche frei verfügbare Goldstandard ist derzeit TIGER, ein Korpus von 900.000 Wörtern aus der Frankfurter Rundschau, vgl. Brants et al. 2004). In Nichtstandardvarietäten sinkt die Genauigkeit des POS-Taggings rapide (vgl. z.B. Scheible et al. 2011), und teilweise sind Aussagen über die Annotationsgenauigkeit mangels Referenzstandards gar nicht möglich. Dies betrifft auch das DTA, dessen POS-Tagging bislang nicht systematisch evaluiert wurde. Insgesamt ist die DH-Community also noch recht weit von einem Goldstandard für historische literarische narrative Texte des Zeitraums entfernt. Unser Beitrag leistet hier einen wichtigen Schritt, indem er erste Ergebnisse zur Einschätzung der Qualität ebenso wie zur Verbesserung des Annotationstools vorlegt. Ausgehend von dem Ziel unser Korpus der Literarischen Moderne (KOLIMO http://kolimo.uni-goettingen.de/) valide mit POS auszuzeichnen, haben wir eine Stichprobe (N= 9.065 ) des DTA manuell nachannotiert. Unsere Methode verbindet einen Tagger-Vergleich mit einer händischen Analyse. Dabei werden folgende Ziele verfolgt: Die Evaluation des POS-Taggings wurde durchgeführt auf einer randomisierten Stichprobe des DTA, die aufgrund unseres Forschungsinteresses auf narrative Texte mit Publikationsdatum ab 1800 beschränkt war, wobei sowohl fiktionale wie auch nicht-fiktionale Texte berücksichtigt wurden (ausschlaggebend waren die Metadaten zur Erstveröffentlichung und Gattung im Header des DTA). Die Grundgesamtheit der aus dem DTA entnommenen Stichprobe umfasste N= 64.924.458 Tokens, die der händisch annotierten Tokens umfasste n= 9.065 Tokens/POS-Tags, also 0,014%). Die Stichprobe wurde in ihrer tokenisierten und normalisierten Form aus dem DTA übernommen (vgl. DTA). Der Taggervergleich nutzte neben dem DTA-Tagger Das Tagging wurde durch vier studentische Hilfskräfte besorgt, wobei iterative Analysen und finale Annotation durch die PI betreut wurden. Mit einem Skript wurden csv-Tabellen erstellt, die die Tokens (fortlaufende Wortformen, inklusive Interpunktion) und POS-Tags in einem Dem automatischen wie händischen Tagging lag das Tagset des STTS (Schiller et al. 1999) zugrunde. Wo angezeigt, wurden im Projekt zusätzliche Regeln für der Handhabung des STTS-Manuals vereinbart und dokumentiert. Hierzu gehört auch u.a. die systematische Einbindung eines korpusbasierten Wörterbuchs (http://www.duden.de/) bei Eigennamen und Fremdwörtern. Das Tagging wurde in drei Phasen durchgeführt. Primäres Ziel des Taggings war es, die Genauigkeit von In Phase II wurden dieselben Coder, Tagger und dieselbe Software eingesetzt, ebenso wie die in Phase 1 erarbeiteten Tagging-Guidelines. Anders als in der ersten Phase wurden jedoch nicht ganze Sätze, sondern jeweils einhundert Token pro POS-Kategorie aus dem DTA extrahiert. Dadurch wurde eine Ungleichverteilung der einzelnen POS-Kategorien, welche in natürlichen Sätzen gegeben ist (vgl. Evert 2006; Kilgarriff 2005), vermieden. Für fünfundfünfzig POS-Kategorien des STTS wurden jeweils n=100 Wort/Token-POS-Paare sortiert nach STTS-Tag annotiert. Dies entspricht einer Grundgesamtheit von N=5.500 Tokens. Jedes Token wurde von zwei Codern unabhängig annotiert. In der anschließenden Diskussionsphase wurden die strittigen Fälle besprochen und finale Annotationen erarbeitet. Zu diesem Zweck wurden Statistiken für die Tags (über Coder und Tagger) analysiert und Nichtübereinstimmung der vergebenen Tags identifiziert. Für die statistische Evaluation wurde die Interrater-Reliabilität als Die Ergebnisse in Tabelle 1 basieren für Phase I auf den finalen Annotationen und für Phase II zum momentanen Zeitpunkt auf etwa der Hälfte der finalen Annotation. Sie zeigen für Tabelle 1: *Es handelt sich nicht um den Mittelwert von Phase I und II sondern um eine gewichtete Statistik, die die unterschiedlichen Stichprobengrößen (zum Zeitpunkt der Rechnung) einbezieht. Die Übereinstimmung zwischen Codern vor der Diskussion und Referenzstandard ist hingegen vergleichsweise hoch, auch wenn sie in der zweiten Tagging-Phase etwas abfällt ( Obwohl die Gesamtergebnisse noch ausstehen, könnte der Unterschied zwischen den Phasen tentativ damit erklärt werden, dass Phase II mehr problematische Tags annotiert, die eine niedrigere Distribution haben und im per-Satz-Tagging seltener auftreten. Für die einzelnen POS-Kategorien variiert die Genauigkeit zwischen 0% und 100%, wobei Tabelle 2: In den Gruppendiskussionen konnten Probleme identifiziert werden, die vornehmlich bei den Taggern lagen (z.B. bei Abkürzungen, Relativpronomen). Es traten aber auch Fälle auf, in denen die STTS-Guideline nicht präzise genug ist (z.B. bei Vergleichspartikeln, Possessivpronomen, Indefinitpronomina). Dabei war die Analyse der Disagreements eine produktive Heuristik, um (computer-)linguistisch und literarturwissenschaftlich interessante Fälle aufzuwerfen. So scheint gerade in literarischen Fällen eine Ambiguität (etwa zwischen Adjektiv und Verb bei Partizipien) geradezu intentional. Öhnlich Insgesamt zeigt unsere praktische Taggerkritik, dass auch eine scheinbar gelöste NLP-Aufgabe wie die Wortartenauszeichnung kein Solitär ist, auf den geistes- und literaturwissenschaftliche Projekte ohne genauere Prüfung bauen sollten. Unsere Ergebnisse zeigen trotz der hochqualitativen Vorverarbeitung des DTA eine Fehlerrate von ca. 9% an, die allerdings stark nach POS-Tag variiert. Die diachrone wie synchrone Heterogenität des literarischen Diskurses führt generische POS-Tagger bislang fast zwangsläufig an ihre Grenzen, durch historische Sprachformen, aber auch die Vielfalt der Gattungen, Erzähltechniken und kreative Lexik und Syntax. Zukünftig bieten sich hier wohl zwei Wege an: zum einen die fortlaufende Verbesserung von generischen Tools, zum anderen gerade aber auch die Feinabstimmung der Tools für spezifische Anwendungen, mit flexibel ansprechbaren Tagging- und Sprachmodellen. So haben wir unsere Annotation an"
2018,DHd2018,DIMPEL_Friedrich_Michael_Die_guten_ins_Töpfchen__Zur_Anwendb.xml,"Die guten ins Töpfchen: Zur Anwendbarkeit von Burrows"" Delta bei kurzen mittelhochdeutschen Texten nebst eines Attributionstests zu Konrads ""Halber Birne""","Friedrich Michael Dimpel (FAU Erlangen-Nürnberg, Deutschland, Germanistik, und TU Darmstadt, Computerphilologie und Mediävistik)","Stilometrie; Autorschaftsattribution, Quantitative Verfahren","Stilistische Analyse, Visualisierung, Sprache, Literatur, Methoden","Die Anwendbarkeit von Burrows"" Delta (Burrows 2002) als Autorschaftstest für das Deutsche ist in Validierungstestreihen wiederholt eindrucksvoll demonstriert worden (Büttner et alia 2017, Eder 2013a/b, Evert et alia 2015. Evert et alia 2016); auch im Mittelhochdeutschen ist Delta anwendbar (Dimpel 2016/2018). Die Stabilität des Verfahrens wurde in Noise-Tests belegt: Wenn man etwa 12% aller Wörter durch Fremdmaterial austauscht, sinkt die Erkennungsquote kaum (Dimpel 2017a/2018). Bei nicht-normalisierten mittelhochdeutschen Texten steigt die Erkennungsquote in einem Validierungstest von 80% auf 91%, wenn man die bei Evert et alia (2016) entwickelte Methode der Z-Wert-Begrenzung mit einem von mir zusammengestellten Normalisierungswörterbuch kombiniert (Dimpel 2017a). Kontraintuitiv ist, dass nur die Kombination dieser Optimierungsverfahren zu einer Verbesserung um 11% führt, während in diesem Setting nur der Einsatz der Z-Wert-Begrenzung zu einer minimalen Verschlechterung führt; der Einsatz nur des Normalisierungswörterbuchs führt nur zu einer Verbesserung um 5,6%. Dieser Befund wird unter dem Stichwort ""Delta-Rätsel"" in einem Dariah-de-Working-Paper (Dimpel 2017b) ausführlich analysiert. Bei der Rätsel-Analyse wurde 'ein Serendipitätseffekt 'eine Möglichkeit entdeckt, wie man bei einem konkreten Vergleich von drei Texten die Wortformen identifizieren kann, die eine korrekte Autorschaftserkennung begünstigen oder behindern 'dazu im Weiteren. Beim Delta-Test berechnet man aus den Wortfrequenzen für ein Korpus jeweils die zugehörigen Z-Werte. Beim Vergleich von zwei Wortformen aus zwei Texten wird die Differenz der jeweiligen Z-Werte gebildet und der Betrag dieser Differenz genommen. Delta ist schließlich der Mittelwert der absoluten Z-Wert-Differenzen für alle Wortformen. Abb. 1. zeigt oben die Z-Werte der Handschrift M von Wolframs ""Parzival"" in einem Test, in dem sich im Vergleichskorpus neben Wolframs ""Willehalm"" noch weitere 19 Distraktortexte von anderen Autoren befinden (ausführlich zum Testverfahren Dimpel 2017b). Der ""Parzival"" soll dem Autor-Vergleichstext (Wolframs ""Willehalm"") zugeordnet werden und nicht etwa Konrads ""Partonopier"". Im oberen linken Viertel sind positive Z-Werte blau aufgetragen und nach der Höhe der Z-Werte sortiert. Ab der Stelle, an der die blauen Balken auf 0 zurückgehen, folgt rechts der Betrag der negativen Z-Werte (blau). Unten stehen (orange) die absoluten Z-Wert-Differenzen zwischen dem Ratetext und dem Autor-Vergleichstext (Differenzen der Z-Werte von Wolframs ""Parzival"" und Wolframs ""Willehalm""). Man könnte A) den Verdacht haben, dass Wortformen bei hohen blauen Balken ""gut"" sind, um einen Text von Distraktortexten zu unterscheiden, da hohe Z-Werte auf erhebliche Abweichung von den übrigen Korpusfrequenzen hindeuten. Man könnte auch B) den Verdacht haben, dass Wortformen bei hohen orangen Balken ""schlecht"" für die Autorerkennung sind: Unterschiede zwischen dem Ratetext und Autor-Vergleichstext (also Unterschiede von zwei Texten des gleichen Autors) sollten eher niedrig sein, damit die Erkennung funktioniert. Allerdings sind bei hohen blauen Balken relativ oft auch hohe orange Balken vorhanden 'auch in anderen Tests (Dimpel 2017b). Dieses Diagramm erlaubt also keine Aussage darüber, welche Wortformen gut für die Autorerkennung sind; hohe Z-Werte allein erlauben noch keine Aussage darüber, ob ein Wort hier gut geeignet ist, um einen Autor zu charakterisieren. Neu ist in Abb. 2 nur die obere Hälfte: Sie enthält Z-Wert-Differenzen des Ratetexts zum Distraktortext (""Partonopier""). Diese grauen Unterschiede sollten bei funktionierender Autorerkennung eher groß sein; gleichzeitig sollten die orangen Unterschiede der Texte vom gleichen Autor niedriger sein als die grauen. Dort, wo die grauen Balken genauso hoch sind wie die orangen, hilft das Wort nicht bei der Autorerkennung 'dies ist bei sehr hohen positiven Z-Werten der Fall. Sind die orangen Balken höher als die grauen, stört die Wortform die Autorerkennung: Die Differenzen zwischen Texten verschiedener Autoren müssen größer sein als die Differenzen zwischen Texten gleicher Autoren, wenn die Autorschaftserkennung funktioniert. Die Differenz zwischen orange und grau sei ""Level-2-Differenz"" genannt: ""Differenz aus der Z-Wert-Differenz zwischen Ratetext und Distraktortext einerseits und der Z-Wert-Differenz zwischen Ratetext und Autor-Vergleichstext andererseits"". Bei positiven Level-2-Differenzen ist eine Wortform vorteilhaft für die Autorerkennung 'mit Blick auf den einen untersuchten Distraktortext. Bei negativen Level-2-Differenzen ist die Wortform schlecht für die Autorerkennung. Über diese Differenz kann man ""gute"" und ""schlechte"" Wortformen einzeln identifizieren. Konrads Autorschaft wurde der ""Halben Birne"" trotz Selbstnennung im Epilog ( Die stilometrische Analyse ist in mehrfacher Hinsicht eine Herausforderung: Eine gattungsübergreifende Attribution ist mangels anderer Vergleichstexte nötig (nach Schöch 2014 wäre eine Gattungsmischung möglichst zu meiden). In Konrads Oevre herrscht eine Vielfalt an Themen, Frivoles wie in der ""Halben Birne"" ist eher selten 'auch im einzigen anderen Märentext Konrads: im ""Herzmäre"" bleibt die Liebe unerfüllt, es kommt zum doppelten Minnetod. Zudem ist die ""Halbe Birne"" recht kurz: sehr gute Quoten erreicht Delta ab 5.000 Wortformen in einer Bag-of-Words (vgl. Abb. 3 sowie Eder 2013a und Eder 2013b). Die ""Halbe Birne"" enthält jedoch nur 2.469 Wortformen. Wenn man nun die ""Birne"" gegen ein Konrad-Korpus testet, kann man entweder die Wörter mit hoher Level-2-Differenz, die einer Erkennung von Konrad entgegenstehen, aus der Liste der untersuchten Most-Frequent-Words (MFWs) streichen. Oder man kann eine Positivliste mit ""guten"" Wörtern bevorzugt verwenden 'Wörter mit hoher positiver Level-2-Differenz. Vorab wird das Verfahren validiert: In einer Ermittlungsgruppe (vier Konrad-Texte) werden ""gute"" und ""schlechte"" Wörter identifiziert. ""Gute Wörter"": Level-2-Differenzen >+2,31 in 6 von 7 Ermittlungsgruppen-Ratetexten, 304 items ""Schlechte Wörter"": Level-2-Differenzen <-1,2 in 2 von 7 Ermittlungsgruppen-Ratetexten, 174 items Im Attributionstest 1 wird die ""Halbe Birne"" als Autor-Vergleichstext verwendet, als Ratetexte werden die acht Konrad-Texte sowie das ""Herzmäre"" verwendet; im ""Herzmäre""-Test bleibt es bei acht Konrad-Ratetexten; das ""Herzmäre"" ist Autor-Vergleichstext. Hier erreicht das ""Herzmäre"" nur 4,5%, ein schlechter Wert, obwohl hier die Autorschaft nicht infrage gestellt wurde. Dagegen liegt die Erkennungsquote bei der ""Halben Birne"" auch ohne zusätzliche Wortlisten bereits über dem Zufallswert: Wenn ein Konrad-Text aus dem Ratekorpus nun nicht einem der 20 Texte von anderen Autoren zuordnet wird, sondern der ""Halben Birne"", dann stehen die Chancen dafür 1 zu 21. Wenn es also auf den Zufall zurückzuführen wäre, dass ein Text dem richtigen Autor zugeordnet wird, dann müsste die Erkennungsquote bei 5% liegen 'so beim ""Herzmäre"". 83,8% bei der ""Halben Birne"" sind ein ordentlicher Wert, wenn man bedenkt, dass nur kurze Bag-of-Words mit 2.000 Wortformen getestet werden können und dass gattungsübergreifend getestet wird. Beim Attributionstest 1 befand sich die ""Halbe Birne"" im Vergleichskorpus. Im Ratekorpus waren inklusive ""Herzmäre"" 9 Konrad-Texte. Nun werden umgekehrt ""Halbe Birne"" bzw. ""Herzmäre"" als Ratetexte verwendet. Ins Vergleichskorpus gebe ich zu den 20 Distraktortexten in separaten Tests jeweils einen Konrad-Text als Autor-Vergleichstext ins Vergleichskorpus. Attributionstest 2: Im Attributionstest 2 übersteigen die meisten Werte 86%. Es gibt lediglich zwei deutliche Ausreißer, an denen jeweils das ""Herzmäre"" beteiligt ist. Dieses Minneleid-und-Minnetod-Märe fügt sich nicht zur politischen Propagandadichtung ""Turnier von Nantes"". Auch zur ""Halben Birne"" passt das ""Herzmäre"" nicht: Dort geht es um eine Dame, die einen Ritter abweist, weil er beim Birnenverzehr keine Tischmanieren an den Tag legt. Die Dame schläft mit einem vermeintlich taubstummen Hofnarren, der sich jedoch später als der abgewiesene Birnen-Ritter entpuppt. Interessante Fehlattributionen (etwa ""Birne"" zu ""Häslein"" statt zum ""Herzmäre"") werden im Vortrag vorgestellt. Als Katharina Zeppezauer-Wachauer (Salzburg) mir einige Mären aus der Mittelhochdeutschen Begriffsdatenbank überlassen hat (vielen Dank dafür!), hat sie notiert: ""Vielleicht können Sie ja wirklich, wie Edith Feistner gefordert hat, ""Konrad seine Birne wiedergeben""!"" Auch wenn die Zahlen in beiden Attributionstests trotz der geringen Textlänge und trotz der Gattungsproblematik überraschend eindeutig sind, möchte ich bei einer vorsichtigen Interpretation bleiben. Zwar ist die Wahrscheinlichkeit sehr gering, dass die gefundene Nähe der ""Halben Birne"" zum Konrad-Korpus auf dem Zufall beruht. Allerdings wären ""Kontrollpeilungen"" (Eibl 2013) wünschenswert: Eine Attribution sollte nicht auf einem einzelnen Test mit einer Methode erfolgen, wünschenswert wären Bestätigungen mit anderen Methoden. Immerhin aber geht es hier nicht um eine blinde Attribution, sondern lediglich um Widerspruch gegen eine Athetese der Forschung. Eine Attribution stünde in Einklang mit Konrads Selbstnennung in fünf von sieben überlieferten Textzeugen. Zudem würde ich den Test gerne mit einem größeren Mären-Korpus wiederholen, in dem idealerweise längere Texte wären und mehr Texte, die näher an Konrads Schaffenszeit liegen. Dass die Birne nicht zu Kaufringer clustert, könnte auch dem zeitlichen Abstand geschuldet sein, der durch gemeinsame groteske oder frivole Inhaltselemente nicht überlagert wird. Wichtig ist mir auch das Verfahren: Bislang ist eine Feature-Eliminierung oder Feature-Selektion häufig auf dem Weg des maschinellen Lernens erfolgt (Büttner et alia 2016) 'mit dem Nachteil, dass der Weg der Kategorisierung teilweise im Dunklen bleibt. Ermittelt man ""gute"" oder ""schlechte"" Wörter via Level-2-Differenzen, so ist transparent, wie man zu den Parametern kommt und wie auf dieser Basis die weiteren Berechnungen erfolgen."
2018,DHd2018,PROISL_Thomas_Delta_vs__N_Gram_Tracing__Wie_robust_ist_die_A.xml,Delta vs. N-Gram-Tracing: Wie robust ist die Autorschaftsattribuierung?,"Thomas Proisl (Friedrich-Alexander-Universität Erlangen-Nürnberg, Deutschland); Stefan Evert (Friedrich-Alexander-Universität Erlangen-Nürnberg, Deutschland)",Autorschaftsattribuierung,"Stilistische Analyse, Sprache, Methoden, Text","Die Autorschaftsattribuierung, also die Zuweisung von Texten unbekannter oder umstrittener Autorschaft zu ihrem wahren Autor, hat vielfältige Anwendungen beispielsweise in der Literatur- und Geschichtswissenschaft oder der forensischen Sprachwissenschaft. Eine populäre Methode zur Autorschaftsattribuierung ist die Anwendung von Deltamaßen (Burrows 2002; Argamon 2008) wie zum Beispiel Cosine-Delta (Smith und Aldridge 2011). Deltamaße verwenden die n häufigsten Wörter im Korpus, standardisieren die Frequenzen auf z-Werte und wenden ein Abstandsmaß, im Fall von Cosine-Delta den Kosinusabstand, an. Typischerweise schließt sich die Anwendung eines (hierarchischen) Clusterverfahrens an, das Texte des selben Autors zusammengruppiert. Eine neue Methode zur Autorschafts-attribuierung ist das sogenannte N-Gram-Tracing (Grieve et al., in Begutachtung). Hierbei werden aus dem zu klassifizierenden Text alle Wort- oder Buchstaben-N-Gramme einer bestimmten Länge extrahiert. Der Text wird dann dem Autor zugewiesen, der im Vergleichskorpus die meisten dieser N-Grammtypen verwendet. Die Häufigkeit der N-Gramme spielt dabei keine Rolle, es geht nur darum, wie viele N-Gramme aus dem zu klassifizierenden Text auch im Vergleichskorpus auftauchen. Wenn Methoden zur Autorschaftsattribuierung angewandt werden sollen um tatsächlich eine strittige Autorschaftsfrage zu klären, ist es sehr wichtig die Zuverlässigkeit und Robustheit der Verfahren abschätzen zu können, schließlich gibt es eine ganze Reihe von Einflussfaktoren. Kritisch sind zum Beispiel die folgenden Fragen: Welchen Einfluss haben die Länge des zu klassifizierenden Textes und die Größe des Vergleichskorpus auf die Genauigkeit der Autorschaftsattribuierung? Gibt es für die beiden Verfahren eine Mindesttextlänge, die nicht unterschritten werden sollte? Wie stark werden die Verfahren durch autor- und werkspezifische Eigenheiten beeinflusst? Ist die Genauigkeit der Autorschaftsattribuierung robust in Bezug auf die Zusammensetzung des Vergleichskorpus oder kann die Auswahl der Autoren und Texte das Ergebnis beeinträchtigen? Um diese Fragen zumindest teilweise beantworten zu können, führen wir eine Reihe von Evaluationsexperimenten durch. Um die Ergebnisse des N-Gram-Tracings besser mit denen von Delta vergleichen zu können, führen wir auf den Deltaabständen zwischen den Texten kein Clustering sondern eine nearest-neighbor-Klassifikation durch, d.h. wir weisen den zu klassifizierenden Text dem Autor des Textes mit dem geringsten Abstand zu. Im Einzelnen handelt es sich um zwei Kürzungs- und zwei Samplingexperimente. Datengrundlage für die Kürzungsexperimente sind die deutschen, englischen und französischen Romankorpora, die unter anderem von Jannidis et al. (2015) und Evert et al. (2017) verwendet wurden. Jedes Korpus besteht aus je drei Romanen von 25 Autoren, also aus 75 Romanen. Für das erste Kürzungsexperiment wird die Größe des Vergleichskorpus stabil gehalten und nur der zu klassifizierende Text gekürzt. Für Delta wird zusätzlich die Anzahl der verwendeten häufigsten Wörter variiert. Im zweiten Kürzungsexperiment werden sowohl der zu klassifizierende Text als auch das Vergleichskorpus gekürzt. Über ein leave-one-out-Verfahren werden alle Texte im Korpus klassifiziert um die Genauigkeit der Verfahren zu ermitteln. Für die Samplingexperimente verwenden wir eine Sammlung von 1018 deutschen Romanen aus dem langen 19. Jahrhundert. Alle Texte wurden von Muttersprachlern verfasst. Für das erste Samplingexperiment ziehen wir 5000 zufällige Stichproben von 25 Autoren und je drei Romanen (die Zusammensetzung der einzelnen Stichproben ist also vergleichbar mit den oben erwähnten Romankorpora). Für das zweite Samplingexperiment beschränken wir uns auf die 25 Autoren, die in unserer Sammlung mit den meisten Romanen vertreten sind, und ziehen 5000 zufällige Stichproben von je drei Romanen pro Autor (also ebenfalls 25√ó3 Texte). Für jede Stichprobe ermitteln wir über ein leave-one-out-Verfahren die Genauigkeit der beiden Verfahren. Aus Platzgründen berichten wir an dieser Stelle nur knapp die Ergebnisse des ersten Kürzungsexperiments und des ersten Samplingexperiments und beschränken und dabei auf auf die deutschen Daten. Die Ergebnisse des ersten Kürzungsexperiments, in dem nur der zu klassifizierende Text gekürzt wird, sind in der folgenden Abbildung dargestellt:  Wir vergleichen Cosine-Delta auf Basis der 3000 häufigsten Wörter mit N-Gram-Tracing auf Basis von Wort-1-bis-3-Grammen, Zeichen-4-bis-10-Grammen und Zeichen-11-Grammen. Bis zu einer Textlänge von 5000 Tokens liefern alle N-Gram-Tracing-Varianten bessere Ergebnisse als Delta, für längere Texte funktionieren Delta und N-Gram-Tracing auf Wort-N-Grammen am besten. Bei weniger als 2000 Tokens brechen die Ergebnisse für Delta ein, bei weniger als 1000 Tokens auch die für N-Gram-Tracing. Die Ergebnisse des ersten Samplingexperiments zeigen, dass die Klassifikationsgenauigkeit bei beiden Verfahren großen Schwankungen unterworfen ist. Hier die Ergebnisse für Cosine-Delta:  Die Grafik zeigt, dass ab ca. den 1000 häufigsten Wörtern zwar im Mittel eine Klassifikationsgenauigkeit von rund 85% erreicht wird, allerdings mit enormen Schwankungen zwischen knapp über 60% und knapp unter 100%. Die Ergebnisse für N-Gram-Tracing auf Basis von Wort-N-Grammen sehen ähnlich aus:  Durch die Kombination von Wort-1- bis Wort-3-Grammen wird zwar eine mittlere Klassifikationsgenauigkeit von über 70% erreicht, aber auch hier mit enormen Schwankungen. Die Ergebnisse zeigen, dass N-Gram-Tracing auf kurzen Texten besser funktioniert als Cosine-Delta, allerdings werden für beide Verfahren längere Texte benötigt, als häufig verwendet werden. Die Wahl der Autoren im Vergleichskorpus und auch, wie das Poster zeigen wird, die Wahl der einzelnen Werke haben einen enormen und schwer vorhersehbaren Einfluss auf die Qualität der Autorschaftszuschreibung, deren Genauigkeit ohne weiteres um 20 Prozentpunkte schwanken kann. Im Licht dieser Erkenntnisse ist es durchaus fraglich, wie valide und generalisierbar bisherige Forschungsergebnisse auf dem Gebiet der Autorschaftsattribuierung sind."
2018,DHd2018,HENNY_KRAHMER_Ulrike_Alternative_Gattungstheorien__Das_Proto.xml,Alternative Gattungstheorien: Das Prototypenmodell am Beispiel hispanoamerikanischer Romane,"Ulrike Henny-Krahmer (Universität Würzburg, Deutschland); Katrin Betz (Universität Würzburg, Deutschland); Daniel Schlör (Universität Würzburg, Deutschland); Andreas Hotho (Universität Würzburg, Deutschland)","Literarische Gattungen, Prototypentheorie, Topic Modeling, MFW, Hispanoamerikanischer Roman","Inhaltsanalyse, Modellierung, Stilistische Analyse, Literatur, Methoden, Text","Die Definition von Gattungen sowohl im Sinne allgemeiner Gattungskonzepte als auch konkreter einzelner Gattungen ist ein altes und nach wie vor zentrales Problem der Literaturwissenschaft und wird immer noch debattiert (Kayser 1956: 330-387, Zymner 2003). Allgemein können Gattungsbegriffe als Sammelbegriffe verstanden werden, deren Aufgabe es ist, zu beschreiben, in welcher Hinsicht Texte zu Textgruppen zusammengefasst werden können. Oft ist dabei auf das Konzept von Gattungen als logischen Klassen zurückgegriffen worden, auch in vielen Untersuchungen zu literarischen Gattungen im Bereich der Digital Humanities, obwohl in der literaturwissenschaftlichen Gattungstheorie bereits seit den 60er-Jahren andere Vorschläge für das Verständnis von Gattungskategorien gemacht worden sind. Im Sinne der ""Kritik der digitalen Vernunft"" ist das Ziel dieses Beitrags, die Problematik der Gattungsbeschreibung auf der theoretischen Basis alternativer Gattungstheorien und mit Hilfe informatischer Mittel aus einer neuen Perspektive zu betrachten. Dazu wird exemplarisch die Anwendbarkeit des Prototypenmodells als Gattungskonzept für digitale gattungsstilistische Studien überprüft. Als Testkorpus dient eine Sammlung von Texten aus der hispanoamerikanischen Romanliteratur des 19. Jahrhunderts. Viele Gattungstheorien beruhen auf der Grundannahme, dass sich konkrete Texte anhand von hinreichenden und notwendigen Attributen eindeutig disjunkten Klassen zuordnen lassen und oft auch, dass sich für Gattungen eine Taxonomie entwickeln lässt (vgl. Zymner 2003: 102-104). Allerdings weisen nicht alle Texte einer bestimmten Gattung gemeinsame Merkmale auf, noch sind die Beziehungen zwischen den einzelnen Gattungen und Texten statisch. Zudem gibt es die Vorstellung von Werken, die initiale, prägende Wirkung haben (z. B. In den Digital Humanities gibt es bereits einige Untersuchungen zu literarischen Gattungen, bei denen jedoch üblicherweise die Annahme zugrunde liegt, dass Gattungen als Klassen im logischen Sinn zu verstehen sind (Calvo Tello et al. 2017, Hettinger el al. 2016a, Hettinger et al. 2016b, Schöch et al. 2016, Schöch 2015, Schöch 2013). Im vorliegenden Beitrag wird exemplarisch dargestellt, inwiefern es möglich ist, das Prototypenmodell zu verwenden, um Einsichten in die Anordnung maschinell gruppierter Texte zu gewinnen, die über den klassischen Ansatz der Klassifikation nicht möglich wären. Der Beitrag leistet damit zum einen, einen konkreten Vorschlag für die Formalisierung des Prototypen-Ansatzes für gattungsstilistische Untersuchungen zu machen. Zum anderen zielt er darauf, durch die Berücksichtigung der internen Strukturierung von Gattungskategorien eine bessere Anbindung der computergestützten Verfahren an literaturgeschichtliche Forschungsergebnisse zu ermöglichen.  Für die Zuordnung der Romane zu den verschiedenen Untergattungen wurde einschlägige Sekundärliteratur ausgewertet. Abb. 1 und 2 zeigen die Verteilung der Romane über die Zeit nach Untergattungen sowie nach Ländern. Bestandteil des Korpus sind neben hispanoamerikanischen Romanen auch fünf Texte aus Spanien sowie je ein Text aus England und Frankreich (in spanischer Übersetzung), welche aufgrund ihres Prototypen-Status einbezogen wurden. Das Korpus umfasst insgesamt 2,3 Mio. Token. Beim historischen Roman und beim Liebesroman handelt es sich bei den Prototyp-Texten um Übersetzungen der ursprünglich auf englisch und französisch verfassten Romane, die auch in Hispanoamerika als Vorbilder für diese Romantypen eingeschätzt werden. Kontroverser wird diskutiert, welche Texte Einfluss auf die kostumbristischen Romane ausgeübt haben. Auf der einen Seite wird der mexikanische Autor Lizardi als Pionier genannt, auf der anderen Seite werden Romane spanischer Autoren (Caballero, Alarcón, Valera, Pereda) angeführt (Calderón 2005). Eine Prototypenanalyse könnte hier Argumente für eine der beiden Thesen liefern. Im Falle des Gaucho-Romans und des Antisklaverei-Romans sind die gesetzten Prototypen als Höhepunkte der jeweiligen Gattung zu verstehen, da die weiteren diesen Untergattungen zugeordneten Romane im Korpus entweder als Vorstufen oder Nachfolger der besonders repräsentativen Gattungsvertreter beschrieben worden sind (Lichtblau 1959: 121-135, Rivas 1990).  Das Topic Modeling (vgl. Blei 2012) wurde mit MALLET  Die zweite Dokument-Repräsentation wurde auf Basis der 10.000 häufigsten Wörter (MFW) in den Roman-Volltexten erstellt (nicht lemmatisiert, gewichtet mit TF-IDF, maximale Dokument-Frequenz von 90 %). Anhand dieser beiden Repräsentationen kann überprüft werden, welche Merkmale zentral für die Abbildung von Gattungsunterschieden zwischen den Texten sind. Die exemplarische Anwendung des Prototypenmodells auf die hispanoamerikanischen Romane verschiedener Untergattungen hat gezeigt, dass Ansätze zur Modellierung literarischer Gattungen, die über das Prinzip logischer Klassen hinausgehen, informatisch umgesetzt werden können. Fragen wie diejenige nach der Prototypizität einzelner Texte, Gattungsmischungen und nach differenzierten Nähe- und Distanzverhältnissen lassen sich erst auf dieser Grundlage angehen. Literaturgeschichtliche Aussagen zu Gattungszugehörigkeiten und Gattungsentwicklungen mit prototypensemantischem Bezug (wie hier die Entwicklung des Gaucho-Romans oder die Frage nach den die hispanoamerikanische Tradition prägenden kostumbristischen Romanen) können so hinsichtlich relevanter Textmerkmale genauer untersucht werden. Künftig sollen weitere theoretische Gattungsmodelle, insbesondere das Prinzip der Familienähnlichkeit, auf ihre Anwendbarkeit für gattungstilitische Untersuchungen hin getestet werden. Außerdem soll geprüft werden, welche Ergebnisse andere Textrepräsentationen als Topic-Modelle und MFW liefern, z.B. Word Embeddings. Zu diskutieren bleibt, wie offene Kategorisierungsmodelle evaluiert werden können."
2018,DHd2018,LAUBROCK_Jochen_Computationale_Beschreibung_visuellen_Materi.xml,Computationale Beschreibung visuellen Materials am Beispiel des Graphic Narrative Corpus,"Jochen Laubrock (Universität Potsdam, Deutschland); David Dubray (Universität Potsdam, Deutschland); André Krügel (Universität Potsdam, Deutschland)","Neuronale Netze, Bildverarbeitung, Graphic Novels","Modellierung, Annotieren, Stilistische Analyse, Bilder, Text","Die digitale Revolution in den Geisteswissenschaften hat diesen eine Reihe neuer Methoden eröffnet. Durch die heute verfügbaren großen Datenmengen und intelligenten Algorithmen haben sich zwar einige bisher offengeliebene geisteswissenschaftliche Kernfragen beantworten lassen, jedoch wird mancherorts eine Methodenfokussiertheit und Theoriemangel kritisiert (Gumbrecht, 2014). Ob die Digitalen Geisteswissenschaften zu einer darüber hinausgehenden tiefergreifenden Veränderung der geisteswissenschaftlichen Erkenntnis führen werden, bleibt abzuwarten; derzeit scheint es, als werde das Potenzial der neuen methodischen Zugänge erst noch ausgelotet. In anderen Wissenschaften hat aber die Verfügbarkeit computationaler Modelle und der damit einhergehende Zwang, implizite Annahmen zu explizieren und Theorien formal testbar zu machen, signifikant zur Theoriebildung und -prüfung beigetragen (cf. Myung & Pitt, 2002; Lewandowsky & Farrell, 2011). Deshalb besteht die begründete Hoffnung, dass computationale Modellierung auch die Geisteswissenschaften bereichern wird. Ein Großteil der Forschung in den digitalen Geisteswissenschaften beschäftigt sich mit Text. Es gibt hier eine fruchtbare interdisziplinäre Zusammenarbeit von Literaturwissenschaften und Computerlinguistik; im Umfeld des ""Distant Reading"" sind umfangreiche Werkzeuge entstanden, mit denen sich etwa stilometrische Analysen oder Topic Modeling computergestützt vornehmen lassen (Blei, 2012; Juola, 2006). Auch netzwerkanalytische Methoden aus der theoretischen Physik und computationalen Soziologie haben hier interessante neue Perspektiven eröffnet (Schich et al., 2014). Dagegen ist die digitale Analyse visuellen Materials noch relativ wenig entwickelt oder standardisiert, obwohl dieses für Disziplinen wie z.B. Kunstgeschichte oder Archäologie von zentralem Interesse ist. In den letzten Jahren wurden durch Entwicklungen im Bereich der Convolutional Neural Networks (CNN) die Möglichkeiten automatisierter Bildanalyse revolutioniert. Während in klassischen Ansätzen der maschinellen Bildverarbeitung ein hohes Ausmaß an Expertenwissen notwendig war, um Merkmale zu definieren, mit denen sich das Material sinnvoll beschreiben ließ (""engineered features""), lernen CNNs die Merkmale durch Fehlerrückführung (Backpropagation) selbst. Convolutional Neural Networks sind eine besondere Klasse künstlicher neuronaler Netze, die sich durch eine 2D-Anordnung der Neuronen, innerhalb einer Schicht geteilte Gewichte und lokale Konnektivität auszeichnen. Sie eignen sich insbesondere für die Analyse von Bildmaterial. Die Netze sind typischerweise auf einer großen Anzahl von Fotos in Objektklassifikationsaufgaben trainiert worden, dabei bilden sich auf verschiedenen Ebenen der CNNs Repräsentationen aus, die denen im menschlichen visuellen System ähnlich sind. Neuronen auf niedrigen Ebenen des Netzwerks haben oft eine Filterantwort, die relativ einfache Merkmale kodiert, vergleichbar z.B. mit Kantendetektoren im frühen visuellen Kortex, während Neuronen auf höheren Ebenen recht komplexe Merkmale kodieren können, z.B. Texturen oder Teile von Gesichtern. Da diese Merkmale relativ generisch sind, ist zu erwarten, dass Transfer auf neuartiges Material gelingt. Es sind heute einige derart vortrainierte Netzwerke verfügbar, die sich mit relativ wenig Aufwand an neues Material anpassen lassen. Der Vergleich der Gewichte für verschiedene Materialtypen erlaubt dann auch Rückschlüsse über deren Unterschiede. Generalisieren die auf Fotos vortrainierten Netzwerke auch auf zeichnerisches Material? Wir berichten von Experimenten, in denen wir das Material des Graphic Narrative Corpus (GNC, Dunst et al., 2017) mit CNNs beschreiben. Der Graphic Narrative Corpus repräsentiert das erste digitale Korpus von englischsprachigen Graphic Novels mit derzeit 130 Titeln. Die ersten Kapitel dieser Werke werden von menschlichen Kodierern annotiert, dabei werden u.a. die Identität und der Ort zentraler Charaktere, Orte von Panels, Sprechblasen und Textboxen (Captions) und Onomatopeia sowie der Text selbst notiert. Außerdem werden Blickbewegungen von Lesern erhoben (Eye-Tracking), um Aufschluss über die Aufmerksamkeitsverteilung auf Seite der Rezipienten zu erhalten. Die Beschreibung des GNC mit CNNs hat verschiedene Ziele. Erstens erhoffen wir uns Aufschluss über stilistische Unterschiede zwischen Werken und Genres, z.B. mittels Berechnung von Distanzmaßen basierend auf den Modellparametern. Allgemeiner könnte so der Weg zu einer visuellen Stilometrie aufgezeigt werden, die auch für inhaltliche Bereiche außerhalb der Graphic Novels relevant ist, etwa im Sinne einer computationalen Kunstgeschichte (Saleh & Elgammal, 2015; Manovich, 2015). Zweitens ermöglicht die Beschreibung mit Hilfe der Merkmale tiefer CNNs durch sogenannte Region Proposal Networks (Girshick et al., 2013) die Detektion von Objektklassen. Beispielsweise könnten sich Sprechblasen oder handelnde Charaktere lokalisieren lassen. Wenn Klassen von Objekten automatisiert lokalisiert werden können, erleichtert dies die Arbeit der Annotatoren sehr. Die Ergebnisse können also zurück in das Annotationswerkzeug fließen, um eine Teilautomatisierung zu ermöglichen. Drittens ist aus kognitionspsychologischer Perspektive interessant, welche Merkmale die Aufmerksamkeit auf sich ziehen. Die Korrelation der Netzwerkbeschreibung mit den Blickbewegungsdaten ermöglicht eine Modellierung der Aufmerksamkeitssteuerung auf einem deutlich höheren Auflösungsgrad als die subjektive Beschreibung. Für die Modellierung des Materials nutzen wir die Architektur VGG der Visual Geometry Group in Oxford (Simonyan & Zisserman, 2014), insbesondere VGG-16 und VGG-19. Diese Wahl ist motiviert durch die Einfachheit der Architektur, die die Interpretation der Gewichte erleichtert. Das zugrundeliegende Netzwerk lässt sich aber prinzipiell austauschen; andere Architekturen wie ResNet (He et al., 2015) oder Inception (Szegedy et al., 2015) sind denkbar und sollten ähnlich gute Ergebnisse liefern. Für die Vorhersage der Aufmerksamkeitsverteilung der Leser nutzen wir die Architektur Deep Gaze II (Kümmer et al., 2016). Deep Gaze II ist ein neuronales Netz, das auf VGG-19 aufsetzt und die Antwort einiger dessen Schichten nutzt, um ""empirische Salienz"" vorherzusagen. Empirische Salienz ist operationalisiert durch Messung von Mauspositionen beim Aufdecken eines verschwommenen Bildes bzw. Messung von Blickbewegungsdaten beim Betrachten von Fotos natürlicher Szenen. Die Fotos sind andere, als die für das Training von VGG-19 benutzten. Man beachte, dass sowohl VGG-19 als auch Deep Gaze II auf Fotos trainiert wurden, also nie Graphic Novels gesehen haben. Da sie jedoch Merkmale und Gewichte herausgebildet haben, die für die Interpretation (von Bildern) der menschlichen Umwelt nützlich sind, kann man vermuten, dass sie sich auch für die Analyse von Zeichnungen eignen. Zwar sind Zeichnungen Abstraktionen, haben aber als solche einen Bezug zur visuellen (Photo-)Realität. Die Ergebnisse zeigen, dass sich mit Hilfe von Neuronen auf höheren Ebenen der tiefen CNNs recht gut bestimmte Klassen von Objekten lokalisieren lassen. Beispielsweise eignen sich einige Kombinationen von Merkmalen zuverlässig als Sprechblasendetektoren (Abb. 1). Dies ist insofern bemerkenswert, als die Detektion von Sprechblasen sich für klassische Ansätzen der maschinellen Bildbverarbeitung als schwieriges Problem dargestellt hat (Rigaud et al., 2013). Auch für die Erkennung gezeichneter Gesichter eignen sich CNNs, allerdings ist hier ein Training auf Ansichten in verschiedenen Perspektiven (Frontal, Profil) notwendig. Und schließlich lässt sich die empirische Fixationsverteilung mit Deep Gaze II insgesamt sehr überzeugend reproduzieren (Abb. 2). Die CNN-Features kodieren also aufmerksamkeitsrelevante Merkmale. Insgesamt eigenen sich auf Fotos trainierte CNNs schon ohne spezifisches weiteres Training recht gut zur Beschreibung gezeichneten Materials in Graphic Novels. Die ""objektive"" Beschreibung eröffnet vielfältige Anwendungen. Einerseits kann, wie oben skizziert, die Annotation visuellen Materials durch Nutzung von vorgeschlagener Regionen deutlich erleichtert werden, etwa vergleichbar mit dem bei verbalen Material durch Verwendung von Optical Character Recognition (OCR) ermöglichten √úbergang von kompletter Transkription zum Korrekturlesen. Hier soll angemerkt werden, dass vielversprechende CNN-basierte Ansätze zur Textlokalisation (Sudholt & Fink, 2016) und OCR existieren (Lee & Osindero, 2016). Andererseits sind durch das Vorliegen visueller Merkmale (Features) vielfältige stilometrische Anwendungen denkbar. Zum Beispiel lassen sich aufgrund der Merkmale √Ñhnlichkeiten verschiedener Zeichner und Künstler berechnen und durch Gruppierung (Clustering) im Merkmalsraum auch Stile definieren. Auch die weitergehende Exploration der Repräsentation auf verschiendenen Schichten des Netzwerks scheint eine vielversprechende Aufgabe weiterer Forschung. Beispielsweise könnte der Vergleich der Antworten auf fotographische versus zeichnerisch abstrahierte Abbilder von Exemplaren einer Kategorie Hinweise auf das Wesen der Abstraktion geben, oder es lassen sich visuelle Merkmale identifzieren, die in besonderem Ausmaß die Aufmerksamkeitszuwendung im Leseprozess und bei der Rezeption von Zeichnungen leiten. Wir haben beispielhaft aufgezeigt, wie sich Werkzeuge der mathematisch-computationalen Modellierung eignen, um grafisches Material zu analysieren und zu beschreiben. Die Hoffnung ist, dass eine visuelle Stilometrie die Digitalen Geisteswissenschaften im Bereich visuellen Materials in ähnlicher Art und Weise bereichert wie computerlinguistische Ansätze im Bereich der Textanalyse. Digitale Analysen liefern mächtige neue Werkzeuge, die mittel- bis längerfristig auch eine neue Theoriebildung fördern könnten."
2018,DHd2018,DU_Keli_Sentimentanalyse_in_unstrukturierten_Texten__am_Bsp_.xml,Sentimentanalyse in unstrukturierten Texten (am Bsp. literaturgeschichtlicher Rezeptionsanalyse),"Katja Mellmann (Universität Göttingen, Deutschland); Keli Du (Universität Göttingen, Deutschland)","Sentimentanalyse, unstrukturierte Texte, literaturgeschichtliche Rezeptionsanalyse","Inhaltsanalyse, Methoden, Forschungsprozess, Text","Die fortschreitende Retrodigitalisierung von Kulturzeitschriften und anderen Publikationsformen mit literaturkritischen Inhalten eröffnet der literaturgeschichtlichen Rezeptionsanalyse die Möglichkeit, mit historisch repräsentativen Korpora zu arbeiten. Dabei stellt sich jedoch das Problem, dass insbesondere Zeitschriftendigitalisate in der Regel nicht als edierte Texte von standardisierter Qualität vorliegen, sondern mit ""schmutzigen Texten"", also Texten mit fehlerhafter OCR und ohne linguistische Strukturierung gearbeitet werden muss. Wir wollen im Rahmen des Themas ""Kritik der digitalen Vernunft"" einen konstruktiven Umgang mit diesem Problem vorstellen. Wir unterscheiden dazu grundsätzlich zwei Zielperspektiven: Die erste Perspektive ist auf qualitativ hochwertige Textkorpora angewiesen, um statistisch aussagekräftige Ergebnisse zu erzielen. Sie ist die Standardperspektive, wenn Korpusanalysen als Forschungsinstrument eingesetzt werden. Wir nehmen hingegen die zweite Perspektive ein: In ihr werden qualitativ minderwertige Textkorpora nicht als bedauerliche Abweichung vom eigentlich Gewünschten aufgefasst, sondern als Forschungsgegenstand eigener Art, der auch eine Methodik eigener Art erfordert. Diese Methodik hebt auf vorläufige Trenddarstellungen ab, die nicht als (noch unvollkommene) Vorstufe einer validen Korpusanalyse, sondern als eigenständige Heuristik zur Identifikation potentieller Ereignisse in einem diachronen Korpus aufgefasst werden. Die Methode soll sozusagen grobe Bewegungsprofile liefern, von denen aus anschließend wieder gezielte hermeneutische Tiefensondierungen unternommen werden können. Digitale und hermeneutische ""Vernunft"" stehen hier also in einem komplementären Verhältnis; nicht versucht wird, die eine durch die andere möglichst perfekt nachzubilden. Bei den angezielten Bewegungsprofilen handelt es sich im Rahmen unseres Forschungsprojekts Historische Rezeptionsanalyse (Mellmann/Willand 2013) rekonstruiert die Aufnahme literarischer Werke durch das originale zeitgenössische Publikum. Dazu zählen insbesondere (a) Inhaltsverständnis, (b) ästhetische Wertung und (c) Kontextualisierung mit außerliterarischen zeitgenössischen Wissensformationen. Wir befassen uns in unserer Studie ausschließlich mit der ästhetischen Wertung (b). Diese ist symptomatisch für einen umfassenden literaturgeschichtlichen Wandel in der zweiten Hälfte des 19. Jahrhunderts: Auf dem Übergang vom Bürgerlichen Realismus zur Klassischen Moderne verlieren ehemals reputierte Autoren noch während ihrer aktien Schaffenszeit ihren führenden Status; neue Stile aus dem Bereich des gesamteuropäischen Naturalismus und Östhetizismus gewinnen an Reputation. Für einzelne Autoren und insbesondere für poetologische Programmatiken wurde dies bereits vielfach gezeigt. Was noch aussteht, ist eine Einschätzung, wie repräsentativ die in Einzelstudien ermittelten Entwicklungen für die Gesamtentwicklung sind, insbesondere unter Einschluss auch der wenig erforschten nichtkanonischen Literatur. Abhilfe schaffen könnte hier die Analyse eines großen Korpus repräsentativer Literaturzeitschriften, die diachrone Wertungsprofile zu symptomatischen Autorengruppen liefert. Langfristiges Ziel ist eine Analyse von sieben repräsentativen Zeitschriften über einen Erscheinungszeitraum von ca. 1860 bis 1900: ""Die Grenzboten"", ""Die Gegenwart"", ""Deutsche Rundschau"", ""Nord und Süd"", ""Blätter für literarische Unterhaltung"", ""Westermanns Monatshefte"" und ""Magazin für Literatur"". Als Volltext verfügbar ist derzeit nur die Zeitschrift ""Die Grenzboten"". Sie dient uns als erster Anwendungsfall nach den überwachten Optimierungsläufen an einem auf der Basis einer Anthologie (Kreuzer 2006, Bd. I und II) erstellten Testkorpus. Literaturkritisches Schrifttum im ausgehenden 19. Jahrhundert ist ein außergewöhnlich stark rhetorisiertes Textgenre, das durch einen hohen Grad an Ironie, Intertextualität, Euphemismus und Understatement besondere Herausforderungen an die Methode der digitalen Sentimentanalyse stellt, zumal in unstrukturierten Texten, die keine Berücksichtigung von grammatischen Komplexitäten (wie z.B. doppelter Verneinung, Konjunktiven, indirekter Rede) zulassen. Die Optimierung der Sentimentwortliste ist deshalb weniger auf eine Verfeinerung als auf eine Vergröberung hin ausgelegt. Die Korpusanalyse soll vor allem eklatante Veränderungen identifizieren. Der Volltext des Testkorpus wurde durch NLTK Punkt Sentence Tokenizer Der Grundwert jedes Sentimentworts wurde mit 1 angesetzt. Die Polarität eines Satzes wurde in zwei Schritten festgelegt: Zuerst wurde das Vorkommen der positiven und negativen Sentimentwörter im Satz gezählt. Anschließend wurde die ""Aggregation function"" (Abb. 1) für die Berechnung des Sentiment-Werts des Satzes verwendet: "" Ist der Sentiment-Wert größer als 0, gilt der Satz als positiv; kleiner als 0 gilt als negativ; ist der Wert gleich 0 (z.B., weil kein Sentimentwort im Satz auftaucht), gilt der Satz als neutral. Die im Satz ermittelten Sentimentwörter wurden in die Ergebnisdarstellung übernommen, um die digitale Analyse anschließend manuell überprüfen und die Sentimentwortliste optimieren zu können. Wörter, die sich als überwiegend dysfunktional erweisen, werden von der Sentimentwortliste gelöscht, fehlende Sentimentwörter werden ergänzt. Im ersten Testlauf wurden nur ca. 47.4% der Sätze richtig erkannt (Tab. 1). Unsere Wortliste konnte vor allem die negativen Sätze schwer identifizieren. Ca. 77% der positiven Sätze wurden richtig identifiziert. Aber auch der Anteil der fälschlich als positiv klassifizierten Sätze war sehr hoch. Außerdem war der Sentiment-Wert von vielen als neutral eingestuften Sätzen nicht gleich 0.  In einem zweiten Testlauf haben wir eine automatische Klassifikation ausprobiert. Dabei wurden die Anzahl der Sentimentwörter und der Sentiment-Wert eines Satzes als Feature verwendet. Im Verhältnis 80% zu 20% wurden die Daten in einen Trainings- und einen Testdatensatz aufgespalten. Es wurde ein Support Vector Machine (SVM) Modell trainiert; die Evaluation erfolgte als 10-fache Kreuzvalidierung (Cross-Validation). Dadurch verbesserte sich das Ergebnis um ca. 10%: Die Trefferquote der Klassifikation lag bei 57% (+/- 7%). Für einen dritten Testlauf wurde die Sentimentwortliste bearbeitet und die Textsnippets wurden einem zweiten manuellen Rating mit mehr als nur 3 Kategorien unterzogen. Insbesondere sollte zwischen tatsächlich neutralen Sätzen (z.B. ""X wurde 1826 in Berlin geboren."" = 0) und Sätzen mit (einander ausgleichenden) positiven und negativen Bewertungen (z.B. ""Trotz dieser erheblichen Schwächen ist X ein Werk gelungen, das ‚Ä¶"" = 0,###) unterschieden werden. Auch Problemfälle (wie z.B. erwartbare Artefakte durch Zitation oder Ironie) wurden separiert, um die Analyseergebnisse gesondert evaluieren zu können. Von den 1687 eindeutig positiven, negativen oder neutralen Sätzen wurden 65,6% richtig erkannt (Tab. 2).  In unserer Präsentation werden wir die ausführliche Ergebnisevaluation des dritten Testlaufs vorstellen und die sich stellenden Probleme im Hinblick auf die eingangs dargestellte Zielsetzung diskutieren. Außerdem soll ein erster Probelauf über das inzwischen provisorisch erstellte Satzkorpus aus den ""Grenzboten"" präsentiert werden, der die angezielte Methodik der diachronen Trenddarstellung illustriert."
2018,DHd2018,SCHLUPKOTHEN_Frederik_Zwischen_Polysemie_und_Formalisierung_.xml,"Zwischen Polysemie und Formalisierung: Mehrstufige Modellierung komplexer intertextueller Relationen als Annäherung an ein ""literarisches"" Semantic Web","Julia Nantke (Bergische Universität Wuppertal, Deutschland); Frederik Schlupkothen (Bergische Universität Wuppertal, Deutschland)","Literaturwissenschaft, Intertextualität, Semantic Web, Situationstheorie","Beziehungsanalyse, Modellierung, Annotieren, Kontextsetzung, Theoretisierung, Text","Die Modellierung textueller und transbiblionomer Relationen mithilfe von Semantic Web-Technologien bildet mittlerweile eines der zentralen Forschungsfelder der Digital Humanities. Die Struktur und Funktionsweise literarischer Texte erfordern in Bezug auf die formale Beschreibung, Erklärung und Kategorisierung von semantischen Strukturen ein besonders differenziertes Vorgehen: Interne und externe textuelle Beziehungen bestehen in Form komplexer, häufig ambiger Zeichenrelationen, die plurale, sich auf verschiedenen Ebenen überlagernde Bedeutungsangebote stiften. Letztere können zudem nicht auf verortbare Ereignisse, stabile Relationen zwischen (bibliografischen, historischen) Artefakten oder ein konkretes argumentatives Ziel bezogen werden. Die Kategorisierung literarischer ""Daten"" erfolgt deshalb systematisch im Spannungsfeld zwischen dem Text als linguistisch-materieller Zeichenformation und deren interpretierender Auffassung. Die Aktualisierung bestimmter Codes hängt hierbei immer auch von kulturellen und historischen Faktoren, methodologischen Vorannahmen sowie der Kontingenz interpretativer Schlussfolgerung ab. Die daraus resultierende Bedingtheit und potentielle Vielfalt semantischer Zuschreibungen muss daher in einer Modellierung transparent abbildbar sein. Das Projekt möchte damit in zweierlei Hinsicht einen Beitrag zur methodologischen Reflexion leisten: Zum einen streben wir mit dieser zunächst auf das genauere Verständnis intertextueller Phänomene gerichteten stufenweisen Modellierung Der Beitrag diskutiert zum einen konkrete Probleme, welche sich im Spannungsfeld zwischen literarischer Polysemie, der Literaturwissenschaft inhärenter Perspektivenvielfalt und technischer Normierung ergeben, denn das Projekt dient nicht zuletzt auch der Reflexion der Möglichkeiten zur Formalisierung literaturwissenschaftlicher Erkenntnisse sowie dem Ausloten der Grenzen für den Einsatz formaler Beschreibungssprachen im Hinblick auf literaturwissenschaftliche Forschungsfragen. Zum anderen wird dargestellt, wie durch die spezifische Anlage der Modellierung auf verschiedenen Ebenen Desideraten bisheriger Ansätze begegnet und gleichzeitig ein Beitrag zum literaturtheoretisch fundierten Einsatz von DH-Methoden geleistet werden kann. Erste Ergebnisse werden in dem vorgeschlagenen Beitrag anhand konkreter Beispiele wie etwa des intertextuellen Netzes um Matthias Claudius"" In vielen bisherigen Projekten zur Erfassung literarischer Beziehungen schränkt die Konzentration auf automatisierbare Analysevorgänge das heuristische Potential digitaler Modellierung für die literaturwissenschaftliche Forschung in verschiedener Hinsicht ein: Erstens werden Modelle zur Beschreibung (innertextueller) literarischer Strukturen an stark Plot-lastigen Texten entwickelt, Zweitens erfolgt eine Modellierung intertextueller Beziehungen anhand einer ""historische[n] Positivtät von Kontext-Dokumenten"" (Wagner/Mehler/Biber 2016, S. 90 mit Bezug auf das Projekt Wikidition), deren Verknüpfungen auf linguistischer Ebene modelliert werden. Auf diese Weise werden zwar viele Probleme im Hinblick auf die Intersubjektivierbarkeit der Modellierung und die Differenzen zwischen verschiedenen Intertextualitätskonzepten vermieden, gleichzeitig wird aber aus literaturwissenschaftlicher Sicht die Aussagekraft der Ergebnisse stark eingeschränkt, indem der literaturwissenschaftlich relevante Fokus auf die Kategorisierung, Funktion und Wirkung von Intertextualität und die hierbei produktiven Schreibweisen und Markierungen (vgl. Kocher 2007, 179) zugunsten einer eher enzyklopädischen Perspektive verloren geht. Unser Projekt richtet sich hingegen weder auf die automatisierte Textanalyse noch beschränkt es sich auf die linguistische Ebene konkreter Wortäquivalenz. Vielmehr steht die Entwicklung eines formalisierten Vokabulars zur semantischen Repräsentation literarischer Intertextualität im Zentrum des Forschungsinteresses. Die Isolierung und formale Beschreibung intertextueller Phänomene dient der Beobachtung und Darstellung des Zusammenwirkens jener Schreibweisen und Markierungen bei der Erzeugung von Intertextualität. Die intertextuellen Beziehungen werden dabei also nicht deduktiv im Sinne einer Qualifizierung als Parodie, Kontrafaktur, Nachahmung, Hommage etc. modelliert, da derartige ""Gattungszuschreibungen"" in systematischen literaturwissenschaftlichen Untersuchungen zur Typisierung von Intertextualität oftmals den Blick für die spezifischen, bei der Erzeugung von Intertextualität wirksamen Faktoren verstellen. Die angeführten literarischen Beispiele dienen dann eher der selektiven Untermauerung der jeweils präfigurierten Typologie (vgl. einschlägig Broich/Pfister 1985; Genette 1993). Im Gegensatz dazu bildet die Modellierung von Beziehungen zwischen konkreten Schreibweisen den Ausgangspunkt unseres Projekts, welcher im Anschluss Schlussfolgerungen über die Relationen der verschiedenen Ebenen, auf denen intertextuelle Verknüpfungen stattfinden, sowie über die jeweils erzeugten Wirkungen ermöglichen soll. Für das angestrebte Forschungsziel stellt die mehrstufige Modellierung einen expliziten heuristischen Gewinn gegenüber bisherigen Ansätzen der Systematisierung dar, indem die umfassende induktive Erfassung und Beschreibung sowie die anschließend abgeleiteten strukturellen Konstanten nicht unverbunden nebeneinander stehen, sondern Mikro- und Makrostrukturen durch das Modell in ihrem Zusammenhang beobachtbar gemacht werden (s. Abbildung¬†1). Indem zunächst Einzeltextphänomene modelliert, darauf aufbauend deren Funktionen im Sinne ihres Beitrags zur ""Bedeutungskonstitution"" (Hempfer 1991, S.¬†19) erfasst und daraus übergreifende Kategorien abgeleitet werden, können zwei bislang getrennt voneinander verhandelte Bereiche der Untersuchung von Intertextualität in einer ganzheitlichen Modellierung verbunden werden: die ""Entwirrung"" des intertextuellen Gefüges eines einzelnen Textes (vgl. hierfür exemplarisch Bauer Lucca 2001; Dudzik 2017) sowie die übergeordnete Suche nach gemeinsamen Strukturen und Funktionsweisen intertextueller Verweise. Das vorgestellte Modell verknüpft also die in der Literaturwissenschaft seit den 1980er Jahren unternommenen Bestrebungen zur Typologisierung intertextueller Strukturen und Funktionsweisen mit einer umfassenden Detailuntersuchung literarischer Texte. Die Differenzierung in Phänomenbeschreibung und ‚Äëbewertung, welche der eingesetzte Formalismus unterstützt (s.¬†u.), sieht explizit die Modellierung funktionaler Überlagerungen und alternativer Forschungsmeinungen vor, sodass im Rahmen der Formalisierung sowohl der Multifunktionalität intertextueller Schreibweisen (vgl. Kocher 2010, S. 179) als auch der maßgeblich auf produktivem Dissens basierenden Dynamik des literaturwissenschaftlichen Diskurses Rechnung getragen wird. Als Ausgangspunkt zur formalen Beschreibung intertextueller Phänomene dient ein situationstheoretischer Ansatz, welcher die Brücke zwischen literaturwissenschaftlicher Analyse und technischer Modellierung darstellt. Die Situationstheorie bietet sich an, da sie im Sinne der angestrebten Beschreibung der Intertextualität einen mehrstufigen Formalismus zur Verfügung stellt, welcher Informationen und Informationsflüsse in Kontextabhängigkeit beschreibt: Basale Phänomene werden durch basale Informationseinheiten (sog. ""Infone"") beschrieben, welche wiederum ""Situationen"" als Phänomene einer höheren Ordnung aus der Perspektive eines oder mehrerer ""Agenten"" zu bilden erlauben. Somit kann formal unterschieden werden zwischen der Modellierung konkreter (sprachlicher, inhaltlicher, stilistischer) Texteigenschaften und -relationen (beschrieben als Infone) und der Klassifizierung der modellierten Informationseinheiten im Sinne ihrer Funktion sowie einer durch sie indizierten, Kontext-abhängigen Wirkung (beschrieben als Situationen). Im Gegensatz zu technischen Beschreibungssprachen (wie etwa RDF oder OWL) liefert die Situationstheorie einen Formalismus, welcher zunächst frei von umsetzungsspezifischen Einschränkungen (wie etwa festgelegten Datentypen oder Objekthierarchien) ist, die sich ungewollt perspektivierend auf die Modellierung auswirken können. Das Modell wird sukzessive unter Einbezug literarischer Texte verschiedener Textsorten und Publikationszeiträume getestet und weiterentwickelt. An diese sukzessive formale Strukturierung anknüpfend wird geprüft, inwieweit etablierte Beschreibungssprachen bei einer technischen Umsetzung des Modells Anwendung finden können. Insbesondere etablierte Sprachen aus dem Umfeld elektronischer Publikation sollen auf ihre Anwendbarkeit bzw. Möglichkeiten der Erweiterung hin betrachtet werden. Dies sind im Rahmen der durch das W3C beschriebene Standards für Verweisstrukturen Sprachen wie XPath, XLink oder XPointer (vgl. einschlägig Wilde/Lowe 2003), für semantische Auszeichnungen die Sprachen des Semantic Web wie RDF oder OWL. Dies schließt ‚Äì unabhängig von der konkreten Sprache ‚Äì die Berücksichtigung unterschiedlicher Auszeichnungskonzepte wie bspw. Standoff- in Abgrenzung zu Inline-Markup ein (vgl. Banski 2010)."
2018,DHd2018,GODLER_Katharina_musilonline___integral_lösen__Dialogfeld_Di.xml,musilonline - integral lösen. Dialogfeld Digitale Edition,"Anke Bosse (Robert-Musil-Institut, AAU Klagenfurt); Walter Fanta (Robert-Musil-Institut, AAU Klagenfurt); Katharina Godler (Robert-Musil-Institut, AAU Klagenfurt); Gerrit Brüning (Goethe-Universität Frankfurt / Freies Deutsches Hochstift); Artur Boelderl (Institut für Germanistik, AAU Klagenfurt)","Digitale Edition, Interface, Manuskript, XML/TEI, Online-Kommentar","Umwandlung, Gestaltung, Annotieren, Archivierung, Kommentierung, Literatur","Digitale Editionen haben sich bereits als geeignete Publikationsform für die Präsentation von umfangreichen Textbeständen im Bereich des kulturellen Erbes etabliert. Für ein so umfangreiches und komplexes textgenetisches Korpus wie Robert Musils literarischen Nachlass und die daran entwickelte Datenstruktur der Klagenfurter Ausgabe stehen jedoch keine fertigen Modelle zur Verfügung. Im Rahmen des Panels soll einerseits diskutiert werden, welche Kriterien eine digitale Edition erfüllen muss, um eine Grundlage zur Erforschung von Robert Musils Gesamtwerk zu erstellen und andererseits, ob die derzeit geltenden Standards zur Langzeitarchivierung, interoperablen Repräsentation und Online-Kommentierung von digitalen Textkorpora noch zeitgemäß sind. Der umfangreiche literarische Nachlass des österreichischen Schriftstellers Robert Musil umfasst 12.000 Manuskriptseiten und wird bereits seit 1985 digital ediert. Die wichtigsten bisherigen Publikationsetappen markieren die CD-ROM-Ausgabe a) Die b) Das künftige Im Kurzvortrag werden noch keine fertigen Lösungen vorgestellt, sondern es erfolgt ein kritischer Aufriss der Problemlage in Folge der komplexen Struktur von Musils Manuskripten und die Präsentation eines Grundkonzepts an Hand von exemplarischen Ausschnitten aus dem Manuskriptbestand. (Walter Fanta, Robert-Musil-Institut/Kärntner Literaturarchiv, AAU Klagenfurt) c) Die fachgerechte d) Die e) Der Das Panel integriert höchst unterschiedliche Bereiche und Aspekte der Digital Humanities, die sich im Projekt"
2018,DHd2018,KESSLER_Linda_Entitäten_im_Fokus_am_Beispiel_von_Captivity_N.xml,Entitäten im Fokus am Beispiel von Captivity Narratives,"Linda Kessler (Universität Stuttgart, Deutschland); Tamara Braun (Universität Stuttgart, Deutschland); Tanja Preuß (Universität Stuttgart, Deutschland)","maschinelles Lernverfahren, Sentimentanalyse, Captivity Narratives","Annotieren, Literatur, benannte Entitäten (named entities)","Eigennamenerkennung (NER) ist im Bereich der maschinellen Sprachverarbeitung bereits viel behandelt worden. Eine Übersicht hierzu findet sich bei Nadeau und Sekine (2007). In den Digital Humanities dient die Erkennung von benannten Entitäten der Identifikation zentraler Akteure und Elemente in Texten, welche unter anderem die Grundlage für tiefergehende Analysen bezüglich Beziehungen, Strukturen und Emotionen in diesen Texten bilden. Jannidis et al. (2015) thematisieren allerdings, dass die reine NER beispielsweise für eine Analyse von Figurennetzwerken in literarischen Texten unzureichend ist, da dabei nur Figurenreferenzen durch konkrete Namensnennung erfasst werden. Um spezifisch auf die Bedürfnisse von Textanalysen im Kontext der Digital Humanities einzugehen, wurden im ""Center for Reflected Text Analytics"" (CRETA) (Kuhn et al. 2016) der Universität Stuttgart Annotationsrichtlinien entworfen, die über die Annotation reiner Eigennamen hinausgehen und sich auf verschiedenartige Entitätsreferenzen in deutschsprachigen Texten unterschiedlicher Genres fokussieren. Ein Beispiel für den Mehrwert der Annotation solcher Entitätsreferenzen findet sich bei Blessing et al. (2017). Um die Übertragbarkeit der Richtlinien nicht nur zwischen verschiedenen Textsorten, sondern auch sprachübergreifend zu evaluieren, stellen wir unser Projekt mit dem Ziel der Annotation von Erzähltexten in englischer Sprache vor. Ausgehend von der durch CRETA geschaffenen Grundlage teilt sich unser Projekt in drei Phasen auf: die manuelle Annotation und Überprüfung der Übertragbarkeit der CRETA-Richtlinien auf die gegebene Textsorte, die Automatisierung der Entitätserkennung und die Einbindung der Entitäten in eine literaturwissenschaftliche Analyse. Als Textgrundlage dient eine Sammlung von englischsprachigen Captivity Narratives. Der so entstandene Goldstandard dient als Trainingsdatensatz zur Entwicklung eines maschinellen Lernverfahrens. Ein Naive Bayes Classifier wurde mit Features trainiert, die sich u.a. auf die äußere Gestalt (z.B. Großschreibung), die Wortart und die Zugehörigkeit zu Wortlisten (Namen und amerikanische Orte) beziehen. Im Kreuzvalidierungsverfahren kann damit ein Micro-Fscore von 0,29 erzielt werden. Für die am häufigsten im Trainingsmaterial vorhandene Klasse PER wurde ein Precision-Wert von 0,45 erzielt. Dies bedeutet, dass fast die Hälfte der automatisch mit PER annotierten Entitäten wirklich Personen sind. Der Recall von 0,3 zeigt, wie unvollständig die Erkennung mit einem knappen Drittel aller relevanten Personen noch ist. Eine Auswertung der Ergebnisse zeigt, dass die Länge und Verschachtelung vieler Entitäten die automatische Klassifizierung erschwert. Da sich im manuellen Annotationsprozess der Kontext häufig als Entscheidungshilfe herausstellte, sollte dieser bei der automatischen NER zukünftig berücksichtigt werden. Darüber hinaus könnte die Erweiterung der verwendeten Features durch syntaktische Informationen und die Verwendung einer größeren Menge an Trainingsdaten zu Verbesserungen führen. Um den Mehrwert der Entitätsreferenzen für eine inhaltliche Fragestellung bezüglich der Captivity Narratives zu veranschaulichen, zeigen wir die textstatistische Analyse von Emotionen im Umfeld bestimmter Entitäten bzw. Entitätsgruppen. Basierend auf den manuell annotierten Texten, lassen sich Personenentitäten mithilfe von Clusteranalysen gruppieren. Anhand von positiven und negativen Wortlisten lassen sich zwei Gruppen bilden, die sich grob als Abschließend lässt sich festhalten, dass auf Grundlage unserer Annotationen eine Abgrenzung der im Text auftretenden Gruppen anhand von emotionsgeladenen Wörtern möglich ist, die der erwarteten negativen Haltung der Verfasser gegenüber den Eingeborenen Nordamerikas entspricht. Die von CRETA entwickelten Annotationsrichtlinien sind grundsätzlich auf die von uns analysierten Texte anwendbar, trotz abweichender Sprache und spezifischer Erzählweise. Um die Breite der enthaltenen Entitätsreferenzen vollständig abbilden zu können, bedarf es allerdings einzelner Spezifizierungen der Annotationsrichtlinien für diese Textsorte. "
2018,DHd2018,KLINGER_Roman_Digitale_Modellierung_von_Figurenkomplexität_a.xml,Digitale Modellierung von Figurenkomplexität am Beispiel des Parzival von Wolfram von Eschenbach,"Manuel Braun (Institut für Literaturwissenschaft, Universität Stuttgart); Roman Klinger (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart); Sebastian Padó (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart); Gabriel Viehhauser (Institut für Literaturwissenschaft, Universität Stuttgart)","Figurenanalyse, Distributionelle Semantik, Distant Reading","Strukturanalyse, Beziehungsanalyse, Modellierung, Stilistische Analyse, Visualisierung","Figuren gehören zu den wichtigsten Bestandteilen literarischer Erzählungen. Narratologische Analysen haben sich bislang insbesondere mit zwei Aspekten von Figuren beschäftigt: ihrer strukturellen Bedeutung und ihrer Charakterisierung. Während der erste Aspekt in den Digital Humanities bereits modelliert worden ist (etwa im Rahmen von Netzwerkanalysen, vgl. Jannidis et. al. 2016, Piper et. al. 2017), steht die datengetriebene Untersuchung des zweiten noch ganz am Anfang, obwohl die hermeneutisch arbeitende Literaturwissenschaft wiederholt auf seine Bedeutung hingewiesen hat (etwa Jannidis 2004, 2009). Dieses Desiderat lässt sich darauf zurückführen, dass Figuren auf die unterschiedlichste Art gekennzeichnet werden 'etwa durch ihr Handeln und Reden, aber auch durch Beschreibungen und Bewertungen des Erzählers 'und dass neben expliziten Charakterisierungen schwerer zu greifende implizite stehen. Diese Informationen lassen sich nur schwer erheben und in einem umgreifenden Modell verrechnen. Um dennoch einen Einstieg in die digitale Erfassung der Figurencharakteristik zu ermöglichen, nehmen wir eine vergleichsweise einfach zu modellierende Facette der Figurendarstellung in den Blick, und zwar das Konzept der Figurenkomplexität, das auch in der traditionellen Theoriebildung eine prominente Rolle spielt. Nach der ebenso verbreiteten wie vielkritisierten Kategorisierung von Forster (1927) lassen sich Figuren grundsätzlich in Als Paradefall für Figurenkomplexität kann der um 1200/1210 abgefasste Artusroman ""Parzival"" angesehen Im Folgenden schlagen wir eine Methodik vor, mit der diese Einschätzungen der hermeneutischen Forschung digital modelliert und überprüft werden kann. Wir untersuchen, ob man bereits mit extrem einfachen Methoden Vorhersagen zur Figurenkomplexität treffen kann, und verwenden dazu Spezifisch betrachten wir zwei Arten frequenzbasierter Kontextvektoren: (a) In der Computerlinguistik werden Kontextvektoren typischerweise miteinander verglichen, um die semantische Öhnlichkeit der Zielwörter zu modellieren und finden breite Anwendung (Pantel und Turney 2010). In unserer Studie betrachten wir stattdessen den Dies entspricht der (aus theoretischer Perspektive natürlich stark vereinfachenden) Annahme, wonach Figuren als komplexer wahrgenommen werden, wenn sie in reicheren und verschiedeneren Kontexten vorkommen. Bei lexikalischen Kontexten (s.o.) sagen wir also voraus, dass höhere lexikalische Variabilität auf Komplexität hinweist 'wobei Entropie freilich nicht zwischen lexikalisch reichen und semantisch widersprüchlichen Kontexten unterscheiden kann. Bei entitätsbasierten Kontexten übernimmt diese Rolle das gemeinsame Auftreten einer Figur mit mehreren anderen Figuren. Methodisch ist anzumerken, dass Frequenz einen Störfaktor bei der Interpretation von Entropie darstellt: Häufigere Zielwörter kommen 'ceteris paribus 'häufiger in verschiedenen Kontexten vor und erhalten damit eine höhere Entropie. Da dieser Zusammenhang aber nicht linear ist, ist eine einfache Normalisierung nicht möglich. In einem ersten Schritt sind also nur Zielwörter mit ähnlicher Frequenz hinsichtlich ihrer Entropie gut vergleichbar. Als Textgrundlage verwenden wir den ""Parzival"" nach der 5. Auflage der Ausgabe Lachmanns (1891) in der digitalisierten, mit Lemmata und der Auszeichnung von Eigennamen versehenen Fassung von Yeandle (2014). Für unser Hauptexperiment beschränken wir uns auf die sieben im Text am häufigsten namentlich genannten Frauenfiguren Cundr√Æe, Herzeloyde, Jesch√ªte, Cunnew√¢re, Arn√Æve, B√™ne und Itonj√™. Hinzu kommt Sig√ªne als ein in der Forschung oft genanntes Beispiel für eine weniger komplexe Figur im ""Parzival"". Sieben der acht Figuren liegen in einem engen Frequenzband zwischen 30 und 40 Nennungen, während Sig√ªne nur 14 Mal vorkommt. Wir präsentieren die Ergebnisse per Streudiagramm, mit Entropie und Frequenz als den beiden Achsen. Abbildung 1 und 2 zeigen die Ergebnisse für die wichtigsten weiblichen Hauptfiguren. Diese lassen sich zwanglos mit den Einschätzungen der hermeneutischen Literaturwissenschaft in Einklang bringen. Die höchsten Werte weisen Cundr√Æe und Herzeloyde auf, auf deren Komplexität die ""Parzival""-Forschung immer wieder hingewiesen hat: Bei Cundr√Æe handelt es sich um die Gralsbotin, die zwar kultiviert und nach höfischen Sitten gekleidet auftritt, zugleich aber monströs hässlich ist. Sie klagt Parzival zwar berechtigterweise an, fällt dabei aber aus dem höfischen Rahmen. Herzeloyde zieht ihren Sohn Parzival aus (übermäßiger?) Trauer um ihren im Kampf gefallenen Ehemann fern von der höfischen Welt in einer Waldeinöde auf, um zu verhindern, dass auch er einst Ritter wird 'ein Verhalten, das die Forschung unterschiedlich bewertet. Mittelwerte erreichen Cunnew√¢re und Jeschute, die in der Forschung durchaus kontrovers diskutiert werden. B√™ne, Arn√Æve und Itonj√™ lassen sich hingegen eher als Nebenfiguren bezeichnen, für die keine hohe Komplexität zu erwarten war. Die in der Trauer um ihren Geliebten Schionatulander verharrende Sig√ªne, die gerade in ihrer Geradlinigkeit einen Gegenentwurf zu den auf Ruhm und Ehre versessenen Artusrittern darstellt, zeigt erwartungsgemäß keine hohen Entropiewerte. Allerdings erlaubt dieser Befund keine starke Interpretation, da für Sig√ªne aufgrund der niedrigeren Frequenz von vorneherein niedrigere Entropiewerte zu erwartet sind. Um dennoch eine Vorhersage zur Komplexität Sig√ªnes zu erhalten, vergleichen wir in den Abbildungen 3 und 4 ein breiteres Spektrum an Figuren im Frequenzbereich zwischen 10 und 20 Nennungen bezüglich ihrer Entropie. Dies entspricht einer erweiterten Version unserer Hypothese: Figuren, die Interessanterweise liefern die beiden Kontextdefinitionen für Sig√ªne abweichende Vorhersagen: Die unterdurchschnittliche soziale Kontextvielfalt entspricht der Isoliertheit der Figur, die sich aus Trauer von der Welt zurückzieht und wenn, dann fast nur noch mit Parzival interagiert. Die vergleichsweise hohe lexikalische Vielfalt könnte demgegenüber darauf zurückzuführen sein, dass Sig√ªne neben ihrer Trauer auch die weitere Funktion hat, Parzival über seine Herkunft aufzuklären: Während Sig√ªne als Figur eher statisch erscheint, sind ihre Erzählungen über die Gralswelt informationshaltig. Für eingehendere Untersuchungen wären daher weitere Faktoren der Figurendarstellung wie Figurenbeschreibung und Figurenrede mit einzubeziehen sowie das Verhältnis von lexikalischer und sozialer Kontextvielfalt näher zu bestimmen. Unsere Ergebnisse legen nahe, dass sich die Analyse der lexikalischen und ""sozialen"" Vielfalt von Figurenkontexten durchaus als Maß für eine erste Annäherung an das Konzept der Figurenkomplexität eignet. Damit konnte mit erstaunlich einfachen Mitteln ein Zugang zum komplexen narratologischen Themenfeld der Figurencharakterisierung gewonnen werden. Auf der technischen Ebene bleiben zwei Fragen offen, zum einen die nach den sprachlichen Mechanismen, die zur Korrelation von Entropie und Figurenkomplexität führen, zum anderen die nach dem tatsächlichen Zusammenhang von Frequenz und Entropie: Gibt es also auch selten auftretende Auf der konzeptuellen Ebene bieten unsere Ergebnisse den Einstieg in eine differenziertere Modellierung von Figurenkomplexität, deren Erarbeitung auf die traditionelle narratologische Theoriebildung zurückwirken kann: Die digitale Modellierung macht es notwendig, die Faktoren, aus denen sich das Konzept der Figurenkomplexität zusammensetzt (etwa Figurenhandeln, -rede, -beschreibung und -bewertung), genauer und expliziter zu bestimmen sowie über ihre Gewichtung nachzudenken. Außerdem wäre zu klären, was das Konzept der Komplexität genau meint und wie es sich zu verwandten Konzepten wie denen der Hybridität, der Dynamik oder der Aktantenhaftigkeit von Figuren verhält. Im Sinne einer kritisch reflektierten digitalen Methodik werden somit vielschichtige literaturwissenschaftliche Phänomene wie die Charakterisierung von Figuren durch den formalisierenden Zugang nicht nivelliert, vielmehr führt die Verbindung von quantitativen und qualitativen Methoden zur wechselseitigen Erhellung."
2018,DHd2018,ODEBRECHT_Carolin_Suche_und_Visualisierung_von_Annotationen_.xml,Suche und Visualisierung von Annotationen historischer Korpora mit ANNIS. Kritik der korpuslinguistischen Analysemethoden in einem erweiterten Nutzungskontext,"Carolin Odebrecht (Humboldt-Universität zu Berlin, Deutschland); Thomas Krause (Humboldt-Universität zu Berlin, Deutschland); Rolf Guescini (Humboldt-Universität zu Berlin, Deutschland); Frank Kühnlenz (Humboldt-Universität zu Berlin, Deutschland); Anke Lüdeling (Humboldt-Universität zu Berlin, Deutschland); Malte Dreyer (Humboldt-Universität zu Berlin, Deutschland)","Suche und Visualisierung, Analyse, historische Korpora, Annotation, Wiederverwendung","Teilen, Inhaltsanalyse, Visualisierung, Daten, Forschungsprozess, Werkzeuge","Historische Korpora (Gippert und Gehrke 2015; Claridge 2008; Rissanen 2008) dienen in vielen geisteswissenschaftlichen Disziplinen als Analysegrundlage und können sehr unterschiedlich aufbereitet sein. Mit korpusbasierten Studien können qualitative und quantitative Analysen, die für die Überprüfung von Hypothesen über ein bestimmtes Phänomen notwendig sind, durchgeführt werden. Dem gegenüber steht methodisch die korpusgetriebene Studie, die das Korpus selbst nutzt, um Hypothesen über ein Phänomen zu generieren (vgl. McEnery und Hardie 2012; Lüdeling und Zeldes 2007). Neben diesen zwei Studientypen können mit Hilfe von Korpora auch einzelne Belege und Kontexte für die Beantwortung verschiedenster Forschungsfragen ermittelt werden. Eine andere methodische Unterscheidung wird mit dem In den digitalen Geisteswissenschaften müssen daher für die jeweiligen Methoden und Forschungsdaten Analyse- und Visualisierungswerkzeuge entwickelt werden, die es den Forscherinnen und Forschern ermöglichen, für ihren jeweiligen Forschungskontext aus einem breiten methodischen Spektrum wählen zu können (vgl. für einen Überblick z.B. Kupietz und Geyken 2016). Ein solches Werkzeug ist ANNIS (Krause und Zeldes 2016), das Such- und Visualisierungstool für Annotationen, das wir in unserem Workshop den Forscherinnen und Forschern aus den Digital Humanities vorstellen möchten. ANNIS erlaubt das Durchsuchen von Korpora, die unterschiedliche Arten von Annotationen, die möglicherweise durch unterschiedliche Forschergruppen unter verschiedenen Gesichtspunkten annotiert worden, in einem Korpus vereinen. Diese Flexibilität erlaubt es, annotierte Phänomene in der Suche zu kombinieren und damit komplexere Strukturen zu finden. Neben der Unterstützung der vielfältigen Analysemethoden ist eine weitere Herausforderung für die Analysewerkzeuge, dass historische Korpora je nach Forschungskontext und -frage unterschiedlich erstellt und aufbereitet werden (Lüdeling 2011). Dies zeigt sich unter anderen in den vielfältigen Transkriptions- und Normalisierungsverfahren (vgl. z.B. Odebrecht et al. 2016; Krasselt et al. 2015; Archer et al. 2015; Bollmann et al. 2012; Jurish 2010) und Annotationsguidelines (für z.B. Annotation von Wortarten für historisches Deutsch Coniglio et al. 2016; Dipper et al. 2013) sowie verschiedenen Formaten (z.B. Romary et al. 2015; Schmidt und Wörner 2009; Burnard und Baumann 2008; Wittenburg et al. 2006; Dipper 2005), die allein für die Erstellung von historischen Korpora eingesetzt werden. Damit historische Korpora mit verschiedenen Methoden analysiert werden können, muss deren Wiederverwendung ermöglicht werden. Die Wiederverwendung von historischen Korpora wird durch u.a. deren freie Veröffentlichung und umfassende Dokumentation möglich (Odebrecht 2014; Borgmann 2012; Büttner et al. 2011). Weiterhin erhöht eine Wiederverwendung ihre Sichtbarkeit und stellt eine Chance zur engeren Vernetzung und Zusammenarbeit in den digitalen Geisteswissenschaften dar. So können auch historische Korpora in unterschiedlichen Wiederverwendungsszenarien gedacht werden (vgl. Simons und Bird 2008) und als empirische Grundlage für die verschiedenen Analysemethoden dienen. Dieser Workshop möchte ausgehend von diesen Themenkomplex mit den Teilnehmerinnen und Teilnehmern folgende Fragen diskutieren: Wie können Analysewerkzeuge den Forscherinnen und Forschern vielfältige Analysemethoden und Visualisierungsmethoden für verschiedene historische Korpora ermöglichen? Wie kann ANNIS die verschiedenen Analysemethoden bislang unterstützen? Wie kann es gelingen, auch die Vielfältigkeit der Forschungsdaten als solche zu berücksichtigen und deren Wiederverwendung zu ermöglichen? Wie können Werkzeuge spezifisch genug entwickelt werden, um genaue und für den Forschungskontext und die Forschungsdaten angepasste Analysen zu ermöglichen? Der Workshop hat das Ziel, anhand mehrerer historischer Korpora des Deutschen das generische Such- und Visualisierungstool ANNIS (Krause und Zeldes 2016) für den Einsatz in den Digital Humanities zu diskutieren und anzuwenden, da es bislang überwiegend für korpusbasierte und korpusgetriebene Studien sowie für das Auffinden von sprachlichen Belegen eingesetzt wird. ANNIS wird seit 2009 als ein generisches webbasiertes Such- und Visualisierungstool für verschiedene Korpustypen und Annotationskonzepte in verschiedenen Kooperationen mit der Humboldt-Universität zu Berlin und der Georgetown University und in mehreren Projekten entwickelt. Der Quellcode von ANNIS ist frei zugänglich veröffentlicht und bietet gleichzeitig eine Desktop- sowie Server-Installation. In ANNIS können Korpora mit Token-, Spannen-, Baum- und Pointingannotationen unabhängig von den einzelnen, jeweils korpusspezifischen Annotationsguidelines in ANNIS analysiert werden. ANNIS bietet weiterhin den Korpuserstellerinnen und -erstellern annotations- oder fachspezifische Visualisierungen für Korpora. Mit einer wiederum generischen und mächtigen Anfragesprache (ANNIS Query Language 'AQL) können alle Korpora in ANNIS nach Annotationen und Kombinationen von Annotationen durchsucht werden. Weiterhin können die Suchergebnisse für bspw. weitere statistische Auswertungen exportiert werden. Jedes Korpus, jede Suchanfrage und jeder Beleg kann über einen permanenten Link stabil referenziert werden. Mit dem Konverterframework Pepper (Zipser und Romary 2010) werden Korpora, die in verschiedenen Formaten vorliegen können, in das ANNIS-Format überführt. Repositorien wie das LAUDATIO-Repository (Odebrecht et al. 2015) ermöglichen einen Open Access Zugang zu verschiedensten historischen Korpora und stellen eine umfassende Korpusdokumentation (Odebrecht 2014) zur Verfügung, die eine Erschließung dieser heterogenen Datengrundlage unabhängig von den Korpuserstellerinnen und -erstellern ermöglicht. Damit wird eine Voraussetzung für die Wiederverwendung der historischen Korpora erfüllt. Für den Workshop werden aus LAUDATIO beispielhaft die Korpora ""Referenzkorpus Altdeutsch"" (Donhauser 2015) und ""RIDGES Herbology Korpus"" (Odebrecht et al. 2016) verwendet. Das Referenzkorpus Altdeutsch ist ein historisches Mehrebenenkorpus der ganzen Sprachperiode des Althochdeutschen mit ca. 650.000 Wörtern (von den ersten Überlieferungen bis Mitte des 11. Jahrhunderts). Als Grundlage für die diplomatischen Transkription sind Editionen der jeweiligen Handschriften, die mit weiteren Annotationen zur Textstruktur sowie mit komplexen Wortartenannotation (Dipper et al. 2013), Annotation zu Flexionsklassen und Lemmatisierung versehen sind. Das RIDGES Korpus ist ein tief annotiertes Korpus mit Auszügen aus gedruckten Kräuterbüchern aus der Zeit zwischen 1487 und 1910, anhand derer die Entwicklung der deutschen Wissenschaftssprache auf vielen Ebenen untersucht wird. Die Drucke sind diplomatisch transkribiert (wo möglich, nach vorher digitalisierten oder durch OCR-Verfahren erstellte Vorlagen, vgl. Springmann und Lüdeling 2017). Die Daten sind mehrfach normalisiert und auf vielen Ebenen annotiert (unter anderem mit Wortart, Lemma, Informationen zu Kompositionstypen (Perlitz 2014), Dependenzsyntax, Informationen zur graphischen Struktur nach den TEI Guidelines). Dabei werden automatische und manuelle Annotationsverfahren und Prüfverfahren genutzt. Um die eingangs formulierten Fragen adressieren zu können, wird der Workshop zwei Schwerpunkte enthalten. Der erste Schwerpunkt wird die Einführung in die Funktionen und Suchanfragesprache von ANNIS sowie die damit verbundene Vorstellung der zwei historischen Beispielkorpora umfassen. Wir wollen den Teilnehmerinnen und Teilnehmern die verschiedenen Analyse- und Visualisierungsmöglichkeiten online und hands-on vorstellen. Über die Vorstellung zweier historischer Korpora mit dem generischen ANNIS können bereits die Herausforderungen der heterogenen Datengrundlage in den digitalen Geisteswissenschaften für Analysetools herausgearbeitet werden. Der zweite Schwerpunkt soll Raum für eine Diskussion mit den Teilnehmerinnen und Teilnehmern sowie auch die Möglichkeit geben, weitere Korpora in ANNIS 'geleitet von den Forschungsinteressen der Teilnehmerinnen und Teilnehmern 'zu durchsuchen. Mit diesem Workshop wollen wir uns gemeinsam mit den Teilnehmerinnen und Teilnehmern kritisch mit den Anforderungen an ein Analysetool für verschiedene Methoden zur Analyse und Visualisierung von historischen Korpora auseinandersetzen und prüfen, in wie weit ANNIS bereits einige dieser Anforderungen erfüllen kann. So wollen wir ANNIS in einem neuen Forschungskontext der Digital Humanities diskutieren und dabei neue Nutzerszenarien für die weitere Entwicklung erarbeiten."
2018,DHd2018,HAAF_Susanne__Perspektiven_auf_ein_Korpus__Kombinationen_qua.xml,Perspektiven auf ein Korpus. Kombinationen quantitativ-qualitativer Analysemethoden zur Ermittlung von Textgliederungsprinzipien,"Susanne Haaf (Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland)",quantitativ-qualitative Analyse; Methodendiskussion; Korpuslinguistik; TEI-Analyse; Textgliederungsprinzipien,"Strukturanalyse, Kontextsetzung, Visualisierung, Text","Im Bereich digital basierter Untersuchungen wird zunehmend eine Verzahnung quantitativen und qualitativen Arbeitens gefordert. In der konkreten Arbeit der Korpusanalyse wird aus dieser scheinbaren Dichotomie jedoch schnell eine Methodenvielfalt, denn gerade durch Kombinationen verschiedener Perspektiven auf die Daten werden unterschiedliche Phänomene greifbar und entfaltet sich das volle Potential quantitativ-qualitativen Arbeitens. Das hier präsentierte Poster soll dies an einem konkreten Beispiel veranschaulichen. Inhaltlicher Ausgangspunkt ist die Frage nach Textgliederungsprinzipien, welche für bestimmte erbauliche Textsorten kennzeichnend sind. Mittel der Textgliederung können als Mittel der Markierung von Teiltexten innerhalb eines Gesamttextes beschrieben werden (Hausendorf/Kesselheim 2008: 41) und können wiederum für Textsorten charakteristisch sein. Sie finden sich auf verschiedenen Ebenen des Textes, u.a. im Bereich der Typographie (Stein 2003: 422). Gerade diese typographischen Gliederungsmerkmale stellen einen guten Ausgangspunkt für eine quantitative Analyse von Textgliederungsmerkmalen dar, da sie in TEI-annotierten Korpora z.B. durch die XML-Strukturierung automatisch greifbar werden. Anders als bei dem Verfahren, textbezogene Phänomene reduziert auf bestimmte TEI-Strukturen zu untersuchen (z.B. Schöch 2016: 351ff., Haaf 2016), gelangen hier die im Korpusvergleich möglicherweise signifikanten Häufigkeiten der TEI-Strukturierungen selbst in den Blick. Die inhaltliche Frage nach Textgliederungsprinzipien erbaulicher Textsorten wird ausführlich behandelt in Haaf (in Vorber.). Im vorliegenden Beitrag stehen 'der thematischen Ausrichtung der Konferenz entgegenkommend 'Überlegungen zur adäquaten Methodik einer solchen Untersuchung im Vordergrund. Der hier präsentierten Studie liegen drei Teilkorpora des 17. Jahrhunderts aus dem Deutschen Textarchiv (2017) zugrunde: Die Texte des DTA-Korpus wurden nach einheitlichen Richtlinien und mittels eines TEI-Subsets, das Ambiguitäten der Auszeichnung möglichst reduziert, ausgezeichnet (Haaf et al. 2014/15). Für die vorliegende Untersuchung wurden einzelne TEI-Strukturen hinsichtlich der Häufigkeit ihres Auftretens (relativ zur Token-Anzahl) und ihrer Verteilung im jeweiligen Korpus verglichen, um speziell die Unterschiede in der Textgliederung zwischen den untersuchten Textsorten herauszuarbeiten. Dabei wurden solche Tags einbezogen, die voraussichtlich Textgliederungsmerkmale repräsentieren. So kann z.B. tei:div die Kapitelstruktur eines Textes anzeigen, durch tei:l wird der Wechsel zwischen Prosatext und Lyrik greifbar, tei:note zeigt Metatexte in Form von Anmerkungen, z.B. Marginalien, an, tei:hi repräsentiert Hervorhebungen von Textpassagen gegenüber dem Grundtext. Die Ergebnisse wurden einer qualitativen Beurteilung unterzogen. Zur Evaluation eines Merkmals war hier nicht allein der Blick auf seine relativen Häufigkeiten in und deren signifikante Unterschiede zwischen den untersuchten Korpora relevant. Die signifikant erhöhte Häufigkeit eines Merkmals kann vielmehr unterschiedliche Gründe haben. So kann sie einerseits zwar durchaus (1) auf die höhere Relevanz des Merkmals im Korpus hindeuten, wie sich am Merkmal der Marginalie zeigt, das signifikant häufig und regelmäßig verteilt im Korpus der Funeralschriften auftritt (Abb. 1). Sie kann andererseits aber auch (2) aufgrund der unausgewogenen Verteilung des Merkmals im Korpus gar nicht aussagekräftig sein, entweder weil (2a) sich das Korpus selbst als in sich unausgewogen und nicht repräsentativ für den zu beschreibenden Gegenstand erweist oder weil (2b) das Merkmal im gegebenen Kontext nicht relevant ist, wie etwa die horizontale Trennlinie zwischen Textteilen, die in allen drei Vergleichskorpora unregelmäßig verteilt war (Abb. 2). Andererseits kann es (3) auch vorkommen, dass die bestehende Ausgewogenheit der Verteilung eines Merkmals in einem Korpus letztlich nicht aussagekräftig für dessen Relevanz ist. Weiterhin konnten im gegebenen Kontext (4) auch Merkmale mit geringeren Häufigkeiten relevant sein, und schließlich ist nicht zuletzt (5) auch der Ort, an dem ein Merkmal im Dokument auftritt, zu berücksichtigen. Beide Aspekte (4 und 5) zeigen sich z.B. am Merkmal der Liste, die in den erbaulichen Prosawerken erwartungsgemäß selten, aber relativ regelmäßig auftritt, und zwar in Form von Registern am Buchbeginn oder Buchende (in tei:front oder tei:back). Methodisch zeigte sich also, dass für eine adäquate Beurteilung der untersuchten Merkmale verschiedene Blickwinkel notwendig sind. Das Poster veranschaulicht anhand der erwähnten und weiterer Beispiele diese genannten methodischen Aspekte. Inhaltlich führte die Untersuchung zutage, dass z.B. Merkmale, die den Zugang zum Text erleichtern und Hilfe zur Orientierung im Text geben, für die erbaulichen Textsorten relevant sind (Näheres vgl. Haaf (in Vorber.))."
2018,DHd2018,FICHTNER_Mark_Von_Drupal_8_zur_virtuellen_Forschungsumgebung.xml,Von Drupal 8 zur virtuellen Forschungsumgebung - Der WissKI-Ansatz,"Mark Fichtner (Germanisches Nationalmuseum, Deutschland)","WissKI, CIDOC CRM, Ontologie, OWL DL, Drupal","Modellierung, Kollaboration, Organisation, Visualisierung, Metadaten, virtuelle Forschungsumgebungen","Im Rahmen des von der DFG finanzierten Projekts ""WissKI"" entstand in zwei Projektphasen eine digitale Forschungsumgebung für die Anwendung im Bereich der Digital Humanities. Mit dem Ende der zweiten Projektphase 2017 wurde die Forschungsumgebung grundlegend aktualisiert und setzt nun auf das Open Source Content Management System Drupal 8 auf. Damit ging eine Aktualisierung der gesamten zugrundeliegenden Frameworks und Technologien (php 7, SPARQL 1.1) einher. Die aktuelle Version der Forschungsumgebung steht nun der wissenschaftlichen Öffentlichkeit als Open Source zur freien Verfügung. Auch die aktuelle Fassung der Software setzt auf die bewährten Kernaspekte: Die Datenerfassung und -haltung in WissKI wird zentral bestimmt durch die semantischen Zusammenhänge zwischen einzelnen Fakten und Datensätzen. Dies wird durch umfassende Unterstützung aktueller Semantic Web Technologien erreicht. Die Einordnung und Speicherung der erhobenen Daten erfolgt auf Grundlage einer Domänenontologie, deren Konzepte und Relationen - zu sogenannten Pfaden verbunden - als Vorlage für die Masken und Felder im System dienen. Auf Basis dieser Technologie werden solitär erscheinende Daten zu einem gemeinsamen, semantischen Netzwerk verbunden und damit die unmittelbare Sichtbarkeit weiterer, tiefergehender Zusammenhänge ermöglicht. Hierdurch werden intuitiv Zusammenhänge in den Daten sichtbar, die sich für den Nutzer als Mehrwert anbieten. Das Webbasierte Systemdesign und der dadurch ermöglichte Zugriff über das Internet, die Anbindung von externen kuratierten Datenquellen (sog. Authority Files) und die Möglichkeit zur Bereitstellung ausgewählter Daten über gängige Online-Schnittstellen (Web-Frontend, SPARQL-Endpoint, ...) betonen den Semantic-Web-Gedanken hinter der Infrastruktur. Die Speicherung der Daten erfolgt in einem TripleStore, der die eingegebenen Fakten in einer Subjekt-Prädikat-Objekt-Satzform ablegt. Die Aneinanderreihung der hier verwendeten Prädikate zu Pfaden erfolgt im Kern des Systems, dem sogenannten Pathbuilder, mit dem die semantische Bedeutung der einzelnen Einträge in Bezug auf das beschriebene Objekt (auch Person, Ort o. Ö.) anhand der Ontologie festgelegt wird. Die Eingabe der Daten erfolgt über eine, mit den gängigen Datenbankoberflächen vergleichbare, Editier-Oberfläche. Sie ist aus Feldern aufgebaut, die wiederum je einem bestimmten Feldtyp zugeordnet sind. Feldtypen bestimmen die Ein- und Ausgabemodalitäten der Daten. Dabei verzichtet die Software nicht auf die aus dem Bereich der Content Management Systeme bekannten Funktionalitäten wie z. B. die Generierung von Websites, Foren, Wikis oder auch die detaillierte Verwaltung der Nutzer und ihrer Zugriffsrechte. Inzwischen ist die Software in verschiedenen Forschungsprojekten an unterschiedlichen, namhaften Institutionen im kunst- und kulturhistorischen, sowie biologischen und technischen Bereich erfolgreich im Einsatz. Als Domänenontologie im Museums- und Sammlungsbetrieb kommen individuelle Erweiterungen des ""Conceptual Reference Model"" des Comité international pour la documentation zum Einsatz (CIDOC-CRM: ISO 21127), dessen Umsetzung in der Web-Ontology-Language OWL ebenfalls vom Projekt besorgt wurde und über die Website http://erlangen-crm.org frei zur Verfügung steht. Das Poster stellt den aktuellen Stand der WissKI-Software nach Vollendung der beiden Projektphasen dar. Neben dem bewährten Modell der Anpassung der Software durch die beiden am DFG-Projekt beteiligten Museen und der Friedrich-Alexander-Universität Erlangen-Nürnberg unterstützt die Interessengemeinschaft für semantische Datenverarbeitung e.V. (http://www.igsd-ev.de/) die gemeinnützigen Aspekte der Software weiter. Darüber hinaus werden bewusst auch Dritte zum Einsatz und zur Anpassung von WissKI eingeladen. Daraus resultierte im vergangenen Jahr der zahlreiche Einsatz der Software in Forschungsprojekten z.B. in Kooperation mit der Landesstelle der Nichtstaatlichen Museen in Bayern oder dem Zentralinstitut für Kunstgeschichte. Das System stellte v.a. durch die Nutzung aller Drupal-Basis-Funktionalitäten wie z.B. ""Views"" seine Stärken unter Beweis. So können neben den altbewährten Textfeldern und -bereichen und Bildern (incl. Zoomviewer für sehr hochauflösende Bilder) auch interaktive Landkarten, 3D-Animationen, Zeitstrahlen und alle denkbaren Medientypen, sowohl zur direkten Ansicht als auch zum Download als Funktionalität genutzt werden. Zusätzlich zu diesen gängigen Formaten ermöglicht die Standardkonformität von WissKI-D8 auch die Einbindung anderer, gängiger Feldtypmodule, die für Drupal 8 zur Verfügung stehen. Zu den erwähnten Erleichterungen zählt ebenso ein Update des System-Kerns, dem Pathbuilder, mit dem die Pfadschablonen durch die Domänenontologie auf einer graphischen Oberfläche ausgewählt bzw. erzeugt werden können. Daneben wird eine umfassende Bibliothek mit Musterontologien, -masken und -pfaden bereitgestellt, die die Einstiegshürde für Erstbenutzer minimal zu halten."
2018,DHd2018,DECLERCK_Thierry_Formalisierung_von_Märchen.xml,Formalisierung von Märchen,"Thierry Declerck (DFKI GmbH, Deutschland); Anastasija Aman (Universität des Saarlandes, Deutschland); Stefan Grünewald (Universität des Saarlandes, Deutschland); Matthias Lindemann (Universität des Saarlandes, Deutschland); Lisa Schäfer (Universität des Saarlandes, Deutschland); Natalia Skachkova (Universität des Saarlandes, Deutschland)","Formalisierung, XML, Python, Märchen","Programmierung, Inhaltsanalyse, Modellierung, Projektmanagement","Im Rahmen eines Softwareprojektes Wir beschreiben in diesem Beitrag zum einen, welche Informationen in dieser formalen Repräsentation enthalten sind, und zum anderen, wie diese Informationen in XML bzw. Python konkret codiert werden. Ein Märchen besteht im Sinne unseres Projektes aus den folgenden Bestandteilen: Im Folgenden werden die verschiedenen Bestandteile, sowie ihre Eigenschaften und Beziehungen untereinander, näher beschrieben.   Die oben beschriebenen Informationen lassen sich im XML-Format darstellen. Dabei wird eine XML-Baumstruktur genutzt, um die Hierarchie der verschiedenen Objekte zu repräsentieren. Das Wurzelelement des Dokuments hat stets den Bezeichner Tale und die Attribute ""title"" und ""annotator"", welche Titel und den Namen des Annotators des jeweiligen Märchens enthalten: <Tale title=""Froschkönig"" annotator=""Lisa Schäfer""> ... </Tale> Diesem Element untergeordnet sind die Elemente Characters, Locations und Text. Das Characters-Element enthält Character-Subelemente, die jeweils die gesammelten Informationen für einen Charakter speichern: <Character id=""ch1"" name=""Frosch"" age=""adult"" gender=""male"" type=""animal_monster"" subtype=""small"" attitude=""neutral"" archetype=""hero""> </Character> Analog dazu enthält das Locations-Element untergeordnete Location-Elemente, die jeweils einen Ort codieren: <Location id=""loc1"" type=""WALD""> </Location> Das Text-Element enthält schließlich den eigentlichen Märchentext. Dieser ist auf die verschiedenen Szenen 'repräsentiert durch Szene-Elemente 'aufgeteilt, welche wiederum die verschiedenen Dialogakte (Dialog-Elemente) enthalten: <Text><Scene id=""s2"" time=""t2"" location=""loc1"" characters=""ch1,ch2"" propp_functions=""d|e"" propp_subfunctions=""D7|E10"" transition=""gehen""> ... <Dialogue id=""d5"" time=""t2.4"" speaker=""ch2"" receiver=""ch1""> Ach, du bist""s, alter Wasserpatscher, </Dialogue> ... </Scene></Text> Beim Entwurf des XML-Schemas wurde besonders Wert auf Übersichtlichkeit und Leserlichkeit gelegt. Trotz der Vielzahl der kodierten Informationen sind die resultierenden XML-Dateien daher vergleichsweise kompakt; so besteht die XML-Repräsentation des (vergleichsweise langen) Märchens "" Diese XML Repräsentation basiert auf und erweitert das Annotation Schema, das in (Scheidel & Declerck, 2010) beschrieben wird. Auf der Grundlage der oben beschriebenen XML-Struktur kann eine Python-Klassenstruktur aufgebaut werden, die ein Märchen sowie seine einzelnen Teile als Python-Objekte repräsentiert. Neben einer Oberklasse Tale gibt es für jeden der oben beschriebenen Teile eine eigene Python-Klasse, d. h. die Klassen Location, Character, Scene und Dialogue. (Insgesamt bestehen die Dateien zur Märchen-Repräsentation aus 288 Zeilen Code.) Jede Klasse enthält dabei als Attribute die oben beschriebenen Eigenschaften, wobei diese auch Verweise auf andere Elemente darstellen können. So verweisen bspw. Dialogue-Objekte auf die Character-Objekte von Sprecher und Empfängern. Der Python-Code dient als Interface für drei Anwendungen. Erstens können Märchen aus bestehenden XML-Dateien eingelesen werden; zweitens können XML-Dateien anhand einer anderweitig (z. B. durch automatische Klassifizierung) erzeugten Python-Märchenstruktur generiert werden; und drittens kann anderer Python-Code auf die Märchen-Information zugreifen, was die Grundlage für Anwendungen wie Text-to-Speech oder Visualisierung bildet. Sowohl die XML Kodierung als auch die Python Objekte interagieren mit einer Märchen-Ontologie interagieren, die eine Erweiterung der in (Koleva et al., 2012) beschriebenen Ontologie ist. Somit haben wir eine formale Repräsentation von Märchen, die in verschiedenen Anwendungen zum Tragen kommen kann."
2018,DHd2018,REITER_Nils_SANTA__Systematische_Analyse_Narrativer_Texte_du.xml,SANTA: Systematische Analyse Narrativer Texte durch Annotation,"Evelyn Gius (Universität Hamburg, Deutschland); Nils Reiter (Universität Stuttgart, Deutschland); Jannik Strötgen (Max-Planck-Institut für Informatik, Saarbrücken, Deutschland); Marcus Willand (Universität Stuttgart, Deutschland)","shared task, narratologie, annotationsrichtlinien","Inhaltsanalyse, Modellierung, Annotieren, Theoretisierung, Community-Bildung, Literatur","In diesem Beitrag wollen wir ein Vorhaben zur Diskussion stellen, das an zwei zentralen Herausforderungen in den Digital Humanities ansetzt: Der Erstellung adäquater Annotationsrichtlinien für geisteswissenschaftlich relevante textuelle Konzepte und der Schnittstelle in der Kooperation zwischen beteiligten Wissenschaftlerinnen und Wissenschaftlern aus Geisteswissenschaft und Informatik. Für DH-Projekte sind Kooperationen unerlässlich, wenn fortgeschrittene Techniken zur Textanalyse eingesetzt werden und/oder es um eine Zusammenführung von Konzepten oder Zugangsweisen geht, die bereits intradisziplinär als komplex gelten. Dabei wird ein signifikanter Anteil der Projektlaufzeit auf die Entwicklung einer ""gemeinsamen Sprache"" und die Identifikation der exakten, gemeinsamen wissenschaftlichen Fragestellung verwendet. Dies ist zweifellos ein produktiver Prozess, dessen erfolgreiche Durchführung allerdings voraussetzt, dass auf beiden Seiten Forscherinnen und Forscher beteiligt sind, die sich auf das interdisziplinäre Vorgehen voll einlassen und auch den nötigen Zeitaufwand tragen. Methodisch-technisch ist ein substanzielles Nadelöhr bei der Entwicklung automatischer Werkzeuge das Fehlen von annotierten Goldstandards, an/auf denen Werkzeuge trainiert, verglichen und feinjustiert werden können. Das Fehlen der Goldstandards ist jedoch eigentlich ein nachgelagertes Problem, wie sich z.B. in narratologisch orientierten Projekten zeigt (heureCLéA: Bögel et al., 2015; Propp annotation: Fisseni et al., 2014): Die Umsetzung narratologischer Theorien als Annotationen ist alles andere als trivial, da narratologische Konzepte nicht im Hinblick auf Annotation entwickelt wurden. Leerstellen in den Definitionen müssen gefüllt, Voraussetzungen geklärt und Unterkategorien geklärt werden. Die Annotation solcher Kategorien ist also kein reiner Umsetzungs- oder Implementierungsprozess, sondern einer bei dem sich tiefe, konzeptionelle Fragen stellen. Als Ergebnis solcher Prozesse stehen dann Annotationsrichtlinien, die die Brücke zwischen Theorie und Praxis schlagen. Erst wenn Annotationsrichtlinien für ein Phänomen (oder eine Gruppe von Phänomenen) etabliert sind, können größere Annotationsprojekte mit Aussicht auf Erfolg durchgeführt werden. Das von uns vorgeschlagene Vorgehen erlaubt den Beteiligten Forscherinnen und Forschern ihre Expertise einzubringen, ohne in einem gemeinsamen Projektkontext zu arbeiten. Die Schnittstelle zwischen D und H wird hierbei von annotierten Daten und Annotationsrichtlinien gebildet, wobei die Richtlinien ohne Kompromisse bezüglich möglicher Automatisierungen erstellt werden. Das Vorhaben gibt somit auch narratologisch/literaturwissenschaftlich anspruchsvoller Konzeptentwicklung und damit Theoriebildung einen Rahmen. Verfügbare annotierte Daten wiederum erlauben Informatikerinnen und Informatikern ohne Expertise in narratologischen Fragen die Entwicklung von Werkzeugen für komplexe technische Probleme. Shared Tasks sind in der Computerlinguistik weit verbreitet und haben für viele Bereiche gezeigt, dass sie ein geeignetes Instrument sind, Forschungsbemühungen verschiedener Gruppen zum gleichen Thema zu bündeln und zu verstärken. In einem Als Rahmen für die Entwicklung von Annotationsrichtlinien organisieren wir einen shared task der sich genau auf dieses Ziel konzentriert (Phase 1: Erstellung von Guidelines). Sind die Richtlinien etabliert, kann anschließend ein großes Korpus annotiert werden, das wiederum in einem NLP-shared task eingesetzt werden kann, um Verfahren zu erproben, die die annotierten Phänomene automatisch finden (Phase 2: Automatisierung). Als Phänomen haben wir uns dabei auf Erzählebenen in englischen und deutschen Texten festgelegt, da diese für zahlreiche, komplexere literaturwissenschaftliche Fragestellungen hilfreich sind, ohne selbst (für einen ersten Während Details zum Gesamtaufbau des Shared Tasks bereits in einem anderen Artikel beschrieben wurden (Reiter et al., 2017), fokussieren wir uns in diesem Beitrag auf die genauere Beschreibung der ersten Phase des (bis Mitte Juni 2018) Im ersten Schritt wird allen Teilnehmerinnen und Teilnehmern ein Die Texte können und sollen von den Teilnehmerinnen und Teilnehmern benutzt werden, um Richtlinien für die Annotation von Erzählebenen zu entwickeln und zu testen. Ob die Texte in einer oder in beiden Sprachen verwendet werden, ist dabei den Teilnehmerinnen und Teilnehmern überlassen. Sie sollten dabei das Ziel verfolgen, eine möglichst breite Anwendbarkeit der Richtlinien sicherzustellen (auch jenseits des Wie genau die Gruppen dabei vorgehen, bleibt ihnen überlassen. In vergangenen Annotationsprojekten (mit und ohne Bezug zu Literaturwissenschaft bzw. literarischen Texten) hat sich aber ein iterativer Prozess als fruchtbar erwiesen. Sobald eine erste Version der Richtlinien erstellt wurde, werden sie auf neuen Texten getestet, um ihre Definitionslücken oder Vagheiten zu identifizieren. Aus dem Schließen der Lücken ergibt sich dann eine weitere Version der Richtlinien, die wiederum auf Texten getestet werden können. (bis Ende Juni 2018) Im zweiten Schritt sollen die Arbeitsgruppen ihre eigenen Richtlinien auf neuen Texten testen. Nach dem Einreichen ihrer Richtlinien erhalten die Teilnehmerinnen und Teilnehmer hierzu sechs neue literarische Texte, die vom Organisationsteams des (bis Mitte Juli 2018) Im dritten Schritt erhält jede teilnehmende Gruppe Richtlinien anderer Gruppen, auf deren Basis Erzählebenen in den sechs Texten erneut annotiert werden, wobei alle Richtlinien von uns zuvor anonymisiert werden. Zusätzlich wird auch eine vom Organisationsteam betreute Gruppe von studentischen Hilfskräften alle eingereichten Annotationsrichtlinien auf den sechs Texten anwenden. (August/September 2018) Im letzten Schritt der ersten Phase des Zur vergleichenden Evaluation von Annotationsrichtlinien sind bisher Ansätze aus der Computer- und Korpuslinguistik zur quantitativen Messung des Inter-Annotator-Agreement (IAA) bekannt (vgl. Artstein, 2017), die im Bereich der Digital Humanities angewendet wurden und werden. Da es aber bei der Erstellung von Annotationsrichtlinien für narratologische Phänomene eben nicht Im Rahmen des Vortrags wollen wir insbesondere zwei der o.g. Aspekte in den Fokus rücken und diskutieren: Die iterative Entwicklung von Annotationsrichtlinien als verteiltes, kollaboratives Projekt sowie die Evaluation und Vergleichbarkeit von Annotationsrichtlinien für literarische Phänomene."
2018,DHd2018,PIELSTRÖM_Steffen__Embedded_Humanities.xml,Embedded Humanities,"Fotis Jannidis (Universität Würzburg, Deutschland); Mike Kestemont (Universität Antwerpen, Belgien)","distributional semantics, word2vec, topic modeling","Inhaltsanalyse, Strukturanalyse, Modellierung","In recent years, the Digital Humanities have witnessed the steadily growing popularity of models from distributional semantics which can be used to model the meaning of documents and words in large digital text collections. Well-known examples of influential distributional models include Latent Dirichlet Allocation for topic modelling (Blei et al.) or Word2vec for estimating word vectors (Mikolov et al. 2013). Such distributional models have recently gained much prominence in the fields of Natural Language Processing and, more recently, Deep Representation Learning (Manning 2016). Humanities data is typically sparse and distributional models help scholars obtain smoother estimations of them. Whereas, for instance, words are conventionally encoded as binary ""one-hot vectors"" in digital text analysis, embedding techniques from distributional semantics allow scholars to obtain dense, yet rich representations of vocabularies. These embedded representations are known to capture all sorts of valuable relationships between data points, although embedding techniques are typically trained using unsupervised objectives and require relatively little parameter tuning from scholars. Inspiring applications of this emergent technology in DH have ranged from more technical work in cultural studies at large (Bamman et al. 2014), case studies in literary history (Mimno 2012; Schoech 2017) or valuable DH-oriented web apps, such as ShiCo (Martinez-Ortiz et al. 2016). The availability of high-quality implementations in the public domain, in software suites as gensim, word2vec, or mallet etc. has greatly added these methods"" popularity. In spite of their huge potential for Digital Humanities, multiple aspects of their application still remain untapped. Unsupervised models such as Word2vec, for instance, are notoriously hard to evaluate directly 'often researchers have to resort to indirect evaluations in this respect. This renders it intriguing to which extent the output of distributional models should play a decisive role in hermeneutical debates or controversies in the Humanities. With other techniques for Distant Reading, distributional models moreover share the drawback that they typically only yield a The DARIAH working group on Text and Data Analytics (@dariahtdawg), in collaboration with the FWO-sponsored scientific community Digital Humanities Flanders (DHuF) proposes to collocate a one-day workshop with the 2018 DHd conference in Cologne. The workshop aims to bring together ca.  10-12 practitioners from the Digital Humanities to present and discuss recent advances in the field, through 30-minute presentations on focused case studies, including work-in-progress or theoretical contributions. Additionally, the workshop aims to reach an audience of non-presenting participants who take an active interest in distributional models and who are planning to apply distributional models to their own data in the near future. We aim to bring together a diverse group of both junior and senior stakeholders in this nascent subfield of DH. The goal of the workshop is to identify the state of the art in the field, identify common challenges and share recommendations for a best practice. Special attention will be given to the (both hermeneutic and quantitative) evaluation of distributional models in the context of Humanities research, which remains a challenging issue. The workshop is open to scholars from all backgrounds with an interest in semantic representation learning and encourages submissions that deal with under-researched resource-scarce and/or historic languages. Abstracts (between 250 and 300 words, not including references) can be submitted to mike.kestemont@uantwerp.be. The¬†workshop¬†also explicitly welcomes submissions presenting previously published research which is of interest to the DH community (although this work should not overlap strongly with work presented at the main conference). Topics which seem of special interest to the DH community nowadays include, but are not limited to: In terms of technical requirements, the workshop would need a beamer to project from a laptop. As keynote speaker, we have found David Bamman (University of California, Berkeley) willing to join our workshop and give a plenary lecture. Bamman is an authority in the field and will certainly increase the attractiveness of the workshop to potential participants. Fotis Jannidis (University of Würzburg, Germany) Fotis.jannidis@uni-wuerzburg.de, Fotis holds the chair for literary computing in the department of German studies at the University of Würzburg. In the last years, the main focus of his work is the computational analysis of larger collections of literature, especially narrative texts. He is interested in developing new research methods for this new subfield of literary studies, but also in new applications for established methods and also in a better understanding, why successful algorithms in this field work. Mike Kestemont (University of Antwerp, Belgium) mike.kestemont@uantwerp.be,  Mike is a tenure track research professor in the department of Literature the University of Antwerp. He specializes in computational text analysis for the Humanities, in particular stylometry or computational stylistics. He has published on the topic of authorship attribution in various fields, such as Classics or medieval European literature. Mike actively engages in the debate surrounding the Digital Humanities and attempts to merge methods from Artificial Intelligence with traditional scholarship in the Humanities. He recently took up an interest in so-called ""deep"" representation learning using neural networks."
2018,DHd2018,KRAUTTER_Benjamin_Quantitatives__close_reading___Vier_mikroa.xml,"Quantitatives ""close reading""? Vier mikroanalytische Methoden der digitalen Dramenanalyse im Vergleich.","Benjamin Krautter (Universität Stuttgart, Deutschland)","Stilometrie, Kopräsenz, Drama, Wortfeldsemantik, Sentiment","Inhaltsanalyse, Strukturanalyse, Kontextsetzung, Stilistische Analyse, Literatur","Jüngste Ergebnisse der computergestützten Forschung legen nahe, dass Romanfiguren 'gemessen an ihrer Figurenrede 'von den jeweiligen Autoren stilistisch distinktiv angelegt werden können (Hoover 2017; Fields, Bassist, Roper 2017). Versierte Autoren könnten ihren Figuren also sogenannte ""distinctive voices"" einschreiben, die sich stilometrisch identifizieren lassen. Anders als bei Autorschafts-, Gattungs- oder Epochensignalen handelt es sich hierbei um ein Erstaunlicherweise beschränken sich die Studien zur stilistischen Differenzierung von Figurenrede größtenteils auf Romane. Dabei ist es doch gerade die Struktur dramatischer Texte, die eine quantitative Untersuchung der Figurenrede plausibel erscheinen lässt 'die Rede wird nicht von einem Erzähler sortiert, kommentiert und in einen Rahmen gebettet. Auch erste Forschungsansätze sind durchaus vorhanden: John Burrows und Hugh Craig zeigen etwa, dass einzelne Dramenfiguren sehr wohl erfolgreich einem Autorsignal zugeordnet werden können (Burrows, Craig 2012). Sie reagieren damit interessanterweise auf Kritiker, die die erfolgreiche Autorschaftsattribution von Dramentexten aufgrund der vielen verschiedenartigen Stimmen 'weil es also keinen Erzähler gibt 'in Frage stellen (Masten 1997). Nachfolgend soll geprüft werden, inwieweit sich Hoovers Vorgehen (2017) zur Ermittlung distinktiver Figurenrede auch auf dramatische Texte übertragen lässt. Die Ergebnisse der stilometrischen Untersuchung werden im Anschluss durch drei weitere quantitative Analyseverfahren kontextualisiert und zugleich kritisch hinterfragt.  Die stilometrische Analyse von Ein einzelnes Dendrogram darf für die Bewertung der Hypothese jedoch nicht mehr sein als ein erstes Indiz, zumal die Segmentierung nicht gemäß einer festen Länge mit vorgegebener Wortanzahl, sondern nach den Aktgrenzen vorgenommen wurde. Um ein potentielles ""Cherry Picking""-Problem an dieser Stelle zu vermeiden, ergänzt  Ein Öhnlichkeitssignal, das Figurentypen, etwa den zärtlichen Vater in den bürgerlichen Trauerspielen Lessings, über das einzelne Drama hinweg verbinden würde, ist zumindest auf diese Weise nicht auszumachen. Die Vermutung liegt nahe, schreibt Lessing seine Dramen doch dezidiert für die am Theater üblichen Rollenfächer des 18. Jahrhunderts (Harris 1992). Sie scheint jedoch nicht auf diese Weise stilometrisch operationalisierbar zu sein. Stilometrische Analysen sind nicht das einzige Verfahren, um relative Öhnlichkeiten innerhalb eines Textkorpus zu bestimmen. Inwieweit sie geeignet sind, offene Fragestellungen 'im Gegensatz etwa zur Autorschaftsattribution 'zu erörtern, ist überhaupt noch zu prüfen. Sollten Parameter wie Distanzmaß, Wortumfang oder Culling tatsächlich je nach Textkorpus neu zu bestimmen sein, wären ""Cherry Picking""-Probleme der Methode inhärent (Schöch 2014; Jannidis 2014; Eder 2013). Nachfolgend ist es deshalb geboten, die bisherigen Beobachtungen weiteren quantitativen Verfahren gegenüberzustellen. Dazu dienen Analysen der Kopräsenz, der Figurensemantik und der Empfindung, sogenannte Sentiment-Analysen.  Die Tabelle in Um diesen Befund weiter zu spezifizieren, soll eine semantische Wortfeldanalyse die thematische Konzeption der Figurenrede operationalisieren (Willand, Reiter 2017). Während also Die Sentiment-Analyse in  Die nähere Betrachtung der Figurenrede in Die vorliegende Arbeit wurde im Rahmen des Projekts ""Quantitative Drama Analytics"" (QuaDramA) durchgeführt, das von der VolkswagenStiftung finanziert wird."
2018,DHd2018,SCHMIDT_Thomas__Kann_man_denn_auch_nicht_lachend_sehr_ernsth.xml,"""Kann man denn auch nicht lachend sehr ernsthaft sein?"" 'Zum Einsatz von Sentiment Analyse-Verfahren für die quantitative Untersuchung von Lessings Dramen","Thomas Schmidt (Lehrstuhl für Medieninformatik, Universität Regensburg); Manuel Burghardt (Lehrstuhl für Medieninformatik, Universität Regensburg); Katrin Dennerlein (Institut für Deutsche Philologie, Julius-Maximilians-Universität Würzburg)",Sentiment Analysis; Dramenanalyse;,"Inhaltsanalyse, Literatur, Methoden, Text, Werkzeuge","Sentiment Analyse (SA) beschreibt eine Reihe von computergestützten Methoden zur Prädiktion der Polarität eines Texts, versucht also vereinfacht gesagt automatisiert herauszufinden, ob ein Text ein positives oder negatives Gefühl ausdrückt (Liu 2016). Darüber hinaus werden teilweise auch komplexere emotionale Kategorien (wie z.B. Zorn und Freude) betrachtet (Mohammad & Turney 2010). Zentrale Anwendungsfelder der SA sind bislang vor allem die Analyse von Online-Reviews (McGlohan, Glance & Reiter 2010) und Social Media-Daten (Kouloumpis, Wilson & Moore 2011). Zur Analyse von literarischen Texten mittels SA-Techniken finden sich bislang nur wenige Studien, z.B. zu Märchen (Alm, Roth & Sproat 2005) und Romanen (Kakkonen & Kakkonen 2011; Elsner 2012; Jannidis et al. 2016). Auf größeren Textkorpora wurde getestet, inwiefern SA-Werte eines Textes und Emotionskurven von Texten zur Genreklassifikation verwendet werden können (Kim, Padó & Klinger 2017) und wie begriffsgeschichtliche Bedeutungsverschiebungen in literarischen Texten mithilfe von erweiterten SA-Methoden erforscht werden können (Buechel, Hellrich & Hahn 2017). In Dramentexten hat man bisher die Verteilung von emotionalen Kategorien (Mohammad 2011) oder die Entwicklung von Figurenbeziehungen (Nalisnick & Baird 2013) in Shakespeare-Dramen untersucht. Auch der vorliegende Beitrag beschäftigt sich mit dem Einsatz von SA im Bereich der Dramenanalyse. Es werden erstmals systematisch verschiedene Methoden der SA für Dramen getestet und evaluiert. Zudem wird exploriert, inwiefern bisher in der Literaturwissenschaft erforschte Aspekte von Dramen mithilfe der SA erfasst werden und inwiefern die SA auch für die Gewinnung neuer literaturwissenschaftlicher Erkenntnisse eingesetzt werden kann. Das im Rahmen dieser Studie verwendete Lessing-Korpus umfasst ein mit Strukturinformationen annotiertes Dramenkorpus mit 11 Dramen, bestehend aus insgesamt 8224 Einzelrepliken. Sämtliche Dramen wurden über die Plattform Innerhalb der SA unterscheidet man zwei wesentliche Ansätze: (1) die Nutzung maschinellen Lernens und (2) die Verwendung lexikonbasierter Verfahren. Für das erstgenannte Vorgehen ist typischerweise ein mit Sentiment-Informationen annotiertes Trainingskorpus notwendig (D""Andrea et al. 2015), welches für die Dramenanalyse bislang nicht vorliegt. Aus diesem Grund werden in der vorliegenden Arbeit lexikonbasierte Verfahren eingesetzt. Ein Sentiment-Lexikon ist dabei eine Wortliste, in der für jedes Wort Sentiment-Informationen angegeben sind (Liu 2016: 10), also z.B. ob es positiv oder negativ konnotiert ist und in welchem Ausmaß (Polaritätsstärke). Ein derartiges Wort nennt man auch Folgende SA-Optionen wurden in unterschiedlichen Kombinationen systematisch evaluiert:   Alle nachfolgenden Berechnungen wurden bezüglich aller kombinatorischen Möglichkeiten der soeben beschriebenen SA-Parameter durchgeführt. Dabei werden die jeweiligen SA-Metriken nach Term-Zähl-Methodik (Kennedy & Inkpen 2006) berechnet, d.h. ein Text wird hinsichtlich vorhandener SBWs untersucht, positive und negative Wörter ausgezählt und für einen Polaritätswert die positive von der negativen Zahl subtrahiert. SA-Metriken wurden auf folgenden Ebenen über die jeweils zugehörigen Texte kalkuliert: Drama, Akte, Szenen, Repliken sowie Sprecher und Sprecherbeziehungen pro Drama, Akt, Szene und Replik. Die Beziehungen zwischen den Figuren wurden nach einer Heuristik von Nalisnick & Baird (2013) berechnet. Zur systematischen Evaluation der Prädiktionsleistung der verschiedenen SA-Ansätze wurde ein Evaluationskorpus bestehend aus 200 Repliken erstellt. Bei der Auswahl der Repliken wurde darauf geachtet, dass die dramenspezifische Verteilung berücksichtigt wird, längere Dramen sind also mit mehr Repliken vertreten. Ferner wurden nur solche Repliken aufgenommen, die mindestens 19 Wörter umfassen. Diese Länge entspricht etwa -25% des Mittelwerts des Gesamtkorpus und vermeidet damit die Selektion von zu kurzen Repliken. Es wurde insgesamt auf eine gleichmäßige Längenverteilung geachtet. Die Repliken wurden von insgesamt fünf Personen (4 weiblich, 1 männlich; alle jeweils mit Deutsch als Muttersprache) jeweils unabhängig voneinander bezüglich deren Polaritätswirkung bewertet. Die Polarität jeder Replik wurde jeweils sechswertig (sehr negativ, negativ, neutral, gemischt, positiv, sehr positiv) und binär (positiv, negativ) bewertet. Die Annotationen wurden bezüglich des Übereinstimmungsgrades analysiert. Dazu wurden das Übereinstimmungsmaß Fleiss"" Kappa (Fleiss 1971) sowie der Durchschnittswert der prozentualen Übereinstimmung aller Annotatoren und Annotatorinnen berechnet (vgl. Tabelle 1). Man erkennt eine geringe Übereinstimmung für die Bewertungsskala mit sechsstufiger Polarität und eine moderate Übereinstimmung für die binäre Variante. Die Ergebnisse verhalten sich konform zu verwandten Studien bei der Interpretation literarischer Texte (Alm & Sproat 2005). Als finale Annotation für eine Replik wird die binäre Polarität gewählt, die die Mehrheit der Annotatoren und Annotatorinnen ausgewählt haben (Endresultat: 139 negativ, 61 positiv). Als Evaluationsmaße wurden Genauigkeit (accuracy), Recall, Precision und F-Werte (Gon√ßalves et al. 2013) herangezogen. Abb. 1 zeigt einen Ausschnitt aus den je fünf besten Kombinationen pro Lexikon, geordnet nach Genauigkeit. Nachfolgend erfolgt eine überblicksartige Zusammenstellung einiger zentraler Ergebnisse aus der Evaluation: Aufgrund der Tatsache, dass hier ein verhältnismäßig simpler SA-Ansatz gewählt wurde und bereits menschliche Annotatoren und Annotatorinnen Schwierigkeiten mit der Polaritätsbestimmung haben, sind die Ergebnisse insgesamt durchaus positiv zu bewerten. Abschließend wurde auf Basis des besten SA-Ansatzes ein Web-Tool für die SA bei Dramen entwickelt. Dieses bietet interaktive Visualisierungen der Sentiment-Verteilungen und -Verläufe für alle berechneten Ebenen. Neben den SentiWS-Metriken wurden auch die Emotionskategorien des NRC integriert. Über das Tool kann man erste Fallstudien auf Dramen-, Akt-, Szenen-, Repliken-, Sprecher- und Sprecherbeziehungsebene durchführen. Die SA-Komponente ist online verfügbar. Trotz der historischen Differenz stimmen die Ergebnisse der automatischen SA tendenziell mit dem überein, was man in der Dramengeschichte über Bewertungen von Figuren und deren Verhalten weiß. Zusätzlich ist aber ein wichtiger heuristischer Mehrwert zu beobachten: eine Analyse allein auf der Basis von Sentiment-Zuschreibungen führt dazu, dass man das Augenmerk gezielt auf Fakten des Textes richtet, die bisher nicht berücksichtigt wurden. Im Folgenden einige Beispiele für die Bestätigung bekannter Ergebnisse und für Entscheidungen von Analysefragen: Die Analyse von Minna von Barnhelm zeigt, dass die negativen emotionalen Bewertungen insgesamt gegenüber den positiven deutlich überwiegen (vgl. Abb. 2). Dieser Befund bestätigt die bekannte Erkenntnis, dass Lessing das Schema des rührenden Lustspiels verwendet hat. Während die Komik im Stück eher das Ergebnis von Schlussprozessen ist, geht es auf der wörtlichen Ebene überwiegend um ernste Vorwürfe und drohenden Identitäts- und Beziehungsverlust. Es ist verschiedentlich behauptet worden (Saße 1993), Minna und nicht Tellheim sei die lächerliche Figur des Stücks. Die Sympathielenkung auf der wörtlichen Ebene des Textes, die in der unten stehenden Sentimentverteilung pro Akt abgebildet ist, kann dazu herangezogen werden, diese Frage negativ zu bescheiden (vgl. Abb. 3). Es ist eine auffällige Abweichung der Polarität im zweiten Akt erkennbar. In diesem Akt tritt Minna von Barnhelm zum ersten Mal auf, Tellheim jedoch nicht. Die letzte Visualisierung kann genutzt werden die Frage zu diskutieren, warum Emilia in Lessings Drama ""Emilia Galotti"" sterben muss (vgl. Abb. 4). Auffällig ist hier die starke negative Bewertung Emilias im zweiten Akt. Entgegen bisheriger Interpretationen, in denen nur die Intrige des Prinzen und Marinelli dafür verantwortlich gemacht werden, dass Emilia um ihre Tugend fürchten und ihren Vater dazu bringen muss, sie umzubringen, wird dadurch die Abwertung allein durch die Avancen des Prinzen sichtbar, die später sowohl Emilias als auch für Odoardos Einschätzung der Ehrbarkeit Emilias in ihrem zukünftigen Leben bestimmen. Insgesamt sind die ersten Analyse-Ergebnisse über das Web-Tool sehr vielversprechend. Dabei ist zu bedenken, dass über die Verwendung von SA-Lexika ein sehr einfacher SA-Ansatz gewählt wurde. Über ML- oder Hybrid-Ansätze können Besonderheiten der poetischen und veralteten Sprache möglicherweise besser beachtet werden. Ferner ist fraglich, ob eine Reduktion auf das sonst in der SA übliche binäre System positiv/negativ ausreichend ist für komplexe Interpretationen von Emotionen in Dramen. Durch Optimierung des SA-Verfahrens, Ausbau der Funktionen im Front-End und Erweiterung des Tools mit zusätzlichen Dramen sollen künftig Möglichkeiten und Nutzen der SA in der Dramenanalyse weiter exploriert werden."
2018,DHd2018,DUNST_Alexander_Hin_zu_einer_Visuellen_Stilometrie__Automati.xml,Hin zu einer Visuellen Stilometrie: Automatische Genre- und Autorunterscheidung in graphischen Narrativen,"Alexander Dunst (Universität Paderborn, Deutschland); Rita Hartel (Universität Paderborn, Deutschland)","Bildanalyse, Stilometrie, Corpus, multimodale Medien, Comics","Bilderfassung, Programmierung, Stilistische Analyse, Bilder, Multimodale Kommunikation","Stilometrische Untersuchungen haben eine lange Tradition in der Literaturwissenschaft und erleben mit der Digitalisierung von Textkorpora und Analysemethoden in den letzten drei Jahrzehnten einen erneuten Aufschwung (Holmes und Calle-Mart√≠n & Miranda-Garc√≠a). Öhnliche Tendenzen sind in den vergangenen Jahren auch in der Kunstgeschichte zu verzeichnen (Kohle; Qu, Taeb & Hughes; Manovich). Im Gegensatz dazu sind stilometrische Untersuchen visueller Erzählungen noch nicht etabliert. In den Bereich visueller, oder multimodaler, Narrative fallen etwa Comics, Filme und Fernsehserien, aber auch zu einem gewissen Grad Computerspiele, und damit viele der populärsten Erzählformen des 20. und 21. Jahrhunderts. Das Fehlen einschlägiger Forschung gründet einerseits auf die technischen Herausforderungen digitaler Bildanalyse, andererseits auf den Fokus auf qualitative und ideologiekritische Zugänge in großen Teilen der Medien- und Kulturwissenschaft. Auf Basis eines Corpus von 138 graphischen Romanen (Comicbüchern in Romanlänge) stellt unser Vortrag eine Methode zur automatischen Genre- und Autorunterscheidung vor. Weiters erörtern wir die Herausforderungen stilometrischer Methoden für Erzählformen, die Text und Bild kombinieren, und diskutieren abschließend die potenzielle Übertragbarkeit des vorgestellten Zugangs auf andere Medien. Die Basis der Untersuchungen stellt das erste repräsentative Corpus graphischer Romane dar, dass sich derzeit im Aufbau befindet (Dunst, Hartel & Laubrock). Zum Zeitpunkt der Untersuchungen im Mai 2017 waren 160 Volltexte digitalisiert; davon wurden aufgrund mangelnder Scanqualität bei einigen Titeln 138 Bücher mit einer Gesamtmenge von rund 33.000 Seiten für die Corpusstudie herangezogen. Der Fokus auf Bildanalyse ergab sich dabei sowohl aus methodischen als auch aus praktischen Überlegungen: einerseits sind Methoden zur stilometrischen Textanalyse bereits etabliert und werden laufend weiterentwickelt. Viele davon lassen sich direkt auf Comicstext anwenden oder dafür adaptieren. Andererseits bereitet eine zuverlässige, automatisierte Textlokalisierung und -Erkennung in Comics weiterhin Probleme. Damit bleibt derzeit nur der Weg über eine semi-automatische und zeitintensive Textannotation. Für größere Korpora bietet sich diese Methode daher nicht an. Drei visuelle Maße stellten die Basis unserer Berechnungen dar. Für all diese Maße wurde das Bild zunächst mithilfe einer linearen Näherung der Luminanz in eine Graustufen-Darstellung des Bildes umgewandelt. Im Anschluss an die Kalkulation dieser Grundmaße berechneten wir aus den Ergebnissen pro Seite die Mediane für jedes einzelne der 138 Werke. Um stilistische Abweichungen innerhalb eines Werkes zu messen, berechneten wir die Standardabweichung von den drei Maßen. Um die stilistische Entwicklung innerhalb eines Werkes zu berechnen, wurden in einem ersten Schritt manuell Inhalts- von Funktionsseiten getrennt und letztere von der Kalkulation ausgeschlossen. Hierzu betrachten wir die Kurvenverläufe der drei visuellen Maße über alle Seiten eines graphischen Romans. Für jedes visuelle Maß ermittelten wir: die Anzahl Extrema je 100 Seiten, die Standardabweichung aller Messwerte, die Standardabweichung innerhalb der lokalen Minima bzw. Maxima, die Regelmäßigkeit einer Kurve (also die Standardabweichung des Abstands in Seiten zweier aufeinanderfolgender Extrema) und eine Klassifikation des Kurven-Beginns und -Endes (die erste/letzte Seite hat einen größeren/kleineren Messwert als die nächste/vorherige Seite). Letztere Berechnungen für die stilistische Entwicklung ergaben sieben Werte je Maß, also insgesamt 21 Ergebnisse pro graphischem Roman. Diese reduzierten wir mit Hilfe von skalierter Hauptkomponentenanalyse (Principal Component Analysis [PCA]). Die errechneten Maße zogen wir im Anschluss für die Analyse folgender literatur-, bzw. kulturwissenschaftlicher Konzepte heran: Genre, Autorschaft, und Publikationsformat. Aus Zeitgründen stellen wir hier nur Teilergebnisse für die ersten beiden Kategorien vor. Unser Korpus enthält sowohl fiktionale als auch nicht-fiktionale Texte, die trotz dieser Unterschiede oftmals als graphische Romane bezeichnet werden. Tatsächlich erscheint es sinnvoller hier von graphischen Narrativen zu sprechen. Dabei handelts es sich um durchgehende Erzählungen im Medium Comics, die sich an ein erwachsenes Publikum wenden und einen Umfang von mehr als 64 Seiten haben, womit sie sich vom traditionellen Seitenformat von Comicbüchern abheben. Einzelnen Büchern wurde auf Basis von Klappentexten, Verlagsinformaionen und Zusammenfassungen eines der folgenden Genres zugewiesen: Graphic Novel, Graphic Memoir, andere nicht-fiktionale Texte (Sachbücher). Texte, denen die Subgenres Superhelden, Science Fiction, Märchen, Fantasy, Mystery und Horror zugewiesen wurden, fassten wir in der Sammelkategorie Graphic Fantasy zusammen. Eine kleine Zahl an Texten, die unter keine dieser Klassifikationen fielen, fassten wir unter dem Begriff ""miscellaneous"". Die Ergebnisse der Untersuchen wurden mit Hilfe von Welch Two Sample T-Tests mit p<0,05 auf ihre statistische Signifikanz untersucht. Dabei zeigte sich, dass sich die Genres Graphic Novel, Graphic Memoir und Graphic Fantasy signifikant voneinander unterscheiden. Im speziellen zeichnen sich graphische Romane und Memoiren durch einen deutlich regelmäßigeren visuellen Stil aus. Titel mit der niedrigsten Anzahl an Flächen und niedriger Standardabweichung von der durchschnittlichen Helligkeit gehören meist zu diesen beiden Genres. Graphische Memoiren sind außerdem deutlich heller als die Erzählungen, die wir unter Graphic Fantasy zusammengefasst hatten. Illustration 1 zeigt, dass sich auch eine historische Entwicklung nachweisen lässt. So werden graphische Memoiren seit Mitte des letzten Jahrzehnts deutlich heller 'eine Entwicklung, die möglicherweise durch die beispielgebende Wirkung von mittlerweile kanonischen Titeln wie Alison Bechdels Öhnlich wie im Fall der Genreunterscheidungen zeigten sich signifikante visuelle Unterschiede zwischen unterschiedlichen Autoren. Illustration 2 visualisiert die stilistische Handschrift von Künstlern, die mir mehr als drei Titeln in unserem Corpus vertreten sind. Jene, die einen von Werk zu Werk vergleichsweise konstanten Stil bevorzugen, nehmen dabei eine kleinere Fläche in der Matrix ein. Darunter fallen etwa die bekannten Autoren Jason Lutes, Chester Brown und der japanische Manga-Autor Ozamu Tezuka. Hingegen nehmen Frank Miller, Dave McKean und David Mazzuchelli aufgrund ihrer visuellen Bandbreite eine größere Fläche im stilistischen Raum (Manovichs ""style space"") graphischer Erzählungen ein. Die Einbeziehung von Tezuka zeigt als Testfall aber auch die momentanen Grenzen dieser Methode auf. Die verwendeten Maße erlauben keine Differenzierung zwischen dem angloamerikanischen graphischen Roman und dem japanischen Manga, zwei Erzählformen, die trotz gegenseitiger Beeinflussung deutliche stilistische Unterschiede aufweisen. In diesem Vortrag haben wir automatische Bildanalysen vorgestellt, die stilometrische Unterscheidungen zwischen Genres und Autoren auf den Bereich visueller Erzählungen übertragen. Dazu ist anzumerken, dass sich dieser Zugang in einem experimentellen Stadium befindet. So werden wir demnächst zusätzliche visuelle Maße zu den hier verwendeten erproben. In einem ersten Schritt ist dabei an Farbigkeitsmaße gedacht. Die Datenbasis wird mit Fortschreiten der Retrodigitalisierung unseres Corpus auf rund 250 Werke erweitert. Bei den Genreunterscheidungen hat sich die relativ aufwändige Berechnung der internen Entwicklung eines Werkes als wenig aussagekräftig herausgestellt. Allerdings fließt diese bei den Autorenunterscheidungen in die PCA ein. Auch die Genrekategorien bedürfen einer weiteren Verfeinerung: bei Versuchen, die interne Kohärenz der Genres zu berechnen (Distanzmaße vom Zentroiden eines Clusters) hat sich die Sammelkategorie Graphic Fantasy als weniger kohärent als eine zufällige Vergleichsgruppe herausgestellt. Hier sollten bei neuerlichen Berechnungen Subgenres wie Superheldennarrative herangezogen werden. Als graphische Kunstwerke, die von Hand gezeichnet werden, unterscheiden sich Comics deutlich von Filmen, Fernsehserien, aber auch von Computerspielen. Dennoch birgt die hier präsentierte Methode unserer Meinung nach Potenzial für diese Medien, da Fragen zur Stilistik von Genre und Autorschaft auch dort eine Rolle spielen. So könnte etwa erforscht werden, ob sich das Genre des Film Noir tatächlich stilistische Kohärenz aufweist, oder ob sich komplexe Qualitätsserien wie"
2018,DHd2018,KLINGER_Roman_A_Reporting_Tool_for_Relational_Visualization_.xml,A Reporting Tool for Relational Visualization and Analysis of Character Mentions in Literature,"Florian Barth (Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland); Evgeny Kim (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Sandra Murr (Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland); Roman Klinger (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland)","reporting tool, network analysis, visualization, text mining","Inhaltsanalyse, Beziehungsanalyse, Netzwerkanalyse, Visualisierung, Literatur, Text","The emergence of computational methods of text processing has created new paradigms of research in literary studies in recent years (Jockers & Underwood, 2016), for instance Existing tools of text analysis and network visualization such as Voyant We present our ongoing effort on closing this gap by developing a literary analysis reporting tool As a use-case study, we apply Previous research on social networks in literary fiction generally fall into one of the two categories: (1) works that explore methods for extracting and formalizing character networks ( Building on graph theory extensively elaborated in the past fifty years (e.g., Bondy and Murty, 1976 or West, 2001), our work is similar to Beveridge & Shan (2016), in particular, in terms of the weighted degree measure, and to Park et al. (2012), in terms of distance measure for detecting closely related characters in a text. In the following, we explain the different components in To detect character mentions in the text we use a fundamental named-entity recognition approach based on dictionaries. This approach is suitable for scholars who analyze texts they already know. Consequently, we opt for a transparent and simple character recognition procedure: The user provides a list of character names to be included in the analysis specifying a canonical name form and all variations thereof she would like to take into account ( We define the closeness of relationship between two characters using a We visualize the network of characters with an undirected graph Word clouds are an approach to visualize the vocabulary of a text. The size of one word corresponds to its frequency. We use two different kinds of word clouds: For each character in the character list, we show word clouds based on the context of a window size We plot the timeline of multiple predefined world fields (specified by word lists) in the text. This feature is helpful in representing how certain fields ( The tool was developed using Python v.3.6 and the Flask For a use-case analysis, we apply In Goethe's epistolary novel, the protagonist Werther describes his unhappy love for Lotte, who is engaged to Albert. The characteristic triangular relationship in the novel arises from this constellation (protagonist - beloved woman - antagonist). With The protagonist Werther shows a degree of 21, which is the number of characters with whom he interacts. The closest relationship measured by edge weight (Figure 2) is observed between Werther and Lotte (81 interactions). The antagonist Albert has a low degree of 3. However, his weighted degree is 36 (third highest after Werther and Lotte), which confirms his important role in the triangular relationship.  Highlighted in red is the typical triangular relationship in Goethe""s novel, which corresponds to the three highest weighted degrees. In further steps, we will use To better characterize the edges, the tool outputs top-  The word clouds enable first conclusions about the relationships of the characters. Werther and Lotte's word cloud characterizes their ambivalent relationship. The key words ""Leidenschaft"" and ""Freude"" reflect Werther's love, whereas the mentions of ""sterben"" and ""Verblendung"" are characteristic of the unrequited love, which leads Werther into his ""disease unto death"". As Werther and Albert""s word cloud reveals, their relationship is dominated by the ""Unruhe"" that Werther feels through his adversary. Additionally, the tool plots the development of the narrative (not bound to specific characters) based on the word fields, an example of which is shown on Figure 6. In this case we used words from the emotion domain (with emotion dictionaries by Klinger et al. (2016)).  The word field development can highlight the prevalence of individual emotion domains across the text. The accumulation of the negative emotion words (Wut,Trauer, Furcht) towards the end suggests, for example, that Goethe""s novel has no ""happy ending"". The striking rash on ""Freude"", however, captures the last happy hours Werther spends with Lotte in the second part of the narration before he kills himself. The next version of the tool will include a character-oriented word field development calculated and plotted for the main characters of the stories. In addition, future releases will include more analysis features and bulk file processing."
2018,DHd2018,KRUG_Markus_Annotation_and_beyond___Using_ATHEN.xml,Annotation and beyond 'Using ATHEN   Annotation and Text Highlighting Environment,"Markus Krug (Universität Würzburg, Deutschland); Ngoc Duyen Tanja Tu (Institut für Deutsche Sprache, Deutschland); Lukas Weimer (Universität Würzburg, Deutschland); Isabella Reger (Universität Würzburg, Deutschland); Leonard Konle (Universität Würzburg, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland); Frank Puppe (Universität Würzburg, Deutschland)","Annotation, Query, NLP, Preprocessing, Extensibility","Programmierung, Annotieren, Bearbeitung, Visualisierung, Metadaten, Software","The workshop presents ATHEN We compare ATHEN to three web-based and four desktop applications in 12 categories by adapting most criteria defined by Neves and Leser (Neves / Leser 2012) to compare different annotation tools: We explicitly do not want to compare subjective features like usability or how the annotations are presented. All of the listed tools have an accessible documentation, either web-based or as a PDF, available for download. Besides UAM (O'Donnell 2008), every other application is listed as open source, so at least extensions based on code level can be made. WebAnno (Yimam et al. 2013) is the only application having a tutorial supporting a new developer to make changes in their project. ATHEN stands out in the sense that extensions to its UI can be made at runtime, therefore easing the process of adding functionality to it. WebAnno supports the largest number of formats and comes with a machine learning based automatic annotation, however lacks integrated NLP-preprocessing. CATMA (Meister 2017) is the only project that has a very good visualization component and also supports TEI-XML (Wittern et al 2009), the unspoken standard of text processing. Being a standalone web application, CATMA itself does not support NLP-preprocessing. ATHEN comes with the support of the execution of UIMA analysis engines, accessible from web repositories or a local repository, giving the user a chance to integrate her custom-made annotators. Four tools, ATHEN, UAM, MMAX2 (Müller / Strube 2006) and CATMA feature an integrated query language which helps to analyze existing corpora. Most tools allow the annotation of user-defined annotation schemas earning therefore the title ""generic annotation tool"". Alongside UAM, ATHEN supports the annotation based on queries, while UAM defines its own language, ATHEN supports the annotation using Apache UIMA Ruta (Kluegl et al. 2016) rules. Three of the listed tools, MMAX2, Knowtator (Ogren 2006) and WordFreak (Morton / LaCivita 2003) are currently no longer in active development. ATHEN is a Java-based desktop application with the vision to be extensible. Therefore, it makes use of the flexible plugin architecture of the eclipse The program is split into four sections: The first section is a presentation which shows the main differences between the existing annotation tools. The second section defines an ordinary annotation scenario and it is used to introduce the participants to the general-purpose annotation view of ATHEN. Afterwards, for tasks to which ATHEN has special support (annotating character references and their coreferences, annotating direct speech and their speaker) an introduction to the special purpose views of ATHEN is given. The third panel introduces the participants to the functionality of ATHEN beyond regular text annotation. It starts with the definition of an OWL ontology (and its utilization for texts). This is centered on relation detection of character references, as well as an attribution of those references. To speed up manual annotation it is helpful to have it preprocessed with existing tools. The task definition is then changed from pure annotation to an application with a consecutive correction of the output of the automatic engines. In this context, Nappi, a submodule of ATHEN is presented and it is shown how to define, execute and integrate custom analysis engines. The next part is dedicated towards extracting knowledge from annotated data, for this purpose, an Apache Lucene Index is created using ATHEN and is queried in a live fashion. This feature allows rapid insight into an existing corpus and enables the user to answer their own hypothesis. The tutorial continues with the presentation of how images can be annotated with polygon-based annotations to show, that ATHEN is not only limited to textual resources. The last part is directed towards Java developers who are interested in developing their own annotation component. Each section starts with a set of slides which introduce the features in focus and presents the participants with one or more tasks that can be fulfilled by using ATHEN. The participants need their own laptops with an active internet connection. The number of participants is limited to 15 to 20. The last section requires knowledge of Java. Data which is necessary for the tutorials will be hosted on our own server and will be made accessible for download. ATHEN is mainly developed in the context of the project Kallimachos at the University of Wuerzburg. Its main purpose was to support the annotation process of DROC ( An extension to ATHEN was made in the project ""Redewiedergabe"" to manually annotate different forms of speech, thought and writing representation (STWR). These annotations will then be used to train an automatic recognizer for STWR."
2018,DHd2018,TRILCKE_Peer_Dramenquartett___Eine_didaktische_Intervention.xml,Dramenquartett 'Eine didaktische Intervention,"Frank Fischer (Higher School of Economics, Moskau); Christopher Kittel (Karl-Franzens-Universität Graz, Open Knowledge Forum Österreich); Carsten Milling (Berlin); Peer Trilcke (Universität Potsdam, Deutschland); Jana Wolf (Mittelbayerische Zeitung Regensburg)","Literatur, Netzwerkanalyse, Gamification","Modellierung, Kommunikation, Netzwerkanalyse, Einführung, Lehre, Text","Ziel dieses Posters ist es, anhand von 32 deutschsprachigen Dramen in die Netzwerkanalyse literarischer Texte einzuführen, eine didaktische Intervention für eine zwar mittlerweile etablierte Methode der literaturwissenschaftlichen Analyse, die aber nicht immer genügend reflektiert wird: Der Errechnung teils komplexer netzwerktheoretischer Maße entspricht nicht immer ein entsprechender Sprung zur Bedeutungsebene. Was bedeutet es zum Beispiel wirklich, dass die durchschnittliche Pfadlänge in Goethes ""Faust. Der Tragödie erster Theil"" genau 1,79 beträgt? Wenn man jedoch diesen Wert in Beziehung zu entsprechenden Werten anderer Stücke setzt, gewinnt er an komparatistischer Bedeutung. Die Anschaulichkeit der Wert und ihre spielerisch erfahrene Dimensionierung ermöglichen so die Einübung in die strukturalistische Betrachtung von Netzwerken am Beispiel von Dramen, wobei damit zugleich kulturelles Grundwissen über die Strukturation von Netzwerken 'immerhin ubiquitäre Gegenstände der sozialen und technischen Welt 'erworben werden kann. Um den komparatischen Blick im Kontext der literaturwissenschaftlichen Netzwerkanalyse zu schulen, setzen wir mit unserem Poster auf einen Gamification-Ansatz. Anders als bei unserem ersten Experiment in dieser Richtung 'der auf der DHd2016 präsentierten Android-App ""Play(s)"" (vgl. Göbel/Meiners 2016), in deren Mittelpunkt die spielerische Korrektur und Anreicherung unserer Korpusdaten stand –, handelt es sich diesmal um eine nicht-technische Anwendung, die auf spielerische Weise netzwerkanalytisches Datenmaterial explorierbar macht. Dabei wird das Posterformat in zweierlei Hinsicht bespielt: Das Poster ist einerseits eine Datenvisualisierung auf Grundlage eines selbst gepflegten größeren Dramenkorpus. Andererseits ist es ein in 32 Teile zerlegbares Dramenquartett, das spielerisch mit den Bedeutungshorizonten verschiedener netzwerktheoretischer Größen bekannt macht und ein Bewusstsein für komparatistische Möglichkeiten trainiert. Dieser Ansatz ist in den Geisteswissenschaften nicht neu, verwiesen sei etwa auf das architekturgeschichtliche Quartettspiel ""Plattenbauten. Berliner Betonerzeugnisse"" (Mangold u.¬†a. 2001), in dem technische Daten verschiedener Plattenbautypen gegenübergestellt werden (vgl. auch Richter 2006). Die Didaxe des Dramenquartetts bezieht sich auf mehrere Dimensionen: eine literaturgeschichtliche, eine quantitative, eine netzwerktheoretische. Die 32 Stücke bilden einen Minimalkanon, der von der Zeit der Gottschedischen Theaterreformen bis in die Moderne reicht. Statt der lexikonartigen Beschreibung eines solchen Kanons (wie etwa im ""Dramenlexikon des 18.¬†Jahrhunderts"", Hollmer/Meier 2001), besteht das Beschreibungsinstrument hier in visuellen und quantitativen Werten, die Vergleichbarkeit herstellen 'erst dieser Umstand vereint die verschiedenen Karten zu einem kompetitiven Spiel. Als visueller Catch der Quartettkarten dient eine Visualisierung des jeweiligen extrahierten sozialen Netzwerks (vgl. Fischer u.¬†a. 2016). Die weiteren Informationen auf den Karten setzen sich aus (Kanonwissen präsentierenden) Metadaten (Autor*in 'Titel 'Untertitel 'Genre 'Jahr) und vor allem aus statischen und dynamischen Netzwerkdaten zusammen (Anzahl von Subgraphen 'Netzwerkgröße 'Netzwerkdichte 'Clustering-Koeffizient 'Durchschnittliche Pfadlänge 'Höchster Degreewert und Name der entsprechenden Figur –), wie sie im Rahmen des dlina-Projekts berechnet wurden. Das Poster wird mit unserer Python-Skriptsammlung ""dramavis"" generiert, die in der neuen Version 0.4 eine entsprechende Funktion erhalten hat (Kittel/Fischer 2017). Für das Konferenzposter haben wir einen Fallback-Kanon zusammengestellt (Stücke von Johann Christoph und Luise Adelgunde Victorie Gottsched, von Gellert, J.¬†E. Schlegel, Caroline Neuber, Klopstock, Lessing, Gerstenberg, Goethe, Lenz, Klinger, Schiller, Kotzebue, Kleist, Zacharias Werner, Müllner, Grillparzer, Grabbe, Büchner, Hebbel, Gustav Freytag, Anzengruber, Arno Holz, Wedekind, Schnitzler, Erich Mühsam). Über eine individualisierbare Kanon-Datei können aber auch eigene Quartette zusammengestellt werden, sodass sich etwa auch epochenspezifische Sets (Dramen der Aufklärung, Dramen der Klassik, Romantische vs. Klassische Dramen, Dramen des Sturm und Drang vs. Dramen des Naturalismus) oder gattungsspezifische Sets erstellen lassen. Auf der Konferenz werden wir neben einem Poster, das das didaktisch-interventionistische Konzept veranschaulicht, auch diverse Quartett-Sets präsentieren."
2018,DHd2018,LAUER_Gerhard_Kulturelle_Evolution__Zur_Kritik_der_literatur.xml,Kulturelle Evolution. Zur Kritik der literaturhistorischen Methode,"Gerhard Lauer (Universität Basel, Schweiz)","Kulturelle Evolution, Literaturgeschichte, Phylogenetik","Beziehungsanalyse, Räumliche Analyse, Modellierung, Theoretisierung, Netzwerkanalyse","Zu den Provokationen der Literaturwissenschaft durch die Digital Humanities gehört ihre Arbeit an großen Datenmengen. Nicht das besondere Buch, der Kanon oder der Großschriftsteller, sondern die vielen Bücher und Literaturen sind der ""andere"" Gegenstand der computergestützten Literaturwissenschaft. Geradezu typisiert werden Digital Humanities und Distant Reading zusammen genannt. Tatsächlich war (Moretti, 2000) als Kritik an der herkömmlichen Literaturwissenschaft angelegt, genauer an ihrer methodischen Beschränkung, die Vielfalt der Literaturen methodisch in den Griff zu bekommen. Die Kritiker Morettis haben sein Anliegen konzediert, ohne konkrete Vorschläge zu machen, wie mit dem ""Mengenproblem"" in der Literaturgeschichtsschreibung besser umgegangen werden könnte (Ross, 2014). Ein radikaler Ansatz, die Geschichte der Literatur anders als bisher zu modellieren, ist der Ansatz der kulturellen Evolution (Lewens, 2013). Meine These lautet: Evolutionäre Modelle und Theorien sind für die Beantwortung literaturhistorische Fragestellung geeignet, besonders um das ""Mengenproblem"" in den Griff zu bekommen. Im Folgenden skizziere ich Theorie und Methodologie eines solchen Ansatzes und frage nach den Folgen für ein Fach wie die Literaturgeschichte. Wie der Name schon andeutet, verschiebt der Ansatz den Akzent von der Geschichte auf die Evolution. In den Blick rückt nicht weniger als die Menschheitsgeschichte. Der Ansatz kommt denn auch aus der biologischen Anthropologie, nicht aus den geisteswissenschaftlichen Fächern. Das leitende Paradigma ist Darwins Theorie der Evolution. Die Theorie der kulturellen Evolution überträgt dieses Modell auf die Kulturgeschichte der Menschheit. Sie geht von der These aus, dass die Entwicklung der menschlichen Kulturen der gleichen evolutionären Entwicklungslogik folgt, der auch die Natur unterliegt (Mesoudi, 2016). Dabei geht es nicht um eine Analogie, vielmehr lautet die Dual-Heritance-These dieser Theorie, dass die Kultur die Natur des Menschen ist, kulturelle Entwicklungen die Natur des Menschen bis in seine genetische und biologische Veranlagung beeinflussen, wie umgekehrt die Naturgeschichte des Menschen seine Kultur bestimmt (Henrich & McElreath, 2007). So generell angelegt versteht sich kulturelle Evolution als eine Supertheorie, die verspricht, die Sozial- und Geisteswissenschaften mit den biologischen Wissenschaften zusammenzuführen (Mesoudi, Whiten & Laland, 2006; Laubichler & Renn, 2015). Aus der Sicht der Digital Humanities ist die Theorie der kulturellen Evolution daher am extremen Ende des Distant Reading angesiedelt. Statt das einzelne Buch versucht die Theorie die Kultur der Menschheit in den Blick der Untersuchung zu nehmen. Kulturelle Evolution ist aber neben dem ""Distant Reading"" auch noch aus einem zweiten Grund von Interesse für Digital Humanities. Sie arbeitet mit computergestützten Modellen. Bereits die Arbeiten aus den 70er und 80er Jahren haben computer-basierte Modelle genutzt, als (Cavalli-Sforza & Marc Feldman, 1981), dann auch (Boyd & Richerson, 1985) begonnen haben, die Evolutionsbiologie für das Verstehen von kulturellen Prozessen zu nutzen. Ihr Modell der kulturellen Evolution geht von Populationen aus, die ihrerseits aus Gruppen von Individuen bestehen, von denen jedes Individuum über variierende kulturelle Eigenschaften verfügt. Soziale Transmission von Informationen ist der wesentliche kulturelle ""Vererbungs""-Mechanismus zwischen Individuen und Populationen. Nur dann, wenn man annimmt, dass durch Prozesse des kulturellen Lernens Wissen vertikal, aber auch horizontal zwischen gleichzeitig lebenden Generationen weitergegeben wird, etwa das Lernen von Sprachen durch Kinder, kann man verstehen, warum sich die horizontale Weitergabe langfristig auch auf die vertikale, genetische Weitergabe von Eigenschaften auswirken kann und sich so etwas wie komplexe Sprachen entwickeln konnten. Hier kommen mathematische Ansätze und Computersimulationen ins Spiel, um die langfristigen Veränderungen mikroevolutionärer Veränderungen in Populationen, die Rate der Ausbreitung und räumlichen Verteilung neuer kultureller Eigenschaften zu modellieren. Quantitative Ansätze waren für Cavalli-Sforza, Feldman, Boyd und Richerson trotz der damals noch bescheidenen Speicherraten notwendig geworden, weil die Prozesses, die Veränderungen in der kulturellen Variationen verursachen, so vielfältig sind, dass sie mit herkömmlichen Methoden nicht mehr gehandhabt werden konnten. Die Linguistik hat die Theorie der kulturellen Evolution rasch adaptiert, um die Evolution der Sprache untersuchen zu können, wie etwa die Befunde zur Diversität von Sprachen, nämlich dass größere Populationen ein größeres Inventar an Wörtern besitzen, ihre Sprachen stärker grammatikalisiert sind, mehr Phoneme haben, aber ihre Morphologie zugleich einfacher ist und ihre Wörter kürzer sind (Atkinson, 2011; Nettle, 2012). Auch kann ein solcher Ansatz zeigen, wie sich wärmere Klimata auf das Klangspektrum der in einer Sprache genutzten Laute auswirken (Munroe, Fought & Macaulay, 2009) oder wie zerklüftete Landschaften bestimmte Distanzsprachen wie etwa Zeichen- oder Pfeifsprachen präferieren (Meyer, 2015). Immer geht es dabei um Einsichten in die Struktur der Kultur und die Prozesse ihrer Veränderung der langen Dauer, die nicht der Granularität etablierter historischer Beschreibungen entsprechen. In der Literaturgeschichte sind kulturevolutionäre Modelle nicht etabliert. Das hat zunächst damit zu tun, dass generell der Aufbau von Korpora und die formale Modellierung von Fragestellungen innerhalb des historisch-hermeneutischen Paradigmas keine Rolle spielen und kaum eine Tradition haben. Linguisten dagegen wie (Labov, 1963) haben variationstheoretische und funktionalistischen Methoden genutzt, um die Systematizität in der sozialen und individuellen Variation des Sprachgebrauchs zu verstehen. Neuere Arbeiten in der Linguistik fragen danach, ob selbst solche fundamentalen Unterscheidungen wie die Unterscheidung der Wortklassen Nomen und Verben nicht in allen Sprachen zu finden sein könnte, die Wortordnung viel variabler als bislang angenommen sein dürfte, sprachliche Register höchst unterschiedlich gebraucht werden, Sprachen in einer höchst unterschiedlichen Interaktion zwischen Kindern und Eltern erworben werden (Evans, 2013; Lieven, 2013). Wenn aber Sprache in ihrer Entwicklungsgeschichte selbst in ihren Grundkategorien diverser sein könnten, als lange angenommen, und die Evolutionsrate ihrerseits je nach Sprache und Umwelt stark zu variieren scheint (Gray et al., 2013), müsste nicht Öhnliches auch für die Literaturen der Welt und ihre Entwicklung gelten, so dass man annehmen könnte, dass die Rate der Evolution von literarischen Formen mit Faktoren wie Gruppengröße, Dichte des sozialen Netzwerks, Menge der geteilten Informationen, soziale Stabilität und das Niveau des Austausches mit anderen Gruppen korreliert (Trudgill, 2011)? Das wäre eine innovative Forschungsagenda. Die wenigen literaturhistorischen Arbeiten, die bislang vorliegen, nutzen verschiedene statistische Modelle und Methoden, um eine kulturevolutionäre Literaturgeschichte zu untersuchen. Besonders naheliegend ist der Ansatz einer Weiterentwicklung von Stemma zur Rekonstruktion von Manuskript-Kulturen. 1998 druckte die Zeitschrift Ein weiterer kulturevolutionärer Ansatz nutzt die Klassifikation von Motiven, wie sie in der historisch-geographischen Schule der Märchenforschung mit dem Aarne-Uther-Thompson-Index vorliegt. Mit Methoden der Kladistik (Most Parsimonious Trees), Bayesian und phylogentischen Netzwerkanalysen konnten (2013; Tehrani & d""Huy, 2106) die weltweite Verwandtschaft des Rotkäppchen-Märchens bestimmen. Von Interesse sind dabei auch Zusammenhänge mit der Populationsstruktur (Ross, Greenhill & Atkinson, 2013), wenn dabei gezeigt werden kann, wie geographische Distanz, Genetik und Variation der Märchenmotive zusammenhängen (Bortolini et al., 2017). Die Annahme dabei ist, dass genetische, linguistische und motivliche Distanz korrelieren. Statt über textuelle Merkmale wie Abschriftenfehler oder Motive zu gehen nutzen andere Ansätze innerhalb dieses Paradigmas Spielexperimente, wie sie besonders in der Verhaltensökonomie gängig sind. So wählen Probanden aus mehr als 60 Geschichten diejenigen aus, die ihnen besonders erzählenswert erscheinen und schreiben eine der Geschichten in kurzen Abschnitten (120-160 Buchstaben) weiter, bevor anderen Probanden die Geschichte weiterschreiben. Jeder Proband kennt jeweils nur die ""Eltern"" der Geschichte, die sie gerade fortschreiben (Cuskley et al., 2016). Gemessen werden qualitativ und quantitativ narrative Innovationen, um so experimentell die Ausbreitung von Geschichten zu messen. Ein weiterer Ansatz nutzt das Konzept der ""Minimally Counterintuitive Narratives"", demzufolge Geschichten dann eher geteilt und weitergegeben werden, wenn sie eine realistische Ontologie leicht durchbrechen, wie das etwa bei Märchen der Fall ist (Porubanova-Norquist, Shaw & Xygalatas, 2013). Auch hier werden phylogenetische Methoden verwendet, um zu berechnen, welche Texteigenschaften in welcher Umwelt evolutionär vorteilhaft sind und daher eher geteilt und weitergegeben werden (Stubbersfield & Tehrani, 2013). Es liegt auf der Hand, dass noch sehr viele andere Dimensionen von (literarischen) Texten eine Rolle im Prozess der Evolution spielen dürften. Systematisch gewendet heißt das: Eine kulturevolutionäre Literaturgeschichtsschreibung hat ein anderes Gegenstandsfeld, nicht den Viktorianischen Roman, sondern die evolutionäre Logik seiner Entwicklung und die Schreibmuster dieses Genres. Literatur ist kein Werk, sondern Teil von Populationen. Sie benutzt zweitens andere Methodensets als die herkömmliche Literaturgeschichte, aber teilweise auch andere als sie sonst in den Digital Humanities gängig sind. Drittens leistet der Ansatz anderes. Er ist auf Erkenntnisse zur Logik der langen Dauer ausgerichtet. Und viertens rückt der Ansatz die Literaturgeschichte nahe an die Biologie heran mit Folgen für die Theoriebildung und Methodenentwicklung. Die Theorie der kulturellen Evolution ist nicht weniger als ein Ansatz, Digital Humanities als Teil eines größeren Forschungsprogramms zu betreiben. Mein Vortrag ist ein Plädoyer für ein solches Forschungsprogramm einer Literaturgeschichte der langen Dauer."
2018,DHd2018,BIGALKE_Ben_Personen__und_Figurennetzwerke_in_Fernando_Pesso.xml,Personenund Figurennetzwerke in Fernando Pessoas Publikationsplänen,"Ben Bigalke (Universität zu Köln, Deutschland); Sviatoslav Drach (Universität zu Köln, Deutschland); Ulrike Henny-Krahmer (Universität Würzburg, Deutschland); Pedro Sep‚àöé¬¨‚à´lveda (Neue Universität Lissabon, Portugal); Christian Theisen (Universität zu Köln, Deutschland)","Fernando Pessoa, Netzwerk, Visualisierung, D3, TEI","Beziehungsanalyse, Netzwerkanalyse, Visualisierung, Manuskript, Personen","  Abstract zur Konferenz Digital Humanities im deutschsprachigen Raum 2018 Personenund Figurennetzwerke in Fernando Pessoas Publikationsplänen Bigalke, Ben bbigalke@smail.uni-koeln.de Universität zu Köln, Deutschland Drach, Sviatoslav sdrach@smail.uni-koeln.de Universität zu Köln, Deutschland ständige Bedeutungsverschiebung hin, die sowohl an fragmentarischen Schriften seines Nachlasses als auch an seinen Publikationsplänen zu erkennen ist (vgl. dazu Cunha 1987, Martins 2003, Gusm√£o 2003 und Sep√∫lveda 2013). Diese Bedeutungsverschiebung hängt stark mit dem Wahl der Autorennamen zusammen, die ebenfalls einem ständigen Wechsel unterlag und in den Plänen zur Edition und Publikation des Werkes eine hohe Bedeutung gewann. Zur Definition eines Publikationsvorhabens gehörte für Pessoa die Zuordnung einer bestimmten Autorfigur, deren fiktionale Persönlichkeit sowohl durch ein bestimmtes Werk konstruiert werden sollte als auch dieses Werk in seiner Besonderheit definieren würde. Die Spannung zwischen Planung und Publikation des Werkes wird daher noch dadurch verstärkt, dass Pessoa unter verschiedenen, insgesamt etwa 120 Autorennamen geschrieben hat oder geplant hat zu schreiben (vgl. dazu Pessoa 2012). Eine besonders wichtige Rolle in Pessoas Werk und seinen Werkplänen spielen dabei die Namen Alberto Caeiro, √Ålvaro de Campos und Ricardo Reis, die er (in Abgrenzung zu den weiteren Pseudonymen) Heteronyme genannt hat. Digitale Edition ""Projekte und Publikationen"" Henny-Krahmer, Ulrike ulrike.henny@uni-wuerzburg.de Universität Würzburg, Deutschland Sep√∫lveda, Pedro pmpsepulveda@gmail.com Neue Universität Lissabon, Portugal Theisen, Christian ctheise2@uni-koeln.de Universität zu Köln, Deutschland Im Nachlass des portugiesischen Dichters Fernando Pessoa (1888-1935) finden sich zahlreiche Listen geplanter Publikationen. Diesen stehen nur wenige zu Lebzeiten tatsächlich realisierte Veröffentlichungen gegenüber. Daraus ergibt sich ein Kontrast zwischen der Ebene des Möglichen und des Verwirklichten in der Literatur und Literaturproduktion. Vor diesem Hintergrund entsteht die digitale Edition ""Fernando Pessoa. Projekte und Publikationen"" und mit ihr die im Folgenden vorgestellte Netzwerkvisualisierung. Mit ihr wird ein Werkzeug zur Verfügung gestellt, das die Exploration des Personenund Figurenkosmos in Pessoas Publikationsplänen über die Zeit ermöglicht. Pessoas Werk zwischen Planung und Publikation Die Notizzettel, Seiten aus Notizbüchern und andere Papiere aus Pessoas Nachlass, auf denen er die Pläne für seine Werke handschriftlich oder mit Schreibmaschine geschrieben festgehalten hat, werden in einer Kooperation zwischen dem Institut für Literatur und Tradition (IELT) der Neuen Universität Lissabon und dem Cologne Center for eHumanities (CCeH) der Universität zu Köln digital ediert (Sep√∫lveda und Henny-Krahmer 2017). 1 Neben den Dokumenten aus dem Nachlass umfasst die digitale Edition auch die zu Lebzeiten von Pessoa publizierten Gedichte. Gegenstand des hier vorgestellten Netzwerkes sind jedoch ausschließlich die Dokumente, auf denen die Figuren und Personen genannt sind und deren Beziehungen ausgehend von ihrer gemeinsamen Erwähnung über die Zeit untersucht werden. Die Dokumente sind in der Edition in TEI codiert, wobei die Namensvorkommen erfasst werden und eine Identifikation der hinter den Namen stehenden Personen und Figuren in einem zentralen Index erfolgt. Transkriptionen und Index bilden zusammen mit den für jedes Dokument festgehaltenen Metadaten die Datengrundlage für das Figurenund Personennetzwerk, wobei insbesondere die für die Chronologie relevante Datierung zu nennen ist. Die Dynamik der Schriften Pessoas ist im Spannungsverhältnis zwischen dem projizierten Werk, das der Dichter im Sinne eines vollkommen Ganzen konzipiert hat, und dem tatsächlich Geschriebenen und nur in geringem Maße Publizierten zu verstehen. Pessoa war bei der Veröffentlichung seiner Werke extrem selektiv und die Dynamik seines Schreibens weist auf eine Abstract zur Konferenz Digital Humanities im deutschsprachigen Raum 2018 Netzwerkvisualisierung zu Personen und Figuren Die Vorkommen von Namen in Pessoas Publikationsplänen werden hier mit Hilfe einer interaktiven Netzwerkvisualisierung analysiert, um zu untersuchen, wie sich der von ihm in den Dokumenten entworfene Personenund Figurenkosmos über die Zeit entwickelt. Für dynamische Netzwerkvisualisierungen gibt es in den DH bereits verschiedene Ansätze (vgl. u. a. Rigal et al. 2016, Xanthos et al. 2016). Für das Pessoa-Netzwerk ergibt sich die Dynamik aus der Möglichkeit, das Gesamtnetzwerk der Personen und Figuren über alle Dokumente hinweg auf Dokumente aus bestimmten Zeiträumen oder Jahren einzugrenzen. Es ist als heuristisches Instrument gedacht, um Hypothesen zur Chronologie von Personenund Figurenkonstellationen zu generieren und im Ansatz überprüfen zu können. Für das Netzwerk, das unter http://www.pessoadigital.pt/ de/network verfügbar ist, sind 249 Dokumente ausgewertet worden, die insgesamt 369 Namen enthalten. Es werden sowohl historische Personen als auch fiktive Figuren aus Pessoas Werkwelt gezeigt. Analysiert wird das Vorkommen der Namen auf Pessoas Publikationsplänen von 1913 bis zu seinem Tod im Jahr 1935. Die frühen Dokumente (vor 1913) werden hier nicht berücksichtigt, da sie noch in Bearbeitung sind. Die Netzwerkdaten sind mit Hilfe von XSLT aus den TEI-Dokumenten generiert worden und liegen im JSON-Format vor. 2 Die interaktive Visualisierung ist mit der Bibliothek D3 erstellt worden, wobei ein Netzwerklayout von Mike Bostock adaptiert und um weitere Funktionalitäten ergänzt wurde. 3 Erweiterungen, die für die vorliegende Anwendung vorgenommen wurden, sind u. a. die Möglichkeit, Teile des Netzwerks einund auszublenden (nach Chronologie; Teilnetzwerke für die Verbindungen, die von einzelnen Personen ausgehen; nur Knoten mit oder auch Knoten ohne Verbindungen) sowie Optionen für die Darstellung (Einblenden von Labels; Dichte bzw. Weite der Anzeige des Netzwerks). Abbildung 1: Optionen für die Anzeige des Netzwerks Die Größe der Knoten im Netzwerk zeigt an, wie häufig einzelne Namen auf den Dokumenten erwähnt werden. Zur Ermittlung der Knotengröße wurde die Formel 2 + log2(size) angewandt, wobei size für die tatsächliche Häufigkeit des Vorkommens steht. Um zwischen fiktiven Figuren und historischen Personen unterscheiden zu können, sind die Knoten unterschiedlich eingefärbt (türkis = fiktiv; dunkelbau = historisch). Bei den Netzwerkkanten verdeutlicht die Dicke, wie häufig Namen gemeinsam auf Dokumenten vorkommen: je häufiger das gemeinsame Auftreten, umso dicker die Linien in der Visualisierung. Dabei ist die minimale Kantendicke 1 Pixel (bei einer gemeinsamen Erwähnung). Pro weiterer gemeinsamer Erwähnung nimmt die Kantendicke um 1 Pixel zu. Abstract zur Konferenz Digital Humanities im deutschsprachigen Raum 2018 Abbildung 2: Teilnetzwerk mit Fernando Pessoa als zentralem Knoten Zentrale Funktionalitäten in der Netzwerkanwendung sind Auswahloptionen, welche die Chronologie der Namenserwähnungen betreffen. So kann das Netzwerk neben einer Gesamtdarstellung auch für Vorkommen in einzelnen Jahren angezeigt werden. Darüber hinaus ist eine Anzeige nach Perioden möglich (z. B. 1919-1927). Da es in der Pessoa-Forschung konkurrierende Vorschläge für eine Periodisierung des Werkes gibt, werden zwei verschiedene Einteilungen in Perioden zur Auswahl angeboten. Auf diese Weise wird es möglich, die Entwicklung von Pessoas Personenund Figurenkosmos, wie er sich in den Publikationsplänen darstellt, kritisch zu untersuchen. Betrachtet man das Netzwerk chronologisch anhand ausgewählter Jahre und Perioden, innerhalb derer die Publikationspläne verfasst wurden, so sind Tendenzen zu erkennen, die historisch, editorisch und auch poetisch für das Werk von maßgeblicher Bedeutung sind. So kann man beispielsweise sehen, wie in den Jahren von 1913 bis 1919, und besonders zwischen 1914 und 1915, die Namen der Heteronyme zusammen mit dem von Fernando Pessoa am häufigsten vorkommen und vor allem untereinander verbunden sind (vgl. Abb. 4). Abbildung 4: Teilnetzwerk 1915 Abbildung 3: Gesamtnetzwerk mit Pessoa als häufigstem Namen Wenn man das Vorkommen aller Namen im gesamten Netzwerk betrachtet, so ist zu erkennen, dass Fernando Pessoa als Name am häufigsten vorkommt, direkt gefolgt von den Namen der Heteronyme Alberto Caeiro, Ricardo Reis und √Ålvaro de Campos, was etabliertes Wissen zu der Bedeutung der Heteronyme in Pessoas Werk bestätigt (vgl. Abb. 2 und 3). An zweiter Stelle stehen dann William Shakespeare, Jos√© Almada-Negreiros, Edgar Allan Poe, M√°rio de S√°-Carneiro und Ant√≥nio Mora. Dabei ist beispielsweise interessant zu sehen, dass die Heteronyme (und Mora, der zeitweise als starker Kandidat für die Rolle eines Heteronyms galt) auch zusammen mit Fernando Pessoa selbst, aber besonders unter sich verbunden sind, was auf eine gewisse Autonomie des heteronymischen Universums hindeutet. Die Namen von Shakespeare und Poe weisen auf die zwei wichtigsten Referenzen Pessoas aus der englischsprachigen Literatur hin, während Almada und S√°-Carneiro die zwei für ihn bedeutendsten zeitgenössischen portugiesischen Schriftsteller waren. Namen anderer Schriftsteller und historischer Figuren kommen tendenziell in späteren Perioden, etwa ab den 20er Jahren, häufiger vor. Sie zeigen ein zunehmendes Interesse Pessoas an der Veröffentlichung eigener √úbersetzungen von einigen für ihn entscheidenden Werke der Weltliteratur. Auch die wichtigsten Beziehungen Pessoas zu zeitgenössischen portugiesischen Schriftstellern und Kritikern sind im Netzwerk deutlich zu erkennen, besonders zwischen 1913 und 1918 (vgl. Abb. 5). Es handelt sich dabei vor allem um die modernistische Generation, die sich um die Zeitschrift Orpheu versammelt hat, und ab 1928 und bis zu Pessoas Tod 1935 dann die sogenannte zweite modernistische Generation um die Zeitschrift Presen√ßa. Abstract zur Konferenz Digital Humanities im deutschsprachigen Raum 2018 dadurch hergestellt, dass die interaktive Visualisierung an die digitale Edition angebunden ist. 5 Fußnoten Abbildung 5: Teilnetzwerk 1913-1918 Fazit 1. Die digitale Edition ist in einer Beta-Version unter http://www.pessoadigital.pt [letzter Zugriff 14. Januar 2018] verfügbar; die Entwicklung wird fortlaufend über GitHub organisiert: https://github.com/cceh/pessoa [letzter Zugriff 14. Januar 2018]. Zur editorischen Herangehensweise aus digitaler Perspektive vgl. HennyKrahmer und Sep√∫lveda 2017. 2. Die dem Netzwerk zugrunde liegenden Daten sind unter https://github.com/cceh/pessoa/tree/master/app/data/ network [letzter Zugriff 14. Januar 2018] einsehbar. 3. Das Ausgangs-Layout von Bostock trägt den Titel ""Force Layout with Mousover"" (Bostock 2017). 4. http://www.pessoadigital.pt/de/network/documentation [letzter Zugriff 14. Januar 2018]. 5. Derzeit über den Menüpunkt ""Chronologie"", vgl. http:// www.pessoadigital.pt/de/index.html [letzter Zugriff 14. Januar 2018]. Die aus der Netzwerkinterpretation gewonnenen literaturhistorischen Erkenntnisse bestätigen, dass für Pessoa die Edition und Publikation des Werkes und dessen Planung nicht von der Bedeutungsebene des Werkes selbst zu unterscheiden sind. Dass bestimmte Namen insgesamt oder zu bestimmten Zeiten besonders häufig auf den Publikationsplänen auftauchen, ist gleichbedeutend mit deren Wichtigkeit für das Werk in der entsprechenden Zeitperiode. Das gilt neben dem Vorkommen einzelner Namen auch für die gemeinsamen Vorkommen mehrerer Namen, wodurch die Bedeutung bestimmter Konstellationen zu bestimmten Zeiten sowohl in Pessoas Publikationsplänen als auch in seinem Werk deutlich wird. Diese grundlegende Erkenntnis für die Interpretation von Pessoas Werk bestätigt einige Intuitionen der Kritiker über den größeren Zusammenhang mehrerer Ebenen von Pessoas Werk (z. B. der Ebene der Edition des Werkes, vgl. dazu Sep√∫lveda und Uribe 2016), sowie die Vorstellung von Pessoas Werkganzem, die einer materiellen Fragmentarität seiner Schriften gegenübersteht (vgl. dazu Martins 2003, Gusm√£o 2003, Sep√∫lveda 2013, Feij√≥ 2015). Methodisch eröffnet die Visualisierung durch das interaktive Element und den höheren Grad der Abstraktion gegenüber den edierten Dokumenten, die in der digitalen Edition studiert werden können, neue Interpretationsspielräume. Dabei ist es jedoch sehr wichtig, die Dokument-, Textund Datengrundlage sowie die methodischen Wege zum Netzwerk und zur visuellen Darstellung stets im Blick zu behalten. Für diesen Beitrag bieten wir dafür eine Dokumentation an, die direkt mit dem interaktiven Netzwerk verbunden ist. 4 Der Zusammenhang zwischen Quellen und Analyse wird auch Bibliographie Bostock, Mike (2017): ""Force Layout with Mouseover Labels"", in: Mike Bostock‚Äôs Blocks. https://bl.ocks.org/ mbostock/1212215 [letzter Zugriff 14. Januar 2018]. Cunha, Teresa Sobral (1987): ""Planos e projectos editoriais de Fernando Pessoa: uma velha quest√£o"", in: Revista da Biblioteca Nacional, S√©rie 2, Vol. 2, N. 1: 92-107. Feij√≥, Ant√≥nio M. (2015): ""Uma admira√ß√£o pastoril pelo diabo (Pessoa e Pascoaes)"", in: Pessoana. Ensaios. Lissabon: Imprensa Nacional-Casa da Moeda. Gusm√£o, Manuel (2003): ""O Fausto ‚Äî um teatro em ru√≠nas"", in: Rom√¢nica 12: 67-86. Henny-Krahmer, Ulrike / Sep√∫lveda, Pedro (2017): ""Pessoa‚Äôs editorial projects and publications: the digital edition as a multiple form of textual criticism"", in: Boot, Peter / Cappellotto, Anna / Dillen, Wout / Fischer, Franz / Kelly, Aodh√°n / Mertgens, Andreas / Sichani, Anna-Maria / Spadini, Elena / van Hulle, Dirk (eds.): Advances in Scholarly Editing. Papers presented at the DiXiT conferences in The Hague, Cologne, and Antwerp. Leiden: Sidestone Press, 125-133 https://www.sidestone.com/books/advances-indigital-scholarly-editing [letzter Zugriff 14. Januar 2018]. Martins, Fernando Cabral (2003): ""Breves notas sobre a alta defini√ß√£o"", in: Rom√¢nica. N.¬∫ 12. 157-164. Pessoa, Fernando (2012): Teoria da Heteron√≠mia. Hsg. von Fernando Cabral Martins und Richard Zenith. Lissabon: Ass√≠rio & Alvim. Rigal, Alexandre / Rodighiero, Dario / Cellard, Loup (2016): ""The Trajectories Tool: Amplifying Network Visualization Complexity"", in: Digital Humanities 2016. Abstract zur Konferenz Digital Humanities im deutschsprachigen Raum 2018 Conference Abstracts. Krak√≥w: Jagiellonian University & Pedagogical University, 328-330 http://dh2016.adho.org/ abstracts/340 [letzter Zugriff 14. Januar]. Sep√∫lveda, Pedro (2013): Os livros de Fernando Pessoa. Lissabon: √Åtica. Sep√∫lveda, Pedro / Henny-Krahmer, Ulrike (eds., 2017): Fernando Pessoa ‚Äì Digitale Edition. Projekte und Publikationen. Editorische Leitung Pedro Sep√∫lveda, technische Leitung Ulrike Henny-Krahmer. Lissabon und Köln: IELT, Neue Universität Lissabon und CCeH, Universität zu Köln http://www.pessoadigital.pt [letzter Zugriff 14. Januar 2018]. Sep√∫lveda, Pedro / Uribe, Jorge (2016): O Planeamento editorial de Fernando Pessoa. Lissabon: Imprensa Nacional-Casa da Moeda. Xanthos, Aris / Pante, Isaak / Rochat, Yannick / Grandjean, Martin (2016): ""Visualizing the Dynamics of Character Networks"", in: Digital Humanities 2016. Conference Abstracts. Krak√≥w: Jagiellonian University & Pedagogical University, 417-419 http://dh2016.adho.org/ abstracts/407 [letzter Zugriff 14. Januar 2018]."
2019,DHd2019,159_final-LAUBROCK_Jochen_Grundzüge_einer_visuellen_Stilometrie.xml,Grundzüge einer visuellen Stilometrie,"Jochen Laubrock (Universität Potsdam, Deutschland); David Dubray (Universität Potsdam, Deutschland)","Convolutional Neural Network, Maschinelles Sehen, Visuelle Stilometrie","Stilistische Analyse, Bilder","  Als Material verwenden wir zwei Sammlungen grafischer Literatur: (a) das Graphic Narrative Corpus (GNC; Dunst, Hartel and Laubrock 2009) und (b) Manga109 (Matsui et al. 2017). Das GNC ist eine kuratierte Sammlung über 200 zeitgenössischer Graphic Novels aus den Jahren 1979 bis 2017 mit einem Gesamtumfang von mehr als 50.000 Seiten. Der GNC beinhaltet Werke verschiedener Genres (z.B. Autobiographie, New Journalism, Crime, Superhelden). Manga109 besteht aus 109 Manga-Bänden (mehr als 20.000 Seiten), die zwischen 1970 und 2010 im Handel erhältlich waren und 2017 der Wissenschaft zur Verfügung gestellt wurden. Die Korpora wurden durch zufällige geschichtete Stichprobenziehung in ein Trainings- und ein Testcorpus unterteilt. Der CNN-Teil eines auf dem ImageNet-Datensatz (Deng et al. 2009) vortrainierten Xception-Netzwerk wurde benutzt, um Illustratoren in den beiden Korpora zu klassifizieren.  Zusätzlich haben wir Klassifikationen basierend auf dem Output einzelner Schichten berechnet. Insgesamt vergleichen wir also die Klassifikation unter Berücksichtigung einzelner Schichten 0, 1, ..., k vs. mit der bei Berücksichtigung aller Schichten von 0 bis k. Der Merkmalsvektor wurde im letzteren Fall durch einfache Verkettung der Signaturen gebildet. Abbildung 1 zeigt die Genauigkeit der Klassifikation als Funktion der zugrundeliegenden Merkmale. Insgesamt lassen sich die Seiten aufgrund rein visueller Analyse sehr gut ihren Urhebern zuordnen. Man erkennt am Abfall der Kurve für Merkmale aus einzelnen Schichten, dass für die Illustrator-Klassifikation die Repräsentationen mittlerer Ebenen am entscheidendsten sind. Die stilistische Signatur einer Graphic Novel basiert scheinbar eher auf Merkmalen mittlerer Komplexität wie Schraffuren, Texturen oder Schwüngen als auf höher integrierten Merkmalen wie Objektteilen oder spezifischen Motiven. Basierend auf den Merkmalsvektoren haben wir eine bildbasierte Öhnlichkeitssuche implementiert. Nach Eingabe eines Suchbildes werden beispielsweise die 10 ähnlichsten Bilder ausgegeben. Die Untersuchung der Klassifikationsfehler ist interessant, sie zeigt beispielsweise, dass unterschiedliche Werke eines Autors zusammen gruppiert werden. Verwechslungen treten eher innerhalb von als zwischen Genres auf. Selbst historische Entwicklungen lassen sich abbilden: In ""750 Years in Paris"" illustriert Vincent Mahé die Entwicklung eines Häuserblocks in Paris von 1265 bis 2015. Die Bildsuche mit einer ""frühen"" Seite liefert Bilder aus der frühen Zeit, ebenso liefert die Bildsuche mit einer ""späten"" Seite Bilder aus einer späteren Epoche. Bei der semantischen Segmentation von Sprechblasen haben wir ein hervorragendes Ergebnis erzielt. Der F1-Score auf dem Testset betrug 0.935. Auch Elemente wie ein geschwungener Hinweisstrich / Dorn und an den Rändern offene Sprechblasen konnten sehr gut segmentiert werden. Abbildung 2 zeigt ein Beispiel einer Seite, auf der alle Sprechblasen korrekt detektiert und sehr gut segmentiert wurden. Wir haben verschiedene Sammlungen grafischer Literatur mit CNNs beschrieben und den Beitrag interner CNN-Repräsentationen unterschiedlicher Schichten zur Klassifikation von Zeichenstilen untersucht. Unsere Ergebnisse zeigen, dass der Individualstil eines Zeichners eher durch Merkmale mittlerer als durch solche höherer Komplexität charakterisiert ist. Allgemein haben CNN-basierte Repräsentationen das Potenzial, eine formale Beschreibung stilistischer Merkmale abzubilden. Sie sind deshalb aussichtsreiche Kandidaten für eine quantitative Fundierung bildwissenschaftlicher Form- und Strukturanalyse."
2019,DHd2019,228_final-SPORLEDER_Caroline_Multimodales_Zusammenspiel_von_Text_und_e.xml,Multimodales Zusammenspiel von Text und erlebter Stimme 'Analyse der Lautstärkesignale in direkter Rede,"Svenja Guhr (GCDH, Universität Göttingen, Deutschland); Hanna Varachkina (GCDH, Universität Göttingen, Deutschland); Geumbi Lee (GCDH, Universität Göttingen, Deutschland); Caroline Sporleder (GCDH, Universität Göttingen, Deutschland)","Lautstärkeanalyse, Literaturanalyse, Expressionismus, Verba Dicendi","Beziehungsanalyse, Stilistische Analyse, Sprache, Multimodale Kommunikation, Ton, Text"," Autorinnen und Autoren (i.F. Autoren) nutzen die direkte Rede als Mittel, um den Figuren eine Stimme zu geben und sie in den Köpfen ihrer Leser sprechen zu lassen (Nord, 1997). Als Möglichkeiten bestehen Geräuschbeschreibungen aber auch die redeeinleitenden Verben, Verba Dicendi, die bei der Verschriftung der Figurenrede eine relevante Rolle spielen. Diese Verbgruppe beschreibt die Art und Weise, wie die Leser sich die Konversation der Figuren vorzustellen haben, d.h. ob die Romanfiguren z.B. schreiend oder flüsternd kommunizieren. Verba Dicendi können anhand ihrer multimodalen Eigenschaft das Inhaltsverständnis unterstützen. Sie beschreiben die Sprechsituation und die Realisierung der direkten Rede und dienen dem multimodalen Zusammenspiel von Text und erlebter Stimme.  Im Rahmen einer Pilotstudie haben wir die Verba Dicendi und die sie beschreibenden Adverbien näher betrachtet mit dem Ziel stichprobenartig zu untersuchen, inwiefern die Lautstärkesignale eines Textes mit literaturwissenschaftlich relevanten Kategorien korrelieren. Eine schon von Katsma (2014) geäußerte Hypothese ist, dass Lautstärke mit ""Emotionalität"" zusammenhänge. Eine literarische Strömung wie der Expressionismus, der durch eine Betonung des inneren Ausdrucks und eine Ablehnung des Rationalen charakterisiert ist, müsste sich daher durch große Schwankungen zwischen lauten und leisen Passagen auszeichnen. Um diese Hypothese zu untersuchen wurden drei Korpora (insg. 161 Texte) zusammengestellt. Das erste besteht aus 57 Prosatexten um 1900 von Autoren, die der expressionistischen Literaturströmung zugeordnet werden können (Anz, 2016: 5f.). Als Vergleichsbasis dient ein zweites Korpus desselben Zeitraums von Autoren, die verschiedenen literarischen Strömungen zugeordnet werden (Fin de Si√®cle, Exilliteratur, Naturalismus, Realismus). Das dritte Korpus, bestehend aus 14 kurzen Texten von Autoren um 1900, dient als manuell annotiertes Kontrollkorpus der Evaluierung der Ergebnisse.  Zudem wurden Adjektive und Adverbien, die die Verba Dicendi als Träger der Beschreibungsinformationen der direkten Rede umgeben, in einem Fenster von vier Tokens vor und nach dem Verb in die Betrachtung miteinbezogen. Dabei wurde herausgefunden, dass vor allem die Adverbien und Kollokationen (z.B. ""mit lauter Stimme"") für das Lautstärkeprofil relevant sind. "
2019,DHd2019,260_final-VITT_Thorsten_Intervalle__Konflikte__Zyklen__Modellierung_vo.xml,"Intervalle, Konflikte, Zyklen. Modellierung von Makrogenese in der Faustedition","Thorsten Vitt (Julius-Maximilians-Universität Würzburg, Deutschland); Gerrit Brüning (Freies Deutsches Hochstift / Frankfurter Goethe-Museum); Dietmar Pravida (Freies Deutsches Hochstift / Frankfurter Goethe-Museum); Moritz Wissenbach (Julius-Maximilians-Universität Würzburg, Deutschland)","Chronologie, Datierung, Faust, Graphen, Werkgenese","Beziehungsanalyse, Modellierung, Daten, Manuskript, Forschungsprozess"," Besonders schwierig ist es, nicht bloß einzelne Objekte zu datieren, sondern eine große Menge in eine chronologische Ordnung zu bringen, wenn die Objekte genetisch voneinander abhängig sind, nur wenige absolute Daten zur Verfügung stehen und sonst nur lokale Anhaltspunkte für relative Chronologien gegeben sind (klassisches Beispiel: die antike Chronographie; Grafton 1993, Burgess/Witakowski 1999). Dies ist auch bei umfangreichen genetischen Handschriftendossiers neuzeitlicher Autoren und Werken mit komplexer Entstehungsgeschichte der Fall. Hier können einzelne Teilentwürfe in relativer zeitlicher Beziehung stehen, vereinzelt sind Datierungen verfügbar; doch die Rekonstruktion der Und so liegen die Dinge auch bei ""Faust"". Nur wenige der makrogenetischen Objekte sind genau datierbar; stattdessen gibt es eine große Menge relativer, aber strikt lokaler Chronologien für jeweils nur einige Objekte. Den bislang einzigen Versuch, Einzelaussagen zu aggregieren und alle Objekte in zeitlich-stemmatische Beziehung zueinander zu setzen, macht Fischer-Lamberg für zwei Akte des ""Faust II"". Ihre Stemmata (Fischer-Lamberg 1955: 150–166) markieren die praktische Grenze dessen, was an Einzelinformationen mit menschlichen Mitteln aggregiert werden kann. Die Rekonstruktion einer (theoretisch beliebig großen) Makrogenese verlangt nach maschineller Verarbeitung und visueller Aufbereitung vorhandener Einzelinformationen. Um dies zu ermöglichen, wurde der Informationsgehalt der verfügbaren einschlägigen Aussagen zur Datierung Eine  Eine  Die verschiedenen Aussagen werden in einem gerichteten Graphen modelliert  Aus diesem Graphen lassen sich Informationen ableiten, die sich erst aus dem Zusammenspiel der Einzelaussagen ergeben: Betrachtet man einen Zeugen Ist die Gesamtheit der Aussagen nicht widerspruchsfrei, so ergeben sich Zyklen im Graphen. Dies induziert einen Teilgraphen, in dem (vgl. Abb. 4 mit relativen Datierungen einiger Handschriften) jeder Knoten von jedem anderen erreichbar ist (der Teilgraph ist  Um den Graphen aus Abb.¬†4 zyklenfrei zu machen, ist die Entfernung von wenigstens drei Kanten notwendig (gestrichelt). Der komplette Makrogenesegraph enthält eine stark zusammenhängende Komponente mit 477 Dokumenten und 2136 Kanten 'zu umfangreich, um die Konflikte manuell zu eliminieren. Eine möglichst kleine Menge von Kanten zu entfernen, um einen zyklenfreien Graphen zu erhalten, ist ein als Entfernt man alle Konfliktkanten, so erhält man einen zyklenfreien gerichteten Graphen (DAG), der die Basis für die automatisierte Weiterverarbeitung ist. Dessen Knoten können in eine Um die aus einer Vielzahl teils widersprüchlicher Aussagen mechanisch gezogenen Schlüsse nachvollziehbar und verbesserbar zu machen, werden die Daten in einer Reihe verlinkter Darstellungen mit GraphViz (Gansner/North 2000) visualisiert. Die Grundlage bildet der Gesamtgraph mit allen Aussagen, in dem algorithmisch entfernte Aussagen (Konflikte) rotgestrichelt visualisiert werden. Zu jedem Zeugen gibt es einen Teilgraphen, der seine Nachbarn, die nächsten erreichbaren absoluten Datierungen und alle Aussagen dazwischen visualisiert. Darunter werden die Aussagen tabellarisch aufgelistet. Zu jeder (entfernten) Konfliktkante zeigt eine separate Visualisierung einen Pfad in der Gegenrichtung, mit dem die Kante in Konflikt stand, so dass der Konflikt erkennbar wird und zu den beteiligten Zeugen weiternavigiert werden kann. Anhand der Visualisierungen können die vorliegenden Reihenfolgeentscheidungen nachvollzogen, aber auch Datierungskontroversen und -lücken identifiziert undin den Quelldateien behoben werden: Die visualisierten Ergebnisse erfüllen so einen mehrfachen Zweck: Sie dienen zur systematischen Erschließung der einschlägigen Forschung, zur Klärung der genetischen Verhältnisse für den gesamten Faust und als Hilfsmittel zur Überprüfung, Vervollständigung und Verbesserung der Datenlage, d.h. zur Erweiterung des Forschungsstand. Darüber hinaus wird die ermittelte Reihenfolge in der Faustediton verwendet, um die Zeilensynopse sowie das Balkendiagramm zu sortieren. Neben der manuellen Nachbearbeitung der Daten kommen zur Verbesserung des Verfahrens alternative Heuristiken für das Minimum-Feedback-Arc-Problem in Frage (z.B. Even et al. 1998).  Eine Alternative zu der oben beschriebenen Abbildung unscharfer Aussagen auf scharf begrenzte Intervalle ist etwa die Modellierung über Fuzzy-Mengen (vgl. z.B. Barro et al. 1994). Dies erfordert jedoch auch die Neudefinition der Relationen (Schockaert/De Cock 2008) und der darauf aufbauenden Verfahren etwa zur Konfliktauflösung. Der vorgestellte Ansatz basiert auf bereits vorhandenen, mit traditionellen philologischen Mitteln gewonnenen Datierungsaussagen. In Wissenbach/Pravida/Middell (2012) wird ein Verfahren vorgestellt, mit der kodierte, textinhärente Eigenschaften von Fassungen für die regelbasierte Bildung genetischer Hypothesen genutzt werden, um auf diesem Weg generelle Hypothesen zur Arbeitsweise des Autors zu verifizieren."
2019,DHd2019,156_final-MEYER_SICKENDIEK_Burkhard_Deep_Learning_als_Herausforderung_.xml,Deep Learning als Herausforderung für die digitale Literaturwissenschaft,"Fotis Jannidis (Julius-Maximilians-Universität Würzburg, Deutschland); Christof Schöch (Universität Trier, Trier Center for Digital Humanities, Deutschland); Jonas Kuhn
(Universität Stuttgart, Centrum für Reflektierte Textanalyse, Deutschland); Timo Baumann (Carnegie Mellon University, Pittsburgh, USA); Hussein Hussein (Freie Universität Berlin, Deutschland); Thomas Haider (Max-Planck-Institut für empirische ästhetik, Frankfurt am Main, Deutschland); Burkhard Meyer-Sickendiek (Freie Universität Berlin, Deutschland)","Deep Learning, Embedding, Prosodieerkennung, Topic Modelling","Deep Learning, Embedding, Prosodieerkennung, Topic Modelling"," Bisher konzentrierten sich die klassischen ""Digital Humanities"" eher auf die Generierung und Reflexion digitaler Ressourcen wie Textausgaben, Repositorien oder Bilddatenbanken. Dagegen gibt es nur wenige Versuche, Deep Learning in die digitalen Geisteswissenschaften einzubringen. Zumeist wurde Deep Learning in sehr großen Datenbanken von Unternehmen wie Google, YouTube, Bluefin Labors oder Echonest getestet, etwa um Social Media Signale und den Inhalt von Medien in sozialen Netzwerken zu analysieren. Gerade deshalb blieb in diesem Feld die alte Kluft zwischen traditionellen Geisteswissenschaften und Informatik bestehen. Unser Panel will einen Beitrag leisten, um diese Lücke zu schließen. Wir wollen vor allem die Probleme erörtern, die bei der rechnerischen Analyse literarischer Texte mit Techniken des tiefen Lernens entstehen, z.B.: Können maschinelle Lerntechniken durch Clustering tatsächlich verdeckte Muster in Textdaten erfassen (Graves 2012)? Wie lassen sich auf der Grundlage eines maschinell erlernten Modells Grenzfälle, Kategorisierungsfehler, Ausreißer und ähnliche Besonderheiten erkennen bzw. in den Klassifikationsprozess einbauen? Wie geht man mit dem großen Problem der ""black box"" um, wie lassen sich die in den ""hidden layers"" stattfindenden Klassifikationsprozesse nachvollziehen bzw. gar transparent machen? Und welche Tools für die manuelle (z.B. Sonic Visualizer) und automatische Annotation (z.B. PRAAT, ToBI, oder Sphinx) bzw. welche Softwares für die Modellierung (DyNet, TensorFlow, Caffe, MxNet, Keras, ConvNetJS, Gensim, Theano, und Torch) sind empfehlenswert? Fragestellung und Aufbau des Panels  Folgende Personen haben dabei eine Teilnahme an dem Panel zugesagt: Wir werden in einem ersten Teil von maximal 30 Minuten einzelne Impulsvorträge präsentieren, und dann in einem zweiten Teil von ebenfalls 30 Minuten Topic Modeling und Embedding als wichtige Themenfelder des tiefen Lernens in den Geisteswissenschaften fokussieren. In einem dritten Teil von ebenfalls 30 Minuten wollen wir dann das Panel für die Diskussionen mit dem Publikum öffnen. Mögliche Themengebiete für die Paneldiskussion im dritten und letzten Teil wären insbesondere die Verwendung tiefer Lernverfahren in den digitalen Geisteswissenschaften, etwa mit Blick auf Stilometrie, Computerstilistik, Reim- und Metrikenanalyse, Aktantenanalyse, oder Themenmodellierung. Dabei soll die Publikumsdiskussion all jenen ein Forum bieten, die sich ein tieferes Verständnis und eine praktische Schulung in deep learning sowie eine Plattform für den Austausch von Praktiken, Ergebnissen und Erfahrungen im Umfeld mit einschlägigen Tools erhoffen. Dies kann sich auch auf Kenntnisse aus den Nachbardisziplinen erstrecken, insofern diese über bereits vorhandenes Wissen hinsichtlich der Anwendung ""tiefer Lerntechniken"" etwa im Bereich des Data Mining, der Statistik oder der Verarbeitung natürlicher Sprache verfügen. Auf diese Weise erhoffen wir uns eine effektive Fokusverlagerung innerhalb der digitalen Geisteswissenschaften: von der Erstellung und Archivierung digitaler Artefakte und Repositorien hin zu echten Rechenlösungen auf der Grundlage maschinellen Lernens."
2019,DHd2019,195_final-DIMPEL_Friedrich_Gute_Wörter_für_Delta__Verbesserung_der_Aut.xml,Gute Wörter für Delta: Verbesserung der Autorschaftsattribution durch autorspezifische distinktive Wörter,Friedrich Michael Dimpel (Universität Erlangen); Thomas Proisl (Universität Erlangen),"Stilometrie, Autorschaftsattribution, Quantitative Verfahren","Modellierung, Stilistische Analyse, Sprache, Personen, Text","Autorschaftsattributionsverfahren sind längst etabliert (vgl. Burrows 2002, Jannidis et al. 2015). Auf DHd-Jahrestagungen wurden Techniken vorgestellt, die die Attributionsverfahren optimieren (Büttner et al. 2016, Dimpel 2017a, 2018a/b). Die Optimierung der Verfahren ist deshalb wichtig, weil ein literaturwissenschaftliches Interesse besteht, die Frage nach der Autorschaft auch bei schwierigen Bedingungen zu klären. Gute Bedingungen, bei denen in Evaluationstests über hohe Erkennungsquoten berichtet wurde, sind dann gegeben, wenn viele Texte aus der gleichen Gattung vorliegen, wenn die Texte eine ausreichende Länge aufweisen (mindestens 5.000 Wortformen), wenn die Texte chronologisch nicht zu weit auseinander liegen und wenn die Texte einen möglichst normierten Sprachstand hinsichtlich Orthographie und Standardsprache aufweisen (Schöch 2014, Eder 2013a und 2013b). All diese Bedingungen sind nicht erfüllt, wenn man mittelhochdeutsche Kleinepik untersuchen möchte 'gerade etwa mit Blick auf die Kleinepik des Strickers wäre eine stilometrische Klärung bei einigen Texten interessant. Mittelhochdeutsche Texte folgen keiner geregelten Orthographie, es liegen sowohl mehr oder weniger normalisierte als auch nicht-normalisierte digitale Texte vor, sie sind oft dialektal geprägt und viele Texte aus dem Bereich der Kleinepik übersteigen kaum eine Anzahl von 2.000 Wortformen. Anhand der ""Halben Birne"" (umstrittene Autorschaft Konrads von Würzburg; 2.469 Wortformen) wurde ein Verfahren vorgestellt, wie mit Burrows"" Delta die distinktiven Wörter ermittelt werden können, die Konrads Wortschatz von anderen Autoren unterscheiden (Dimpel 2018a). Damit konnte die Erkennungsqualität so optimiert werden, dass Delta auf die ""Halbe Birne"" angewendet werden konnte. Nun wird anhand eines weniger problematischen Korpus (Romane ca. 19. Jahrhundert Für Burrows"" Delta ermittelt man relative Worthäufigkeiten 'hier für Die guten Wörter beruhen auf den Level-2-Differenzen: Die Formal ausgedrückt werden die Level-2-Differenzen anhand von zwei Teilkorpora ermittelt: Einem Teilkorpus Wir prüfen, ob mit ""guten Wörtern"" Texte des Zielautors besser von denen anderer Autoren abgegrenzt werden können als mit dem üblichen Delta-Ansatz, der rein auf den Für die Berechnung der guten Wörter und zur Evaluation werden ein Ratekorpus und ein Vergleichskorpus verwendet. Im Vergleichskorpus befinden sich ein Autorvergleichstext (Text vom Zielautor), je nach Testreihe zwanzig oder mehr Distraktortexte von Autoren, die nicht im Ratekorpus vertreten sind, sowie in manchen Experimenten Texte von den übrigen im Ratekorpus vertretenen Autoren. Wenn der Delta-Abstand zwischen Ratetext und Autorvergleichstext der niedrigste ist, gilt dieser Text als dem Zielautor zugeordnet; ist der Abstand zu einem Distraktortext von einem anderen Autor am niedrigsten, gilt der Text als falsch erkannt. Die guten Wörter werden jeweils auf den alphabetisch ersten drei Texten des Autors für jeden Autor in vier Varianten berechnet. Ausgangspunkt sind jeweils die häufigsten 1.200 Wörter: Wir verwenden stets 400 gute Wörter, da auch kurze Texte mit 2.000 Token getestet werden sollen. Wenn eine Variante mehr als 400 gute Wörter liefert, verwenden wir die 400 Wörter mit den größten Level-2-Differenz-Mittelwerten; wenn sie weniger als 400 gute Wörter liefert, füllen wir mit MFWs auf. Wir führen drei Testreihen durch: Dadurch werden wichtige Einsatzszenarien (lange Texte, kurze Texte mit langen Vergleichstexten und kurze Texte mit kurzen Vergleichstexten) abgedeckt. Hier evaluieren wir die Methoden bei vollständigen Romanen. Einmal enthält das Vergleichskorpus Texte von allen Autoren im Ratekorpus, im zweiten Schritt enthält das Vergleichskorpus zwar einen Text des Zielautors, jedoch keinen Text der anderen Autoren im Ratekorpus. Für das erste Experiment verwenden wir ein Vergleichskorpus, das je einen Text von allen 32 Autoren enthält. Für jeden Autor ermitteln wir die guten Wörter auf Basis des Vergleichskorpus und zwei weiteren Texten des Autors (also insgesamt drei Autortexten und 31 Distraktortexten). Die Ratekorpora für jeden Autor umfassen drei Texte des Autors und drei zufällige Texte von den übrigen Autoren. Mittelwerte über alle 32 Autoren: Die klassischen Delta-Varianten (MFW400, MFW1200) haben in Bezug auf den Zielautor eine hohe Precision und einen niedrigeren Recall, während sich das Verhältnis für Texte anderer Autoren umdreht: Niedrige Erkennungsquote, dafür kaum False Positives (Texte, die fälschlicherweise dem Zielautor zugeschrieben werden). Die Gute-Wörter-Varianten führen zu deutlichen Recall-Verbesserungen beim Zielautor auf Kosten einer etwas geringeren Precision. Die Precision für Texte anderer Autoren steigt zu Lasten des Recalls. Mit den guten Wörtern werden also mehr Texte des Autors richtig erkannt (>97%), dafür gibt es minimal mehr False Positives (2,5%). Mehr häufigste Wörter funktionieren besser als weniger. Alle vier Gute-Wörter-Varianten funktionieren besser als die reinen Delta-Varianten (Verbesserungen von 6–8 Punkten bei Accuracy und F Für das zweite Experiment wird lediglich das Vergleichskorpus verändert. Es enthält jetzt jeweils einen Text des Zielautors und je einen Text von 24 zusätzlichen Autoren (die sich nicht mit den 32 getesteten Autoren überschneiden). Die wahren Autoren für drei der sechs Texte in den Ratekorpora befinden sich nicht mehr im Vergleichskorpus, was die Klassifikation der entsprechenden Texte erschwert. Tatsächlich bewegen sich alle Ergebnisse auf einem etwas niedrigeren Niveau als im ersten Experiment. Auch hier: Mittels guter Wörter steigt der Recall für Texte des Zielautors deutlich, während die Precision leicht sinkt. Für Texte anderer Autoren verhält es sich umgekehrt. Auch hier führen die guten Wörter zu besseren Accuracy- und F In dieser Testreihe wollen wir prüfen, wie gut die einzelnen Methoden für kurze Rate- und lange (vollständige) Vergleichstexte funktionieren. Dazu ziehen wir für jeden Ratetext 100 Stichproben √† 2.000 Wörter; der übrige Versuchsaufbau ist identisch zu Testreihe 1. Test 2a) Alle Autoren aus den Ratekorpora sind im Vergleichskorpus vertreten: Die Ergebnisse sind auf den 2.000-Wort-Samples spürbar schlechter. Das Gesamtbild ist jedoch dasselbe: Die guten Wörter verbessern die Klassifikation und führen zu deutlich höheren Recall-Werten für den Zielautor bei etwas niedrigerer Precision. Test 2B) Die Nicht-Zielautoren sind nicht im Vergleichskorpus vertreten: Auch hier verbessern die guten Wörter die Erkennung der Zielautortexte (Erkennungsquote >96%). Größere Einbußen gibt es bei der Erkennung von Texten, die nicht vom Zielautor stammen, so dass die guten Wörter zwar die Erkennung von Texten des Zielautors enorm verbessern (Reduktion der Fehlerrate >80%), die Ergebnisse insgesamt aber leicht schlechter sind. Nunmehr werden die guten Wörter mit zwei Ratetexten und einem Autorvergleichstext berechnet; wenn jedoch mehr als sechs Texte pro Autor vorliegen, werden entsprechend mehr Ratetexte verwendet. Da diese drei (oder mehr) Texte nicht wie oben reihum als Rate- und Autorvergleichstext verwendet werden, ist die Qualität der Wortlisten etwas schlechter. Im Evaluationstest werden nicht nur im Ratekorpus kleine Bag-of-Words mit 2.000 Wortformen verwendet, sondern auch im Vergleichskorpus, wodurch die Erkennungsquoten deutlich schlechter werden. Die Parameter für die Author-Recall-Ermittlung sind: 250 Stichproben bei 400 MFWs, im Vergleichskorpus befinden sich neben dem Autorvergleichstext 24 Texte von anderen Autoren. Die False-Positives werden hier mit je zwei Texten von allen anderen 31 Autoren als Ratetexte in 32 Tests gegen das Vergleichskorpus ermittelt, in dem neben 24 Texten von anderen Autoren reihum alle 32 Autoren mit einem Autorvergleichstext vertreten waren (Parameter: Bag-of-Words mit 2.000 Wortformen, 400 MFWs, 100 Stichproben). Auch hier übertrifft der Gewinn bei der Verbesserung der Erkennungsquote die Verschlechterung bei den False-Positives. Wenn sich der fragliche Autor im Vergleichskorpus befindet, ergibt sich sogar bei den False-Positives bei A) und B) eine leichte Verbesserung. Bei C) ist die Verschlechterung bei den False-Positives problematisch. Die Gute-Wörter-Verfahren führen zu besseren Erkennungsquoten; in geringem Umfang treten mehr False-Positives auf, doch sind die positiven Effekte größer als die negativen. Die detaillierten Zahlen, für die hier kein Platz verfügbar ist, zeigen, dass die False-Positives je nach Autor erheblich schwanken. In einem Anwendungsfall kann vorab getestet werden, ob es bei dem fraglichen Autor zu erhöhten False-Positives kommt 'um zu prüfen, ob das Verfahren bei diesem Autor gut anwendbar ist. Künftig wollen wir untersuchen, ob ein mittels Maschinellem Lernen erstelltes autorspezifisches Vokabular zu weiteren Verbesserungen führt."
2019,DHd2019,183_final-KREMER_Gerhard_Maschinelles_Lernen_lernen__Ein_CRETA_Hackato.xml,Maschinelles Lernen lernen: Ein CRETA-Hackatorial zur reflektierten automatischen Textanalyse,"Gerhard Kremer (Institut für maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Kerstin Jung (Institut für maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland)","maschinelle Lernverfahren, Entitätenreferenzen, Reflexion, Korpus, Evaluation, Programmiercode, Methodik, Praxiserfahrung","Programmierung, Inhaltsanalyse, Annotieren, Lehre, Organisation, Methoden","Das Ziel dieses Tutorials ist es, den Teilnehmerinnen und Teilnehmern konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten. Es werden dabei keinerlei Vorkenntnisse zu maschinellem Lernen oder Programmierkenntnisse vorausgesetzt. Es gibt keinen Grund, den Ergebnissen von maschinellen Lernverfahren im Allgemeinen und NLP-Tools im Besonderen blind zu vertrauen. Durch die konkreten Einblicke in den ""Maschinenraum"" von maschinellen Lernverfahren wird den Teilnehmenden ermöglicht, das Potenzial und die Grenzen statistischer Textanalysewerkzeuge realistischer einzuschätzen. Mittelfristig hoffen wir dadurch, den immer wieder auftretenden Frustrationen beim Einsatz automatischer Verfahren für die Textanalyse und deren teilweise wenig zufriedenstellender Ergebnis-Daten zu begegnen, aber auch die Nutzung und Interpretation der Ergebnisse von maschinellen Lernverfahren (d.h. in erster Linie von automatisch erzeugten Annotationen) zu fördern. Zu deren adäquater Nutzung, etwa in hermeneutischen Interpretationsschritten, ist der Einblick in die Funktionsweise der maschinellen Methoden unerlässlich. Insbesondere ist die Art und Herkunft der Trainingsdaten für die Qualität der maschinell produzierten Daten von Bedeutung, wie wir im Tutorial deutlich machen werden. Neben einem Python-Programm für die automatische Annotierung von Entitätenreferenzen, mit und an dem während des Tutorials gearbeitet werden wird, stellen wir ein heterogenes, manuell annotiertes Korpus sowie die Routinen zur Evaluation und zum Vergleich von Annotationen zu Verfügung. Das Korpus enthält Entitätenreferenzen, die im ""Center for Reflected Text Analytics"" (CRETA) Als empirisches Phänomen befassen wir uns mit dem Konzept der Entität und ihrer Referenz. Das Konzept steht für verschiedene linguistische und semantische Kategorien, die im Rahmen der Digital Humanities von Interesse sind. Es ist bewusst weit gefasst und damit anschlussfähig für verschiedene Forschungsfragen aus den geistes- und sozialwissenschaftlichen Disziplinen. Auf diese Weise können unterschiedliche Perspektiven auf Entitäten berücksichtigt werden. Insgesamt werden in den ausgewählten Texten fünf verschiedene Entitätenklassen betrachtet: PER (Personen/Figuren), LOC (Orte), ORG (Organisationen), EVT (Ereignisse) und WRK (Werke). Unter Entitätenreferenzen verstehen wir Ausdrücke, die auf eine Entität in der realen oder fiktiven Welt referieren. Das sind zum einen Eigennamen (Named Entities, z.B. ""Peter""), zum anderen Gattungsnamen (z.B. ""der Bauer""), sofern diese sich auf eine konkrete Instanz der Gattung beziehen. Dabei wird als Referenzausdruck immer die maximale Nominalphrase (inkl. Artikel, Attribut) annotiert. Pronominale Entitätenreferenzen werden hingegen nicht annotiert. In In den Die Grundlage für (überwachte) maschinelle Lernverfahren bilden Annotationen. Um die Annotierung von Entitätenreferenzen automatisieren zu können, bedarf es Textdaten, die die Vielfalt des Entitätenkonzepts abdecken. Bei diesem Tutorial werden wir auf Annotationen zurückgreifen, die im Rahmen von CRETA an der Universität Stuttgart entstanden sind (vgl. Blessing et al., 2017; Reiter et al., 2017a). Das Korpus enthält literarische Texte aus zwei Sprachstufen des Deutschen (Neuhochdeutsch und Mittelhochdeutsch) sowie ein sozialwissenschaftliches Teilkorpus. Der  Das Der Ablauf des Tutorials orientiert sich an sog. Zunächst diskutieren wir kurz die zugrundeliegenden Texte und deren Annotierung. Annotationsrichtlinien werden den Teilnehmerinnen und Teilnehmern im Vorfeld zur Verfügung gestellt. Im Rahmen der Einführung wird auch auf die konkrete Organisation der Annotationsarbeit eingegangen, so dass das Tutorial als Blaupause für zukünftige Tätigkeiten der Teilnehmenden in diesem und ähnlichen Arbeitsfeldern dienen kann. Die Teilnehmerinnen und Teilnehmer versuchen selbständig und unabhängig voneinander, eine Kombination aus maschinellen Lernverfahren, Merkmalsmenge und Parametersetzungen zu finden, die auf einem neuen, vom automatischen Lernverfahren ungesehenen Datensatz zu den Ergebnissen führt, die dem Goldstandard der manuellen Annotation am Öhnlichsten sind. Das bedeutet konkret, dass der Einfluss von berücksichtigten Features (z.B. Groß- und Kleinschreibung oder Wortlänge) auf die Erkennung von Entitätenreferenzen empirisch getestet werden kann. Dabei sind Intuitionen über die Daten und das annotierte Phänomen hilfreich, da simplem Durchprobieren aller möglichen Kombinationen (""brute force'"") zeitlich Grenzen gesetzt sind. Wir verzichten bewusst auf eine graphische Benutzerschnittstelle (vgl. Reiter et al., 2017b) 'stattdessen editieren die Teilnehmerinnen und Teilnehmer das (Python)-Programm direkt, nach einer Einführung und unter Anleitung. Vorkenntnisse in Python sind dabei nicht nötig: Das von uns zur Verfügung gestellte Programm ist so aufgebaut, dass auch Python-Neulinge relativ schnell die zu bearbeitenden Teile davon verstehen und damit experimentieren können. Wer bereits Erfahrung im Python-Programmieren hat, kann fortgeschrittene Funktionalitäten des Programms verwenden. Wie am Ende jedes maschinellen Lernprozesses wird auch bei uns abschließend eine Evaluation der automatisch generierten Annotationen durchgeführt. Hierfür werden den Teilnehmerinnen und Teilnehmern nach Ablauf einer begrenzten Zeit des Experimentierens und Testens (etwa 60 Minuten) die finalen, vorher unbekannten Testdaten zur Verfügung gestellt. Auf diese Daten werden die erstellten Modelle angewendet, um automatisch Annotationen zu erzeugen. Diese wiederum werden dann mit dem Goldstandard verglichen, wobei die verschiedenen Entitätenklassen sowie Teilkorpora getrennt evaluiert werden. Auch das Programm zur Evaluation stellen wir bereit. Am hier verwendeten Beispiel der automatischen Annotation von Entitätenreferenzen demonstrieren wir, welche Schritte für die Automatisierung einer Textanalyseaufgabe mittels maschinellen Lernverfahren nötig sind und wie diese konkret implementiert werden können. Die Teilnehmenden des Workshops bekommen einen zusammenhängenden Überblick von der manuellen Annotation ausgewählter Texte über die Feinjustierung der Lernverfahren bis zur Evaluation der Ergebnisse. Die vorgestellte Vorgehensweise für den gesamten Ablauf ist grundsätzlich auf ähnliche Projekte übertragbar. Das Tutorial schärft dabei das Verständnis für den Zusammenhang zwischen untersuchtem Konzept und den dafür relevanten Features, die in ein statistisches Lernverfahren einfließen. Durch Einblick in die technische Umsetzung bekommen die Teilnehmerinnen und Teilnehmer ein Verständnis für die Grenzen und Möglichkeiten der Automatisierung, das sie dazu befähigt, zum einen das Potenzial solcher Verfahren für eigene Vorhaben realistisch(er) einschätzen zu können, zum anderen aber auch Ergebnisse, die auf Basis solcher Verfahren erzielt wurden, angemessen hinterfragen und deuten zu können.  Im Vorfeld der Veranstaltung: Installationsanweisungen und Support Der Workshop wird ausgerichtet von Mitarbeitenden des ""Center for Reflected Text Analytics"" (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine ""black box"" sein, sondern auch für Nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird.  Der Interessenschwerpunkt Gerhard Kremers ist der reflektierte Einsatz von Werkzeugen der Computerlinguistik für geistes- und sozialwissenschaftliche Fragestellungen. Damit zusammenhängend gehören die Entwicklung übertragbarer Arbeitsmethoden und die angepasste, nutzerfreundliche Bedienbarkeit automatischer linguistischer Analysetools zu seinen Forschungsthemen.  Kerstin Jungs Forschungsinteressen liegen im Bereich der Nachhaltigkeit von (computer)linguistischen Ressourcen und Abläufen sowie der Verlässlichkeitsbeschreibung von automatisch erzeugten Annotationen. Dabei verfolgt sie einen aufgabenbasierten Ansatz und arbeitet an der Schnittstelle zwischen Computerlinguistik und anderen sprach- und textverarbeitenden Disziplinen. Zwischen 15 und 25. Es wird außer einem Beamer keine besondere technische Ausstattung benötigt. Es sollte sich um einen Raum handeln, in dem es möglich ist, den Teilnehmenden über die Schulter zu blicken und durch die Reihen zu gehen."
2019,DHd2019,191_final-GIUS_Evelyn_Korpuserstellung_als_literaturwissenschaftliche_.xml,Korpuserstellung als literaturwissenschaftliche Aufgabe,"Evelyn Gius (Universität Hamburg, Deutschland); Katharina Krüger (Universität Hamburg, Deutschland); Carla Sökefeld (Universität Hamburg, Deutschland)","Korpora, Korpuserstellung, Dubletten","Sammlung, Bereinigung, Literatur, Methoden, Forschungsprozess, Text","Die Praxis der Zusammenstellung von Primärtexten zu einem Korpus ist gewissermaßen literaturwissenschaftliches Alltagsgeschäft, trotzdem wird sie selten problematisiert. Die beiden Standardfälle der nicht-digitalen Korpusanalyse erscheinen auch bezüglich der Korpuszusammenstellung unproblematisch: (1) Die Forschungsfrage erfordert eine bestimmte Textbasis (etwa bei einer Untersuchung zu Krankheitsdarstellungen bei Thomas Mann), (2) Die Korpuserstellung basiert auf der Kanonizität der Texte. Zumindest im zweiten Fall ist das geeignete Vorgehen allerdings nicht selbstverständlich, denn es müsste begründet werden, nach welchen Kriterien Werke tatsächlich exemplarisch und repräsentativ sind. Dies wird jedoch kaum thematisiert (vgl. Gius, in Vorbereitung). Bei der konkreten Erstellung eines Ein Grund für die geringe Auseinandersetzung mit dem Thema kann sein, dass die Praxeologie der Zusammenstellung literarischer digitaler Korpora verhältnismäßig neu ist und sich bislang keine literaturwissenschaftlichen Routinen etablieren konnten, die das Zusammenstellen des Untersuchungsobjektes betreffen. Die digitale literaturwissenschaftliche Korpuserstellung stellt also ein ungelöstes methodologisches Grundproblem dar. Wir erläutern im Folgenden die Problematik der literaturwissenschaftlichen Korpuserstellung exemplarisch, um eine Diskussion darüber anzustoßen sowie mögliche Lösungsansätze vorzustellen. Im Rahmen unseres Forschungsprojektes zu genderspezifischer Darstellung von Krankheit in literarischen Texten im Forschungsverbund hermA (""Automatische Modellierung hermeneutischer Prozesse"") Da die Entwicklung von Strategien zur Bildung themenspezifischer Subkorpora ebenfalls Ziel des Forschungsvorhabens ist, gab es für die Textauswahl zunächst keine inhaltlichen Einschränkungen. Kriterien zur Textauswahl waren deshalb nur das Datum der Erstveröffentlichung, Textsprache, das Genre und Textlänge; Grundlage für ihre Ermittlung bildeten die Kolimo-Metadaten. Metadaten  Kinder- und Jugendbuch (im Kernkorpus)  Ohne spezifisches Genre Metadaten Kategorisierung der Angaben in den Metadaten Recherche  Ins Korpus aufgenommen wurden Texte mit Erstveröffentlichung zwischen 1870 und 1920. Die Auswahl wurde anhand der vorliegenden Metadaten getroffen und bei Bedarf weiter ergänzt. Je nach Repositorium unterschied sich die zusätzlich nötige Recherchearbeit stark: elf DTA-Texte, 7.602 Gutenberg-Texte, und 27.300 TextGrid-Texte hatten keine Datumsangabe. Da wir uns auf Phänomene konzentrieren, die in der deutschsprachigen zeitgenössischen Literatur verhandelt wurden, lag der Fokus auf standarddeutschen Texten. Übersetzungen wurden in ein Sonderkorpus aufgenommen, zu weniger als 90% deutschsprachige Texte aussortiert. Das nach diesen Kriterien manuell erstellte Kernkorpus umfasst nur noch etwa 2.700 Werke.  Die dargestellten Schwierigkeiten bei der Entscheidung über die Aufnahme eines Textes in das Kernkorpus basierten auf unvollständigen, widersprüchlichen oder nicht ohne weiteres aufeinander abbildbaren Metadaten und konnten anhand der dargelegten Setzungen vergleichsweise einfach gelöst werden. Weitaus schwieriger gestaltete sich hingegen die Identifikation von Dubletten und die Konzipierung einer geeigneten Problembehandlung –¬†ein leider typisches Problem bei der Aggregation eines Korpus aus verschiedenen Quellen. So zeigte sich bei einer ersten manuellen Durchsicht der Liste und der stichprobenartigen Überprüfung der Volltexte, dass Dubletten nicht eindeutig anhand von Metadaten identifizierbar sind. Die naheliegende Lösung, ein Volltextvergleich, müsste jedoch bei 2.700 Texten über sieben Millionen mögliche Paare vergleichen und würde auch bei der Nutzung von Cluster Computing Jahrzehnte dauern. Deshalb mussten Heuristiken entwickelt werden, um das Verfahren abzukürzen. Entsprechend haben wir einen Workflow entworfen, um mit möglichst hoher Treffgenauigkeit Dubletten zu identifizieren. Ziel war, alle echten Dubletten zu finden und das Korpus entsprechend zu bereinigen, ohne Texte vorschnell zu streichen. Dafür wurden folgende, größtenteils automatische Prüfungen durchgeführt: Bei den verbleibenden Textpaaren gehen wir von echten Dubletten aus. Für diese erfolgt eine Repositorien-Priorisierung nach der Qualität der Repositorien (1. DTA, 2. TextGrid, 3. Gutenberg).  Die Digitalisierung fördert die literaturwissenschaftliche Korpuserstellung in neuem Umfang und macht dadurch die Textzusammenstellung als literaturwissenschaftliches Problem offensichtlich, das methodologisch kaum beleuchtet ist. Oft dominiert die Frage, welche Texte überhaupt ins Korpus können, also aus welchen bereits digitalisierten oder noch digitalisierbaren Texten ausgewählt werden kann, die literaturwissenschaftlichen Überlegungen zur Textauswahl. Da die Menge digitalisiert vorliegender Texte stetig steigt, haben Korpora außerdem immer häufiger eine Größe, die von einzelnen Forscher/innen nicht mehr überschaubar ist. Deshalb muss sich die literaturwissenschaftliche Begründung der Relevanz der Texte in einem Korpus einer gewissen Größe im Zweifelsfall auf Aspekte, die als Informationen in den Metadaten der Texte vorliegen 'wie Zeit, Gattung/Genre oder Autor/innen 'beschränken. Sowohl die Menge der zur Verfügung stehenden Texte als auch die Qualität der Primär- und Metadaten sind noch steigerungsfähig. Hier ist die Forschungsgemeinschaft genauso gefordert wie Bibliotheken und Archive. Im Sinne einer wissenschaftlichen Qualitätssicherung ist es daher umso wichtiger, im Hinblick auf das Da das literaturwissenschaftliche Wissen über ein Korpus mit steigender Korpusgröße notwendigerweise ab- und damit die Gefahr unerwünschter Korrelationen zunimmt, sollte das Korpus außerdem mit Informationen angereichert werden, die für die Interpretation von Analyseergebnissen genutzt werden können (Epochenzugehörigkeit, thematische Zuordnung etc). Dadurch können entdeckte Korrelationen auf einer größeren Basis an Texteigenschaften 'also: besser 'interpretiert werden. Da das manuelle Ergänzen von Informationen sehr arbeitsaufwändig ist, sollten dafür (halb-)automatische Verfahren entwickelt bzw. genutzt werden. Die Anforderungen an ein Korpus variieren je nach Kontext und Forschungsfrage des Projekts, deshalb müssen allgemeine Qualitätskriterien für die Korpuserstellung eine gewisse Offenheit aufweisen. Grundsätzlich gilt aber: Für die Korpuserstellung muss frühzeitig eine Strategie zur Priorisierung von Problemen entwickelt 'und dokumentiert! 'werden. Dabei geht es darum, sich wie in diesem Beitrag skizziert eine Übersicht über konzeptuelle und technische Aspekte zu verschaffen und anschließend einfach zu lösende Probleme und konzeptuell wichtige Entscheidungen in eine geeignete Reihenfolge zu bringen 'den Workflow für die Korpuserstellung."
2019,DHd2019,151_final-WILLAND_Marcus_Ein_neues_Format_für_die_Digital_Humanities__.xml,Ein neues Format für die Digital Humanities: Shared Tasks. Zur Annotation narrativer Ebenen,"Marcus Willand (Universität Heidelberg, Universität Stuttgart); Evelyn Gius (Universität Hamburg); Nils Reiter (Universität Stuttgart)","Annotation, Shared Task, Evaluation","Annotieren, Theoretisierung, Bewertung, Projektmanagement, Methoden, Text","Dieses Paper führt unsere letztjährige Präsentation Bei einem ST bewerben sich Teams mit einem Vorschlag für die Lösung eines durch die Organisatoren ausgeschriebenen Problems, den Task. STs sind kompetitive Verfahren, weil die Lösungsvorschläge vergleichend evaluiert und gemäß einer definierten Metrik in eine Rangfolge gebracht werden. Vor allem in der Sprachverarbeitung (NLP, natural language processing) sind diese Arbeitszusammenhänge weit verbreitet und ein wesentlicher Antrieb für die Fortschritte bei wichtigen Aufgaben, etwa des syntaktischen Parsings. Wir haben uns für ein zweiphasiges Verfahren entschieden. Die erste Phase '""SANTA"" genannt: Systematic Analysis of Narrative Texts through Annotation 'widmet sich der Erstellung von Annotationsrichtlinien für das Phänomen narrativer Ebenen. Die acht Teams divergieren hinsichtlich ihrer: Der Workshop selbst war konzeptionell offen angelegt und sollte den Teilnehmer/innen wie auch uns Organisator/innen ermöglichen, den geplanten Ablauf in Reaktion auf die Arbeitsergebnisse zu verändern. Dies war realisierbar, da bis auf wenige Kurzvorträge (z.B. stellte jedes Team zu Beginn in 5 Minuten die zentralen Aspekte seiner Richtlinien vor) hauptsächlich in kooperativen Formaten wie Gruppenarbeiten, Feedback-Runden und Plenumsdiskussionen gearbeitet wurde. Am Der Diese Kriterien wurden zuerst während des Workshops im Plenum reflektiert und anschließend per online-Fragebogen live in die Evaluation überführt. Jede Dimension sollte auf nachvollziehbare Weise potentielle Guideline-Stärken vergleichbar machen und so für eine ausgewogene Beurteilung durch die Workshopteilnehmer/innen sorgen. Die drei Dimensionen sind 'um in der Tagungssprache zu bleiben 'Die erste Dimension beurteilt anhand von vier Fragen die Qualität der Guidelines hinsichtlich ihrer Die zweite Dimension evaluiert die Die dritte Dimension bewertet anhand von vier Fragen, wie der auf Basis einer Richtlinie annotierte Text das T Jede der Fragen wurde von den Teams in einer Feedback-Runde erläutert und anhand einer vierstufigen Likert-Skala online beurteilt. Die dergestalt relativ differenziert abgefragten drei Evaluationsdimensionen lassen sich 'stark abstrahiert 'auch verstehen als prozedurale Vergewisserungskriterien für gute Guidelines zur Annotation literaturwissenschaftlicher (oder allgemein: geisteswissenschaftlicher) Konzepte auf Texten unterschiedlicher Genres. Sie können prozessorientiert abgebildet werden: Am Die drei Dimensionen erwiesen sich damit als praktikables Instrument einer differenzierten Bewertung der Guidelines. Das Experiment ""Shared Task für die DH"" ist also geglückt. Die drei Bewertungsdimensionen stehen allerdings auch weiterhin für eine der großen methodischen Herausforderungen im"
2019,DHd2019,204_final-TRAUTMANN_Marjam_DER_STURM__Digitale_Quellenedition_zur_Gesc.xml,DER STURM. Digitale Quellenedition zur Geschichte der internationalen Avantgarde. Drei Forschungsansätze.,"Anne Katrin Lorenz (Akademie der Wissenschaften und der Literatur, Mainz); Lea Müller-Dannhausen (Johannes Gutenberg-Universität Mainz); Marjam Trautmann (Akademie der Wissenschaften und der Literatur | Mainz)","Briefe, Digitale Edition, Modellierung, Netzwerkanalyse, Simultaneitätsstudie","Transkription, Annotieren, Netzwerkanalyse, Daten, benannte Entitäten (named entities), Forschung","Digitale Editionen gehören zu den ""prominentesten Themen"" (Sahle 2017: 237) in den Digital Humanities. Sie leisten Grundlagenarbeit für die geisteswissenschaftliche Forschung und bilden ein eigenes Forschungsfeld in den Digital Humanities. Hier verortet sich auch das digitale Editions- und Forschungsprojekt ""DER STURM. Digitale Quellenedition zur Geschichte der internationalen Avantgarde"" (https://sturm-edition.de). Gegenstand ist das 1910 gegründete Berliner Kunstunternehmen ""Der Sturm"" um den Publizisten, Komponisten und Kritiker Herwarth Walden, der zahlreichen Künstlerinnen und Künstlern unterschiedlicher Kunstgattungen eine Plattform bot. Das Unternehmen umfasste die Zeitschrift ""Der Sturm"", die ""Sturm""-Galerie, die ""Sturm""-Bühne sowie den ""Sturm""-Verlag. Neben den Veröffentlichungen des ""Sturm"" selbst bezeugen die überlieferten Briefe an das Ehepaar Walden den internationalen Einfluss des Unternehmens. Unser Editionsprojekt, in dem bereits digital verfügbares Material aus dem ""Sturm""-Kontext transkribiert, standardkonform nach XML/TEI P5 aufbereitet und mit Normdaten versehen wird, führt diese Quellen erstmals zentral zusammen und setzt sie mittels digitaler Methoden in Relation zueinander. Integraler Teil des Editionsprojektes ist die Forschung mit den STURM-Quellen. Auf dem Poster stellen wir neben der Quellenedition die folgenden drei Forschungsansätze vor, die sich explizit mit den im Projekt edierten Materialien beschäftigen. Ein Ansatz arbeitet mit den in der digitalen Quellenedition DER STURM zusammengeführten Quellen und modelliert deren Verknüpfungen und Beziehungen untereinander. Die Grundlage bildet hierbei das CIDOC Conceptual Reference Model als Domain-Ontologie im Bereich Cultural Heritage (Doerr 2009), die im Bereich der musealen Sammlungen und der bildenden Kunst weit verbreitet ist. Ergänzt wird diese Ontologie durch fachspezifische Vokabulare wie den Getty Art & Architecture Thesaurus (AAT). Die semantischen Verknüpfungen zwischen den einzelnen Quellen und Quellen-Typen beziehen sich auf die in den bereits edierten Quellen annotierten Entitäten (Personen, Orte und Werke) sowie auf weitere Entitäten, die noch in den Editionsprozess aufgenommen werden: auf Körperschaften, Ereignisse und Themen. In der digitalen Quellenedition des STURM sind für diese Entitäten bereits URIs vorhanden bzw. vorgesehen. Durch Anreicherung der Entitäten mit Normdaten werden Verknüpfungen mit weiteren Ressourcen ermöglicht. Die semantische Modellierung der STURM-Domäne 'bestehend aus den Quellen der Edition und dem, was sie bezeugen 'dient der Weiterentwicklung der digitalen Quellenedition DER STURM sowie perspektivisch der Weiternutzung der dabei gewonnenen Daten, denn sie bildet die Grundlage für eine Verfügbarmachung der Daten und ihrer Zusammenhänge in Form von Linked Open Data. In dieser Form können die Daten beim Ermitteln und Zeigen von Zusammenhängen helfen und somit die Basis bilden für weitere Forschungen in einzelnen Fachwissenschaften, insbesondere in der Kunst- und in der Literaturwissenschaft. Ein weiterer Ansatz im Projekt beschäftigt sich mit der historischen Netzwerkforschung (Düring et al. 2016). In der bisherigen ""Sturm""-Forschung steht aufgrund der Komplexität und Dezentralität der Quellen eine dezidiert ""kunsthistorische Beschäftigung mit dem Das Netzwerk ist multimodal, bestehen die einzelnen zueinander in Relation stehenden Entitäten doch aus im Kontext des ""Sturm"" aufkommenden Personen, multimedialen Werken, Briefdaten und einigem mehr. Diese Daten gilt es in der Visualisierung des erhobenen Netzwerkes anschaulich zu machen (Baillot 2018: 357). Darüber hinaus offenbart die computergestützte Historische Netzwerkforschung auch in der Analyse komplexer Netzwerke ihre Stärken. Durch eine rein klassische Untersuchung des facettenreichen ""Sturm""-Netzwerkes, so die These, würden Darstellung und Analyse unübersichtlicher und damit fehleranfälliger werden. Die kritische Untersuchung der Quellen selbst 'und damit einhergehend die Interpretation des erhobenen Gesamtnetzwerkes ""Sturm"" 'bildet einen weiteren wichtigen Schritt in der Gesamtanalyse des Kunstunternehmens hinsichtlich eines möglichen ""gesamtgesellschaftlichen Spiegels"". Ziele der historischen Netzwerkstudie sind das Ausmachen und die Analyse von Ein Beispiel für die fachspezifische Nutzung digitaler Editionen gibt die Studie zu avantgardistischen Simultaneitätskonzepten im ""Sturm"". Als ein die Avantgarde bestimmendes Strukturprinzip kommt Simultaneität in verschiedenen Kunstrichtungen vor, die innerhalb ihrer Programmatik mitunter konkurrierende Modelle entwickeln. F. M. Marinettis ""Technisches Manifest der futuristischen Literatur"", die kubistischen ""Fenster""-Bilder Robert Delaunays oder die Simultangedichte der Dadaisten ähneln sich im grundlegenden Bestreben, nach dem avantgardistischen Primat von ""Transgression und Diffundierung"" (Asholt / Fähnders 2000: 17) beim Rezipienten gleichzeitig unterschiedliche Wirklichkeitsansichten und -ebenen zu erzeugen. Im medialen Komplex des ""Sturm"", mit seinen vielfältigen Publikations- und Distributionswegen sowie seinen dezidiert antimimetischen Gestaltungsprinzipien, finden diese bildkünstlerischen wie literarischen Werke trotz Gattungs- und Stilunterschieden gleichermaßen eine adäquate Präsentationsform. Eingebettet in den spezifischen historischen Kontext des ""Sturm""-Netzwerks werden sie von Para- und Metatexten begleitet, die ihre Rezeption lenken und kommentieren 'und die nicht zuletzt in den privaten Korrespondenzen an Walden vorbereitet und verhandelt werden. Die webbasierte Zusammenführung der verschiedenen Quellen des ""Sturm"" erlaubt es nun, wechselseitige Bezüge zwischen den unterschiedlichen Textsorten offenzulegen und so diskursiv etablierte sprachlich-rhetorische Muster zu rekonstruieren. Mit Hilfe korpuslinguistisch informierter diskursanalytischer Verfahren wird der Frage nachgegangen, inwiefern solche multimodalen Diskurspraktiken die Bedeutung von Gleichzeitigkeit im historischen ""Sturm""-Netzwerk unterschiedlich konstituieren und auf diese Zeitkonstruktion als typisch avantgardistischen Diskurs verweisen. Um die entsprechenden ""Diskursfragmente"" (Jäger 2012: 98ff.) im Netzwerkkontext situieren und den einzelnen Künstlern und Kunstrichtungen zuordnen zu können, sollen zusätzlich netzwerkanalytische Zugänge berücksichtigt werden. Schließlich zielt die Auswertung der im Projekt erarbeiteten Daten darauf ab, den Begriff der Simultaneität im Problemfeld von Scheitern (Habermas) und Überleben (Luhmann) der Avantgarde zu konturieren."
2019,DHd2019,271_final-STAECKER_Thomas__Eine_digitale_Edition_kann_man_nicht_sehen_.xml,"""Eine digitale Edition kann man nicht sehen"" - Gedanken zu Struktur und Persistenz digitaler Editionen","Thomas Staecker (ULB Darmstadt, Deutschland)","Digitale Edition, Textwissenschaft, Markup, Persistenz","Umwandlung, Strukturanalyse, Modellierung, Annotieren, Schreiben, Text","Digitale Editionen haben nach einer Phase des Ausprobierens und Entwickelns nunmehr eine Reife erreicht, dass sie in vielen Disziplinen nicht mehr als exotischer Sonder-, sondern als Regelfall angesehen werden, was sich in Publikationen wie ¬†(Apollon et. al 2014; Driscoll/Pierazzo 2016) oder Förderbedingungen (DFG 2015) spiegelt. Trotz dieser greifbaren Fortschritte stehen digitale Editionen nach wie vor in der Kritik. Genannt wird immer wieder die fehlende Stabilität und ungelöste Frage der Langzeitarchivierung und - verfügbarkeit. Doch dieses Gefühl des Mangels, so die These des Beitrags, resultiert nicht aus noch nicht geklärten methodischen oder technischen Fragen, sondern beruht auf einer Fehleinschätzung der Natur digitaler Editionen, die in Analogie zum Druckmedium meist nur von ihrer Oberfläche her beurteilt werden. Mit einem Perspektivwechsel, der die Eigentümlichkeiten digitaler Editionen und die zugrundeliegende strukturellen und algorithmischen Komponenten ernst nimmt, ist indes vergleichbare Stabilität möglich, zumindest wenn man sich über das Dokumentenmodell und über die Form seiner technischen Realisierung verständigt. Die Entwicklung und Nutzung der Markupsprache XML war von Anbeginn an begleitet von Kritik über die Unzulänglichkeit des hierarchischen OHCO Modell (DeRose et al. 1990) für die Repräsentation von Text. Trotz verschiedener Vorschläge konnte bis heute keine abschließende, alle spezifischen Kodierungsprobleme klärende Lösung gefunden werden. Nun hat das der Popularität von XML im Allgemeinen und der in diesem Feld maßgeblichen TEI im Besonderen nicht geschadet. Nach wie vor erfreut sich XML/TEI großer Beliebtheit, auch wenn in jüngerer Zeit der Unterschied von TEI und XML betont wird (Cummings 2017). Das ist umso erstaunlicher, als es an alternativen Ansätzen nicht gemangelt hat (DeRoses 2004; Speerberg-McQueen 2007). Von MECS, GODDAG, TexMECS über LMNL bis zuletzt Text as a Graph (TAGML) entstanden Markup-Konzepte, die für sich in Anspruch nehmen und nahmen, XML und seine Beschränkungen zu überwinden. Gerade mit dem neuesten Konzept des Was aber von Anfang an bei der Diskussion um Overlap und die Unzulänglichkeiten von XML vernachlässigt wurde, ist, dass die Limitationen des Textmodells nicht unbedingt mit Limitationen der Serialisierung und der digitalen Technik in eins gesetzt werden können. So ist es zwar richtig, dass SGML und in der Folge XML mit dem OHCO Modell im Kopf entwickelt wurden, doch haben bereits (Renear et al.1993) in ihrer Revision darauf abgehoben, dass im pragmatischen Sinne deskriptives Markup auch unabhängig von der OHCO These verwendet werden kann. Eben weil XML vor allem eine Syntax ist, war es letztlich immer wieder möglich, für ""konforme"" Lösungen zu sorgen, wie die Vorschläge der TEI zu nicht-hierarchischen Strukturen (TEI Guidelines: Chap. 20) verdeutlichen, aber auch die Beiträge von Renear zum Konzept des ""trojanischen Markup"" (Renear 2004) und zur XMLisierung von LMNL in CLIX (Renear 2004) oder xLMNL (Piez 2012). Der Grund lag auch darin, dass XML nicht nur in gut etablierte Strukturen der X-Familie eingebettet ist (XSD, XSLT, XQuery etc.), sondern auch allgemeiner die zentrale Document Object Modell (DOM)-Schnittstelle mit allen wichtigen Webelementen wie HTML bzw. XHTML (HTML 5), CSS oder Javascript teilt. Trotz aller Experimente ist heute in der Praxis kaum strittig, dass die TEI und das in ihr entwickelte Dokumentenmodel und mit Abstrichen auch ihre Serialisierung in XML das Mittel der Wahl für digitale Editionen ist. Allerdings bleibt das Modell unvollständig, wenn man nicht auch die anderen Komponenten der Edition in die Überlegungen einbezieht. Wie das analoge Buch über die Rolle zum Kodex und über die Handschrift zum Druck gefunden hat, so muss auch das digitale Buch zu einer stabilen Struktur und Form finden, um nicht nur in der Wissenschaft, sondern auch in Gedächtniseinrichtungen wie Bibliotheken langfristig gesichert, reproduziert und als wissenschaftlich referenzierbares Objekt über Schnittstellen zur Verfügung gestellt und genutzt werden zu können. Dabei sind die Besonderheiten digitaler Dokumente bzw. ihre spezifische Dynamik bzw. Potentialität zu beachten (PDF z.B. erfüllt diese Kriterien nicht). So ist es für das Verständnis der digitalen Edition wichtig, in die Debatte um die Zu- oder Unzulänglichkeit bestimmter Markupsprachen auch das eine Edition verwirklichende Ensemble von Dateien und Funktionen einzubeziehen, deren logisches Zusammenspiel zu bestimmen und nicht nur nach dem, wie die TEI es nennt, ""abstract model"" und dessen Serialisierungen zu unterscheiden, sondern auch Regelstrukturen, Präsentationsmodelle und differenzierte Metadatenformate als zum Verständnis notwendige Aspekte zu berücksichtigen. Editionen treten uns typischerweise als eine Kombination von Text mit Markup, Schemadatei, Stylesheets, Transformations- und sonstigen Skripten entgegen. Konkret handelt es sich um eine Reihe von Dateien (oder Datenströmen), wie z.B. .xml,.xslt,.xsd,.css oder .html, die zusammen ein funktionales Ganzes bilden, das als solches nicht nur die Stelle des physischen Dokumentes einnimmt, sondern auch die Grundlage der Langzeitarchivierung bildet. Dieser ganzheitlich betrachteten digitalen Edition ist eigentümlich, dass sie erst durch eine konkrete, meist nutzergesteuerte algorithmische Verarbeitungsanweisung nach dem klassischen EVA-Prinzip im Viewport oder Empfängersystem ""verwirklicht"" wird, während ihre Persistenz in den in die Edition hineinkodierten und in ihren Darstellungsfunktionen niedergelegten Möglichkeiten, keineswegs aber in der sichtbaren Oberfläche liegt. Letztere reduziert sich zu einem Ausschnitt, der nur bedingt das gesamte Potential der Edition aufzeigen kann. Wenn diese kombinatorisch vollständig beschreibbaren Möglichkeiten der Präsentation, die zutreffend mit dem Begriff der Schnittstelle verbunden werden (Boot/Zundert 2011; Zundert 2018), den Kernbegriff der digitalen Edition konstituieren, resultieren daraus eine Reihe von praktischen und theoretischen Konsequenzen. Ein erster wichtiger Schritt liegt in der Erkenntnis der Superiorität der Kodierungsgrundlage über die erzeugte angezeigte Oberfläche (Turska/Cummings/Rahtz 2016): ""Data is the important Long-term Outcome"". Das heißt aber nicht, dass die Oberfläche gleichgültig wäre. Sie darf nur nicht, weil sichtbar, als das einzig wichtige, ja nicht einmal als für die Edition maßgebliche Layer begriffen werden. Die Oberfläche, Visualisierung, die Ausgabe, die Schnittstelle, allgemein das algorithmische Erzeugnis, können über die primäre, immer aber reduzierte und mit Blick auf die kombinatorischen Möglichkeiten ausschnitthafte Darstellungsfunktion hinaus ihrerseits eigenständige Produkte bzw ""Interpretationen"" sein, vgl. (Zundert 2018). Die Edition selbst sind sie aber nicht, denn eine digitale Edition kann man, streng genommen, nicht sehen. Entsprechend sind zum einen eine Reihe von Text- bzw. Dokumentelemente wie das Layout als eigenständiges bedeutungstragendes oder zumindest -beinflussendes Phänomen als digitale ""Textästhethek"" und als ein Ergebnis einer Funktion mit vielfältigen Parametern neu zu interpretieren (Stäcker 2019), zum anderen verändern sich Nutzungsszenarien etwa bei der Archivierung und Zitierbarkeit von Editionen, denn wenn die Oberfläche eine von mehreren Möglichkeiten ist, kann sie nicht ohne weitere Vorkehrung Gegenstand des Zitierziels sein. ¬†Auf anderer Ebene bedeutet es, dass der/die Autor/Autorin oder, vermutlich genauer, das Autorenteam ein genaues Verständnis auch der technischen Dimension des digitalen Textes haben muss, um seinen nicht nur natürlichen, sondern auch maschinellen ""Leser"" zu erreichen, oder aber, dass die Autorintention die Schaffung von Möglichkeiten der digitalen Hermeneutik und Analyse einschließen muss. Eine wesentlich Dimension der digitalen Edition ist ferner ihre Verankerung im ""Netz"". Daraus ergeben sich generell Anforderungen an ihre ""Hypertextualität"", ihre ""Schnittstellen"" ¬†(Zundert 2018; Stäcker 2019) und Fähigkeit, sich in das ""semantic web"" zu integrieren (Ciotti/Tomasi 2016). Dazu zählt auch die unmittelbare Integration der genutzten ""Forschungsdaten"", etwa der digitalen Faksimiles, die im Rahmen der Es besteht die Hoffnung, dass mit dem Blickwechsel von dem zweidimensionalen sichtbaren Ergebnis auf die unsichtbare Potentialität der Edition sich das eher Proteushafte der Oberfläche der digitalen Edition auflöst und auch für die schon lange gärende Frage nach deren Persistenz und Nachhaltigkeit ein zufriedenstellender Ansatz gerade in ihrer, mit dem Motto der Tagung gesprochen: Multimodalität, gefunden werden kann. Der Beitrag möchte diesen Gedanken anhand von Beispielen weiter ausführen, um einen tragfähigen Begriff von einer persistenten digitale Edition als einem funktionalen und organischen Ensemble von exakt definierbaren Komponenten zu entwickeln."
2019,DHd2019,181_final-KONLE_Leonard_Makroanalytische_Untersuchung_von_Heftromanen.xml,Makroanalytische Untersuchung von Heftromanen,"Fotis Jannidis (Universität Würzburg, Deutschland); Leonard Konle (Universität Würzburg, Deutschland); Peter Leinen (Deutsche Nationalbibliothek, Frankfurt a.M.)","Gattungen, Komplexität, Schemaliteratur, Hochliteratur","Inhaltsanalyse, Strukturanalyse, Stilistische Analyse, Kollaboration, Literatur, Text","Heftromane, früher als ""Romane der Unterschicht"" (Nusser 1981) abgewertet, sind aufgrund eines weniger wertungsfreudigen Umgangs mit Populärliteratur (Hügel 2007, Kelleter 2012) in den letzten 10-15 Jahren wieder Gegenstand der Literaturforschung geworden (z.B. Nast 2017, Stockinger 2018). ""Heftromane"" wurden immer definiert durch das eigene Publikationsformat (zumeist rd. 64 Seiten), eigene Formen der Distribution über den Zeitschriftenmarkt und nicht über den Buchhandel, und auch die Soziographie der Heftromanleser weicht deutlich von der der sonstigen Literatur ab. Im Folgenden berichten wir über erste Ergebnisse einer Auswertung von 9.000 deutschsprachigen Heftromanen aus den Jahren 2009-2017. Möglich wurde die Forschung durch eine Kooperation zwischen der Würzburger Arbeitsgruppe zur literarischen Textanalyse und der Deutschen Nationalbibliothek (DNB), die die Daten vorhält. Ziel dieser ersten, noch weitgehenden explorativen Studie, sind die Antworten auf zwei Fragen: Wie unterscheiden sich die Gattungen der Heftromane untereinander und wie unterscheiden sich die Heftromane von Hochliteratur? Im ersten Schritt gehen wir der Frage nach, wie gut sich die Gattungen klassifizieren lassen und welche Texteigenschaften dabei eine Rolle spielen. Im zweiten Schritt werden die Gattungen inhaltlich erfasst. Zuletzt geht es um die angeblich einfachere Sprache der Heftromane. Die Analyse stützt sich auf die digitalen Texte, die an die Deutsche Nationalbibliothek abgeliefert wurden. Die Deutsche Nationalbibliothek (DNB) sammelt, archiviert, verzeichnet im gesetzlichen Auftrag die ab 1913 in Deutschland veröffentlichten Medienwerke sowie die im Ausland veröffentlichten deutschsprachigen Medienwerke, Übersetzungen deutschsprachiger Medienwerke in andere Sprachen und fremdsprachige Medienwerke über Deutschland und stellt diese der Öffentlichkeit zur Verfügung. Seit der Gesetzesnovelle von 2006 gehört auch das Sammeln von Medienwerken, die online publiziert werden, ausdrücklich zu den Aufgaben der DNB. Der Bestand der DNB umfasst derzeit etwa 5 Millionen digitale Objekte, darunter ca. 900.000 E-Books, ca. 1,5 Millionen E-Journal Ausgaben und ca. 2 Millionen E-Paper Ausgaben. Neben dem umfangreichen physischen Bestand steht den Nutzerinnen und Nutzern der DNB damit ein wachsender Fundus von ""born digital"" Objekten zur Verfügung. Die Anforderungen an die Informationsversorgung haben sich durch den digitalen Wandel insgesamt stark verändert. Die Einführung neuer Forschungsmethoden wie z.B. automatisierte Daten- und Textanalysen großer digitaler Bestände gehen mit der Notwendigkeit veränderter Formen der Bereitstellung von Beständen einher.  In der Vorverarbeitung wurden, soweit das aufgrund der selbst innerhalb eines Verlags sehr heterogenen Ausgangslage automatisch möglich war, Werbung, Leseproben usw. entfernt. Problematische Texte wurden nicht für die Analyse verwendet. Die unausgewogene Verteilung in Abbildung 1 kommt durch die Neupublikation älterer Hefte zustande. Die Verteilung über die Gattungen (Abbildung 2) ist sehr unausgewogen. Abb. 3 zeigt, dass manche der Gattungen von einzelnen Serien dominiert werden. Außerdem haben wir ein Vergleichskorpus ""Hochliteratur"" mit 500 Romanen von Autoren erstellt, die einen literarischen Preis gewonnen haben oder dafür vorgeschlagen wurden. Wir nehmen das Verhältnis von Types zu Tokens in einem Text als Maß für die Variabilität der Sprache und als Größe des Wortschatzes. Wie die Boxplots in Abbildung 13 zeigen, unterscheidet sich Hochliteratur (""hlit"") in diesem Punkt keineswegs grundlegend vom Heftroman. t-Test.  Auch bei der durchschnittlichen Länge der Worte, häufig verwendet für Maße der Leseschwierigkeit von Texten, ist die Varianz innerhalb der Heftromane größer der Unterschied zwischen den Heftromanen und der Hochliteratur (siehe Abbildung 14).   "
2019,DHd2019,222_final-BURGHARDT_Manuel__The_Bard_meets_the_Doctor____Computergest_.xml,"""The Bard meets the Doctor"" 'Computergestützte Identifikation intertextueller Shakespearebezüge in der Science Fiction-Serie Dr. Who","Manuel Burghardt (Universität Leipzig, Deutschland); Selina Meyer (Universität Regensburg, Deutschland); Stephanie Schmidtbauer (Universität Regensburg, Deutschland); Johannes Molz (Ludwig-Maximilians-Universität München, Deutschland)","Text Reuse, Intertextualität, Shakespeare","Entdeckung, Inhaltsanalyse, Kontextsetzung, Methoden, Text","In der jüngeren Literatur- und Kulturtheorie geht man davon aus, dass alle literarischen Texte immer auch durch eine reiche Tradition, ja ein ganzes Ökosystem, bestehender Literatur beeinflusst sind (Allen, 2000, S. 1). Dieser Einfluss, der sich im Text sowohl in impliziten als auch in expliziten Querverbindungen durch Zitate offenbart, wird gemeinhin als Intertextualität bezeichnet Wenngleich Film in erster Linie ein visuelles Medium ist, so bietet sich über die Dialoge zusätzlich ein verbaler Analysezugang (vgl. Kozloff, 2000), Klarer (1998, S. 54) spricht in dem Zusammengang gar von Film als semi-textuelles Genre. Die den Filmen zugrunde liegenden Skripte lassen sich als Dramen lesen, da sie ausschließlich aus Figurennamen, Sprechakten und Bühnenanweisungen bestehen. Für die quantitative Analyse von Intertextualitätsphänomenen bei Filmen liegt ein großer Vorteil in der Verfügbarkeit vollständig transkribierter Dialoge die als Untertitel Vor diesem Hintergrund exploriert der vorliegende Beitrag intertextuelle Bezüge auf das Werk Shakespeares in der britischen Fernsehserie Dr. Who und setzt dabei auf computergestützte Analysemethoden, die bislang vor allem im Bereich klassischer Altertumswissenschaften und historischer Sprachen eingesetzt wurden. Dr. Who eignet sich dabei in besonderer Weise, da die TV-Serie fester Bestandteil der britischen Kultur ist, was automatisch eine größere Nähe zum Werk ihres Landsmannes Shakespeare bedeutet. Andererseits gibt es einige Folgen bei Dr. Who, bei denen bereits im Titel explizite Shakespeare-Bezüge deutlich werden Im Rahmen unserer Fallstudie werden systematisch Referenzen aus zehn der bekanntesten Shakespeare-Stücke ( Für die Identifikation von textuellen Übereinstimmungen und Textähnlichkeiten ( Weiterhin wurde das Shakespeare-Korpus in jeweils überlappende 9-Gramme ( Gleichzeitig sind allerdings sehr kurze Referenzen, bspw. die Nennung einer Figur wie ""Othello"" oder ""Macbeth"", nicht ausgeschlossen, gehen aber bei diesem Ansatz verloren. Wir ergänzen den Insgesamt wurden mit der  Bei genauerer Überprüfung der Treffer wird schnell klar, dass Figurennamen aus Shakespeares Historiendramen häufig auch in der ursprünglichen Bedeutung der zugrundliegenden historischen Figur zitiert werden (bspw.  Auffällig niedrig scheint im Vergleich zu den   Gleichzeitig erhalten wir mit dem local aligment-Ansatz auch viele falsch-positive Treffer, die zwar längere, in beiden Texten vorkommende Sequenzen beschreiben, aber keine genuinen Shakespeare-Zitate sind, sondern eher hochfrequente Idiome, bspw.:  Der zweigleisige Ansatz einer"
2019,DHd2019,134_final-BRUNNER_Annelen_Das_Redewiedergabe_Korpus.xml,Das Redewiedergabe-Korpus.   Eine neue Ressource,"Annelen Brunner (Institut für Deutsche Sprache, Deutschland); Lukas Weimer (Universität Würzburg, Deutschland); Ngoc Duyen Tanja Tu (Institut für Deutsche Sprache, Deutschland); Stefan Engelberg (Institut für Deutsche Sprache, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland)","Redewiedergabe, Annotation, Linguistik, Literatur, Korpus, Historisch, Zeitschriften, Zeitungen","Annotieren, Veröffentlichung, Daten, Literatur, Forschung, Text","In diesem Beitrag Redewiedergabe ist sowohl für die Linguistik als auch die Literaturwissenschaft ein interessanter Untersuchungsgegenstand. Die Repräsentation der Figurenstimme in Erzähltexten hat in der Narratologie viel Aufmerksamkeit erfahren und wurde in zahlreichen Kategoriesystemen abgebildet (vgl. z.B. Genette 2010; Mart√≠nez / Scheffel 2016). In der Linguistik besteht ein Interesse an sprachlichen Formen der Redewiedergabe, sowie an Redeeinleitungsverben (vgl. z.B. Hauser 2008, Engelberg 2015). Detaillierte, manuell annotierte Korpora mit diesem Themenschwerpunkt sind bislang vor allem für das Deutsche sehr rar. Ein Vorbild mit detaillierter, literaturwissenschaftlich motivierter Annotation mehrere Redewiedergabetypen für das Englische ist das Korpus von Semino/Short 2004. Das ebenfalls manuell annotierte DROC-Korpus hat seinen Schwerpunkt auf Figurenreferenzen in Romanen, enthält in diesem Kontext allerdings auch Annotationen direkter Wiedergabe mit Sprecherzuordnung (Krug et al. 2018b). Unser Korpus ist eine direkte Weiterentwicklung des aus 13 Erzähltexten bestehenden Korpus aus Brunner 2015, unterscheidet sich jedoch von diesem vor allem in folgenden Aspekten: Es enthält neben fiktionalen auch nicht-fiktionale Texte, die Annotationen sind durch Mehrfachannotation wesentlich verlässlicher und es ist deutlich umfangreicher (für das Beta-Release ca. 350.000 Tokens vs. 57.000 Tokens in Brunner 2015). Das RW-Korpus umfasst Textmaterial aus dem Zeitraum 1840-1920. Es beruht auf den folgenden drei Textquellen, aus denen jeweils nur die Texte ausgewählt wurden, die in den Untersuchungszeitraum passen: Bei der Korpuszusammenstellung sollte eine möglichst große Diversität der enthaltenen Texte erzielt werden. Um dies zu erreichen, setzt sich das Korpus aus Textausschnitten (Samples) zusammen. Diese haben mindestens 500 Wörter für fiktionale Texte bzw. 200 Wörter für nicht-fiktionale Texte 'mit dieser großzügigeren Grenze war es möglich, auch kurze, abgeschlossene Artikel aufzunehmen, die für Zeitungen/Zeitschriften typisch sind. Die Samples wurden mit folgenden Besonderheiten randomisiert aus dem vorhandenen Textmaterial gezogen: Bei den Texten der Digitalen Bibliothek wurde erzwungen, dass jeder vertretene Autor innerhalb einer Dekade gleichermaßen berücksichtigt wird. Entsprechend wurde beim MKHZ erzwungen, dass alle in einer Dekade vertretenen unterschiedlichen Zeitungen/Zeitschriften gleichermaßen berücksichtigt werden. Damit wurde verhindert, dass Autoren bzw. Zeitungen/Zeitschriften mit wenig Material beim Sampling-Prozess vollkommen herausfallen. Das Beta-Release enthält Texte von etwa 140 unterscheidbaren Autoren und aus 20 unterschiedlichen Zeitungen/Zeitschriften. Die Quelltexte wurden größtenteils in ihrem Ursprungszustand belassen, mit zwei Ausnahmen: Da für die Zeitschrift ""Die Grenzboten"" nur automatische OCR-Erkennung durchgeführt wurde, wurden die Samples aus dieser Textquelle manuell nachkorrigiert. In den Texten aus den beiden anderen Quellen wurden häufige Sonderzeichen, wie das Schaft-S, durch moderne Öquivalente ersetzt, jedoch weisen die Texte dennoch in unterschiedlichem Maße altertümliche Schreibungen und z.T. auch Sonderzeichen auf. Insgesamt ist festzuhalten, dass die Textformen im RW-Korpus sehr divers sind, so sind z.B. Texte im Dialekt enthalten, sowie Zeitungsausschnitte, die reine Listen sind. Wir haben uns bewusst dagegen entschieden, solch ungewöhnliches Material herauszufiltern, um eine realistische Repräsentation des Textmaterials aus den untersuchten Dekaden zu erhalten. Beim RW-Korpus wurde eine Ausgewogenheit in der zeitlichen Dimension (Textmaterial pro Dekade) sowie zwischen fiktionalen und nicht-fiktionalen Texten angestrebt. Entgegen ursprünglicher Annahmen stellte es sich als nicht sinnvoll heraus, die Trennung fiktional - nicht-fiktional rein aufgrund der Textquelle zu treffen: Es liegt in der Natur der Textsorte Zeitung/Zeitschrift, dass dort auch fiktionale Texte abgedruckt werden (im Feuilleton, als Fortsetzungsromane u.Ö.). Somit wurde das Kriterium ""fiktional"" für jedes Sample individuell festgelegt. Unsere Definition für ""Fiktion"" ist dabei angelehnt an Gabriel 2007: ""Ein erfundener (""fingierter"") einzelner Sachverhalt oder eine Zusammenfügung solcher Sachverhalte zu einer erfundenen Geschichte"" (Gabriel 2007: 594). Bei der Identifizierung wurde besonderer Wert auf paratextuelle Merkmale (z.B. Untertitel, Rubriken u.Ö.) gelegt. Von den Samples aus dem MKHZ und den ""Grenzboten"" wurden auf diese Weise ca. 12% als fiktional eingestuft. Die folgende Tabelle zeigt die wichtigsten Metadaten des RW-Korpus, welche nach dem Sampling und der Textkorrektur vergeben werden. Aufgrund der Diversität der in Zeitungen/Zeitschriften vertretenen Texte wurde für jedes Sample eine nähere Klassifikation des Texttyps vorgenommen, so dass auch dessen Einfluss auf die Verteilung der Redewiedergabetypen untersucht werden kann. Die folgende Abbildung gibt einen ersten Eindruck, welch deutliche Abweichungen hier erkennbar sind. Gezeigt werden nur die Texttypen, für beim Korpusstand vom 25.09.2018 mehr als 10 Samples vorlagen. Die Y-Achse zeigt Prozent der Tokens im Text. Wir unterscheiden die vier Typen direkte, indirekte, frei indirekte (""erlebte"") und erzählte Wiedergabe, sowie die drei Medien Rede ( Außerdem annotieren wir die Rahmenformel, die eine direkte oder eine indirekte Wiedergabe einleiten kann. In den Rahmenformeln sowie den Instanzen von erzählter Wiedergabe wird das zentrale Wort markiert, das auf die Sprech-/Gedanken-/Schreibhandlung verweist (z.B. Während die Unterscheidung der drei Medien nur in Ausnahmefällen problematisch ist, bedürfen die vier Typen genauerer Definitionen. Die direkte Wiedergabe ( Die indirekte Wiedergabe ( Die freie indirekte Wiedergabe ( Die erzählte Wiedergabe ( Ein Sonderfall sind uneingeleitete Konjunktivsätze, die zur Wiedergabe verwendet werden. Diese werden als Mischform zwischen indirekter und frei indirekter Wiedergabe markiert. Darüber hinaus gibt es zusätzliche Attribute, die Besonderheiten bei der Wiedergabe markieren und in der folgenden Tabelle dargestellt werden: Die detaillierten Annotationsrichtlinien können unter http://redewiedergabe.de/richtlinien/richtlinien.html eingesehen werden. Die genaue Identifizierung und Klassifizierung der Redewiedergaben auf der Grundlage des detaillierten Annotationssystems ist eine schwierige Aufgabe. Jedes Sample des RW-Korpus durchläuft darum einen mehrschrittigen Prozess. Zunächst wird es von zwei Annotatoren unabhängig voneinander annotiert. Danach vergleicht ein weiterer Experte die Annotationen und erstellt, falls notwendig, eine Konsens-Annotation, die dann ins finale Korpus aufgenommen wird. Jedes Sample wird also von drei Personen bearbeitet, um größtmögliche Konsistenz zu gewährleisten. Die Annotatoren arbeiten mit dem eclipse-basierten Annotationswerkzeug ATHEN (entwickelt von Markus Krug im Projekt Kallimachos, www.kallimachos.de), für das im Projekt eine spezielle Oberfläche für die Redewiedergabe-Annotation implementiert wurde (für eine detaillierte Beschreibung vgl. auch Krug et al. 2018a). Das Werkzeug ist frei verfügbar unter der Adresse Das Beta-Release wird in einem standardisierten und dokumentierten Textformat im Langzeitarchiv des Instituts für Deutsche Sprache zur freien Nutzung zur Verfügung gestellt ( Nutzungsszenarien für das Korpus sind vielfältig: Aus NLP-Perspektive kann es als Test- und Trainingsmaterial für automatische Redewiedergabeerkenner verwendet werden. Aus linguistischer Perspektive bieten sich Korpusstudien zu sprachlichen Eigenheiten der Redewiedergabe an, wie z.B. die laufenden Studien zu Redewiedergabeeinleitern von Tu. Aus literaturwissenschaftlicher Perspektive erlaubt das Korpus z.B. Untersuchungen zu der Häufigkeit und Form von Wiedergaben in Erzähltexten in ihrer Relation zur Figurencharakterisierung."
2019,DHd2019,250_final-VAUTH_Michael_Netzwerkanalyse_narrativer_Informationsvermitt.xml,Netzwerkanalyse narrativer Informationsvermittlung in Dramen,"Michael Vauth (Technische Universität Hamburg, Deutschland)","Netzwerkanalyse, Narrativität, Informationsvermittlung, Dramenanalyse","Annotieren, Netzwerkanalyse, Visualisierung, Literatur","In diesem Beitrag wird ein Verfahren vorgestellt, das Netzwerkvisualisierungen dramatischer Texte für eine spezifische Form der kommunikativen Interaktion zwischen Figuren fokussiert. Es wird gezeigt, inwiefern gewichtete, gerichtete und dynamische Figurennetzwerke narrative Informationsvermittlung in der Figurenrede visualisieren können und auf diesem Weg dramennarratologische Analysen bzw. Annotationen ausgewertet werden. Im Gegensatz zu literaturwissenschaftlichen Netzwerkanalysen, die um die automatisierte Analyse Darüber hinaus werden mit Rückgriff auf die ermittelten Netzwerkdaten Deutungspotenziale exemplarisch an Kleists Ausgangspunkt der vorgestellten Netzwerke ist eine Typologie narrativer Figurenrede bzw. von Binnenerzählungen, die zur Annotation der Dramen Heinrich von Kleists genutzt wurden. Tabelle 1: Vorkommen narrativer Figurenrede in Kleists Dramen Diese manuellen Annotationen sind die Grundlage dafür, dass unterschiedliche Formen der Informationsvermittlung netzwerkgraphisch visualisiert werden können. Unter Rückgriff auf die Annotationen der narrativen Figurenrede und die TEI-Annotationen von Sprecherfiguren und Szenen- sowie Aktwechseln im TextGrid-Korpus wurden chronologisierte Sender-Adressaten-Kanten erstellt. Dazu wurden die vier Sprecherfiguren, die auf eine narrative Öußerung folgen oder ihr vorangehen, als Adressaten berücksichtigt, sofern keine Akt- oder Szenenwechsel zwischen narrativer Figurenrede und potenziellem Adressaten liegt und es sich um unterschiedliche Figuren handelt. Die Anzahl der erzeugten Kanten ist somit deutlich höher als die Anzahl der narrativen Öußerungen (siehe exemplarisch in Tabelle 2 die Kanten 4 und 5 sowie 8-10). Tabelle 2: Auszug aus der Kantenliste zu DFS Das Kantengewicht ( Die Figurennetzwerke, die auf dieser Grundlage erstellt werden, illustrieren, welche Figuren sich zu welchem Zeitpunkt des Dramenverlaufs narrativ äußern, welche Figuren narrative Informationen bekommen und wie häufig Figuren an narrativem Informationsaustausch beteiligt sind. Abbildung 1 zeigt dieses Potential der Netzwerkvisualisierung exemplarisch für Die Größe der Knotenbeschriftung repräsentiert hier die betweenness centrality Tabelle 3: Betweenness centrality, (weighted) in- und outdegree in DFS Schon anhand dieses Beispiels und der netzwerkmetrischen Daten in Tabelle 3 lassen sich einige Vorzüge einer netzwerkgraphischen Annotationsauswertung zeigen: Zudem können unterschiedliche Formen der narrativen Figurenrede netzwerkgraphisch miteinander verglichen werden. Die Abbildungen 3 und 4 zeigen dies exemplarisch. In Abbildung 3 werden Figurenerzählungen visualisiert, in denen sich Figuren in Übereinstimmung mit der fiktionalen Wirklichkeit äußern. Abbildung 4 zeigt narrative Öußerungen, bei denen das Gegenteil der Fall ist. Es handelt sich also um narrative Falschaussagen. Der Vergleich ist in diesem Fall aufgrund der vorhandenen Parallelen Wie Solange die automatische Annotation narrativer Figurenrede nicht möglich ist, setzt das vorgestellte Verfahren einen relativ großen Annotationsaufwand voraus. Es ermöglicht somit keinen umfassenden Vergleich von Dramen, was unter anderem zur Einordnung der vorgestellten quantitativen Netzwerkanalysen wünschenswert wäre. In diesem Beitrag wurde jedoch exemplarisch gezeigt, inwiefern netzwerkgraphische Visualisierungen für die Auswertung narratologischer Annotationen einen analytischen Mehrwert haben können. Die formalen Annotationen können und sollen durch inhaltsbezogene Annotationen angereichert werden. Auf dieser Grundlage könnte netzwerkgraphisch der Informationsaustausch über bestimmte Themen oder Figuren visualisiert werden. Tabelle 4: Figuren mit höchster betweenness centrality in Kleist"
2019,DHd2019,142_final-HORSTMANN_Jan_Texte_digital_annotieren_und_analysieren_mit_C.xml,Texte digital annotieren und analysieren mit CATMA 6.0,"Jan Horstmann (Universität Hamburg, Deutschland); Jan Christoph Meister (Universität Hamburg, Deutschland); Marco Petris (Universität Hamburg, Deutschland); Mareike Schumacher (Universität Hamburg, Deutschland)","Annotation, Analyse, Visualisierung","Strukturanalyse, Annotieren, Einführung, Visualisierung, Methoden, Text","In diesem hands-on Workshop werden wir die Möglichkeiten der für Geisteswissenschaftler*innen entwickelten Annotations- und Analyseplattform CATMA 6.0 praktisch erkunden. Es werden keinerlei technische Vorkenntnisse vorausgesetzt. Inhaltlich werden wir uns vor allem den theoretischen und praktischen Aspekten der digitalen Annotation von (literarischen) Texten, als auch der Analyse und Visualisierung dieser Texte und der erstellten Annotationen widmen. CATMA (Computer Assisted Text Markup and Analysis; CATMA unterstützt Von linguistischen Textanalysetools unterscheidet sich CATMA insbesondere durch seinen Zudem bietet CATMA auch die Möglichkeit, bereits annotierte Texte zu verarbeiten (z.B. durch den Upload von XML-Dateien) und die in anderen Tools erstellten Annotationen anzuzeigen, mit zu analysieren und damit wissenschaftlich nachzunutzen. Außerdem lassen sich in CATMA auch automatische (z.B. POS für deutschsprachige Texte) und halb-automatische Annotationen generieren. Die Annotation von Texten gehört seit Jahrhunderten zu den textwissenschaftlichen Kernpraktiken (vgl. Moulin 2010). Genauer lassen sich hier Freitextkommentare, taxonomiebasierte Annotation und Textauszeichnung unterscheiden, wobei die Übergänge häufig fließend sind (vgl. Jacke 2018, ¬ß 9). Während CATMA 6.0 auch eine Funktion für Freitextkommentare bietet, ist die taxonomiebasierte Annotation das eigentliche Kerngeschäft des Tools 'wobei die Taxonomie prinzipiell undogmatisch erstellt werden kann und die Form von sog. Tagsets annimmt, denen für kollaborative Annotationsprojekte wahlweise eine Annotations-Guideline beigegeben werden kann (vgl. auch Bögel et al). Im Workshop werden wir den Unterschied von Neben der Annotation sind die Analyse und Visualisierung der Text- und Annotationsdaten das andere wichtige Standbein von CATMA. Hier wird Neben diesen grundlegenden Funktionen, die alle per Klick ausgeführt werden können, bietet CATMA den sog. Im Analysebereich können außerdem halbautomatische Annotationen erstellt werden, d.h. man annotiert wiederkehrende Wörter oder Wortgruppen auf einmal mit einem bestimmten Tag, statt dies manuell und wiederholt im Annotationsmodul zu tun. Der Wechsel zwischen der Arbeit im Annotations- und Analyse- und Visualisierungs-Modul ist ein iterativer Prozess, der die klassisch-zirkuläre hermeneutische Interpretationsarbeit in der Literaturwissenschaft widerspiegelt (vgl. Gius, in Vorbereitung). Im Workshop werden wir uns in einer Mischung aus Präsentations- und Hands-on-Phasen der textanalytischen Arbeit in CATMA 6.0 nähern. Nach einer generellen Einführung in das Tool werden die Teilnehmer*innen anhand eines vorgegebenen Beispieltextes den gesamten Workflow von der individuellen taxonomiebasierten Textannotation, über die Analyse hin zur Visualisierung und Interpretation der Text- und Annotationsdaten kennenlernen und praktisch erproben können. Die Teilnehmer*innen sollen ausgehend vom digitalen Text in die Lage versetzt werden, Annotationen manuell und automatisch unterstützt zu erstellen und in Annotation Collections zu speichern, Tagsets/Taxonomien zu entwickeln und den Text alleine und in Kombination mit den Annotationen zu analysieren und zu visualisieren. Für Diskussionen und individuelle Rückfragen (theoretischer, praktischer und technischer Art) auf jedem Niveau und in Bezug auf die Projekte der Teilnehmer*innen wird ausreichend Möglichkeit bestehen. Im Workshop werden wir den Arbeitsablauf der digitalen Texterforschung praktisch kennenlernen: Dr. Jan Horstmann Universität Hamburg, Institut für Germanistik, Überseering 35, Postfach #15, 22297 Hamburg Jan Horstmann ist Postdoc und koordiniert das DFG-Projekt forTEXT, in dem neben der Dissemination von digitalen Routinen, Ressourcen und Tools in die klassischeren Fachwissenschaften auch die Weiterentwicklung von CATMA eine wesentliche Rolle spielt. Als Literaturwissenschaftler interessiert er sich vor allem für die neuen Perspektiven und Erkenntnispotentiale, die DH-Methoden auf literarische Artefakte bereithalten können, und forscht in diesem Sinne unter anderem zu Entsagung und Ironie bei Goethe. Prof. Dr. Jan Christoph Meister Universität Hamburg, Institut für Germanistik, Überseering 35, Postfach #15, 22297 Hamburg Jan Christoph Meister ist Professor für Digital Humanities mit dem Schwerpunkt Literaturwissenschaft. Als ursprünglicher Erfinder von CATMA hat er etliche Forschungsprojekte zur Annotation und Visualisierung textueller Daten und der Entwicklung und Verbesserung von DH-Tools geleitet. Marco Petris, Dipl. Inform. Universität Hamburg, Institut für Germanistik, Überseering 35, Postfach #15, 22297 Hamburg Marco Petris ist Informatiker mit starker Affinität zu geisteswissenschaftlichen Fragestellungen. Er ist von Anfang an an der Entwicklung von CATMA beteiligt und beschäftigt sich mit allen Aspekten der DH-Toolentwicklung, des Tool-Designs und der Implementierung. Mareike Schumacher, M.A. Universität Hamburg, Institut für Germanistik, Überseering 35, Postfach #15, 22297 Hamburg Mareike Schumacher promoviert als digitale Literaturwissenschaftlerin über Orte und narratologische Ortskategorien in literarischen Texten, beschäftigt sich besonders mit den Methoden des distant reading (u.a. Named Entity Recognition oder Stilometrie) und ist im forTEXT-Projekt u.a. für die Dissemination in den (sozialen) Medien zuständig. Bis zu 30 Personen. Teilnehmer*innen bringen ihren eigenen Laptop mit, der mit dem Internet verbunden ist (Achtung: Touch-Devices werden derzeit noch nicht unterstützt). Am Workshop können bis zu 30 Personen teilnehmen. Neben einer stabilen Internetverbindung werden ein Beamer und eine Leinwand benötigt."
2019,DHd2019,193_final-USLU_Tolga_text2ddc_meets_Literature___Ein_Verfahren_für_die.xml,text2ddc meets Literature - Ein Verfahren für die Analyse und Visualisierung thematischer Makrostrukturen,"Alexander Mehler (Goethe University of Frankfurt, Deutschland); Tolga Uslu (Goethe University of Frankfurt, Deutschland); Rüdiger Gleim (Goethe University of Frankfurt, Deutschland); Daniel Baumartz (Goethe University of Frankfurt, Deutschland)","topic, visualization, literature","Inhaltsanalyse, Visualisierung, Literatur","In diesem Poster geht es um die thematische Analyse und Visualisierung literarischer Werke mithilfe automatisierter Klassifikationsalgorithmen. Hierfür wird ein bereits entwickelter Algorithmus namens text2ddc (Uslu et. al. 2018) verwendet, um die Themenverteilungen literarischer Werke zu identifizieren. Darüber hinaus thematisiert der Beitrag, wie diese Verteilungen von Themen und deren Abhängigkeiten untereinander visualisiert werden können. Bei text2ddc handelt es sich um einen Klassifikator auf Basis neuronaler Netze, der Texte einer bestimmten Anzahl von Sprachen nach der Dewey-Dezimalklassifikation (DDC) kategorisiert. Die DDC ist ein internationaler Standard für die Themenklassifikation im Bereich von (digitalen) Bibliotheken. Um text2ddc zu trainieren, wurde die Wikipedia verwendet. Da viele Artikel der Wikipedia mit der Gemeinsamen Normdatei (GND) verlinkt sind und die GND Informationen zu den entsprechenden DDC-Kategorien hinterlegt, war es möglich, ein vergleichsweise großes und zugleich breites DDC-orientiertes Trainingskorpora für das Deutsche aufzubauen. Am Beispiel dieses Korpus erreicht unser Algorithmus einen F-Score von 87,4%. Da die Artikel der Wikipedia auch über Sprachgrenzen hinweg untereinander verlinkt sind, war es zudem möglich, text2ddc für über 40 Sprachen zu trainieren. text2ddc wurde auf Korpora verschiedener Genres angewandt, um deren Themenverteilungen zu analysieren. Zum einen betrifft dies die Wikipedia selbst, aber auch Korpora basierend auf StadtWikis, anhand derer bestimmt wurde, welche Themen dominant sind und wie diese zusammenhängen. Ein drittes Beispiel betrifft literarische Texte bzw. historische Texte der Wissenschaft. Abbildung 1 zeigt etwa die Themenverteilung von Der zugrundeliegende Algorithmus basiert auf folgendem Prozedere: Zunächst wird der Inputtext in Sektionen untergliedert, wofür die jeweilige logische Dokumentstruktur ausgewertet wird. Anschließend werden verschiedene NLP-Methoden angewendet, um Informationen über Lemmata und Das Poster untersucht anhand der Werke einer Reihe von deutschsprachigen Autoren (u.a. Karl Marx, Sigmund Freud, Franz Kafka, Friedrich Nietzsche, Thomas Mann und Martin Heidegger) die Möglichkeiten und Grenzen von Themenkarten zur Erfassung makrostruktureller Themenzusammenhänge von Texten, wie sie unser Algorithmus erfasst. Auf diese Weise soll eine Alternative zu den in den DH omnipräsenten"
2019,DHd2019,149_final-KRUG_Markus_Detecting_Character_References_in_Literary_Novel.xml,Detecting Character References in Literary Novels using a Two Stage Contextual Deep Learning approach,"Markus Krug (Chair of Applied Computer Science and Artificial Intelligence, Universität Würzburg, Deutschland); Sebastian Kempf (Chair of Applied Computer Science and Artificial Intelligence, Universität Würzburg, Deutschland); Schmidt David (Chair of Applied Computer Science and Artificial Intelligence, Universität Würzburg, Deutschland); Weimer Lukas (Chair of Literary Computing, Universität Würzburg, Deutschland); Puppe Frank (Chair of Applied Computer Science and Artificial Intelligence, Universität Würzburg, Deutschland)","Character Reference detection, Deep Learning, BiLSTM-CRF","Datenerkennung, Programmierung, Inhaltsanalyse, Daten, Metadaten","In recent years analyzing constellations of fictional entities in literary fiction has seen a lot of interest (Elson et al. 2010, Agarwal et al. 2012). Those constellations are often visualized as networks of entities apparent in the document. Even though the pipelines used for preprocessing vary drastically they all share some common steps. In order to draw a network, one needs to define nodes and edges. An obvious choice for the nodes are the fictional entities. Those entities appear either as pronouns, nominal references or names. The detection of those character references (CR) was used for a multitude of different applications in automatic digital humanities processing, most notably genre detection (Hettinger et al. 2015) and coreference resolution (e.g. Lee et al. 2013). This section only mentions the most dominant work in terms of Named Entity Recognition techniques as well as those with comparable results in terms of domain. The standard approach is to segment the input document into sentences and apply a sequence classifier on these sentences. The most robust classifier applied to this task was a Conditional Random Field (CRF) (Lafferty et al. 2001). Until the success of deep learning approaches, the classifier published by Stanford (Finkel et al. 2005) and its adaptation to German (Faruqui et al. 2010) were the dominant approaches. Lately the combination of a Bi-LSTM with word-embeddings (e.g. Mikolov et al. 2013) which automatically derived features for a CRF classifier in a deep learning approach surpassed manually created features (Huang et al. 2015). Most recently, Riedl and Padó (2018) included pretraining into the Bi-LSTM-CRF architecture and achieved state of the art results. A maximum entropy classifier with cluster features derived by word2vec and manually crafted features yielded an F1-score of 90% (Jannidis et al. 2017), serving as the only comparable work for German literary data. However, all of these approaches classify each sentence and every token inside on their own so that subsequent sentences do not benefit from previously detected names. This does not only overcomplicate the detection, it can also introduce inconsistent results (e.g. ""Effi"" is detected as a name in sentence 10 and 27, but was not detected in sentence 25). Introducing no dependencies between each individual reference seems a wasted opportunity, especially in novels, because the same character reappears multiple times. This paper experimented with network architectures to leverage this shortcoming. The idea of this approach is not new and can even be dated back to the Brills Tagger (Brill 1992) - the classification by two separate classifiers each with its individual perspective on the problem. The corpus DROC (Krug et al. 2018) provides the data used in this paper. It contains about 393.000 tokens from 90 different samples taken from German novels. Each sample comprises at least one chapter. In total, this corpus contains about 53.000 manually annotated character references. 100-dimensional Word2Vec word embeddings trained on 1700 novels of project Gutenberg The method used in this paper follows the intuition that, especially in literary fiction, entities appear many times throughout the text. Because each document introduces its own fictional world, each word (meaning the set of all appearances of a token with the same string) has a dominant meaning. However, not every instance of a word can be easily detected. While some might be surrounded by verbs of communication (""sagen"", ""antworten"", ...), others might only be surrounded by stop words, which are not beneficial for classification. Therefore, this work introduces two passes through the text. The first pass tries to assign the dominant meaning to a word and is assumed to produce a high recall but a mediocre precision. The purpose of the second pass is to disambiguate individual instances which have been classified as a character reference but could have multiple senses. Furthermore, while the first pass might detect ""Effi"" and ""Briest"" as references, there is no information about whether the string ""Effi Briest"" is a single reference or two distinct references. This is solved in the second pass, which is trained for a sequential prediction and is supposed to detect the exact span of a reference. The architectures of both neural networks can be depicted as follows: An instance fed into the first network consists of a list of tuples, each comprising the span of the token, encoded by a word embedding, as well as a left context and a right context. Our previous work determined a context of the previous two and the next two tokens (also encoded by a pre-trained word embedding) as best performing for the determination of character references. The last input vector was derived by a Bi-LSTM character encoding of the target word. The tuples were arranged in order of appearance in the original text and encoded by a Bi-LSTM, feeding an additional tupel at each time step. The Bi-LSTM subsequently generates a condensed representation of those tuples into a vector of 256 units. The intuition is that this vector contains the most informative parts of all contexts for a given target word. The network is trained using log-loss and predicts whether the target word is a reference or not. The network was trained for 15 epoches on 58 documents (longer training did not necessarily result in a better classification accuracy) and applied to a separate set spanning 14 documents. This second set is then used to train the second network, using Bi-LSTM character embeddings with a subsequent Bi-LSTM. However, the network is only applied to tokens that had been classified as a character reference in the first pass. This follows the intuition that it can now be decided if the current instance is of a different semantic category, which can be detected by analyzing its context. The input of this network is the snippet around the target word with a context size of two. The second task of ¬†this network, detecting the exact bounds of a reference, is done by predicting labels in an I-O-B setting. It is noteworthy that words that were not detected in the first pass can not be recovered. The second network is trained with 25 epochs and finally tested on 18 test documents. We compared the architecture described in Section 4 (denoted 2-stage) with the state of the art architecture (Bi-LSTM-CRF) similar to Riedl and Padó (2018). A Bi-LSTM-CRF using character embeddings in Tensorflow Table 1: Results of the two systems applied to DROC. The numbers were derived in a 5-fold scenario and are noted in %. Evaluation is done on token and on entity level using Precision, Recall and F1. Even though the 2-stage approach seems intuitive at first, it can not compete with the results obtained by the state of the art architecture. Surprisingly, the architecture failed to provide a high recall (this is already apparent after the first pass, where the recall is similar to the state of the art system, however no exact borders can be predicted). A possible explanation for this result is the high amount of about 50% of tokens with only a single appearance in the text. Since only two context tokens to the left and the right are used, the architecture has a shortcoming compared to the Bi-LSTM-CRF, which encodes the entire sentence. The architecture does especially fail to recognize references that contain the token ""von"" (such as ""Baron von Instetten)"". While being competitive in terms of the precision, further work has to be done to increase the recall for this approach. This paper presents a 2-stage contextual approach to detect character references using deep learning. The results show that while the precision yields competitive results, the recall is still much lower. Possible approaches for this shortcoming might be changing the loss function - currently a false negative and a false positive yield the same penalty - and combining both models. The state of the art model can then be used for words that only appear a single time in the text and the 2-stage approach for words appearing more than once. This could retain the high quality while still generating a consistent labeling by making use of the dependencies between individual appearances of a word."
2019,DHd2019,239_final-KRAUTTER_Benjamin_Klassifikation_von_Titelfiguren_in_deutsch.xml,"Klassifikation von Titelfiguren in deutschsprachigen Dramen und Evaluation am Beispiel von Lessings ""Emilia Galotti""","Benjamin Krautter (Universität Stuttgart, Deutschland); Janis Pagel (Universität Stuttgart, Deutschland)","Dramen, Klassifikation, maschinelles Lernen, Titelfiguren, Literatur","Inhaltsanalyse, Strukturanalyse, Netzwerkanalyse, Literatur","In seiner Studie zu Gotthold Ephraim Lessings bürgerlichem Trauerspiel Zur Einteilung und Abstufung des Dramenpersonals mithilfe quantitativ erfassbarer Kriterien führt Manfred Pfister in den späten 1970er Jahren die Terminologie der ""quantitative[n] Dominanzrelationen"" (Pfister¬†2001¬†[1977]:¬†226) ein. Er benennt hierfür zwei Kriterien, die Haupt- von Nebenfiguren unterscheiden sollen: die ""Dauer der Bühnenpräsenz einer Figur"" (ebd.) und den Anteil der Figurenrede am Haupttext. Laut Pfister fehle es allerdings an einer differenzierten Handlungsgrammatik, die auch funktionale Relationen 'etwa die aktiven Handlungsschritte von Figuren 'operationalisieren könne (vgl. ebd.:¬†227). Er wirbt letztlich für einen nicht weiter explizierten multidimensionalen Ansatz, der die quantitative Einteilung des Personals zuverlässiger und feingliedriger machen soll. An diese Idee der quantitativen und zugleich multidimensionalen Einteilung dramatischer Figuren versuchen wir im Folgenden mittels digitaler Analysetechniken anzuschließen (vgl.¬†Fischer¬†u.a.¬†2018). Dazu fassen wir das Problem der Figureneinteilung als Klassifikationsaufgabe. Zielsetzung ist es, titelgebende Dramenfiguren mit maschinellen Lernverfahren automatisch auszuzeichnen. Dadurch sind wir in der Lage, die genauen Einflussfaktoren zu prüfen und die Ergebnisse transparent zu evaluieren. Nach unserem Dafürhalten sind es zumindest drei Gründe, die titelgebende Dramenfiguren zu einer geeigneten Zielkategorie der Klassifikation machen. Den möglichen alternativen Konzepten mangelt es erstens an einer konsensfähigen Definition und Differenzierung. Das gilt insbesondere für die Begriffe ""HeldIn"" und ""ProtagonistIn"", die teilweise synonym verwendet werden (vgl. etwa Plett¬†2002:¬†21f., Jannidis¬†2004:¬†90,¬†104f.). Überdies ist gerade das Heldenkonzept stark von literaturgeschichtlichen Entwicklungen geprägt und somit historisch variabel (vgl. etwa Alt¬†1994:¬†167f., Platz-Waury¬†2007¬†[1997]:¬†591, Martus¬†2011:¬†15). Die intersubjektive Annotation der Figurenkategorien bereitet zweitens Schwierigkeiten, vor allem dann, will man ProtagonistInnen oder HeldInnen mit Blick auf ihre Bedeutung für die Handlung bzw. den zentralen Konflikt des Dramas bestimmen. In der aktuellen Forschung folgen mehrere Studien der von Wladimir Propp (1986¬†[1928]) am russischen Volksmärchen eingeführten Figurentypologisierung, um literarische Figuren auf formaler oder automatischer Basis (sub-)klassifizieren zu können (etwa Declerck¬†/¬†Koleva¬†/¬†Krieger¬†2012 oder Finlayson¬†2017). Moretti (2011 und 2013) nutzt indessen Netzwerkdarstellungen von Shakespeares Das untersuchte Korpus umfasst 38 Dramen mit mindestens einer Titelfigur, deren Veröffentlichung sich von der Mitte des 18. bis ins frühe 20. Jahrhundert erstreckt.  Um dem Vorsatz eines multidimensionalen Modells gerecht zu werden, kombinieren wir als Features zählbasierte Metriken ( Die Anhand Lessings Gleich vier Titelfiguren benennt die Klassifikation für Lessings bürgerliches Trauerspiel Warum also wird Emilia trotzdem als Titelfigur erkannt? Die Featureanalyse in Für den Leser ist es dagegen wohl eher Emilias durchgängige passive Präsenz in den Dialogen und Monologen anderer Figuren, die sie als Titelfigur kennzeichnet. Exemplarisch dafür steht bereits der erste Akt. Ausgelöst durch eine Bittschrift verliert sich der Prinz in unruhigen Gedanken an Emilia Galotti: ""Ich kann doch nicht mehr arbeiten. 'Ich war so ruhig, bild"" ich mir ein, so ruhig 'Auf einmal muß eine arme Bruneschi, Emilia heißen: 'weg ist meine Ruhe, und alles! –"" (Lessing 2000 [1772]:¬†293). Mit wechselnden Gesprächspartnern 'Maler Conti, Marinelli und Camillo Rota 'wird Emilia immer wieder zum Mittelpunkt der folgenden Dialoge. Das gilt insbesondere für die sechste Szene, als Marinelli die anstehende Vermählung Emilias mit dem Grafen Appiani preisgibt, woraufhin der eifersüchtige Prinz seinem Kammerherren Marinelli die völlige Handlungsfreiheit in dieser Angelegenheit zugesteht (vgl. ebd.,¬†300–305). Diese passive Präsenz Emilias lässt sich über weite Teile des Dramas nachvollziehen. In 16 Szenen wird in der Figurenrede mit ihrem Namen auf sie referiert, obwohl sie selbst zu diesem Zeitpunkt nicht aktiv am Bühnengeschehen beteiligt ist. Verglichen mit anderen Dramenfiguren ist dieser Wert sehr hoch. Marinellis Name wird beispielsweise nur in fünf Szenen aufgerufen, auf Emilias Mutter Claudia wird ohne ihre Anwesenheit auf der Bühne gar nicht namentlich referiert, wie Wir konnten zeigen, dass unser multidimensionales Modell sinnvolle Ergebnisse für die Klassifikation titelgebender Figuren liefert (MCC¬†0.66). Titelfiguren werden sehr zuverlässig erkannt ( Arnim, L. A. von: Marino Caboga Brentano, C.: Ponce de Leon Büchner, G.: Dantons Tod Büchner, G.: Leonce und Lena Büchner, G.: Woyzeck Goethe, J. W.: Götz von Berlichingen mit der eisernen Hand Goethe, J. W.: Iphigenie auf Tauris Goethe, J. W.: Torquato Tasso Gottsched, J. Ch.: Der sterbende Cato Grabbe, Ch. D.: Don Juan und Faust Grabbe, Ch. D.: Hannibal Grabbe, Ch. D.: Herzog Theodor von Gothland Grabbe, Ch. D.: Napoleon oder Die hundert Tage Gutzkow, K.: Richard Savage, Sohn einer Mutter Gutzkow, K.: Uriel Acosta Hofmannsthal, H. von: Elektra Hofmannsthal, H. von: Ödipus und die Sphinx Kotzebue, A. von: Die beiden Klingsberg Laube, H.: Monaldeschi Laube, H.: Struensee Lessing, G. E.: Emilia Galotti Lessing, G. E.: Miss Sara Sampson Pfeil, J. G. B.: Lucie Woodvil Romantik Iffland, A. W.: Figaro in Deutschland Schiller, F.: Die Jungfrau von Orléans Schiller, F.: Die Piccolomini Schiller, F.: Die Verschwörung des Fiesko zu Genua Schiller, F.: Maria Stuart Schiller, F.: Wallensteins Tod Schiller, F.: Wilhelm Tell Schlaf, J.: Meister Oelze Schlegel, A. W.: Alarkos Schlegel, A. W.: Ion Schlegel, J. E.: Canut Schnitzler, A.: Anatol Schnitzler, A.: Professor Bernhardi Tieck, L.: Der gestiefelte Kater Tieck, L.: Prinz Zerbino Tieck, L.: Ritter Blaubart Uhland, L.: Ludwig der Bayer Wieland, Ch. M.: Klementina von Porretta Wieland, Ch. M.: Lady Johanna Gray"
2019,DHd2019,139_final-TU_Ngoc_Duyen_Tanja_Automatic_recognition_of_direct_speech_w.xml,Automatic recognition of direct speech without quotation marks. A rule-based approach,"Ngoc Duyen Tanja Tu (Institut für Deutsche Sprache, Deutschland); Markus Krug (Universität Würzburg, Deutschland); Annelen Brunner (Institut für Deutsche Sprache, Deutschland)","direkte Rede, regelbasiertes Verfahren, automatische Erkennung","Datenerkennung, Programmierung, Annotieren","Many texts incorporate multiple voices: the voice of the narrator and those of the characters or people who are quoted by the narrator. Separating these quotations from the surrounding text is relevant for many applications: In the field of literary studies it is a requirement for studies concerning character representation like sentiment analysis (e.g. Blessing et al. 2016; Schmidt / Burghardt / Dennerlein 2018) or character networks (e.g. Rydberg-Cox 2011; Dimpel 2018). For non-fictional texts, recognizing quotations is relevant for question answering and similar tasks. As those applications rely on processing a lot of textual material, having a way to automatically detect instances of direct speech (DS) is crucial. As long as a specific pattern of quotation marks is used consistently this is a trivial task. Unfortunately, this is not necessarily the case: First, there is an astounding number of ways to encode quotation marks and incorrect or inconsistent usage is very common. Mistakes like missing closing quotation marks happen easily and can throw off a parser relying solely on those markers. (Brunner 2015: 180-182) The problem is worse for older texts, where the typographic rules are even less standardized and additional errors can occur in the digitization process. Finally, in literature it is not uncommon that authors deliberately choose not to use any markers for stylistic reasons. Those types of texts are especially common in the Digital Humanities and it is useful to have a tool that is not dependent on the use of quotation marks. In addition to that, the tool presented in this talk is rule-based and thus requires no training material. The detection of DS is usually a pre-processing step for another task so it rarely gets much focus in the respective papers. There are many applications for English that could be cited here, but we will deliberately focus on applications for German. Pouliquen / Steinberger / Best 2007 develop a tool for automatic quotation detection and speaker attribution in newspaper articles for several languages. They look for the proper name of a public character, followed by a verb associated with speech representation, followed by quotation marks. Due to this strict pattern, their recall is relatively low (76%) but they identified 81.7% correct quotations. The tool GutenTag, developed by Brooke et al. 2015, implements several NLP techniques to use on the texts of Gutenberg corpus. The GutenTag DS recognition relies solely on quotation marks. Before processing the text, the tool checks which types of quotation marks (single or double quotes) are used in it. In her study about the automatic recognition of speech representation in German literary texts, Brunner 2015 implements two strategies for DS detection: Her rule-based approach uses quotation marks as well as pattern matching to identify frames (proper name - speech verb - colon/comma). This leads to some success in texts with unmarked DS. On a corpus of 13 German narrative texts an f-score of 0.84 for the category Jannidis et al. 2018 implemented a recognizer for DS based on deep learning, specifically trained to work without quotation marks. It is trained on a corpus of 300 German fictional texts in which the quotation marks were removed. This recognizer achieves an accuracy of 0.84 in sentence-wise evaluation and 0.90 in token-wise evaluation on the corpus that is called ""Gutenberg"" in our evaluation. To our knowledge, there is at the moment no recognizer for DS in German texts that is rule-based and does not use quotation marks. We evaluated our algorithm on four different and distinct data sets. 1) Gutenberg 2) DROC_red 3) RW 3a) RW_fict: 222 fictional text samples 3b) RW_nonfict: 206 non-fictional text samples The algorithm tries to detect whether a given token or a given sentence is part of a DS. Currently, it cannot reliably determine the exact borders of individual DS instances. The technique is purely rule-based and does not rely on machine learning or training data of any sort. The following pre-processing steps were performed: a) tokenization with the OpenNLP Tokenizer The algorithm tries to solve the problem in the following steps:  If meaningful paragraphs are present in the text, those paragraphs usually tend to represent either narrative sections or dialogue sections. As it is unlikely that a narrative section contains DS at all, this knowledge can be used to adapt the recognizer rules. However if no such paragraphs are available, the algorithm starts by reconstructing surrogate sections, using the concept of ""coherence"" between narrative perspective and tempus. It is determined whether the sentence contains first/second or third person pronouns and which tense is used. If consecutive sentences agree on both, they belong to the same segment. If the narrative perspective is in third person and the dominant tense is past, the segment is categorized as ""narrative"". Inside those sections a penalty is introduced, so that more than a single weak indicator has to be found to assume DS. The resulting sentences are the final result for an evaluation based on sentence borders.  For evaluation, two performance metrics are applied: 1) Sentence level accuracy ‚Üí a true positive is achieved by correctly predicting whether DS is contained in the sentence 2) Token level accuracy ‚Üí each token is evaluated individually. The results on the different corpora are depicted in the table below. We report micro accuracy values to not favor documents which are much shorter than others. Both accuracy metrics show very similar results on three corpora. An interesting fact is that the corpus which was used to create the rules (DROC_red) shows the worst results of all three fictional datasets. Gutenberg contains rather schematic narratives which appear to be easier compared to the other data sets. A more fine grained analysis shows that the best document for DROC_red yields a token level accuracy of 99.7% and the worst document an accuracy of 21.5%. The largest gap can be found in the RW_fict dataset with 0% accuracy for the worst and 100% accuracy for the best document. This shows that while in average the results are promising, there are still phenomena that need to be addressed separately. For the only non-fictional corpus, RW_nonfict, the scores drop by a large margin. This is because the algorithm finds anchors in sections without DS and propagates those incorrectly to the surrounding section. Those incorrect detected anchors are sentence written in first person or rhetorical questions, which are mistaken as DS. This resulted in about 1800 false positives while only 89 sentences of DS were not detected. We proposed a rule-based approach to detect DS without the help of any quotation markers. Our approach creates coherent sections which segment the documents. Specialized rules detect DS on the sentence level inside those segments. The annotation is then expanded from those anchor sentences. Post-processing removes frame sub-sentences to get an exact span for the utterance. Our evaluation shows that the results appear stable throughout different datasets in the fictional domain and are comparable to the results achieved in related work. The tool even achieves a higher score compared to Jannidis et al. 2018 on the sentence level. The current algorithm still has issues with non-fictional texts and some types of fictional texts (especially romantic letters and reports written in first person singular) which suggests that it should be extended to detect the type of document in advance in order to classify in a more robust approach across different domains."
2019,DHd2019,150_final-FISCHER_Frank_Programmable_Corpora___Die_digitale_Literaturw.xml,Programmable Corpora 'Die digitale Literaturwissenschaft zwischen Forschung und Infrastruktur am Beispiel von DraCor,"Frank Fischer (Higher School of Economics, Moskau, Russland); Ingo Börner (Universität Wien); Mathias Göbel (WU Wien); Angelika Hechtl (Niedersächsische Staats- und Universitätsbibliothek Göttingen); Christopher Kittel (Karl-Franzens-Universität Graz); Carsten Milling (Berlin); Peer Trilcke (Universität Potsdam)","Literaturwissenschaft, Infrastruktur, Dramenforschung","Infrastruktur, Literatur","Obwohl sich infrastrukturell einiges getan hat, sieht ein typischer Operationsmodus der digitalen Literaturwissenschaft immer noch so aus, dass eine bestimmte Forschungsmethode auf ein oft nur ephemeres Korpus angewandt wird. Im besten Fall ist das Ergebnis Doch seit kurzem gibt es Anzeichen, dass sich dies ändert. Einige Digital-Humanities-Projekte stellen Schnittstellen zu stabilen Korpora zur Verfügung, über die man mannigfaltige Zugriffsmöglichkeiten bekommt und reproduzierbar arbeiten kann. Eines dieser Projekte ist DraCor, eine offene Plattform zur Dramenforschung, die in diesem Vortrag vorgestellt werden soll (zugänglich unter Öhnlich wie die COST Action zu europäischen Romanen (Schöch et al. 2018), versucht das DraCor-Projekt als Basis für eine digitale Komparatistik einen Stamm an multilingualen Dramenkorpora aufzubauen, die in basalem TEI kodiert sind. Ein selbst betriebenes russischsprachiges  ( Die Vorteile von frei auf GitHub gehosteten Korpora liegen auf der Hand. Unabhängig von den letztlich durch die Plattform zur Verfügung gestellten Schnittstellen können die Korpora alternativ durch Klonen oder andere Downloadmethoden, etwa über den SVN-Wrapper von GitHub, direkt bezogen und individuell weiterverarbeitet werden. Ein offen zugängliches GitHub-Repositorium heißt auch, dass Pull Requests zur Fehlerkorrektur und Forks für Erweiterungen möglich und erwünscht sind. DraCor als Plattform setzt auf die eXist-Datenbank, um die TEI-Dateien zu verarbeiten und Funktionen zur Beforschung der Korpora zur Verfügung zu stellen. Das Frontend wurde mit ReactJS gebaut, ist responsiv und einfach erweiterbar. Der Schwerpunkt liegt aber nicht auf der GUI, sondern auf der API (vgl. generell zur Unterscheidung zwischen beiden Schnittstellenansätzen Bleier/Klug 2018). Um dem Ideal und der Möglichkeit nahe zu kommen, auf einfache Weise ""alle Methoden auf alle Texte"" anwenden zu können (Frank/Ivanovic 2018), braucht es mehr als offene Korpora. Der zitierte Text von Frank/Ivanovic macht sich hinsichtlich dessen für SPARQL-Endpunkte stark; auch DraCor bietet einen solchen an, besitzt darüber hinaus aber eine reiche API, die über Swagger dokumentiert und erläutert wird  ( Ein einfaches Use-Case-Szenario sieht dann so aus, dass man etwa im RStudio mit zwei, drei Zeilen Code einen Blick in ein Korpus werfen kann, etwa über die zeitliche Entwicklung der Anzahl der Charaktere im russischen Drama zwischen 1740 und 1940, die in der Metadatentabelle festgehalten sind  (  Die Möglichkeiten beschränken sich aber nicht darauf, vorgefertigte API-Funktionen zu benutzen. Neue Forschungsideen zeitigen immer auch neue Bedarfe an einfach bezieh- und reproduzierbaren Daten und Metriken; die API kann dementsprechend erweitert werden. Dies wird dadurch erleichtert, dass über Apache Ant die gesamte Entwicklungsumgebung auf dem eigenen System nachgebaut werden kann. Durch bereits implementierte Funktionen können neben Struktur- und Metadaten etwa auch Volltexte ohne Markup bezogen werden (auch Untermengen von Volltexten wie Regieanweisungen), etwa wenn Methoden wie die Stilometrie oder das Topic Modeling der Endzweck sind, also Methoden, die nach dem ""bag of words""-Prinzip arbeiten, für das kein Markup vonnöten ist. Insgesamt wird durch den Aufbau und die Dokumentation offener APIs die bisher oft aufwendige Reproduzierbarkeit von Forschungsergebnissen erheblich erleichtert. Ein Beispiel für die vielseitigen Nutzungsmöglichkeiten der DraCor-API ist die Shiny App, die Ivan Pozdniakov aufgesetzt hat  ( Das Markup oder andere Formalisierungen literarischer Texte sind nicht selbsterklärend. Zwar gibt es einige Standards, aber die jeweilige Operationalisierungslösung hängt von der Forschungsfrage ab. Allein das Extrahieren von Figurennetzwerkdaten ist auf viele Arten und Weisen möglich, was dazu führt, dass etwa alle von verschiedenen Forschungsgruppen extrahierten Netzwerke aus Shakespeares ""Hamlet"" zu leicht verschiedenen Ergebnissen kommen. Selbst für Dramen ist dies also schon ein nicht-trivialer Akt, von Romanen dann ganz zu schweigen (beispielhaft seien Grayson et al. 2016 genannt, die verschiedene Extraktionsmethoden für Romane durchtesten und die Ergebnisse vergleichen). Um diese Erkenntnis schon in der Lehre zu fördern, wurde das Tool ""Easy Linavis""  ( Neben einem Ansatz zur Gamifizierung des TEI-Korrekturvorgangs (Göbel/Meiners 2016) haben wir für Lehrzwecke auch ein Dramenquartett entwickelt, um spielerisch das Verständnis von Netzwerkwerten zu trainieren (Fischer at al. 2018). Die aufgezählten, um die Plattform herumgruppierten didaktischen Mittel sind integraler Bestandteil des ganzen Projekts, da sie auf dessen Daten und Operationalisierungen aufsetzen. Wichtig dabei war die Erkenntnis, dass Daten mehrere Gestalten annehmen und für Forschung und Lehre gleichermaßen von Bedeutung sein können. Im TEI-Code sind PND- bzw. Wikidata-Identifier sowohl für Autor*innen als auch für die Werke hinterlegt. Auf diese Weise lassen sich verschiedene Realien, die außerhalb der eigenen Korpusarbeit liegen, hinzufügen. Eine automatisch erstellte Autor*innengalerie hat dabei noch eher illustrativen Charakter (de la Iglesia/Fischer 2016). Darüber hinaus kann man aber zum Beispiel feststellen, ob es nicht einen unbewussten regionalen Bias im Korpus gibt. Dafür lässt man sich über die Wikidata-Identifier die Verteilung der Geburts- und Sterbeorte der Autor*innen auf einer Karte anzeigen. So konnte dann für das deutschsprachige Korpus GerDraCor ausgeschlossen werden, dass es einen solchen Bias gibt, da sich die Orte relativ gleichmäßig über die (historisch) deutschsprachigen Gebiete verteilen (Göbel/Fischer 2015). Ebenso lässt sich über die Wikidata-ID der Stücke herausfinden, wo diese uraufgeführt worden sind (Beispiel-Query: Projekte wie DraCor versuchen nichts anderes als den digitalen Literaturwissenschaften eine verlässliche und ausbaufähige Infrastruktur zu geben, damit sie sich stärker auf eigentliche Forschungsfragen konzentrieren und reproduzierbare Ergebnisse hervorbringen können. Eine wichtige Folgerung für uns war, dass wir die Weiterentwicklung unserer seit vier Jahren entwickelten all-in-one Python-Skriptsammlung In Anlehnung an das Projekt ""ProgrammableWeb"" 'das eine Datenbank von offenen APIs unterhält und dessen Slogan lautet: ""APIs, Mashups and the Web as Platform"" (zugänglich unter Programmable Corpora erleichtern es, Forschungsfragen auf viele Arten und Weisen um Korpora herum programmieren zu können. Es steht zu erwarten, dass sich infrastrukturelle Anstrengungen dieser Art für die gesamte Community auszahlen mit Effekten, wie sie John Womersley in seiner Präsentation auf der ICRI2018 in Wien aufgezählt hat: a) dramatically increase scientific reach; b) address research questions of long duration requiring pooled effort; c) promote collaboration, interdisciplinarity, interaction. Der Anschlussmöglichkeiten sind viele, egal ob man gar nicht programmieren möchte, sondern nur eine GEXF-Datei für Gephi benötigt, ob ein Korpus über seine Verbindungen zur Linked Open Data Cloud beforscht oder einfach aus R oder Python heraus bestimmte Daten bezogen werden sollen, ohne dass man sich mit dem Korpus und dessen Maintenance und Reproduzierbarkeit selbst kümmern muss (all dies bleibt natürlich aber eine Option). Programmable Corpora erleichtern die Entscheidung, auf welcher Ebene der eigene Forschungsprozess einsetzt."
2019,DHd2019,212_final-DUNST_Alexander_Multimodale_Stilometrie__Herausforderungen_u.xml,Multimodale Stilometrie: Herausforderungen und Potenzial kombinatorischer Bild- und Textanalysen am Beispiel Comics,"Alexander Dunst (Universität Paderborn, Deutschland); Rita Hartel (Universität Paderborn, Deutschland)","Multimodal, Comics, Stilometrie","Annotieren, Stilistische Analyse, Bilder, Literatur, Multimedia, Multimodale Kommunikation","Stilometrische Untersuchungen blicken auf eine lange Tradition in der Literaturwissenschaft zurück (Holmes). Im Gegensatz dazu befinden sich quantitative Untersuchungen des Stils visueller Kunst und multimodaler Medien in einem frühen Experimentierstadium, das von explorativen Untersuchungen, Methodenentwicklung und -Adaption geprägt ist. Auch hier sind Fortschritte erkennbar, etwa in der digitalen Kunstgeschichte, der Filmwissenschaft und in der Comicforschung (Manovich, Douglas & Zepel; Qui, Taeb & Hughes; Baxter, Khitrova & Tsivian; Cutting et al.; Dunst & Hartel 2018a). Dabei konzentriert sich die stilistische Klassifikation bisher entweder auf visuelle oder sprachliche Kanäle. Beispielhaft zu nennen sind die Analyse der historischen Entwicklung von Filmfarben bei Barbara Flückiger oder Ben Schmidts thematische Untersuchungen populärer TV-Serien (Flückiger; Schmidt). Während dieser monomodale Fokus bei visueller Kunst der oftmaligen Dominanz der Bildebene geschuldet ist, so sind dafür bei multimodalen Medien wie Film, Fernsehen, Computerspielen oder Comics andere Gründe ausschlaggebend. Mit dem Topic Modeling oder der Textstilometrie stehen Methoden zur Verfügung, die auf sprachlichen Daten basieren und in der digitalen Literaturwissenschaft laufend weiterentwickelt werden. Visuelle Stilometrie, obwohl weit weniger ausgereift, kann auf die erwähnten Arbeiten aus der Kunstgeschichte und empirischen Filmwissenschaft zurückgreifen. Auch technische Hürden tragen zu monomodalen Analysen bei: je nach Medium ist die digitale Erschließung und Analyse von Informationskanälen mit erheblichen Schwierigkeiten verbunden, etwa die Spracherkennung von Filmdialogen, die automatische Erkennung von handschriftlichen Texten in Comics oder die computergestützte Verarbeitung großer Mengen an Bildmaterial. Die Kombination unterschiedlicher Informationskanäle in visuellen Medien führt jedoch unweigerlich zur Frage, inwieweit Stil durch die Analyse einzelner Modi erfasst werden kann. Auch thematische Untersuchungen setzen sich dem Vorwurf aus, komplexe Medien auf zu schmaler Datenbasis zu interpretieren, wenn Filmanalysen alleine auf Untertiteln oder Drehbüchern basieren. Im Gegenzug verbindet sich mit der Einbeziehung mehrerer Informationskanäle in stilometretische Untersuchungen die Hoffnung, multimodale Medien vollständiger beschreiben und einzelne Autoren, Genres oder Epochen genauer voneinander unterscheiden zu können. Im Folgenden werden erste Untersuchungen vorgestellt, die auf Basis eines Corpus an englischsprachigen Comicbüchern 'so genannten ""Graphic Novels"" 'visuelle und Textstilometrie kombinieren (Dunst, Hartel & Laubrock). Wie bereits dokumentiert, haben wir auf Basis kunsthistorischer und filmwissenschaftlicher Vorarbeiten eine visuelle Stilometrie für Comicbücher entwickelt, die zwischen einzelnen Genres, Autoren und Publikationsformen unterscheiden und diese auf repräsentativer Datenbasis stilistisch beschreiben kann (Dunst & Hartel 2018b). Die relativ geringe Datenbasis, die mit der Analyse eines kulturellen Nischenproduktes einhergeht, bedeutete jedoch, dass nicht in allen Fällen signifikante Ergebnisse erzielt werden konnten. Insbesondere die Entwicklung einzelner Gattungen innerhalb eines Mediums lässt sich im historischen Verlauf nicht signifikant belegen. Auch nationale Traditionen konnten nicht immer stilistisch voneinander unterschieden werden, selbst wo sich diese für den Betrachter deutlich voneinander unterscheiden. Abbildung 1 zeigt, dass die von uns verwendeten visuellen Maße den japanischen Manga-Autor Osamu Tezuka stilistisch nicht von anglo-amerikanischen Werken abgrenzen konnten (Dunst & Hartel 2018a). In beiden Fällen 'also sowohl bei Autor- als auch bei Genreunterscheidungen 'liegt es nahe, dass die kombinatorische Analyse von visueller und Textstilistik die Wahrscheinlichkeit erfolgreicher Unterscheidungen erhöhen würde. Allerdings stellen diese zusätzlichen Dimensionen eine multimodale Stilometrie vor methodische Herausforderungen. Wird eine große Zahl an Maßen für die Klassifikation herangezogen, so erschwert dies die qualitative Interpretation der Ergebnisse. Zwar lässt sich mit Hilfe einer Principal Component Analysis (PCA) darstellen, ob sich Genres oder Autoren signifikant unterscheiden. Je mehr Dimensionen in die PCA einfließen, desto schwerer fällt allerdings die Aussage, auf welchen Maßen diese Unterschiede fußen. Hinzu kommen wie erwähnt technische Hürden: abgesehen von den Angeboten bekannter Softwaregiganten führen automatische Spracherkennungssysteme noch zu relativ schlechten Resultaten. Öhnlich liegt der Fall bei Comics, deren Texte auf Handschriften basieren. Erst seit kurzem können diese Texte mit Hilfe automatischer Erkennungssysteme, die auf ""Deep Learning"" basieren, zugänglich gemacht werden. Dennoch liegen Fehlerraten weit über jenen, die die Basis der meisten literaturwissenschaftlichen Analysen bilden. Der nächste Abschnitt stellt eine Methode vor, die dennoch die Verwendung einfacher Textmaße für die multimodale Stilometrie ermöglicht. Die Analysen basieren auf dem ersten repräsentativen Corpus englischsprachiger Comicbücher, von uns ""Graphic Narrative Corpus"" (GNC) genannt (Dunst, Hartel & Laubrock). Wie in früheren Arbeiten beschrieben (Hartel & Dunst), nutzen wir die Bag Error Rate (BER) für eine Abschätzung, ob die Qualität der erkannten Texte ausreichend gut ist, um diese für die Textanalysen heranzuziehen. Hierzu haben wir als Gold Standard rund 10% der Seiten einer Graphic Novel manuell annotiert. Wir betrachten in unseren Analysen die Multi-Menge (oft als ""Bag"" bezeichnet) aller Wörter, also die ungeordnete Menge aller Wörter, wobei Wörter 'im Gegensatz zur herkömmlichen Menge 'in dieser Multimenge auch mehrfach vorkommen. Wir berechnen also für jedes in einem der beiden Texte enthaltenen Wörter die Differenzen der Häufigkeiten freq BER := (Œ£ Frühere Analysen haben gezeigt, dass wir den Text als geeignet für die Analyse erachten können, wenn für eine Graphic Novel die BER kleiner als 40 ist. Auf den erkannten Texten betrachten wir die Textähnlichkeit basierend auf einer euklidischen Vektordistanz der Dokument-Vektoren der Term-Dokument-Matrix, die jeweils die für jedes Dokument D die relative Vorkommenshäufigkeit tf(D,t) der 2000 häufigsten Wörter t enthält. Bzgl. der visuellen Maße betrachten wir die mittlere Helligkeit jeder Seite, die Entropie und die Anzahl der Flächen als Maß für die visuelle Unruhe eines Bildes, sowie den Color Layout Descriptor und den Edge Histogram Descriptor des Standards MPEG7 (Mart√≠nez, Koenen & Pereira). Diese haben sich in früheren Arbeiten als vielversprechende Maße herausgestellt (Dunst & Hartel 2018a). Wann immer eine Dimensionsreduktion notwendig ist, führen wir diese mithilfe einer PCA durch. Um z.B. die textuellen und visuellen Maße kombiniert zu betrachten, haben wir 'um einem Ungleichgewicht der 2000 Dimensionen für die 2000 häufigsten Wörter im Vergleich zu den ca. 40 visuellen Maßen entgegenzuwirken - zunächst via PCA die textuellen Dimensionen auf 40 reduziert. Anschließend haben wir die Dimensionen der textuellen PCA und die Dimensionen der visuellen Maße mit Hilfe einer weiteren PCA kombiniert. Zur Untersuchung signifikanter Zusammenhänge nutzen wir die ANOVA (ANalysis Of VAriance), die untersucht, ob die Varianz zwischen den Kategorien größer ist als die Varianz innerhalb der Kategorien. Abbildung 2 stellt die Ergebnisse der multimodalen Stilometrie im Vergleich mit rein visuellen oder Textmaßen dar. Dabei zeigt sich, dass die Kombination von Bild- und Textanalyse nicht immer zum Erfolg führt. Zwar ergeben sich aus der Analyse beider Informationskanäle statistisch deutlichere Signifikanzen bei der Autor-Identifikation und Genreunterscheidung. Der Effekt ist allerdings gering. Im Fall der Klassifikation nach Originalsprache des Werks sowie unterschiedlicher Publikationsformen 'etwa als Einzelband oder als fortgesetzte Serie'führt die Hinzunahme der Textanalyse derzeit nicht zu signifikanten Ergebnissen. Bei der Analyse unterschiedlicher Formen von Autorschaft (Einzelautor*innen, Zusammenarbeit von einer Autor*in und einer Illustrator*in und größeren Autor*innen-Teams) liegt das Resultat der Textanalyse weit über der Signifikanzgrenze. Die deutlich signifikanten Ergebnisse der visuellen Stilometrie setzen sich allerdings auch in der Kombination beider Kanäle durch. Insgesamt lässt sich sagen, dass trotz der gleichen Anzahl der verwendeten visuellen und Textmaße erstere derzeit aussagekräftiger erscheinen. Weiter erscheint es sinnvoll, die Analyse beider Informationskanäle immer auch einzeln zu betrachten und diese nicht immer sofort zu kombinieren. Wie bereits kurz erwähnt, zeigt sich für eine Klassifikation der entscheidende Einfluss der verwendeten Textmaße. Abbildung 3 stellt alle von uns untersuchten Werke in Streudiagrammen dar und unterscheiden diese farblich nach Originalsprache. Im Fall japanischer Manga handelt es sich hier außerdem um eine eigenständige Nationaltradition. Obwohl uns japanische und französischsprachige Werke in englischer √úbersetzung vorliegen, führt nur die Textanalyse zu einer klaren Unterscheidung. Dies steht im klaren Gegensatz zu den Ergebnissen in Abbildung 1. Zwei potenzielle Ursachen können für dieses, auf den ersten Blick kontraintuitive, Ergebnis angeführt werden. Erstens erscheint es möglich, dass diese Unterscheidung eine Folge des √úbersetzungsprozesses sind. Wahrscheinlicher ist, dass sich typische Merkmale der Texte von Manga auch in √úbersetzung erhalten 'in diesem Fall die Frequenz einzelner Wörter. Wir haben erste Untersuchungen vorgestellt, die am Beispiel von Comicbüchern visuelle und Textmaße für eine multimodale Stilometrie kombinieren. Dabei handelt es sich, insbesondere in letzterem Fall, um sehr einfache Maße, die dem derzeitigen Stand der automatischen Texterkennung für Comicschriften geschuldet sind, und insgesamt um erste Pilotversuche. Wie sich zeigte, führt die Kombination der Text- und Bildebene in der Analyse bisher nicht immer zu besseren Ergebnissen. Allerdings ist dies trotz der geringen Anzahl der untersuchten Werke sowohl bei der Gattungsunterscheidung als auch bei der Autoridentifikation der Fall, für die in der Literaturwissenschaft seit längerem ähnliche Maße herangezogen werden. Insgesamt erscheint es sinnvoll, vor einer Kombination die Analyse der visuellen und textlichen Informationskanäle immer auch einzeln zu betrachten. Einen alternativen Zugang zu dem hier gewählten bietet die stilistische Klassifikation mit Hilfe neuronaler Netzwerke (für Comics: Laubrock & Dubray). Obwohl hier potenziell bessere Ergebnisse erzielt werden können, präferieren wir aus mehreren Gründen einen niederschwelligen Ansatz: trotz der Zuhilfenahme der PCA in den hier abgebildeten Darstellungen versprechen wir uns von der Verwendung einzelner Maße eine bessere qualitative Interpretation. Zweitens ist dieser Zugang weniger datenhungrig und daher der geringen Anzahl an Werken in unserem Corpus angemessen. In einem nächsten Schritt wollen wir die Ergebnisse der Texterkennung verbessern. Dies wird es ermöglichen, zusätzliche Textmaße und Werke für unsere Analyse heranzuziehen und unsere Ergebnisse zu verbessern."
2019,DHd2019,115_final-CALVO_TELLO_Jose_Gattungserkennung_über_500_Jahre.xml,Gattungserkennung über 500 Jahre,"José Calvo Tello (Universität Würzburg, Deutschland)","Gattung, Maschinelles Lernen, Korpora, Spanisch","Datenerkennung, Programmierung, Inhaltsanalyse, Stilistische Analyse, Literatur, Text","Wie gut lassen sich Gattungen und Untergattungen durch Maschinelles Lernen über eine längere Periode erkennen? Obwohl eine Reihe von Artikeln die Frage hauptsächlich für Englisch (Kessler, Numberg, und Schütze 1997; Petrenz und Webber 2011; Underwood 2014) und Deutsch (Hettinger et al. 2016) beantwortet hat, befasst sich wenig Forschung mit diesem Thema aus einer diachronischen Perspektive oder wird auf spanischen Texte angewendet (Henny-Krahmer 2018). Welche Gattungen sind leichter zu erkennen, welche komplizierter? Welche Algorithmen, Transformationen und Anzahl der lexikalischen Einheiten funktionieren am besten? Zur Beantwortung ob verschiedene Gattungen durch Maschinelles Lernen erkannt werden können, wurde das umfangreichste historische Korpus des Spanischen analysiert, CORDE. Dieses Korpus wurde von der Real Academia Espa√±ola kompiliert (Rojo S√°nchez 2010; S√°nchez S√°nchez und Dom√≠nguez Cintas 2007) und ist ein standard-Tool in der Hispanistik über das online Such-Interface (Kabatek und Pusch 2011). Für die Analyse wurden die Frequenzen der Tokens und die Metadaten jedes Texts an Forscher weitergegeben. Das Korpus beinhaltet ca. 300 Millionen Tokens (34.000 Texte) und die Texte sind mit expliziten Metadaten über Jahrhunderte, Länder und Gattungen markiert. Die Daten der mittelalterlichen Sektion des Korpus präsentieren mehrere Probleme (Rodr√≠guez Molina und Octavio de Toledo y Huerta 2017), wie beispielsweise ausgeprägte Unausgewogenheit der Anzahl der Texten im Vergleich zu anderen Jahrhunderten oder schwankende philologische Qualität. Deswegen wurden für diese Analyse nur die Texte der letzten 500 Jahre des Korpus selektiert, die länger als 100 Tokens sind. Somit beinhaltet das analysierte Korpus über 22.000 Texte (über 244 Millionen Tokens). Die Metadaten unterscheiden: Eine komplette Liste der Gattungen ist auf den Abbildungen zu finden. Die Klassifikation wurde binarisiert durchgeführt, d. h. jeder Text könnte zu jeder Gattung gehören oder nicht. Verschiedene Parameter wurden evaluiert: Das Korpus wurde für jede Gattung undersampled: die gleiche Anzahl an positiven wie an negativen Fällen wurden für jede Gattung gesamplet. Die Evaluation wurde mit Hilfe von Cross-Validation (10 folds) durchgeführt und der Mittelwert der F1 Scores berechnet. Der Code wird als Python Notebook über GitHub zugänglich sein. Die höchsten F1 Scores der Kombinationen von Parametern für jede Gattung lagen zwischen 0,9 und 1,0 mit einem Mittelwert der verschiedenen Gattungen von 0,96 (Standardabweichung von 0,03). Diese sehr hohen Ergebnisse ähneln sich denen von Underwood (2014), der an einem sehr großen Datensatz forschte. Die häufigsten Parameter bei den besten Ergebnissen waren Logistic Regression (16 Fälle von 27), binäre Häufigkeit (16, was nicht zu erwarten war) und 6.000 MFW (9). Auf den nächsten Boxplots sind die 10 besten Kombinationen zu sehen. Jeder Punkt entspricht dem Mittelwert der F1 Scores der Cross-Validation der 10 besten Kombinationen von Parametern. Diese sind nach Gattung differenziert aufgelistet:  Folgende Gattungen wurden am besten erkannt: Theater (Vers und Prosa), Romane, lyrischer Vers, und Fachtexte über Naturwissenschaften und Kunst. Lyrische Prosa zeigt heftige Schwankungen, außerdem wurden die niedrigsten Ergebnisse von folgenden Gattungen erreicht: Autobiografie, narrativer Vers, Essay, lyrische Prosa, Prosa sowie Fachtexte über Gesellschaft, Geschichte und Geisteswissenschaften. Ein interessanter Aspekt ist, welche die allgemeinen Tendenzen der Parameter und dessen Kombinationen sind. Dafür eignet sich ein Facet Grid Scatter Plot mit den Algorithmen als Spalten und den Transformationen als Reihen (einzelne Punkte entsprechen den Mittelwert der F1 Scores pro Gattung):  Hinsichtlich der Transformation (Reihen) zeigen die relative und die logarithmierte Häufigkeit niedrigere Ergebnisse als TF-IDF, z-scores und die binäre Häufigkeit. Bei den Algorithmen (Spalten) ist KNN merklich schlechter als die anderen drei. Zuletzt ist noch zu erkennen, dass die Qualität der Ergebnisse bis zu einer Anzahl von 2.000 Tokens zunimmt, und mit Schwankungen bis 6.000 stabil bleibt. Ein interessanter Aspekt ist die Tatsache, dass spezifische Kombinationen (SVC + TF-IDF, binäre + Logistic Regression, relative + Random Forest) von Vorteil im Vergleich zu anderen sind."
2020,DHd2020,191_final-ULRICH_Mona_Science_Data_Center_für_Literatur.xml,Science Data Center für Literatur,Mona Ulrich (Deutsches Literaturarchiv Marbach); Jan Hess (Deutsches Literaturarchiv Marbach); Roland Kamzelak (Deutsches Literaturarchiv Marbach); Heinz Werner Kramski (Deutsches Literaturarchiv Marbach); Kerstin Jung (Institut für maschinelle Sprachverarbeitung der Universität Stuttgart); Jonas Kuhn (Institut für maschinelle Sprachverarbeitung der Universität Stuttgart); Claus-Michael Schlesinger (Institut für Literaturwissenschaft / Digital Humanities der Universität Stuttgart); Gabriel Viehhauser (Institut für Literaturwissenschaft / Digital Humanities der Universität Stuttgart); Björn Schembera (Höchstleistungsrechenzentrum Stuttgart); Thomas Bönisch (Höchstleistungsrechenzentrum Stuttgart); Andreas Kaminski (Höchstleistungsrechenzentrum Stuttgart),"Digitale Literatur, Literatur im Netz, Repositories, Plattform, Forschungsdaten, Datenlebenszyklus","Sammlung, Annotieren, Archivierung, Link, Literatur, Forschung"," Das jüngst ins Leben gerufene interdisziplinäre Für die Archivierung, Analyse und Vermittlung von Digitaler Literatur wird eine Forschungsplattform entwickelt. Da eine solche Plattform nur in der interdisziplinären Zusammenarbeit zu bewerkstelligen ist, sind im Projekt Partner mit unterschiedlichen Expertisen in den einzelnen Teilbereichen vereint, nämlich das Deutsche Literaturarchiv Marbach, das Höchstleistungsrechenzentrum Stuttgart, sowie das Institut für Maschinelle Sprachverarbeitung und die Abteilung Digital Humanities der Universität Stuttgart. Die born-digital Bestände des Deutschen Literaturarchivs bestehen zum einen aus digitalen Nachlässen und zum anderen aus archivierten netzliterarischen Werken. Der umfangreichste digitale Nachlass am Deutschen Literatuarchiv ist von Friedrich Kittler und umfasst 1,5 Millionen Dateien. Zur deutschsprachigen Netzliteratur können weitaus weniger Objekte gezählt werden. Netzliteratur ist durch Verlinkungen und Multimedialität geprägt. Das erschwert die Definition von Objektgrenzen und führt zu nichtlinearen Objektstrukturen, die in der Rezeption nichtlineare Handlungen ermöglichen Zum einem scheinen sich diese Texte also zur Anwendung computergestützter und computerlinguistischer Methoden besonders anzubieten, da sie genuin in elektronischer Form vorliegen. Zum anderen bringt gerade diese Form für ihre Archivierung und Bereitstellung eine Reihe von besonderen Anforderungen mit sich. Digitale Nachlässe sind aufgrund großer Mengen an Daten ohne computergestützte Methoden kaum erschließbar und zugänglich zu machen. Um auf diese wachsende Herausforderung in Archiven und Bibliotheken einzugehen, soll der Einsatz von Methoden der Digital Humanities für die inhaltliche Erschließung textbasierter born-digital Bestände erprobt werden. Wenn digitale Nachlässe bereits obsolete Dateiformate enthalten, sind diese nicht ohne vorherige Formatmigration für aktuelle computergestützte Analysen zugänglich. Auch literarische Webseiten sind von den hochfrequenten Erneuerungszyklen digitaler Technik betroffen. Weiterentwicklungen der Betriebssysteme, der Browser, des HTML-Standards und gängiger Webtechnologien können zu fehlerhafter Darstellung oder fehlenden Funktionen einer Webseite führen. Um ein Werk der Netzliteratur dokumentieren zu können, sind daher neue Formen der Modellierung von Texten, die über eine bloß lineare Form hinausgehen, gefragt. Diese und weitere Bestände sollen mit modernen digitalen Methoden erschlossen, erforscht und vermittelt werden können. Im Zentrum stehen daher der Aufbau verteilter langzeitverfügbarer Repositories für Digitale Literatur inklusive Forschungsdaten und die Entwicklung der SDC4Lit-Forschungsplattform. Die Repositories werden vom Projekt und seinen Kooperationspartnern regelmäßig erweitert und bilden den zentralen Speicher für das Harvesting von Netzliteratur und weiteren Formen elektronischer Literatur im künftigen Betrieb des SDC. Die Forschungsplattform bietet die Möglichkeit zum computergestützten Arbeiten mit den Beständen der Repositories. Bereits entwickelte oder in der Entwicklung befindliche Ansätze zur Archivierung und Bereitstellung von WARC-Archiven (Lin et al. 2017), Textkorpora (Fischer et al. 2019) und Analysefunktionen (Hinrichs et al. 2010) sowie strukturierte Reflexionen eigener Strategien (Kramski, von Bülow 2011) weisen auf eine modulare und integrierte Lösung bei der Bereitstellung von Daten und Services. Die entsprechend geplante modulare Architektur der bereitgestellten Services ermöglicht eine nachhaltige Integration von Repositories und Analysemethoden sowie die Möglichkeit zur späteren bedarfsorientierten Einbindung von Korpora und Analysewerkzeugen. Für die Entwicklung des Repositories und der Forschungsplattform ist der Kontakt zu an der Herstellung, Verbreitung, Erforschung und Vermittlung von elektronischer Literatur beteiligten Communities ein entscheidendes Element. Diese Beteiligung wird über einen mehrköpfigen Beirat und Outreach-Maßnahmen wie Workshops, Seminare und die Arbeit mit Fokusgruppen erreicht. Eine wichtige Aufgabe des Projekts ist in diesem Zusammenhang die Modellierung von Formen digitaler Literatur, die zunächst beispielorientiert im Umgang mit einem bereits vorhandenen Corpus digitaler Literatur erfolgt. Neben digitalen Objekten und entsprechenden Metadaten wird auch ein Repository der anfallenden Forschungsdaten nachvollziehbar und nachhaltig gespeichert. Zu den Forschungsdaten zählen erstens die bei der Arbeit des SDC anfallenden Forschungsdaten, insbesondere solche, die für das Anbieten von Diensten auf der Plattform notwendig sind, etwa mittels Machine Learning errechnete Datenmodelle für an das Corpus angepasste computerlinguistische Analysewerkzeuge (Eigennamenerkenner, Parser, Topic Models etc.). Zweitens soll das Repository die Möglichkeit bieten, die von Nutzer*innen der Forschungsplattform generierten Forschungsdaten strukturiert zu speichern und für die weitere Forschung zur Verfügung zu stellen, etwa Annotationen oder ergänzte Metadaten zu einzelnen Objekten oder zu Objektklassen. Die Sammlung, Bereitstellung, Erforschung und Vermittlung von Literatur im medialen Wandel ist eine Aufgabe, die Forschung und Archive gleichermaßen betrifft. SDC4Lit verfolgt deshalb das Ziel, diese Aufgabe und die entsprechenden Unteraufgaben interdisziplinär zu bearbeiten."
2020,DHd2020,227_final-KRAUTTER_Benjamin_Ein_Schritt_zurück__Distinktive_Eigenschaf.xml,Ein Schritt zurück: Distinktive Eigenschaften im deutschsprachigen Drama,"Benjamin Krautter (Universität Stuttgart, Deutschland)","Topic Modeling, Netzwerkanalyse, Klassifikation, Operationalisierung, Methodologie","Strukturanalyse, Theoretisierung, Netzwerkanalyse, Literatur, Text"," Ein mögliches Anwendungsszenario zeigt Ziel dieses Beitrags ist es jedoch, einen Schritt hinter solche makroanalytischen Befunde zurück zu treten. In einem vorgelagerten Arbeitsschritt möchte ich erörtern, welche quantitativ erfassbaren Merkmale dramatischer Texte überhaupt geeignet sind, um eine literarhistorische Einordnung und Unterscheidung der Dramen vorzunehmen. Anders formuliert sollen also die Kriterien ermittelt werden, die mit Blick auf die Entstehungszeit der Dramen unterscheidungstragend sind. Zu diesem Zweck dient im vorliegenden Fall eine einfach gehaltene Klassifikationsaufgabe. Ließen sich die dramatischen Texte erfolgreich ihrem Veröffentlichungszeitraum zuweisen, könnte daraus auf die Kriterien rückgeschlossen werden, die den entscheidenden Beitrag zu dieser Klassifikation leisten. Daran anschließend wäre eine Rückübersetzung der ermittelten Merkmale denkbar 'analog zur Operationalisierung –, die eine Interpretation anleiten könnten. Die folgenden Untersuchungen konzentrieren sich auf 443 deutschsprachige Dramen zwischen 1730 und 1930 aus dem Die historische Verortung der dramatischen Texte fasse ich als basal gehaltene Klassifikationsaufgabe. Ziel der Klassifikation ist es, mittels maschineller Lernverfahren näherungsweise den Veröffentlichungszeitraum der Dramen zu bestimmen, um daran anschließend die Einflussfaktoren identifizieren und untersuchen zu können. Dazu greife ich auf Metadaten zurück, die Angaben zur Erstaufführung und zur Erstpublikation umfassen. Diese Metadaten werden genutzt, um jedes Drama einer von vier heuristisch gesetzten Zeitspannen zuzuordnen, die jeweils circa 50 Jahre umfassen: 1730–1785 (93 Dramen), 1786–1832 (116 Dramen), 1833–1881 (105 Dramen) und 1882–1930 (129 Dramen). Die dadurch entstehenden Zeiträume dienen als Zielpunkt der Klassifikation und orientieren sich an wichtigen literaturgeschichtlichen Zäsuren: Aufklärung, Goethezeit, Realismus und literarische Moderne (vgl. etwa Brenner¬†2011). Basis der Netzwerk- und Zentralitätsmetriken sind Netzwerkgraphen, die auf Präsenz- bzw. Adjazenzmatrizen fußen. Knoten und Kanten repräsentieren hierbei Dramenfiguren und deren Interaktion, wobei Interaktion als das gemeinsame Sprechen innerhalb einer Szene operationalisiert ist (vgl. Trilcke 2013:¬†238f.). Eine Kante zwischen zwei Knoten wird also genau dann instanziiert, wenn die beiden fraglichen Figuren innerhalb derselben Dramenszene sprechen. Das bedeutet auch, dass verschiedene poetologische Vorstellungen von Akt und Aufzug sowie Szene und Auftritt, die im Verlauf der Dramengeschichte einem Wandel unterliegen, einen Eingang in die Graphen findet (vgl. etwa Vogel¬†2012). Für die Klassifikation nutze ich die folgenden acht Maße:  Das maschinelle Klassifikationsverfahren selbst nutzt den Algorithmus  Die in Auf Basis dieser Daten lassen sich nun analog zu  Auch der in Die größte Schwierigkeit bei der Interpretation dieser Daten bleibt jedoch nach wie vor bestehen und ist den hier gezeigten Analysen vorgelagert. Es ist die Operationalisierung der Fragestellung, die zumeist mit einem großen konzeptuellen Aufwand verbunden ist (vgl. Gius 2019:¬†2f.; Reiter / Willand 2018). Denn unklar bleibt, wie sich Zentralitätsmetriken in einem Figurennetzwerk oder Wahrscheinlichkeitsverteilungen von Worthäufigkeiten zu literaturwissenschaftlichen Kategorien verhalten. Die bemessenen Werte müssten sich konzeptionell so rückübersetzen lassen, dass sie auch mit Blick auf spezifisch literaturwissenschaftliche Fragestellungen interpretiert werden können. Dass etwa die Handlung literarischer Texte nicht einfach durch ein Figurennetzwerk abzubilden ist, muss auch Moretti erkennen, weshalb er die Netzwerktheorie letztlich nurmehr als Vorstufe, als ""beginning of the beginning"" (Moretti 2011:¬†2) zu einer quantifizierbaren Handlung einordnet. Das vorgestellte multidimensionale Modell liefert sinnvolle Ergebnisse und kann den recht weit gefassten Veröffentlichungszeitraum deutschsprachiger Dramen mit angemessener Genauigkeit (F"
2020,DHd2020,167_final-GIUS_Evelyn_Korpusbereinigung_für_größere_Textmengen.xml,Korpusbereinigung für größere Textmengen. Eine (kurze) Problematisierung und ein Lösungsansatz für Duplikate,"Benedikt Adelmann (Universität Hamburg, Deutschland); Evelyn Gius (Technische Universität Darmstadt, Deutschland)","Korpuserstellung, Dubletten, Bereinigung","Umwandlung, Sammlung, Bereinigung, Text"," Mit der Verfügbarkeit digitaler Texte wurde die Praxis der Korpuserstellung jedoch erweitert und es wurde offensichtlich, dass sie nicht ohne weiteres in die bestehende literaturwissenschaftliche Disziplinarmatrix (Kuhn 1970) integriert werden kann. Viele der digital verfügbaren Texte sind nämlich weder kanonisch noch repräsentativ, die Qualität einzelner Texte ist aus philologischer Sicht oft fragwürdig und ein Korpus enthält trotz der Vielzahl der verfügbaren digitalen Texte selten die gesamte Population relevanter Texte, sondern nur eine Teilmenge. Über die philologisch einwandfreie Auswahl von Texten hinaus birgt die Kuration von Korpora weitere Herausforderungen. Der vorgestellte Ansatz wurde für ein Korpus entwickelt, das in einem Forschungsprojekt zur geschlechtsspezifischen Darstellung von Krankheit in literarischen Texten im Rahmen der Forschungskooperation hermA Im daraus resultierenden Korpus von mehr als 2.500 Texten mussten Artefakte behandelt werden, die durch unterschiedliche Digitalisierungsstrategien verursacht wurden. Nicht nur die Erhaltung von Sonderzeichen wie das recht häufige lange s (≈ø) zwischen oder innerhalb der Repositorien war inkonsistent, sondern auch die Verwendung von Bindestrichen (Wortverbindung, Worttrennung an Zeilenumbruch, andere Bindestriche, Gedankenstriche) oder die Kodierung von Zeilenumbrüchen und Absätzen. Diese Probleme konnten mit einer relativ einfachen Heuristik angegangen werden. Ein schwierigeres Problem ist die Frage der Duplikate: Insbesondere bei der Zusammenstellung eines Korpus aus verschiedenen Quellen kann es vorkommen, dass der gleiche Text mehrfach vorhanden ist. In der Regel ist es nicht erwünscht, mehr als eine Instanz desselben Textes im Korpus zu haben, da die Überrepräsentation einzelner Werke bei statistischen Analysen zu verzerrten Ergebnissen führen kann. Daher sollte die Identifizierung von Duplikaten ein wesentlicher Bestandteil der Korpuserstellung sein. Dabei gibt es zwei Probleme: Erstens wächst die Anzahl der ungeordneten Werkpaare, die alle potenziell Duplikate sein könnten, quadratisch mit der Anzahl der Werke. In unserem Korpus mit gut 2.500 Texten müssten deshalb 3,1 Millionen Werkpaare überprüft werden. Zweitens ist auch für jedes einzelne Textpaar die Feststellung, ob es sich um Duplikate handelt, aufgrund von Metadaten- und Textinkonsistenzen eine nicht-triviale Aufgabe. Ansätze zur Wir haben mit zwei Methoden zur automatischen Duplikatidentifizierung experimentiert. Beide sind Heuristiken für die Suche nach Werkpaaren, die Duplikate sind; sie lösen aber nicht das daran anschließende Problem, zu entscheiden, welche von mehreren Instanzen tatsächlich in das Korpus aufgenommen werden sollen. Für die Evaluation wurden alle Duplikatkandidaten, die von mindestens einer der beiden Methoden gefunden wurden, manuell auf ihre Richtigkeit überprüft. Wir berichten über den Prozentsatz der automatisch als Duplikate identifizierten Paare, die tatsächlich Duplikate sind (Precision), und den Prozentsatz der tatsächlichen Duplikate, die automatisch identifiziert werden (Recall). Allerdings lässt sich der Recall nur exakt bestimmen, wenn alle 3,1 Millionen Werkpaare manuell untersucht werden. Als Annäherung verwenden wir daher stattdessen den Prozentsatz der bei der manuellen Prüfung identifizierten tatsächlichen Duplikate (Gesamtzahl: 355), die ebenfalls automatisch gefunden werden. Die erste Methode basiert auf Metadaten und ist deshalb schnell genug, um alle ungeordneten Werkpaare im Korpus zu testen. Für jedes Werkpaar werden Autor*innen- und Titelinformationen verglichen. Was die Autor*innen-Informationen betrifft, so wird die sogenannte Edit-Distanz (Levenshtein 1965) der vollständigen Namen der Autor*innen berechnet; die Edit-Distanz ist die kleinste Anzahl von Zeicheneinfügungen, Zeichenlöschungen und Zeichenersetzungen (""Edits""), mit der der erste Autor*innen-Name in den zweiten umgewandelt werden kann. Für die Titelinformationen haben wir das Maß leicht modifiziert: Wir verwenden die kleinste Anzahl von Edits, die einen der Titel in einen Zwei Texte werden als Duplikate betrachtet, wenn der Abstand sowohl beim Autor*innen-Namen als auch beim Titel höchstens so hoch wie der Schwellenwert ist. Bei einem Schwellenwert von 2 wurden 672 Duplikatpaare mit einer Precision von 51,9 % und einem Recall von 98,3 % identifiziert. Unter den sechs nicht identifizierten Duplikaten finden sich beispielsweise zwei Werke von Karl May mit abweichender Behandlung von Sammelband-/Einzelwerktitel (""Ardistan und Dschinnistan. 1. Band"" vs. ""Der Mir von Dschinnistan""; ""Satan und Ischariot III"" vs. ""Im Todesthale"") und ein Fall, in dem bei einem der beiden Werke fälschlich der Name des Autors als Titel eingetragen war (Ferdinand von Saar, ""Vae victis!""). Das zweite Verfahren berechnet die Edit-Distanzen von Volltexten. Da dies eine zeitaufwändige Operation ist, beschränken wir uns auf den Vergleich von Texten, bei denen der Autor*innen-Name eine Edit-Distanz von maximal zwei hat. Manuelle Überprüfungen zeigten, dass diese Schwelle alle Rechtschreibfehler und Varianten in unseren Daten erfasst, verschiedene Autor*innen mit ähnlichen Namen jedoch ausnimmt. Für Volltexte verwenden wir wieder Teilzeichenketten-Edit-Distanzen, da ein Text mehr oder weniger vollständig in einem anderen enthalten sein kann (z. B. bei Anthologien). Zugunsten der Rechenzeiten verwenden wir wortbezogene Distanzen mit Insertionskosten gleich Deletionskosten gleich Substitutionskosten gleich eins. Zwei Texte gelten als Duplikate, wenn die Teilzeichenketten-Edit-Distanzen für beide Richtungen (1 als Teilzeichenkette von 2 oder umgekehrt), dividiert durch die Länge des jeweils als Teilzeichenkette einzubettenden Texts, unter 15 % liegt. Auf diese Weise können wir 307¬†Duplikate mit einer Precision von 98 % und einem Recall von 84,8 % bestimmen. 678¬†Paare wurden so durch mindestens ein Verfahren als Duplikate identifiziert (Precision: 52,4 %), Die Digitalisierung hat die Arbeit mit literarischen Korpora erheblich gefördert. Die schiere Menge an Texten in einem Korpus muss sowohl technisch als auch konzeptionell unterstützt werden. Für die Erreichung dieser Ziele ist es umso wichtiger, Qualitätskriterien für die Zusammenstellung von Korpora im Hinblick auf die verfügbaren Daten, d. h. für Texte aus heterogenen Quellen unterschiedlicher Qualität, zu entwickeln und umzusetzen. Zusätzlich zu diesen noch zu entwickelnden Kriterien für die wissenschaftliche Qualitätssicherung können einige pragmatische Entscheidungen die Qualität eines Korpus und seiner Texte in Fällen mit geringer Daten- und insbesondere Metadatenqualität erheblich verbessern. Der vorgestellte Ansatz zum Umgang mit Duplikaten kann zusammen mit den genannten Vorverarbeitungsschritten ein wichtiger erster Schritt in diesem Prozess sein."
2020,DHd2020,192_final-KONLE_Lenard_Information_Leakage_in_Sub_Genre_classification.xml,Confounding variables in Sub-Genre classification: instructive problems,Fotis Jannidis (Universität Würzburg); Leonard Konle (Universität Würzburg); Peter Leinen (Deutsche Nationalbibliothek),"Gattung, Predictive Modeling, Bias","Inhaltsanalyse, Modellierung, Stilistische Analyse, Text"," Most research on genre classification has been looking into what you could call ""high level classes"" like newspaper genres (news, editorials etc.; e.g. Frank and Bouckaert, 2006) or web genres (blog, personal website etc.; e.g. Eissen and Stein, 2004). Under this perspective all texts we are looking at belong to one genre: the novel. The subgenres are types of love stories like the doctor novel (""Arztroman"") or the country novel (""Heimatroman"") and types of adventure novels, mainly distinguished by the setting: the war novel (""Kriegsroman"") or the science fiction novel. These novels are cheap (""dime novels"") and published in a booklet format and are usually distributed via magazine kiosks and not book shops (Stockinger 2018). From the very beginning it was clear to us, that they don""t contain a random collection of each genre. On the contrary, the crime novels for example are just a small and very specific subsection of crime novels in general. But nevertheless we assumed that genre is the main aspect to group novels - for publishers and readers. Our dataset consists of 11,600 dime novels from 12 different genres (see Fig.1). The genre label come from the four publishers who divide the market among themselves. (Bastei, Martin Kelter, Pabel Moewig and Cora). The corpus has been documented in previous studies such as Jannidis et. al. (2019a) and Jannidis et. al (2019b).   To understand the first phenomenon better, we plotted the distribution of the authors across the genres¬† (see Fig. 2): Many authors write exclusively within a genre. The greatest overlap can be found in the genres In order to gain an insight into the influence of genre and publisher on the text form, we use Ivis (Szubert 2019) for unsupervised dimensionality reduction. The coloring of the data points according to publisher (figure 3) and genre (figure 4) shows the strong influence of these variables on the texts. It is also clear that Cora Verlag allows less variance among genres and thus becomes the most discriminatory factor. Figure 5 shows a detail of the previous plot, but focuses on microstructures. Theses structures indicate, that on this level genre and publisher are not enough to explain the distribution and that something else 'author or series 'comes into play.  We created a restricted setup with a clear separation of authors, series and publishers between training and test data (i.e. authors which were in the training data, were not included in the test data etc.), and tested the subgenres in an one-vs-rest scheme. Figure 6 shows the results of this setup with at least 30 different combinations of test and training data per genre and a sample size of 200 novels split in half for training and test data.  Though now we control for confounding variables, it is less clear, what it implies for the genre model. It is not unusual in genre theory to conceptualize genre in an ideal way as independent of other factors like authorship, time, publisher etc. which corresponds to the ""strict"" version of splitting train and test data.¬† But at the same time, these factors may be so intertwined with the genre features, that it is difficult, if not impossible to separate them at all (Hempfer 2010). Under this perspective our attempt to construct a ""clean"" and strict model of genre, independent of publishers etc. is a misguided attempt. Looking back we now see that we started our research with some assumptions which seem to be unfounded for this part of the literary market which is dominated by four publishers: We assumed that the genre labels have the same function as in the rest of the literary market. But the small number of publishers seems to create a different situation. We assume now, that at least in some instances combinations of genre names with publisher names (loves stories from Cora vs. love stories from Bastei-Lübbe) describe the clusters best. To start to evaluate this hypothesis, we trained the corpus on label combinations: 1) Genre and Publisher, e.g. ""Cora-Love"", 2) Genre and Series. Figure 6 shows, that in many, but not all cases these combinations achieve very good results, which indicates that a clear-cut set of features corresponds these combinations. In some genres the same is true for series, for example doctoral or horror, while in others the series have no clear feature set (erotic, love).  Following up the indications for confounding variables we uncovered the complicated situation of genre in this subfield of the literary market. We succeeded to explore some of its substructures which haven""t been described yet in literary studies, though it has been always one of its topics that this kind of literature is a commodity (Nusser 1973, Nusser 1991, Nutz 1999, Stockinger 2018). It is quite astonishing that almost every genre behaves differently, but this may be the result of a decades-old competition between this small number of publishers.¬† Probably the different structures correspond to different strategies of each publisher. Bastei-Lübbe for example seems to follow a strategy where each series has a distinct profile, while Cora is focussing more on the publisher name as brand (Fig. 7 and Fig. 4) - though the clustering may also be influenced by the fact that Cora translates many novels from English. It would be an interesting follow-up-project, to find out, whether the readers of these genres know about these structures and how this knowledge directs their choices. Last but not least, we think that the strategies to control for known and unknown confounding variables in text classification, especially if it is done to understand existing structures and not so much to predict really new data, needs to be explored in more detail. We like to thank Reviewer 2 for providing detailed and very informative feedback especially on the relation between data leakage and confounding variables as well as on the evaluation of dimension reduction techniques."
2020,DHd2020,148_final-DU_Keli_Der_Spielraum_zwischen__zu_wenig__und__zu_viel_.xml,"Der Spielraum zwischen ""zu wenig"" und ""zu viel""","Keli Du (Universität Würzburg, Deutschland)","Topic Modeling, Evaluation, Topics-Anzahl","Entdeckung, Inhaltsanalyse, Methoden, Text","Als eine quantitative textanalytische Methode wurde Topic Modeling  Außerdem, wenn Topic Modeling für Forschung in Digital Humanities eingesetzt wird, interagieren die Benutzer normalerweise direkt mit Topics. Deshalb sind die standardmäßigen internen Evaluationsmethoden  Das Korpus der Untersuchung besteht aus 2000 deutschen Zeitungsartikeln zwischen 2001 und 2014  Vor der Topic Modeling basierten Dokument-Klassifikation wurde Bag-of-Words (BoW) basierte Klassifikation zuerst durchgeführt, um eine Baseline der Klassifikation zu definieren. Die Tests erfolgten auch als 10-fache Kreuzvalidierung mit linearer SVM "
2020,DHd2020,248_final-PIELSTR_M_Steffen_Metadaten_basierte_Visualisierungen_im_Sti.xml,"Metadaten-basierte Visualisierungen im Stilometrie-Paket ""Stylo""","Steffen Pielström (Julius-Maximilians-Universität Würzburg, Deutschland); Maciej Eder (Pedagogical University of Kraków, Polen)","Stilometrie, Software, R, stylo","Stilistische Analyse, Visualisierung, Literatur, Text","Dabei ist Ein Aspekt, der immer wieder zu Nachfragen von Usern geführt hat, ist der Umgang mit Metadaten in der durch die Community wohl am häufigsten genutzte Die Informationen über die Gruppenzugehörigkeit eines Textes entnimmt Bislang war die systematische Benennung der Textdateien der einzige Weg, solche Information zur Gruppenzugehörigkeit an die Funktion zu übermitteln. Von Nutzerweite wurde immer wieder der Wunsch nach zusätzlichen Möglichkeiten geäußert, Metadaten zur Gruppenzugehörigkeit der Texte an die Funktion zu übergeben. In den neueren Die Funktion akzeptiert sowohl Faktor als auch einen Vektor von Strings als Gruppierungsvariable. Die andere Möglichkeit ist, die Information zur Gruppenzugehörigkeit der Texte in einer CSV-Datei zu hinterlegen und dem Parameter den Dataipfad als String zu übergeben. Die betreffende CSV-Datei enthält eine Spalte mit der Überschrift ""filename"", die alle Dateinamen des Corpus in alphabetischer Reihenfolge enthält, und mindestens eine weitere Spalte mit Gruppenlabels. Um die Spalte mit der gewünschten Gruppierungsvariable auszuwählen wird der Titel der gewünschten Spalte an den Funktionsparamter Der Default-Wert ist ""author"". Wenn dem Paramter Dieser zusätzliche Parameter in der"
2020,DHd2020,113_final-WEIMER_Lukas_Stilometrische_Untersuchung_von_Figurenreden_in.xml,Stilometrische Untersuchung von Figurenreden in realistischen Erzähltexten,"Lukas Weimer (Julius-Maximilians-Universität Würzburg, Deutschland)","Korpus, Annotation, Figurenrede, Stilometrie","Sammlung, Annotieren, Stilistische Analyse, Literatur","Das Poster stellt ein Korpus deutschsprachiger Erzählungen des 19. Jahrhunderts vor, in dem Figurenreden und ihre jeweiligen Sprecher annotiert und extrahiert wurden. Sie dienen als Basis für stilistische Auswertungen mit dem etablierten Abstandsmaß Delta. Es stellt sich die Frage, ob sich der Autorenstil in den jeweiligen Figurenreden niederschlägt, sich also Figuren desselben Autors zusammengruppieren, oder ob Figurentypen dominanter sind, sich gleiche Figurentypen also werkübergreifend stilistisch ähneln. Erste Ergebnisse hiervon werden als Grafiken präsentiert. Stilometrische Verfahren gehen v.a. auf John Burrows zurück. Sein entwickeltes Abstandsmaß Das Korpus setzt sich aus acht realistischen Erzähltexten zwischen 1848 und 1871 zusammen, da dieser Zeitraum allgemein als Kernzeit des Realismus anerkannt ist (Aust 2006, Plumpe 2007). Um Vergleiche zu ermöglichen, enthält das Korpus zusätzlich drei Erzähltexte von vor 1848. Die Korpusauswahl beruht auf einem mehrschrittigen Prozess: Mit der Längenbegrenzung von 8.000-20.000 Wörtern wurde darauf geachtet, dass die Erzählungen einerseits lang genug sind, um stilometrische Verfahren anwenden zu können und andererseits kurz genug, um die manuelle Annotation in einem angemessenen zeitlichen Rahmen durchzuführen. Außerdem wurde darauf geachtet, sowohl kanonisierte als auch gänzlich unbekannte Texte zu integrieren, weibliche Autoren ins Korpus aufzunehmen und die Erstpublikationsorgane zu variieren. Wie in der damaligen Zeit üblich, wurde ein Großteil der Erzählungen in Zeitschriften, Almanachen oder Taschenbüchern veröffentlicht. Diese waren auf ganz verschiedene Leserschichten ausgerichtet, so dass eine Variation hier alle Stilniveaus erfassen sollte. Die Korpustexte sind die folgenden elf Erzählungen: Da einige der Texte noch nicht erschlossen waren, wurden sie vor der Annotation OCR-korrigiert. Für die Annotation wurde der im Zuge des Redewiedergabe-Projekts (Brunner et al. 2018) entstandene STWR-View des Annotationstools ATHEN (Krug et al. 2018) verwendet. Bei der Annotation wurden sämtliche direkten Figurenreden manuell annotiert und ihrem jeweiligen Sprecher zugeordnet (zur automatischen Zuordnung von Sprechern: Krug et al. 2016). So konnte die gesamte direkte Redemenge einzelner Figuren extrahiert werden. In direkte Reden einer Figur A eingelagerte Reden einer Figur B wurden dabei nur der Figur B als zugehörig annotiert. Auf diese Weise wurde sichergestellt, dass Figuren ausschließlich ihre eigenen Reden zugeordnet wurden (diese Problematik ist besonders relevant bei Binnenerzählungen). Zusätzlich wurden ausschließlich Figuren in die Auswertung integriert, deren gesamte Redemenge 200 Wörter übersteigt, um stilometrische Verfahren wirksam anwenden zu können. Diese Grenze ist für stilometrische Verfahren noch immer vergleichsweise niedrig. Eder (2015) hat evaluiert, dass korpusabhängig mindestens 2500-5000 Wortformen nötig sind, damit Auswertungen mit Delta zu guten Ergebnissen führen. Aufgrund des Korpus dieser Studie kann dieser Mindestwert allerdings nicht eingehalten werden. Die folgenden Grafiken zeigen den Output des R-package Auswertung mit 100 häufigsten Wörtern: Auswertung mit 1000 häufigsten Wörtern: In beiden Auswertungen ist zu erkennen, dass sich häufig Figuren desselben Autors zueinander gliedern. Besonders beim Mundartdichter Gotthelf ( Im weiteren Verlauf der Arbeit müssen die Maße verfeinert und sollen andere Abstandsmaße getestet, Variablen geändert und Ergebnisse evaluiert werden. Die Problematik der Kürze der Texte könnte durch eine Optimierung des Verfahrens verringert werden. So könnten eine Kombination aus Wortform-"
2020,DHd2020,116_final-KREMER_Gerhard_Maschinelles_Lernen_lernen__Ein_CRETA_Hackato.xml,Maschinelles Lernen lernen: Ein CRETA-Hackatorial zur reflektierten automatischen Textanalyse,"Gerhard Kremer (Universität Stuttgart, Deutschland); Kerstin Jung (Universität Stuttgart, Deutschland)","Entitäten, Entitätenreferenzen, maschinelle Lernverfahren, Evaluation, Python, Programmiercode, Annotationen","Programmierung, Annotieren, Bewertung, Software, Text, Werkzeuge","Das Ziel dieses Tutorials ist es, den Teilnehmenden konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten. Es werden dabei keinerlei Vorkenntnisse zu maschinellem Lernen oder Programmierkenntnisse vorausgesetzt. Es gibt keinen Grund, den Ergebnissen von maschinellen Lernverfahren im Allgemeinen und NLP-Tools im Besonderen blind zu vertrauen. Durch die konkreten Einblicke in den ""Maschinenraum"" von maschinellen Lernverfahren wird den Teilnehmenden ermöglicht, das Potenzial und die Grenzen statistischer Textanalysewerkzeuge realistischer einzuschätzen. Mittelfristig hoffen wir dadurch, den immer wieder auftretenden Frustrationen beim Einsatz automatischer Verfahren für die Textanalyse und deren teilweise wenig zufriedenstellender Ergebnis-Daten zu begegnen, aber auch die Nutzung und Interpretation der Ergebnisse von maschinellen Lernverfahren (d.h. in erster Linie von automatisch erzeugten Annotationen) zu fördern. Zu deren adäquater Nutzung, etwa in hermeneutischen Interpretationsschritten, ist der Einblick in die Funktionsweise der maschinellen Methoden unerlässlich. Insbesondere ist die Art und Herkunft der Trainingsdaten für die Qualität der maschinell produzierten Daten von Bedeutung, wie wir im Tutorial deutlich machen werden. Neben einem Python-Programm für die automatische Annotierung von Entitätenreferenzen, mit und an dem während des Tutorials gearbeitet werden wird, stellen wir ein heterogenes, manuell annotiertes Korpus sowie die Routinen zur Evaluation und zum Vergleich von Annotationen zu Verfügung. Das Korpus enthält Entitätenreferenzen, die im ""Center for Reflected Text Analytics"" (CRETA) Als empirisches Phänomen befassen wir uns mit dem Konzept der Entität und ihrer Referenz. Das Konzept steht für verschiedene linguistische und semantische Kategorien, die im Rahmen der Digital Humanities von Interesse sind. Es ist bewusst weit gefasst und damit anschlussfähig für verschiedene Forschungsfragen aus den geistes- und sozialwissenschaftlichen Disziplinen. Auf diese Weise können unterschiedliche Perspektiven auf Entitäten berücksichtigt werden. Insgesamt werden in den ausgewählten Texten fünf verschiedene Entitätenklassen betrachtet: PER (Personen/Figuren), LOC (Orte), ORG (Organisationen), EVT (Ereignisse) und WRK (Werke). Unter Entitätenreferenzen verstehen wir Ausdrücke, die auf eine Entität in der realen oder fiktiven Welt referieren. Das sind zum einen Eigennamen (Named Entities, z.B. ""Peter""), zum anderen Gattungsnamen (z.B. ""der Bauer""), sofern diese sich auf eine konkrete Instanz der Gattung beziehen. Dabei wird als Referenzausdruck immer die maximale Nominalphrase (inkl. Artikel, Attribut) annotiert. Pronominale Entitätenreferenzen werden hingegen nicht annotiert. In In den Die Grundlage für (überwachte) maschinelle Lernverfahren bilden Annotationen. Um die Annotierung von Entitätenreferenzen automatisieren zu können, bedarf es Textdaten, die die Vielfalt des Entitätenkonzepts abdecken. Bei diesem Tutorial werden wir auf Annotationen zurückgreifen, die im Rahmen von CRETA an der Universität Stuttgart entstanden sind (vgl. Blessing et al., 2017; Reiter et al., 2017a). Das Korpus enthält literarische Texte aus zwei Sprachstufen des Deutschen (Neuhochdeutsch und Mittelhochdeutsch) sowie ein sozialwissenschaftliches Teilkorpus. Der  Das Der Ablauf des Tutorials orientiert sich an sog. Zunächst diskutieren wir kurz die zugrundeliegenden Texte und deren Annotierung. Annotationsrichtlinien werden den Teilnehmerinnen und Teilnehmern im Vorfeld zur Verfügung gestellt. Im Rahmen der Einführung wird auch auf die konkrete Organisation der Annotationsarbeit eingegangen, so dass das Tutorial als Blaupause für zukünftige Tätigkeiten der Teilnehmenden in diesem und ähnlichen Arbeitsfeldern dienen kann. Die Teilnehmerinnen und Teilnehmer versuchen selbständig und unabhängig voneinander, eine Kombination aus maschinellen Lernverfahren, Merkmalsmenge und Parametersetzungen zu finden, die auf einem neuen, vom automatischen Lernverfahren ungesehenen Datensatz zu den Ergebnissen führt, die dem Goldstandard der manuellen Annotation am Öhnlichsten sind. Das bedeutet konkret, dass der Einfluss von berücksichtigten Features (z.B. Groß- und Kleinschreibung oder Wortlänge) auf die Erkennung von Entitätenreferenzen empirisch getestet werden kann. Dabei sind Intuitionen über die Daten und das annotierte Phänomen hilfreich, da simplem Durchprobieren aller möglichen Kombinationen (""brute force"") zeitlich Grenzen gesetzt sind.Zusätzlich werden bei jedem Testlauf Informationen über die Entscheidungen protokolliert, um die Erklärbarkeit der Ergebnisse zu unterstützen. Wir verzichten bewusst auf eine graphische Benutzerschnittstelle (vgl. Reiter et al., 2017b) 'stattdessen editieren die Teilnehmerinnen und Teilnehmer das (Python)-Programm direkt, nach einer Einführung und unter Anleitung. Vorkenntnisse in Python sind dabei nicht nötig: Das von uns zur Verfügung gestellte Programm ist so aufgebaut, dass auch Python-Neulinge relativ schnell die zu bearbeitenden Teile davon verstehen und damit experimentieren können. Wer bereits Erfahrung im Python-Programmieren hat, kann fortgeschrittene Funktionalitäten des Programms verwenden. Wie am Ende jedes maschinellen Lernprozesses wird auch bei uns abschließend eine Evaluation der automatisch generierten Annotationen durchgeführt. Hierfür werden den Teilnehmerinnen und Teilnehmern nach Ablauf einer begrenzten Zeit des Experimentierens und Testens (etwa 60 Minuten) die finalen, vorher unbekannten Testdaten zur Verfügung gestellt. Auf diese Daten werden die erstellten Modelle angewendet, um automatisch Annotationen zu erzeugen. Diese wiederum werden dann mit dem Goldstandard verglichen, wobei die verschiedenen Entitätenklassen sowie Teilkorpora getrennt evaluiert werden. Auch das Programm zur Evaluation stellen wir bereit. Am hier verwendeten Beispiel der automatischen Annotation von Entitätenreferenzen demonstrieren wir, welche Schritte für die Automatisierung einer Textanalyseaufgabe mittels maschinellen Lernverfahren nötig sind und wie diese konkret implementiert werden können. Die Teilnehmenden des Workshops bekommen einen zusammenhängenden Überblick von der manuellen Annotation ausgewählter Texte über die Feinjustierung der Lernverfahren bis zur Evaluation der Ergebnisse. Die vorgestellte Vorgehensweise für den gesamten Ablauf ist grundsätzlich auf ähnliche Projekte übertragbar. Das Tutorial schärft dabei das Verständnis für den Zusammenhang zwischen untersuchtem Konzept und den dafür relevanten Features, die in ein statistisches Lernverfahren einfließen. Durch Einblick in die technische Umsetzung bekommen die Teilnehmerinnen und Teilnehmer ein Verständnis für die Grenzen und Möglichkeiten der Automatisierung, das sie dazu befähigt, zum einen das Potenzial solcher Verfahren für eigene Vorhaben realistisch(er) einschätzen zu können, zum anderen aber auch Ergebnisse, die auf Basis solcher Verfahren erzielt wurden, angemessen hinterfragen und deuten zu können. Neben diesem CRETA-Hackatorial befindet sich noch ein weiterer Workshop des Stuttgarter DH-Zentrums CRETA in Begutachtung. Auch wenn es eine gewisse Schnittmenge zwischen den Workshops gibt (Textgrundlagen, Anwendungsfälle), ist die jeweilige Zielsetzung grundsätzlich verschieden: Während es beim hier vorgestellten CRETA-Hackatorial um Verfahren des Maschinellen Lernens geht, konzentriert sich der parallel ausgearbeitete CRETA-Workshop auf den grundlegenderen Schritt der Operationalisierung 'es geht also darum, Ansätze aufzuzeigen, wie ein Untersuchungsvorhaben oder theoretisches Konzept überhaupt für die computergestützte Analyse ""vor- bzw. aufbereitet"" werden kann. Beide Workshops ergänzen einander sinnvoll, was die Teilnahme an beiden oder an nur einem der Workshops möglich macht. Im Vorfeld der Veranstaltung: Installationsanweisungen und Support Der Workshop wird ausgerichtet von Mitarbeitenden des ""Center for Reflected Text Analytics"" (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine ""black box"" sein, sondern auch für Nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird. Der Interessenschwerpunkt Gerhard Kremers ist der reflektierte Einsatz von Werkzeugen der Computerlinguistik für geistes- und sozialwissenschaftliche Fragestellungen. Damit zusammenhängend gehören die Entwicklung übertragbarer Arbeitsmethoden und die angepasste, nutzerfreundliche Bedienbarkeit automatischer linguistischer Analysetools zu seinen Forschungsthemen. Kerstin Jungs Forschungsinteressen liegen im Bereich der Nachhaltigkeit von (computer)linguistischen Ressourcen und Abläufen sowie der Verlässlichkeitsbeschreibung von automatisch erzeugten Annotationen. Dabei verfolgt sie einen aufgabenbasierten Ansatz und arbeitet an der Schnittstelle zwischen Computerlinguistik und anderen sprach- und textverarbeitenden Disziplinen. Zwischen 15 und 25. Es wird außer einem Beamer und ausreichend Stromanschlüssen für die Laptops der Teilnehmenden keine besondere technische Ausstattung benötigt. Es sollte sich um einen Raum handeln, in dem genügend Platz ist, durch die Reihen zu gehen und den Teilnehmenden über die Schulter zu blicken."
2020,DHd2020,163_final-HORSTMANN_Jan_Annotieren__Analysieren__Visualisieren___Einf_.xml,"Annotieren, Analysieren, Visualisieren 'Einführung in CATMA 6","Jan Horstmann (Universität Hamburg, Deutschland); Jan Christoph Meister (Universität Hamburg, Deutschland); Marco Petris (Universität Hamburg, Deutschland); Mareike Schumacher (Universität Hamburg, Deutschland); Marie Flüh (Universität Hamburg, Deutschland)","kollaboratives Arbeiten, Interpretation, Hermeneutik","Inhaltsanalyse, Strukturanalyse, Annotieren, Organisation, Visualisierung, Literatur","Der ohne technische Vorkenntnisse besuchbare hands-on Workshop bildet eine Einführung in die Möglichkeiten der für Geisteswissenschaftler√Ønnen entwickelten, webbasierten Annotations- und Analyseplattform CATMA, deren sechste Version im Oktober 2019 veröffentlicht wurde. Im Zentrum stehen theoretische und praktische Aspekte der digitalen Annotation von (literarischen) Texten sowie die Analyse und Visualisierung dieser Texte und der erstellten Annotationen. CATMA ( CATMA unterstützt... Von linguistischen Textanalysetools unterscheidet sich CATMA insbesondere durch seinen ""undogmatischen"" Ansatz: Das System schreibt mit seiner hermeneutischen Annotation (vgl. Piez 2010) weder definierte Annotationsschemata oder -regeln vor, noch erzwingt es die Verwendung von starren Ja-/Nein- oder Richtig-/Falsch-Taxonomien. Wenn eine Textstelle mehrere Interpretationen zulässt (wie es in literarischen Texten häufig der Fall ist), ist es in CATMA (durch die Nutzung von Standoff-Markup) daher möglich, mehrere und sogar widersprechende Annotationen zu vergeben und so der Bedeutungsvielfalt der Texte Rechnung zu tragen. Mit der Die seit Jahrhunderten zu den textwissenschaftlichen Kernpraktiken gehörende Annotation (vgl. Moulin 2010) lässt sich in sog. Highlights, Freitextkommentare sowie taxonomiebasierte Annotation und Textauszeichnung aufteilen, wobei die Übergänge häufig fließend sind (vgl. Jacke 2018, ¬ß 9). Während CATMA 6 auch die Möglichkeit für Highlights und Freitextkommentare bietet, ist die taxonomiebasierte Annotation das eigentliche Kerngeschäft des Tools 'wobei die Taxonomie prinzipiell undogmatisch erstellt werden kann und die Form von sog. Tagsets annimmt, denen für kollaborative Annotationsprojekte wahlweise eine Annotations-Guideline beigegeben werden kann (vgl. auch Bögel et al). Im Workshop werden wir den Unterschied von Neben der Annotation sind die Analyse und Visualisierung der Text- und Annotationsdaten das andere wichtige Standbein von CATMA. Hier wird Neben diesen grundlegenden Funktionen, die alle per Klick ausgeführt werden können, bietet CATMA die sog. Im Analysebereich können außerdem halbautomatische Annotationen erstellt werden, d.¬†h. man annotiert wiederkehrende Wörter oder Wortgruppen auf einmal mit einem bestimmten Tag, statt dies manuell und wiederholt im Annotationsmodul zu tun. Der Wechsel zwischen der Arbeit im Annotations- und Analyse- und Visualisierungs-Modul ist ein iterativer Prozess, der die klassisch-zirkuläre hermeneutische Interpretationsarbeit in der Literaturwissenschaft widerspiegelt (vgl. Gius, im Erscheinen). Im Workshop werden wir uns in abwechselnden Präsentations- und Hands-on-Phasen der textanalytischen Arbeit in CATMA 6 nähern. Nach einer generellen Einführung in das Tool werden die Teilnehmer√Ønnen anhand eines vorgegebenen Beispieltextes den gesamten Workflow von der individuellen taxonomiebasierten Textannotation, über die Analyse hin zur Visualisierung und Interpretation der Text- und Annotationsdaten kennenlernen und praktisch erproben können. Die Teilnehmer√Ønnen sollen ausgehend vom digitalen Text in die Lage versetzt werden, Annotationen manuell und automatisch unterstützt zu erstellen und in Annotation Collections zu speichern, Tagsets/Taxonomien zu entwickeln und den Text alleine und in Kombination mit den Annotationen zu analysieren und zu visualisieren. Für kritische Reflektionen, Diskussionen sowie individuelle Rückfragen (theoretischer, praktischer und technischer Art) auf jedem Niveau und in Bezug auf die Projekte der Teilnehmer√Ønnen wird ausreichend Möglichkeit bestehen. Im Workshop werden wir anhand von Kafkas Erzählung Jan Horstmann ist Postdoc und koordiniert das DFG-Projekt forTEXT ( Jan Christoph Meister ist Professor für Digital Humanities mit dem   Schwerpunkt Literaturwissenschaft. Als ursprünglicher Erfinder von   CATMA hat er etliche Forschungsprojekte zur Annotation und   Visualisierung textueller Daten und der Entwicklung und Verbesserung   von DH-Tools geleitet. Marco Petris ist Informatiker mit starker Affinität zu   geisteswissenschaftlichen Fragestellungen. Er ist von Anfang an an   der Entwicklung von CATMA beteiligt und beschäftigt sich mit allen   Aspekten der DH-Toolentwicklung, des Tool-Designs und der   Implementierung. Mareike Schumacher promoviert als digitale Literaturwissenschaftlerin über Orte und narratologische Ortskategorien in literarischen Texten, beschäftigt sich besonders mit den Methoden des Marie ist Master of Education und interessiert sich besonders für Christoph Martin Wieland und seine Zeitgenossen. Außerdem liegt ihr Forschungsschwerpunkt auf der Wertung von Literatur. Sie studierte in Kiel und Hamburg und ist nun wissenschaftliche Mitarbeiterin an der Universität Hamburg. Bis zu 30 Personen. Teilnehmer√Ønnen bringen ihren eigenen Laptop mit, der mit dem Internet verbunden ist ("
2020,DHd2020,136_final-PAGEL_Janis_Vom_Phänomen_zur_Analyse___ein_CRETA_Workshop_zu.xml,Vom Phänomen zur Analyse 'ein CRETA-Workshop zur reflektierten Operationalisierung in den DH,"Nora Ketschik (Universität Stuttgart); Benjamin Krautter (Universität Stuttgart); Sandra Murr (Universität Stuttgart); Janis Pagel (Universität Stuttgart); Nils Reiter (Universität zu Köln, Universität Stuttgart)","Operationalisierung, reflektiert, Entitäten, Erzählebenen, Wertherness, CRETA","Modellierung, Annotieren, Theoretisierung, Visualisierung, Literatur, Text","Der Workshop adressiert eine der großen Herausforderungen für Arbeiten in den Digital Humanities 'die Operationalisierung geisteswissenschaftlicher Konzepte und Fragestellungen für computergestützte Methoden (vgl. Jannidis 2010, 109–132; Moretti 2013; Flanders, Jannidis 2015; Jacke 2014, 118–139). Während Geisteswissenschaftler vor allem mit komplexen, häufig textübergreifenden Phänomenen arbeiten und als relevant erachtete Kontexte der behandelten Themen heranziehen, ist die computergestützte Arbeit an identifizierbare Phänomene auf der Textoberfläche gebunden. Die hieraus erwachsende Diskrepanz zwischen Erwartungen und Ergebnissen gilt es über eine adäquate Operationalisierung, Als Anwendungsfälle stellen wir drei unterschiedliche literatur- und sozialwissenschaftliche Phänomene vor, zu denen wir im Rahmen des Stuttgarter ""Center for Reflected Text Analytics"" (CRETA) Zum einen befassen wir uns mit dem Konzept der Entität und ihrer Referenz in literatur- und sozialwissenschaftlichen Texten (vgl. Reiter u.a. 2017, 19–22; Blessing u.a. 2017). Als Entitätenreferenzen gelten alle Ausdrücke, die auf eine Entität der realen oder fiktiven Welt referieren. Dazu zählen Personen/Figuren, Orte, Organisationen sowie Ereignisse, so dass das Konzept der Entität bewusst weit gefasst und für verschiedene Forschungsfragen anschlussfähig ist. Auf Entitäten kann auf verschiedene Weise referiert werden, u.a. über Eigen- und Gattungsnamen (z.B. ""Angela Merkel"", ""die Kanzlerin""). Um Entitäten in einem Text zu extrahieren, müssen folglich die Entitätenreferenzen annotiert und kookkurrente Ausdrücke aufgelöst werden. Die Herausforderungen bestehen vor allem in der Festlegung der Referenzausdrücke (welche Ausdrücke werden berücksichtigt?), in der Abgrenzung von Entitätenreferenzen gegenüber Generika sowie im Umgang mit Verschachtelungen, Metonymien und textspezifischen Besonderheiten. Am Beispiel zweier Textsorten (mhd. Artusroman und Bundestagsdebatten) stellen wir das Phänomen und Möglichkeiten der Umsetzung vor. Des Weiteren beschäftigen wir uns mit der Annotation von Erzählebenen. Als dritten Anwendungsfall stellen wir die sog. ""Wertherness"" vor, womit eine Sammlung von Texteigenschaften gemeint ist, die Texte als ""Wertheriaden"" identifizieren können. Die Veröffentlichung von Goethes ""Die Leiden des jungen Werthers"" 1774 zog eine Reihe an literarischen Adaptationen nach sich, die sich durch verschiedene Bezugnahmen auf den Originaltext als sog. Wertheriaden ausweisen. Die Referenzen können dabei sowohl formaler (z.B. Briefroman, Dreiecksbeziehung) als auch inhaltlicher (z.B. Rolle der Natur, Verhältnis Subjekt-Gesellschaft) Art sein. Für eine computergestützte Analyse solcher Referenztexte müssen einerseits die einzelnen formalen und semantischen Kategorien operationalisiert und in den Texten identifiziert werden, andererseits ist zu untersuchen, welche Kriterien in bekannten Wertheriaden in Kombination miteinander auftreten. Im Workshop stellen wir zwei Ansätze zur Operationalisierung vor, die sich 'in verschiedenen Phasen des Forschungsprozesses 'sehr gut gegenseitig ergänzen. Der erste Ansatz besteht dabei in der Schärfung von Als zweiten Ansatz stellen wir die Idee vor, Zielphänomene In einem Theorieteil führen wir in die Problematik der Operationalisierung von geisteswissenschaftlichen Phänomenen für die computergestützte Analyse ein. Anhand der drei oben genannten Beispiele aus der CRETA-Praxis thematisieren wir die Problematik und stellen die Ansätze der Operationalisierung im Detail vor. Je nach Interesse kann anschließend einer dieser Anwendungsfälle ausgewählt und bearbeitet werden. Im praktischen Teil des Workshops haben die Teilnehmenden die Möglichkeit, beide Operationalisierungsansätze an ihrem gewählten Anwendungsfall zu erproben. Hierfür befassen sie sich zunächst mit dem Phänomen, indem sie es anhand eines Textauszugs manuell annotieren und parallel stichpunktartig die Richtlinien schärfen. In einer ersten Diskussionsrunde werden die verschiedenen Ergebnisse gesammelt und diskutiert. Zur Erprobung des zweiten Ansatzes stellen wir für jeden Anwendungsfall einen Operationalisierungs-""Baukasten"" vor. Dieser besteht aus einer Sammlung von Python-Skripten in einem Jupyter-Notebook Ziel unseres Workshops ist es, die Teilnehmenden für die Wichtigkeit der Operationalisierung in den Digital Humanities zu sensibilisieren und ihnen Lösungsangebote vorzustellen. Durch die interdisziplinäre Ausrichtung von DH-Arbeiten kommt der Operationalisierung eine Schlüsselposition zu, indem diese eine Brücke zwischen geisteswissenschaftlichem Phänomen und computergestützter Umsetzung schlägt. Mit den gewählten Anwendungsfällen wollen wir den Teilnehmenden ein ""Repertoire"" für die Operationalisierung verschiedener Aufgabentypen mitgeben. Wir zeigen zum einen, dass die Annotation eines Phänomens als Methode seiner Operationalisierung dienen kann (vgl. Gius, Jacke 2017, 233–254); zum anderen führen wir für textbasierte Phänomene eine approximative Operationalisierung ein (vgl. Reiter/Willand, 2018). Beide Verfahrensweisen sind auf andere Anwendungsfälle übertragbar. Gleichzeitig möchten wir deutlich machen, dass es für jedes Untersuchungsvorhaben nicht nur eine, sondern verschiedene Wege der Operationalisierung gibt. Die Spielräume, die bei der Operationalisierung geisteswissenschaftlicher Fragestellungen entstehen, machen es notwendig, Entscheidungen reflektiert zu treffen, sie offenzulegen und ihren Einfluss auf die Ergebnisse als Voraussetzung für eine angemessene Interpretation zu bedenken.  Neben diesem Workshop zur Operationalisierung wird noch ein weiterer Workshop des Stuttgarter DH-Zentrums CRETA während der diesjährigen DHd-Konferenz stattfinden (Gerhard Kremer, Kerstin Jung: ""Maschinelles Lernen lernen: Ein CRETA-Hackatorial zur reflektierten automatischen Textanalyse""). Auch wenn es eine gewisse Schnittmenge zwischen den Workshops gibt (Textgrundlagen, Anwendungsfälle), ist die jeweilige Zielsetzung grundsätzlich verschieden: Während es beim CRETA-Hackatorial um Verfahren des Maschinellen Lernens geht, konzentriert sich der hier vorgestellte Workshop auf den grundsätzlicheren Schritt der Operationalisierung. Es geht also darum, Ansätze aufzuzeigen, wie ein Untersuchungsvorhaben oder theoretisches Konzept überhaupt für die computergestützte Analyse ""vor- bzw. aufbereitet"" werden kann. Beide Workshops ergänzen einander sinnvoll, was die Teilnahme an beiden oder an nur einem der Workshops möglich macht. (insgesamt 3 Stunden + 30 Min. Pause) - Kaffeepause (30 Min.) - Zwischen 15 und 25. Abgesehen von Beamer und ausreichend Steckdosen ist keine besondere technische Ausstattung erforderlich. Die Teilnehmenden arbeiten im praktischen Teil an ihrem eigenen PC. Informationen zu eventuellen Vorab-Installationen werden rechtzeitig mitgeteilt. Der Workshop wird von Mitarbeitenden des ""Center for Reflected Text Analytics"" (CRETA) der Universität Stuttgart veranstaltet, die bereits erfahrene Workshop-Leiter/-innen im DH-Bereich sind (DHd 2017, DH 2017,DHd 2018, ESU 2018,DHd 2019, HCH 2019). Das BMBF-geförderte eHumanities-Zentrum CRETA ist auf die interdisziplinäre Zusammenarbeit von Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung ausgerichtet. Die übergreifende Zielsetzung besteht in der Erarbeitung systematischer und transparenter Workflows, in denen die Entwicklung komputationeller Modelle und Methoden kritisch reflektiert und adäquat auf die unterschiedlichen geistes- und sozialwissenschaftlichen Forschungsfragen angepasst wird. Nora Ketschik ist Promotionsstudentin in der Abteilung für Germanistische Mediävistik. Im Rahmen von CRETA führt sie Netzwerkanalysen zu ausgewählten mittelhochdeutschen Romanen durch und setzt sich dabei kritisch mit der Verwendung computergestützter Methoden für literaturwissenschaftliche Analysezwecke auseinander. Benjamin Krautter ist Promotionsstudent in der Abteilung für Neuere Deutsche Literatur II und Mitarbeiter im Projekt QuaDramA - Quantitative Drama Analytics. Dort arbeitet er an der Operationalisierung Aristotelischer Kategorien für die quantitative Dramenanalyse. Er beschäftigt sich zudem mit der Integration quantitativer Methoden in literaturwissenschaftliche Fragestellungen ( Sandra Murr ist Promotionsstudentin in der Abteilung für Neuere Deutsche Literatur I. In CRETA arbeitet sie an der digitalen Analyse des ""Wertheriaden-Korpus"", Texte, die in der Folge von Goethes ""Werther"" seit 1774 erschienen sind. Mittels computergestützter Verfahren wird sich mit der Frage auseinandergesetzt, anhand welcher charakteristischer Kriterien eine ""Wertheriade"" als solche definiert wird und wie sich entsprechende strukturelle und inhaltliche Kriterien operationalisieren, in den Texten automatisch identifizieren und reflektiert vergleichen lassen. Janis Pagel ist Promotionsstudent am Institut für Maschinelle Sprachverarbeitung und Mitarbeiter im QuaDramA-Projekt. Er forscht zu Anwendungen von computerlinguistischen Methoden auf literaturwissenschaftliche Fragestellungen und innerhalb von CRETA hauptsächlich zu Koreferenzresolution für literarische Texte. Nils Reiter hat Computerlinguistik/Informatik an der Universität des Saarlandes studiert, wurde 2013 an der Uni Heidelberg promoviert und ist seit 2014 Post-Doc am Institut für Maschinelle Sprachverarbeitung. Seit seiner Promotion ist er im Bereich Digital Humanities unterwegs, mit einem besonderen Interesse an Fragen der Operationalisierung, und zwar sowohl im Hinblick auf Automatisierung wie auch auf manuelle Annotation. Er arbeitet dabei auch an praktischen Fragen der Kooperation zwischen Geistes- und Computerwissenschaftler*innen, und organisiert einen shared task zur Erkennung von Erzählebenen. Derzeit ist er Vertretungsprofessor für Sprachliche Informationsverarbeitung/Digital Humanities an der Universität zu Köln."
2020,DHd2020,147_final-REITER_Nils_Intertextualität_in_literarischen_Texten_und_dar.xml,Intertextualität in literarischen Texten und darüber hinaus,Julia Nanette; Ben Sulzbacher; Nils Reiter; Axel Pichler,"Intertextualität, Literatur, Philosophie, Modellierung, Annotation","Intertextualität, Literatur, Philosophie, Modellierung, Annotation","Die Analyse der Formen und Funktionen von Intertextualität ist ein Forschungsbereich, dessen heuristischer Anspruch seit dem erstmaligen Auftreten des Terminus ""Intertextualität"" in Julia Kristevas Aufsatz Diese divergierenden Tendenzen innerhalb der Intertextualitätsforschung können nicht zuletzt auf die Tatsache zurückgeführt werden, dass sich literarische Intertextualität selbst 'wie bereits das Wort ""An Gerade dieses Changieren zwischen Regelhaftigkeit und Dynamik bereitete bisherigen Untersuchungen literarischer Intertextualität mit den Mitteln analoger Textarbeit stets große Probleme: Klassische Textanalysen und abstrakte Modelle erweisen sich gleichermaßen als defizitär, indem für eine nachvollziehbare Erfassung der bestehenden Vielfalt intertextueller Relationen gerade das Ineinandergreifen von Modellierung und Interpretation entscheidend ist (vgl. Nantke/Schlupkothen 2018, 2019). Die formale Modellierung bietet hier gesteigerte Möglichkeiten der systematischen Erfassung und der induktiven Kategorienbildung sowie der unmittelbaren Visualisierung. Auf diese Weise können Modelle entstehen, welche flexibel genug sind, um unterschiedlichste Formen von Intertextualität adäquat zu erfassen, und dabei gleichzeitig eine formale Strenge aufweisen, die einer maschinellen Abfrage sowie der Kombination mit (teil-)automatisiert erzeugten Analyseergebnissen offensteht. Bislang finden sich Beispiele für den Einsatz computergestützter Verfahren zur Intertextualitätsdetektion vor allem im Bereich der Konkret soll im Panel anhand verschiedener Beispiele aufgezeigt und diskutiert werden, wie und wo sich digitale Ansätze zur Erfassung und Modellierung intertextueller Beziehungen zwischen den Polen ""Formalisierung"" und ""interpretative Freiheit"" verorten lassen. Dabei verstehen wir eine intertextuelle Referenz als eine von einer Leserin/einem Leser wahrgenommene ""Wiederholung"" aus einem anderen Text, wobei die Wiederholung im Regelfall nicht (nur) die Textoberfläche betrifft, sondern Ideen, Gedanken, Formulierungen, Syntax- oder Plotstrukturen. Im Rahmen des Panels werden Verbindungsmöglichkeiten von quantitativen und qualitativen Verfahren zur Erschließung von Intertextualität evaluiert. Ebenfalls wird dabei erörtert, wie im Zuge der Modellierung interpretative Spielräume immer wieder zur Herausforderung für die Formalisierungsbestrebungen werden und wie derartige Situationen positiv gewendet spezifische Funktionsweisen von Intertextualität sichtbar machen können.  Für die systematisierende Erfassung intertextueller Relationen wurde im Projekt Mithilfe der Daraus abgeleitet erfolgt eine Modellierung der relevanten Kategorien in drei ""Bäumen"", welche die Phänomene nach den Ebenen Die Struktur der   Intertextuelle Referenzen spielen neben der Literatur auch in der Philosophie eine große Rolle. So werden etwa in der Zeitschrift Zunächst stellen wir ein Kategoriensystem vor, dass in einem Bottom-Up-Verfahren etabliert wurde. Dazu wurden die Nietzsche-Nachweise als existierende Annotationen aufgefasst und eine ""Meta-Annotation"" zugefügt, die die Art der Referenz charakterisiert (z.B. ""semantisch äquivalente Paraphrase"" oder ""syntaktische Öhnlichkeit""). Mit den üblichen Methoden aus der reflektierenden Annotationspraxis (Übereinstimmung) können Definitionen für diese Charakterisierungen geschärft werden, so dass ein robuster Überblick über verschiedene Arten der Referenzen vorliegt. Im Gegensatz zu Ansätzen, die vollständig ""from scratch"" annotieren, bewahrt der Rückgriff auf existierende Referenzen davor, eine subjektiv motivierte Teilmenge an Referenzen in Betracht zu ziehen. Anknüpfungspunkte und Gemeinsamkeiten mit den im ersten Beitrag vorgestellten Kategorien zu eruieren ist eines der Ziele des Panels. Anstelle eines neuen Modells implementiert WordWeb/IDEM Es ist geplant, dass das Panel der folgenden Struktur folgt: Frau Prof. Dr. Evelyn Gius, TU Darmstadt, hat zugesagt, die Moderation des Panels zu übernehmen."
2020,DHd2020,156_final-HORSTMANN_Jan_Routinen__Ressourcen_und_Tools_der_digitalen_T.xml,"Routinen, Ressourcen und Tools der digitalen Texterforschung. Ein einfacher Einstieg","Jan Horstmann (Universität Hamburg, Deutschland); Marie Flüh (Universität Hamburg, Deutschland); Marco Petris (Universität Hamburg, Deutschland)","digitale Methoden, Dissemination, Lernen und Lehren","Annotieren, Einführung, Lehre, Literatur, Forschungsprozess, Werkzeuge","Die Anwendung computergestützter Verfahren in den Geistes- und Kulturwissenschaften prägt seit geraumer Zeit die Entwicklung unterschiedlicher Fachdisziplinen (vgl. Thaller 2012). Neue Methoden bahnen sich ihren Weg in den Methodenkanon ganz unterschiedlicher Domänen (vgl. Sahle 2015). Wie aber kann man Lehrenden 'mit den unterschiedlichen Ansprüchen universitär Dozierender oder Lehrender an Schulen 'einen möglichst niedrigschwelligen, aber dennoch wissenschaftlich seriösen Zugang zu dem Repertoire digitaler Methoden der Texterforschung eröffnen, das zum Spektrum der Digital Humanities zählt? Wie kann man sowohl Begeisterung wie kritische Kompetenz im konkreten Umgang mit Verfahren der digitalen Textanalyse so vermitteln, dass die Alltagspraxis des Lehrens und Forschens davon profitiert? Man muss nicht immer gleich einen theoretischen ""Paradigmenwechsel"" ausrufen, sondern kann das ""neue"" Feld besser zunächst im ""hands-on""-Modus erschließbar machen. Durch einen niedrigschwelligen Disseminationsansatz entsteht die Möglichkeit, dass alte Fragen und neue Methoden sinnvoll aufeinander bezogen werden können (vgl. etwa Horstmann / Kleymann 2019). Das im November 2017 an der Universität Hamburg gestartete DFG-Projekt forTEXT ( In der Rubrik Routinen stellen wir einführende Einträge zu digitalen Ausgewählte und etablierte deutschsprachige Für jede vorgestellte Methode stellen wir mindestens ein Tool vor, das für die praktische Umsetzung dieser Methode eingesetzt werden kann. Die Tools werden bedarfsgerecht hinsichtlich ihrer Funktionalität, Anwendungsfreundlichkeit, Nutzerbetreuung, Datensicherheit, Nutzungsbedingungen und des Grads ihrer Etablierung im wissenschaftlichen Diskurs befragt. Die Tooleinträge folgen 'wie auch die einzelnen Beitragsformate in den Kategorien Routinen und Ressourcen 'einem wiedererkennbaren Schema, in dem konkrete Fragen aus Nutzer√Ønnenperspektive gestellt und beantwortet werden. Mit der Entwicklung der sechsten Version von CATMA ( Das Projekt wird durch umfangreiche Maßnahmen der nicht-digitalen Dissemination seiner Inhalte begleitet. Einerseits bieten die Projektmitarbeiter√Ønnen bedarfsgerechte Workshops und Vorträge für Forschungsgruppen oder Veranstaltungsreihen an Universitäten und auf Konferenzen an. Darüber hinaus werden schulinterne Workshops durchgeführt, die auf die z.¬†T. sehr unterschiedliche technische Infrastruktur vor Ort eingehen und sich in der inhaltlichen Ausrichtung ebenfalls eng an der spezifischen Bedarfslage der Teilnehmer√Ønnen orientieren. Die umfangreiche Social-Media-Strategie von forTEXT (vgl. Horstmann / Schumacher 2019) ist ein essentieller Teil des gesamten Dissiminationsprogramms: Auf Twitter, Youtube, Facebook und Pinterest treten wir in unterschiedlichen Modi mit diverse Zielgruppen in Kontakt und führen diese in die digitale Arbeit mit Texten ein. So tritt forTEXT nicht nur an neue Nutzer√Ønnengruppen heran, sondern integriert sich auch selbst im fachwissenschaftlichen/DH-Diskurs. Im Januar 2020 wird ein digitales Empfehlungssystem implementiert, das im Frage-Antwort-Schema die Projekte der Nutzer√Ønnen so klassifiziert, dass die automatische Generierung individualisierter Empfehlungen von Routinen, Ressourcen und Tools zur Bearbeitung der jeweiligen Fragestellung möglich sein wird. Das Empfehlungssystem wird somit dafür sorgen, dass die einzelnen Bereiche von forTEXT einerseits zusammengefasst, andererseits aber auch bedarfsorientiert und effektiv durch sie navigiert werden kann. Das System macht damit insbesondere Nutzer√Ønnen ohne vorherige DH-Erfahrung den Einstieg in digitale Methoden zur Unterstützung ihrer Projekte individuell möglich."
2020,DHd2020,174_final-WEIMER_Lukas_Redewiedergabe_in_Heftromanen_und_Hochliteratur.xml,Redewiedergabe in Heftromanen und Hochliteratur,"Annelen Brunner (Leibniz-Institut für Deutsche Sprache, Mannheim); Fotis Jannidis (Julius-Maximilians-Universität Würzburg, Deutschland); Ngoc Duyen Tanja Tu (Leibniz-Institut für Deutsche Sprache, Mannheim); Lukas Weimer (Julius-Maximilians-Universität Würzburg, Deutschland)","Redewiedergabe, Heftromane, Genre, Automatische Erkennung","Programmierung, Annotieren, Stilistische Analyse, Sprache, Literatur, Text","Die Art und Weise, wie die Rede und Gedanken einer Figur im Erzähltext eingebunden werden, ist einer der traditionellen Aspekte der Narratologie (vgl. z.B. Genette 2010; Mart√≠nez/Scheffel 2016). Die vorgestellte Studie untersucht die Anteile unterschiedlicher Redewiedergabeformen im Vergleich zwischen zwei Literaturtypen von gegensätzlichen Enden des Spektrums: Hochliteratur 'definiert als Werke, die auf der Auswahlliste von Literaturpreisen standen 'und Heftromanen, massenproduzierten Erzählwerken, die zumeist über den Zeitschriftenhandel vertrieben werden und früher abwertend als ""Romane der Unterschicht"" (Nusser 1981) bezeichnet wurden. Unsere These ist, dass sich diese Literaturtypen hinsichtlich ihrer Erzählweise unterscheiden, und sich dies in den verwendeten Wiedergabeformen niederschlägt. Der Fokus der Untersuchung liegt auf der Dichotomie zwischen direkter und nicht-direkter Wiedergabe, die schon in der klassischen Rhetorik aufgemacht wurde (vgl. McHale 2014). Die Studie geht von manuell annotierten Daten aus und evaluiert daran die Validität automatischer Annotationswerkzeuge, die im Anschluss eingesetzt werden, um die Menge des betrachteten Materials beträchtlich zu erweitern. Zur Kontrastierung von Heftromanen und Hochliteratur mit quantitativen Methoden liegen bereits Studien vor, welche sich mit Fragen der sprachlichen und thematischen Komplexität beschäftigen (Jannidis/Konle/Leinen 2019a/2019b). Das verwendete Annotationssystem sowie die Erkenner wurden im Rahmen des Redewiedergabe-Projekts entwickelt (Brunner et al. 2019a/2019b). Für die Voruntersuchung wurden aus 22 Hochliteratur-Texten und 22 Heftromanen zufällige Textausschnitte von ca. 1000 Tokens gezogen. Da Heftromane typischerweise in Reihen mit unterschiedlichem Fokus erscheinen, betrachten wir auch das Verhalten dieser unterschiedlichen Heftroman-Genres. Die Heftroman-Ausschnitte wurden darum je zur Hälfte aus den Genres Liebesroman und Horrorroman gewählt. Die Texte wurden von zwei Erstannotatoren unabhängig voneinander bearbeitet. Eine dritte Person erstellte dann auf dieser Grundlage eine Konsensannotation, indem sie die beiden Annotationen verglich, Unstimmigkeiten bereinigte und wenn nötig offensichtliche Fehler korrigierte. Das Annotationssystem (Brunner et al. 2019a) erfasst sowohl die Wiedergabe von Rede als auch von Gedanken und Geschriebenem. Es umfasst vier Haupttypen von Wiedergabe: Frei-indirekte Wiedergabe ist im Folgenden ausgeschlossen, da sich die automatische Erkennung für diese Form als noch zu unzuverlässig erwiesen hat. Die Formen indirekte und erzählte Wiedergabe sind zu ""nicht-direkt"" zusammengefasst. Diese Form umfasst damit sowohl die klassische indirekte Wiedergabe mit Rahmenformel und abhängiger Proposition als auch strukturell abweichende und häufig stärker zusammenfassende. Sie steht im Gegensatz zur direkten Wiedergabe insofern, als die Rede, Gedanken oder schriftliche Öußerung einer Figur in den Erzählertext integriert anstatt in einem Zitat klar davon abgesetzt wird. Die automatischen Erkenner beruhen auf DeepLearning. Um eine bessere Einschätzung der Erkennungswerte zu geben, ein paar Worte zur Vorkommenshäufigkeit der Redewiedergabetypen: Im den konsensannotierten Testdaten liegt der durchschnittliche Anteil von direkter Wiedergabe knapp unter 30% der Tokens (mit starken Schwankungen), von nicht-direkter bei ca. 15%. Tabelle 1 zeigt die Übereinstimmungswerte zwischen den Erstannotatoren, um einen Eindruck zu vermitteln, wie verlässlich eine von Menschen durchgeführte Annotation wäre. Tabelle 2 zeigt nun für die Formen direkt und nicht-direkt die Übereinstimmungsquoten der automatischen Methoden im Vergleich zur Konsensannotation. Wenn man als Baseline einen Erkenner annimmt, der jedes Token mit 50% Wahrscheinlichkeit als Teil von Wiedergabe klassifiziert, käme man für die Testdaten (alle Samples) für direkt auf einen F1-Score von 0,36 (Precision: 0,28; Recall: 0,50), für nicht-direkt auf einen F1-Score von 0,23 (Precision: 0,17; Recall: 0,50), wobei die Einzelscores für Heftromane vs. Hochliteratur bei direkt gleich wären, bei nicht-direkt etwas besser für Hochliteratur (F1: 0,25). Bei direkter Wiedergabe sind die Erkennungsraten der automatischen Methoden vor allem bei den Heftromanen gut, es gibt jedoch Schwankungen zwischen den Textausschnitten. Probleme treten insbesondere bei Ich-Perspektive in Kombination mit unmarkierter Wiedergabe auf, was in Hochliteratur häufiger vorkommt. Dennoch sind die mit dem maschinellen Erkenner erzielten Ergebnisse 'gerade für solche Fälle 'deutlich stabiler als eine Identifikation von direkter Wiedergabe anhand von Anführungszeichen gewesen wäre. Insgesamt neigt der Erkenner dazu, den Anteil von direkter Wiedergabe eher zu über- als zu unterschätzen. Der durchschnittliche absolute Fehler bei der Abschätzung der Anteile liegt im Schnitt bei ca. 10%. Für die nicht-direkte Wiedergabe ist zu betonen, dass die Übereinstimmungsquote auch zwischen Menschen deutlich schlechter ist (vgl. Tabelle 1). Ursache ist, dass durch die stärkere Integration in den Erzähltext sowohl die genaue Abgrenzung als auch die Entscheidung, was als Wiedergabe zu werten ist, schwieriger sind. Die automatischen Methoden erreichen bei den Heftromanen fast gleiche Verlässlichkeit, während die Hochliteratur-Abschnitte sich wiederum als etwas schwieriger erweisen. Da die Anteile von nicht-direkt geringer sind und weniger Schwankungen unterliegen als die Anteile von direkt, ist auch der durchschnittliche absolute Fehler deutlich geringer (ca. 3%), wobei der Anteil von nicht-direkt eher unterschätzt wird. Da für die Erzählweise eines Textes auch das Zusammenspiel der beiden Wiedergabetypen von Interesse ist, visualisieren wir die Textausschnitte der unterschiedlichen Untersuchungsgruppen in einem Scatterplot (Abb. 1). In dieser Darstellung auf Basis der manuellen Konsens-Annotation lässt sich ein Trend der Horrorroman-Textausschnitte erkennen, mit niedrigen Werte sowohl in direkt als auch in nicht-direkt zusammenzuclustern, während Hochliteratur und Liebesroman stark gestreut erscheinen. Mit einem Permutationstest (p=0,01) (Koplenig 2019) lassen sich im Vergleich Heftroman vs. Hochliteratur allerdings auf keiner der beiden Dimensionen signifikante Unterschiede nachweisen. Bei dem Vergleich mit Genres sind lediglich die Abweichungen zwischen Hochliteratur und Horrorroman im Anteil nicht-direkter Wiedergabe signifikant. Legt man die automatisch annotierten Daten zugrunde, verschwindet auch diese Signifikanz. Im nächsten Schritt erweitern wir unser Untersuchungsmaterial stark. Das Korpus wurde aus Volltexten zusammengestellt, wobei diesmal die Unterschiede zwischen Hochliteratur und den einzelnen Genres in den Fokus gerückt wurden: 50 Hochliteratur-Texte wurden mit jeweils 50 Heftromanen aus vier unterschiedlichen Reihen kontrastiert, die unterschiedliche Genres repräsentieren (vgl. Tab. 3). Ein Ziel war, eine größtmögliche Diversität von Autoren zu erreichen, um zu verhindern, dass das Autorensignal die Gruppenzugehörigkeiten überlagert, die uns eigentlich interessieren. Problematisch war dies bei Horrorromanen, wo ein Autor die Reihe extrem dominiert und Krimis, bei denen so gut wie keine Autoreninformationen verfügbar waren. Es ist allerdings bekannt, dass die Reihe ""Jerry Cotton"" von über 100 unterschiedlichen Autoren verfasst wurde (vgl. Karr 2019). Die Texte wurden mit den automatischen Erkennern komplett annotiert. Da die Variation der Textlängen insbesondere in der Gruppe Hochliteratur stark ist, wurden die Texte in 1000-Token-Abschnitte zerlegt, für diese die Anteile von direkter und nicht-direkter Wiedergabe berechnet und die Ergebnisse anschließend für jeden Text gemittelt (analog zur standardisierten Type-Token-Ratio). Anders als bei den Testdaten zeigen sich bei der Auswertung nun klare Unterschiede in beiden Dimensionen: Der Anteil direkter Wiedergabe ist bei Hochliteratur geringer, während der Anteil nicht-direkter Wiedergabe höher ist. Die Signifikanz dieser Unterschiede, wie auch viele Unterschiede zwischen den Genres, lassen sich mit dem Permutationstest mit p=0.01 bestätigen (vgl. Abbildung 2 und 3). Bei der Betrachtung beider Dimensionen in Relation (Abb. 4) fällt sofort auf, dass die Hochliteratur-Texte eine deutliche Streuung aufweisen, während nicht nur die einzelnen Genres, sondern auch die Heftromane als Gruppe zusammenclustern. Angesichts der Tatsache, dass die Heftroman-Genres bewusst reglementierte Reihen sind, während die Hochliteratur-Gruppe nur dadurch definiert ist, dass die enthaltenen Werke als literarisch hochwertig eingeschätzt wurden, ist dieser Befund nicht erstaunlich. Es ist jedoch durchaus bemerkenswert, dass sich der Unterschied zwischen konventionalisiertem und individualistischem Erzählen auf der Dimension der Redewiedergabetypen so deutlich quantitativ nachweisen lässt. Die Hochliteratur-Texte sind zudem die einzige Gruppe, in der ein ‚ÄöÜbergewicht"" an nicht-direkter im Gegensatz zu direkter Wiedergabe auftritt. Die Autoren sind also in der Art und Weise, wie sie Figurenstimmen in den Text einbinden, sowohl individualistischer als auch eher bereit, nicht das direkte Zitat zu wählen. Innerhalb der Gruppe der Heftromane man kann für die Genres Liebesroman, Horrorroman und Krimi einen nahezu linearen Anstieg der beiden Wiedergabeformen in Relation zueinander beobachten, wobei der Anteil direkter Wiedergabe stets höher ist. Es differenziert sich recht klar das Horror-Genre mit einem insgesamt geringeren Wiedergabeanteil, während die ‚Äökommunikativeren"" Genres Liebesroman und Krimi sich stark überlagern. Für diese beiden Genres lassen sich auch keine signifikanten Unterschiede nachweisen. Science-Fiction nimmt eine Zwischenstellung ein: Die Texte sind diverser und streuen ähnlich wie Hochliteratur, wenn auch nicht so extrem. Es ist das einzige Heftroman-Genre, für das sich auf keiner der beiden Dimensionen signifikante Unterschiede zu Hochliteratur nachweisen lassen. Dies passt zu Beobachtungen von Jannidis/Konle/Leinen (2019a), dass Science-Fiction unter den Heftroman-Genres eine Sonderstellung einnimmt und auch bei unterschiedlichen Komplexitätsmaßen wie standardisierter Type-Token-Ratio und Wortlänge höher abschneidet als die anderen Genres. Warum zeigen sich diese interessanten Muster erst in den Volltextdaten und nicht in der Voruntersuchung? Die Erklärung ist, dass die Schwankungen in den Anteilen von Wiedergabe innerhalb eines Erzähltextes so stark sind, dass sie die beobachteten Trends überlagern. Abb. 5 zeigt einen Datenpunkt für jeden der 1000 Token-Abschnitte aus dem Untersuchungskorpus. Zwar werden in der Gesamtheit dieser Datenpunkte die gleichen Trends sichtbar wie in Abb. 4, doch wenn man 'wie bei der Testauswertung 'nur wenige zufällig gezogene 1000-Token-Abschnitte betrachtet, ist es unwahrscheinlich, dass sie erkennbar wären. Die Ausweitung auf mehr Material, die durch die Anwendung automatischer Methoden möglich wurde, führt hier also zu einem Erkenntnisgewinn, der sonst nur mit extremem Annotationsaufwand möglich gewesen wäre. Da die schlechteren Erkennungsraten des Direkte-Wiedergabe-Erkenners für Texte in der Ich-Perspektive bekannt sind, wurde für einen großen Teil der Texte die Erzählperspektive ermittelt. Die Durchmischung ist sowohl bei den Hochliteraturtexten als auch bei den Heftromanen gegeben und Texte beider Perspektiven platzieren sich an unterschiedlichen Stellen. Einzig der Bereich mit sehr niedrigem Anteil von direkter Wiedergabe (<17%) ist ausschließlich durch Texte in Er-Perspektive besetzt. Der Einfluss der Erzählperspektive ist ein Faktor, der in weiteren Untersuchungen genauer betrachtet werden sollte. Mit den vorhandenen Werkzeugen ist es denkbar, die Studien auf noch mehr Textmaterial auszuweiten und dabei auch weitere Genres von Heftromanen zu untersuchen. Zudem lässt sich die Methodik leicht auf Fragestellungen zu den Anteilen von Redewiedergabe in anderen Textgruppen übertragen, z.B. zwischen fiktionalem und nicht-fiktionalem Material oder im diachronen Vergleich. Wir arbeiten zudem im Redewiedergabe-Projekt weiter daran, unsere automatischen Erkenner zu verbessern, insbesondere auch den für freie-indirekte Wiedergabe (zum aktuellen Stand vgl. Brunner et al. 2019c). Die im Redewiedergabe-Projekt entwickelten Erkenner werden nach Abschluss des Projekts im Frühjahr 2020 der Forschungsgemeinschaft zur Verfügung gestellt werden, ebenso wie große Teile des verwendeten manuell annotierten Trainingsmaterials."
2020,DHd2020,247_final-SCHMIDT_Thomas_Der_Einsatz_von_Distant_Reading_auf_einem_Kor.xml,Der Einsatz von Distant Reading auf einem Korpus deutschsprachiger Songtexte,"Thomas Schmidt (Universität Regensburg, Deutschland); Marlene Bauer (Universität Regensburg, Deutschland); Florian Habler (Universität Regensburg, Deutschland); Hannes Heuberger (Universität Regensburg, Deutschland); Florian Pilsl (Universität Regensburg, Deutschland); Christian Wolff (Universität Regensburg, Deutschland)","Songtexte, Lyrics, Distant Reading, Sentiment Analysis, Topic Modeling","Programmierung, Inhaltsanalyse, Sprache, Text","Die Idee des Distant Reading (Moretti, 2002) ist davon geprägt, durch den Einsatz von Methoden der computergestützten Textanalyse und Textvisualisierung große Mengen an Literatur zu explorieren, um Einsichten zu gewinnen, die mit herkömmlichen Methoden nicht möglich sind. Der Einsatz von Distant Reading wird dabei mittlerweile auch außerhalb der Literaturwissenschaften untersucht wie z.B. in den Religionswissenschaften (Pfahler et al., 2018). Im folgenden Beitrag wird ein Projekt vorgestellt, in dem der Einsatz und Nutzen von Distant Reading in ersten Analysen auf einer größeren Menge deutschsprachiger Songtexte exploriert wird. Ziel des Projekts ist es, mittels Distant Reading Unterschiede in gängigen Genres populärer Musik herauszukristallisieren. Im Bereich des Text Mining wird die Analyse von Songtexten vor allem im Kontext von Retrieval- und Recommender-Aufgaben betrieben. Ziel ist meist die automatische Klassifikation und Vorhersage verschiedener Kategorien, z.B. dem Genre (Fell & Sporleder, 2014; De Sousa et al., 2016). Außerhalb dieses Arbeitsgebiets findet man in Bereichen der Kultur- und Literaturwissenschaften sowie der Psychologie Studien mit Songtexten als Untersuchungsgegenstand (Cole, 1971; Kuhn, 1999). Forschungsinteressen umfassen dabei Analysen spezifischen Musikern ( Als Plattform für die Akquise der Songtexte wurde Für jeden gewählten Interpreten wurden über ein Skript alle Songtexte mit Metadaten von LyricWiki akquiriert. Die Akquise des Korpus wurde mittels eines frei verfügbaren angepassten ruby-Skripts durchgeführt Abbildung 1 illustriert Eckdaten zum Gesamtkorpus und den Künstlern. In der Spalte ""Bekannte Vertreter"" werden einige Künstler beispielhaft angegeben. Abbildung 2 zeigt die Songverteilung im zeitlichen Verlauf und Genre-Kontext auf. Im Bereich des Preprocessing wurden Stoppwörter entfernt und alle Wörter zu Normalisierungszwecken in Kleinschreibung gebracht. Für die allgemeine Textanalyse und das Topic Modeling wurden alle Analysen mittels Die Repetition von besonders bedeutenden Wörtern ist ein gängiges Stilmittel bei der Gestaltung von Songtexten. Aus diesem Grund betrachten wir die Analyse der häufigsten Wörter von Songtexten als besonders aufschlussreich. Die folgenden Bilder (Abbildung 3-6) illustrieren die 10 häufigsten Wörter (Most Frequently Used Words; MFWs) der einzelnen Genres. Man erkennt, dass es drei Wörter gibt, die in allen vier Genres gleichmäßig stark vertreten sind: ""Welt"", ""Leben"" und ""Zeit"". Diese Konzepte sind demnach konsistenter Inhalt deutschsprachiger Liedtexte unabhängig vom Genre. Die größte Differenzierung zeigen die Genres Rap, in dem Terme der Umgangs- und Jugendsprache enthalten sind, aber auch thematische Schwerpunkte deutlich werden (""Geld"") sowie das Genre Schlager, das vor allem von emotionalen Termen wie ""Liebe"", ""Herz"" oder ""Glück"" dominiert wird.  Man erkennt, dass für alle 4 Genres insbesondere Varianten von Liebe einen erheblichen Beitrag zur positivien Polarität leisten. Rap grenzt sich deutlich mit für das Genre typischen Themen ab, ausgedrückt durch Wörter wie ""reich"" und mit Slang (""hart"", ""alter""). Alle Genres weisen insgesamt auf eine negative Polarität hin. Entgegen der naiven Intuition sind die Genres ""Rap"" und ""Rock"" dabei noch am positivsten (gemessen an den normalisierten Werten) bewertet. Erste Analysen machen jedoch auch Probleme der lexikonbasierten Sentiment-Analyse deutlich. Die Wörter ""wein"" (weinen) und ""feuer"" (das Feuer) sind in SentiWS als negativ markiert, haben aber in unseren Texten oft eher positive Konnotationen. Bei dem Wort ""wein"" dann, wenn dieses durch die Normalisierung von ""der Wein"" hergeleitet wird. In zukünftigen Arbeiten wollen wir mit einem domänenspezifischen Lexikon arbeiten, das für die jeweilige Anwendungsdomäne optimiert ist. Topic Modeling ist eine Methode, um den Anteil verschiedener Themen in Dokumenten zu analysieren. Ein Thema ist dabei ein selbst definiertes Label für eine Liste von Wörtern, die besonders häufig zusammen auftreten. Als Algorithmus wurde Latent Dirichlet Allocation (LDA) gewählt (Blei et al., 2003). Das Topic Modeling wurde separat für die einzelnen Genres durchgeführt, um Unterschiede und Gemeinsamkeiten zu untersuchen. Wir sind momentan noch am Anfang der Analyse der einzelnen Topics, aber neben Differenzen werden auch Topics gefunden, die ähnliche Konzepte widerspiegeln. Folgende Visualisierungen geben die Wortlisten wider, die wir jeweils als das Topic ""Liebe"" in den einzelnen Genres benannt haben. Die Wortgröße gibt die Häufigkeit des Wortes im jeweiligen Sub-Korpus wider (Abbildung 8). Auffällig ist, dass insbesondere bei Rap familiäre Begriffe wie ""Mama"", ""Vater"" oder auch ""Bruder"" Bestandteil des Topics sind, was traditionellerweise ein häufiger Schwerpunkt im Rap-Genre ist. In unseren zukünftigen Arbeiten wollen wir insbesondere das Korpus systematisch vergrößern und verbessern. Momentane Probleme sind z.B. die Ungleichverteilung in der Menge bezüglich der Genres aber auch ein Fokus auf eher aktuelle Künstler. Wenngleich wir schon erste Eigenheiten der Genres feststellen konnten, wollen wir Methoden wie Sentiment Analysis und Topic Modeling noch weiter explorieren, indem wir beispielsweise die Varianz der Sentiments untersuchen. Des Weiteren wollen wir unsere Arbeit aber auch auf andere Textanalyse-Möglichkeiten wie Kollokationsprofile von Keywords, Named Entity Recognition und Stilometrie ausweiten. Durch die Zusammenarbeit mit Musik- und Literaturwissenschaftlern wollen wir in Zukunft auch explorieren, welche weiteren Forschungsfragen mit Hilfe größerer Korpora und Distant Reading-Methoden beantwortet werden können."
2020,DHd2020,246_final-SCHOLGER_Martina_Das_Theater_mit_dem_Theater__Thementransfer.xml,Das Theater mit dem Theater: Thementransfer in den Spectators,"Alexandra Fuchs (Universität Graz, Österreich); Bernhard Geiger (Know-Center Graz, Österreich); Elisabeth Hobisch (Universität Graz, Österreich); Philipp Koncar (Technische Universität Graz, Österreich); Jacqueline More (Universität Graz, Österreich); Sanja Saric (Universität Graz, Österreich); Martina Scholger (Universität Graz, Österreich)","Zeitschriften, Aufklärung, TEI, Topic Modelling, Netzwerkanalyse, 18. Jahrhundert","Transkription, Inhaltsanalyse, Annotieren, Netzwerkanalyse, Text, Visualisierung","Die journalistische Gattung der ""Spectators"" des 18. Jahrhunderts stellt ein wichtiges Kulturerbe aus der Zeit der Aufklärung dar. Die Zeitschriften entsprachen dem demokratischen Ideal, kulturelle und moralische Fragen in nicht-akademischen Kreisen zu verbreiten und Werte der Aufklärung wie Weltoffenheit, Toleranz, intellektuelle Kritik und soziale Verantwortung zu popularisieren. Ausgehend von den englischen Modellzeitschriften Anhand der Untersuchung von populären Themen mit maschinellen Methoden präsentiert der Beitrag zentrale Linien des Transfers von Diskursen innerhalb des Genres der Spectators und reflektiert damit den Zeitgeist des 18. Jahrhunderts. Mit Hilfe einer Kombination aus Als Fallbeispiel kann das Thema Theater angeführt werden, das in zahlreichen Zeitschriften unterschiedlicher Länder unabhängig voneinander diskutiert wurde. Während in Frankreich das neoklassizistische Theater im 18. Jahrhundert bereits vollends etabliert und somit als Thema in den Spectators weniger relevant war, erlebten Italien und Spanien eine bewegte nationale Theaterdiskussion. Seit dem 16. Jahrhundert hatte sich das italienische Theater stetig weiterentwickelt. Im 18. Jahrhundert wurde es jedoch als Repräsentationsmedium für ein modernes Italien auserkoren und erlebte eine massive Veränderung. Unabhängig davon wurde auch in Spanien der politische Streit zwischen Progressisten und Traditionalisten über das Thema des Theaters ausgetragen: Spanische Intellektuelle versuchten zunehmend durch kulturelle Reformen den intellektuellen Anschluss an Europa zu schaffen und lieferten sich mit Traditionalisten einen regelrechten Streit über eine radikale Reform des Theaters nach neoklassizistisch französischem Vorbild. (Vgl. z.B. Guinard 1973, 133-138; Ertler 2003, 120-124) Die Basis der Untersuchungen bildet das Korpus von etwa 4000 Texten, das im Darüber hinaus werden die (manuellen und maschinellen) Annotationen verwendet, um das zeitliche Auftreten ausgewählter Themen innerhalb einer Sprachgemeinschaft und sprachenübergreifend zu analysieren. Als Referenz für diese Untersuchungen dient das Zeitschriftennetzwerk der Spectators: Dieses stellt die Abhängigkeit von Zeitschriften unterschiedlicher Sprachgemeinschaften hinsichtlich Übersetzung, Adaption und Imitation dar. "
2020,DHd2020,261_final-DENNERLEIN_Katrin_Datamodelling_Drama_and__Musical_theater.xml,Datamodelling Drama and (Musical)theater,Birk Weiberg; Klaus Illmayer; Katrin Bicher; Gesa zur Nieden; Katrin Dennerlein,"Ontologies, Metadata, Databases, Conversion, Discovering, Gathering","Ontologies, Metadata, Databases, Conversion, Discovering, Gathering","Die Katalogisierung von Sammlungs- und Bibliotheksbeständen und zahlreiche Datenbankprojekte aus der Dramen-, Theater- und Musikforschung haben in den letzten Jahrzehnten eine bisher kaum berücksichtigte Fülle von Material zum (Musik-)theater in gedruckter und handschriftlicher Form zu Tage gefördert und recherchierbar gemacht. Die ebenfalls rasch voranschreitende Bild- und Metadatendigitalisierung macht dieses Material der Forschung einfach zugänglich. Ein Portal, das die vielen Einzelprojekte zu Aufführungsdaten, Texten, Noten und Werken gemeinsam recherchierbar machen würde, so dass Strukturen und Zusammenhänge wie Werk- und Aufführungsserien, Gattungszusammenhänge oder Popularität und Wirkung sichtbar werden, gibt es derzeit jedoch nicht. Stattdessen wurden in den  letzten Jahren inhaltlich und technisch sehr heterogene Datenbanken zur Erfassung von Dramentexten, Libretti, Noten, Aufführungen und Theatertruppen angelegt. Um diesen Teil des kulturellen Erbes strukturiert zugänglich zu machen, wäre es nötig Material aus Bibliothekskatalogen, Archivbeständen, Findbüchern und bereits bestehenden Datenbankprojekten zu verknüpfen. Grundlage dafür wären eine umfassende Ontologie des (Musik-)Theaterbereichs, das die Relationen der Objekte und Metadaten abbildete und die Abfrage der Zusammenhänge möglich machte. Zu erfassen wären einerseits die Materialien wie handschriftliche Soufflierbücher, Noten, Theaterzettel, Ariendrucke, Kupferstiche, Videomaterial, Zeitungsberichte usw., aber auch die sie verknüpfenden Praktiken des Produzierens, Bearbeitens, Übersetzens, Aufführens, Kombinierens, Druckens und Distribuierens. Einen zentralen Ansatzpunkt scheint  Swiss Performing Arts Datamodel zu bieten.   In der Aufführungsdatenbank ""Theadok"",  An der Staats- und Universitätsbibliothek Dresden (SLUB) soll der RISM zu einem zentralen Nachweisinstrument für Musikdrucke des 16. bis 18. Jahrhunderts ausgebaut werden.   "
2020,DHd2020,239_final-KUHN_Jonas_Textanalyse_mit_kombinierten_Methoden___ein_konze.xml,Textanalyse mit kombinierten Methoden 'ein konzeptioneller Rahmen für reflektierte Arbeitspraktiken,"Jonas Kuhn (Universität Stuttgart, Deutschland); Axel Pichler (Universität Stuttgart, Deutschland); Nils Reiter (Universität Stuttgart, Deutschland); Gabriel Viehhauser (Universität Stuttgart, Deutschland)","Arbeitspraktiken, Methoden, Reflexion","Modellierung, Kontextsetzung, Theoretisierung, Methoden, Forschungsprozess, Text","Die Zahl und Intensität der Aktivitäten im interdisziplinären Spektrum der Dieser Beitrag fokussiert auf denjenigen Teilbereich der DH, der sich zum Ziel setzt, adaptierbare datenorientierte Computermodelle methodisch adäquat in kombiniert komputationell/geisteswissenschaftliche Arbeitspraktiken zu integrieren. Methodologisch zielt diese Teildisziplin also darauf ab, Forschungsfragen aus einem geisteswissenschaftlichen Kontext mit kombinierten Methoden (bzw. mit ""mixed methods"") adäquat bearbeiten zu können. Es erscheint uns daher an der Zeit, intensiver über einen geeigneten konzeptionellen Rahmen für Arbeiten aus einem der DH-Teilbereiche zu diskutieren, in denen kombinierte Methoden zum Einsatz kommen 'einen Rahmen, der eine gleichermaßen adäquate Reflexion für alle einfließenden Vorannahmen ermöglicht und zudem einfach genug darstellbar ist, dass sich ein methodisch adäquater Workflow mit vertretbarem Aufwand und ohne Brüche konstruieren lässt. In diesem Beitrag stellen wir Kernpunkte eines generalisierten arbeitspraktischen Vorgehensmodells dar, das wir aus den Erfahrungen des Stuttgarter  Als konkrete Illustration des Vorgehens mögen Arbeiten aus dem QuaDramA-Projekt dienen (Krautter/Pagel 2019, Krautter et al. 2018): ein Korpus von deutschsprachigen Dramen wird mit kombinierten Methoden erschlossen; ein exemplarischer Analyseschritt dabei liegt in der Klassifikation von Figuren nach bestimmten Typen. Einige Figurentypen sind bereits literaturwissenschaftlich etabliert (zärtlicher Vater) oder lassen sich relativ treffsicher aus der Figurentafel (Tochter) oder den Metadaten (Titelfigur) extrahieren. Andere Typen wie z.B. die Protagonistin bzw. der Protagonist entziehen sich einer aus unmittelbar verfügbaren Texteigenschaften oder Metadaten ableitbaren Zuweisung, sind jedoch von Bedeutung für literaturhistorische Betrachtungen (etwa für die Frage, inwieweit Emilia Galotti als Titelfigur aus G. E. Lessings bürgerlichem Trauerspiel (1772) den Status einer Protagonistin hat). Eine Annäherung an derartige Kategorien mit kombinierten Methoden kann ausgehend von klaren Fällen eine vorläufige Operationalisierung ansetzen, darauf aufbauend datenbasierte Computermodelle erzeugen und die Modellvorhersagen auf dem Gesamtkorpus in den Prozess einer Verfeinerung der Operationalisierung einfließen lassen. Als Ausgangspunkt skizziert Abbildung 1 ein DH-Vorgehen, das sich bei nicht-trivialen Modellierungsaufgaben etabliert hat 'in Anlehnung an ausgeprägte methodische Konventionen in der Korpus- und Computerlinguistik (vgl. u.a. Hovy/Lavid 2010, Kuhn/Reiter 2015, Stefanowitsch 2018, Kuhn 2019): Die Analysekategorie, die im Rahmen einer geisteswissenschaftlichen Gesamtfragestellung angewendet werden soll, wird konzeptuell operationalisiert 'gängiger Weise in Form von präzisen Annotationsrichtlinien. Der erste zentrale Aspekt für eine effektive Praxis der Methodenkombination liegt in der Das bislang geschilderte Vorgehen fokussiert ausschließlich auf die technische Optimierung der Vorhersagemodelle für fixierte Referenzdaten. Ein effektiver konzeptioneller Rahmen für die Methodenkombination muss daneben Prozessen Raum geben, die eine sukzessive Verfeinerung der Analysekategorien vornehmen, um einem geisteswissenschaftlichen Fragenkomplex gerecht zu werden. Hier sind mehrere Aspekte zu unterscheiden: Letzteres ist allerdings bei einer komputationell/geisteswissenschaftlichen Methodenkombination der Fall. Die Gütekriterien, die zur Revision einer geisteswissenschaftlichen Analysekategorie führen, können grundsätzlich anderen methodischen Prinzipien und Vorannahmen folgen. Eine probehalber vorgenommene Operationalisierung eines vielschichtigen Konzepts in der Textanalyse (z.B. Protagonisten in Dramen) mag sich zum Beispiel als unergiebig erweisen, obgleich die komputationelle Umsetzung entsprechend den Referenzdaten eine hohe Vorhersagequalität ermöglicht. Abbildung 2 zeigt entsprechend eine stärker ausdifferenzierte Skizze des konzeptionellen Rahmens. Der vierte Aspekt ist hier bereits angedeutet: Wie Abbildung 2 suggeriert findet die Übersetzung aus der geisteswissenschaftlichen in die Sphäre der datenbasierten Computermodellierung sinnvollerweise für einzelne Inferenzschritte separat statt (obgleich wie in Fn. 5 angedeutet die Computerarchitektur für einen Schritt selbst technisch komplex sein kann). Es wird deutlich, dass bei der Bearbeitung von nicht-trivialen Fragestellungen rasch ein vielschichtiges Geflecht von Komponenten mit unterschiedlichem Status entsteht. Ein Hauptziel der hier vorgeschlagenen Konzeption liegt darin, das Augenmerk auf genau jene Statusunterschiede zu lenken, die für ein methodisch reflektiertes Vorgehen zu relevanten Vorannahmen relevant sind. Strukturell sind die Elemente unserer Konzeption trotz der darstellbaren Komplexität vergleichsweise einfach 'sie konzentrieren sich auf die Aufgabe der methodenübergreifenden Übersetzung mittels der referenzdatengestützten Operationalisierung und komputationellen Modellierung. Für die konkrete arbeitspraktischen Projektroutine sollte also eine verhältnismäßig übersichtliche Sicht auf die relevanten Komponenten möglich sein. Die abschließende Abbildung 3 demonstriert jedoch, dass der konzeptionelle Rahmen bei Bedarf eine Schnittstelle zu einer sehr differenziert ausgearbeiteten wissenschaftstheoretischen Konzeption wie der von Danneberg/Albrecht 2017 bietet. (Aus Platzgründen können wir in diesem Abstract nicht auf Details eingehen.) Ein reflektiertes Vorgehen kann also auch auf fundamentalere Fragen etwa zur Problematisierung von divergierenden Wissensansprüchen über Disziplingrenzen hinweg eingehen."
2020,DHd2020,173_final-WILLAND_Marcus_Passive_Präsenz_tragischer_Hauptfiguren_im_Dr.xml,Passive Präsenz tragischer Hauptfiguren im Drama,"Marcus Willand (Universität Heidelberg, Deutschland); Benjamin Krautter (Universität Heidelberg, Deutschland, Universität Stuttgart); Janis Pagel (Universität Stuttgart); Nils Reiter (Universität Stuttgart, Universität zu Köln)","Digitale Dramenanalyse, Operationalisierung, Korpusanalyse, CLS","Inhaltsanalyse, Strukturanalyse, Beziehungsanalyse, Modellierung, Netzwerkanalyse","Dramen entwerfen einen fiktiven sozialen Raum (Bourdieu 1985), dessen Bewohner sich ständig In unserem Beitrag möchten wir den Zusammenhang zwischen aktiver und passiver Figurenpräsenz in dramatischen Texten untersuchen, indem wir quantitative und qualitative Analysen kombinieren. In einem ersten Schritt entwickeln wir eine Operationalisierung für eine computergestützte Analyse aktiver und passiver Präsenz und werden in einem zweiten Schritt die aus den Analysen resultierenden Ergebnisse mit besonderem Fokus auf Hauptfiguren diskutieren. Seit einigen Jahren gilt die Netzwerkanalyse als eine der zentralen Forschungsgebiete innerhalb der digitalen Dramenanalyse. Typischerweise modellieren Netzwerke auf Basis von Konfigurationsmatrizen (vgl. Marcus 1973, ins. S. 308ff. und Pfister 2001, S. 235–240) aber nur die aktive (szenische) Präsenz von Figuren (Moretti 2011; Trilcke u.a. 2015; Piper u.a. 2017), obwohl Ko-Präsenz-Relationen nur einen eingeschränkten Aussagewert bezüglich der ""soziale Welt"" eines Dramas zulassen. Denn sie beruhen lediglich auf Informationen über die Anzahl an Szenen, in denen Figuren gemeinsam auftreten. Aktive Figuren wurden aber natürlich auch anders beforscht. Karsdorp u.a. (2015) stellen einen Ansatz zur automatischen Bestimmung von Liebesbeziehungen vor, Willand und Reiter (2017) verwenden semantische Wörterbücher, um Figurenrede und Geschlecht in einen Zusammenhang zu stellen. Nalisnick und Baird (2013) analysieren das Die aktive Präsenz von Figuren lässt sich unterschiedlich operationalisieren, etwa indem der Anteil der Rede einer Figur an der Gesamtrede einer Szene oder eines Akts gemessen wird. Abb. 1 zeigt dies für die fünf Akte von In den Akten 1 und 4 ist Emilia überhaupt nicht aktiv präsent. In den Akten 2, 3 und 5 ist sie es, aber der Anteil ihrer Rede vergleichsweise gering. Wieso aber ist sie titelgebende Protagonistin dieses Stücks, wenn sie doch kaum handelt? Deutlich wird das, betrachtet man ihre passive Präsenz: Die Punkte in Abb. 2 repräsentieren die Erwähnungen des Namens ""Emilia"" in der Rede anderer Figuren (y-Achse) im Verlauf des Stücks (x-Achse). Sie zeigen, dass Emilia während des gesamten Stücks von allen Figuren wiederholt erwähnt wird. Diese Zusammenfassend liefern diese Analysen Argumente für die Interpretationshypothese, dass Emilia den dramatischen Konflikt nicht selbst aktiv löst 'was sie zur positiven Hauptfigur machen würde –, sondern lediglich auslöst. So wird sie zum passiv-tragischen Gegenstand der Figurenhandlung. Für jede Figur definieren wir die Gleich in mehrfacher Hinsicht handelt es sich bei der Methode um eine Heuristik: Einerseits erfasst die Analyse von Figurennamen längst nicht alle Erwähnungen einer Figur. Wir gehen aber davon aus, dass der Figurenname mindestens einmal in jeder Szene genannt wird, in der auf eine Figur referiert wird. Andererseits können Szenen sehr unterschiedlich lang ausfallen, was für die hier durchgeführten Analysen der passiven Präsenz unberücksichtigt bleibt. Anders formuliert: Jede Szene ist bei dieser Form der Analyse gleich gewichtet. Unterschiedliche poetologische Funktionalisierungen von Szenen, wie sie im Verlauf der Dramengeschichte zu beobachten sind, u.a. anhand des Abrückens von der Sowohl die aktive als auch die passive Präsenz wird der besseren Vergleichbarkeit halber über die Zahl der Szenen normalisiert. Dafür wird die Menge an aktiven Auftritten sowie passiven Erwähnungen einer Figur durch die Gesamtzahl der Szenen im Drama geteilt, sodass der Gesamtwert der Figurenaktivität immer zwischen 0 (spricht nie/wird nie erwähnt) und 1 (spricht in jeder Szene/wird immer erwähnt) liegt. Somit ergibt sich für die Berechnung: Die automatische Erkennung von Hauptfiguren in dramatischen Texten ist bisher nur in Ansätzen versucht worden (Krautter & Pagel 2019; Fischer u.a. 2018), sie würde auf dem Gebiet der digitalen Dramenanalyse aber die Grundlage für erkenntnisversprechende Anschlussfragen schaffen. Diese Operationalisierung erlaubt es uns, die figuren- und gattungsspezifische Verteilung der Resultate zu vergleichen und so bisher ungesehene Aspekte von Hauptfiguren zu identifizieren. In diesem Beitrag stellen wir das Genre Unser Korpus enthält deutschsprachige dramatische Texte aus der Zeit zwischen 1750 und 1800 (Fischer u.a. 2019). Es ist in zwei Teilkorpora aufgeteilt, die auf sehr unterschiedlichen Poetiken beruhen: Sechs Stücke des Abb. 4 visualisiert die präsentische Auswertung des BT-Korpus. Jeder Punkt stellt dabei eine Figur dar. In den Stücken treten fast so viele weibliche wie männliche Figuren auf, wobei die sowohl aktiv als auch passiv präsentesten Figuren überraschenderweise weiblich sind. Zudem werden keine Extremwerte erreicht: Alle aktiven Präsenzwerte liegen unter 0,7, alle passiven unter 0,5. Eine Gesamtpräsenz von 1 ist bei keiner Figur zu beobachten. Basierend auf den Präsenzwerten kann ein Schwellenwert etabliert werden, der ungefähr bei 0,4 liegt. Dieser Schwellenwert (gestrichelte Linie in Abb. 4) ergibt sich hier nicht vollständig induktiv aus den Daten, sondern wird theoriegeleitet gesetzt. An dieser Stelle greifen die formale, quantitative und die qualitative Analyse ineinander. Die Figurenverteilung in Abb. 5 (SD-Korpus) unterscheidet sich von derjenigen in Abb. 4 deutlich. Der Schwellenwert liegt hier mit 0,6 viel höher. Um als Hauptfigur zu gelten, muss eine Figur im SD also eine höhere Gesamtpräsenz aufweisen als im BT. Dies ist eine der zentralen Erkenntnisse dieses Forschungsbeitrags. Die Figuration (vgl. hierzu Elias 2002) dramatischer Hauptfiguren scheint somit textgruppenspezifisch und durch die Ermittlung des Präsenzwertes analysierbar zu sein. Darüber hinaus ist das Geschlecht ein relevanter Faktor im Gattungsvergleich. Im SD treten insgesamt weniger weibliche Figuren auf und nur 3 von 11 Hauptfiguren sind weiblich. Zudem sind die weiblichen Figuren eher passiv präsent, während die männlichen überwiegend aktiv präsent sind. Auch die insgesamt aktivsten Figuren sind jeweils männlich. Nur eine einzige Figur erreicht den maximalen Präsenzwert von 1, nämlich Guelfo in Klingers Nicht zuletzt ist die Diese Ergebnisse sind ein gewichtiger Hinweis auf grundlegend divergierende Bauprinzipien dramatischer Texte, die sich offenbar nicht nur durch Handlungen und Themen unterscheiden, sondern auch durch die spezifische Präsenzgestaltung von Hauptfiguren. Da diese Unterschiede durch lineares Lesen jedoch kaum identifiziert werden können, möchte dieser Forschungsbeitrag als Argument für die Erweiterung der qualitativ-interpretierenden Dramenanalyse durch quantitative Methoden verstanden werden. Die aktive Präsenz einer Figur lässt sich auch anhand anderer Einheiten skalieren, etwa anhand der Akte oder der Gesamtzahl der in einem Drama gesprochenen Repliken. Es wäre ebenfalls möglich, den Wert der aktiven Präsenz als die Zahl der gesprochenen Tokens (i.d.R. Wörter und Satzzeichen) und den Wert der passiven Präsenz als die Zahl namentlicher Nennungen aufzufassen. Dadurch könnten einige zuvor beschriebene Problematiken ausgeräumt werden, etwa die der differierenden Szenenlängen. Figuren, die nur kurze Passagen sprechen oder punktuell erwähnt werden, hätten dann vermutlich kleinere Aktivitätswerte, als es bei der szenisch gebundenen Präsenzberechnung der Fall ist. Die Rede- und Erwähnungsverteilung dürfte als näher an der vom Zuschauer bzw. Leser wahrgenommenen Realität des fiktiven sozialen Raums liegen. Hierbei stellen sich allerdings auch neue Herausforderungen. So kommt der Koreferenz von Figuren ein deutlich größeres Gewicht zu. Da wir zuverlässige Koreferenzen momentan nur für einzelne Stücke manuell annotiert vorliegen haben und somit auf die namentlichen Erwähnungen beschränkt sind, ergeben sich unter Umständen stark fehlerbehaftete Werte: Wenn etwa Figuren, die nur selten namentlich Erwähnung finden, überproportional stark auf andere Weise referenziert werden. Ist die namentliche Erwähnung hingegen an einzelne Szenen gebunden, hat diese Fehlerquelle geringeren Einfluss auf die Werte. Um die Auswirkungen der unterschiedlichen Operationalisierungen zumindest einer ersten Exploration zu unterziehen, nehmen wir die Präsenzanalyse von Klingers Abb. 7 zeigt die Präsenzanalyse für Verglichen mit den Präsenzwerten in Abb. 6 ergeben sich erhebliche. Aufgrund der wenigen namentlichen Erwähnungen sinkt vor allem die passive Präsenz von Ferdinando von fast 0,8 auf etwa 0,1. Ferdinando wird also konsistent in vielen Szenen erwähnt, die Zahl der Erwähnung bleibt aber insgesamt vernachlässigbar, vergleicht man seinen Wert mit Guelfo. Wir gehen jedoch davon aus, dass die Fehleranfälligkeit der Koreferenzheuristik hier insgesamt ungenauere Ergebnisse liefert. Der Beitrag stellt eine Methode vor, die ein erweitertes Präsenzkonzept operationalisiert, das neben der aktiven Präsenz dramatischer Figuren auch die passive Präsenz umfasst. Die passive Präsenz operationalisieren wir als Zahl der Szenen, in der eine Figur namentlich erwähnt wird, ohne selbst aktiv auf der Bühne zu stehen. Die Ergebnisse unserer Korpusanalysen lassen auf unterschiedliche Bauprinzipien dramatischer Texte schließen, die an die spezifische Präsenz von Hauptfiguren gebunden sind. Für die Zukunft erscheint es fruchtbar, die hier eruierten Erkenntnisse im Lichte poetologischer Setzungen und Funktionalisierungen 'etwa der Vorbildfunktion Shakespeares 'zu untersuchen.  "
2020,DHd2020,153_final-WIEDMER_Nathalie_Romeo__Freund_des_Mercutio__Semi_Automatisc.xml,"Romeo, Freund des Mercutio: Semi-Automatische Extraktion von Beziehungen zwischen dramatischen Figuren","Nathalie Wiedmer (Universität Stuttgart, Deutschland); Janis Pagel (Universität Stuttgart, Deutschland); Nils Reiter (Universität Stuttgart, Deutschland, Universität Köln, Deutschland)","Figurenbeziehungen, Figurenverzeichnis, Dramen, Dramenanalyse, GerDraCor","Beziehungsanalyse, Annotieren, Literatur, Text","In diesem Beitrag stellen wir eine Methode vor, um Informationen über Figurenrelationen in dramatischen Texten, die innerhalb der  Das Verfahren 'und dessen Implementierung in einem Python-Skript 'ist auch für in Zukunft digitalisierte Dramen anwendbar, und wird von uns als quelloffene Software zur Verfügung gestellt. Es ist vergleichsweise einfach auf neue Sprachstufen oder Genres anpassbar und liefert 'auch bei nicht-perfekten Ergebnissen 'eine gute Vorlage. Eine Evaluation des Verfahrens erfolgt auf ungesehenen Testdaten. Außerdem veröffentlichen wir einen Datensatz mit extrahierten Figurenrelationen aus deutschsprachigen Dramen, die manuell validiert und korrigiert wurden. Diese Daten werden zur einfachen und breiten Nutzung im TEI-Format in das GerDraCor Unsere Methode unterscheidet zwischen sieben Kategorien von Figurenrelationen (Tabelle 1). Ausschlaggebend für die Zuordnung zu einer der Kategorien sind Signalwörter wie ""Vater"", ""Kammerdiener"", ""Geschwister"" etc. Diese Signalwörter werden in einer kontextfreien Grammatik der entsprechenden Kategorie zugeordnet. Kontextfreie Grammatiken bezeichnen in der Informatik eine Sammlung aller syntaktisch korrekten Programme einer Programmiersprache (Böckenhauer und Hromkoviƒç 2013, 177). Die formalisierte Art, in der die Grammatik alle Regeln einer Programmiersprache enthält, erlaubt es, automatisierte Syntaxanalysen von Programmen durchzuführen (Böckenhauer und Hromkoviƒç 2013, 177). Die Regeln werden mit Hilfe zweier Alphabete beschrieben: Das Terminalalphabet enthält alle Wörter einer Sprache, wohingegen das Nichtterminalalphabet Variablen enthält, die vorgeben, auf welche Art und Weise die Wörter kombiniert werden können (Böckenhauer und Hromkoviƒç 2013, 178). Wir nutzen eine solche Grammatik, um drei verschiedene Zeilenarten im Figurenverzeichnis zu unterscheiden, bei denen es sich um Nichtterminale handelt. Alle in den Sätzen vorkommenden Tokens sind Terminale, deren Kombination und Anzahl Aufschluss darüber gibt, um was für eine Art von Zeile es sich jeweils handelt. Auf diese Weise können auch zeilenübergreifende Relationen erkannt werden. Zu Beginn des Programmablaufs werden die in GerDraCor vorhandenen Figuren-IDs zusammen mit dem Figurenverzeichnis ausgelesen und gespeichert. Da wir die Beziehungen zwischen den Figuren ausschließlich anhand der Angaben im Figurenverzeichnis konstruieren, muss der Dramentext nicht extra eingelesen werden. Daraus ergibt sich die Beschränkung, dass jegliche Beziehungen, die nicht im Figureverzeichnis explizit gemacht werden, vom Programm auch nicht erkannt werden können. Es geht demnach ausschließlich darum, das Personenverzeichnis maschinenlesbar und -interpretierbar zu machen. So ignoriert das Programm beispielsweise auch alle Zeilen, die eine Gruppe von Figuren als Kollektiv einführt, da diese als ""Nummern oder als anonyme Angehörige von Untergruppen"" (Schlaffer 1972, 11) meistens keine eigenen Namen haben und auch keine explizit gemachten Beziehungen. Anschließend werden alle Tokens jeder Zeile des Figurenverzeichnisses daraufhin untersucht, ob es sich dabei um Figurennennungen oder Signalwörter handelt und die Grammatik einem Parser übergeben, der die Zeilen des Figurenverzeichnisses in Baumstrukturen überführt (Abbildung¬†2).  Aus den erstellten Baumstrukturen werden einzelne Informationen ausgelesen, die grundlegend für die Erkennung der Figurenrelationen sind. Zuerst wird überprüft, wie viele IDs sich in einer Zeile befinden. Die erste oder einzige wird zur Erstellung späterer Relationen abgespeichert. Befindet sich in einer Zeile zusätzlich zu einer ID noch ein Signalwort für eine Figurenrelation, bezieht sich die Zeile in der Regel auf die vorangegangene, wie beispielsweise in <castItem corresp=""#saladin"">Sultan Saladin.</castItem> <castItem corresp=""#sittah"">Sittah, seine Schwester.</castItem> Die zweite Zeile enthält neben dem Namen noch das Signalwort ""Schwester"", das auf die Beziehungsart siblings hinweist, eine ungerichtete Relation. Da keine zweite Figurenbezeichnung in der Zeile vorkommt, entnimmt das Programm als zweiten Part für die Geschwisterbeziehung den Namen bzw. die daraus abgeleitete ID saladin aus der vorherigen Zeile: <relation name=""siblings"" mutual=""#sittah #saladin"" /> Wenn die beiden benötigten IDs für das Erstellen der Figurenrelation feststehen, wird die Art der Relation durch das Auslesen des Signalworts aus der Baumstruktur festgestellt. Danach werden daraus die Zeilen mit den Figurenrelationen erstellt und diese anschließend in die jeweilige TEI-Version des Textes geschrieben. Befindet sich in einer Zeile eine zweite Figuren-ID, bezieht sich die Zeile nicht auf eine vorangegangene, sondern stellt selbst den zweite Bezugspunkt der Relation. Das ist beispielsweise bei der Figur ""Camillo Rota"" in <castItem corresp=""#camillo_rota"">Camillo Rota, einer von des Prinzen Räten.</castItem> Die erste erkannte ID ist camillo_rota, die zweite der_prinz, abgeleitet aus ""des Prinzen"". Die IDs werden in gerichtete Relationen mit aktivem und passivem Part überführt: <relation name=""associated_with"" active=""#camillo_rota"" passive=""#der_prinz"" /> Das Programm arbeitet dabei ausschließlich mit den IDs. Dafür ist es nicht nötig, dass Figurennamen explizit als Namen oder Adelstitel als Titel erkannt werden. Es geht ausschließlich darum aus den einzelnen Wörtern einer Zeile im Figurenverzeichnis Namen bzw. Namensteile und Titelangaben herauszufiltern, die den IDs entsprechen, um die Zeilen einer oder mehreren Figuren zuordnen zu können. Um auch IDs zu erkennen, die sich geringfügig von den Namensnennungen im Figurenverzeichnis unterscheiden, überprüft das Programm pro Wort eine Reihe an Varianten. So trennt es beispielsweise vom oben gennannten Wort ""Prinzen"" das Suffix ab und überprüft, ob ein Artikel Teil der ID ist. So kann ""des Prinzen"" der ID ""der_prinz"" zugeordnet werden. In manchen Fällen funktioniert diese Abwandlung aber nicht so reibungslos. In Um die Methode zu evaluieren, wurden die automatisch erzeugten Relationen manuell nachkorrigiert und so ein Goldstandart erzeugt. Im Schnitt bearbeiteten die Korrektoren 12 Texte pro Stunde. Beim Abgleich der automatisch erzeugten Ergebnisse mit dem Goldstandart lag der Macro-Average-Recall Wert bei 0,3 (Standardabweichung: 0,3) und der Wert von Macro-Average-Precision bei 0,55 (Standardabweichung: 0,4), was einen Macro-Average-F-Score von 0,49 (Standardabweichung: 0,25) ergibt. GerDraCor ist ein deutsches Dramenkorpus, das nach TEI-P5 Standarts kodiert ist und im Dezember 2019 474 Dramen enthält, die im Zeitraum von 1730 bis 1940 veröffentlicht wurden (Fischer u.¬†a. 2019). Es ist Teil des größeren DraCor (Fischer u.¬†a. 2019), das als Im Rahmen der manuellen Nachkorrektur wurden außerdem interessante Fälle identifiziert. So wird etwa eine Gruppe von Figuren in dem oben abgebildeten Figurenverzeichnis von Schillers Wir stellen im folgenden zwei Analysen vor, in denen von den automatisch extrahierten Relationen Gebrauch gemacht wird, sowohl eine Einzeltext- als auch eine Korpusanalyse. Diese illustrieren Möglichkeiten, die Relationen in der Textanalyse zu berücksichtigen. Im ersten Beispiel betrachten wir Shakespeares  Auch wenn Abbildung¬†3 eine gewisse Symmetrie suggeriert, ist diese keineswegs gegeben wenn wir die Redeanteile nach Familien aufschlüsseln, wie es aus den Annotationen ebenfalls direkt möglich ist. Abbildung¬†4 zeigt die aggregierten Redeanteile der Figuren, wobei Figuren, die durch Verwandtschaft oder Arbeitsverhältnis zu einer der Familien gehören, zusammengefasst wurden (mit Ausnahme von Mercutio und Paris, die beide mit dem Prinzen verwandt sind). Es zeigt sich, dass Angehörige der Familie Capulet etwas weniger als doppelt so viele Wörter äußern als Angehörige der Familie Montague. Betrachtet man das annotierte Gesamtkorpus stellt man fest, dass die Relationen ungleich verteilt sind. Während Ehen/Verlobungen, Elternschaft und sonstige Assoziationen relativ häufig vorkommen, spielen Geliebte, sonstige Verwandtschaften, Freundschaften und Geschwister eine vergleichsweise kleine Rolle. In Abbildung¬†6 sehen wir die Anzahl der Relationen bestimmter Typen ins Verhältnis gesetzt zur Großgattung (Komödie/Tragödie). Dabei wurden die Angaben auf den Titeln der Dramen übernommen und leicht vereinheitlicht (z.B. Bürgerliches Trauerspiel Eine Verteilung der genannten Relationen nach Autor zeichnet jedoch ein anderes Bild (Abbildung 7). Bestimmte Autoren, vor allem Ludwig Anzengruber (1839-1889) und Johann Nestroy (1801-1862), haben klare Tendenzen dazu, mehr Relationen im Figurenverzeichnis zu nennen. Beide verfassen tendenziell Possen und Komödien. Mit den von uns bereitgestellten maschinenlesbaren Informationen ermöglichen wir Analysen dramatischer Figuren, die die als bekannt vorausgesetzten Informationen im Figurenverzeichnis mit berücksichtigen können. Neben den oben skizzierten Analysen können die Informationen auch in inhaltliche Analysen einfließen und etwa die soziale Nähe mit der Bühnennähe korrelieren o.ä. Kontextfreie Grammatiken haben sich hier 'trotz der bekannten Schwächen im Bezug auf natürliche Sprache 'als effizienter Formalismus herausgestellt, um die Figurenverzeichnisse maschinenlesbar zu machen. Wir halten dieses Verfahren für geeignet, um auch in anderen Kontexten mit semi-strukturierten Textdaten zu arbeiten, wo aufgrund der begrenzten Menge ein maschinelles Lernverfahren nur bedingt zum Einsatz kommen kann."
2020,DHd2020,168_final-GIUS_Evelyn_Computationelle_Textanalyse_als_fünfdimensionale.xml,Computationelle Textanalyse als fünfdimensionales Problem,"Evelyn Gius (Technische Universität Darmstadt, Deutschland)","Komplexität, Phänomene, Erkenntnisinteresse","Modellierung, Theoretisierung, Bewertung, Text","In diesem Beitrag wird ein Modell vorgestellt, das zu einer Einschätzung der Komplexität von Forschungsansätzen dient, die sich Texten mit computationellen Analysen nähern. Das Modell wurde vor dem Hintergrund der (literaturwissenschaftlichen) Analyse von literarischen Texten entwickelt, es ist jedoch 'ggf. mit leichten Anpassungen 'für Textanalysen generell geeignet. Die Komplexität von Digital Humanities-Projekten ist bestimmt von der Aushandlung von Vorannahmen, Methoden, der Passung zum Gegenstand, der konkreten interdisziplinären Zusammenarbeit, die fachlich, persönlich und oft auch karrierestrategisch eine große Herausforderung für die Beteiligten sein kann, bis hin zur Darstellung von Ergebnissen für eine oder mehrere Forschungscommunities. Neben Fragen der Projektplanung und -steuerung, wissenschaftspolitischen und wissenschaftskommunikativen Aspekten geht es auch um Fragen, die das eigentliche Forschungsgeschehen betreffen. Dieses wird aktuell in Bezug auf seine Relevanz und Ausrichtung diskutiert: Eine harsche Kritik von Nan Z. Da (2019a) an den Verfahren der DH initiierte eine mit dem etwas überzogenen Begriff ""Digital Humanities War"" bezeichnete Auseinandersetzung. Diese Auseinandersetzungen gehen zum Großteil an den eigentlichen Forschungszugängen vorbei. Dabei wäre es aus Sicht der Digital Humanities Ausgangspunkt des Modells sind die drei Aspekte, die für jede computergestützte Textanalyse wesentlich sind: Die Phänomene, denen das Interesse gilt, die Texte, die untersucht werden, und die Art, wie Erkenntnis erzeugt wird. Eine Einschätzung der Phänomene, die in einer computationellen Textanalyse untersucht werden, kann anhand der Phänomenbeschreibung stattfinden. Für diese kann man fragen: Wird das Phänomen als einfach, nicht weiter unterteilt, oder als aus mehreren Phänomenen zusammengesetzt betrachtet? Dabei geht es wohlgemerkt nicht um eine allgemein gültige Definition des entsprechenden Phänomens, sondern um die von den Forscher*innen genutzte Beschreibung. Beschreibungen für dasselbe Phänomen können in unterschiedlichen Forschungsprojekten entsprechend unterschiedlich ausfallen. In Bezug auf ein aktuelles Forschungsprojekt zu Gender und Krankheit in literarischen Prosatexten Neben der Bestimmung der Teile, aus denen eine Phänomenbeschreibung zusammengesetzt ist, geht es auch um die Frage, welches Wissen zur Bestimmung des Phänomens herangezogen werden muss. Dies kann zum einen Wissen sein, das der Text vermittelt. Aber es kann auch weiteres Wissen nötig werden, wie etwa spezielles Domänenwissen, zusätzliches (innerfiktionales oder außerfiktionales) Weltwissen u.ä. Die Kernfrage ist entsprechend: Braucht man über das Textwissen hinausgehendes weiteres Wissen, um ein Phänomen zu identifizieren? Auch hier gilt: Die Einstufung der Komplexität gilt für den betrachteten Anwendungsfall, andere Fälle haben ggf. für dieselben Phänomene andere Komplexitätsgrade. Im Projekt Gender und Krankheit wurde etwa mit Koreferenz-Auflösung experimentiert, die überwiegend auf Textphänomenen basiert. Das Krankheitskonzept wiederum wurde unter Rückgriff auf Wissen für zeitgenössische Krankheiten und Krankheitsbezeichnungen bearbeitet (etwa ""Phthise"" als Bezeichnung für Tuberkulose). Abbildung 1 stellt beispielhaft die beiden Dimensionen der Komplexität einiger Phänomene dar, die im Projekt Gender und Krankheit eine Rolle spielen. Oft wird vorschnell angenommen, dass für die textorientierten Digital Humanities die nun wesentlich größere Menge an untersuchten Texten distinktiv ist. Dabei ist die Frage, ob es sich um 'vermeintliche 'Big Data handelt oder nicht, aus Sicht der computationellen Textanalyse nur insofern interessant, als damit die Frage zusammenhängt, ob man die Texte, die man analysiert, kennt bzw. kennen kann oder nicht. In Bezug auf die Komplexität der genutzten Texte relevanter ist hingegen die umfassendere Frage: Wie viele (wie) verschiedene Texte werden analysiert? Dabei fällt unter Heterogenität von Texten die Anzahl der Texte selbst, aber auch die Anzahl von verschiedenen Texteigenschaften, die für die Fragestellung relevant sind bzw. sein könnten. Im Fall literarischer Texte sind das typischerweise Eigenschaften wie Gattung, Genre, Epoche, Autorgender, Erscheinungsort etc. Die Textheterogenität reicht von einem Text bis zu sehr vielen, sehr heterogenen Texten reicht In der Komplexitätsdimension des Analysemodus geht es darum, wer die Erkenntnisse produziert. Hier sind die beiden Möglichkeiten recht offensichtlich: Auf der einen Seite steht (menschliches) Lesen, auf der anderen Seite maschinelles Erschließen. Die Hauptfrage ist also: Wird die Textbasis durch Menschen oder durch Computer erschlossen? Dabei wird für alle Zugänge als gegeben vorausgesetzt, dass der Computer genutzt wird. Während das Lesen in Annotationen von Textstellen oder zumindest in die Ergänzung der Texte um Metainformationen resultiert, wird beim maschinellen Erschließen im Normalfall Textmining betrieben. Beide Textzugangsarten können weiter differenziert werden nach der Interpretationstheorie (etwa in text-, leser- oder autororientierte Zugänge) bzw. dem angewendeten maschinellen Verfahren (etwa in regelbasierte und Lernverfahren). In konkreten Forschungsprojekten kommen fast immer beide Modi vor. So werden im Projekt Gender und Krankheit manuelle Annotationen von Textpassagen und halb-automatische Verfahren zur Wortfeldgenerierung für die weitere Verarbeitung oder die Methodenentwicklung mit automatischen Verfahren zur Figurenerkennung, Segmentierung und Sentimentanalyse kombiniert. Da die Zwischenschritte in der Analyse zumeist manuell überprüft und teilweise ergänzt werden, handelt es sich hier um ein Verfahren zwischen Lesen und automatischem Erschließen und damit um eine eher geringe Komplexität. Schließlich geht es bei der Betrachtung von computationellen Textanalysen auch darum, wie der Computer eingesetzt wird, um Erkenntnisse zu generieren. Wenn man von der literaturwissenschaftlichen Praxis der Textanalyse ausgeht, ist die komplexeste Aufgabe jene, die Textbasis insgesamt im Hinblick auf die gewählte Fragestellung zu interpretieren. Interpretation ist jedoch bislang nicht der Fokus computationeller Zugänge zu literarischen Texten. Trotzdem lohnt es sich, Interpretation als ein Extrem der Dimension der Erkenntnis zu denken. In Anlehnung an die literaturwissenschaftliche Praxis kann man die Komplexitätsdimension des Erkenntnisbeitrags computationeller Analysen als von der Analyse des Textes für ein erstes Textverständnis bis hin zur Interpretation der Textbasis als Ganzes ausgedehnt sehen. Unabhängig von der Frage, welche Systematik man für die Tätigkeiten verwendet, die mit Textverstehen befasst sind, ist die zentrale Frage in der letzten Komplexitätsdimension: Wie weit geht der Erkenntnisbeitrag der computationellen Methode? Es geht also um die Frage nach der Neuheit des computationell Erforschten. Grob kann man die Komplexitätsstufen des Erkenntnisbeitrags wie folgt erfassen: Werden in einer deduktiven bzw. einfachen Textanalyse aufgrund von bestehenden Hypothesen bzw. Regeln (also bestehenden Analysekategorien und -verfahren) durchgeführt, werden aus der Betrachtung von Texten neue Analysekategorien oder auch Taxonomien entwickelt oder handelt es sich um Hypothesen über größere Zusammenhänge in den Texten, also um ihre Interpretation? Bei der Auseinandersetzung mit der Komplexitätsdimension des Erkenntnisbeitrags ist zu beachten, dass in einer typischen literaturwissenschaftlichen Textanalyse meist alle Modi vorliegen und fließend ineinander übergehen. Für die Komplexitätseinschätzung ist relevant, welche Modi davon computationell unterstützt werden sollen. Im Fall des Projekts zu Gender und Krankheit soll etwa deduktiv die Veränderung der Figurenkonstellation anhand der Figurennennungen analysiert werden. Ein induktives Verfahren liegt vor, wenn Genderkategorien durch Clustering von Figurenrede herausgearbeitet werden (die dann wieder deduktiv in der Analyse genutzt werden). Und schließlich liegt ein abduktiver Zugang vor, wenn durch eine Gesamtbetrachtung ein neues Element entdeckt würde, das Figurenkrankheit beeinflusst. Wie bereits dargelegt, betrifft die Bestimmung der Komplexität in den fünf Dimensionen primär die normativen Setzungen durch die Forscher*innen. Ausschlaggebend ist weniger, wie Texte, Phänomene und Erkenntnis an sich modelliert werden Für die Betrachtung und Kritik eines Zugangs sollten alle fünf Dimensionen berücksichtigt werden. Damit vermeidet man auch vorschnelle Kritik, die sich auf eine einfache Modellierung einer Dimension beschränkt und den Zugang insgesamt als unterkomplex betrachtet, obwohl er in einer oder mehreren anderen Dimensionen Erhebliches leistet. Darüber hinaus eignet sich das Modell als Instrument für den Entwurf eines Zugangs. Es kann in allen Phasen computationeller Textanalyse genutzt werden 'vom Design des Forschungszugangs zu Beginn der Forschungsarbeit über die wiederholten Bestandsaufnahme oder Nachjustierung im Projektverlauf bis hin zur Einordnung der erzielten Ergebnisse am Ende und der Reflektion des gesamten Prozesses. Abschließend seien noch einmal die fünf Dimensionen mit ihren Kernfragen dargestellt:"
2020,DHd2020,141_final-FISCHER_Frank_Besuch_im__Marstheater____Eine_Netzwerkmodelli.xml,"Besuch im ""Marstheater"" 'Eine Netzwerkmodellierung von Karl Kraus' Riesendrama ""Die letzten Tage der Menschheit""","Frank Fischer (Higher School of Economics, Moskau, Russland); Anna Busch (Universität Potsdam); Angelika Hechtl (WU Wien); Peer Trilcke (Universität Potsdam); Andreas Vogel (Hamburg)","Karl Kraus, Drama, TEI, Netzwerkanalyse","Beziehungsanalyse, Modellierung, Veröffentlichung, Visualisierung","Karl Kraus' Endzeitdrama ""Die letzten Tage der Menschheit"", 1919 zum ersten Mal vollständig erschienen (Buchausgabe 1922), ist in vielerlei Hinsicht inkommensurabel. Der schiere Umfang sprengt alle Gattungsnormen (638 Seiten in der ""Volk und Welt""-Ausgabe von 1978). Die fünf Akte plus Vorspiel und Epilog sind in 220 Szenen unterteilt, es gibt je nach Zählweise um die 1.000 sprechende Figuren bzw. Instanzen (zum Vergleich: als nächstgrößtes deutschsprachiges Drama gilt Grabbes ""Napoleon oder Die hundert Tage"" von 1831 mit 259 Figuren). Die Zählweise ist nicht nur deshalb kontingent, weil es zahlreiche Rufe aus der Menge gibt, die sich nicht quantifizieren lassen (wozu vor allem auch das undurchsichtige Stimmengewirr im Epilog gehört), sondern auch, weil es konkrete Gruppierungen wie die ""Fünfzig Drückeberger"" (III/26) oder ""Die zwölfhundert Pferde"" (V/55) gibt, die man theoretisch quantifizieren könnte, auch wenn dies nicht unmittelbar sinnvoll erscheint. Insgesamt spricht man tatsächlich besser von Sprecherinstanzen, die von historischen Personen über namenlose Zwischenrufer und allegorische Figuren (etwa den ""Hyänen, die Menschengesichter tragen"") bis hin zur ""Stimme Gottes"" reichen. Es ist nicht nur auf das Thema des Stücks bezogen 'die Apokalypse des Ersten Weltkriegs –, sondern auch auf die Form, wenn Kraus im Vorwort schreibt: ""Die Aufführung des Dramas, dessen Umfang nach irdischem Zeitmaß etwa zehn Abende umfassen würde, ist einem Marstheater zugedacht. Theatergänger dieser Welt vermöchten ihm nicht standzuhalten."" (Kraus 1978, S.¬†5) Die Handlung der Tragödie ist ""unmöglich, zerklüftet, heldenlos"" (ebd.) und erschwert jede Absicht, das Stück darzustellen, zumal vollständig. Dies betrifft sowohl Inszenierungen auf der Bühne oder als Hörspiel (obwohl es schon Kompletteinspielungen gibt) als auch digitale Modellierungen der Figurenbeziehungen. Es ist Konsens innerhalb des Forschungszweigs der Netzwerkanalyse dramatischer Texte, dass sich eine Einzelanalyse der verhältnismäßig übersichtlichen Figurennetzwerke selten lohnt. Das Augenmerk liegt daher normalerweise auf der Untersuchung struktureller Entwicklungen hunderter oder tausender Stücke über verschiedene historische Zeiträume (Algee-Hewitt 2017, Trilcke/Fischer 2018). ""Die letzten Tage der Menschheit"" gehören hier zu den Ausnahmen. Ziel dieses Projekts ist es, das Stück als soziales Netzwerk zu visualisieren, basierend auf Kookkurrenzen von Sprecherinstanzen in den einzelnen Szenen. Voraussetzung dafür ist eine brauchbare Formalisierung des Gesamttextes. Dieser ist einerseits bereits digitalisiert, in annehmbarer Qualität innerhalb des Projekts Gutenberg-DE (obwohl es in dieser Version kaum eine Seite ohne zumindest kleinere OCR-Fehler gibt). Andererseits gibt es noch keine digitale Fassung in einem Format, das die wissenschaftliche Auswertung ermöglicht. Am Beginn dieses Projekts stand daher die Herstellung einer TEI-Version des Dramas, die vor Konferenzbeginn veröffentlicht wurde und damit der wissenschaftlichen Community zum ersten Mal eine Version des Textes zur Verfügung stellt, die auf die FAIR-Prinzipien setzt (findable, accessible, interoperable, reusable). Neben einem Qualitätssprung hinsichtlich der Textbasis im Vergleich zur Gutenberg-DE-Version stand dabei die Auszeichnung der Sprecher-IDs im Mittelpunkt. Da, wie bereits angedeutet, diese Auszeichnung kontingent ist, also je nach Formalisierungsentscheidung anders aussehen kann, wird dieser Prozess offengelegt. So werden etwa die Vielzahl an Stimmen aus Menschenmengen oder die Unzahl ausrufender Zeitungsverkäufer nachvollziehbar individualisiert, speziell die Massenszenen in Wien, etwa die Geschehnisse an der Sirk-Ecke, die das Vorspiel und jeden der fünf Akte eröffnen. Ergebnis ist ein visualisiertes Netzwerk, das auf einem Poster im A0-Format einen Blick ins Kraus'sche ""Marstheater"" erlaubt, auf die schiere Masse der Auftritte und Stimmen, aus der doch eine Struktur hervorscheint, wie sie bisher im Kontext der Kraus-Forschung noch nicht visualisiert worden ist. So werden viele ""innere Symmetrien"" sichtbar (Matala de Mazza 2018), die das Stück strukturieren, wiederkehrende Konstellationen wie etwa die vier Offiziere am Beginn jedes Aktes oder die Szenen in der Schulklasse (I/9 und V/23). Deutlich wird im Netzwerkgraph auch die Diskrepanz zwischen Front und Heimat, zwei Welten für sich, wobei Kraus den Fokus auf die entlarvende Sprache von nicht direkt am Krieg beteiligten Personen legt: ""Wenn nicht Krieg wär, möcht man rein glauben, es is Friede."" (Kraus 1978, S.¬†95) Da der Text nunmehr als Volltext-TEI-Dokument vorliegt, lässt sich auch der Word Space in das Netzwerk hineinmodellieren, d.¬†h., die Anzahl der Wörter pro Sprecherinstanz. Auf diese Weise scheinen deutlich die (quantitativ gesehen) Hauptfiguren dieses ""heldenlosen"" Dramas auf (etwa der ""Nörgler"" und der ""Optimist"" sowie der ""Patriot"" und der ""Abonnent""), die oft über dutzende Seiten als Zweierkonstellationen auftreten, die aber darüber hinaus, wie der Graph verdeutlicht, auch anderweitig vernetzt sind. Um auch komparatistische Aspekte abzudecken, werden auf dem Poster vergleichend einige Netzwerkmetriken präsentiert, um die Gigantomanie des Dramas mit Zahlen zu verdeutlichen. Zur Gewährleistung der Nachnutzbarkeit und Nachhaltigkeit der Modellierung wurde das Stück auch dem German Drama Corpus hinzugefügt ("
2020,DHd2020,280_final-HERRMANN_J__Berenike____hungere_schon_nach_dem_nächsten_Band.xml,... hungere schon nach dem nächsten Band. Eine Untersuchung von Metaphern für Leseerfahrungen in Web 2.0 Literaturrezensionen,"Berenike Herrmann (Universität Basel, Schweiz); Thomas Messerli (Universität Basel, Schweiz)","Metaphern, Social Reading, Web 2.0, Leser, Literaturkritik, Metaphernidentifikation","Modellierung, Annotieren, Stilistische Analyse, Sprache, Methoden, Text","Kaum ein geisteswissenschaftlicher Forschungsgegenstand hat eine so intensive Diskussion erfahren wie die Metapher (Eggs, 2000). Doch existieren nur wenige Ansätze zu ihrer Formalisierung innerhalb der Digital Humanities. Unser Beitrag stellt einen einfachen Ansatz der Metaphernanalyse auf grösseren Datenmengen vor, um auch in nicht-annotierten Texten metaphorische Mappings zu finden. Mit diesem Ansatz analysieren wir konzeptuelle Strukturen des Leseerlebens von Laien-RenzensentInnen. Mittels einer Verschränkung korpusbasierter und korpusgetriebener Methoden (Tognini-Bonelli, 2001) untersuchen wir ein Korpus von Laienrezensionen (ca. 1,3 Mio Beiträge) explorativ auf den Metapherngebrauch mit der Zieldomäne ""Leseerleben"". Metaphern werden mit der Kognitiven Theorie der Metapher (KTM) als Denk- bzw. Erfahrungsfiguren (Lakoff & Johnson, 1980, S. 4) gefasst. Ausgehend von Befunden zu Laienbuchrezensionen im Englischen (Stockwell, 2009; Nuttall & Harrison, 2018) und zu feuilletonistischen Rezensionen (Köhler, 1999) operieren wir auf der Sprachoberfläche und inferieren von dort konzeptuelle Mappings zwischen Ziel- und Quelldomänen (Herrmann, im Druck; Shutova, 2017; Steen et al., 2010), wobei besonderes Augenmerk auf das Mapping LESEN IST NAHRUNGSAUFNAHME gelegt wird. Ausgangspunkt ist der Befund Nuttall und Harrisons (2018), dass Nahrungsmetaphern in englischsprachigen Das LoBo-Korpus (extrahiert von der Social Reading-Plattform ""Lovelybooks"") beinhaltet ca. 1,3 Mio. deutschsprachige Laienrezensionen von 54.000 NutzerInnen, die sich auf jeweils ein Buch beziehen. Die Bücher sind kategorisiert nach 15 Genres, die der Plattform selbst entnommen sind. Das Korpus ist PoS-annotiert (Tree-Tagger), lemmatisiert und in CWB ( Angesichts der Herausforderungen einer reliablen automatischen Metapherndetektion (Veale, Shutova, & Klebanov, 2016) wählen wir bewusst eine korpusstilistische Herangehensweise (Deignan & Semino, 2010). Wir verschränken als induktiven Schritt A Kookurrenzanalyse und manuelle Identifikation mit einem deduktiven Schritt B (regelbasierte Suche nach spezifischen Quelldomänen-Indikatoren). Ziel ist eine möglichst hohe Vollständigkeit und Genauigkeit der Identifikation potenzieller Metapherntypen, wobei eine formale Evaluation der Methode im gegenwärtigen Stadium mangels Goldstandard jedoch nicht möglich ist. Um in Schritt A die Metaphern zu finden, die sich auf Leseerleben beziehen, müssen zunächst Objekte des Leseerlebens (OdL) identifiziert werden. OdL sind Referenten des Leseerlebens ( Ergänzend zur Korpusanalyse annotieren wir eine Stichprobe auf metaphorischen Sprachgebrauch (Herrmann, Woll, & Dorst, 2019). Unsere erste Fallstudie untersuchte insgesamt 18 randomisiert ausgewählte Rezensionen zu sechs Büchern (je drei pro Buch). Dieses Subkorpus enthält zu gleichen Teilen Rezensionen von ""anspruchsvollen Bestsellern"" und Fantasy-Romanen. Ziel war es, die Sequenz metaphorischer Ausdrücke sowie die lexikalische und konzeptuelle Variation abzuschätzen. In Schritt B untersuchten wir ausgehend von der Annahme eines systematischen Mappings LESEN IST NAHRUNGSAUFNAHME (ausgehend von entsprechenden Befunden durch Nuttall & Harrison, 2018, Köhler, 1999) die je hundert häufigsten Lemmata der drei ""Inhaltswortklassen"" Substantiv, Adjektiv und Verb auf mögliche Indikatoren. Innerhalb eines Fensters von zehn Wörtern um die in Schritt A festgelegten OdL (Ausdrücke, die Zieldomäne LESEN indizieren, z.B. Lemma (semantisches Feld: Essen & Trinken) Die Ergebnisse der Schritte 1 und 2 zeigen eine grosse Vielfalt metaphorischer Ausdrücke auf, die sich auf verschiedene Objekte des Leseerlebens beziehen. Aufschlussreich ist dabei nicht die absolute Häufigkeit der Metaphernkandidaten im Korpus 'zumal keine zuverlässigen Vergleichsdaten zur Verfügung stehen –, wohl aber die quantitative Analyse der relativen Verteilung auf Rezensionen verschiedener Ratings und Genres. Das Auftreten der hier untersuchten stark wirkungsbezogenen Metaphorik gibt etwa Aufschlüsse über Rezensionsmuster, die sich je nach quantitativer Bewertung und je nach literarischer Gattung unterscheiden. Die qualitative Untersuchung ermöglicht dagegen eine erste Typologie von Mappings, die wir im Folgenden mit Beispielen für konventionelle und kreative Metaphern illustrieren. Viele Ausdrücke sind erwartete, stark konventionalisierte Metaphern, wie etwa Es finden sich darüber hinaus aber auch viele Beispiele, die einen kreativen Umgang mit Metaphern illustrieren: Unsere Resultate zeigen bislang fünf verschiedene Typen von LESEN IST NAHRUNGSAUFNAHME auf. Lesen wird etwa (A) als eine Form von Nahrungsaufnahme konzeptualisiert, bei der Lesende als ""Essende"" und literarische Werke und deren Bestandteile als ""verzehrbar"" positioniert werden. Weiter wird (B) Schreiben als ""Kochen"" und ""Bewirten"" dargestellt, wobei Autoren als Köche, Lesende als Gäste und Lektüre als bekocht/bewirtet erscheinen. Darüber hinaus findet sich aber auch (C) ein anderes Mapping, das zwar auf die gleiche Quelldomäne zurückgreift, aber dem Objekt des Leseerlebens selbst als Agnes konstruiert. Andere Mappings (D) beziehen sich dagegen direkt auf die Objekte des Leseerlebens, werden sprachlich aber als Vergleich realisiert, der nach Steen et al. (2010) stärker intentional markiert ist. Schliesslich findet sich eine Reihe (E) von Mappings, die zwar auf die Quelldomäne NAHRUNGSAUFNAHME und Zieldomäne LESEN rekurrieren, sich dabei aber nicht auf Vorgänge der Nahrungsaufnahme, sondern auf das Embodiment von Emotionen zu beziehen scheinen. Unsere Studie leistet einerseits einen methodischen Beitrag zur Metaphernidentifikation mit einfachen korpusstilistischen Mitteln und gibt andererseits Aufschluss über die Produktivität der Quelldomäne NAHRUNGSAUFNAHME für die Konzeptualisierung von Leseerleben. Schliesslich zeigt unsere manuelle Annotation weitere Mappings auf, die den Umgang mit Objekten des Leseerlebens nicht als Nahrungsaufnahme konzeptualisieren, sondern als ""Reisen"" und ""Bewegung"", oder auch auch als ""Interaktion mit externen Kräften"". Bücher und Geschichten werden einerseits als ""Behälter"", andererseits wie ""Personen"" mit Qualitäten und Intentionen konzeptualisiert, ja als ""Freunde"" der Lesenden, wobei ""gegenseitige Kompatibilität"" als axiologischer Wert - situiert zwischen den inhaltlichen und (hedonistisch sowie praktisch) wirkungsbezogenen Werten nach Heydebrand und Winko (1996) - erscheint. Folgestudien sollen die Verbesserung der automatisierten Detektion leisten, unter anderem durch die Einbindung von semantischen Informationen aus GermaNet. So sollen durch systematische Untersuchung der häufigen konzeptuellen Metaphern und ihre Korrelation mit der Lovelybooks-Sterne-Wertung weitere Rückschlüsse auf zugrundeliegende Wertmassstäbe bei der Bewertung von online-Laienrezensionen ermöglicht werden."
2020,DHd2020,157_final-HALL_Mark_Die_Kanonfrage_2_0.xml,Die Kanonfrage 2.0,Corinna Dziudzia (KU Eichstätt - Ingolstadt); Mark Hall (Martin-Luther-Universität Halle-Wittenberg),"kanon, literaturgeschichte, portal","Theoretisierung, Community-Bildung, Crowdsourcing, Webentwicklung, Personen, Text","Maßgeblich leitete die amerikanische Literaturwissenschaft in den 1970er Jahren eine kritische Revision mit der Frage ein, wer eigentlich anhand welcher Kriterien entscheidet, welches literarische Werk zum Kanon gehört (vgl. Ziolkowski 2009).  Diese Kritik findet in einem anhaltenden Prozess des ""Entdeckens"" und ""Sichtbarmachens"" nichtkanonisierter Autor_innen und Werke Niederschlag (vgl. u.a. Brinker-Gabler 1978; Hilger 2015) und fördert ein breites Spektrum heterogenen Literatur- und Kunstschaffens zu Tage. Die im Rahmen der Kanondebatte im Wesentlichen formulierte Kritik am zu ""weißen"" und zu ""männlichen"" Kanon als ""Machtinstrument"" (Winko 1996, 500) wird auf der theoretischen Ebene mit dem Nachdenken über diskursive Machtpraktiken ebenso reflektiert wie mit der Konzeption des Archivbegriffs (vgl. Foucault 1990; Derrida 2009) und für den digitalen Raum erweitert. Abigail De Kosnik spricht entsprechend von einer potentiellen Verschmelzung von Kanon und Repertoire: [‚Ä¶] digital archives potentially redefine what A. Assmann calls a ""active memory"" and ""passive memory"", in the sense that these become highly individualized: all materials contained in an online database are equally available to the user 'no materials are any more ""hidden"" or ""stored away"" than any other materials, all materials that are indexed can be retrieved from the database 'and so users of an Internet Archive may ""activate"" whichever of the materials they wish, constructing their own personal canons based on the materials that they use. [‚Ä¶] Digital archives erect no physical barriers between categories of information, so conceivably any piece of information, any archived data, can enter into one person""s repertoire and canon; thus, there are as many possible canons as there are archive users, and no possibility for a single canon, achieved by a consensus of cultural archive users, that would be distinct from the culture""s archive. (De Kosnik 2016, 66) So wie durch die frühe Kanondebatte die grundlegende Frage gestellt worden ist, wessen Werke eigentlich publiziert, rezensiert und damit potentiell kanonisiert werden können, muss diese Frage heute aktualisiert werden: Wessen Werke werden wie digitalisiert, um im Sinne De Kosniks überhaupt derart aktiviert werden zu können? Die Kanonkritik verweist zudem auf grundlegende Reflexionen der Wissenschaftsgeschichte. Wesentlich ist hierfür etwa die Wiederentdeckung Ludwig Flecks und sein durch Thomas Kuhn verstärktes Betonen, wie sehr das jeweils tradierte Wissen Ausweis von Selektionsprozessen ist, das konkreten Rahmenbedingungen genauso wie Irrtümern und Denkzwängen unterliegt (Fleck 1980, 31; Kuhn 1996): Insofern die vorherige Generation das tradierenswerte Wissen in Lehrbüchern für die nachfolgende Generation auswählt, erscheint die jeweils angebotene bzw. fehlende Wissensrepräsentation aufschlussreich. Denn ungeachtet der seit mittlerweile Jahrzehnten geäußerten Kritik an der Homogenität des Kanons ist noch in jüngerer Zeit, etwa durch die Frauenforschung, festgestellt worden, wie überraschend wenig, teilweise gar nicht, schreibende Frauen in Form ihrer Namen und Werke aktuell in deutschen Leselisten, Literaturgeschichten und Lehrbüchern repräsentiert sind (vgl. Sylvester-Habenicht 2009). Die Stabilität des kleinen Kernkanons (vgl. Braam & Hagstedt 2017, 83), in dem sich immer noch vor allem männliche Autoren präsent zeigen, scheint sowohl durch die Debatte als auch die große Zahl an Entdeckungen vergessener schreibender Frauen (vgl. u.a. die Forschungsarbeiten Brinker-Gablers oder Becker-Cantarinos) wenig veränderbar, das Wissen um die Existenz der Texte ist nach wie vor marginal und auf kleine Expert_innenkreise beschränkt. In der digitalen Welt erweist sich Google Books zwar als umfangreiches Archiv der Texte auch von Autorinnen, stellt allerdings die Werke vorrangig als Scans zur Verfügung, weswegen diese für die automatisierte DH-Analyse weniger nutzbar sind, aber zumindest für die Lektüre verwendet werden können (vorausgesetzt, sie werden gefunden). Das wird ergänzt durch (allerdings nicht textverlässliche) Primärliteratur-Archive wie das Es gibt allerdings digitale Archivprojekte, welche das erklärte Ziel haben, die Arbeiten von Frauen sichtbarer zu machen, unter anderem das ""Women Writers Project"" (Connell et al. 2017), ""Orlando: Women""s Writing in the British Isles from the Beginnings to the Present"" (Booth 2017) oder ""DaSind - Die Datenbank Schriftstellerinnen in Deutschland, Österreich, Schweiz 1945-2008"" (Schulz 2008). Die ersten zwei sind jedoch kostenpflichtig und zielen nur auf englischsprachige Texte, und das dritte Projekt ist seit über einem Jahr nicht mehr online verfügbar (Stand: Dezember 2019). Digitale literaturwissenschaftliche Forschung der deutschen Literatur scheint entsprechend vorrangig mit jenen Digitalisaten unternommen zu werden, die prominent zur Verfügung stehen, leicht zugänglich sind und in entsprechend nutzbaren Formaten vorliegen, daher überrascht es nicht, dass sich die Digital Humanities tendenziell eines recht kleinen und männlichen Kanons deutscher Literatur bedienen (Hall, 2019). Um diese Schieflage zunächst aufzuzeigen und dann potentiell zu korrigieren, wurde das Das Projekt verfolgt vier Ziele: 1. Die Kanonfrage vor dem Hintergrund der Digitalisierung neu zu stellen 2. Lücken in der Digitalisierung von Werken außerhalb des Kanons aufzuzeigen 3. Den Zugang zu vorhandenen Digitalisaten zu vereinfachen, um die Schwelle zur Nutzung dieser Werke zu reduzieren 4. Autor_innen und ihre Werke als bisher eher marginalisiertes Wissen auch für die nicht-wissenschaftliche Öffentlichkeit zugänglich zu machen Um diese Ziele erreichen zu können, wird im Rahmen des Projekts ein Online-Portal entwickelt, zusammen mit den notwendigen Werkzeugen, die darin enthaltenen Daten zu verwalten. Eine Grundidee in der technischen Umsetzung ist es, nicht noch ein weiteres Archiv für Digitalisate bereitzustellen, sondern die in den verschiedenen existierenden Archiven vorhandenen Digitalisate mit allgemeinen Informationen über tendenziell vergessene Autor_innen zusammenzuführen. Der These folgend, dass Vieles ""unter der Oberfläche"" schlummert, wenngleich nicht immer in optimalen Formaten, geht es dem Projekt primär um das Sichtbarmachen dessen, was da ist und seien es zunächst nur die Namen von Autorinnen. Es geht dem Projekt nicht um die Digitalisierung oder Archivierung von Werken, die noch nicht vorliegen, vielmehr um das Aufzeigen von potentiell systematischen Leerstellen. Den Ausgangspunkt für das Vorhaben bildet eine erste Liste an Namen von Autor_innen, die von den Projektmitgliedern entwickelt wurde. Langfristig ist das Projekt so angelegt, dass aus der DH-Community Namen hinzugefügt werden können und das Portal so langsam wächst. Basierend auf den Namen werden Werke sowie weitere relevante Informationen in verschiedenen Archiven identifiziert. Zur Zeit werden dazu vor allem vier Quellarchive genutzt: Die Basierend auf den derart identifizierten Daten, wird dann das Online-Portal generiert. So vorhanden, wird dann im Rahmen des zweiten Projektziels für alle Autor_innen eine Liste der bekannten Werke geführt 'unabhängig davon, ob und in welcher Form diese digitalisiert sind. Daraus generiert das Portal eine Reihe von Statistiken, welche einen Überblick darüber geben, in welchem Grad die Werke einzelner Autor_innen, bzw. der Gesamtbestand, digital bereits aufgearbeitet sind. Diese Statistiken sind auch über das Suchsystem zugänglich, es ist also möglich, zum Beispiel nach Autor_innen zu suchen, welche während eines gewissen Zeitraums an einem bestimmten Ort gewirkt haben. Potentiell können darüber Verbindungen von Autor¬≠_innen in Form von Netzwerken erkennbar werden. Dies unterstützt Geisteswissenschaftler_innen nicht zuletzt in der Identifikation potentiell interessanter Forschungsfragen. Parallel dazu werden für alle Daten und Metadaten maschinenlesbare Versionen bereitgestellt. Dies unterstützt das dritte Projektziel, da die Daten des Portals nahtlos in digitale Arbeitsabläufe der DH-Forschung integriert werden können. Um die nicht-wissenschaftliche Öffentlichkeit anzusprechen (Johnson 2008), bietet das Portal eine Reihe von Funktionalitäten an. Es ist bekannt, dass es Nicht-Experten schwer fällt, erfolgreich zu suchen (Geser 2004; Wilson & Elsweiler 2010) und sie eine Präferenz für Browsing haben (Walsh et al. 2018). Daher entwickelt das Projekt eine Reihe von browsing-basierten Schnittstellen. Unter anderem ""ein Werk/eine Autorin des Tages"", welches entweder zufällig oder basierend auf Lebensdaten der Autorin ausgewählt und den Nutzern als Impuls vorgeschlagen wird. Auch sollen die Werke, basierend auf ihren Themen, automatisch gruppiert werden, damit Benutzer_innen durch die daraus entstehende Themenstruktur stöbern können. Zusätzlich wird das Portal eine Lesefunktion für jene Textdokumente anbieten, welche in den Quellarchiven in maschinenlesbarer Form vorhanden sind. Ziel ist es dabei nicht, eine Schnittstelle zur wissenschaftlichen Arbeit mit den Texten zu bieten, sondern eine Komponente, mit denen Texte wie ein gedrucktes Buch gelesen werden können. Die gewählte Lösung eines (weiteren) Portals birgt natürlich die Frage, wie macht man es sichtbar und warum sollten Nutzer_innen es nutzen? Die Sichtbarkeit innerhalb der DH-Community soll über Beiträge in Konferenzen und Zeitschriften erreicht werden. In einem ersten Pilotversuch wurde das Portal in der universitären Lehre zum Gegenstand eines Seminars mit Studierenden der germanistischen Literaturwissenschaft gemacht, darüber stellt sich idealerweise perspektivisch ein Multiplikatoreneffekt ein. Der Zugriff auf Nutzer_innen aus der nicht-wissenschaftlichen Öffentlichkeit ist natürlich wesentlich schwieriger. Es ist aber so, dass soziale Medien von den unterrepräsentierten Gruppen oft stark genutzt werden und wir sehen das als die primäre Methode, um eine breitere Sichtbarkeit des Projekts zu erreichen (McLean and Maalsen 2013). Die Problematik des Bias im Kanon kann natürlich nicht vom Projekt direkt gelöst werden. Unser Ziel ist es vielmehr, einen ersten Schritt zu unternehmen, um die Sensitivität für die Kanonfrage in den DH zu unterstützen und einen kritischen Diskurs zur Frage, welche Texte in welcher Form eigentlich digitalisiert werden, bzw. darüber hinaus, woran digitale literaturwissenschaftliche Forschung erfolgt, bzw. erfolgen kann, zu fördern. Durch eine breitere Aufstellung der für DH-Forschung genutzten Daten wäre entsprechend zu hoffen, dass sich der mutmaßlich bisher unbewußte Bias der Daten reduziert. Momentan wird digitale Forschung tendenziell an einem verengten und homogenen Kanon deutscher Literatur betrieben, während literarische Werke jenseits einer ""männlichen"" Auswahl, teils durch fehlende Digitalisate, teils durch mangelnde Sichtbarkeit, kaum berücksichtigt werden. Damit werden die Bemühungen der einzelnen Fachdisziplinen um Heterogenität und Diversität konterkariert, denn es betrifft nicht nur das Schreiben von Autorinnen, sondern ein breiteres Spektrum an unterrepräsentierten Gruppen. Langfristig ist das Ziel des Projekts, sich selbst unnotwendig zu machen, insofern der Bias in den digitalen Quellarchive behoben ist, aber bis dahin will das Projekt die Leerstellen sichtbarer und greifbarer machen."
2020,DHd2020,243_final-GUHR_Svenja_Doctoral_Consortium__Svenja_Guhr.xml,Raise your voice! - Über den Zusammenhang zwischen Lautstärkemerkmalen in literarischen Prosatexten und der Emanzipation der Frau von 1848 bis 1920,"Svenja Guhr (TU Darmstadt, Deutschland)","(Audio-)Narratologie, Deutsche Prosaliteratur, Digitale Philologie, Emanzipation der Frau, Lautstärke, Verba Dicendi","(Audio-)Narratologie, Deutsche Prosaliteratur, Digitale Philologie, Emanzipation der Frau, Lautstärke, Verba Dicendi","Mein im Rahmen des Die Untersuchungen bauen auf der Hypothese auf, dass in der Mitte des 19. Jahrhunderts Frauenfiguren in literarischen Prosatexten prozentual weniger, kürzere und leisere Redebeiträge zugeschrieben werden als männlichen Figuren, was sich jedoch mit der ansteigenden Emanzipation der Frau verändert. Das Projekt zielt darauf herauszufinden, ob sich Frauen- und Männerfiguren im Verlauf der betrachteten Zeitperiode in ihrer Anzahl an Redebeiträgen und ihrer Lautstärke annähern. Die steigende Lautstärke zeichnet sich dabei durch eine ""lautere"" Beschreibung von Redebeiträgen aus, die u.a. durch als ""lauter"" wahrnehmbare redeeinleitende Verben gekennzeichnet sind. Lautstärke wird dabei als ein narratologisches Element betrachtet, das in der Literaturwissenschaft bisher nur wenig Aufmerksamkeit erhalten hat. Der Umgang mit Erzählformen und Diskursen als ein wichtiges Kriterium der Narratologie fand seine Erweiterung durch das neue Forschungsfeld der Audionarratologie. Diese widmet sich u.a. der Relation zwischen Narrativen und den in der Lesevorstellung bei der stillen Lektüre erlebten Geräuschen, Tönen und Dynamik (Mildorf / Kinzel 2016; Kuzmiƒçov√° 2013). Literarische Texte beinhalten neben Beschreibungen von natürlichen und industriellen Geräuschen (z.B. Natur- und Maschinengeräusche) auch Wiedergaben von Figurenrede. Insbesondere in Prosa ordnen Autoren und Autorinnen (i. F. generisches Femininum) ihren Figuren durch beschreibende Einleitungen von Redebeiträgen Stimmen zu, die in den Gedanken von Rezipientinnen wahrzunehmen sind. Ein neuer Ansatz der Figurenanalyse findet in der Betrachtung von Tönen, Lautstärke und Stimmvolumen in literarischen Prosatexten Anwendung. Vor allem die redeeinleitenden Verben, die direkte wie indirekte Redebeiträge einleiten und somit die Art und Weise der Figurenrede beschreiben, ermöglichen den Rezipientinnen die Wahrnehmung von Figurenstimmen und -lautstärke, z.B. ob eine Figur schreit oder flüstert. In Anlehnung an Hunt (2017), die in ihrer Studie ein Korpus aus anglophoner Kinder- und Jungendliteratur auf stereotypische Rollendarstellungen untersuchte, die sie anhand der ""gendered nature"" von redeeinleitenden Verben herausstellte, wird auch in meinem Forschungsprojekt eine genderdifferenzierte Betrachtung dieser Verbgruppe vorgenommen. In Hunts Ausführungen stützt sie sich auf Caldas-Coulthards (1992) Unterscheidung von redeeinleitenden Verben in neutrale (z.B. Ziel der Lautstärkeuntersuchung ist es herauszufinden, ob es einen Zusammenhang zwischen der beschriebenen Sprechweise weiblicher Prosafiguren und der ansteigenden Emanzipation der Frau in der deutschsprachigen Gesellschaft ab der zweiten Hälfte des 19. Jahrhunderts bis zum Erhalt des deutschen Frauenwahlrechts 1919 gibt (erweitert um das Jahr 1920, damit die Auswirkungen der Einführung des Frauenwahlrechtes mit aufgenommen werden). Weitere Unterhypothesen beschäftigen sich mit dem Zusammenhang zwischen der beschriebenen Lautstärke einer Frauenfigur mit ihrem Bildungsstand sowie ihrer gesellschaftlichen Stellung. Weiterhin wird untersucht, ob Frauenfiguren in der Öffentlichkeit leiser beschrieben werden als im privaten Raum (vgl. Howe 2000). Darüber hinaus soll herausgestellt werden, ob Frauenfiguren in Anwesenheit von Männerfiguren leiser dargestellt werden als in der alleinigen Gesellschaft von Frauenfiguren, wobei untersucht wird, ob die plötzliche Anwesenheit auch nur einer Männerfigur in einem weiblichen Beisammensein die Art und Weise, in der Frauenfiguren miteinander sprechen, beeinflusst und ob diese Verhaltensänderung von der Beziehung der anwesenden männlichen Figur zu den Frauenfiguren (z.B. Vater, Bruder, Cousin, Fremder, Arbeitgeber, etc.) abhängt. Die Studie basiert auf der Analyse eines deutschsprachigen Korpus bestehend aus literarischen Prosatexten (Ziel: ca. 500). Das betrachtete thematische Korpus (vgl. Baker 2007: 26, Gür-SÃßeker 2014: 585) befindet sich aktuell (Stand: Januar 2020) noch in der Erstellungsphase, wobei auch auf existierende und teilweise bereits annotierte Korpora wie z.B. auf das Redewiedergabekorpus der Kooperation zwischen dem Leibniz-Institut für Deutsche Sprache, Mannheim und der Universität Würzburg (Brunner et al.) zurückgegriffen werden wird. Die strömungsübergreifend deutschsprachigen Prosatexte werden nach den folgenden Kriterien ausgewählt: deutschsprachig, Zeit der Publikation zwischen 1848 und 1920, Für einen ersten Analyseansatz wurde ein Probekorpus erstellt, das 80 deutschsprachige Prosatexte umfasst und in zwei vergleichbare Subkorpora unterteilt wurde (2x 40 Prosatexte). Bei der Erstellung wurden die Texte nach den zuvor genannten Kriterien ausgewählt, wobei der Fokus auf die zwei Zeiträume 1865-75 und 1885-95 gelegt wurde, in denen jeweils eine überregional bedeutende Frauenrechtsaktion stattfand (Richards 2004). Das Probekorpus diente als Grundlage zur Entwicklung einer regelbasierten Methode, mit deren Hilfe redeeinleitende Verben sowie die sie umgebenden Adjektive, Adverbien (z.B. Im Laufe des Dissertationsvorhabens sollen die Untersuchungen zur Lautstärkewertzuweisung zu redeeinleitenden Verben in einem umfangreicheren und wissenschaftlich fundierten Verfahren wiederholt werden. Zudem werden methodische Herausforderungen wie die Auflösung von Koreferenzen und Anaphern sowie die Erkennung von (unregelmäßigen) Redebeiträgen, Szenengrenzen und Figurenkonstellationen einen großen Bestandteil meiner Forschung einnehmen."
2020,DHd2020,233_final-HORSTMANN_Jan_Netzwerkanalyse_spielerisch_vermitteln_mit_Dra.xml,"Netzwerkanalyse spielerisch vermitteln mit DraCor und forTEXT: Zur nicht-digitalen Dissemination einer digitalen Methode in Form des Kartenspiels ""Dramenquartett""","Jan Horstmann (Universität Hamburg, Deutschland); Marie Flüh (Universität Hamburg, Deutschland); Mareike Schumacher (Universität Hamburg, Deutschland); Frank Fischer (Higher School of Economics Moskau, Russland); Peer Trilcke (Universität Postdam, Deutschland); Jan Christoph Meister (Universität Hamburg, Deutschland)","niedrigschwellige Dissemination, Kartenspiel, Lerntypen","Netzwerkanalyse, Einführung, Lehre, Visualisierung, Interaktion, Literatur","Mit ELTeC (European Literary Text Collection; Das DFG-Projekt Die Videos vermitteln die Methode über eine Text-Bild-Audio-Kombination: Das Methodenvideo bietet eine Fallstudie zum Figurennetzwerk von Angesprochen werden hier theoretische, strukturelle und emotionale autodidaktische Vermittlungsmuster (zur Bedeutung von Emotionen für autodidaktisches Lernen vgl. Mega u.a. 2014). Auf der Tonebene ist ein erklärender Duktus vorherrschend. Die ,selfmade""-Anmutung der Videos vermittelt, dass die autodidaktische Erarbeitung der Inhalte Betrachter√Ønnen und Ersteller√Ønnen des Videos miteinander verbindet (vgl. Horstmann & Schumacher 2019). Die Tutorial-Reihe schließlich funktioniert ähnlich wie die Lerneinheit als Schritt-für-Schritt-Anleitung und bietet die Möglichkeit, die Arbeit mit Gephi als Screencast zu erlernen. Das Tutorial-Video zur Nutzung des DraCor-Tools ezlinavis verknüpft die praktische Erstellung von Netzwerken mit der Nutzung der Ressource TextGrid Repository (vgl. Horstmann 2018) und den Methoden Named Entity Recognition (vgl. Schumacher 2018a) und Annotation in CATMA (vgl. Jacke 2018 und Schumacher 2019a). Die Dissemination einer digitalen Methode wie der Netzwerkanalyse durch ein nicht-digitales Kartenspiel bietet Möglichkeiten, die die bisher genannten digitalen Medien nicht abdecken konnten. Die Spieler√Ønnen werden in einer nicht-digitalen Umgebung mit den funktional reduzierten Ergebnissen einer digitalen Analyse konfrontiert, können diese visuell und haptisch erfahren und spielerisch explorieren. Der empfohlene Spielmodus ist ,Supertrumpf"" Die im Folgenden vorgestellte, reflektierte und erprobte Pipeline geht von einer ersten theoretischen Annäherung durch forTEXT-Tutorials aus, auf die eine spielerische Vertiefung der spezifischen Objektkonstitution qua Netzwerkanalyse und der entsprechenden Metriken mittels des Dramenquartetts folgt. Anschließende Arbeitsphasen könnten, wie in 3. skizziert, z.¬†B. die formalisierte Erstellung, Gestaltung und Analyse von Dramennetzwerken mittels ezlinavis und Gephi oder die konkrete Bearbeitung von literarhistorischen Forschungsfragen mittels DraCor umfassen. Der quantifizierende Zugriff auf Dramentexte kann als ""radikale ,Anästhetisierung"" der Objekte"" (Trilcke, im Erscheinen) beschrieben werden. Auf die qua Formalisierung erfolgende Anästhetisierung, bei der die ursprüngliche ästhetische Dimension des literarischen Kunstwerks zunächst ausgesetzt wird, folgt jedoch eine reästhetisierende Transformation im Zuge der Diagrammatisierung (vgl. ebd.). Ein entscheidender Vorteil digitaler Diagramme ist die Möglichkeit der Interaktion (vgl. Horstmann, im Erscheinen): Netzwerke lassen sich je nach Wahl des Layoutalgorithmus unterschiedlich darstellen, ein semantischer Zoom ermöglicht überdies, zusätzliche Informationen des Ausgangsmaterials zu visualisieren. Dramennetzwerke in einer festgelegten (und damit nicht mehr veränderbaren) Form als Spielkarte zu drucken, bedeutet daher in erster Linie eine funktionale Reduktion. Gerade diese funktionale Reduktion eröffnet jedoch didaktische Spielräume: Das Wissen, dass die abgedruckten Netzwerke ebenfalls in digitaler Form vorhanden und dort sogar manipulierbar sind, wird im Laufe des Spielprozesses die Neugier auf diese Funktionsvielfalt steigern, sodass der Übergang in die ,digitale Arbeit"" fließend stattfinden kann und nicht mehr als etwas kategorial anderes empfunden wird. Die Interaktion zwischen Benutzer√Ønnen und Netzwerken als konzeptioneller Bestandteil digitaler Netzwerkdarstellungen wird übertragen auf die Interaktion zwischen den Spieler√Ønnen, wodurch nicht zuletzt die von Jenkins (2006, 2) sog. Das Kartenspiel entfaltet seinen didaktischen Mehrwert auch, weil es situational gerahmt ist: Es wird in kollektiven Unterrichtsphasen eingesetzt, die darauf abzielen, sich einem abstrakten Unterrichtsgegenstand auf spielerische Weise anzunähern. Da Menschen in ihrer Rolle als Im Fokus steht der Versuch, nicht nur kumulatives bzw. assimilatives Lernen zu initiieren, wodurch v.¬†a. begrenztes, anwendungsorientiertes Wissen oder thematisch, anwendungsorientiertes Wissen produziert werden würde (vgl. Illeris 2010). Die 'von der konkreten Kenntnis des Spielprinzips ,Supertrumpf"" unabhängige 'spielerische Aktivierung unterschiedlicher Sinneskanäle und die damit einhergehende Diskussion über Fachinhalte zielt auf die Einleitung akkommodativer und transformativer Lernprozesse und darauf, über Fachwissen in relevanten Kontexten frei verfügen zu können. Das im Wintersemester 2019/2020 an der Universität Hamburg durchgeführte Seminar ""Digitale Literaturwissenschaft und pädagogische Praxis"" hat unterschiedliche Standardverfahren und Werkzeuge erprobt, die gegenwärtig in der digitalen Literaturwissenschaft eingesetzt werden. Dieses Feld wird zunehmend auch für Lehrer√Ønnen insbesondere im gymnasialen Bereich relevant: Bereits die heutige Schüler√Ønnengeneration zählt zu den Um den Effekt des Dramenquartetts auf den Lernerfolg der Studierenden zu untersuchen, wurde eigens ein Testverfahren entwickelt, das die Wissensstände vor und nach dem Einsatz des Quartetts mess- und v.¬†a. vergleichbar macht. Das Verfahren setzt sich aus fünf aufeinander aufbauenden Phasen zusammen: (1) Vorbereitend befasst sich ein Teil der Lerngruppe mit schriftlichen forTEXT-Lernmaterialien zur digitalen Netzwerkanalyse, während der andere Teil die Video-Fallstudien und -Tutorials konsultiert. (2) Ausgangspunkt der Erhebung stellt folglich ein gruppenspezifisch relativ homogener Wissensstand dar, der grundlegende Kenntnisse über die Methode der digitalen Netzwerkanalyse beinhaltet. Um die Wissensstände beider Gruppen vor dem Einsatz des Quartetts zu erfassen, wurde eine Umfrage entworfen und zu Beginn des Seminars in Einzelarbeit mit dem Audience Response System ARSnova durchgeführt. Die Umfragen adressieren mit jeweils neun Fragen drei Anforderungsbereiche (I: Reproduktionsleistung, II: Reorganisation- und Transferleistung, III: Reflexion und Problemlösung). Den Anforderungsbereichen entsprechend beinhalten sie Single-Choice-, Multiple-Choice- sowie Freitextfragen. (3) Nach der ersten Quizphase wurde die gesamte Testgruppe in Kleingruppen eingeteilt, die im Supertrumpf-Modus das Dramenquartett spielen. (4) Eine zweite Umfrage erfasst den Wissensstand beider Gruppen, nachdem sie das Dramenquartett gespielt haben. (5) Die Auswertung des ersten Testdurchlaufs, der mit 11 Teilnehmenden durchgeführt wurde, verweist auf einen lernförderlichen Effekt des Dramenquartetts. Im Rahmen der ersten Quizrunde wurden 43% der Fragen, nach der zweiten Umfrage 52% der Fragen richtig beantwortet. Darüber hinaus verweist ein erster Blick auf die Freitextantworten darauf, dass der spielerische Zugang die intrinsische Motivation, sich über den Seminarkontext hinaus mit digitaler Netzwerkanalyse auseinanderzusetzen, steigert. Das erarbeitete Verfahren zur vergleichenden Lernstandserhebung hat sich bewährt und wird in einem weiteren Seminar eingesetzt, um den Einfluss einer spielerischen Wissensvermittlung auf Kompetenz- und Wissensstand zu untersuchen. Das Projekt lotet das didaktische Potenzial von Gamification-Ansätzen in den DH konzeptionell und praktisch aus, indem es das DraCor-Kartenspiel mit Tools und Tutorials in einer didaktischen ,Pipeline"" verbindet und damit in die Disseminationsstrategie von forTEXT integriert. Der damit entwickelte Prototyp eines Konzepts, das auch fachdidaktisch Weiterentwicklungspotenzial birgt, ermöglicht diverse Adaptionen und Transformationen: in Hinblick auf die Netzwerkanalyse literarischer Texte, in Hinblick auf andere Methoden der Digital Humanities sowie in Hinblick auf das didaktische Szenario einer Verzahnung von analogen und digitalen Ansätzen. So ließen sich auf der Grundlage der Netzwerkdaten aus anderen DH-Projekten, etwa zu Romanen, andere generische Karten-Sets entwerfen, wobei auch die 'durch ezlinavis in Kombination mit Gephi ermöglichte 'kollaborative Erstellung eigener Sets denkbar ist. Diese selbstständige Erstellung von Karten-Sets würde nicht zuletzt auch den haptischen Lerntyp ansprechen. Eine Weiterentwicklung der didaktischen Engführung von Analogem und Digitalem ließe sich über eine Verzahnung des Kartenspiels mit der digital-interaktiven Repräsentation der einzelnen Dramen auf DraCor vornehmen (z.¬†B. über QR-Codes). Unter didaktischen Gesichtspunkten bietet sich des Weiteren die Möglichkeit, kreativ-produktionsorientierte Elemente in die skizzierte Pipeline einzubauen, etwa indem die Lernenden Netzwerke ,erfinden"", die sie zunächst händisch zeichnen und dann 'den Schritt in den digitalen Raum machend 'mittels ezlinavis formal erfassen müssen. Der im Projekt durchgeführte Testlauf soll in diesem Sinne zu einer weiteren Diskussion über didaktische Potenziale sowohl von Gamification-Ansätzen als auch der Verzahnung von analogen und digitalen Lehrmitteln anregen und damit grundsätzlich der Reflexion über didaktische Szenarien dienen, die den spielerischen, kreativen Übergang zwischen lebensweltlich vertrauten Situationen und der Abstraktion digitaler Forschungsprozesse gestalten."
2020,DHd2020,114_final-MEYER_SICKENDIEK_Burkhard_Requirements_on_the_Punctuation_Re.xml,Requirements on the Punctuation Reconstruction for the Translation of Post-modern Poetry,"Burkhard Meyer-Sickendiek (Freie Universität Berlin, Deutschland); Timo Baumann (Universität Hamburg, Deutschland); Hussein Hussein (Freie Universität Berlin, Deutschland)","Postmodern Poetry, Translation, Punctuation Reconstruction","Übersetzung, Modellierung, Stilistische Analyse, Literatur, Ton, Text","Punctuation is an important and cohesive device in all kinds of written discourse. Standard marks used to separate words, phrases, clauses and sentences for the purpose of cohesion. Already [2][5][1] pointed out that through punctuation marks, one can signal different information structures in written language. Regarding the translation of texts, we use such marks to identify the ends of sentences, closely related sentences or clauses, etc. This is why missing punctuation burdens the translations and forces the translator to go over the text several times to understand its meaning [10]. Understanding the uses and functions of punctuation marks, therefore, is extremely important for translators, as their purpose is to clarify the meaning of a particular construction within a text. On the other hand, modern poetry often disregarded such punctuations. Ever since Italian Futurism around 1900 spoke of the ""parole in libert√†"", i.e. the liberation of words from grammatical and syntactic limitations, modern poetry has hardly used punctuation. This lack of punctuation makes analysis, but also translation, more difficult. The only way to reconstruct this punctuation is by listening to the poems, i.e. by subsequently identifying sentence boundaries. However, this lack of punctuation can be found very often in modern and post-modern poetry, so the challenge is to recognize the phrase boundaries. We contribute in the paper an application towards the problem of identifying left-out punctuation in post-modern poetry, by proving that only a very simple type of punctuation - the semicolon - is needed to improve machine translation. This simple punctuation refers to phrase boundaries, the so-called ""grammetrical units"", which Donald Wesling defined in his study ""The Scissors of Meter"" [11]. Such units must be identified in order to improve machine translation. The need for adding left-out punctuation becomes in case of creating machine translations obvious with regards to the poem ""bitte verlassen sie diesen raum"" (english: please leave this room) written by the German poet Nicolai Kobus [6] (Text A):  bitte verlassen sie diesen raum so wie sie ihn vorfinden möchten danke möchten sie diesen raum vorfinden wie sie ihn verlassen haben bitte räumen sie alles so vorgefundene als wären sie verlassen worden danke sie möchten doch nicht daß man sie so verlassen im raum vor findet bitte seien sie für einen so verlassen vorgefundenen raum dankbar [...]  please leave this room in the state in which you would like to find it thank you would you like to find this room in the state in which you have left it please clear out everything thus found as though you had been left thank you you would not like somebody to find you left abandoned in the room now would you please be grateful for a room a space found in such an abandoned state (...)  In the human translation or the target poem, made by Hales, there is just a little difference. This difference is caused by the missing punctuation. And it can basically be explained by the fact that Hales has chosen a different line arrangement. In terms of content, however, her translation is reproduced correctly. Since there is no specific translation system trained with poem data with/without punctuation (small amounts of training data), we used a Google machine translation (GMT) system [3]. When we compare this (human) translation with the GMT system, we recognize the difficulty of recognizing the sentence boundaries within the poem without punctuation (Text C):  please leave this room as they would like to find him Thank you for wanting this room find out how to leave him please have everything clear found as if they were Thank you you do not want that one So leave them in the room please find one for you leave found space (...)  Obviously, this machine translation (MT) becomes much better if we add the full punctuation marks to the source text, when listening to the audio of the poem (Text D):  please leave this room as you would like him to find Thank you. Do you want this room find how they leave him to have? Please clear everything up found as if they were been left. thank you Do not want that one So leave them in the room please, please be for one leave found space grateful. (...)  Punctuation is an essential aspect of poetry translations, as it is for discourse analysis in general [8]. Punctuation ""gives a semantic indication of the relationship between sentences and clauses, which may vary according to languages"", as well as to translations [4]. The philological scholar of our project annotated the punctuation information manually by using text and audio information in the 120 poems, focusing on the intonation of poets reading their poems. In order to clarify the question which type of punctuation has to be added, we inserted two kinds of punctuation in the source text. In a first step, we focused on six different punctuation marks: full stop (.), comma (,), semicolon (;), colon (:), exclamation mark (!), and question mark (?). In a second step, we simplified this insertion by reducing these six marks to a single semicolon. The human reference translations are compared with the automatic translation of GMT system without/with consideration of punctuation information. The experiment consists of three tasks based on the GMT system: The translation enhancement should be observable from improved translation quality scores. The results are calculated by bilingual evaluation understudy (BLEU) [9] score, which used for evaluating the quality of text by translation. The BLEU score of tasks 1, 2, and 3 are 0.256, 0.275, and 0.280, respectively. The results indicate that we need just one type of punctuation - semicolon - to improve the scoring for automatic translations of post-modern poetry. Every generic translation system is trained with data in which segments are defined by end points. It is astonishing that even the addition of a semicolon to segmental boundaries is sufficient to improve machine translation. This also explains the central problem: machine translation does not fail because of mixing up questions and statements, but because of mixing up segmental units and enjambements. In our future work, we plan to train a specific system on translating unpunctuated poetry in order to compare the results with manual translations. The fact that we add punctuation signs on the basis of oral representations of the poems is acceptable when it comes to audio poems, in which the oral representation is an essential part of the poem as a piece of art, closely connected to the written form."
2020,DHd2020,176_final-HODEL_Tobias_Maschinelles_Lernen_in_den_Geisteswissenschafte.xml,Maschinelles Lernen in den Geisteswissenschaften. Systemische und epistemologische Konsequenzen einer neuen Technologie,,"Maschinelles Lernen, Methodendiskussion, Epistemologie","Maschinelles Lernen, Methodendiskussion, Epistemologie","Seit einigen Jahren machen maschinelles Lernen und Überlegungen zu den Konsequenzen der dadurch entstehenden Artificial Intelligence Schlagzeilen. Von Spracherkennung über selbstfahrende Autos bis hin zu komplexen Spielen, maschinelles Lernen macht Computer in einzelnen Handlungsfeldern leistungsfähiger als Menschen. In der Theorie werden drei Formen ( Ein Ansatz, das sogenannte Ebenso werden andere unüberwachte und überwachte Verfahren des maschinellen Lernens eingesetzt, um Strukturen in großen Datenmengen zu finden und die Zusammenhänge zwischen den Daten und ihnen zugeordneten Kategorien zu erkennen (z.B. Verfahren zur Dimensionalitätsreduktion, Clustering, Klassifikation, Die Technologien, die auf die 1980er Jahre zurückgehen, wurden lange nur testweise eingesetzt, weil die Leistungsfähigkeit der Computersysteme nicht ausreichend war Das Panel hat zum Ziel, die Entwicklung und Anwendung des maschinellen Lernens mit einer Reflexion zu verbinden, die die Konsequenzen des Einsatzes aufzeigt. Dabei soll weder der häufig mit euphorischen Erwartungen verbundene Nutzen, noch unberechtigte Fundamentalabwehr befeuert werden. Vielmehr ist die differenzierte Beurteilung aus unterschiedlichen Blickwinkeln das Ziel. Im Panel zentral gesetzt werden epistemologische Fragen, die gerade aufgrund der imitierenden Natur des maschinellen Lernens entscheidend sind für die Aufbereitung von Trainingsmaterial oder die Implementierung in Entscheidungsprozesse. Gleichzeitig ähneln die Prozesse, die die Algorithmen übernehmen Vorgehensweisen geisteswissenschaftlicher Verstehensprozesse, die unter dem Begriff der ""Hermeneutik"" versammelt werden. Maschinelles Lernen hat entsprechend das Potential, als Methode unsere Zugänge und den Blick auf unser Material fundamental zu erweitern. Im Rahmen des Panels werden vier Protagonist*innen ihre Perspektive auf die Konsequenzen der Nutzung des maschinellen Lernens werfen:  Der Einsatz des maschinellen Lernens erfordert insbesondere bei der Erstellung neuer Algorithmen Fertigkeiten aus den Computerwissenschaften. Genau dieser Aufgabe stellt sich Sofia Ares Oliveira täglich, wenn sie als Ingenieurin selbständig neuronale Netze für dhlab der Eidgenössisch Technischen Hochschule in Lausanne (EPFL) erstellt. Im Rahmen des Panels wird sie verantwortlich sein für eine kurze Einführung in maschinelles Lernen. Aufgrund jahrelanger Beschäftigung mit der visuellen Analyse digitalisierter Dokumente, ist Ares Oliveira Spezialistin für den Aufbau und die Umsetzung neuronaler Netze zur semantischen Aufbereitung von Dokumenten (Segmentierung und Annotation). ""DH segment"" Die zwei Teilbeiträge von Sofia Ares Oliveira werden auf Englisch vorgetragen.  Anhand von Beispielen aus der jüngeren Forschung in den Computational Literary Studies (u.a. Underwood 2019 und So 2019) möchte der Beitrag aufzeigen, dass Verfahren des überwachten  In dem Beitrag werden verschiedene Möglichkeiten vorgestellt, maschinelle Lernverfahren für die Erforschung historischer Gattungen anhand des Textstils einzusetzen, insbesondere Clustering, Klassifikation und Topic Modeling  Im Rahmen von Projekt READ wurde mit der Einführung von maschinellen Lernverfahren die Erkennung von Handschriften und alten Drucken markant verbessert. Da die neuronalen Netze auf Trainingsmaterial basieren (also Die Panelisten werden kurz und thesenhaft ihre Perspektive auf die Technologie darlegen, dabei sollen sie u.a. zu drei Komplexen Stellung nehmen: Wo wird der Einsatz der Technologie in den Geisteswissenschaften neue Erkenntnisse bringen, welche Dokumente/Materialien/Daten eignen sich nicht für die Behandlung mit Fragen nach Erkenntnismöglichkeiten werden in diversen geisteswissenschaftlichen Disziplinen seit Jahrzehnten diskutiert. Die Nutzung von Algorithmen des maschinellen Lernens erfordern jedoch klare Aussagen zur untersuchten Materie, unabhängig davon, ob es sich um Neben der Angst vor dem Kontrollverlust und etwaigem Rückgang von Arbeitsplätzen oder der Überwachung von Menschenmassen, sind es nicht zuletzt Skandale zur Verletzung der Privatsphäre, die in den vergangenen Monaten zum Ruf nach der Regelung des Einsatzes der Technologie führten Im wissenschaftlichen Bereich sind es zurzeit vor allem die angewandte Informatik und Mathematik sowie die Computerlinguistik, die maschinelles Lernen in ihre Forschungen integrieren. In den Digital Humanities spielt die Technologie bislang von wenigen Zentren abgesehen eine untergeordnete Rolle. In absehbarer Zeit dürfte sie ein wichtiger Teil der Disziplin werden 'nicht nur im Recherche –, sondern auch im Auswertungs- und Schreibprozess. Insbesondere im Umgang mit digitalisierten Dokumenten, großen Datenmengen und Bildquellen können neuronale Netze ein wichtiges Mittel sein, um Daten zu finden, zu sortieren und auszuwerten. Die digitalen Geisteswissenschaften umfassen mit ihrem Methodenapparat sowohl komplexe Softwareentwicklung, als auch die Anwendung statistischer Modelle und das Erklären mit hermeneutischen Verfahren. Daher ist die Disziplin prädestiniert in den Diskussionen dieser gesellschaftsverändernden Technologie eine Vorreiterrolle einzunehmen."
2020,DHd2020,187_final-BURGHARDT_Manuel__The_Vectorian____Eine_parametrisierbare_Su.xml,"""The Vectorian"" 'Eine parametrisierbare Suchmaschine für intertextuelle Referenzen","Manuel Burghardt (Computational Humanities Group, Universität Leipzig); Bernhard Liebl (Computational Humanities Group, Universität Leipzig)","intertextuality, text reuse, information retrieval, NLP, word embeddings","Entdeckung, Programmierung, Inhaltsanalyse, Annotieren, Bewertung, Text","Shakespeare ist überall. Über alle zeitlichen und medialen Grenzen hinweg finden sich intertextuelle Bezüge auf die Werke von Shakespeare (vgl. Garber, 2005; Maxwell & Rumbold, 2018), der damit nicht nur der meistzitierte und meistgespielte Autor aller Zeiten, sondern auch der meistuntersuchte Autor der Welt ist (Taylor, 2016). Doch wenngleich in zahllosen Studien diverse Einzelaspekte von Shakespeares Werk aus Perspektive der Intertextualitätsforschung gründlich mittels By the Eine weitere methodische Einschränkung machen wir, indem wir Phänomene wie strukturelle Öhnlichkeit (Versmaß, Figurenkonstellation) und stilistische Öhnlichkeit Abb. 1 zeigt die Systemarchitektur der besagten Suchmaschine, die fortan als ""The Vectorian"" Kern des Abb. 2 zeigt das Frontend des Der Neben den beiden Der Die am besten bewerteten Ergebnisse sind zunächst viele Varianten nach dem Schema ""under the X tree"", bspw. ""under the Beim Parameter der POST-STSS, ein Parameter der unterschiedliche POS unterschiedlich stark gewichtet, ist in Kombination mit dem Im aktuellen Stadium dient der"
2020,DHd2020,179_final-PRELL_Martin_Altbausanierung_mit_Niveau___die_Digitalisierun.xml,Altbausanierung mit Niveau 'die Digitalisierung gedruckter Editionen,Frederike Neuber (); Thorsten Schaßen (); Dominik Kasper (); Martina Gödel (); Thomas Stäcker (),"(Retro)Digitalisierung, Digitale Editionen, Druckeditionen, Best Practices","(Retro)Digitalisierung, Digitale Editionen, Druckeditionen, Best Practices","Während das Buch immer noch den höchsten Stellenwert in der geisteswissenschaftlichen Forschung im Allgemeinen besitzt, sind Editionen, die als Buch erscheinen, seit Jahren rückläufig (Eggert 2009). Bestehende Druckeditionen wirken mittlerweile neben ihren digitalen Nachfolgerinnen wie Relikte aus einer anderen Zeit. Ihr wissenschaftlicher Wert bleibt weitestgehend in den Grenzen des Buches verhaftet, während der digitale Editionskosmos wächst und perspektivisch zu einem dichten Wissensnetz wird. Um Druckeditionen besser verfügbar zu machen, sie mit anderen Editionen zu vernetzen, oder einen neuen Blick auf die Quellen zu ermöglichen, häufen sich in den letzten Jahren Unternehmungen zur Digitalisierung von Druckeditionen. Die mit der Digitalisierung von Editionen verbundenen, generalisierbaren Anforderungen und Implikationen sind, trotz ihrer unmittelbaren Relevanz für den Bereich der Digitalen Editionen, bisher noch nicht systematisch und projektübergreifend untersucht worden. Da bis dato zudem kaum auf die zahlreichen Erfahrungen bestehender Digitalisierungsprojekte zurückgegriffen werden kann, existiert sowohl bei laufenden als auch neuen Projekten stets die Gefahr, dass die organisatorischen, konzeptionellen und technischen Herausforderungen unterschätzt oder gar nicht erst erkannt werden. So entpuppen sich bspw. Projekte, die zunächst mit geringem Aufwand umsetzbar scheinen, nicht selten als Mammutaufgaben, die in Bezug auf Komplexität und Ressourcenbedarf die Anforderungen vergleichbarer Aus wissenschaftstheoretischer Perspektive stellt sich die Frage, welchen Stellenwert digitalisierte Editionen im Kosmos digitaler Editionstypen einnehmen können, wenn sie, wie Sahle formuliert, gar keine digitalen Editionen sind (Sahle 2013: 58ff.). In diesem Spannungsfeld gilt zu diskutieren, wie gedruckte editorische Leistungen der Vergangenheit unter den neuen medialen Bedingungen methodisch angemessen transformiert und für die Zukunft gesichert werden können. Das Panel richtet sich als Forum für den Erfahrungsaustausch und die Diskussion über theoretische und praktische Implikationen bei der Digitalisierung von Editionen sowohl an SoftwareentwicklerInnen aus den digitalen Geisteswissenschaften als auch an FachwissenschaftlerInnen. Vier Fragefelder sollen aus der Perspektive verschiedener Akteure im Panel diskutiert werden: Das Panel beginnt mit einer Einleitung durch die Moderatoren, der kurze Statements der Beitragenden mit Schwerpunkt auf bestimmte Fragefelder folgen und die mit einer These oder Fragestellung enden. Sie dienen als Problemaufriss und zur Identifizierung unterschiedlicher Positionierungen im Kontext der (Retro)Digitalisierung, über die im Anschluss debattiert wird. Es folgt eine Diskussion im Plenum. Darauf aufbauend werden die Beitragenden (sowie weitere Interessierte) im Nachgang der DHd2020 die Arbeit an einem Leitfaden aufnehmen, der sowohl technisch-praktische als auch methodische Fragen der Digitalisierung von Druckeditionen berücksichtigt und als Ausgangspunkt für einen weiterführenden Diskurs dient. Der Entwurf des Leitfadens soll online vorab veröffentlicht werden. Die diskutierte und finalisierte Fassung (in englischer und deutscher Sprache) wird dauerhaft zugänglich gemacht werden.  Im editionswissenschaftlichen Diskurs unterscheidet man im Spektrum der digitalen Editionstypen meist zwischen "" In der Umsetzung der Retrodigitalisierung können vor allem zwei paradigmatische Schwierigkeiten ausgemacht werden: Zum einen wird der mit dieser Transformation verbundene Aufwand unterschätzt. Zum anderen wird der gedruckten Vorlage allzu häufig ein sakrosankter Status zugeschrieben. Damit verbunden sind zahlreiche Fragen, die einer Klärung im jeweiligen Projektkontext bedürfen. Häufig unklar ist bspw. ob und wenn ja, in welcher Form in den Text eingegriffen werden darf; sei dies aus Gründen der Fehlerkorrektur oder der Angleichung an den aktuellen Forschungsstand. Zentraler Diskussionspunkt wird im Statement die Frage nach dem Einfluss des Layouts der Druckedition auf die digitale Präsentation sein. Ebenso wird in die Debatte der Aspekt eingebracht, dass die Retrodigitalisierung häufig als rein technischer Prozess ohne philologischen Anspruch und wissenschaftlichen Mehrwert bewertet wird (Ball et. al 2016; Sahle 2012) und die beteiligten digital affinen WissenschaftlerInnen zum Dienstleister marginalisiert werden. Dies wird auch durch Missverständnisse bedingt, die mit dem Eingang neuer Terminologie in das Editionsprojekt aufgrund der  Auf dem Weg vom Druck zur digitalisierten oder gar digitalen Edition können unterschiedliche Welches Vorgehen Anwendung findet, wird unter anderem dadurch bestimmt, welche Erwartungen und Mentalitäten das Projekt prägen. Unterschiedliche  Ein Projekt Projekten, die in der Planungs- oder  Bibliotheken bewahren gedruckte Editionen. Mit der Durchsetzung des digitalen Paradigmas werden diese selbst Gegenstand editorischer Prozesse. Nicht nur die Edition, sondern auch der digitale Transformationsprozess stellt eine neue erschließende Dimension dar: Frederike Neuber ist wissenschaftliche Mitarbeiterin bei der TELOTA-Initiative der Berlin-Brandenburgischen Akademie der Wissenschaften. Sie ist Mitherausgeberin von ""Jean Paul - Sämtliche Briefe digital"" und im Institut für Dokumentologie und Editorik u. a. als Torsten Schaßan ist wissenschaftlicher Mitarbeiter an der Herzog August Bibliothek Wolfenbüttel. Er betreut dort den Bereich Digitale Editionen. An der HAB wurden mehrere Retrodigitalisierungsvorhaben umgesetzt, darunter die Briefe der Fruchtbringenden Gesellschaft und ""Controversia et Confessio"". Dominik Kasper ist wissenschaftlicher Mitarbeiter an der Akademie der Wissenschaften und der Literatur Mainz. Erfahrungen mit Retrodigitalisierung konnte er in den Projekten ""Deutsche Inschriften Online"" und ""PROPYLÖEN 'Goethes Biographica"" (Leiter der Frankfurter Arbeitsstelle) sammeln. Martina Gödel ist seit 2011 freiberuflich unter dem Namen Thomas Stäcker ( Max Grüntgens (Moderation) ist wissenschaftlicher Mitarbeiter an der Akademie der Wissenschaften und der Literatur Mainz. Erfahrungen mit Retrodigitalisierung konnte er in den Projekten ""Deutsche Inschriften Online"" (Leiter der Mainzer Arbeitsstelle) und ""PROPYLÖEN 'Goethes Biographica"" sammeln. Martin Prell (Moderation) ist DH-Koordinator der PROPYLÖEN-Edition (Goethe- und Schiller-Archiv Weimar) und des ""Editionenportal Thüringen"" (Universität Jena). Er gibt unter anderem die Briefe Erdmuthe Benignas von Reuß-Ebersdorf heraus."
2020,DHd2020,226_final-SCHUMACHER_Mareike_m_w_Figurengender_zwischen_Stereotypisier.xml,m*w Figurengender zwischen Stereotypisierung und literarischen und theoretischen Spielräumen. Genderstereotype und -bewertungen in der Literatur des 19. Jahrhunderts,"Mareike Schumacher (University of Hamburg, Deutschland); Marie Flüh (University of Hamburg, Deutschland)","Gendertheorie, Bewertungsanalyse, digitale Annotation","Inhaltsanalyse, Modellierung, Annotieren, Kontextsetzung, Theoretisierung, Visualisierung","Während in den Digital Humanities bereits erste korpusbasierte Analysen von Figurengender in der Literatur vorgelegt wurden (Underwood 2019: 111 ff), wird in den Kulturwissenschaften zu diesem Thema selten korpusbasiert gearbeitet. Stattdessen sind Theorien zur Genderthematik häufig philosophisch-soziologisch (z.¬†B. bei Beauvoir oder Bourdieu), diskurstheoretisch (z.¬†B. Foucault) oder dekonstruktivistisch (z.¬†B. Butler) Die leitende Fragestellung des m*w-Projektes ist: Wie werden Genderrollen in der Literatur des 19. Jahrhunderts dargestellt und bewertet? Um uns dieser Fragestellung zu nähern, erstellen wir mithilfe einer Auswahl theoretischer Ansätze ein Modell. Dieses nutzen wir, um im überwachten Machine-Learning-Verfahren der Named Entity Recognition (NER) ein Tool darauf zu trainieren, Figuren und ihre Genderzuschreibungen automatisch zu erkennen. Die Ergebnisse des NER-Verfahrens nutzen wir um unser Modell weiter zu schärfen. Im abschließenden Close Reading der Novellen werden schließlich Genderbeschreibungen und -bewertungen analysiert und mit dem Modell abgeglichen. Grundsätzlich nehmen wir sowohl Modell als auch Korpus als variable bzw. dynamische Größen wahr, die sich für den Praxistest der Operationalisierung theoriebasierter Modelle zur Erforschung literarischer Genderrollen eignen.  Davon ausgehend, dass sich sowohl in der Theorie als auch in Erzähltexten Spielräume als Zwischenräume auftun, die dadurch sichtbar werden, dass sie sich von den sie umgebenden normierten Räumen unterscheiden Um entscheiden zu können, ob Eigenschaften und Handlungen von Figuren stereotypen Rollenbildern zugeschrieben werden können oder nicht, haben wir zunächst möglichst viele Rollenbilder in unser Modell integriert. Jede dieser Rollen kann sowohl im Sein (Gender-Identität) als auch im Handeln (Gender-Performanz) von Figuren verankert sein. Die sechs in Abb. 1 abgebildeten Oberkategorien von Eigenschaften sind ebenfalls der Theorie entnommen. Allerdings wurden hier die Beschreibungen stärker kondensiert, um die zahlreichen genannten Einzeleigenschaften für die digitale Annotation besser handhabbar zu machen. Alle drei eingesetzten Methoden - NER, Annotation von Stereotypen und Emotionsanalyse haben sich im ersten Proof of Concept als fruchtbar für die digitale Erforschung von Figurengender erwiesen. Besonders eklatante Zwischenergebnisse fassen wir im Folgenden zusammen. Im Laufe des NER-Trainingsprozesses erwies sich die Kategorie ""divers"" nicht als dem Korpus angemessen, weshalb wir die NER-Kategorien auf ""männlich"", ""weiblich"", ""genderneutral"" Um Hinweise auf eine mögliche Verzerrung durch die Zusammensetzung des Trainingskorpus zu bekommen, haben wir einen dritten Testtext hinzu genommen   (  Anschließend wurden die NER-Kategorien mit Unterkategorien versehen, die den Benennungen der Genderrollen des Modells entsprechen (Abb. 5). Sofern es keine adäquate, in der Theorie erwähnte Rolle oder Eigenschaft gab, wurden zusätzliche Annotationskategorien erstellt und den Oberkategorien ""unsortierte Rollen"" und ""unsortierte Eigenschaften"" zugewiesen. Die digitale Annotation des ersten Beispieltextes zeigt, dass stereotype Beschreibungen vor allem zu Beginn der Erzählung häufig und somit besonders für die Etablierung der Figuren von Bedeutung sind. Stereotype Eigenschaften sind hier zwar divers, für männliche und weibliche Figuren gibt es aber jeweils einige wenige, die quantitativ herausstechen. Für weibliche Figuren ist das vor allem Öußerlichkeit/Schönheit für männliche sind es Körperlichkeit/Trinkfestigkeit und Wirksamkeit/Herrschaft. Unsortierte Rollen und Eigenschaften sind zumeist in einem binären Rollensystem verankert und dennoch werden in dieser Novelle zum Teil Stereotype aufgebrochen. Dies wird hauptsächlich durch die Zuschreibung einzelner Eigenschaften zu einer Figur eines Genders erreicht, die in der theoretischen Literatur eher dem Stereotyp des anderen zugeschrieben werden (ausführlichere Auswertung des ersten Beispieltextes in Schumacher 2020 [3]).  Die Emotionen werden in den allermeisten Fällen verbal (407 Annotationen) von den Figuren ausgedrückt. Über die Veränderung des körperlichen Zustands (19 Annotationen), und nonverbal (107 Annotationen) werden Emotionen vergleichsweise selten repräsentiert. Die Auswertung der Properties ergibt genderspezifische Emotionsinformationen (s. Abb. 7 und 8). Weibliche Figuren treten ängstlicher auf als männliche. Männliche Figuren reagieren häufiger zornig als weibliche. Sie empfinden außerdem häufiger Ekel 'hier meistens im Sinne von Abneigung 'als weibliche Figuren. Diese leiden häufiger unter gedrückter Stimmung und empfinden deutlich häufiger negative Basisemotionen als männliche Figuren. Diese treten im Schnitt fröhlicher auf als die weiblichen Figuren. Die männlichen Figuren zeigen häufiger positive Basisemotionen als die weiblichen Figuren und auch die positive Basisemotion LIEBE überwiegt seitens der männlichen Figuren. Scham 'als Unterkategorie der Problemfälle 'ist seitens der weiblichen Figuren deutlich stärker ausgeprägt (für eine ausführlichere Auswertung des ersten Beispieltextes vgl. Flüh 2020).  "
2020,DHd2020,143_final-HORSTMANN_Jan_Interpretationsspielräume__Undogmatisches_Anno.xml,Interpretationsspielräume. Undogmatisches Annotieren literarischer Texte in CATMA 6,"Jan Horstmann (Universität Hamburg, Deutschland); Janina Jacke (Universität Hamburg, Deutschland)","Annotation, Interpretation, Kollaboration","Inhaltsanalyse, Annotieren, Theoretisierung, Literatur, Metadaten, Forschungsprozess","Werden Da DH-Tools möglichst an disziplinspezifische geisteswissenschaftliche Theorien, Methoden und Praktiken rückgebunden werden sollen (vgl. Sahle 2015), sollten digitale Zugänge zur Literaturerforschung diese Spielräume berücksichtigen. Undogmatisches Annotieren mit CATMA 6 ( Wie CATMA 6 geisteswissenschaftlichen Anforderungen (und damit den genannten Spielräumen) gerecht zu werden sucht, soll anhand von vier Funktionskomplexen demonstriert werden: unterschiedlichen Annotationsmodi, Mehrfachannotation, Metaannotation und kollaborativem Annotieren. Dieser Beitrag kann somit auch als exemplarische Umsetzung der Forderung verstanden werden, einen Brückenschlag zwischen DH- und traditionell-geisteswissenschaftlichen Methoden zu schaffen. Die Interpretation literarischer Texte wird gemeinhin als eine Kernaufgabe literaturwissenschaftlichen Arbeitens betrachtet. Regeln der Textinterpretationen oder Gütekriterien für Interpretationshypothesen sind aufgrund der Theorie- und Methodenvielfalt nicht eindeutig festgelegt. Zwei Überzeugungen scheinen jedoch über unterschiedliche Ausrichtungen hinweg in der literaturwissenschaftlichen Forschungsgemeinschaft (relativ) allgemein anerkannt: (a) Trotz der Pluralität zulässiger Interpretationen gibt es auch Interpretationen, die einem Text Angesichts dieser Sachlage lässt sich leicht erkennen, warum die Methode der Annotation im Zusammenhang mit Interpretationen fruchtbar angewandt werden kann: Der Prozess des Annotierens geht mit textnahem Arbeiten einher, und Annotationen werden grundsätzlich bestimmten Textstellen zugewiesen. Damit ist Annotation besonders geeignet für kleinschrittige textdeskriptive bzw. -analytische Vorhaben, die dann eine Grundlage für Interpretationen liefern können. Interpretationen selbst werden dann 'auch in DH-Projekten 'häufig in Form zusammenhängender Texte erstellt, innerhalb derer auf textanalytische Ergebnisse (Annotationen) Bezug genommen wird. Hier kann Annotation demnach als Werkzeug von Interpretation gelten. Es kann allerdings durchaus sinnvoll sein, auch Interpretationshypothesen selbst im Prozess des Annotierens zu entwickeln und als Annotationen festzuhalten. Denn zum einen verschwimmt sogar bei gemeinhin als deskriptiv geltenden Operationen wie der narratologischen Analyse otf die Grenze zu (inhaltsspezifizierender) Interpretation. Wie groß die Spielräume im Rahmen von Annotation sein müssen 'sowohl hinsichtlich des Grads der Formalisiertheit der Annotation (vgl. 2.2) als auch der Einigkeit unter verschiedenen Annotator√Ønnen –, hängt entsprechend davon ab, ob es sich um deskriptiv-analytische Annotationen handelt, die als Vorarbeit für Interpretationen fungieren, oder um genuin interpretative Annotationen (vgl. 3).  Viele Annotationstools (bisher auch CATMA) ermöglichen ausschließlich die Annotation mithilfe von Tagsets, also hierarchisch gegliederten Kategorien. Dafür müssen Forschende allerdings schon ein formalisiertes Kategoriensystem haben, mit dem sie den Text untersuchen wollen. Annotation sollte abern auch zur noch unstrukturierten Textexploration nutzbar sein. Zudem sollte auch Interpretation selbst mithilfe von Annotation ermöglicht werden, was aber meist nicht (allein) unter Nutzung von Kategorien umsetzbar ist. In CATMA 6 werden deshalb folgende Annotationsmodi implementiert: (1) (2) (3) Damit ein strukturiertes Annotieren mit Tagsets nicht nur im Rahmen heuristischer Text Neben freiem Generieren und iterativem Überarbeiten von Tagsets ist eine Bedingung für die Nutzung tagsetbasierten Annotierens als interpretationsunterstützender Methode die Möglichkeit der (diversen oder sogar widersprüchlichen) Mehrfachannotation derselben Textstelle. Dies trägt zum einen dem Umstand Rechnung, dass ein Text aus unterschiedlichen Perspektiven untersucht werden kann: Beispielsweise kann eine Textpassage zugleich intermediale Bezüge enthalten und Passagen literarischer Texte sind zudem häufig interpretationsoffen, weshalb unterschiedliche, teilweise auch widersprüchliche Interpretationen gleichermaßen gültig sein können. So mögen etwa (inkompatible) Thesen darüber, wer/was durch eine im Text auftretende Figur verkörpert werden soll, plausibel sein. Da bei der Interpretation von Literatur die Spielräume nicht grenzenlos sind und nach diversen Regeln gespielt werden muss (vgl. bspw. Jannidis 2003), benötigt eine Annotationsumgebung, die taxonomiegestütztes Interpretieren ermöglicht, auch Optionen zur Einordnung, Erläuterung und Aushandlung von Interpretationen. Diese Rolle erfüllen in CATMA 6 Metaannotationen, die wiederum taxonomiebasiert (als Annotationskategorien lassen sich mit Properties versehen, denen pro Annotation feste oder ad hoc vergebbare Values zugeordnet werden können, um Annotationen genauer zu qualifizieren. Die gleiche Funktion erfüllen freitextbasierte Metaannotationen, die erstmalig in CATMA 6 nutzbar sind. Ob Metaannotationen als freie Kommentare oder auf Taxonomiebasis zur Anwendung kommen, kann vom Grad der theoretischen Ausarbeitung der genutzten Interpretationsheuristik abhängen oder eine Frage des Anwendungskontextes bzw. der persönlich präferierten Arbeitsweise sein. In technischer Hinsicht sind Annotationen gemäß dem Web Annotation Data Model Während Metaannotationen eingesetzt werden können, um einem Tagset Analysekategorien auf einer horizontalen Gliederungsebene hinzuzufügen, Kollaboratives Annotieren ist in der Linguistik eine etablierte Methode, um Annotationsentscheidungen abzusichern (vgl. Wissler et al. 2014). In der Literaturwissenschaft ist es noch wenig etabliert (vgl. Röcke 2016); auch ist ein behutsameres Vorgehen angebracht, wenn es darum geht, Annotations- bzw. Interpretationsentscheidungen abzusichern. Als fruchtbar hat sich ein iteratives Vorgehen erwiesen, bei dem Forscher√Ønnen diskrepant annotierte Passagen diskutieren, um Gründe für unterschiedliche Entscheidungen herauszustellen (vgl. Gius & Jacke 2017). Durch eine gründliche Metaannotation kann dieser Workflow verschlankt werden. Je nach Grund kann abgewogen werden, ob es sich um eine legitime Uneinigkeit handelt. So können Interpretationsspielräume bei kollaborativem Annotieren zugleich gewahrt und sinnvoll eingegrenzt werden. Kollaboration wird in CATMA ermöglicht durch die mit GitLab per API verknüpfte projektzentrierte Systemarchitektur (vgl. Fig.¬†2). Eine GitLab-Group Das neue Rollen- und Rechtesystem erlaubt somit eine differenzierte Festlegung von Spielräumen auch bei der Konzeption eines kollaborativen Annotationsprojekts. Um einem Projekt weitere Mitglieder hinzuzufügen bietet CATMA 6 zwei Möglichkeiten: (1) das manuelle Hinzufügen einzelner CATMA-Nutzer√Ønnen durch Eingabe des CATMA-Usernames. Diese Funktion bietet sich für den asynchronen Arbeitsmodus etwa in Forschungsprojekten an. (2) Die Die Verknüpfung mit GitLab bietet zudem Versionierungs- und damit einhergehende Konfliktlösungsfunktionen. Denn während CATMA beispielsweise inhaltlich widersprüchliche Mehrfachannotationen erlaubt, stellt das zeilenbasiert arbeitende Versionierungssystem Git einen Konflikt fest, wenn Nutzer√Ønnen inkompatible Önderungen in ihren Projekten vorgenommen haben, die dieselbe Zeile des zugrundeliegenden Codes betreffen. Nehmen wir beispielsweise an, in einem kollaborativen Forschungsprojekt wurde dieselbe Metaannotation von Nutzer√Ønnen 1 und 2 je unterschiedlich geändert. Sobald Nutzer√Øn 2 die Arbeit mit dem Team synchronisiert, meldet CATMA einen Konflikt, anstatt eigenmächtig einer Version den Vorzug zu geben (siehe Fig. 3). Dabei werden eigene und fremde Version nebeneinandergestellt und Nutzer√Øn 2 kann sich informiert für eine Version entscheiden, ohne tiefergehende Kenntnisse über die zugrunde liegenden technischen GitLab-Prozesse haben zu müssen. Diese Funktionalität unterstützt den 'im kollaborativen Modus noch stärker im Vordergrund stehenden 'diskursiven Aushandlungsprozess von Annotation und Interpretation und reagiert somit auf die Forderung nach flexiblen disziplinspezifischen Arbeitsabläufen. Das Schaubild (vgl. Fig. 4) verdeutlicht die identifizierten literaturwissenschaftlichen Spielräume (linke Spalte) und die daraus erwachsenden generellen Anforderungen an digitale Arbeitsumgebungen (mittlere Spalte). Wie CATMA 6 diese Anforderungen konkret umsetzt, findet sich in der rechten Spalte."
2022,DHd2022,FISCHER_Frank_Dramatische_Metadaten___Die_Datenbank_deutschs.xml,Dramatische Metadaten   Die Datenbank deutschsprachiger Einakter 1740–1850,"Dîlan Canan Çakir (Universität Stuttgart); Frank Fischer (Higher School of Economics, Moskau)","Einakter, Drama, Literatur, Datenbank, API","Archivierung, Literatur, Metadaten"," Der Einakter galt zu jener Zeit auch als Einübungsform und hat viele Erstlingswerke hervorgebracht, beispielhaft seien die später mit umfangreicheren Werken äußerst erfolgreich gewordenen Dramatiker Gotthold Ephraim Lessing oder Adolph Müllner genannt. Die Datenbank macht auch den Anteil von Autorinnen an der Dramenproduktion sichtbarer (dank der Verknüpfung der Autor*innen mit ihren Wikidata-Einträgen können die Informationen zum Geschlecht automatisch bezogen werden; eine Statistik dazu befindet sich auf unserer Website). Zwar sind nur knapp 5% der erfassten Einakterautor*innen weiblich, allerdings treten so neben bekannteren Vertreterinnen wie Luise Adelgunde Victorie Gottsched und produktiven Theaterautorinnen wie Johanna Franul von Weißenthurn oder Charlotte Birch-Pfeiffer auch unbekanntere Verfasserinnen von Einaktern zutage.  Der Hinweis auf Frankreich ist auch deshalb wichtig, weil ein knappes Drittel der Einakter nachweislich aus Übersetzungen oder Adaptionen besteht, und zwar in großer Mehrzahl (über 90%) aus dem Französischen; andere Sprachen spielen kaum eine Rolle. Die Vorlagen für diese Übersetzungen oder Bearbeitungen haben wir ebenfalls in der Datenbank erfasst, inklusive Metadaten zu Werken und Autor*innen. Über Wikidata werden dann auch die Geokoordinaten bezogen und mit der JavaScript-Bibliothek Leaflet auf eine Weltkarte gemappt, die ebenfalls auf der Website zu finden ist. Dadurch wird auch ein geodatenbasierter Zugang zum Korpus möglich. Auf besagter Karte wird deutlich, dass sich die Handlung deutschsprachiger Einakter für den von uns beobachteten Zeitraum vor allem in den Grenzen des Alten Reichs entfaltet. Neben Berlin und Wien ist allerdings Paris die mit Abstand häufigste Ortsangabe, was freilich an der Vielzahl von Übersetzungen liegt. Einakter, die in der Neuen Welt, im Nahen, Mittleren oder Fernen Osten spielen, bilden die absolute Ausnahme. Durch unseren exhaustiven Ansatz bei der Korpuszusammenstellung lassen sich diese Ausnahmen aber zum ersten Mal systematisch erfassen.  Abb. 2 zeigt die Anzahl von Szenen pro Einakter in chronologischer Verteilung. Der Durchschnitt liegt bei 14 Szenen. Für die Mehrzahl der Stücke ist die Szenenanzahl zwischen 7 und 20 angesiedelt, wie man im dunklen Innern der Datenwolke erkennen kann. Als Vergleich seien die 131 zwischen 1740 und 1850 publizierten fünfaktigen deutschsprachigen Dramen des GerDraCor-Korpus (vgl. Fischer et al. 2019) herangezogen, deren durchschnittliche Aktlänge bei knapp unter 7 Szenen liegt. Die Spieldauer für die Einakter beträgt typischerweise zwischen 15 Minuten und einer Stunde (teils ist diese explizit mit angegeben). Die Figurenanzahl der Einakter liegt durchschnittlich bei 7, wobei die Mehrzahl der Stücke zwischen 5 und 7 Personen bzw. Sprechinstanzen im Personenverzeichnis auflistet (Abb. 3). Um wieder mit GerDraCor zu vergleichen: Dort liegt die durchschnittliche Anzahl der Figuren für die 131 Fünfakter des Zeitraum 1740–1850 bei 29. Das Verhältnis zwischen weiblichen und männlichen Figuren liegt für dieselben Fünfakter bei 23:100; hingegen in der Einakter-Datenbank (momentan 2305 Stücke) bei 46:100. Der höhere Anteil weiblicher Figuren in einaktigen Stücken lässt sich unter anderem mit bestimmten Handlungsschwerpunkten erklären (viele einaktige Eheanbahnungskomödien bei nur wenigen Tragödien und historischen Dramen).  Die hier präsentierten Ergebnisse können jeweils auf dem aktuellen Stand der Datenbank überprüft werden. Vorgehaltene und errechnete Daten werden über leicht zugängliche Endpunkte exponiert, die unsere Daten im CSV- und JSON-Format anbieten. Die Daten können entweder für die Nutzung in Tabellenkalkulationen wie Microsoft Excel oder LibreOffice Calc heruntergeladen oder direkt über eine Programmiersprache bezogen werden. Beispiele für die Verwendung in R gibt es auf der Website des Projekts."
2022,DHd2022,REBORA_Simone_Towards_a_Computational_Study_of_German_Book_R.xml,Towards a Computational Study of German Book Reviews   A Comparison between Emotion Dictionaries and Transfer Learning in Sentiment Analysis,"Simone Rebora (University of Bielefeld, Germany); Thomas Messerli (University of Basel, Switzerland); Berenike Herrmann (University of Bielefeld, Germany)","Book reviews, Machine learning, Sentiment analysis","Inhaltsanalyse, Annotieren"," The dictionary-based and TL approaches were evaluated on two manually annotated datasets, working with two annotators: in the first dataset (~21,000 sentences), the annotation task was that of identifying evaluative language (vs. descriptive language); in the second dataset (~13,500 sentences), the task focused on the distinction between positive and negative sentiment. These two classification tasks form the basis for a large-scale analysis of the LOBO corpus, which segments reviews into evaluative and descriptive passages, to describe differences in evaluation practices across genres (e.g., romance, science fiction) and ratings (1-5 stars).   The evaluation procedure was repeated on Task 2 (positive vs. negative sentiment). Again, inter-annotator agreement was strong for manual annotation of the Gold Standard (Cohen""s Kappa = 0.79). Annotation percentages are shown by Fig. 2 (where the ""other"" category indicates both mixed feelings and the absence of evaluation).  Our results highlight the higher efficiency of TL-methods (see Table 4) and of dictionaries based on vector space models (like SentiArt and AffectiveNorms). They show that computational methods can reliably identify sentiment of book reviews in German. In order to fruitfully use similar methodology to identify types of engagement by reviewers with literature beyond the descriptive/evaluative and positive/negative dichotomies, a useful next step will be to attempt the design of TL-tasks for the identification of more fine-grained evaluative practices. These include the construction of and orientation to particular evaluative scales (e.g. reading pleasure, literary quality) and particular subjects of evaluation (e.g. novels, authors, characters)."
2022,DHd2022,ROEDER_Torsten_Aufbau_eines_Referenzkorpus__Erste_Sätze_in_d.xml,"Aufbau eines Referenzkorpus ""Erste Sätze in der deutschsprachigen Literatur""","Anna Busch (Theodor-Fontane-Archiv, Universität Potsdam); Torsten Roeder (Bergische Universität Wuppertal, Germany)","Korpuserstellung, Segmentierung, deutschsprachiche Literatur","Sammlung, Inhaltsanalyse, Annotieren, Stilistische Analyse, Visualisierung, Literatur","  Der sämtlichen bisherigen Studien zu ersten Sätzen ""mangelnden Gesamtsicht"" (Alt 2020: 246) zu begegnen, ist Anliegen des Korpus ""Erste Sätze in der deutschsprachigen Literatur"". Dazu wird ein Datenkorpus erstellt, publiziert und anschließend in einer Verzahnung von quantitativen und textanalytischen Herangehensweisen eine erste Auswertung unternommen. Als Ausgangsmaterial dienen mehrere Volltextkorpora (Deutsches Textarchiv, Zeno, u.a.), aus denen Texte nach Gattungen extrahiert wurden. Es ist deutlich, dass die vorhandenen Volltextangebote zwar unterschiedlich reichhaltige Strukturinformationen über das jeweilige Dokument bieten, aber die automatische Abgrenzung geschlossener Texteinheiten oft nicht trivial und ohne Einzelprüfung nicht zuverlässig möglich ist (z.B. bei Sammelbänden, Texten mit mehreren Kapiteln, Texte in mehreren Bänden). Dies bildet allerdings die Voraussetzung für das Extrahieren der ersten Sätze. Hinzu kommt, dass der Beginn des ""poetischen Texts"" durch z.B. vorangestellte Vorworte, Widmungstexte oder Einleitungen automatisiert nicht immer eindeutig zu lokalisieren ist. Ferner ist die Abgrenzung von ""ersten Sätzen"" ein semantisches Problem. Sätze lassen sich als grammatisch-analytische Einheiten begreifen, die durch bestimmte Satzzeichen voneinander abgetrennt werden, was der maschinellen Verarbeitung entgegenkommt. Jedoch unterscheiden und verändern sich die zur Abgrenzung eines Satzes verwendeten Zeichen erheblich (man betrachte allein die Entwicklungen zwischen dem 17. und 18. Jahrhundert). Die absolute Trennschärfe mancher Satzzeichen steht zudem kontextabhängig infrage, weshalb Sätze teils auch als Sinneinheiten zu begreifen sind, in denen Satzzeichen eine strukturierende, aber nicht unterbrechende Funktion innewohnt (vgl. Abb. 2a/b). Sollte man also eher von einem fließenden ""Beginn"" oder ""Anfang"" sprechen? Bei der Bestimmung der ""ersten Sätze"" spielen somit Unschärfebereiche hinein, die sich wiederum auf Korpuskonsistenz und -vergleichbarkeit auswirken können. Das derzeitig erstellte Korpus ist vollständig mitsamt Metadaten und Quellenangaben inkl. Positionsangaben in TEI codiert. Gattungsabhängig bewegt sich die Anzahl der Satzanfänge zwischen 100 und 1000 Einträgen. Mithilfe der manuell und automatisch erstellten Annotationen lässt sich das Korpus nach verschiedenen Parametern analysieren und visualisieren, beispielsweise nach Veröffentlichungsdatum, Textgattung, Geschlecht von Verfasserin oder Verfasser, Personen-, Orts- oder Zeitbezüge im Text (vgl. Abb. 1c) oder Länge des Gesamttexts. Außerdem wird dokumentiert, welchen Auswahlkriterien die jeweiligen Datenquellen unterlagen und wie dies im Hinblick auf die Ausgewogenheit des Korpus bei der Auswertung berücksichtigt werden sollte (vgl. Hug/Boenig 2021). Zur Dissemination des Korpus wurde 2021 das Twitter-Projekt ""@satzomat"" gelauncht, das täglich zwei erste Sätze sendet (vgl. Abbildungen 1–3). Ziel ist es, eine ""Typologie des ersten Satzes"" mithilfe computerphilologischer Auswertungsverfahren zu erstellen sowie zu fragen, inwieweit Gattungen im Verlaufe der Geschichte bestimmte Typen von ersten Sätzen determinierten (z.B. Landschaftsbild, Rahmenhandlung) und ob sich weitere Korrelationen mithilfe der Metadaten und Annotationen feststellen lassen.   "
2022,DHd2022,CHARVAT_Vera_Maria_Computational_Literary_Studies_Data_Lands.xml,Computational Literary Studies Data Landscape Review,"Ingo Börner (Universität Potsdam); Vera Maria Charvat (Österreichische Akademie der Wissenschaften, Austrian Centre for Digital Humanities and Cultural Heritage (ACDH-CH)); Matej None (Österreichische Akademie der Wissenschaften, Austrian Centre for Digital Humanities and Cultural Heritage (ACDH-CH)); Micha≈Ç Mrugalski (Humboldt-Universität zu Berlin); Carolin Odebrecht (Humboldt-Universität zu Berlin)","Computational Literary Studies, Metadaten, Linked Open Data, Data Landscape, Research Discovery, FAIR principles","Modellierung, Community-Bildung, Organisation, Infrastruktur, Metadaten, Forschungsergebnis"," Das übergeordnete Ziel von ""Computational Literary Studies Infrastructure"" Um die Auffindbarkeit und den forschungsorientierten Zugang zu literarischen Daten für die CLS-Community zu ermöglichen, ist eine Inventarisierung der CLS-Datenlandschaft erforderlich, die forschungsrelevante Kriterien für die Datenauswahl sowie deren Erfassung und Beschreibung anwendet. Mit dieser Inventarisierung, die wir in Form einer Dabei stellen wir uns unter anderem folgende Fragen: Welche Beschreibungsmerkmale sind für die Daten als Kollektion im Sinne einer eigenen epistemischen Einheit wesentlich? Welche Beschreibungsmerkmale sind in Bezug auf die literarischen Vorlagen und deren Aufbereitungen wichtig? Wie kann nach Kollektionen oder einzelnen Datensätzen im Sinne der Die Ergebnisse unserer Aufbauend auf der Review werden die Ergebnisse in Form eines stetig wachsenden, interaktiven Online-Katalogs literarischer Corpora für die CLS-Community bereitgestellt. Dieser wird eine umfassende Übersicht über die verfügbaren Ressourcen inklusive ausführlicher beschreibender Metadaten liefern und die üblichen Abfrage- und Erschließungsmöglichkeiten mittels verschiedener Such- und Filtermechanismen bieten. Konzeptueller Ausgangspunkt für die strukturierte Sammlung der Informationen ist das Metamodell für Korpusmetadaten (MKM; Odebrecht 2018) 'ein, generisches erweiterbares Beschreibungsmodell, für die zentralen Entitäten Während das Modell selbst abstrakt definiert ist, erarbeiten wir eine kongruente/entsprechende Ontologie im OWL-Format (OWL, 2012), welche eine Repräsentation der Daten in RDF (Resource Description Framework) Ebenso ist zu berücksichtigen, dass dieser Katalog Teil von einem komplexen Gefüge an Ressourcen, Providern und Disseminationskanälen bzw. Aggregatoren ist. Die Position des Katalogs in diesem Gefüge und seine Beziehung zu verwandten Aggregatoren wie CLARIN VLO (Virtual Language Observatory) Die"
2022,DHd2022,BROTTRAGER_Judith_Doctoral_Consortium__Judith_Brottrager.xml,Relating the Unread   Modellierungen der Literaturgeschichte,"Judith Brottrager (TU Darmstadt, Germany)","Distant Reading, Kanonisierung, Digitale Literaturwissenschaft","Distant Reading, Kanonisierung, Digitale Literaturwissenschaft"," Mein Dissertationsprojekt folgt dieser Tradition, indem kanonisierte und nicht-kanonisierte literarische Werke auf unterschiedlichen Ebenen mit Rückgriffen auf literaturhistorische Daten miteinander in Beziehung gesetzt werden. Die für die Untersuchungen erstellten Korpora und Datensätze umfassen etwa 1.200 englisch- und deutschsprachige Texte von 1688 bis 1914, wodurch diachrone und synchrone Vergleiche von Kanonisierungsprozessen und -mustern über Sprachgrenzen hinweg ermöglicht werden. Durch diese Datenvielfalt sollen einerseits etablierte literaturhistorische Narrative untersucht und quantitativ überprüft werden und andererseits literaturwissenschaftliche Kategorien wie Kanonisierung und Wertung so operationalisiert werden, dass sie mit computationellen Methoden zur Bestimmung von Textähnlichkeiten gewinnbringend kombiniert und diese Öhnlichkeit schließlich als Netzwerkmodelle dargestellt werden können. Der Korpusaufbau folgt einem systematisch angepassten Ansatz von Algee-Hewitt und McGurl, der darauf abzielt, von einem vorgefundenen zu einem maßgeschneiderten Korpus zu gelangen, indem Bestenlisten, Bestsellerlisten und von Expert*innen kuratierte Literaturlisten kombiniert werden, um ein repräsentatives Korpus für die englischsprachige Literatur des 20. Jahrhunderts zu erstellen (Algee-Hewitt/McGurl 2015). Durch die Kombination dieser Listen decken Algee-Hewitt und McGurl drei Ebenen der literarischen Produktion ab: den normativ-exklusiven Kanon, populäre Texte und von Expert*innen für Postkoloniale und Feministische Literaturwissenschaft vorgeschlagene Werke. Für die Umsetzung für die Zeitspanne von 1688-1914 wurde dieser Ansatz systematisch adaptiert, indem narrative Literaturgeschichten, Anthologien und (spezialisierte) Sekundärtexte, die diese Ebenen abdecken, identifiziert und als bibliografische Quellen für die Korpuserstellung genutzt wurden. Der Workflow umfasst Webscraping, X-Technologien/Transformationen und Retro-Digitalisierungen. Analog zur Korpuserstellung wurden Daten zur Kontextualisierung der jeweiligen Korpustexte, aber auch der gesamten literarischen Produktion gesammelt. Durch diese Daten können die Korpora mit den von Algee-Hewitt et al. als ""the published"" Die gesammelten Daten werden neben diesen Metadatenanalysen auch für die Operationalisierungen der literaturwissenschaftlichen Konzepte der Kanonisierung und Wertung eingesetzt. Aufbauend auf die theoretischen Grundlagen von Heydebrand und Winko werden Kanonisierung und Wertung als Scores implementiert, die ausdrücken, wie hoch die Wahrscheinlichkeit ist, dass ein bestimmter Text sehr kanonisiert ist beziehungsweise zur Entstehungszeit sehr gut rezipiert wurde (Heydebrand/Winko 1996). Ein besonderes Augenmerk liegt hierbei auf der Differenzierung der Konzepte und der Einbindung der Rezeptionsebene über Marker für Publikumsinteresse (wie Einträge in Leihbibliothekskatalogen und Zweitauflagen innerhalb einer Generation) und sprachliche Werturteile, die durch Sentiment Analysen vergleichbar werden. Unter Verwendung der generierten Scores soll schließlich untersucht werden, ob Kanonisierung und Wertung mit textintrinsischen Merkmalen in Verbindung gebracht werden können. Stilometrische Berechnungen, Topic Modeling und Word Embeddings sowie wortartenbasierte Ansätze sollen dabei als alleinstehende Analysen der Textähnlichkeiten durchgeführt werden. Als zusätzliche Analysen- und Visualisierungsmethode dienen Netzwerkmodelle, die anhand der Ergebnisse der Textähnlichkeitsberechnungen erstellt werden, zur Exploration von Öhnlichkeitsstrukturen. Besonders auf dieser Ebene soll der Bezug zur literaturwissenschaftlichen Forschung durch die Identifikation von dichten stilistischen Öhnlichkeitsgruppen und durch aus den Modellen abgeleitete Einzeltextanalysen hergestellt werden."
2022,DHd2022,SCHUMACHER_Mareike__Wie_Wölkchen_im_Morgenlicht____zur_autom.xml,"""Wie Wölkchen im Morgenlicht"" Zur automatisierten Metaphern-Erkennung und der Datenbank literarischer Raummetaphern laRa","Mareike Schumacher (Technische Universität Darmstadt, Germany)","Metaphern, Textanalyse, Methodenvergleich","Kontextsetzung, Literatur, Metadaten, Methoden, benannte Entitäten (named entities), Text"," Zwei grundlegende Probleme bei der Betrachtung von Metaphern im Vergleich zu nicht-metaphorischen Ausdrücken sind Uneigentlichkeit und Variabilität. Metaphern werden grundsätzlich aus drei Größen konstruiert: dem sprachlichen Ausdruck, dem, was der sprachliche Ausdruck im Wortsinne bezeichnet und etwas Öhnlichem (vgl. Wenz 1997: 32). Dabei unterscheidet sich die Grammatik metaphorischer Ausdrücke meist in nichts von wörtlich gemeinten Phrasen (vgl. Thaller 2021: 91). Metaphern sind dynamische Konstrukte, die eine Entwicklung durchlaufen von Einführung, über Etablierung zur Konventionalisierung und schließlich bis hin zum Übergang in den eigentlichen Sprachgebrauch (vgl. Thaller 2021), d.h. Ausdrücke können die einmal aufgebaute Metaphorik auch wieder verlieren. Metaphern sind aber nicht nur ein interessantes sprachliches Phänomen, sondern können auch als prägende Ausdrücke menschlichen Handelns fungieren (vgl. Blumenberg 1971: 213 und Wenz 1997: 33). Sie sind eine wesentliche Basis menschlichen Denkens.  Im Rahmen meiner Dissertation Named Entity Recognition (NER) ist ursprünglich eine computerlinguistische Methode zur automatischen Erkennung und Klassifizierung klar benannter Einheiten (vgl. Schumacher 2018: ¬ß1). Die am häufigsten in Named-Entity-Recognition-Tools implementierten Kategorien sind Personen, Orte und Organisationen. Für die literaturwissenschaftliche Nutzung von NER bedarf es allerdings einer Domänenadaption, bei der sowohl die implementierten Kategorien als auch die Traningsdaten angepasst werden müssen. Die Methode wurde bereits erfolgreich für die Erkennung literarischer Figuren adaptiert (vgl. Jannidis et al. 2015) und auch eine Unterklassifizierung nach Genderzuweisungen ist möglich und für literaturwissenschaftliche Forschung gewinnbringend (vgl. Schumacher und Flüh 2020). Die Kategorie des Ortes ist für literarische Texte nahezu ebenso relevant wie die der Person bzw. Figur. Statt eines komplexen Raumkonzeptes, das in mehrere Unterkategorien aufgeteilt wird, werden bei der linguistischen Nutzung von NER-Tools lediglich Ortsnamen erkannt. Um ein NER-Tool so zu adaptieren, dass es Raumreferenzen erkennen und in eine von sieben Kategorien literarischen Raumes einsortieren kann, wurde ein Machine-Learning-Training durchgeführt. Im Folgenden werden diejenigen Ausschnitte des Trainings vorgestellt, die zeigen, inwiefern die automatisierte Erkennung von Raummetaphern dabei gescheitert ist. Die Wahl des Tools fiel auf den in den Digital Humanities gut etablierten Stanford Named Entity Recognizer (Finkel, Grenager und Manning 2005), in dem kontextsensitive Conditional-Random-Fields-Algorithmen (zu CRF-Algorithmen vgl. Sutton und McCallum 2010) implementiert sind (Manning et al. 2014). Das Trainingskorpus besteht aus Ausschnitten aus 80 Romanen aus vier Jahrhunderten (18–21). Aus jedem Jahrhundert wurden 20 Erzähltexte integriert, sodass das Trainingskorpus einen gleichmäßigen Aufbau aufweist Die Testergebnisse können wie in Abb. 1 visualisiert werden: Der Vergleich mit der insgesamt am besten erkannten Kategorie ""Ort"" zeigt, dass sich für die Testergebnisse der Kategorie ""Metapher"" keine Regelmäßigkeiten ergeben. Für einzelne Testtexte zeigen jahrhundertspezifische Trainingsdaten oder Trainingsdaten aus zwei Jahrhunderten den größten Trainingserfolg. Für andere ist ein Trainingskorpus aus einem ganz anderen zeitlichen Kontext passender. Eine schrittweise Ausweitung des alle vier Jahrhunderte umfassenden Trainingsmaterials zeigt keinerlei regelmäßigen Zuwachs der Erkennungsgenauigkeit. Einen viel typischeren Trainingsverlauf zeigt die Ortskategorie. Hier ist hauptsächlich die Größe des Trainingskorpus ausschlaggebend. Je mehr Trainingsdaten eingesetzt werden, desto höher die Erkennungsquote. Auch der zeitliche Kontext ist hier nur bei wenigen Testtexten bedeutend. Insgesamt zeigt die Kummulierung der Trainingsdaten eine zwar langsame aber kontinuierliche Steigerung. Das Training der Metaphernerkennung gleicht hingegen einem Glücksspiel: Mal führt das Hinzufügen neuer Trainingsdaten zu einer Verbesserung des Classifiers, mal wird dadurch alles buchstäblich zurück auf Null gesetzt. Als alternatives Recherche-Tool und um besser zu verstehen, warum die Automatisierung von Metaphernerkennung so problematisch ist, wurde die relationale Graphdatenbank literarischer Raummetaphern Durch die Anreicherung mit Propertys kann die Datenbank auf vielfältige Weise durchsucht werden. Wenn sowohl das Wortmaterial als auch die Bedeutung einer Metapher extrem ähnlich waren, wie z.B. der Fall bei ""in schlechte Hände geraten"" und ""in schlechten Händen sein"", wurden die Phrasen mit einem zweiten Typ von Relation untereinander verbunden (Relationstyp Das Netzwerk der Raummetaphern in Abb. 3 zeigt, dass es nur wenige zentrale Metaphern gibt, d.h. Metaphern, die sowohl viele Varianten aufweisen als auch in vielen Texten vorkommen. Viele Texte bilden mit ihren Raummetaphern eigene Cluster, die vielfach nur über Variationen mit anderen Metaphern, also nur indirekt mit anderen Texten verknüpft sind. Manche Text-Metaphern-Cluster sind gar nicht mit dem Hauptnetzwerk verbunden. Noch deutlicher wird das Gefüge, wenn eine Raummetapher einzeln betrachtet wird. Die Wahl fiel auf die Ein-Wort-Metapher ""Weg"", die keine der zentralsten Metaphern ist, sondern einen mittleren Vernetzungsgrad aufweist. Abb. 4 zeigt diese Raummetapher mit ihren Varianten und den Erzähltexten, in denen sie vorkommen: "
2022,DHd2022,TRILCKE_Peer_Poesie_als_Fehler__Ein__Tool_Misuse__Experiment.xml,"Poesie als Fehler   Ein ""Tool Misuse""-Experiment zur Prozessierung von Lyrik","Henny Sluyter-Gäthje (Universität Potsdam, Germany); Peer Trilcke (Universität Potsdam, Germany)","Lyrik, NLP, Fehler","Programmierung, Annotieren, Bereinigung, Stilistische Analyse, Literatur, Text","Analysen der Computational Literary Studies (CLS) vorverarbeiten ihre Untersuchungsgegenstände typischerweise mit Tools des Natural Language Processing (NLP). Dabei weichen literarische Texte aufgrund ihrer historischen und/oder ästhetischen Eigenart teils eklatant von den Daten ab, auf deren Grundlage die Der Das folgende Experiment geht von dieser basalen Überlegung aus. Gegenstand des Experiments ist die Lyrik, der regelmäßig eine ""Tendenz zu erhöhter Devianz"" (Müller-Zettelmann 2000: 100) attestiert wird, die sich in Form gattungsspezifischer ""Störungen"" (Zymner 2019: 29f.) ausdrückt. Wir entwickeln eine Pipeline, die gezielt ""Fehler"" von NLP-Tools provoziert und diese ""Fehler"" regelbasiert typologisiert. Damit möchten wir für die CLS auch exemplarisch den Ansatz des Fehler von Tools wären idealerweise über Daten mit Gold-Standard-Annotationen zu ermitteln. Solche Daten liegen für unser Szenario nicht vor. Deshalb implementieren wir als Workaround eine Pipeline, die folgende Idee umsetzt: Die Verarbeitung einer Zeichenkette durch ein NLP-Tool (Tokenisierung, Lemmatisierung, POS-Tagging) sollte eine Zeichenkette ergeben, die in einem Wörterbuch zu finden ist: Die Ausgabe, die aus der Eingabe ""gehst"" resultiert, sollte sich als Lemma ""gehen"" der Wortart ""Verb"" in einem Wörterbuch finden. Da wir nicht die spezifischen Sprachverarbeitungsprobleme eines individuellen NLP-Tools in den Blick nehmen wollen, lassen wir unser Lyrikkorpus von mehreren Tools prozessieren und betrachten jene Types als potenzielle Fehler, deren von den Tools ausgegebene Lemmata (inkl. POS-Tag) wir nicht in einem Wörterbuch finden. Prozessiert wird ein deutsches kanonbasiertes Korpus mit ""prototypischer"" Lyrik, das Texte von 12 Autor:innen umfasst, die in der statistisch begründeten Anthologie von Braam (2019) am häufigsten mit Gedichten vertreten sind. Ins Korpus aufgenommen wurden sämtliche im TextGridRepository Unsere ""Fehler-Pipeline"" präferiert Die in die Pipeline eingespeisten Textdateien (txt-Format) werden tokenisiert, POS-tagged und lemmatisiert (Abb. 1). Um potenzielle Fehler als taggerspezifische Fehler einzuordnen, verwenden wir vier NLP-Tools. Diese sind frei verfügbar, stellen Bei der Verb-Rekonstruktion werden abgetrennte Verbpartikel regelbasiert (mithilfe von POS-Tag und Abstandsmaßen) an das dazugehörige Lemma des Verbs angefügt. Die darauffolgende Fehlerbetrachtung ist auf dem Vergleich der Lemmata mit Wörterbüchern gestützt. Da wir erwarten, dass Fehler, die ihren Ursprung in der Domänenspezifität haben, sich hauptsächlich auf Inhaltswörter, d.i. Nomen (NOUN), Verben (VERB) und Adjektive (ADJ), beschränken, werden der Wörterbuchvergleich und die darauffolgenden Schritte nur für diese Wortarten durchgeführt. Die Lemmata werden unter Berücksichtigung der Wortart in der lexikalisch-semantischen Ressource GermaNet Für jeden Type erzeugen wir Listen von Lemmata pro Tool. Für die 5.144 Gedichttexte ergibt das eine Typezahl von 70.422 (Tab. 1, Spalte ""all""), die je nach Tagger aufgrund verschiedener Tokenisierung und POS-Tagging zu unterschiedlichen Tokenzahlen führt. Um zu verhindern, dass Fehler toolspezifisch sind, werten wir einen Type nur dann als potenziellen Fehler, wenn mindestens zwei Tools den Type lemmatisiert haben und kein Lemma aus den Listen im Wörterbuch gefunden wurde (Tab. 01, Spalte ""pFail""). Diese potenziellen Fehler werden regelbasiert in Fehlertypen eingeteilt. Auf dem pFail-Set führen wir eine regelbasierte Typologisierung durch. Die Typen postulieren wir ausgehend von manuellen Inspektionen des pFail-Set. Für jeden Typen wird eine Regel formuliert. Die Regeln werden daraufhin in einer spezifischen Reihenfolge auf das pFail-Set angewendet. Mehrfachtypisierungen sind nicht möglich; die Reihenfolge der Regelanwendung hat mithin Konsequenzen für die Menge an jeweils identifizierten Vorkommnissen. Die Reihenfolge lautet: CONTRACT, ELISION_APO, PUNC, SHORT, COMP_DASH, COMP, PART_ADJ, ELISION_SIMPLE, ORTH_UPPER, ORTH_SZ, PREFIXED, EPITHESIS, ELISION_END. Die Typendefinitionen führt Tab. 2 auf. 53,33 % der Types im pFail-Set für Lyrik und 59,88 % der Types im pFail-Set für Prosa werden identifiziert (vgl. Tab. 3). Die identifizierten Typen können zu Gruppen zusammengefasst werden: PUNC und SHORT sind überwiegend unterhalb der Wortebene anzusiedelnde Zeichen, meist Rauschen, das bei Lyrik und Prosa in vergleichbarem Umfang auftaucht. ORTH_SZ dokumentiert den ebenfalls bei Lyrik und Prosa vergleichbar ausgeprägten Effekt der Die 10 weiteren Typen lassen sich zu drei Gruppen zusammenführen. COMP_DASH, COMP, PART_ADJ, PREFIXED versammeln Zu resümieren, dass die spezifisch lyrische ""Störung"" für die NLP-Tools insbesondere aus der Darin zeigen sich zwei Felder für Anschlussforschungen: Erstens wäre zu erproben, ob sich bessere Pipelines für die automatisierte NLP-Tool-Fehleridentifikation ohne Annotationsdaten konzipieren lassen, dafür wäre es hilfreich, die Pipeline auf einem kleinen Set an Gold-Standard-Annotationen zu evaluieren; zweitens könnte auf der Grundlage unserer Pipeline gegen die Baseline von 53,33 % das regelbasierte Typologisierungsverfahren optimiert werden. Die manuelle Annotation einer kleinen Sammlung von Gedichten mit Informationen zum Abweichungscharakter jedes einzelnen Wortes würde es ermöglichen, unsere Annahme, dass unser Verständnis der Devianz durch die Nutzbarmachung des Problems der Domänenadaption von NLP-Tools operationalisiert werden kann, zu prüfen. So könnte sichergestellt werden, dass wir durch die Fehlertypisierung der NLP-Tools tatsächlich etwas über die Spezifik des Literarischen erfahren. In jedem Fall haben wir mit dem vorliegenden"
2022,DHd2022,RATH_Brigitte_Muster_von__you__und__thou___Modellierung_der_.xml,"Muster von ""you"" und ""thou"" Modellierung der Anrede im englischen Sonett","Brigitte Rath (Universität Innsbruck, Austria)","Machine Learning Prediction Modelle, Lyrik, Anrede","Annotieren, Stilistische Analyse, Metadaten, Personen, Text","Bekanntermaßen unterscheidet das Early Modern English zwei Reihen von Pronomina der zweiten Person Singular: V-Formen (you, your etc.) und T-Formen (thou, thy etc.) Die T-Formen gehen im Zuge des Sprachwandels zum Modern English im Verlauf des 17. Jahrhunderts verloren (vgl. z.B. Lass 1999: 153), bleiben jedoch in der Lyrik erhalten. So beginnt etwa ein berühmtes, 1850 von Elizabeth Barrett Browning veröffentlichtes Sonett mit diesem Vers: ""How do I love thee? Let me count the ways."" Diese wie selbstverständliche Verwendung der T-Formen in der englischsprachigen Lyrik auch weit nach dem Wandel zum Modern English ist bisher nicht systematisch untersucht. Dieses Projekt sucht daher mit Hilfe digitaler Methoden Antworten auf folgende zwei Forschungsfragen: (1) Ist die Verwendung von T-Formen und V-Formen in Sonetten nach dem Sprachwandel synonym? (2) Falls nein: Welche Muster lassen sich beschreiben? Die Hypothese zur Frage (1) lautet, dass die Verwendung von T-Formen und V-Formen sich als nicht synonym erweisen wird, weil es gerade in der Lyrik eine gesteigerte Sensibilität für die Anrede gibt, und so zu erwarten steht, dass die linguistisch gebotenen Möglichkeiten für Differenzierung voll ausgeschöpft werden. Diese Erwartung widerspricht dem in Gedichtkommentaren häufig anzutreffenden und üblicherweise nicht weiter belegten Hinweis, ""thou"" sei einfach eine in Gedichten anzutreffende Version von ""you"". Hypothesen für Faktoren, die eine Rolle bei der Frage (2) interessierenden Musterbildung spielen könnten, werden vor allem aus der historischen Soziolinguistik gewonnen: So kommen neben potentiellen individuellen Vorlieben von Autor:innen sowie der Entstehungszeit als plausible Faktoren nominale Anredeformen in Frage, weil Studien aus der historischen Soziolinguistik nahelegen, dass bestimmte Anreden (z.B. ""terms of endearment"") mit der Verwendung von T-Formen verbunden sind (vgl. Nevala 2004: 2146; Mazzon 2010), sowie die jeweilige Kategorie des:der Angesprochenen, weil Studien einen Zusammenhang zwischen bestimmten Kategorien von Angesprochenen wie etwa Kinder, Tiere oder Geister und der Verwendung von T-Formen zeigen (vgl. Yang 1991: 258; Carter/McRae 2002: 120-121). Basis für diese Untersuchung ist ein selbsterstelltes Korpus von (bisher) 1.611 englischsprachigen, auf den britischen Inseln zwischen 1530 und 1910 publizierten Sonetten, für das die Gedichttexte manuell in TEI-5 konformem XML transkribiert und mit Metadaten (Autor:in, Titel, Entstehungsjahr, Publikationsjahr) und Annotationen zu nominalen Anredeformen, Kategorie der Adressat:innen (Gott, Mensch, Tier, Naturphänomen etc.), intertextuellen Verweisen und Reimschemata angereichert werden. Mit diesem Korpus wurde eine Reihe von Experimenten mit Machine Learning Prediction Modellen gemacht. Mit fünf verschiedenen Machine Learning Prediction Modellen (Naive Bayes, Support Vector Machine, Decision Tree, Random Forest und XGBoost) wurde jeweils der k-fold cross validation approach (vgl. z.B. Han, Pei, Kamber 2011) durchgeführt. Die jeweiligen Trefferquoten wurden mit drei Baseline-Modellen (ZeroR sowie zwei Modellen, die alle Sonette jeweils einer Klasse zuordnen, hier: AlwaysT und AlwaysV) auf der Basis üblicher Standardwerte für Machine Learning verglichen: Precision, Recall, FMeasure, Accuracy und Area Under the ROC Curve (AUC). (vgl. z.B. Han, Pei, Kamber 2011; Mohri, Rostamizadeh, Talwalkar 2012) Es zeigt sich, dass Machine Learning Modelle, insbesondere XGBoost, bessere Ergebnisse als die Baseline-Modelle für die Vorhersage liefern können. Da dieses Modell einen hohen Anteil an Fällen korrekt zuordnet, folgt, dass das Modell Regeln in der Verteilung von T- und V-Formen erkennt, dass T- und V-Formen im Sonett also nicht austauschbar sind. Auf der Basis dieser Experimente kann so die erste Frage, ""Ist die Verwendung von T-Formen und V-Formen in Sonetten nach dem Sprachwandel synonym?"" tentativ mit nein beantwortet werden. Für Hinweise auf mögliche Faktoren, die die Musterbildung beeinflussen, wurden mit den Machine Learning Prediction Modellen Ablation-Experimente durchgeführt: Input-Faktoren wurden individuell entfernt und die jeweilige Performanz des Modells erneut gemessen. Sinkt die Vorhersagekraft des Modells durch das Entfernen eines Faktors, so spielt dieser Faktor für die Vorhersage dieses Modells eine Rolle, was als ein erstes Indiz dafür gewertet werden kann, dass der entsprechende Faktor zur Musterbildung auch jenseits des Modells beitragen könnte. Es zeigt sich, dass dabei die Verwendung des Pronomens ""ye"", das bisher bei der Entwicklung von V- und T-Formen kaum beachtet wird, eine wichtige Rolle spielt; als weiterer möglicher Faktor erweist sich die Kategorie der Angesprochenen. Das Projekt bietet für die historische Linguistik einen Beitrag zur präziseren Beschreibung der Sprachentwicklung. Für die Literaturwissenschaft erlaubt diese erstmalige systematische Beschreibung der Verteilung von T-Formen und V-Formen in englischsprachigen Sonetten bessere Gedichtinterpretationen, weil sie erstens überhaupt ein Augenmerk auf die verwendeten Pronomen der Anrede legt und zweitens die im Einzeltext gewählten Formen nun vor dem Hintergrund eines Musters gelesen werden können. Das Projekt trägt so zur aktuellen Forschungsdiskussion zur Anrede in der Lyrik bei. (vgl. z.B. Culler 1981, Culler 2015, Hedley 2009, Keniston 2006, Pollard 2012, Waters 2012) Dieses Projekt wurde vom Vizerektorat Forschung der Universität Innsbruck mit Mitteln aus der Aktion D. Swarovski und vom Forschungszentrum Digital Humanities der Universität Innsbruck durch Mittel aus dem DI4DH Programm unterstützt und so erst ermöglicht. Die Korpuserstellung übernahmen mit einem ebenso scharfen Blick fürs Detail wie für das Gesamtprojekt Marina Höfler, Serena Obkircher und Teresa Wolf. Die Machine Learning Prediction Models wurden mit großer Umsicht von Ario Santoso und Mingzi Kong konzipiert, implementiert und trainiert."
2022,DHd2022,SCHMIDT_David_Adapting_Coreference_Algorithms_to_German_Fair.xml,Adapting Coreference Algorithms to German Fairy Tales,"David Schmidt (Universität Würzburg, Germany); Markus Krug (Universität Würzburg, Germany); Frank Puppe (Universität Würzburg, Germany)","Koreferenzauflösung, Märchen, Domänenadaption","Inhaltsanalyse, Literatur, benannte Entitäten (named entities), Text","Coreference Resolution has been posing an ongoing challenge to researchers for more than 50 years. It is the task of grouping mentions (concrete or abstract references, represented as textual spans) into clusters representing entities. The approaches for solving this problem have been manifold and range from rule-based approaches (Lee et al., 2013) over classical machine learning approaches (Rahman and Ng, 2009) to modern approaches based on Deep Learning (Lee et al., 2017; Joshi et al., 2020). Coreference Resolution can act as a ""glue"" between information that is extracted on a local level (usually sentences) in order to obtain representations for an entire document or a collection of documents. This enables many interesting downstream applications such as the creation of character networks (Elson et al., 2010; Krug, 2020) or tracking of events involving central objects in textual media (such as the dagger in Emilia Galotti) (Hatzel and Biemann, 2021). The transfer of existing approaches to new domains or types of text usually comes with a drop in performance. In this work, we examine the performance of a rule-based and an end-to-end Deep Learning algorithm and their adaptability to the domain of German fairy tales. These experiments should provide insight into: a) the drop experienced from one kind of texts to another b) the reliability of state-of-the-art Deep Learning approaches compared to rule-based approaches for a change of texts and c) Capabilities for the adaptation to mitigate this natural drop in performance. This helps to estimate the required amount of manual work that is to be expected when transferring to a new kind of text, especially when the new type features a low number of annotated documents. For this we use fragments of German novels provided from the DROC corpus (Krug et al., 2018) and as target domain we make use of annotated fairy tales by the Brothers Grimm. In the next section we present our data, followed by the coreference algorithms as well as the methods for the domain adaptation in more detail. We conclude the paper by presenting and discussing the results of our experiments and potential follow up work. There are several recent works that evaluate the performance of coreference resolution models when applied to a different domain than they have been trained on. Srivastava et al. (2018) examine the performance of several coreference resolution systems (rule-based, statistical and projection-based) on English and German out-of-domain data and find that the rule-based system is the best choice for their use cases. Han et al. (2021) train a coreference resolution model based on c2f (Lee et al., 2018) and SpanBERT (Joshi et al., 2020) on two different corpora, Ontonotes (Hovy et al., 2006) and their new corpus FantasyCoref. They then evaluate both models on FantasyCoref and find that the model trained on the same domain outperforms the other one. (Toshniwal et al., 2021) examine the generalization capabilities of coreference models by evaluating the performance of longdoc (Toshniwal et al., 2020) on out-of-domain data using several English datasets and find that models which have been trained on several datasets jointly perform better than those trained on a single dataset. The data sets for our experiments were the DROC corpus (Krug et al., 2018), comprising 90 fragments of German novels, and 46 tales from the seventh edition of the Children""s and Household Tales by the Brothers Grimm Table 1 shows some statistics about the mentions in the documents of DROC and the fairy tales. One can see that a document in DROC is on average about twice as long as a document of the fairy tales. Names are used a lot less often in fairy tales, while the usage of noun phrases increases and that of pronouns is comparable. There is also an important difference regarding the entities that are referred to by the annotated mentions: In DROC, only human characters are annotated. In the fairy tales, animals and legendary beings (like giants) are also annotated because they are important (and sometimes the only) characters (e.g. the Wolf in A notable difference of both corpora to a lot of other corpora like OntoNotes (Hovy et al., 2006) and LitBank (Bamman et al., 2020) is how the mentions are annotated: OntoNotes annotates the maximal extent of a span (e.g. ""[eine kleine süße Dirne]"") while DROC and the fairy tales only annotate the heads (""eine kleine süße [Dirne]""). For the experiments, DROC was split (a fix split) into a training set and a test set in a ratio of 80% to 20%: 72 documents for training and 18 for evaluation. The fairy tales were evaluated via five-fold cross validation. In order to assess the capabilities of domain adaptation from German novels to German fairy tales, we made use of a rule-based coreference resolution system and a model based on neural networks. We briefly present both methods followed by the way of adaptation. The rule-based approach we use is an adaptation of the sieves algorithm by (Lee et al., 2013) to German (Krug et al., 2015). It partitions its rules into so-called sieves, which are ordered by the precision of their rules and applied one after the other to a document. This enables the rules to make use of the decisions of previously applied rules. Most rules use string matching to resolve names and noun phrases. Among the first sieves is also one that uses information about direct speeches to resolve all first person pronouns to the speaker and all second person pronouns to the addressee, and another that resolves relative and reflexive pronouns based on dependency parse trees. All other pronouns are resolved at the end since they do not possess much helpful information and can only be resolved unreliably (compared to a lot of names and noun phrases). As Deep Learning architecture, we decided to use c2f (Lee et al., 2018) The adaptation of both approaches was done as follows: Rule-based approach: Most rules in the sieves algorithm previously skipped family relation words and did not try to resolve them to an antecedent. In fairy tales, family relation words are most often unique (e.g. there is only one character called mother and one called father), so family relation words now are resolved to an antecedent if they are preceded by a definite article. Reflexive pronouns are resolved with the help of a dependency parse tree, which was not possible for several reflexive pronouns in the fairy tales. These are now resolved together with most other pronouns (lacking information about gender and number, hardly any antecedent can be ruled out, so they are usually resolved to the first that is checked). In addition to that, there were a few small changes that were done as the result of an error analysis on the fairy tales but are not motivated by the domain (they would probably also slightly improve the results on DROC). Deep Learning approach: We trained and evaluated three variants of the c2f algorithm: c2f trained on DROC (c2f D) for 75000 steps, c2f trained on the fairy tales (c2f FT) for about 50000 steps and c2f pre-trained on DROC for 75000 steps and fine-tuned on the fairy tales for an additional 20000 steps (c2f D+FT) Table 2 displays the results of the sieves algorithm (old and adapted version) and c2f (trained on DROC, the fairy tales or both) on DROC (first two rows) and the fairy tales. As metrics we use MUC (Vilain et al., 1995), B¬≥ (Bagga and Baldwin, 1998), CEAF The results show multiple interesting aspects: We have shown that domain adaptation of both, a rule-based system and a Deep Learning based system, yields substantial improvements to coreference resolution on a target domain (in our case fairy tales). The evaluation also opens possibilities for further combination of the results of the rule-based system and the Deep Learning based system, which we leave for further work."
2022,DHd2022,GÖGGELMANN_Michael_Auf_den_Spuren_einer_altnordischen_Saga__.xml,Auf den Spuren einer altnordischen Saga-Östhetik Poetologische Aussagen in den Erzählerbemerkungen der Isländersagas,"Michael Göggelmann (Universität Tübingen, Germany); Anna Katharina Heiniger (Universität Tübingen, Germany); Nils Reiter (Universität Köln, Germany); Angelika Zirker (Universität Tübingen, Germany)","Annotation, Östhetik, Isländersagas, Literarisierung, Poetologie","Annotieren, Stilistische Analyse, Visualisierung, Literatur, Text, Visualisierung","Der Vortrag stellt die systematischen Annotationen von Erzählerbemerkungen in den anonym überlieferten, mittelalterlichen Während narrative Texte und deren systematische Annotation bereits vielfach Untersuchungsobjekt innerhalb der Digital Humanities waren (Zinsmeister 2016; Gius/Jacke 2017; Adelmann et al. 2018; Ketschik et al. 2020), zeigt sich das Innovationspotenzial der vorliegenden Studie in zweierlei Hinsicht: es wird sowohl das bislang quantitativ gänzlich unerschlossene Die ca. 40 überlieferten Obwohl sich die Bei intratextuellen Verweisen handelt es sich um in der Forschung bislang kaum beachtete Phänomene in den Erzählerbemerkungen, die wir als Mittel des produktiven Austauschs zwischen der intradiegetischen literarischen Praxis und der extradiegetischen Welt des Publikums betrachten. Um diese zu sammeln, zu systematisieren und zu kontextualisieren sowie im Hinblick auf die narrative (Selbst-)Reflexion in den Isländersagas auszuwerten, wurden deshalb solche Öußerungen der Erzählstimme als Ausgangspunkt gewählt, die sich mit dem Erzählen selbst befassen. In Vorarbeiten zu dieser Studie wurden Erzählerbemerkungen in den Isländersagas in fünf Kategorien eingeteilt, die ihrerseits die Grundlage für die ersten Annotationsrichtlinien bilden. In der vorliegenden Studie liegt das Augenmerk auf vier näher untersuchte Sagas. Bereits zu Beginn des Annotationsprozesses (wobei wir der Anleitung in Reiter 2020 folgten) wurde deutlich, dass diese für eine produktive Umsetzung in mehreren Schritten geschärft und durch zusätzliche Kategorien ergänzt und ausdifferenziert werden müssen. Die Überarbeitung der Richtlinien ist bisher in fünf aufeinanderfolgenden Runden vorgenommen worden, so dass diese nun eine erste stabile Form mit sechs Annotationskategorien und meist mehreren Unterkategorien erreichten. Nachfolgend konzentrieren wir uns aus Platzgründen auf die Kategorie der intratextuellen Bezüge 'die Annotationen der anderen Kategorien Die Annotation der Erzählerbemerkungen in den Isländersagas wurde mit Hilfe der Software CorefAnnotator (Reiter 2018) vorgenommen. Die Kategorie umfasst alle intratextuellen Bezüge, die die Erzählstimme in einer Saga herstellt und gehört zu den am häufigsten annotierten Kategorien. In mehreren Unterkategorien werden bei der Annotation der Sagas verschiedene Arten intratextueller Verweise erfasst. Auf der intratextuellen Ebene nimmt die Erzählstimme eine narrative Selektion vor, erinnert an frühere Geschehnisse, kündigt Geschehnisse an, die erst noch erzählt werden und informiert darüber, welche Figuren neu eingeführt werden oder für die weitere Handlung keine Rolle mehr spielen. Als Beispiele dieser Kategorie lassen sich oft verwendete Phrasen wie ""sem fyrr var sagt"" (Laxd≈ìla saga: S. 71; ""Wie zuvor erzählt wurde""), Ebenfalls zu den intratextuellen Verweisen zählen häufig verwendete formelhafte Phrasen. Mit Phrasen dieser Art werden zum einen neue Figuren eingeführt (""M[a√∞r] er nefndr B√°r√∞r Heyangrs-Bjarnarson"" (B√°r√∞ar saga Sn√¶fells√°ss: S. 107) Eine weitere intratextuelle Spezifizierung ist die Vorahnung ( Die folgenden Analysen wurden auf Basis der bisherigen manuellen Annotationen vorgenommen. Zum jetzigen Zeitpunkt wurden vier Sagas vollständig annotiert, die in der Forschung als ""randständig"" innerhalb der Gattung der Die unterschiedlich langen Sagas machen einen direkten Vergleich der absoluten Zahlen von Annotationen in ihnen schwierig; in den folgenden Auswertungen werden Häufigkeiten daher normalisiert. Wir betrachten zunächst die Häufigkeit der Annotationen (Abb. 1). Bei den Diese Auswertung der vier Sagas deutet somit darauf hin, dass die Erzählerbemerkungen in jeder Saga ein eigenständiges Profil bilden. Die Hypothese, nach der wir den Isländersagas eine individuelle Ausgestaltung trotz den allen gemeinsamen Typen von Erzählerkommentaren attestierten, konnte also bereits an dieser Stelle plausibilisiert werden. Einen visuellen Eindruck von der Verteilung der Annotationen im Textverlauf liefert Abb. 2. Jeder senkrechte Strich markiert dabei die Annotation eines intratextuellen Verweises, wobei die verschiedenen Farben die Unterkategorien der intratextuellen Bezüge repräsentieren. Grundsätzlich verteilen sich die Annotationen, wie zu erwarten war, über den gesamten Text. Die sich dazwischen befindlichen, teilweise recht großen Lücken sollen kapitelweise anhand der Annotationsdichte nachfolgend genauer untersucht werden. Die Dichte der Annotationen wird in Abb. 3 gezeigt. Für jedes Kapitel und jede Kategorie ergibt sich dabei ein Datenpunkt, die Datenpunkte einer Kategorie sind dann durch Linien verbunden. Die Dichte der Annotationen ist hierbei als relative Anzahl an Annotationen pro Kapitel definiert. Anhand der Grafiken lassen sich die Höhe- und Wendepunkte der jeweiligen Saga ablesen und nachvollziehen. Die ersten zwei großen Ausschläge der Die hier für die Zu bedenken ist, dass Abbildung 3 gegenwärtig ausschließlich die Annotationsdichte der intratextuellen Verweise zeigt. Um ein umfassendes Bild der Annotationsverteilung zu erhalten, müssen in einem nächsten Analyseschritt auch die anderen Annotationskategorien berücksichtigt werden. Daran anschließend wird sich zeigen, ob die Schlüsselstellen in erster Linie mit intratextuellen Kommentaren versehen, oder ob diese auch mit anderen Arten der Erzählerkommentare gekoppelt sind. Durch die vorliegende quantitative Analyse auf Grundlage systematischer Annotationen konnten für die Erzählstimme der Isländersagas Textmerkmale aufgezeigt werden, die zwar zuvor im Einzelfall erkannt, aber nicht im Hinblick auf einen oder mehrere Gesamttexte systematisch erfassbar waren. Mithilfe ihrer visualisierten Distributionen wurde eine neue Perspektive auf die Erzählerbemerkungen geschaffen, deren textübergreifende Bedeutsamkeit bislang nicht erkannt worden war. So lässt die vorliegende Untersuchung annehmen, dass die Distribution der Erzählerkommentare nicht zufällig, sondern an den Handlungsverlauf der Isländersagas gekoppelt ist. Diese Verbindung aus Form und Inhalt wird in einem nächsten Schritt einerseits auf Grundlage der Ergebnisse aus der quantitativen Analyse wieder in die genaue Textanalyse ( Bereits jetzt aber zeigen sich die Erzählerbemerkungen in ihrem regelmäßigen Auftreten als Gestaltungsmittel von Höhe- und Wendepunkten der Sagas als derart prägend, dass sich diese als Teil einer Literarisierungsstrategie über Einzelbelege hinweg tatsächlich zu einer poetologischen Aussage verdichten und deren Annotation damit zur ästhetischen Verortung dieser Texte beitragen kann. Auf der Suche nach ästhetischem Potenzial in den Isländersagas trafen wir anhand unserer Methode auf differenzierte Ergebnisse, die auf ein großes Bewusstsein hinsichtlich der Literarisierung und der Östhetisierung hinweisen. Weitere Analyseschritte zielen auf eine Korpuserweiterung und eine vergleichende Auswertung der Ergebnisse über ein größeres Textkorpus hinweg sowie einer Verfeinerung der Annotations-Tools und ggf. -Kategorien. Darüber hinaus prüfen wir die Möglichkeit einer automatischen Erkennung von Erzählerkommentaren mittels maschineller Lernverfahren auf Grundlage unserer Annotationen. Bei einer hinreichenden Erkennungsrate können ggf. weitere Sagas automatisch annotiert werden. Durch die bei maschinellen Lernverfahren zunächst oft fehlerhaften Verallgemeinerungen erhoffen wir uns zudem auch weitere Einsichten in die Annotationskategorien und deren Verwendung."
2022,DHd2022,PICHLER_Axel_Vom_Begriff_über_das_Phänomen_zur_Analyse___ein.xml,Vom Begriff über das Phänomen zur Analyse Ein CRETA-Workshop zur Operationalisierung in den DH,"Melanie Andresen (University of Stuttgart, Germany); Benjamin Krautter (Heidelberg University, Germany); Janis Pagel (University of Stuttgart, Germany); Axel Pichler (FU Berlin, Germany)","Operationalisierung, Modellierung, Textanalyse","Inhaltsanalyse, Modellierung, Annotieren, Theoretisierung, Methoden, Forschungsprozess","Der Workshop stellt eine weiterentwickelte und personell anders besetzte Version des auf der DHd 2020 abgehaltenen, beinahe gleichnamigen Workshops dar. Er adressiert eine der zentralen Herausforderungen für Arbeiten in den Digital Humanities 'die Operationalisierung geisteswissenschaftlicher Konzepte und Fragestellungen für computergestützte Forschungsansätze (vgl. Jannidis 2010:¬†109–132; Moretti 2013; Flanders/Jannidis 2015; Jacke 2014:¬†118–139; Pichler/Reiter 2020, Pichler/Reiter 2021). Während Geisteswissenschaftler*innen vor allem mit komplexen, häufig mehrere Textphänomene umfassenden Konzepten arbeiten und als relevant erachtete Kontexte zu deren Deutung heranziehen, ist die computergestützte Arbeit an identifizierbare Phänomene auf der Textoberfläche gebunden. Die hieraus erwachsende Diskrepanz zwischen theoretischen Erwartungen und konkreten Ergebnissen gilt es über eine adäquate Operationalisierung zu überbrücken (vgl. Moretti 2013:¬†1). Ziel ist es also, Verfahren zu entwickeln, die theoretische Begriffe über potenziell mehrere Teilschritte auf Textoberflächenphänomene zurückführen. Oder kurz gesagt: Als Anwendungsfälle stellen wir Phänomene vor, zu denen wir im Rahmen des ""Center for Reflected Text Analytics"" e.V. (CRETA) In einem ersten Anwendungsfall befassen wir uns mit dem Konzept der Entität und ihrer Referenz in literarischen Texten (vgl. Reiter u.a. 2017:¬†19–22; Blessing u.a. 2020). Dabei fassen wir den Begriff der Entität sehr weit: ""Alles, was man als Einheit denken kann, kann als Entität behandelt werden"" (Jannidis 2017:¬†103). Zu den Entitäten zählen dementsprechend Personen/Figuren, Orte, Organisationen sowie Ereignisse. Das Konzept ist also für verschiedene Forschungsfragen anschlussfähig. Auf Entitäten kann auf verschiedene Weise referiert werden, etwa über Eigen- und Gattungsnamen (z.¬†B. ""Angela Merkel"", ""die Kanzlerin""). Um Entitäten in einem Text zu extrahieren, müssen folglich die Entitätenreferenzen annotiert und kookkurrente Ausdrücke aufgelöst werden. Die Herausforderungen bestehen vor allem in der Festlegung der Referenzausdrücke (welche Ausdrücke werden berücksichtigt?), in der Abgrenzung von Entitätenreferenzen gegenüber generischen Ausdrücken sowie im Umgang mit Verschachtelungen, Metonymien und textspezifischen Besonderheiten. Der zweite Anwendungsfall setzt sich mit der Identifikation von Protagonisten im Drama auseinander, fokussiert also ein holistisches Textphänomen. Die verschiedenen Perspektiven der Literaturwissenschaft auf Protagonisten, Hauptfiguren und Helden von Dramen (vgl. die Ausführungen in Krautter u.a. 2018:¬†6–16 und Wulff 2002:¬†431–448) haben zur Folge, dass eine Reihe von Definitionen und Identifikationsstrategien koexistieren, die häufig an historische Normvorstellungen geknüpft sind. Diese historische Gebundenheit erschwert die operationale Definition von Protagonisten, wenn man auf größere Abschnitte der Literaturgeschichte blickt. Direkt anschlussfähig für die Methoden der Digital Humanities erscheint die in den späten 1970er Jahren von Manfred Pfister skizzierte Annahme, dass ""quantitative[] Dominanzrelationen"" (Pfister 2001:¬†227) hilfreich für die Differenzierung des Bühnenpersonals seien. Pfister nennt zwei Kriterien, die dabei helfen können, dramatische Figuren schon aufgrund quantitativer Eigenschaften als Haupt- oder Nebenfiguren zu identifizieren: nämlich die Zeitdauer, die sie auf der Bühne stehen, und ihr Anteil an der gesamten Figurenrede (vgl. Pfister 2001:¬†226–227). Diese Auffassung Pfisters lässt sich mit digitalen Methoden der Dramenanalyse um weitere Eigenschaften der Figuren, etwa durch Netzwerkmetriken oder Topic Modeling, zu einem multidimensionalen Ansatz ergänzen. Die größte Herausforderung stellt hierbei die Validierung der Ergebnisse dar, da diese an die Gültigkeit der operationalen Definition für die manuelle Annotation gebunden ist. Im Workshop stellen wir zwei Ansätze zur Operationalisierung vor, die sich 'in verschiedenen Phasen des Forschungsprozesses 'sehr gut gegenseitig ergänzen. Der erste Ansatz besteht in der Als zweiten Ansatz stellen wir eine Vorgehensweise vor, die Zielphänomene In einem Theorieteil führen wir in Geschichte und Praxis der Operationalisierung von geisteswissenschaftlichen Fragestellungen und Konzepten für die computergestützte Analyse ein. Anhand der oben genannten Beispiele aus der CRETA-Praxis thematisieren wir die Problematik und stellen Ansätze zur Operationalisierung im Detail vor. Je nach Interesse kann anschließend, im praktischen Teil, einer dieser Anwendungsfälle ausgewählt und bearbeitet werden. Dabei haben die Teilnehmenden die Möglichkeit, beide Operationalisierungsansätze an ihrem gewählten Anwendungsfall zu erproben. Hierfür befassen sie sich zunächst mit dem Konzept, indem sie es anhand eines Textauszugs manuell annotieren und parallel stichpunktartig die Richtlinien schärfen. In einer ersten Diskussionsrunde werden die verschiedenen Ergebnisse gesammelt und diskutiert. Zur Erprobung des zweiten Ansatzes stellen wir für jeden Anwendungsfall einen Operationalisierungs-""Baukasten"" vor. Dieser besteht aus einer Sammlung von Python-Skripten in einem Jupyter-Notebook, das auf das jeweilige Untersuchungsvorhaben zugeschnitten ist und den Teilnehmenden die Möglichkeit gibt, sich dem zu untersuchenden Phänomen über computergestützte Verfahren anzunähern. Die Teilnehmenden können in Kleingruppen in diesem Baukasten verschiedene Parameter einstellen sowie manuell Eigenschaften an- oder abwählen, wobei sie auf ihr Vorwissen über den Untersuchungsgegenstand aus der ersten Praxisrunde zurückgreifen können. Nachdem die Teilnehmenden die Eigenschaften ausgewählt und ggf. parametrisiert haben, können sie die Ergebnisse visualisieren und mit den Texten abgleichen. Damit erhalten die Teilnehmenden ein direktes Feedback zu den ausgewählten Parametern und können prüfen, ob das Untersuchungsvorhaben mit den festgelegten Einstellungen angemessen umgesetzt wird. Der Baukasten ist zur iterativen Nutzung vorgesehen, sodass der Einfluss verschiedener verwandter Eigenschaften auf die Ausgaben sichtbar wird und die Teilnehmenden sich einer geeigneten technischen Umsetzung sukzessiv annähern können. In einer abschließenden Diskussion werden die Ergebnisse gesammelt und es wird ausgewertet, wie adäquat sich die jeweiligen Zielphänomene mittels der gewählten Annahmen abbilden haben lassen. Ziel unseres Workshops ist es, die Teilnehmenden für die Wichtigkeit der Operationalisierung in den Digital Humanities zu sensibilisieren und ihnen Wege zu ihrer erfolgreichen Realisierung vorzustellen. Durch die interdisziplinäre Ausrichtung von DH-Arbeiten kommt der Operationalisierung eine Schlüsselposition zu, da sie eine Brücke zwischen geisteswissenschaftlichen Konzepten und computergestützter Umsetzung schlägt (vgl. Moretti 2013:¬†1). Mit den gewählten Anwendungsfällen wollen wir den Teilnehmenden ein ""Repertoire"" für die Operationalisierung verschiedener Aufgabentypen mitgeben. Wir zeigen zum einen, dass die Annotation eines Phänomens als Methode seiner Operationalisierung dienen kann (vgl. Gius/Jacke 2017:¬†233–254); zum anderen führen wir für textbasierte Phänomene eine indirekte Operationalisierung ein (vgl. Reiter/Willand 2018). Beide Verfahrensweisen sind auf andere Anwendungsfälle übertragbar. Gleichzeitig möchten wir deutlich machen, dass es für jedes Untersuchungsvorhaben nicht nur eine, sondern verschiedene Wege der Operationalisierung gibt. Die Spielräume, die bei der Operationalisierung geisteswissenschaftlicher Fragestellungen entstehen, machen es notwendig, Entscheidungen reflektiert zu treffen, sie offenzulegen und ihren Einfluss auf die Ergebnisse als Voraussetzung für eine angemessene Interpretation zu bedenken. (insgesamt 3 Stunden + 30 Min. Pause) 1. Einführung und Ablauf (10 Min.) 2. Theoretischer Teil (insgesamt 30 Min.) ‚Ä¢ Erläuterung der Problemstellung ‚Ä¢ Vorstellung der Anwendungsfälle 3. Praktischer Teil ‚Ä¢ Einführung in die Primärtexte und Tools, Ausgabe der skizzierten Guidelines (10 Min.) ‚Ä¢ Erste Praxisrunde (Kleingruppen): Manuelle Annotation eines Phänomens, parallele Erweiterung/Überarbeitung der Guidelines, iterativ (30-40 Min.) 'Kaffeepause (30 Min.) '‚Ä¢ Sammeln der Ergebnisse und Diskussion der Herangehensweisen (20 Min.) ‚Ä¢ Zweite Praxisrunde (Kleingruppen): Arbeit am Operationalisierungsbaukasten, Feedback über Ausgabedatei, iterativ (30-40 Min.) 4. Abschlussdiskussion: Sammeln der Ergebnisse, Diskussion der Erfahrungen und Lernziele (30 Min.) Die Durchführung des Workshops auf der DHd 2020 hat gezeigt, dass das gestraffte dreistündige Format gute didaktische Resultate zeitigt. Der Fokus auf die praktischen Dimensionen der Operationalisierung ist dabei gewollt: Aus der konkreten praktischen Arbeit heraus lässt sich unserer Ansicht nach am besten der theoretische Rahmen und die theoretischen Probleme bei der Operationalisierung reflektieren. Zwischen 15 und 25 Abgesehen von Beamer und ausreichend Steckdosen ist keine besondere technische Ausstattung erforderlich. Die Teilnehmenden arbeiten im praktischen Teil an ihrem eigenen Laptop. Informationen zu eventuellen Vorab-Installationen werden rechtzeitig mitgeteilt. Der Workshop wird von Mitgliedern des Center for Reflected Text Analytics (CRETA) e.V. veranstaltet, die bereits erfahrene Workshop-Leiter*innen im DH-Bereich sind (DHd 2017,DHd 2018, ESU 2018,DHd 2019, HCH 2019,DHd 2020). CRETA konzentriert sich auf die Entwicklung von Methoden zur kritisch-reflektierten Textanalyse im Forschungsbereich der Digital Humanities. Die Methoden werden fachübergreifend für textanalytische Fragestellungen aus der Literatur-, Sprach-, Geschichts- und Sozialwissenschaft sowie Philosophie erarbeitet und eingesetzt. Das bis 2020 vom BMBF geförderte eHumanities-Zentrum ist Ende 2020 mit der Gründung eines Vereines in eine neue Phase übergegangen. Mit der Vereinsgründung wird der Tatsache Rechnung getragen, dass über Stuttgart hinaus inzwischen Wissenschaftler*innen an ähnlichen Zielen arbeiten und CRETA in vielfältiger Weise verbunden sind, etwa durch gemeinsame Projekte. Melanie Andresen Universität Stuttgart Institut für Maschinelle Sprachverarbeitung Pfaffenwaldring 5b 70569 Stuttgart Melanie Andresen ist Postdoc am Institut für Maschinelle Sprachverarbeitung an der Universität Stuttgart. Sie hat Germanistische Linguistik an der Universität Hamburg studiert und ist dort 2020 im Bereich der Korpuslinguistik promoviert worden. Aus den Projekten Benjamin Krautter Benjamin.Krautter@uni-koeln.de Universität zu Köln Institut für Digital Humanities Albertus-Magnus-Platz 50931 Köln Benjamin Krautter ist Promotionsstudent am Germanistischen Seminar der Universität Heidelberg und Mitarbeiter im Projekt Q:TRACK. Dort arbeitet er u.¬†a. an der Operationalisierung literaturwissenschaftlicher Kategorien für die quantitative Dramenanalyse. Im Zentrum seines Forschungsinteresses steht dabei die mögliche Verbindung quantitativer und qualitativer Methoden für die Analyse und Interpretation literarischer Texte. Janis Pagel Universität zu Köln Institut für Digital Humanities Albertus-Magnus-Platz 50931 Köln Janis Pagel ist Promotionsstudent am Institut für Maschinelle Sprachverarbeitung der Universität Stuttgart und Mitarbeiter am Institut für Digital Humanities der Universität zu Köln. Er studierte Germanistik und Linguistik in Bochum, sowie Computerlinguistik in Stuttgart und Amsterdam. Er forscht zu Anwendungen von computerlinguistischen Methoden auf literaturwissenschaftliche Fragestellungen und Koreferenzresolution auf literarischen Texten. Axel Pichler Universität Stuttgart Institut für Maschinelle Sprachverarbeitung Pfaffenwaldring 5b 70569 Stuttgart Axel Pichler studierte Philosophie und Germanistik in Wien und Graz. Im Sommersemester 2021 war er Gastprofessor für Digital Humanities am EXC ""Temporal Communities‚Äù der FU Berlin. Zurzeit arbeitet er als Postdoc unter anderem an der Entwicklung und Reflexion von Methoden der computergestützten Textanalyse am Institut für Maschinelle Sprachverarbeitung der Universität Stuttgart. 1."
2022,DHd2022,SEIFERT_Sabine_Datenbiographik_im_Literaturarchiv__Konzept_u.xml,Datenbiographik im Literaturarchiv Konzept und Umsetzung digitaler Dienste am Theodor-Fontane-Archiv,"Sabine Seifert (Theodor-Fontane-Archiv, Universität Potsdam); Anna Busch (Theodor-Fontane-Archiv, Universität Potsdam); Peer Trilcke (Theodor-Fontane-Archiv, Universität Potsdam); Kristina Genzel (Theodor-Fontane-Archiv, Universität Potsdam); Juliane Heilmann (Theodor-Fontane-Archiv, Universität Potsdam); Klaus-Peter Möller (Theodor-Fontane-Archiv, Universität Potsdam)","Archiv, Datenbiographik, Metadaten, digitale Dienste, Chronik, Kulturdaten","Veröffentlichung, Bibliographie, Literatur, Metadaten, virtuelle Forschungsumgebungen","Die Arbeit von Literaturarchiven steht seit deren ersten Konzeptualisierungen im 19. Jahrhundert (Dilthey 1970 [1889]; vgl. Thaler 2011, Schöttker 2016) in einem komplexen Wechselverhältnis zu den philologischen Tätigkeiten der Editorik und Biographik, die im 20. und 21. Jahrhundert noch ergänzt werden u.a. um Textgenetik und Material Media Studies. Während das Zusammenspiel von Archiv und Editorik dabei zuletzt vor dem Horizont der Digitalisierung intensiv diskutiert wird (vgl. exemplarisch Nutt-Kofoth 2019), steht eine Neujustierung des Verhältnisses von Archiv und Biographik (vgl. Fetz 2009) im Zeichen der digitalen Transformation (Wettmann 2018) noch aus. Im Zuge der digitalen Erweiterung seiner Dienste (Trilcke 2019; Trilcke, Busch, Seifert 2021) hat das Theodor-Fontane-Archiv in den vergangenen Jahren nicht nur bio- und bibliographische Datenbestände zu Fontane erstellt und offen im Web nutzbar gemacht, es hat auch an einem Konzept für eine digital-biographische Ressource und deren Umsetzung gearbeitet. Anders als für die Buch-Biographik typisch, wurde dabei kein narrativer Ansatz gewählt. Ziel war es vielmehr, eine Konzeption Orientierungspunkt für die Konzeption und (Daten-)Grundlage für die Umsetzung dieses datenbiographischen Dienstes war die fünfbändige Druckausgabe der Die Die Einzeldatenbestände, für die individuelle Dienste mit eigenen Interfaces entwickelt wurden, umfassen die wichtigsten Forschungsdaten zu Fontane. Als digitale Dienste werden dabei die Die ursprünglich als Druck publizierte Die Aus XML-Druck-Daten der Auf den drei Einzeldatenbeständen aufbauend operiert die Mit der Implementierung des chronikalen Prinzips in Form einer Datenbiographik ist ein entscheidender Entwicklungsschritt im digitalen Ausbau des Fontane-Archivs abgeschlossen. Auf der nun bestehenden Infrastruktur aufbauend, steht vor allem die Qualitätssteigerung der Daten (Normdaten, LOD) sowie die Anbindung und Öffnung qua APIs im Vordergrund der Entwicklungsarbeiten. "
2022,DHd2022,PIELSTRÖM_Steffen_Das_DFG_Schwerpunktprogramm_Computational_.xml,Das DFG Schwerpunktprogramm Computational Literary Studies,"Steffen Pielström (Julius-Maximilians-Universität Würzburg, Germany); Kerstin Jung (Universität Stuttgart, Germany)","Computational Literary Studies, Community, Literaturwissenschaft, Textanalyse","Community-Bildung, Literatur, Forschung","Die Computational Literary Studies (CLS) sind ein wachsendes, interdisziplinäres Forschungsfeld angesiedelt zwischen Literaturwissenschaft, Computerlinguistik und Informatik, in dem computergestützte Verfahren zur Analyse literaturwissenschaftlicher Fragestellungen zum Einsatz kommen. Insgesamt elf Einzelprojekte aus Deutschland und der Schweiz, die zur Zeit in diesem Emerging Field arbeiten, gehören dem seit 2020 aktiven Schwerpunktprogramm SPP 2207 ""Computational Literary Studies"" der Deutschen Forschungsgemeinschaft (DFG) an, davon erhalten 10 Projekte direkte Förderung aus dem Programm, ein weiteres Projekt ist mit dem Programm assoziiert. Hinzu kommt ein Zentralprojekt das, als Besonderheit neben der organisatorischen und inhaltilichen Koordination der Fortschungsvorhaben, über eine eigens eingerichteten Personalstelle für das projektübergreifende Forschungsdatenmanagement verfügt. So bietet das Programm eine enge Begleitung und Abstimmung der Projekte in Fragen des Forschungsdatenmanagements über die gesamte Laufzeit. Für das kooperative Arbeiten wird vom Zentralprojekt u.a. eine Gitlab-Instanz zur Verfügung gestellt. In den einzelnen Projekten kooperieren erfahrene Digital Humanists eng mit etablierten Literaturwissenschaftler*innen um an aktuell relevanten Fragen der Literaturwissenschaft zu arbeiten. Die Forschung im SPP 2207 konzentriert sich vor allem auf die deutschsprachige Literatur. Hier reicht das Spektrum der Forschungsgegenstände von Romanen über Dramen bis hin zur Poesie, die untersuchten Texte entstammen verschiedenen Epochen vom Mittelhochdeutschen bis ins späte 20. Jahrhundert. Hinzu kommen methodologische Untersuchungen die zum Ziel haben, das methodische Repertoire der Computational Literary Studies für die spezifischen Anforderungen des Faches zu validieren und weiter zu entwickeln. So haben sich für Sentimentanalyse, Wordembeddings und Annotationen projektübergreifende Arbeitsgruppen etabliert und ein ganzes Projekt widmet sich der Methodenforschung im Bereich der kontrastiven Stilometrie. Die Projekte in der ersten, dreijährigen Förderperiode sind im einzelnen: Angesichts der Vernetzung und Verankerung nahezu aller Programmbeteiligten in der nationalen wie internationalen Fachcommunity - so engagieren sich Mitglieder u.a. in der ADHO Special Interest Group ""Digital Literary Stylistics"", der EU COST Action ""Distant Reading for European Literary History"", dem EU-Programm ""Computational Literary Studies Infrastructure"" (CLSInfra) und der ACL Special Interest Group on Humanities (SIGHUM) - sieht sich SPP 2207 nicht nur als Einrichtung für eine begrenzte Zahl geförderter Projekte sondern auch als Multiplikator und ""Netzwerkknoten"" für die gesamte CLS-Community, insbesondere im deutschsprachigen Raum. Veranstaltungen des Schwerpunktprogramms wie Meetings und Workshops sind daher in der Regel ebenso offen für Interessierte wie die projektübergreifenden Arbeitsgruppen, um die aktive Beteiligung weiterer Teile der Fachcommunity an den Aktivitäten von SPP 2207 zu fördern. Mit dem vorliegenden Poster präsentiert sich SPP 2207 in seiner Gesamtheit und zeigt, wie die Vernetzung in einem solchen Programm Synergien und Gelegenheiten zur Zusammenarbeit schafft, die in der Zukunft auch in die weitere Foschungscommunity hinein wirken sollen."
2022,DHd2022,VAUTH_Michael_Inter_Annotator_Agreement_und_Intersubjektivit.xml,Inter Annotator Agreement und Intersubjektivität   Ein Vorschlag zur Messbarkeit der Qualität literaturwissenschaftlicher Annotationen,"Evelyn Gius (Technische Universität Darmstadt, Germany); Michael Vauth (Technische Universität Darmstadt, Germany)","Inter Annotator Agreement, Digitale Literaturwissenschaft, Annotation, Ereignisse","Strukturanalyse, Modellierung, Annotieren, Forschungsprozess, Forschungsergebnis, Standards","Die in den Sozialwissenschaften und der Computerlinguistik schon lange etablierte Praxis der computergestützten und häufig kollaborativen manuellen Annotation ist mittlerweile auch im Zentrum der digitalen Geisteswissenschaften angekommen. Deshalb möchten wir unsere Beobachtungen zu einem zentralen Punkt teilen: dem Inter Annotator Agreement bzw. Inter Coder Agreement. Wir betrachten dieses aus der Sicht der Computational Literary Studies (CLS) anhand unseres Projekts ""Evaluating Events in Narrative Theory  (EvENT)"" Es gibt eine Vielzahl von Inter Annotator Agreement-Metriken, die als Maß eingesetzt werden, um die Verlässlichkeit manuell erstellter Annotationen zu beurteilen, die zum Überprüfen einer These oder zur Entwicklung und zum Testen computationeller Modelle genutzt werden (Artstein & Poesio 2008:556). Da bei der Betrachtung des Inter Annotator Agreements von Menschen annotierte Daten 'und damit deren Analysen bestimmter Texte (oder anderer Artefakte) 'miteinander verglichen werden, ist dies auch literaturwissenschaftlich interessant. Der literaturwissenschaftliche Erkenntnisgewinn basiert nämlich, in Ermangelung objektiver Fakten, ganz wesentlich auf intersubjektiver Übereinstimmung bzw. deren Abgleich. Grundsätzlich lassen sich fünf Einsatzgebiete von Inter Annotator Agreement-Messungen unterscheiden: Für das EvENT-Projekt sind diese Einsatzbereiche unterschiedlich stark von Interesse. Literaturwissenschaftliche Befunde basieren meistens weder auf streng formalisierten Schlussfolgerungssystemen Mit Blick auf Intersubjektivität kann man in den fünf genannten Bereichen, in denen Inter Annotator Agreement-Messungen zum Einsatz kommen, feststellen: Im Kontext der Reliabilität von Annotator*innen (Fall 1) geht es um den Abgleich einer an sich aber Bei der Entwicklung bzw. Qualität von Guidelines (Fall 2 bzw. 3) geht es hingegen um die Frage, inwiefern eine Im Kontext der Qualität bzw. Validität der Daten und der Operationalisierbarkeit von Phänomenen (Fall 4 und 5) steht schließlich die intersubjektive Übereinstimmung bei der Beurteilung der Phänomene im Text im Fokus. Aus literaturwissenschaftlicher Sicht ist die Intersubjektivität insbesondere in den letzten beiden Fällen abgebildet. Bei der Frage nach Qualität bzw. Validität der Daten und der Operationalisierbarkeit von Phänomenen wird nämlich der Grad der Übereinstimmung zwischen Annotationen auf die oben erwähnten ""Eigenheiten bestimmter Texte"" bezogen. Die beiden Aspekte sind auch aus computationeller Sicht wichtig, denn sie betreffen die analysierten Phänomene und damit das zentrale Forschungsinteresse vieler literaturwissenschaftlicher Ansätze in den Digital Humanities. Wie bereits angesprochen, fehlen allerdings gerade zu diesen beiden Fällen Erfahrungswerte, auf die zurückgegriffen werden kann. Da die Inter Annotator Agreement-Werte in literaturwissenschaftlichen Annotationsprojekten zudem meist deutlich unter den in anderen Disziplinen gängigen Grenzwerten liegen, können diese nicht sinnvoll genutzt werden. Stattdessen müssen Strategien entwickelt werden, die eine Beurteilung der Annotationsqualität in philologischen Forschungskontexten ermöglichen. Wir stellen deshalb im Folgenden eine Anpassung des Verfahrens der Annotation und der Inter Annotator Agreement-Messung vor, mit der man diesem Manko in bestimmten Forschungszusammenhängen begegnen kann. Inter Annotator Agreement-Metriken basieren auf differenzierten Formeln, die typischerweise erwartete (Nicht-)Übereinstimmungswerte berücksichtigen und z.T. auch die Gewichtung bestimmter Aspekte der Annotationen zulassen (z.B. durch das Festlegen von Öhnlichkeiten zwischen Kategorien oder die Gewichtung der Segmentierungsentscheidungen). Die Wahl der eingesetzten Metrik sollte in Abhängigkeit von den Eigenschaften der Annotationen getroffen werden. Zu diesen Eigenschaften gehören die Anzahl und Verteilung der genutzten Annotationskategorien, die Häufigkeit, mit der Annotationskategorien auftreten, die Frage, ob die Bestimmung der zu annotierenden Textsegmente Teil der Annotationsaufgabe ist und viele mehr (vgl. dazu Artstein & Poesio 2008 sowie Mathet et al. 2015). Das Problem, vor dem wir zumindest bislang stehen, ist nicht nur, dass es eine ziemliche Herausforderung ist, diese Eigenschaften zu identifizieren, sondern noch mehr, dass uns etablierte Strategien fehlen, um diese zu beurteilen. Ein wesentlicher Grund dafür ist, dass literaturwissenschaftliche Textanalysen oft Phänomene in den Blick nehmen, die bei näherer Betrachtung keine Merkmale der Textoberfläche sind. Da diese Phänomene nicht direkt an bestimmten Texteigenschaften festgemacht werden können, muss man bei der Operationalisierung auf mit dem Phänomen mutmaßlich zusammenhängende Merkmale zurückgreifen, die sich textlich realisieren. Eine Folge dieser indirekten Annäherung an die untersuchten Phänomene ist, dass eine Agreement-Messung mit den üblichen Metriken für bestimmte literaturwissenschaftliche Einsatzgebiete nicht sinnvoll ist, da diese für die Annotation von Textphänomenen wie etwa Wortarten oder semantische Klassen entwickelt wurden. Nun könnte man versuchen neue, für literaturwissenschaftliche Fragestellungen passende Annotationsmetriken zu entwickeln. Öhnlich hilfreich und leichter umsetzbar ist allerdings eine Anpassung des Operationalisierungsverfahrens an das, was mit bestehenden Metriken gemessen wird. Konkret sollte man versuchen, die genutzten Annotationskategorien so zu gestalten, dass sie: Beim ersten Punkt ist es erstrebenswert, dass die genutzten Kategorien eine möglichst eindeutig festlegbare Texteinheit umfassen und im ganzen Text vorkommen. Eine mögliche Umsetzung dieser Punkte lässt sich an unserem Beispiel verdeutlichen. Ausgehend von den erzähltheoretischen Ereigniskonzepten haben wir im EvENT-Projekt vier Annotationskategorien definiert: Wir haben also eine syntaktisch weitgehend eindeutige Einheit 'die Verbalphrase 'identifiziert, die sich als Annotationseinheit eignet und deren Inhalt zur Bestimmung der Kategorisierung geeignet ist. Durch die Ausweitung der non_event-Kategorie auf nicht vollständige Verbalphrasen kann ein Text außerdem durchgängig mit unseren Kategorien annotiert werden kann. Auch die Überführung der kategorialen Skalierung in eine numerische Skalierung basiert auf dem literaturwissenschaftlichen Verständnis der Kategorien. Entsprechend dem literaturwissenschaftlichen Ereignisverständnis nehmen wir an, dass diese vier Kategorien in unterschiedlichem Maß die Ereignishaftigkeit eines Textes konstituieren: Zustandsveränderungen, aber auch Bewegungs- und Kommunikationsvorgänge tun dies in stärkerem Maß als Landschafts-, Raum- oder Figurenbeschreibungen, die in vielen erzählenden Texten eher Expositionsfunktionen erfüllen. Doch dies war noch nicht ausreichend, um ein Agreement zu erzielen, welches aus computerlinguistischer Sicht gut ist. Hinzu kommt, dass die Agreement-Werte unsere Intuition über die Qualität der Annotationen nicht widerspiegelten (vgl. Tabelle 1). Deshalb haben wir unser Vorgehen entsprechend weiterentwickelt. Der Schlüssel zu einer aussagekräftigeren Inter Annotator Agreement-Perspektive lag in der Erkenntnis, dass uns die Entwicklung von Ereignishaftigkeit im Textverlauf und entsprechend¬† Narrativitätsverläufe interessieren. Wir haben deshalb nicht nur die Ergebnisse der Annotationen als Verlauf visualisiert, sondern auch entschieden, die Einschätzung des Inter Annotator Agreement 'ebenso wie übrigens die Qualität der automatisierten Erkennung von Ereignissen –¬†anhand von Verläufen vorzunehmen. Für die Darstellung des Narrativitätsverlaufs wurden die Werte der Annotationen innerhalb eines Textabschnitts anhand der Narrativitätswerte der umliegenden 50 Verbalphrasen mit einer Kosinusgewichtung geglättet. Die Kosinusgewichtung sorgt dabei dafür, dass näher liegende Textsegmente einen stärkeren Einfluss auf den Narrativitätswert des untersuchten Textsegments haben. Auf Grundlage dieser Zuweisungen konnten wir die Narrativitätsverläufe in Einzeltexten wie in Abbildung 1 untersuchen:  Um die Stabilität des Verfahrens zu prüfen, haben wir mit der Zuweisung der Zahlen zu den Kategorien experimentiert, dabei aber ihre Anordnung gemäß ihrer Narrativität nicht verändert. Eine umfassende Evaluation steht noch aus, aber die bisherigen Versuche deuten darauf hin, dass die Narrativitätsverläufe dabei strukturell nicht stark variieren (vgl. Abbildung 2).  Wir konnten also auf der Grundlage unserer Wertzuweisung für die Ereignistypen die Annotationen der unterschiedlichen Annotator:innen miteinander vergleichen (vgl. Abbildung 3). Unsere Annäherung an ein Inter Annotator Agreement, das auf die Modellierung eines literarischen Phänomens ausgerichtet ist, scheint also unsere literaturwissenschaftlich fundierte Intuition besser abzubilden als gängige Inter Annotator Agreement-Metriken. Dafür sind zwei Aspekte entscheidend: Durch dieses Vorgehen gelingt es uns, den Fokus auf das eigentlich untersuchte Phänomen 'in unserem Fall die Ereignishaftigkeit von erzählenden Texten 'zu richten. Damit lässt sich die Intersubjektivität der Analysen besser messen als anhand der Annotationen, die das Phänomen anhand von Oberflächenphänomenen (Verbalphrasen) operationalisieren und die im Kontext von gängigen Inter Annotator-Metriken entsprechend nur bedingt aussagekräftig sind. Hinzu kommt, dass es zwei wichtige Fehlerquellen bei literaturwissenschaftlichen Annotationen –¬†nämlich einfache Fehler sowie divergierende Voranalysen (vgl. Gius & Jacke 2017) 'ausgleicht."
2022,DHd2022,ANDRESEN_Melanie_Nathan_nicht_ihr_Vater____Wissensvermittlun.xml,Nathan nicht ihr Vater? Wissensvermittlungen im Drama annotieren,"Melanie Andresen (Universität Stuttgart, Germany); Benjamin Krautter (Universität Stuttgart, Germany); Janis Pagel (Universität Stuttgart, Germany); Nils Reiter (Universität zu Köln, Germany)","Annotation, Modellierung, Drama","Beziehungsanalyse, Modellierung, Annotieren, Sprache, Literatur, Text","Die quantitative Dramenanalyse hat sich lange Zeit vor allem auf formale Merkmale der Textoberfläche konzentriert In diesem Beitrag gehen wir zunächst auf die Bedeutung von Wissen und Wissensvermittlungen für die Handlung wie auch die Wirkung von Dramen ein. Anschließend beschreiben wir, wie solche Prozesse der Wissensvermittlung in Annotationen erfasst und modelliert werden können. Die Interferenz von innerem und äußerem Kommunikationssystem im Drama, also die Kommunikation der fiktiven Figuren auf der einen Seite und die Wahrnehmung dieser Kommunikation durch das Publikum auf der anderen Seite, gilt als eine zentrale ""Differenzqualität dramatischer Kommunikation"" (Pfister 2001: 80). Die Bühnenfiguren zeichnen sich schon mit Blick auf die Vorgeschichte des Dramas potentiell durch einen unterschiedlichen Wissensstand aus, der sich im Laufe des Stücks fortwährend verändern kann, etwa hinsichtlich ihrer Handlungsziele. Dadurch wird zugleich das Verhältnis zwischen dem Informationsstand des Publikums und demjenigen der einzelnen Dramenfiguren immer wieder neu justiert. Die Exposition reduziert etwa den zu Beginn eines Dramas vorherrschenden Wissensrückstand des Publikums gegenüber den Figuren (vgl. etwa Asmuth 2015: 122). Die Unterschiede im ""Grad der Informiertheit"" 'Manfred Pfister spricht hierbei in Rekurs auf den Shakespeare-Forscher Bertrand Evans von ""diskrepante[r] Informiertheit"" (Pfister 2001: 80, vgl. Evans 1960: viii) 'lassen sich vor allem auf zwei ursächliche Unterschiede zwischen innerem und äußerem Kommunikationssystem zurückführen: Während das Publikum in seiner Beobachterrolle jede Szene des Stücks wahrnimmt und dadurch geäußertes partielles Wissen der Figuren abgleichen und aggregieren kann, bleibt bisweilen unklar, über welches Wissen die Figuren tatsächlich verfügen. Das gilt auch für mögliche Zeitsprünge, etwa zwischen zwei Akten des Dramas. Unklar kann zudem sein, inwieweit die Öußerungen einer Figur mit den ""Tatsachen"" der fiktionalen Welt übereinstimmen, ob die Öußerungen also glaubwürdig sind (vgl. Jeßing 2015: 50-51). Je nach Handlungsverlauf verfügt das Publikum also zu unterschiedlichen Zeitpunkten des Dramas über einen Informationsvorsprung oder einen Informationsrückstand gegenüber den auf der Bühne agierenden Figuren. Gleiches gilt isoliert betrachtet auch für das interne Kommunikationssystem des handelnden Bühnenpersonals. Die ""diskrepante Informiertheit"" zweier Figuren kann so zu unterschiedlichen Bewertungen derselben Situation führen. Figuren, die etwa über das Wissen verfügen, dass zwei verlobte Figuren Geschwister sind, werden diese Verlobung anders beurteilen, als Figuren, denen dieses Wissen fehlt. Diese Kluft zwischen dem Wissensstand der Figuren und demjenigen des Publikums ist als wichtiges Spannungselement des Dramas aufzufassen Ziel unseres Annotations- und Modellierungsvorhabens ist es deshalb, das sich verändernde Wissen über Familienrelationen sowohl im internen als auch im externen Kommunikationssystem abzubilden. Wir wollen dabei nicht nur die zentralen Szenen der Wiedererkennung annotieren, sondern vor allem die einzelnen Schritte nachvollziehen, die einen solchen für die dramatische Wirkung entscheidenden Wissensumschlag anleiten. In einem ersten Schritt werden Textstellen im Drama, an denen Wissen über Familienrelationen vermittelt wird, manuell annotiert. Entscheidend für die Annotation ist, dass sich der Wissensstand einer Figur oder des Publikums tatsächlich verändert. Relevante Textstellen werden mit einem strukturiert zusammengesetzten Label versehen, das sowohl das vermittelte Wissen als auch die Quelle und das Ziel der Wissensvermittlung benennt. Optional können Attribute hinzugefügt werden, sodass die Annotationslabel nach dem folgenden Schema funktionieren:  Quelle und Ziel sind in der Regel entweder Figuren des Dramas oder das Publikum (oder eine Liste mehrerer dieser Entitäten). Als Quelle kann aber auch ein Objekt oder Vorgang in der Welt in Betracht kommen (z. B. eine Beobachtung). Das für unsere Annotationen relevante Wissen ist auf Familienrelationen und Liebesbeziehungen zwischen den Figuren beschränkt, wobei die Annotationsrichtlinien ein festes Inventar von Relationen vorgibt. Formal können hierbei gerichtete Relationen wie  Durch die optionalen Attribute kann das vermittelte Wissen spezifiziert, also beispielsweise als unsicher oder als Lüge gekennzeichnet werden. Durch ein vorangestelltes Ausrufezeichen können Relationen oder Wissensbestände negiert werden. Beim Wissensstand kann es sich auf einer Metaebene auch um ein Wissen über Wissen handeln. So kann etwa annotiert werden, dass Daja dem Tempelherrn (und dadurch auch dem Publikum) anvertraut, dass Recha gar nicht bewusst ist, dass Nathan nicht ihr leiblicher Vater ist:  Weitere Details zur Annotation lassen sich den auf unserer Webseite veröffentlichten Richtlinien entnehmen. Indem wir erfahren, dass Figur¬†A Elternteil einer Figur B ist, lässt sich schließen, dass Figur¬†B das Kind von Figur A ist, ohne dass dies im Text explizit gemacht werden müsste. Falls weitere Verwandte von Figur¬†A bekannt sind, ergeben sich zudem weitere Verwandtschaftsverhältnisse für Figur¬†B. Die annotierten Wissensvermittlungen müssen deshalb im Anschluss an die Annotation um alle weiteren, logisch inferierbaren Figurenrelationen ergänzt werden. Ziel des Projektes ist es, diese logischen Schlüsse durch ein formalisiertes Regelsystem zu ziehen, das auf die annotierten Wissensveränderungen angewendet werden kann und diese automatisch ergänzt. An einem ersten Prototyp dieses Inferenzsystems arbeiten wir derzeit. Die zentrale Wiedererkennung in Lessings Für das Publikum wird dieses Wissen bereits durch die Figurentafel ersichtlich. Recha wird dort im Anschluss an Nathan als ""dessen angenommene Tochter"" (Lessing 1971: 206) eingeführt. Dies verleitet zu der Annahme , dass es sich dabei um ein von allen Figuren geteiltes Wissen handelt. Direkt im 1. Auftritt spielt Daja, Rechas Gesellschafterin, auf diese Tatsache an. Sie ist demnach eingeweiht. In Bezug auf Rechas Kenntnis über ihre Herkunft bleibt das Publikum zunächst im Dunkeln. Ihr Ausruf, ""Da kommen die Kamele meines Vaters"" (Lessing 1971: 209), ist auch für eine Pflegetochter, die sich dieses Umstands bewusst ist, denkbar. Dem Tempelherrn gegenüber stellt sich Nathan im 5. Auftritt des 2. Aufzugs als Rechas Vater vor. Dass Nathan tatsächlich Rechas Pflegevater ist, erfährt der Tempelherr zum Ende des 3. Aufzugs von Daja, die auf eine christliche Heirat von Recha hofft. Auf der Metaebene (""Wissen über Wissen"") wird dem Tempelherrn zudem offenbar, dass Recha sich ihrer tatsächlichen familiären Relation zu Nathan nicht bewusst ist, und klärt diese Frage damit ebenfalls für das möglicherweise noch zweifelnde Publikum. Der Tempelherr gibt dieses Wissen, empört über die zurückhaltende Reaktion Nathans auf seinen Heiratsantrag, an Saladin weiter (4. Aufzug, 4. Auftritt). In der folgenden Aussprache mit Nathan (5. Aufzug, 5. Auftritt) gibt der Tempelherr ihm gegenüber zu, von Daja bereits die wahren Verwandtschaftsverhältnisse erfahren zu haben. Abseits der Bühne hat Daja inzwischen auch Recha über ihren Status als Pflegetochter informiert. Dies erfährt das Publikum, indem Recha diesen Umstand auch Saladins Schwester Sittah berichtet (5. Aufzug, 6. Auftritt), sodass das Wissen nun alle Figuren im Kern des Dramas erreicht hat. Für gleich mehrere Figuren lässt sich aus den Annotationen jedoch nicht direkt ableiten, zu welchem Zeitpunkt sie erstmals über das relevante Wissen verfügen. Nathan und Daja wissen bereits vor Beginn der Dramenhandlung, dass Nathan nicht Rechas leiblicher Vater ist. Direkt aus der Figurentafel lässt sich dieser Umstand indes nicht ableiten. Dass ein Vater¬† darüber informiert ist, wer (nicht) seine leiblichen Kinder sind, ist auch in Dramen wahrscheinlich (principle of minimal departure, vgl. etwa Ryan 1980), aber nicht alternativlos. Im ersten Auftritt erfährt das Publikum also zunächst expositorisch, dass Nathan und Daja über dieses Wissen schon vor Handlungsbeginn verfügen. Öhnlich dazu wird auch der Moment, in dem Recha erfährt, nicht Nathans leibliche Tochter zu sein, nicht auf der Bühne dargestellt. Erst durch ihren Dialog mit Sittah wird offenbar, dass sie es zwischenzeitlich abseits der Bühne erfahren haben muss. Abbildung 2 stellt den Wissensverlauf für die Information, dass Recha und der Tempelherr Geschwister sind, dar. Nathan (und damit das Publikum) hegen einen entsprechenden Verdacht (in der Abbildung hell dargestellt), seit der Tempelherr im zweiten Akt seinen Familiennamen genannt hat. Erst nachdem sich dieser Verdacht im Gespräch mit dem Klosterbruder bestätigt, eröffnet Nathan allen anderen anwesenden Figuren am Ende des Dramas, dass Recha und der Tempelherr Geschwister sind. Liegt eine größere annotierte Stichprobe vor, können die annotierten Daten auf Muster untersucht werden, die im Hinblick auf zeitgenössische Dramenpoetiken und deren Normvorstellungen zu interpretieren sind. Anhand unserer bislang annotierten Dramen wollen wir dazu abschließend eine erste statistische Auswertung skizzieren. Das dazugehörige Analysekorpus umfasst zum gegenwärtigen Zeitpunkt elf Dramen. Darüber hinaus lässt sich feststellen, dass die Textstellen, an denen Wissensvermittlungen annotiert werden, ungleich über den Verlauf der Dramen verteilt sind. So treten zu Beginn und gegen Ende eines Dramas gehäuft Annotationen auf (jeweils 13% aller Annotationen), während die übrigen Annotationen relativ homogen über den Handlungsverlauf verteilt sind. Ausgehend von diesen ersten Auswertungen ergeben sich für künftige quantitative Analysen vielversprechende Perspektiven. Neben der bloßen Anzahl an Relationen, die im Verlauf der Stücke als neues Wissen an andere Figuren weitergegeben werden, und der Frage nach dem Zeitpunkt der Wissensweitergabe im Verlauf des Dramas, ergeben sich auch literaturwissenschaftlich avanciertere Fragestellungen. Unterscheiden sich die Muster der Wissensweitergabe für verschiedene Gattungen, also etwa die dramatischen Großgattungen Tragödie und Komödie? Welche Figuren geben das Wissen über familiäre Figurenrelationen weiter, an welche Figuren wird es weitergegeben? Lassen sich hierbei Muster identifizieren, etwa hinsichtlich des Geschlechts der Figuren? Ist es darüberhinaus möglich, die Szenen der Wissensweitergabe näher zu charakterisieren: Wie viele Figuren stehen in diesen Szenen auf der Bühne? Wie viele sind davon an der Wissensweitergabe aktiv beteiligt? Eine systematische Annotation von Prozessen der Wissensvermittlung im Drama ermöglicht eine Analyse, die über formale Merkmale der Textoberfläche hinausgeht. Liegen die Wissensbestände der Figuren und ihre Entwicklung im Verlauf des Dramas in maschinenlesbarer Form vor, lassen sich Zusammenhänge zwischen verschiedenen Textstellen identifizieren, an denen Widersprüche im Wissen der Figuren deutlich werden oder konflikthafte Relationen auftreten, wenn etwa zwei Figuren zugleich Geschwister und Liebespaar sind. Diese Widersprüche sollen über ein formalisiertes Regelsystem automatisch aus der Annotation der Familienrelationen inferiert werden. Die Erweiterung der quantitativen Analyse auf Phänomene jenseits der Textoberfläche ist naturgemäß mit größeren Herausforderungen für die Automatisierung verbunden. Vielfach zeigen sich aber sprachliche Muster, etwa Wiederholungen und Rückfragen, die einen als überraschend markierten Wissenszuwachs verdeutlichen (siehe Abbildung 3) und Hoffnung für die automatische Identifikation derartiger Textstellen machen. TEMPELHERR. Nicht mehr! Ich bitt"" Euch! 'Aber Rechas Bruder? Rechas Bruder ... NATHAN. Seid Ihr! TEMPELHERR. Ich? ich ihr Bruder? RECHA. Er mein Bruder? SITTAH. Geschwister! SALADIN. Sie Geschwister!"
2022,DHd2022,BUSCH_Anna_Digitale_Archive_für_Literatur.xml,Digitale Archive für Literatur,"Anna Busch (Theodor-Fontane-Archiv, Universität Potsdam); Bernhard Fetz (Literaturarchiv und Literaturmuseum Österreichische Nationalbibliothek); Marcel Lepper (Goethe- und Schiller-Archiv Weimar); Irmgard Wirtz Eybl (Schweizerisches Literaturarchiv); Sandra Richter (Deutsches Literaturarchiv Marbach); Peer Trilcke (Theodor-Fontane-Archiv, Universität Potsdam)","Literaturarchiv, Digitalisierung, Transformation, Standards, Infrastruktur, DACH","Literaturarchiv, Digitalisierung, Transformation, Standards, Infrastruktur, DACH","Die Reflexion der institutionellen Transformation von Literaturarchiven angesichts der Digitalisierung überschreitet die Einzelinstitutionen notwendig, gerade dort, wo es gemeinsame Praktiken, Routinen, Standards und Infrastrukturen zu entwickeln gilt. Das Panel greift diesen Bedarf durch seinen internationalen und interinstitutionellen Ansatz auf. Literaturarchive stehen durch die Digitalisierung vor einer Vielzahl an Herausforderungen: Begriff, Praxis und Materialität des Literaturarchivs befinden sich in einem Transformationsprozess, den die Institution ""Literaturarchiv"" in dieser Grundsätzlichkeit seit ihrer konzeptionellen Erfindung im 19. Jahrhundert (Goethe 1823, Dilthey 1970 [1889], Thaller 2011) nicht durchlaufen hat. Die Anforderungen nach Partizipation (Theimer 2018), die Digitalisierung der Bestände und die Umsetzung von Open Access- und Data-Strategien (Szekely 2017), die Adressierung der Fragen, die born-digitals mit sich bringen, gehen in vielen Fällen einher mit einem Umbau von Routinen und Handlungsprogrammen wie mit einer Befragung und Neuerfindung der eigenen Identität als Institutionen (Cook 2013). Archivmitarbeiterinnen und -mitarbeiter werden mit immer größeren informatischen Herausforderungen und Aufgabenspektren betraut, immer öfter übernehmen Informatikerinnen und Informatiker entscheidende Rollen beim Sammlungszugang und bei der Überlieferungspräsentation. Digital- und Datenkompetenzen werden zum unverzichtbaren Handwerkszeug für moderne Literaturarchive, die sich zu Datendienstleistern wandeln 'ein Prozess, mit dem sich Bibliotheken bereits seit längerer Zeit beschäftigen (exemplarisch Stäcker 2019). Es gilt folglich, über die Aufgaben und Herausforderungen, die die fortschreitende Digitalisierung des kulturellen Gedächtnisses speziell für Literaturarchive mit sich bringt, nachzudenken und Lösungsansätze zu entwickeln, wie ihnen zukünftig begegnet werden kann. Mit dem Panel soll ein in der Community der Literaturarchive 'etwa im Netzwerk ""KOOP-LITERA"", in der Schriftenreihe Das Panel forciert diesen Austausch durch einen strukturiert-systematischen Impuls, bei dem wir die digitale Transformation in Literaturarchiven auf drei Ebenen adressieren: Das Digitale, das allerorten vermeintliche ""Archive"" hervorbringt, erweitert und stellt den gewachsenen Begriff des Archivs in Frage. Eine neue Phase der begrifflichen Reflexion setzt ein, in der auch Literaturarchive ihre Selbstbeschreibung überdenken. Digitale Werkzeuge und Infrastrukturen durchdringen die Praktiken heutiger ArchivarInnen, die beim Sammeln, Bewahren, Erschließen, Vermitteln immer häufiger zugleich Daten- und CodeexpertInnen sein müssen. Im Kontext der Digitalisierung ist ein neues Verständnis von Praktiken und Handlungsprogrammen der Tätigkeiten in Literaturarchiven zu entwickeln. Das Spektrum der Objekte, die von Literaturarchiven ""prozessiert"" werden, wandelt und weitet sich. Literaturarchive befinden sich in einer Situation, in der sie die Materialität und Objekthaftigkeit ihrer Bestände und Sammlungen neu begreifen müssen. Das Panel versammelt VertreterInnen von bedeutenden Literaturarchiven aus dem DACH-Raum: Bernhard Fetz, (Literaturarchiv und Literaturmuseum der Österreichische Nationalbibliothek, Wien), Marcel Lepper (Goethe- und Schiller-Archiv Weimar), Sandra Richter (Deutsches Literaturarchiv Marbach) und Irmgard Wirtz Eybl (Schweizerisches Literaturarchiv, Bern). Die Moderation übernehmen Anna Busch und Peer Trilcke (Theodor-Fontane-Archiv, Potsdam). Mit theoretischem, konzeptionell-institutionellem und praxeologischem Blick sucht das Panel nach dem neuen Selbstverständnis der ""Digitalen Archive für Literatur"". Um die Diskussion vorzubereiten, haben die vier VertreterInnen Positionierungen und Reflexionen zu den drei systematischen Ebenen ausformuliert. Die digitale Kommunikation des Archivs könnte eine Bewegung auslösen, so die Utopie des Archivs, die die Objekte und deren HüterInnen zu MediatorInnen eines umfassenden Bildungsbegriffs werden lässt, eines Prozesses, der Traditionen, (nationale) kulturelle Repräsentationen und die Werke der ""Großen"" fluide macht. Die ubiquitäre Verfügbarkeit der Archivalien geht mit Prozessen der Entkanonisierung einher. Die Hochkultur und das ""Gipfelprinzip"" verlieren an Geltung, der Fokus verschiebt sich von einzelnen Werken zu den diversen Lebens- und Arbeitsspuren in digitalen Archiven und sozialen Netzwerken (zu Tage- und Notizbüchern, biografischen Projekten, zu Recherche als Form und Selbstvergewisserung). Die ArchivarInnen als sichtende, selektierende, bewertende, bewahrende Instanzen mutieren zu den DatenkuratorInnen von morgen. Der Begriff des ""data curators"" ist schillernd: Die digitalen KuratorInnen sind die HerrscherInnen über die Schnittstellen, sie sind aber auch SammlungsmanagerInnen, mehr oder weniger kuratorische Freigeister, abhängig vom institutionellen Selbstverständnis der Archive. Sie stellen Corpora zu bestimmten Themen in Labs oder auf Plattformen zusammen, bieten digitale Werkzeuge zu deren Nutzung an und richten die virtuellen Archivräume der Zukunft ein, in denen wir forschen, uns weiterbilden und ""erleben"", in denen wir Teilhabe an Kultur erproben sollen. Die Archivzeugen in den Depots, wiedergeboren als digitale Objekte, multiplizieren deren kulturelle und soziale Erscheinungsformen 'als visualisierte, transkribierte und kommentierte Handschriften im Rahmen eines digitalen Editionsprojektes, als¬† Ausgangspunkt von Geschichten im analogen und virtuellen Museum, als Beweisstück aus dem Webportal in der öffentlichen Debatte, als Flaschenpost in den sozialen Medien, als im Kontext eines Nachlasses zu erschließendes Objekt in der Praxis des Archivs. Die digitalen Sammelobjekte der Zukunft 'seien es E-Mails, Social Media-Beiträge, Netzliteratur oder Serien 'transformieren den traditionellen Literaturbegriff. Die digitale Transformation, die gegenwärtig in den Wissenschafts- und Kultureinrichtungen zu gestalten ist, bringt ihre eigenen öffentlichen Irrtümer und ihre eigenen falschen Begriffe mit (Francis Bacon, In der öffentlichen Wahrnehmung kämpft hochgradig ausdifferenzierte Forschung 'und nicht allein historische und philologische 'aktuell gegen Erfahrbarkeitsdefizite. Archive haben in den vergangenen Jahren dazu beigetragen, den abstrakten sprachlichen Gegenstand erfahrbar und vorstellbar zu machen. Nicht die Wahl zwischen der Welt des Papiers und der Welt der Daten, sondern die erfindungsreiche Gestaltung von Anschaulichkeit und Erfahrung im digitalen Modus ist die Herausforderung, vor der Archive für Literatur gegenwärtig stehen. Flachware war lange die Krux der Literaturausstellungen. Wie arbeiten Archive im Zeitalter von 3D und 4D? Visualisierungspraktiken, die ein Manuskript nicht mehr als Pixelfläche, sondern als Datenkubus präsentieren, und digital erzeugte Objekte nicht mehr in genetisch-qualitativer, sondern in struktural-quantitativer Form, verändern das Grundverständnis vom Gegenstand der lesenden und schreibenden Fächer. Archive waren jahrhundertelang als räumliche Ordnungen gedacht: als Gebäude mit Gängen, Regalen, Schränken, in denen Schätze liegen, die jemand besitzt und die es aufgrund schwieriger konservatorischer Bedingungen am Ort zu untersuchen gilt. Zu den Versprechen des Digitalen gehört die virtuelle Verfügbarkeit, Durchsuchbarkeit und Erweiterbarkeit digitaler Daten, das Archiv als Literaturdatenzentrum. Damit löst sich, pointiert formuliert, der Begriff vom Archiv auf: Literaturdatenzentren kennen nurmehr virtuelle Räume, in denen existierende und künftige Daten überall zugänglich sind, sich teilen, verknüpfen und neu ordnen lassen. Aus den Praktiken der Datenspender und -nutzer entstehen Korpora und andere Forschungsdaten, die sich durch ihre Qualität und Anschlussfähigkeit zur Nachnutzung empfehlen. Die Vision von einem solchen Literaturdatenzentrum erscheint jedoch in mindestens zweierlei Hinsicht als unrealistisch: Zum einen lässt sich die Datenqualität und -vergleichbarkeit auf Dauer nicht nur durch temporär diese Daten Spendende und Nutzende sicherstellen, und die Daten lassen sich auch nicht einfach erhalten, ergänzen, pflegen. Zum anderen sind die physischen Objekte, die derzeit in Archiven liegen oder dort künftig eingehen, erst in digitale Daten zu übertragen; außerdem werden Sammlungen von Forschungsdaten in einigen Jahren selbst Archivobjekte. Die Objektgruppen der Archive vervielfältigen sich durch das Digitale ein weiteres Mal, und ihre Entwicklung zu Literaturdatenzentren ist möglicherweise bloß ein weiteres historisches Stadium einer erstaunlich stabilen epistemologischen Ordnung. Literaturarchive sind mehr als Sammlungen ihrer Vor- und Nachlässe. Sie entwickeln und verwalten Wissen um die Erhaltung und die Derzeit befinden wir uns in einer langwierigen technischen Transformationsphase, einer Materielle Dokumente oder digitale Daten 'das ist eine falsche Distinktion, auch digitale Daten haben materielle Träger. Daten lassen sich ablösen, unterscheiden sie sich darin vom Original? Wir stehen in einer Verunsicherung in Bezug auf das Original (nicht das Kunstwerk) und seine Reproduzierbarkeit (Walter Benjamin). Dabei ist das Sammeln digitaler Dokumente nicht zu verwechseln mit der Digitalisierung von Dokumenten: Diese setzt minimal beim Scan ein und geht maximal bis zur genetischen Edition. Eine andere Aufgabe der Literaturarchive ist das Sammeln und Aufbereiten digitaler Daten (digital born). Eine Herausforderung ist es, die technischen Zugänge und die Lesbarkeit der digitalen Datenträger aus den Anfängen des PC, der Mails und der fotografisch-/filmischen Selbstdokumentation zu gewährleisten. Wichtiger wird die Entwicklung der Standards & Normen, die Pflege und Sicherung der Metadaten und ihre intelligente Vernetzung. Erhalten (im doppelten Wortsinn) die Archive und verwalten sie künftig technisch und rechtliche aufbereitete Daten und dokumentieren deren Nutzung?"
2022,DHd2022,SCHUMACHER_Mareike_GitMA_oder_CATMA_für_Fortgeschrittene___P.xml,GitMA oder CATMA für Fortgeschrittene Projektdaten via Git abrufen und mittels Python-Bibliothek weiterverarbeiten,"Mareike Schumacher (Technische Universität Darmstadt, Germany); Michael Vauth (Technische Universität Darmstadt, Germany); Dominik Gerstorfer (Technische Universität Darmstadt, Germany); Malte Meister (Technische Universität Darmstadt, Germany)","digitale Annotation, Datenanalyse, Visualisierung","Inhaltsanalyse, Annotieren, Einführung, Visualisierung, Literatur, Text","Dieser CATMA-6-Workshop richtet sich an fortgeschrittene CATMA User*innen mit Vorkenntnissen in digitaler Annotation, die im Rahmen der eigenen Arbeit oder von Forschungsprojekten mit größeren Mengen von Annotationsdaten operieren (wollen). Im Zentrum steht die Weiterverarbeitung und Analyse von Annotationsdaten. Wie greife ich über Git auf meine CATMA-Annotationsdaten zu? Wie erstelle ich individuelle, interaktive Visualisierungen meiner Annotationsdaten? Wie berechne ich die Übereinstimmung zwischen mehreren Annotator*innen? Diese und ähnliche Fragen werden während des Workshops beantwortet. CATMA (Gius et al. 2021) ist eine webbasierte, kollaborative Textannotations- und Analyse-Plattform, die seit 2008 an der Universität Hamburg und im Rahmen des DFG-geförderten Projektes forTEXT seit 2020 an der Technischen Universität Darmstadt entwickelt wird. Der Workshop bietet: Eine der wichtigsten Neuerungen von CATMA 6 gegenüber früheren Versionen ist die Umstellung auf eine projektzentrierte Nutzungsarchitektur. Am Beginn der Arbeit mit CATMA steht das Anlegen eines Projektes mit beliebig vielen Dokumenten, die analysiert werden sollen, und beliebig vielen Team-Mitgliedern, die daran arbeiten wollen. Zur Annotation können eigene Taxonomien entworfen oder auf der Plattform Niedrigschwelligkeit und Nähe zu traditionell-analogen Methoden der Geisteswissenschaften sind nach wie vor wichtige Grundsätze, die in CATMA implementiert sind. Doch mit zunehmender Verbreitung des Tools in den digitalen Geisteswissenschaften sind neben der Möglichkeit zu hermeneutisch-vielfältiger Textanalyse auch die Einhaltung von Best Practices und Standards, die innerhalb der Digital-Humanities-Community entwickelt wurden, von Bedeutung. Eine Verschmelzung von CATMA und Git zu ""GitMA"" ermöglicht beides. Dabei bleibt der Annotationsprozess selbst völlig frei gestaltbar. Die resultierenden Daten aber können zum Beispiel nach der Übereinstimmung der Annotierenden untereinander ausgewertet werden. Es ist möglich eine der Annotationen als ""Silver Annotation"" festzulegen und die anderen daran zu messen. Das festgestellte Disagreement kann zur Grundlage eines Disagreement-Tagsets werden, das über das Backend auch wieder ins Frontend der CATMA-GUI zurückgespielt werden kann (siehe Abb. 1). Dasselbe gilt für die nicht übereinstimmend annotierten Passagen, welche wiederum selbst durch Annotationen dargestellt/hervorgehoben werden können. So ergibt sich ein harmonischer Workflow vom Frontend zum Backend und zurück, der in Zukunft auch die Erstellung von Goldannotationen unterstützen wird. Die GitMA-Funktionalitäten werden im Rahmen dieses Workshops erstmals einem Fachpublikum vorgestellt. Neben der Vermittlung von Nutzungskompetenzen möchten wir darum auch eine kritische Diskussion anregen. Feedback zu Idee und Umsetzung der CATMA-Backend-Nutzung sind uns überaus willkommen! Der Workshop wird als ganztägiges hands-on Tutorial angeboten, das an einem oder an zwei aufeinander folgenden (halben) Tagen stattfinden kann. Teil 1 Pause Nutzer*innen, die Annotationen mit CATMA in Forschungsprojekten oder Lehrsituationen managen, sowie alle, die einen schnellen Workflow zwischen Annotation bzw. Annotationsbearbeitung und Annotationsauswertung benötigen. 30 Die benötigten Vorinstallationen von Git, Anaconda und Plotly können durch die Bereitstellung eines Docker-Image vermieden werden. Die Teilnehmer*innen sollten die Installation von Docker selbst auf einem eigenen Laptop (Touch Devices werden nicht unterstützt), den sie zum Workshop mitbringen, möglichst schon erledigt haben. Für die Durchführung des Workshops benötigen wir außerdem einen Beamer. Zur Vorbereitung sollten Teilnehmer*innen außerdem schon einen CATMA-Account erstellt (unter Die Teilnehmer*innen sollten über grundlegende Kenntnisse der Kommandozeile, Git und Python sowie Jupyter verfügen. Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Landwehrstraße 50A, 64293 Darmstadt Michael Vauth promoviert über ""Zur Annotation intradiegetischen Erzählens. Binnenerzählungen im literarischen Werk Heinrich von Kleists"" an der Technischen Universität Darmstadt. Er ist wissenschaftlicher Mitarbeiter im Forschungsprojekt EvENT (Evaluating Events in Narrative Theory) an der Technischen Universität Darmstadt. Zuvor hat er an der Technischen Universität Hamburg im Projekt hermA (Automatisierte Modellierung hermeneutischer Prozesse - Der Einsatz von Annotationen für sozial- und geisteswissenschaftliche Analysen im Gesundheitsbereich) gearbeitet. Er beschäftigt sich insbesondere mit der digitalen Narratologie und der Methodik der Netzwerkanalyse. Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Landwehrstraße 50A, 64293 Darmstadt Dominik Gerstorfer promoviert über ""Philosophische Fragen der Digital Humanities"" an der Universität Stuttgart. Derzeit ist er im DFG-Projekt forTEXT tätig, zuvor war er im Digital-Humanities-Projekt CRETA in Stuttgart beschäftigt. Dominik hat an der Universität Tübingen Philosophie, Politikwissenschaften und Soziologie (M.A.) studiert. Seine Forschungsschwerpunkte liegen in den Bereichen Wissenschaftstheorie, formale Methoden und Argumentationsanalyse. Im Rahmen von forTEXT beschäftigt sich Dominik u.a. mit Intertextualität, Ontologien und der Entwicklung von Kategoriensystemen. Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Landwehrstraße 50A, 64293 Darmstadt Malte Meister hat 2009 sein Informatik-Diplom (B.Sc.) in Kapstadt erworben. Im Rahmen des Abschlussprojekts für sein Diplom wurde er beauftragt, das Text-Annotations und -Analysetool CATMA, für die Universität Hamburg zu erstellen. Bis Anfang 2010 wirkte er im Team an CATMA mit, bevor er sich auf seine Karriere in der freien Wirtschaft konzentrierte. Nach mehr als zehn Jahren Berufserfahrung als Softwareentwickler und Teamleiter entschied er sich, wieder in die CATMA-Entwicklung einzusteigen. Er ist seit 2021 technischer Mitarbeiter an der TU Darmstadt und beschäftigt sich dort im Rahmen von forTEXT hauptsächlich mit dem Betrieb und der Weiterentwicklung von CATMA und den damit verbundenen Systemen. Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Landwehrstraße 50A, 64293 Darmstadt Mareike Schumacher koordiniert das DFG-Projekt forTEXT ("
2022,DHd2022,KONLE_Leonard_Genitivmetaphern_in_der_Lyrik_des_Realismus_un.xml,Genitivmetaphern in der Lyrik des Realismus und der frühen Moderne,"Merten Kröncke (Universität Göttingen, Germany); Leonard Konle (Universität Würzburg, Germany); Fotis Jannidis (Universität Würzburg, Germany); Simone Winko (Universität Göttingen, Germany)","Metapher, Lyrik, Realismus, Moderne","Datenerkennung, Programmierung, Annotieren, Stilistische Analyse, Literatur, Text","Ein wichtiger Aspekt der sprachlichen Gestaltung literarischer Texte besteht im Einsatz von Metaphern, Metonymien und Tropen im Allgemeinen. Einzelnen Werken, aber auch ganzen Gattungen oder Epochen wird zugeschrieben, dass ihre Spezifik nicht zuletzt in einer jeweils charakteristischen Verwendungsweise uneigentlicher Rede gründe. Unter anderem betrifft das die Geschichte der Lyrik, das heißt die Geschichte einer Gattung, die laut Benjamin Specht ""in Bezug auf die Verwendung von Metaphern die weitesten Lizenzen besitzt"" (Specht, 2017: 90). Das Ziel dieses Beitrags besteht darin, den Gebrauch von Metaphern in der deutschsprachigen Lyrik des Realismus und der frühen Moderne Die literaturwissenschaftliche Forschung macht die Unterscheidung von realistischer und moderner Lyrik unter anderem am Aufkommen neuer, innovativer Formen uneigentlichen Sprechens fest. In der modernen Lyrik treten Metaphern auf, die als ""Radikalisierung, Komplizierung und Steigerung"" lyrischer Bildlichkeit (Hiebel 2005: 28), als Beitrag zur sprachlichen ""Verfremdung"" (Lamping, 2010: 148; vgl auch Lamping, 2008: 25f), als ""assoziativ-hermetische"" Muster (Specht, 2014: 5) oder auch als ""Blume[n] ohne Stiel auf der Oberfläche des Gedichts"" (Neumann, 1970: 195f) zu charakterisieren seien. Die Forschung dürfte sich einig sein, dass die moderne Metaphorik gegenüber der vorherigen, traditionellen Bildlichkeit zu größerer Individualität und Heterogenität tendiert (vgl. zur Homogenität der realistischen (Massen-)Lyrik und ihrer Sprachbilder z. B. Stockinger, 2010: 88). Doch durch welche Textmerkmale sich die neuen, modernen Metaphern im Einzelnen auszeichnen, wird unterschiedlich und zum Teil sogar gegensätzlich konzeptualisiert. Unser Beitrag untersucht nur Genitivmetaphern (""Das Lächeln der Natur"" usw.); andere Formen, zum Beispiel Adjektivmetaphern (""Die lächelnde Natur"" usw.), bleiben unberücksichtigt. Zumindest Hugo Friedrich ist allerdings der Auffassung, dass es sich bei Genitivmetaphern ohnehin um den häufigsten Typ von Metaphern in der (modernen) Lyrik handelt (Friedrich, 1992: 205). Im Normalfall der Genitivmetapher bezieht sich das Die hier untersuchten Phänomene (Metaphern in Genitivkonstruktionen) sind mithin nicht exakt identisch mit dem Gegenstand der literaturwissenschaftlichen Forschungsthesen (Metaphern im Allgemeinen), auch wenn man davon ausgehen darf, dass Aussagen über den einen Bereich ebenfalls relevant für den anderen Bereich sind. Eine weitere Relativierung betrifft das Untersuchungskorpus: Während sich sich die Forschungsaussagen in der Regel auf kanonisch-moderne sowie des Öfteren auf deutlich nach 1900 erschienene Gedichte beziehen, enthält das hier zu analysierende Korpus lediglich Texte der Jahrhundertwende um 1900 und damit der Die analysierten Gedichte stammen aus den 7 Anthologien des Realismus und den 13 Anthologien der Lyrik um 1900 mit insgesamt 6249 Texten, die wir im Rahmen des Projekts ""The beginnings of modern poetry - Modeling literary history with text similarities"" untersuchen. Bei den Anthologien um 1900 handelt es sich um Sammlungen, deren Herausgeber Gedichte aufgrund ihrer Modernität ausgewählt haben (siehe Tabelle 1). Aus diesem Korpus sind unter Verwendung von spaCy (Montani et al., 2021) 4300 Genitivkonstruktionen Drei Annotatoren haben Genitivkonstruktionen als metaphorisch oder nicht-metaphorisch annotiert. Annotiert wurden die beiden oben beschriebenen Formen von Genitivmetaphern, aber nur wenn sie in dem Muster Annotationen Realismus Annotationen Moderne Der annotierte Datensatz umfasst 625 Genitivkonstruktionen mit einem Agreement von 0.53 Für die automatische Metaphernerkennung verwenden wir neben den Annotationen ein deutsches Bert Model Die Erkennung der Metaphern geschieht in zwei Schritten. Im ersten Schritt werden regelbasiert Genitivkonstruktionen erkannt (siehe Ressourcen); im zweiten Schritt werden diese als ""nicht-metaphorisch"" oder ""metaphorisch"" klassifiziert. Das System zur Klassifikation von Genitivmetaphern setzt sich aus drei Komponenten zusammen: Supersenser, Affecter und Bert (siehe Abb. 1). Damit folgt der Aufbau dem in Tsvetkov et al. (2014) vorgestellten Ansatz Metaphern unter Berücksichtigung von Abstraktheit, Vorstellbarkeit und Wortklasse ihrer Komponenten zu klassifizieren. Das Training des Supersenser Moduls wird auf den FastText Vektoren der Wörter aus den 20 größten Supersense-Klassen für Substantive aus GermaNet durchgeführt. Diese werden durch überwachte Dimensionsreduktion (Szubert et al., 2019) in einen kleineren Raum mit 10 Dimensionen projiziert. Im Gegensatz zur direkten Verwendung von GermaNet als Eingabe in das System können so out-of-vocabulary Probleme vermieden werden. Außerdem wird durch die Projektion eine reichhaltigere Repräsentation erzeugt in der Supersense-Klassen in Beziehung gesetzt werden können. Eine Evaluation mittels kNN Klassifikation von Substantiven im projizierten Raum in ihre Supersense Klasse ergibt einen F-Score von 0.65. Das Affecter Modul erhält ebenfalls FastText Vektoren, sowie die zugehörigen Werte aus der Wortliste von Köper und Schulte im Walde (2016). Eine 4-fach Regression durch ein MLP erreicht R Für die Klassifikation von Metaphern werden die Ausgaben aus Supersenser und Affecter an ein nach (Gao et al., 2019) modifiziertes BERT Modell übergeben. Dieses reicht nicht nur das CLS-Token, sondern auch die Embeddings der Genitivkonstruktion weiter. Bei der Unterscheidung zwischen Metaphern und sonstigen Genitivkonstruktionen erreicht das System einen F1 Score von 0.75 (Details siehe Tabelle 3). Die Klassifikation von Metaphern aus dem Realismus wird zwar leicht besser evaluiert als die aus den modernen Anthologien, der Effekt ist für die weitere Analyse aber vernachlässigbar. Es gibt keinen Unterschied zwischen den Epochen in Hinsicht auf die Menge der Genitivkonstruktionen und den Anteil der Metaphern daran. Die These von der größeren Heterogenität und Individualität der modernen Metaphern haben wir mit zwei Operationalisierungen untersucht: Nimmt der Anteil an seltenen Wörtern zu und steigt der Type-Token-Ratio bei Metaphern der Moderne? Den Anteil der seltenen Wörter haben wir mit einem einfachen Verfahren überprüft, nämlich ob die Token im Wordembedding enthalten sind; wenn nicht, wurden sie als ""seltene Wörter"" identifiziert. Solche Wörter sind in der Lyrik häufig, zumeist handelt es sich um Komposita oder um Schreibvarianten aufgrund der Anpassung ans Metrum. Die Verwendung solcher seltenen Wörter ist auch schon im Realismus häufig: 26%, findet sich aber noch einmal häufiger in der Moderne 33%. Wir haben den Type-Token-Ratio beider Bestandteile der Genitivmetaphern jeder Epoche untersucht und können auch hier einen signifikanten Unterschied feststellen: Bei den Metaphern der Moderne ist die Variabilität des Wortmaterials deutlich größer (siehe Abb. 2 und 3). Insgesamt können wir also den Eindruck bestätigen, dass die Metaphern der Moderne -- und das noch vor dem Expressionismus -- heterogener und individueller sind. Die These zur Vergrößerung bzw. Verkleinerung der Bildspanne in den Metaphern haben wir überprüft, indem wir den Kosinusabstand der Substantive im FastText-Wordembedding gemessen haben. Wie Abb. 4 zeigt, hat der Abstand weder zu- noch abgenommen. Die These, dass in der Moderne der Abstand zwischen den Nomina einer Genitivmetapher in Hinsicht auf die Abstraktheit zugenommen hat, haben wir mit den Abstraktheits-Werten überprüft, die das oben dargestellten Affecter-Moduls vorhergesagt hat (siehe Abb. 5), ebenso wie die These, dass die Moderne insgesamt zu abstrakteren (oder gerade zu weniger abstrakten) Metaphern neigt. Die Forschungsthesen lassen sich durch unsere Daten nicht bestätigen: Weder verändert sich das Level der durchschnittlichen Abstraktheit oder Konkretheit noch die Ein Blick auf die beliebtesten Wörter der Metaphern, getrennt nach der Verwendung in der Kopfposition oder im Genitiv, zeigt einige interessante Verschiebungen. Bei den Kopfnomina hat ""Meer"" eine steile Karriere von Rang 10 auf Rang 1 gemacht, während ""Hauch"", ""Geist"" und ""Traum"" deutlich weniger verwendet werden (siehe Abb. 6). Insgesamt sind die Wiederholungen bei den Kopfnomina weniger häufig und sie nehmen in der Moderne noch ab. Die Genitivnomina (Abb. 7) dagegen zeichnen sich durch zahlreiche Wiederholungen aus, allerdings nimmt auch hier die Frequenz in der Moderne ab. Auffällig ist, wie stabil die Wortlisten in den oberen Rängen sind, mit der Ausnahme von ""Glücks"" und ""Todes"", die in der Moderne deutlich häufiger werden. Insgesamt konnten wir einige der von der Literaturwissenschaft aufgestellten Vermutungen bestätigen, z. B. dass die Komplexität der Metaphern in der Moderne ansteigt. Da viele der Forschungsthesen für die Moderne insgesamt formuliert wurden, müssen wir allerdings dort, wo wir diese nicht bestätigen konnten, es zumindest offen halten, ob sie nicht auf spätere Phasen zutreffen. Allerdings deuten wir unsere Befunde doch so, dass die bisherige Forschung den Bruch zwischen den Epochen deutlich überbetont hat. Unsere Ergebnisse weisen vielmehr darauf hin, dass es sich um eine semantische Evolution handelt. Die hier dargestellten Ergebnisse sollen außerdem in Zukunft noch verbessert werden, indem die regelbasierte Erkennung der Genitivkonstruktionen auf eine bessere Grundlage gestellt wird: Da wir zur Zeit kein Korpus an Gedichten haben, in dem alle Genitivmetaphern annotiert sind, können wir den Recall nicht einschätzen. Außerdem soll ein umfassendes Korpus aus Texten des 19. Jahrhundert das Training eines domänenangepassten Fasttext-Modells ermöglichen, das historisch angemessenere semantische Distanzen zurückgibt. Nicht zuletzt soll auch die Erkennungsgenauigkeit des Systems zur Erkennung der Metaphern durch eine Vergrößerung des Trainingskorpus, durch eine aufwendigere Hyperparameter-Optimierung und durch Arbeit an der Architektur verbessert werden. Ein etwas anspruchsvolleres Ziel besteht außerdem in dem Versuch, die Typisierung der Metaphern auf eine andere Grundlage zu stellen, etwa durch Anschluss an die Kategorien linguistischer Metapherntheorien oder an große Ontologien. Nicht zuletzt werden wir untersuchen, wo die kanonisierten Lyriker dieser Zeit, George, Hofmannsthal, Holz und Rilke, stehen."
2022,DHd2022,MEISTER_Malte_GitMA_Poster__CATMA_Daten_via_Git_abrufen_und_.xml,GitMA-Poster   CATMA-Daten via Git abrufen und mittels Python-Bibliothek weiterverarbeiten,"Malte Meister (Technische Universität Darmstadt, Germany); Michael Vauth (Technische Universität Darmstadt, Germany); Dominik Gerstorfer (Technische Universität Darmstadt, Germany)","CATMA, Git, Python","Annotieren, Bearbeitung, Einführung, Visualisierung, Daten, Software","Etwas zu erinnern heißt nicht, es abzuspeichern, sondern auch, es abzurufen und weiter zu prozessieren. Denn nur im produktiven Anschluss erhält die Erinnerung eine Bedeutung. Diese Beobachtung trifft CATMA (Computer Assisted Text Markup and Analysis) ist eine kollaborative Textannotations- und Analyse-Plattform, die in den Digital Humanities gut etabliert ist und von vielen Projekten aktiv genutzt wird Der genaue Aufbau der Datenstrukturen wird auf der CATMA Webseite dokumentiert (Petris 2020): Jedes Dokument, jede Annotation Collection einschließlich der Annotationen, sowie jedes Tagset einschließlich der zugehörigen Tags werden im Backend einzeln repräsentiert. Besonders wichtig für die Weiterverarbeitung der Annotationsdaten sind die Informationen, mit denen die einzelnen Annotationen repräsentiert werden: Die Nutzer:innen können sowohl auf eigene als auch auf mit ihnen geteilte Daten in Form von Git Repositorien zugreifen. Diese stellen damit eine Art Programmierschnittstelle (API) zum Abruf von CATMA-Annotationen dar, welche auf den lokalen Rechner heruntergeladen oder in anderen Tools weiterverarbeitet werden können. Im Fachbereich für Digital Philology an der TU Darmstadt ist außerdem eine Insgesamt ist das zentrale Anliegen des Git Access, CATMA-Daten direkt verfügbar zu machen, damit Nutzer:innen nicht unbedingt an die schon in CATMA vorhandenen Funktionalitäten gebunden sind. Dadurch kann der Workflow zwischen Annotation, Annotationsauswertung und Annotationsüberarbeitung deutlich schneller werden. Das ist besonders für Nutzer:innen relevant, die sich 'unter anderem im Rahmen von Forschungsprojekten 'um die Organisation und Evaluierung von Annotationen kümmern. Mit unserem Poster werden wir diesen Workflow detailliert darstellen. Das Poster soll also auch als eine Art Bedienungsanleitung für die Nutzung des CATMA Git Access fungieren und Best Practices zeigen. Dabei werden wir folgende Schritte abdecken:"
2022,DHd2022,GERSTORFER_Dominik_Kategorientheoretische_Ontologieentwicklu.xml,Kategorientheoretische Ontologieentwicklung und Wissensmodellierung für die Digital Humanities,"Dominik Gerstorfer (TU Darmstadt, Germany)","Ontologie, Kategorientheorie, Formalisierung, Wissensrepräsentation","Strukturanalyse, Modellierung, Theoretisierung","Formale Modellierung von Wissen spielt in den Digital Humanities eine nicht zu unterschätzende Rolle. Daten müssen organisiert, kategorisiert, gespeichert, abgerufen und verarbeitet werden. Hierzu werden häufig Datenbank- und Ontologiesprachen wie SQL und RDF/OWL oder auch semantische Netzwerke eingesetzt, die als In diesem Beitrag werden Die mathematische Kategorientheorie wurde in den 1940er-Jahren von Saunders Mac Lane und Samuel Eilenberg entwickelt, um verschiedene mathematische Felder und Theorien zu vergleichen. Die grundlegende Einsicht hinter der Kategorientheorie formuliert Mac Lane (1998: 1) wie folgt: ""Category theory starts with the observation that many properties of mathematical systems can be unified and simplified by a presentation with diagrams of arrows."" Eine Kategorie In der Regel wird die Kategorientheorie in der Mathematik verwendet, um Theorien zu analysieren und weiter zu verallgemeinern. Man könnte auch davon sprechen, dass es sich um Metamathematik handelt, die Mathematik als Anwendungsgegenstand hat. Und obwohl die Kategorientheorie als Die angewandte Kategorientheorie oder Dies ist unter anderem in der Physik (Abramsky / Coecke 2007; Baez / Stay 2010), Linguistik (Coecke et al.¬†2010), den Neurowissenschaften (Brown / Porter 2008), Informatik (Ehrig et al. 2001) und der Philosophie (Landry 2017) geschehen. Gerade in der Informatik spielt die Kategorientheorie eine ausgezeichnete Rolle, da formale Logik und Mengentheorie in der theoretischen Informatik und funktionale Programmiersprachen wie Haskell oder Module wie Catlab.jl für Julia in der angewandten Informatik durch sie verbunden sind. In den oben genannten Fällen hat sich gezeigt, dass die angewandte Kategorientheorie einige für die Digital Humanities attraktive Eigenschaften aufweist: Der hohe Abstraktionsgrad und der Umstand, dass nur Objekte und Pfeile in einer Kategorie vorkommen, erlauben einen leichten Einstieg in kategorienthoretische Modellierung und der Einsatz von kommutativen Diagrammen zur Analyse macht die Verwendung benutzerfreundlich. Gleichzeitig können Modellierungen aufgrund der Modularität und Kompositionalität der Theorie auch sehr komplexe Sachverhalte repräsentieren, ohne selbst unüberschaubar zu werden. Darüber hinaus bietet die Kategorientheorie vielfältige Anschlussmöglichkeiten an andere mathematische Bereiche. Sollte sich herausstellen, dass formale Logik oder Grafentheorie benötigt wird, ist es ein Leichtes die nötigen Übergänge herzustellen. In diesem Sinne kann die Kategorientheorie in den Digital Humanities als leichtgewichtiges Modellierungstool eingesetzt werden, das nach Bedarf erweitert und skaliert werden kann. Dies soll nun anhand der von Spivak und Kent (2012) entwickelten  Zu beachten ist, dass Da es jederzeit möglich ist ein bestehendes Da  Im Vergleich zu RDF/OWL zeichnen sich Im Vergleich mit Datenbanken zeichnen sich Semantische Netze und Der Vortrag ist zweigeteilt: Im ersten Teil werden Im zweiten Teil werden dann die im ersten Teil gewonnen Einsichten anhand von Beispielen erläutert und gezeigt, wie die Wissensmodellierung durch Das übergreifende Ziel des Vortrags ist es zu zeigen, dass eine mathematisch-theoretische Fundierung der Ontologieentwickung und Wissensmodellierung sowohl das geisteswissenschaftliche Denken und Arbeiten unterstützen kann, als auch die technische Entwicklung und informatische Implementation treiben kann. Die Kategorientheorie hat in der Mathematik und Informatik sowie in Naturwissenschaften wie Physik, Chemie und Genetik bereits erfolgreich unter Beweis gestellt, ein geeignetes Denkwerkzeug zu sein. Der größte Nutzen in diesen Feldern ist durch Systematisierung und Vergleichbarkeit mit bzw. Anschlussfähigkeit an andere Forschungsgebiete entstanden. Diese Effekte gilt es auch für die"
2022,DHd2022,FLÜH_Marie_Jung__wild__emotional__Rollen_und_Emotionen_Jugen.xml,"Jung, wild, emotional?   Rollen und Emotionen Jugendlicher in zeitgenössischer Fantasy-Literatur","Marie Flüh (Universität Hamburg, Germany); Mareike Schumacher (Technische Universität Darmstadt)","Netzwerkanalyse, Emotions- und Genderanalyse, NER","Entdeckung, Beziehungsanalyse, Annotieren, Netzwerkanalyse, Visualisierung, Text","Im literatur- und kulturwissenschaftlich ausgerichteten Projekt Dieser Beitrag zielt darauf ab, das für phantastische Literatur als genrekonstitutiv geltende, aber recht allgemein beschriebene ""Klima des Grauens"" (Caillois 1974: 56), das Unheimliche (Todorov 2018), auszudifferenzieren. Darüber hinaus betrachten wir die für phantastische Literatur als spezifisch herausgestellten Emotionstypen in Abhängigkeit zu den Genderrollen, die den Emotionssender:innen zugeschrieben werden. Dabei rücken wir die 'ebenfalls für das betrachtete Genre typischen 'jugendlichen Hauptfiguren in den Fokus. Der Beitrag widmet sich drei eng miteinander verknüpften Forschungsfragen: Welche Genderrollen werden den Protagonist:innen in Fantasyromanen zugeschrieben? Welche Emotionstypen bestimmen das Korpus und wie sehen die Rollen- und Emotionsprofile der Protaginist:innen aus? Fallen genderstereotype, statische Muster auf? Unser Beitrag knüpft direkt an eine allgemeinere Fallstudie zum Thema Genderrollen in zeitgenössischer Jugend-Fantasy-Literatur an (Flüh, Horstmann und Schumacher, im Erscheinen) und vertieft die gewonnenen Einsichten. Grundlage der bereits abgeschlossenen Fallstudie stellen 28 deutschsprachige kontemporäre (im Zeitraum zwischen 2015 und 2020 publizierte) Fantasy-Romane für Jugendliche dar. Hierbei haben wir im Rahmen eines Mixed-Methods-Ansatzes Emotionen in Abhängigkeit zu Genderkategorien (männlich, weiblich, neutral) betrachtet, indem ein eigens trainierter Gender-Classifier (Schumacher 2021), der auf Conditional-Random-Fields-Algorithmen (vgl. Sutton und McCallum 2010) basiert und mit dem Stanford Named Entity Recognizer (vgl. Finkel et al. 2005) kompatibel ist, zur automatischen Annotation von Genderrollen mit der digitalen manuellen Annotation von Emotionsinformationen kombiniert wurde. Das Korpus wurde zunächst mit dem Gender-Classifier annotiert, der in allen Texten männliche, weibliche und neutrale Figurenrollen markiert. Die Erkennungsgenauigkeit der genutzten Version erreicht über alle Kategorien hinweg bei gattungsspezifischem Testmaterial einen F1-Score von rund 72% (vgl. Schumacher 2021). Auf die automatische Vorannotation aufbauend, wurde in 25 Romanen mit dem Textanalysetool CATMA (Gius et al. 2021) eine taxonomiebasierte Emotionsanalyse durchgeführt. Die digitale manuelle Annotation funktioniert auf Grundlage eines für die Emotionsanalyse in literarischen Texten entworfenen Tagsets und hierfür entworfenen Guidelines (vgl. Flüh 2020). Die Emotionsanalyse bezog sich auf die Textstellen, an denen vom Gender-Classifier besonders zahlreiche Genderannotationen gemacht wurden. An diesen Gender-Peaks wurde jeweils das semantische Umfeld der Gender-Annotationen nach Emotionsinformationen untersucht. Wir fokussieren also das unmittelbare Textumfeld der Figurenreferenzen und bestimmen, ob und welche emotionstragenden Textstrukturen zu finden sind. Unabhängig von Genderrollen zeigt sich, dass im Korpus Basisemotionen mit negativer Qualität romanübergreifend die Erzählwelten bestimmen. Im gesamten Korpus etabliert sich ein Emotionsprofil, das sich mit abnehmender Häufigkeit zusammensetzt aus: Ein Klima des Grauens etabliert sich deutlich über die besonders häufig vorkommenden Angst-Emotionen. Ein genauer Blick auf die Vertreter dieser Kategorie zeigt ein facettenreiches Emotionsprofil, das unterschiedliche Angstzustände und Spielarten der Angst beinhaltet (s. Tabelle 1). Die Annotation der Romane zeigt darüber hinaus, dass gerade die beiden bewusst bedeutungsoffen gestalteten Annotationskategorien ""UNCATEGORIZABLE"" (797 Annotationen) und ""PROBLEMFÖLLE"" (813 Annotationen) quantitativ ins Gewicht fallen. Während in die Kategorie ""Problemfälle"" Emotionstypen fallen, die u.a. mehrere polare Gegensätze vereinen, also nicht eindeutig als positiv oder negativ kategorisiert werden können, versammelt die Oberkategorie ""Uncategorizable"" Emotionen, die nicht einer der Basisemotionen zugeordnet werden können, die aber in ihrer Polarität durchaus eindeutig sind. Viele Textpassagen lassen sich auf Grundlage der strukturorientierten Typologisierung, die sich an der Einteilung von Basisemotionen orientiert (Ekman 1972, Schwarz-Friesel 2007), nicht adäquat beschreiben. In diesen Fällen wurden weitere Emotionstypen definiert; besonders ins Gewicht fallen dabei: Die häufigsten Vertreter dieser Kategorien nuancieren das Emotionsprofil. Scham, Aggression und Bedauern weisen eine negative Qualität auf, während Interesse und Erstaunen eher in die Kategorie positiver Emotionstypen fallen. Auffällig ist, dass alle hier vertretenen Emotionstypen im Diskurs über Sprache und Emotionen als strittige Emotionstypen verhandelt werden. Scham, eine eher intrasubjektiv empfundene Emotion, wird häufig mit ähnlichen Emotionskategorien wie Schuld, Reue oder Bedauern in Verbindung gebracht. Unklar ist hierbei, ob Scham eine eigene Kategorie darstellt oder nicht. Deutlich explosiver und als Selbst- oder Fremdschädigung auftretende Aggressionen lassen sich als Trieb oder als Emotion beschreiben. Fraglich ist auch, ob Erstaunen und Interesse als Emotion klassifiziert werden sollen (Schwarz-Friesel 2007). Hier offenbart sich ein facettenreiches negatives Emotionsprofil, das neben recht eindeutig bestimmbaren negativen Basisemotionen auch eher nach innen gerichtete und Verbundemotionen wie Scham beinhaltet. Da im Annotationsprozess für jede Emotionsannotation die Values männlich, weiblich oder neutral festgelegt wurden, lässt sich nachvollziehen, welche Emotionen mit männlichen Figuren und welche mit weiblichen Figuren in Verbindung stehen. Innerhalb der untersuchten Textpassagen konnten weiblichen Figuren 2200 Mal eine emotionale Reaktion zugeordnet werden, männliche Figuren lediglich 1474 Mal. Es zeigt sich, dass Angst im untersuchten Korpus die zentrale Emotion für beide Gender darstellt. Charaktere beider Geschlechter bilden ein negativ geprägtes Emotionsprofil aus, das unterschiedliche Angstzustände beschreibt: Besorgnis, Erschrecken und Panik bestimmen das Korpus. Nach diesem übergeordneten Blick auf das Korpus, stellt sich nun die Frage, welche Genderrollen für die jungen Protagonist:innen besonders häufig sind, ob diese spezifisch für einzelne Erzähltexte sind, oder ob sich im gesamten Korpus romanübergreifende Muster bilden. Um herauszufinden, welche Genderrollen für die jungen Protagonist:innen besonders häufig sind, ob diese spezifisch für einzelne Erzähltexte sind, oder ob sich im gesamten Korpus romanübergreifende Muster bilden, haben wir für alle Protagonist:innen in einer relationalen Graphdatenbank mithilfe der Webapplikation Graphcommons (vgl. Arƒ±kan et al., o.J.) Rollenprofile angelegt. Die vom Classifier annotierten Genderrollen wurden durch eine Kollokationsanalyse mit den Protagonist:innen in Verbindung gebracht. Dabei haben wir nach den Gendertags gesucht, in deren Wortumfeld (fünf Wörter davor und danach) der Name der Hauptfigur steht. Anschließend wurde der Annotationskontext daraufhin überprüft, ob mit der annotierten Genderrolle die Hauptfigur bezeichnet wird. Wenn dies der Fall war, wurde die Rolle im Graph mit der Figur verknüpft. Da unser Korpus auch Romanreihen beinhaltet und einige der Hauptfiguren in mehr als einem Roman als Protagonist:in auftreten 'was die Vermutung nahelegt, dass die Protagonist:innen von Reihen aufgrund der größeren Textmenge auch mit mehr Rollen bezeichnet werden könnten 'wurden die Texte ebenfalls als Knoten im Graphen angelegt und die Hauptfiguren mit den jeweiligen Texten verknüpft. Auf diese Weise entstand im ersten Schritt ein komplexer Graph mit drei Ebenen von Knoten: Texte, Hauptfiguren und Rollen (vgl. Abb. 1). In einem weiteren Schritt wurden Emotionen als vierter Knotentyp hinzugefügt. Betrachtet man die Rollenstruktur des Fantasy-Korpus, fallen mehrere Eigenheiten auf. Zunächst einmal beinhalten die Romane mehr Protagonistinnen als Protagonisten. Elf weibliche Hauptfiguren der Romane und Romanzyklen stehen vier männlichen gegenüber. Zwei der vier männlichen Protagonisten fallen durch besonders vielfältige Rollenprofile auf, während die weiblichen Hauptfiguren insgesamt weniger vielfältige Rollenprofile ausbilden. Die einzelnen Rollen sind dabei häufig stereotyp angelegt, männlichen Protagonisten werden hauptsächlich männlich stereotype Genderrollen zugeschrieben und Protagonistinnen hauptsächlich weibliche. In seltenen Fällen weisen die Protagonist:innen zusätzlich zu ihren binär gegenderten Rollen auch genderneutrale Rollen auf wie ""Kind"", ""Findelkind"", ""Mensch"" und ""guter Mensch"". Die meisten Hauptfiguren bilden im Rollennetzwerk Cluster Darüber hinaus sind die Genderrollen der ""Freundin"" und des ""Mädchens"" romanübergreifend bedeutsam; sie sind jeweils mit sieben Protagonistinnen verknüpft. Die Hauptfiguren in dieser Stichprobe deutschsprachiger Jugend-Fantasy-Romane sind also meist Mädchen, für die einerseits das familiäre und andererseits das freundschaftliche Umfeld eine wichtige Bedeutung haben. Zwei der Hauptfiguren fallen dadurch auf, dass sie nur mit einer einzigen Rolle verbunden sind. Es handelt sich dabei um Edda aus der Um zu erproben, wie das Zusammenspiel von Genderrollen und Emotionen gemeinsam betrachtet werden kann, haben wir zunächst Emotionen in einem kompletten Roman manuell annotiert. Dabei handelt es sich um einen Text mit einer Protagonistin 'Ana aus der Trilogie Ana ist die Heldin einer Trilogie (manuell annotiert wurde allerdings nur der erste Teil) und wird stark über Emotionen charakterisiert. Fünf Rollen stehen hier vierzig Emotionen gegenüber. Zwar wurden die Genderrollen automatisch und die Emotionen manuell annotiert, dennoch ist die Differenz hier signifikant. Im gesamten Korpus wurden 68 Emotionen und 83 Genderrollen ausgemacht, die sich auf Protagonist:innen beziehen. Zwar erreicht der Classifier nur eine Erkennnungsgenauigkeit von 72% und annotiert somit höchstwahrscheinlich nicht alle Genderrollen, die mit einer Figur verbunden sind. Auf der anderen Seite handelt es sich aber um eine kontextsensitive automatische Annotation, d.h. die Anzahl der Genderrollen, die erkannt werden kann, ist potentiell unendlich. Die Protagonistin tritt als ""Herrin"", ""Erbin"" und ""Fürstin"" in Erscheinung, gleichzeitig nimmt sie auch die im Korpus insgesamt sehr bedeutsame familiäre Rolle der Tochter ein. Dass Ana eine Protagonistin ist, die vergleichsweise stark über Emotionen charakterisiert wird, zeigt eine Gegenüberstellung mit den anderen Hauptfiguren im Korpus (vgl. Abb. 4). Die in Abb. 4 in Pink dargestellten Emotionsrelationen überwiegen die dunkelblauen Genderrollenzuschreibungen deutlich; stärker als dies bei anderen Protagonist:innen der Fall ist. Neben unterschiedlichen positiven Emotionen (Erheiterung, Dankbarkeit, Vertrauen, Zufriedenheit, Zuneigung, Verlangen, Zuversicht und Gelassenheit) steht eine deutlich höhere Anzahl negativer Emotionen (Abneigung, Trübsal, Leid, Schrecken, Kummer, Nervosität, Wut, Bedauern, Bestürzung, Verzweiflung, Widerwille, Scham, Panik, Ratlosigkeit, Aversion, Entsetzen, Verärgerung und Melancholie), die diese Figur charakterisieren und dem negativen genrespezifischen Emotionsprofil entsprechen. Um zu prüfen, ob es sich hierbei um ein genderspezifisches Muster handelt, haben wir abschließend die anderen Romane im Korpus betrachtet. Um die größere Stichprobe analysieren zu können, haben wir die Romane nicht im Close-Reading-Verfahren annotiert, sondern lediglich Gender-Peaks 'Passagen, in denen der Gender-Classifier besonders viele Annotationen hinzugefügt hat 'und ein Fenster von sechs Sätzen pro Genderzuschreibung betrachtet (drei vor der Erwähnung einer Genderrolle innerhalb eines Peak-Abschnitts und drei danach). Um anschließend diejenigen Emotionsannotationen ausfindig zu machen, die Emotionen markieren, die den Protagonist:innen zugeschrieben wurden, haben wir erneut Kollokationsabfragen durchgeführt, die ausschließlich Emotionsannotationen im Wortumfeld von namentlichen Erwähnungen der Hauptfigur aufzeigen. Der Graph in Abb. 5 zeigt ein interessantes Bild: Nur eine männliche und zwei weibliche Hauptfiguren werden stärker über Rollen als über Emotionen charakterisiert. Bei drei Hauptfiguren sind die Rollen- und Emotionsprofile relativ ausgeglichen. Sechs Protagonistinnen zeigen ein eindeutig stärker über Emotionen als über Rollen definiertes Profil. Es zeigt sich also eine leichte Tendenz zu einer stärkeren Rollenprofilierung der männlichen Hauptfiguren. Die Mehrzahl der Protagonistinnen wird stärker über Emotionen als über Rollen ausgestaltet. An dieser Stelle sind zwei methodenkritische Aspekte zu berücksichtigen: Erstens findet der hier vorgestellte Classifier nicht alle relevanten Entitäten; hier zeigt sich eine grundsätzliche Schwäche von NLP-Ansätzen zur Analyse von (literarischen) Texten. Zweitens führt der vorgestellte Mixed-Methods-Ansatz automatische, d.h. weniger zuverlässige, Annotationen mit manuellen zusammen. Da der Gender-Classifier mit einer Quote von 72% F1-Score nicht alle Vorkommnisse von Genderrollen annotiert, ist es möglich, dass die Figuren eigentlich mehr Genderrollen zugeschrieben bekommen, als hier gezeigt. Die Kontextsensitivität des Tools gewährleistet allerdings, dass sehr viele unterschiedliche Genderrollen automatisch annotiert werden, davon aber nicht immer unbedingt alle Vorkommnisse. Wir gehen darum davon aus, dass die Mehrheit der vorhandenen Genderrollen berücksichtigt werden konnte und der Gender-Classifier eine valide Tendenz der Verteilung aufzeigt. Auch ist bei der hier vorliegenden vergleichenden Analyse vor allem die Balance zu den anderen Texten bedeutsam. Interessant ist auch, welche Emotionen mit den meisten weiblichen Hauptfiguren verknüpft sind. Erstaunen ist mit allen acht Protagonistinnen der Stichprobe verbunden. Besorgnis empfinden sechs der acht weiblichen Hauptfiguren. Bedauern ist bei fünf Charakteren zu finden und Neugier und Zuneigung bei jeweils vier von ihnen. Von diesen fünf am meisten verknüpften Emotionen ist nur eine der Basisemotion der Angst zuzuordnen, nämlich die Besorgnis. Dabei handelt es sich allerdings um eine relativ schwache Form von Angst. Das Klima des Grauens, das für das Genre der Phantastischen Literatur postuliert wurde und das die Gesamtbetrachtung der Emotionen in unserem Korpus bestätigen konnte, geht also nicht wesentlich von den überwiegend weiblichen Hauptfiguren aus. Diese wirken dem eher besorgt, mitfühlend und auch neugierig entgegen. Bis hierhin erweist sich das geschilderte Verfahren als sinnvoll, um das Zusammenspiel von Emotionen und Genderrollen zu analysieren. Es eignet sich, um häufig vorkommende Genderrollen und Emotionsprofile zu ermitteln. Genderstereotype Muster zeichnen sich zwar in dieser Fallstudie schon ab, müssten aber durch die Analyse eines größeren Korpus noch bestätigt oder revidiert werden. Der beispielhafte Vergleich mit einem ebenfalls zunächst im Close-Reading betrachteten männlichen Hauptcharakter steht noch aus. Die Betrachtung des Gesamtkorpus gibt einen vorläufigen Hinweis darauf, dass männliche Hauptfiguren etwas weniger stark durch Emotionen und eher durch stereotype Rollenbilder charakterisiert werden. Um diese Tendenz weiter zu untersuchen, müsste allerdings in einer Anschlussstudie die Stichprobe erweitert werden, um mehr Protagonisten in die Untersuchung einbeziehen zu können."
2022,DHd2022,VARACHKINA_Hanna_Reflexive_Passagen_und_ihre_Attribution.xml,Reflexive Passagen und ihre Attribution,"Hanna Varachkina (Georg-August-Universität Göttingen, Deutschland); Florian Barth (Georg-August-Universität Göttingen, Deutschland); Luisa Gödeke (Georg-August-Universität Göttingen, Deutschland); Anna Mareike Hofmann (Georg-August-Universität Göttingen, Deutschland); Tillmann Dönicke (Georg-August-Universität Göttingen, Deutschland)","reflexive Passagen, Attribution, Korpus, automatische Erkennung","Modellierung, Annotieren, Literatur","Im Projekt MONA (Modes of Narration and Attribution) werden Phänomene in fiktionaler Literatur untersucht, die mit reflexiven Passagen assoziiert sind. Reflexive Passagen kommentieren die Handlung im Text oder den Schreibprozess oder generalisieren über die fiktive und‚Äâ/‚Äâoder reale Welt. Da das Konzept der reflexiven Passagen in der Literaturwissenschaft bisher nicht formalisiert wurde, werden diese nicht direkt annotiert. Stattdessen annotieren wir drei Phänomene, die wir für starke Indikatoren reflexiver Passagen halten: Kommentar (Bonheim 1975; Chatman 1980), nicht-fiktionale Rede (Konrad 2017; Searle 1975) und Generalisierung (Leslie et al. 2016; Dönicke et al. 2021). Darüber hinaus beschäftigt sich das Projekt mit der Zuschreibung reflexiver Passagen zu Sprechinstanzen. Für die Identifikation und Klassifikation dieser Phänomene werden Modelle entwickelt. Dafür wird ein annotiertes Korpus deutschsprachiger fiktionaler Texte erstellt, das die Entwicklung dieser Phänomene über 350 Jahre der Literaturgeschichte abbildet. Basierend auf den bisherigen Arbeiten haben wir Definitionen für die mit reflexiven Passagen assoziierten Phänomene formuliert bzw. weiterentwickelt. Unter Generalisierungen werden quantifizierte Aussagen über angenommene Instanzen einer Klasse oder Gruppe von Objekten, Individuen oder (Zeit-)Räumen verstanden, auf die nicht kontextuell referiert wird. Kommentare schließen Textstellen ein, in denen die erzählte Zeit unterbrochen und eine ergänzende Information zu Erzählung, Figuren, Handlung oder dem Akt des Erzählens eingefügt wird (Bonheim 1975). Nicht-fiktionale Rede bezeichnet Passagen in fiktionalen Texten, die Behauptungen bzw. Hypothesen über die reale Welt nahelegen (Konrad 2017). Generalisierung, Kommentar und nicht-fiktionale Rede können sich vollständig oder teilweise überlappen. In diesem Beispiel treten alle drei Phänomene auf. Die erzählte Zeit, die in der Erzählerrede fließt, wird unterbrochen und ein Kommentar über Naturwesen vorgenommen. Zugleich wird eine Aussage über angenommene Instanzen der Klasse der Naturwesen getroffen. Da auch in der realen Welt Naturwesen (jeglicher Art) vorkommen, ist die Proposition grundsätzlich auf die reale Welt übertragbar. Für reflexive Passagen erstellen wir eine Goldannotation, auf der in einem nächsten Schritt eine Attributionsannotation vorgenommen wird. Die Attribution bestimmt, wem die in der Passage enthaltene Information zugeschrieben werden kann, wofür grundsätzlich Figuren, die Erzählinstanz oder die AutorIn in Frage kommen. Einige sprachliche Mittel im Text sind prädestiniert für bestimmte Attributionen, so markieren bestimmte Satzzeichen i. d. R. (in)direkte Rede und damit die Sprecher im Text. Dennoch gibt es Passagen, in denen sich die Sprechinstanz nicht eindeutig identifizieren lässt und sich unterschiedliche Interpretationen (Zuschreibungen) aufdecken lassen. Zur automatischen Erkennung und quantitativen Analyse erstellen wir das Korpus MONACO (Modes of Narration and Attribution Corpus) Die Annotation der Texte wird in CATMA 6.2 Bisher wurden Goldstandards für achtzehn Texte erstellt. Der älteste Text stammt aus dem Jahr 1616, der jüngste aus dem Jahr 1930. Die annotierten Texte weisen im Durchschnitt ein moderates (> 0,4) oder gutes (> 0,6) Inter-Annotator Agreement mit Œ∫-Werten (Fleiss et al. 1981) von 0,59 für Generalisierung, 0,44 für Kommentar und 0,66 für nicht-fiktionale Rede auf. Die Inter-Annotator Agreement-Werte für Œ≥ (Mathet et al. 2015) sind etwas höher: 0,66 für Generalisierung, 0,52 für Kommentar und 0,72 für nicht-fiktionale Rede. Mit der zunehmenden Menge annotierter Texte werden schrittweise regelbasierte, statistische und neuronale Tagger für die einzelnen Phänomene entwickelt. Ihre Anwendbarkeit wird dabei auch für andere Textsorten wie Essays und enzyklopädische Texte erprobt. Letzten Endes soll eine ausreichend große Menge annotierter Daten nicht nur bessere Modelle ermöglichen, sondern auch diachrone oder genreübergreifende Perspektiven auf reflexive Passagen und ihre Attribution eröffnen."
2022,DHd2022,HINZMANN_Maria_Linked_Open_Data_für_die_Literaturgeschichtss.xml,"Linked Open Data für die Literaturgeschichtsschreibung Das Projekt ""Mining and Modeling Text""","Maria Hinzmann (Universität Trier, Germany); Christof Schöch (Universität Trier, Germany); Katharina Dietz (Universität Trier, Germany); Anne Klee (Universität Trier, Germany); Katharina Erler-Fridgen (Universität Trier, Germany); Julia Röttgermann (Universität Trier, Germany); Moritz Steffes (Universität Trier, Germany)","Linked Open Data, Informationsextraktion, Datenmodellierung, Literaturgeschichte","Datenerkennung, Modellierung, Identifizierung, Literatur, Metadaten","Im Umgang mit dem stetig wachsenden ""digitalen Kulturerbe"" bietet die Weiterentwicklung der systematischen Datenerschließung und Wissensrepräsentation bisher nicht ausgeschöpfte Potentiale für die Literaturgeschichtsschreibung. Vor diesem Hintergrund werden im Projekt Bezogen auf den Gegenstandsbereich ist ein Ausgangspunkt, dass die über rund zwei Jahrhunderte akkumulierten literaturhistorischen Forschungserkenntnisse größtenteils nicht unmittelbar nutzbar sind, da diese sehr umfangreich sind, nicht digital vorliegen oder auf unterschiedliche Orte und Quellen verteilt und in unterschiedlichen Sprachen publiziert sind. Für die Bereitstellung der Daten sowie die Arbeit und Infrastruktur im Projekt sind Open Science-Prinzipien tragend. Dies betrifft u.a. die Veröffentlichung FAIRer Daten (Röttgermann/Schöch 2020) im Open Access (Schöch 2021), die Nutzung von Open Source-Tools wie Für unsere Domäne ist die 1977 von Mylne, Martin und Frautschi veröffentlichte Im Teilprojekt zur Primärliteratur wird schrittweise ein Korpus von etwa 200 Romanen aufgebaut, wovon bereits reichlich 100 Texte in XML-TEI verfügbar sind (vgl. Röttgermann 2021; Klee/Röttgermann 2020). Das mittelfristige Ziel besteht darin, durch überwachtes Lernen die automatische Extraktion von Aussagen (RDF-Tripel) zu ermöglichen. Um Trainingsdaten zu generieren und Tripel in das Wissensnetzwerk einspeisen zu können, werden aktuell literaturgeschichtliche Texte in INCEpTION annotiert. Die Daten sollen als Statements über eine noch zu entwickelnde toolübergreifende Pipeline in unsere projektspezifische Wikibase importiert werden. Im Aufbau des LOD-Wissensnetzwerks werden literaturgeschichtliche Aussagetypen in einer systematischen Ontologie modelliert und die extrahierten Informationen als RDF-Tripel repräsentiert (vgl. Abb. 3). Der Mehrwert des Netzwerks wird durch exemplarische Nutzungsszenarien in Form von SPARQL-Abfragen konkretisiert. Frageoptionen wie ""Tritt Thema x in einem bestimmten Zeitraum y gehäuft auf?"" verdeutlichen, welcher Nutzen daraus für eine datenbasierte Literaturgeschichtsschreibung entstehen kann. Exemplarisch werden Einblicke in die Infrastruktur gegeben (vgl. Abb. 4). Ein Standardisierungsprozess wie er sich beispielsweise im Das Poster veranschaulicht die Integration der verschiedenen Informationsquellen in der Datenmodellierung sowie das Zusammenspiel der verschiedenen Teilprojekte und Tools im Aufbau und in möglichen Nutzungsszenarien des mehrsprachigen Wissensnetzwerks."
2022,DHd2022,BÖRNER_Ingo_Einführung_in_DraCor___Programmable_Corpora_für_.xml,Einführung in DraCor Programmable Corpora für die digitale Dramenanalyse,"Ingo Börner (Universität Potsdam); Frank Fischer (National Research University Higher School of Economics Moscow, DARIAH-EU); Carsten Milling (Universität Potsdam); Henny Sluyter-Gäthje (Universität Potsdam)","Digitale Literaturwissenschaft, Dramenanalyse, API, Linked Data","Community-Bildung, Netzwerkanalyse, Stilistische Analyse, Einführung, Infrastruktur, Literatur","In dem ganztägigen Workshop wird DraCor ( Der Workshop richtet sich an Personen, die Es erfolgt eine Vorstellung des Konzepts der ""Programmable Corpora"" sowie einer Demonstration der exemplarischen Umsetzung in der Plattform DraCor inklusive einer Vorstellung aller Komponenten. In Form von Hands-on-Tutorials wird den Teilnehmer*innen eine praktische Einführung in das Erstellen und Kuratieren eigener Dramenkorpora zur Analyse mit DraCor geben. Ein weiterer Teil führt anhand praktischer Beispiele zu den Methoden Stilometrie und Netzwerkanalyse in die Verwendung der DraCor-API sowie der Python-Bibliothek PyDraCor ein. Die API-Schnittstelle (Application Programming Interface) ermöglicht den maßgeschneiderten direkten Zugriff auf bestimmte Teile der Korpora. Die Möglichkeiten zu korpusübergreifenden Abfragen und Einbeziehung von Informationen aus der Linked-Open-Data-Cloud mit SPARQL werden ebenso erprobt. Den Kern von DraCor bilden Korpora von Dramen in elf Sprachen (Deutsch, Russisch, Französisch, Italienisch, Schwedisch, Spanisch, Altgriechisch, Elsässisch, Lateinisch, Baschkirisch und Tatarisch) sowie zwei weitere Autoren-Korpora (Shakespeare, Calderón), zu denen die Plattform eine Vielzahl an möglichen Forschungszugängen bietet: Die Dramen sind als XML-Dateien entsprechend der TEI-Guidelines kodiert und unter einer offenen Lizenz frei über GitHub unter Neben diesem ""klassischen"" modus operandi der korpusbasierten Forschung bietet DraCor als offenes digitales Ökosystem jedoch noch weitere Schnittstellen und angeschlossene Tools (Netzwerkvisualisierungen, Shiny App, Easy Linavis). Grundlegend hierfür ist die DraCor REST API ( Für die Programmiersprachen Python (PyDraCor: Korpusbasierte, in der Regel quantitative Methoden verwendende Analysen von Dramen haben sich in den vergangenen Jahren zu einem eigenen Subfeld der Computational Literary Studies (CLS) entwickelt (vgl. Willand et al. 2017; Reiter 2021). Dabei hat sich die Bereitstellung gemeinsam kuratierter und offener Ressourcen wie DraCor als produktiv auch für angrenzende Disziplinen wie die Computerlinguistik erwiesen (vgl. beispielsweise Pagel, Reiter 2020). Auf Wortebene operierende Verfahren haben sich dabei etwa auf die Autorschaftsattribution (Schöch 2014) oder Genreklassifikation mit Topic Modeling (Schöch 2017) fokussiert. Aktuell werden vielversprechende Neukonzeptualisierungen stilometrischer Maße wie das Kontrastmaß Zeta entwickelt und angewendet (Schöch 2018). Auf der Grundlage von strukturell ausgezeichneten Korpora lassen sich darüber hinaus gezielte Analysen etwa von Bühnenanweisungen durchführen, die mit POS-Informationen oder semantischen Feldern operieren (Trilcke et al. 2020). Im Bereich der strukturellen Analyse wurden Dramenkorpora früh schon, beginnend mit den Arbeiten von Stiller, Nettle, Dunbar (2003) und fortgesetzt etwa bei Moretti (2011), mit netzwerkanalytischen Ansätzen untersucht. Typologische Arbeiten beispielsweise zum Konzept der Small Worlds (Trilcke et al. 2016) stehen hier u.a. neben Ansätzen zur quantitativen Klassifizierung von Figurentypen (Fischer et al. 2018). Wenngleich semantische Technologien mittlerweile zum festen Bestandteil des Methodenspektrums der Digitalen Geisteswissenschaften zählen, gelangen sie in den korpusbasierten CLS bisher selten Anwendung (zu Prosa bspw. Frank und Ivanovic 2018; Dittrich 2017). Die Erfassung von Metadaten als Linked Data und die Anbindung an externe Referenzressourcen, insbesondere Wikidata, ermöglichen jedoch weitreichende Abfragemöglichkeiten und lassen sich zur Analyse von literarischen Korpora gewinnbringend nutzen. Beispielsweise sind in den DraCor-Korpusdaten keine detaillierten Informationen zu Autor*innen und Aufführungsorten enthalten. Da aber zu den einzelnen Stücken die eindeutigen Wikidata-Identifikatoren hinterlegt sind, können diese Informationen per federated queries in SPARQL abgerufen und in unterschiedlichen Visualisierungsformen, wie zum Beispiel als Karte, dargestellt werden. Im ersten Teil des Workshops wird zunächst das Konzept der ""Programmable Corpora"" eingeführt und diskutiert. Daran anschließend werden die Plattform DraCor und die einzelnen Komponenten vorgestellt, wobei auch immer wieder kürzere Übungsphasen vorgesehen sind, in denen die Teilnehmer*innen die vorgestellten Komponenten und Tools unmittelbar ausprobieren können. Insbesondere werden die unterschiedlichen Möglichkeiten zum Bezug und zur Analyse der Korpusdaten erprobt. Ein Fokus liegt dabei auf der Verwendung der API. Anhand der interaktiven Dokumentation werden die API-Funktionalitäten erläutert und können von den Teilnehmer*innen ausgiebig getestet werden. Im Anschluss daran wird ein kurzer Überblick zur Korpuserstellung und zu den Besonderheiten der TEI-Kodierung geben, wie sie in DraCor zum Einsatz kommen. Den zweiten Teil des Workshops bilden Gruppenarbeitsphasen, in denen drei Themenbereiche vertieft werden können: (1) Korpuserstellung und -kuratierung mit DraCor: Die Teilnehmenden vertiefen die TEI-Kodierung von Dramen anhand von praktischen Übungen und lernen, wie eine lokale Instanz der Plattform mittels Docker aufgesetzt, gegebenenfalls angepasst und mit eigenen Korpora bestückt werden kann. (2) Dramenanalyse mit DraCor-API und Python: Mittels Jupyter Notebooks mit ausführlich dokumentiertem Python-Programmcode werden die Teilnehmer*innen an Methoden der digitalen Dramenanalyse unter Verwendung der DraCor-API herangeführt. Die Notebooks sollen es auch Teilnehmer*innen, die bisher noch keine Erfahrungen im Programmieren mit Python gemacht haben, im Sinne eines Literate-Programming-Ansatzes ermöglichen, die einzelnen Analyseschritte nachzuvollziehen und auch selbst adaptieren zu können. Die Notebooks setzen konkrete Forschungsfragen zur Dramenanalyse um, etwa zur literaturhistorischen Entwicklung netzwerkanalytischer Maße oder zur quantitativen Dominanz von Figuren. (3) Dramenanalyse mit Linked Data: Den Schwerpunkt bilden praktische Analysen, die aus der Anbindung von DraCor an die Linked Open Data Cloud möglich werden. Im Workshop wird ein kurzer Crashkurs in die Abfragesprache SPARQL gegeben, um dann im Anschluss gemeinsame Abfragen von DraCor und Wikidata vorzunehmen und die Ergebnisse zu visualisieren. Die Ergebnisse der Arbeitsgruppen werden anschließend im Plenum präsentiert und diskutiert. Anzahl der möglichen Teilnehmer*innen: 25 Teilnehmer*innen benötigen einen eigenen Laptop mit Internetzugang; Hinweise zu vorab zu installierender Software (Oxygen XML-Editor, Docker, ...) werden im Vorfeld bekanntgegeben. Die Materialien werden auf GitHub bereitgestellt; die Jupyter Notebooks werden unter ( Weitere benötigte technische Ausstattung am Veranstaltungsort: Beamer, WLAN   DraCor wird gegenwärtig im Rahmen des von der EU Horizon 2020 geförderten Projekts ""CLSInfra"" (Fördernummer: 101004984,"
2022,DHd2022,ARNOLD_Frederik_Lesen__was_wirklich_wichtig_ist__Die_Identif.xml,"Lesen, was wirklich wichtig ist Die Identifikation von Schlüsselstellen durch ein neues Instrument zur Zitatanalyse","Frederik Arnold (Humboldt-Universität zu Berlin, Germany); Benjamin Fiechter (Humboldt-Universität zu Berlin, Germany)","Schlüsselstelle, Praxeologie, Literaturanalyse, Zitaterkennung, Text Reuse","Entdeckung, Strukturanalyse, Modellierung, Visualisierung, Werkzeuge, Visualisierung","Literaturinterpretationen heben in der Regel einige wenige Bestandteile des analysierten Primärtextes hervor, die für die jeweilige These und die damit verbundene Interpretation besonders wichtig erscheinen, und interpretieren sie mehr oder weniger ausführlich. Diese Passagen fassen wir als Mithilfe des unten beschriebenen Konzepts, seiner Umsetzung als Algorithmus ( Im Folgenden gehen wir zunächst kurz auf verwandte Arbeiten ein, erläutern dann den Kontext von Konzept und Instrument, ehe wir den Algorithmus und die Webseite zur Exploration vorstellen; dabei wird der Fokus weniger auf technische Details des Algorithmus als auf dessen praktische Anwendung gelegt. Daran schließen sich Überlegungen zur Konzeption und eine Verortung in den Digital Humanities sowie ein Ausblick auf mögliche Erweiterungen des Instruments an. Die hier vorgestellte Herangehensweise ist ein zentrales Instrument, um im Rahmen des Projekts Exemplarisch haben wir die Untersuchung mit zwei kanonischen Texten der deutschen Literatur begonnen, zu denen zahlreiche Interpretationen vorliegen: Lotte ist ein in Python implementierter Algorithmus zur Erkennung von Zitaten. Gegeben einen Quelltext und einen Zieltext, findet er alle Instanzen ab einer Länge von fünf Wörtern, in denen der Zieltext (in unserem Fall eine Literaturinterpretation) einen Teil des Quelltextes, also des literarischen Textes, enthält. Die Idee der Verwendung von (interaktiven) Visualisierungen in den Digital Humanities zur Unterstützung der Arbeit mit Texten aller Art ist nicht neu. Es gibt eine große Anzahl von Ansätzen zur Visualisierung von Strukturen, Häufigkeiten, Mustern etc. 'sowohl zur Unterstützung beim Distant Reading als auch beim Close Reading bzw. zur Kombination beider Perspektiven, wie auch wir sie in dieser Arbeit vorstellen. Für einen ausführlichen Überblick sei verwiesen auf Jänicke et al. (2015a). Auch für die Visualisierung von wiederverwendetem Text, wie beispielsweise Zitaten, gibt es verschiedene Ansätze (Vgl. Jänicke et al. 2015b). Im Unterschied zu Annette liegt der Fokus dieser Ansätze auf der Visualisierung der Art und Häufigkeit des wiederverwendeten Texts oder der Alignierung von verschiedenen Varianten des gleichen Texts, zum Beispiel verschiedener Übersetzungen. Eine Webseite zur Visualisierung von (wörtlichen) Zitaten aus Shakespeares Werken wurde von Miller vorgestellt. Für Annette werden die von Lotte gefundenen Übereinstimmungen weiterverarbeitet, um Schlüsselstellen zu identifizieren. Hierfür kombinieren wir überlappende Übereinstimmungen zu einer Ein Screenshot der Webseite Rechts neben der Heatmap ist der literarische Text selbst dargestellt. Die Grauskala wird dadurch bestimmt, wie viele Interpretationstexte einen Teil einer Stelle oder die Stelle insgesamt zitieren. Die Farbe ist dabei für eine gesamte Stelle immer die gleiche. Die Schriftgröße wird dadurch bestimmt, wie oft ein minimales Segment zitiert wird, somit kann sie auch innerhalb einer Stelle variabel sein. Rechts unten neben dem literarischen Text wird eine Liste aller Interpretationstexte gezeigt. Ganz rechts werden die zehn häufigsten Passagen aufgelistet. Darüber wird bei einer entsprechenden Auswahl, wie unten beschrieben, der gesamte Text einer Interpretation angezeigt. Ausgehend vom Startbildschirm kann der*die Benutzer*in zwischen verschiedenen Zugriffsmöglichkeiten wählen. Die erste Möglichkeit ist die Auswahl einer Stelle, indem diese im Primärtext angeklickt wird. Anstelle der Gesamtliste werden nun rechts davon nur noch diejenigen Interpretationstexte angezeigt, die zu der ausgewählten Stelle beitragen, zusammen mit einer kurzen Vorschau des Textes. Durch Anklicken eines der Interpretationstexte können wir eine bestimmte Interpretation anzeigen lassen, deren Text oben rechts dargestellt wird. Wir können dann diesen Text durchgehen und andere zitierte Passagen auswählen. Rechts unten wird angezeigt, wie oft die ausgewählte Stelle zitiert wird und von wie vielen Interpretationstexten. Darunter finden wir die zehn meistzitierten Segmente dieser Stelle. Als weitere Möglichkeit kann über die Liste der zehn am häufigsten zitierten Stellen auf die möglicherweise relevantesten, jedenfalls am breitesten rezipierten Stellen zugegriffen werden. Nach Auswahl einer dieser Stellen passt sich wie oben beschrieben die Liste der Interpretationstexte an und es kann von hier aus weiter diese Liste oder ein einzelner Interpretationstext untersucht werden. Durch die beschriebene Funktionsweise werden im Primärtext Stellen konstituiert, die teilweise sehr häufig, teilweise nur wenige Male zitiert werden. Im Fokus unserer Projektarbeit stehen die besonders häufig zitierten Stellen, da sie am ehesten die Schlüsselqualität einer Stelle anzeigen. Dabei darf die reine Quantität selbstverständlich nicht absolut gesetzt werden; um ein Korpus im Querschnitt zu überblicken, scheint uns die Häufigkeit von zitierten Stellen aber ein wichtiger Anhaltspunkt zu sein. Das Konzept, nach dem diese Stellen konstituiert werden, ist dabei die Pointe an unserem Ansatz, da es weit über das bloße Auffinden von Zitaten hinausgeht. Die auf der Webseite visualisierten Stellen ergeben sich nämlich häufig erst durch die Überlappung verschieden langer Segmente bzw. Zusammenfassung verschieden langer Zitate aus unterschiedlichen Interpretationstexten (s.¬†o.). Somit sind Lotte und Annette im Zusammenspiel umso wirkungsvoller, je mehr Texte das Korpus umfasst. Umgekehrt können aber auch jene Stellen von besonderem Interesse sein, die überhaupt nicht zitiert werden. Bei einer überschaubaren Textlänge, wie sie Diese Funktionsweise eröffnet auch den Horizont für ein integratives Scalable Reading, das wir im Anschluss an Thomas Weitin als Überwindung der ""Frontstellung von Ein erstes Ziel ist die Einführung weiterer Visualisierungen, die einen umfangreicheren Zugang zu den Texten bieten sollen. Im Folgenden wollen wir eine davon genauer vorstellen, die sich mit der Entwicklung der zitierten Stellen über die Zeit befasst, sowie abschließend ein paar weitere kurz skizzieren. Darüber hinaus wollen wir in Zukunft Zitate, die kürzer als fünf Wörter sind, berücksichtigen. Hierfür muss in erster Linie Lotte angepasst werden. In Annette könnten diese dann so visualisiert werden, dass beispielsweise untersucht werden kann, ob kurze Zitate neue Informationen bringen und ob diese ähnlich zu langen Zitaten verteilt sind oder ob sich hier Unterschiede zeigen. Das Ziel dieser Visualisierung ist die Darstellung des Anteils neuer Zitate eines Interpretationstexts im Verhältnis zu allen Zitaten früherer Interpretationstexte. Die Abbildungen¬†2 und 3 zeigen diese Visualisierung für Bei genauerer Betrachtung von Abbildung¬†2 lässt sich erkennen, dass die drei Texte mit den wenigsten neuen Zitaten auch insgesamt nur sehr wenig zitieren. Gleichzeitig gibt es auch Texte, die verhältnismäßig wenig zitieren und dennoch einen hohen Anteil an neuen Zitaten aufweisen. Abbildung¬†3 zeigt analog die Auswertung für Diese Diagramme sollen in interaktiver Form auf der Webseite verfügbar gemacht werden. Es wird dann zum Beispiel möglich sein, einen Interpretationstext auszuwählen und sich anzeigen zu lassen, welche zitierten Stellen neu sind. Diese können dann auch mit allen früheren und zukünftigen Zitaten verglichen werden. Für die Zukunft sind noch weitere Visualisierungen angedacht. So soll zum Beispiel für alle Interpretationstexte ermittelt und visualisiert werden, ob diese in der Reihenfolge der Zitate dem literarischen Werk folgen. Weiterhin soll die Länge von Zitaten analysiert werden, was dann beispielsweise in die Bestimmung von Schlüsselstellen einfließen kann. Außerdem wollen wir untersuchen, welcher Anteil eines Werks mengenmäßig zitiert wird und wie sich die Zitate über den Interpretationstext verteilen. Im Fokus unseres Beitrags stand ein neues Konzept und dessen Umsetzung zur Identifikation von Schlüsselstellen in literarischen Texten. Neben der praktischen Realisierung als Algorithmus (Lotte) und der Visualisierung zur Erkundung von literarischen Texten (Annette) lag der Schwerpunkt auf den Möglichkeiten und zukünftigen Visionen für Annette. Darüber hinaus haben wir die Überlegungen zur Schlüsselstelle und zur Konstituierung von Stellen vorgestellt und als Beitrag zur Realisierung einer Scalable Reading-Methodik kontextualisiert, die auf Strukturidentifikation abzielt. Die Arbeit wurde durch das DFG-Schwerpunktprogramm (SPP) 2207"
2022,DHd2022,BRANDL_Stephanie_Textexplorationen_in_der_digitalen_Literatu.xml,Textexplorationen in der digitalen Literaturwissenschaft Eine kritische und angewandte Auseinandersetzung mit Repräsentations- und Interpretationsansätzen von Text,"Stephanie Brandl (K√∏benhavns Universitet, Technische Universität Berlin); David Lassner (Technische Universität Berlin); Cora Krömer (Universitätsbibliothek Erlangen-Nürnberg); Anne Baillot (Le Mans Université)","Textrepräsentation, Lesen, Visualisierung, Maschinelles Lernen","Entdeckung, Programmierung, Inhaltsanalyse, Strukturanalyse, Kontextsetzung, Bereinigung","Maximalanzahl Teilnehmende: 25 Räumliche Anforderungen: Idealerweise bringen Teilnehmende ihre eigenen Laptops mit, die bestenfalls schon die nötige Software vorinstalliert haben. Wir werden kurz vor der Konferenz eine Willkommens-E-Mail mit einer Liste der relevanten Software verschicken. Die praktischen Sitzungen werden mithilfe von Jupyter Notebooks (Python3, Jupyter) abgehalten. Wir planen zusätzlich als Absicherung einen Online-Zugang zu einem JupyterHub Server mit vorinstallierten Paketen für Teilnehmende, bei denen die Installation Schwierigkeiten macht. Wir ermutigen die Teilnehmenden ausdrücklich einen eigenen Datensatz mitzubringen, an Beispiel dessen die Aufgaben ausgeführt werden können. Wir gehen davon aus, dass dies aufschlussreich für die jeweiligen Teilnehmenden ist und zusätzlich den Workshop bereichert. Wir haben außerdem bereits mit zwei Dateninstitutionen Kontakt aufgenommen. Beide haben Interesse bekundet sowohl Daten für den Workshop zur Verfügung zu stellen als auch am Workshop teilzunehmen. Damit stellen wir sicher, dass auch für Teilnehmende, die keine Daten mitbringen, Material vorhanden ist, und im Zweifel auch Expert:innen vor Ort sind, die etwaige Zwischenergebnisse der verwendeten Methoden evaluieren und in Kontext setzen können. Traditionelle Methoden und Theorien der Literaturwissenschaft können in vier Typen unterteilt werden: autor-, text-, leser- und kontextorientiert [Köppe & Winko, 2013]. Die Literaturwissenschaft zeichnet sich durch einen Methodenpluralismus aus [Nünning & Nünning 2020], der in den letzten Jahren vor allem durch kognitionswissenschaftliche [Zunshine, 2015] und empirische Ansätze [Kuiken & Jacobs, 2020] weiter ausdifferenziert wurde. Gerade diese haben zu neuen Erkenntnissen in leserorientierten Ansätzen geführt. Umgekehrt lässt sich auch vermuten, dass neue Lektüreweisen, –in unserem Fall maschinengenerierte Lektüren'literaturwissenschaftliche Innovationen direkt oder mittelbar als Folgewirkung hervorbringen [Parr & Honold, 2018]. Nicht nur die Literaturwissenschaft an sich, sondern auch ihre leserorientierten Ansätze zeichnen sich durch Theorien- und Methodenpluralismus aus [Rautenberg & Schneider, 2015], z. B. rezeptionstheoretische Ansätze [Willand, 2014]. Die Verbreitung von digitalen Lesemedien führt zu einer neuen Beschäftigung mit Repräsentationsformen von Texten und Textorganisation [Saemmer, 2015], da der Sinn eines Textes nicht davon unabhängig gedacht werden kann. Dies führt zu neuen und zu erlernenden Lesestrategien, die es zu erforschen gilt. Auf der anderen Seite prägen visuelle Repräsentationen von Literatur schon lange ihre Institutionalisierung: Literarische Bewegungen werden in Schulbüchern in Form von Zeitleisten dargestellt, dramatische Handlungen mit der Freytagschen Pyramide abgebildet, und dies schon seit dem 19. Jahrhundert. Bereits diese Ansätze setzen sich zum Ziel, übergeordnete Strukturen anschaulich zu machen, die sich aus dem literarischen Text selbst ableiten lassen. Mit der bourdieuschen Literatursoziologie etablierte sich in den 60er Jahren die Vorstellung der Literatur als ein Feld, das von diversen Dynamiken durchzogen wird. Die digitalen Methoden, spätestens seit Morettis Word Embeddings bilden häufig die Brücke zwischen dem Text als Zeichenfolge und der Darstellung, mit der digitale Literaturwissenschaftler:innen ihr Korpus lesen, also bspw. als Input für ein ML-Modell. In Eine weitere große Herausforderungen bei der Verwendung von Word Embeddings ist die zuverlässige Evaluation ihrer Qualität, das heißt, wie sehr sie mit der gewünschten Bedeutung übereinstimmen. Es gibt zwar eine Menge von Evaluationsmethoden, wie Analogy Tests oder die Applikation von Downstream Tasks, häufig klopfen diese allerdings nur Teile der zugrundeliegenden Repräsentation ab und geben kein umfassendes Bild über ihre Qualität. Ein in diesem Zusammenhang viel diskutierter Aspekt von Word Embeddings sind 'Biases', der zugrundeliegenden Daten, die durch die Modellarchitekturen reproduziert, oder sogar verstärkt werden können. Dies ist insbesondere bei Systemen ein Problem, die auf so großen und heterogenen Datenmengen trainiert wurden, dass es sich im Nachhinein schwer dokumentieren lässt, welche Biases dem jeweiligen Modell innewohnen (Bender et al., 2021). In Bezug auf die häufig kleineren, gut mit Metadaten versehenen Korpora in den DH, steckt in diesen reproduzierten Biases aber tatsächlich ein interessantes Forschungsfeld (Gebru et al., 2018). So lässt sich beispielsweise sehen, in welchen Zeiträumen welche Biases besonders vorherrschend waren, bzw. wann diese möglicherweise wieder verschwinden. Hierfür könnten mehrere Word Embeddings für verschiedene Zeiträume trainiert, und dann miteinander verglichen werden. Diese Art von Vergleich ist aber nicht nur auf Biases beschränkt, man kann sie ebenfalls als eine Form des 'Realitätschecks' durchführen, wenn man Dynamiken zwischen Word Embeddings betrachtet: Bei einem Korpus, das einen größeren Zeitraum umfasst, verändern sich darin allgemein die Repräsentationen der Worte so, wie man es von der Bedeutung der Worte auch erwarten würde? In dem Workshop möchten wir uns der beschriebenen Thematik von zwei Seiten nähern, wir geben (1) einen Überblick über traditionelle Theorien und Methoden der Literaturwissenschaft, insbesondere auch der Leseforschung und kontextualisieren letztere¬† mit Erkenntnissen aus neurowissenschaftlichen Laborstudien. Wir geben (2) eine Zusammenfassung der aktuellen Methoden zu Textrepräsentation [z.B. Glove, Bert] und Beispiele für DH-Projekte, in denen diese bereits verwendet werden, insbesondere fokussieren wir uns hier auch auf gängige Visualisierungsmethoden und die damit zusammenhängenden Limitationen. Beides wollen wir durch praktische Übungen an eigenen oder bereitgestellten Datensätzen näherbringen. Ziel ist es, dass im Laufe des Workshops Verknüpfungen und Schnittmengen der unterschiedlichen Lesebegriffe aus den verschiedenen Fachrichtungen hergestellt und gefunden werden. Zum Abschluss bieten wir konkrete Möglichkeiten an, welche Visualisierungen auf welche Weise ihren Platz als literaturwissenschaftliche Methode finden können. Der Ablauf des Workshops gliedert sich in Vorträge sowie praktische Sitzungen, bei denen das im Vortrag zuvor besprochene direkt von den Teilnehmenden ausprobiert werden soll. Im folgenden soll ein kurzer Überblick über die einzelnen Vortragsteile gegeben werden. (Cora Krömer) (David Lassner)  (Stephanie Brandl) In welcher Form und Geschwindigkeit Menschen Texte lesen wird bereits seit Jahrzehnten untersucht [Rayner, 1998] und modelliert [Reichle et al., 2003]. Oberflächliche Eigenschaften wie Worthäufigkeiten oder Wortlängen beeinflussen unsere Lesegewohnheiten. Neuere Forschung zeigt zudem, dass auch die Darstellung des Textes eine Rolle spielt, ob ein Gedicht beispielsweise in der ursprünglichen Form oder als Prosatext gezeigt wird [Fechino et al., 2020]. Dementsprechend ist es auch wichtig, Word Embeddings so darzustellen, dass der sehr dichte Informationsgehalt verarbeitet und evaluiert werden kann. (Stephanie Brandl) Word Embeddings sind stark abhängig vom zu Grunde liegenden Datensatz, beispielsweise lernen statische Methoden wie GloVe genau eine Repräsentation pro Wort, so dass zeitliche oder andere Entwicklungen im Datensatz nicht beachtet werden und in einem Vektor verschmelzen. Diese Abhängigkeit führt auch dazu, dass Verzerrungen (Biases) im Datensatz möglicherweise in den Wort-Vektoren auftauchen. Wenn beispielsweise alle Personen des medizinischen Personals im Datensatz weiblich sind, wird ein Algorithmus einen männlichen Bewerber möglicherweise für eher ungeeignet halten, um als Arzt zu arbeiten. Verschiedene Methoden wurden bereits veröffentlicht, um solche problematischen Strukturen (zeitliche Abhängigkeit, Biases uvm) in Word Embeddings aufzuzeigen [Basta et al., 2019; Brandl & Lassner, 2019; Gonen et al., 2020] und auch aufzulösen [Gonen et al., 2019; Manzini et al., 2019]. Wir werden einige davon besprechen und im Anschluss auch an den gelernten Word Embeddings anwenden. (Anne Baillot) Zum Schluss wird auf die Entwicklung literaturwissenschaftlichen Umgangs mit Analyse bzw. Interpretation von Text eingegangen: zuerst auf das Faszinosum Netzwerk, dann auf die Herausforderung der Definition von Beziehungen zwischen Textelementen und ihrer Bedeutung, schließlich auf veröffentlichungstechnische Fragen, die mit der Einbettung dieser graphischen Repräsentationen einhergehen."
2022,DHd2022,HERRMANN_J__Berenike_Lieblingsgegenden__Fenster_und_Mauern__.xml,"Lieblingsgegenden, Fenster und Mauern Zur emotionalen Enkodierung von Raum in Deutschschweizer Prosa zwischen 1850 und 1930","Berenike Herrmann (Universität Bielefeld, Germany); Giulia Grisot (Universität Bielefeld, Germany)","Schweizer Literatur, Raum, Sentiment","Inhaltsanalyse, Räumliche Analyse, Modellierung, Annotieren, Artefakte, Text","Raum ist eine wichtige Dimension von ""Kultur"", nicht zuletzt in literarischen Artefakten. Definiert als ""area, as perceived by people, whose character is the result of the action and interaction of natural and/or human factors"" (Council of Europe, 2000, S. 2) impliziert besonders die Landschaft, das ""Gebiet, wie wahrgenommen"", einen oftmals vergleichenden En- und Dekodierungsakt. Die Räume der deutschschweizer Literatur sind wie bei Spitteler offenbar regelmäßig solche ""Lieblingsgegenden"", die dann doch Untiefen offenbaren, wie die Dörfer Gotthelfs, die Kleinstädte Kellers ( Spittelers scheinbar zahme Alpen, Spyris urbanes Gefängnis und schließlich Hölderlins erhabener ""furchtbarherrlicher Haken"" des Hochgebirgsaufstiegs ( Unser Beitrag möchte die beiden beschriebenen Ebenen von Kultur mit der des ""digitalen Gedächtnisses"" zusammenbringen, indem wir computationelle Verfahren auf literarische Texte (als ""Schweizer digitales Kulturerbe"") anwenden, um die affektive Enkodierung dargestellter Raumtypen (als ""Reflektion der Reflektion"") zu untersuchen. Ausgehend vom übergreifenden Forschungsinteresse einer Komparatistik der deutschsprachigen Länder möchte unser Beitrag erste Ergebnisse berichten über die emotionale Enkodierung von fiktionalem Raum. Anhand des DCHLi (Deutschschweizer Literaturkorpus), zurzeit als Pilotkorpus mit 76 Texten, und ausgehend von einem semiotischen Zugang zu textuell enkodierten Emotionen (z.B. Schiewer, 2007; vgl. Anz, 2007; Winko, 2022) und Raumanalyse (Balshaw, & Kennedy, 2000; Bologna, 2020) legen wir die in gängigen Sentiment-Diktionären vorgehaltenen Affekt-Kategorien zwischen dimensionalen (Valenz, Arousal) und diskreten Emotionen (""Angst"", ""Freude"", ""Wut"", ""Trauer"", ""Ekel"") an. Wir fragen:  Unsere quantitativen Befunde sollen Bezüge herstellen zu ikonischen Kultur/Natur-Dichotomien im Erbe der Romantik, zu historischen Stadt/Land-Konstellationen, aber auch zu einem nationalliterarischen Rahmen mit vielbeklagtem Schweizer ""Mythos"" (Böhler, 2010) einerseits und identifikatorischen (oftmals Alpen-orientierten) Angeboten (Zimmer, 1998) für die ""imagined community"" (Anderson) der sogenannten Willensnation andererseits. Unser DCHLi Pilotkorpus umfasst derzeit 76 fiktionale Prosatexte von AutorInnen, die der deutschschweizer ""Nationalliteratur"" zugeordnet werden und die zwischen 1854 und 1930 zuerst publiziert wurden (N= 2,025,529 Wörter). DCHLi enthält das wachsende Deutschschweizer ELTeC-gsw (Grisot & Herrmann, 2021) das wiederum Teil der European Literary Text Collection (ELTeC, Odebrecht et al., 2021) ist. Ausgehend vom derzeit Im ersten Schritt erstellten wir ein möglichst umfassendes und feingranuliertes Diktionär ""räumlicher Entitäten"", das auf höchster Taxonomie-Ebene die Kategorien RURAL und URBAN zusammenfasst, die sich wiederum in fünf Subkategorien ""natural entity"", ""rural entity,"" und ""geographic entity"", sowie ""urban entity"" und ""geopolitical entity"" auffächern (vgl. Wartmann et al. 2018, p. 1580; siehe Abb. 1). Hier wurden unter Rückgriff auf Ressourcen wie Openthesaurus und das Schweizer Idiotikon historisch wie sprachlich relevante Elemente berücksichtigt (i.e. Im zweiten Schritt erstellten wir für einen systematischen Vergleich ein Repositorium mit acht der frei verfügbaren Sentiment-Diktionäre (ADU, BAWL, Germanlex, LANG, Klinger, Plutchik, SentiWS, SentiArt, siehe Tabelle 2). Deren unterschiedlichen Formate wurden für die automatische Sentiment-Annotation in einer processing pipeline vereinheitlicht. Abbildung 2 zeigt die lexikalische Abdeckung der acht Diktionäre auf dem DCHLi, die angibt, wie viele der Wörter von der jeweiligen Ressource erkannt wurden (geordnet in abfallender Reihenfolge). Im dritten Schritt errechneten wir das semiotische Emotionspotenzial innerhalb der räumlichen Seedword-Spannen. Abbildung 3 zeigt den Abbildungsprozess der räumlichen Entitäten auf das Korpus beispielhaft für das BAWL-R-Diktionär: Sobald eine Entität identifiziert ist, werden innerhalb einer Gesamtspanne von 101 Wörtern je 50 Wörter vor und nach der Entität für die Berechnung von Emotions- und Sentimentwerten einbezogen (ohne Stoppwörter). Mittels dieses Verfahrens kann die Repräsentation von Emotionen (Valenz, Diskrete Emotionen) und ihre Ausprägung (Arousal) bezüglich des fiktionalen Raums näherungsweise untersucht werden, wobei uns zunächst die potenzielle Differenz in der Emotionsrepräsentation zwischen ländlichen und städtischen Räumen interessierte. Wir verwendeten R (Version R 4.1.0, R Core Team, 2021) um mittels mixed linear models den Effekt des Entitätstyps (rural, urban) auf die jeweiligen Sentiment-Werte zu beobachten, mit AutorIn und Titel als randomisierte Faktoren. Verwendete Pakte waren v.a. tidyverse (Wickham et al., 2019); LmerTest (Kuznetsova et al., 2017), und tm (Feinerer, 2020). Wir konnten statistisch signifikante Effekte des Entitätstyps u.a. auf die Valenz/Polarität in verschiedenen Diktionären beobachten, wobei LANG und BAWL ""positives Sentiment"" häufiger für ""rural"" Passagen aufwiesen, Germanlex jedoch den entgegengesetzten Befund (Tabelle 3). Für die diskreten Emotionen berücksichtigte das mixed model jede einzelne Emotion als zusätzlichen ""fixed factor"" (Tabelle 4). Abweichungen zwischen den Diktionären konnten wieder beobachtet werden, wobei SentiArt signifikante Differenzen für fünf Basisemotionen (ausser Obwohl die Sentimentdetektion, das räumliche Matching der Emotionen und die Korpusgröße weiter verbessert werden sollen, interpretieren wir die vorliegenden Daten vorsichtig dahingehend, dass Textpassagen mit ländlichen und Natur-Referenzen in unserem Korpus häufiger positiv enkodiert sind. Es scheint, dass diese ""ruralen"" und ""Natur-"" Räume im Vergleich insgesamt mehr unterschiedliche und möglicherweise reichhaltigere Emotionen repräsentieren. Angesichts der Zusammensetzung des vorliegenden Korpus kann dies nicht nur auf eine topische Assoziation von positiv enkodierter Natur vs. negativ enkodierter Stadt/industrialisierter Zivilisation bezogen werden, sondern scheint auch Landschaft und Natur als vornehmlichen Schauplatz der Diegese abzubilden. Schlägt man den Bogen weiter, und projiziert noch hypothetisch auf die Grundgesamtheit der Deutschschweizer Prosa (Herrmann et al., 2021), könnte der Vorschlag, dass Deutschschweizer Literatur in dieser Zeit vornehmlich auf dem Lande und in der Natur stattfindet, im Luhmannschen Sinne als ""kultureller"" Differenzvorschlag verstanden werden: ein Identifikationsangebot, das ""Schweiz"" ebendort, und nicht anderswo, verortet. Wohlgemerkt wäre gerade unter solchen Bedingungen die evidente Rolle von Technik, Infrastruktur, Handel und Industrialisierung mitzumodellieren. Wir schließen mit einer unabdingbaren methodologischen Notiz. In der vorliegenden Studie war es unsere Absicht, diktionärbasierte Sentimentanalyse als im Feld der DH gegenwärtig noch kanonischen Ressourcentyp in Anschlag zu bringen (Kim & Klinger, 2019). Die niedrige lexikalische Abdeckung für die meisten Diktionäre, die im Umlauf sind (Abb. 2), zeigt auf, dass hier neue Ressourcen und ein erweitertes Methodenbewusstsein nötig sind. Untersucht man die Reliabilität und Domänenspezifik der einzelnen Diktionäre genauer, wie wir es getan haben, wird schnell deutlich, dass es sich für die DH lohnt, den Anschluss an den "
2022,DHd2022,SCHMIDT_Thomas_Evaluation_computergestützter_Verfahren_der_E.xml,Evaluation computergestützter Verfahren der Emotionsklassifikation für deutschsprachige Dramen um 1800,"Thomas Schmidt (Lehrstuhl für Medieninformatik, Universität Regensburg); Katrin Dennerlein (Institut für Deutsche Philologie, JMU Würzburg); Christian Wolff (Lehrstuhl für Medieninformatik, Universität Regensburg)","Emotionsklassifikation, Transformer-basierte Modelle, Drama, Sentiment Analysis, Maschinelles Lernen","Datenerkennung, Programmierung, Inhaltsanalyse, Sprache, Literatur, Text","Transformerbasierte Sprachmodelle wie BERT (Devlin et al. 2018) und ELECTRA (Clark et al. 2020) gelten als state-of-the-art und Ausgangspunkt für zahlreiche Aufgaben des Natural Language Processing (NLP) (Shmueli / Ku 2019; Munikar et al. 2019; Cao et al. 2020; Dang et al. 2020; Gonz√°les-Carvajal et al. 2021; Cortiz 2021). Als ein entscheidender Vorteil dieser Modelle hat sich die dynamische Repräsentation von Tokens in Abhängigkeit von ihrem Kontext herausgestellt. Der Großteil dieser Modelle wird jedoch mit zeitgenössischer Sprache, vor allem mit Sach- und Fachtexten aus dem Web (z.B. In den Digital Humanities (DH) werden Sentiment-Analyse (die Einteilung, ob ein Text eher positiv/negativ konnotiert ist) und Emotionsklassifikation (die Erkennung bzw. Zuordnung distinkter Emotionskonzepte in Texten) in den letzten Jahren immer populärer. Sie werden verwendet, um moderne Textsorten wie Songtexte (Schmidt et al. 2020a), Filmtexte (Schmidt et al. 2020b) und Texte aus den sozialen Medien zu analysieren (Moßburger et al. 2020; Schmidt et al. 2020c; 2020d) finden aber auch Einsatz für literarische Genres wie beispielsweise Märchen (Alm / Sproat 2005; Mohammad 2011), Romane (Kakkonen / Kakkonen 2011; Mohammad et al. 2011; Reagan et al. 2016; Zehe et al. 2016) oder Dramen (Mohammad 2011; Schmidt / Burghardt 2018; Schmidt et al. 2018a; 2018b; Schmidt 2019; Schmidt et al. 2019a; 2019b; 2019c; Yavuz 2020; Schmidt et al. 2021). Die Ziele variieren dabei von der Exploration von Sentiment- und Emotionsverläufen in einzelnen Werken bis zu Gruppenvergleichen (siehe Kim / Klinger 2019). Die steigende Popularität ist wenig überraschend, da die hermeneutische Analyse von Emotionen eine lange Tradition in der Literaturwissenschaft hat, z.B. in der Dramenanalyse (Pikulik 1965; Wiegmann 1987; Anz 2011; Schonlau 2017). Im folgenden Proposal präsentieren wir eine Studie aus dem DFG-Projekt Zur Evaluation und zum Training von Algorithmen wurde ein Goldstandard für ein Sub-Korpus unseres Gesamtkorpus"" annotiert. Emotion wird definiert als der Bewusstseinszustand einer Figur, wie sie sich auch in Text ausdrückt. Annotiert wird die eigene oder zugeschriebene Emotion von Figuren in Abhängigkeit von Kontext und Interpretation. Das Schema hebt sich von üblichen Schemata, die meist von der Psychologie inspiriert sind (Wood et al. 2018a; 2018b) ab, um literarische Interessen zu integrieren. Es besteht aus 13 Ein Sonderfall des Schemas ist Das zu analysierende Hauptkorpus unseres Gesamtprojektes setzt sich aus unterschiedlichen Dramenkollektionen für die Jahre 1650-1815 aus TextGrid Für die Annotation wurde das Tool CATMA (Gius et al. 2020) verwendet. Die Dramen wurden vollständig von Anfang bis Ende annotiert. Die Lektüre des gesamten Dramas ist notwendig, da kontextabhängig annotiert wird. Je zwei studentische Hilfskräfte haben jedes Werk unabhängig voneinander annotiert. Die Hilfskräfte wurden vor der Annotation mittels Pilotstudien von einer Expertenannotatorin trainiert und hatten Zugriff auf eine Annotationsanleitung. Je nach Länge des Textes hatten die Annotator*innen 1-2 Wochen Zeit pro Drama. Der Goldstandard besteht insgesamt aus 6.596 Emotionsannotationen (Abbildung 1). Auf Polaritätsebene sind die meisten Annotationen negativ (56%), 34% positiv und 11% mit der Klasse ""emotionale Bewegtheit"" markiert. Einige Kategorien (z.B. Lust und Freundschaft) wurden selten markiert. Die Token-Statistiken verdeutlichen die Varianz in den Annotationslängen: im Schnitt besteht eine Annotation aber aus 25 Tokens für alle Kategorien. Da Texteinheiten von variabler Länge und überlappende Texteinheiten annotiert werden können, muss zur Berechnung von Übereinstimmungsmetriken eine Festlegung auf eine Texteinheit getroffen werden. Dazu wird folgende Heuristik angewendet: Für jede Replik oder Regieanweisung wird pro Annotator*in diejenige Annotation markiert, die am meisten (gemessen an der Zahl an annotierten Token) markiert wurde. Keine Annotation pro Replik/Regieanweisung wird als zusätzliche Klasse markiert und dann replikenweise Übereinstimmungen kalkuliert (vgl. Abbildung 2). Zur Interpretation von Cohen""s Œ∫ werden im Folgenden in Klammern die Wertebereiche für einzelne Intervalle gemäß Landis und Koch (1977) mitangegeben. Im Schnitt kann man für die Polarität eine moderate Übereinstimmung (laut Landis und Koch gilt moderat für 0,4<Œ∫<=0,6) und für die anderen Kategorien eine schwache Übereinstimmung (0,1<Œ∫<= 0,4) feststellen. Im Vergleich zu anderen Textsorten ist dies eine geringe Übereinstimmung (Wood et al. 2018a; 2018b), die jedoch vergleichbar mit anderen Sentiment- und Emotionsannotationsprojekten mit literarischen und/oder historischen Texten ist (Alm / Sproat 2005; Sprugnoli et al. 2016; Schmidt et al. 2018b; Schmidt et al. 2019b; 2019d). Mehr Erläuterungen und Ergebnisse zur Annotation findet man bei Schmidt et al. (2021c). Im Folgenden werden die Ergebnisse für denjenigen Fall präsentiert, bei dem als Trainings- und Evaluationsmaterial (""Goldstandard"") alle Annotationen des obigen Annotationskorpus"" verwendet werden (also je zwei Annotationssätze pro Drama). Dadurch liegt folgende Besonderheit vor: Eindeutige und partielle Annotationswidersprüche werden nicht aufgelöst, sondern dem Modell mit als Trainingsmaterial übergeben. Je nach kategorialem System gibt es eine unterschiedliche Menge an partiellen und absoluten Widersprüchen (ca. 16% für Polarität, 14% für Dreifach-Polarität, 28% für Hauptklassen, 47% für Sub-Emotionen). Dieses Verfahren wurde dennoch gewählt, da aufgrund der variablen Annotationspraxis die Auflösung eindeutiger Annotationswidersprüche schwerfällt (siehe Kapitel Wir definieren die Emotionsklassifikation als single-label-Klassifikationsaufgabe für Textsequenzen variabler Länge für folgende Klassengruppen: Alle Verfahren wurden in Obschon die Leistung lexikonbasierter Sentiment-Analyse meist von Wir evaluieren zudem zwei klassische Methoden des maschinellen Lernens: (1) Repräsentation über Termfrequenzen in einem bag-of-words-Modell und dem Lern-Algorithmus Statische Sprachmodelle repräsentieren Wörter als Vektoren in Vektorräumen, so dass geometrische Verhältnisse der jeweiligen Semantik entsprechen. Diese Repräsentationen ( Als transformerbasierte Sprachmodelle werden dynamische Für die Klassifikationsaufgabe werden die Modelle in einem ""Fine-Tuning""-Schritt mit dem Goldstandard trainiert. Für die konkrete Implementierung folgen wir den jeweiligen Empfehlungen für die gewählte Architektur (Devlin et al. 2018; Clark et al. 2020) Die Performanz von Klassifikations-Aufgaben kann verbessert werden, indem Texte der gleichen Domäne zum Vortraining von transformerbasierten Modellen genutzt werden (siehe Rietzler et al. 2020; Gururangan et al. 2020). Man kann entweder (1) selbst ein Modell von Grund auf mit domänennahen Texten erstellen oder (2) Modelle zeitgenössischer Sprache mit domänenspezifischen historischen Texten nachtrainieren. Beide Methoden wurden bereits erfolgreich im Kontext deutscher, historischer Sprache angewendet (Labusch et al. 2019; Schweter / Baiter 2019; Schweter / März 2020; Brunner et al. 2020). Auch hier evaluieren wir etablierte vortrainierte Modelle, die über die Hauptmetrik zur Interpretation der Ergebnisse ist die Alle gewählten Methoden übertreffen in den einzelnen Settings die Obschon die Menge an annotiertem Material im Vergleich zu Studien auf der Basis anderer Textsorten limitiert ist, konnten wir erste Erkenntnisse für die Optimierung computergestützter Methoden sammeln. Für Polarität und Dreifach-Polarität erreichen die besten Modelle in ihren Default-Settings bereits Ergebnisse, die durchaus vergleichbar sind mit state-of-the-art-Resultaten für Sentiment- und Emotionsklassifikation in anderen Bereichen (Yang et al. 2019; Munikar et al. 2019; Cao et al. 2020; Dang et al. 2020). Die besten Ergebnisse erzielen grundsätzlich die derzeit größten transformerbasierten Modelle für die deutsche Sprache. Die Optimierung für historische oder poetische Sprache hat lediglich geringfügige Verbesserungen gegenüber den äquivalenten kontemporären Modellen aufgezeigt. Ein Grund dafür ist möglicherweise, dass die gewählten historischen Modelle noch zu viele Texte aus dem 19. und 20. Jahrhundert enthalten, die doch zu weit entfernt von unserer Zeitepoche sind. Wir befinden uns momentan im Prozess der Akquise großer Textmengen aus dem entsprechenden Zeitraum, um vortrainierte Modelle zu evaluieren, die noch stärker an unsere Domäne angepasst sind. Für die mehrklassigen Kategoriensysteme können keine zufriedenstellenden Ergebnisse erzielt werden. Dies ist ohne größere Optimierung für derartige Klassifikationsverfahren nicht ungewöhnlich. Wir planen sowohl die Anwendung verschiedener empfohlener Verfahren, um mit dem Klassenungleichgewicht umzugehen (Buda et al. 2018) und die Optimierung von Hyperparametern als auch die Exploration des Einsatzes einer neutralen ""Nicht-annotiert""-Klasse. Im Bereich der Annotation soll eine Expertenannotation eingefügt werden, welche die Entscheidungen der ersten beiden Annotationen berücksichtigt, aber eine eigenständig verwendbare, widerspruchsfreie Annotationsschicht darstellt. Evaluationsergebnisse mittels der Anwendung von manuellen Widerspruchsauflösungen findet man bei Schmidt et al. (2021b). Wir lassen derzeit weitere Texte annotieren und explorieren historische"
2022,DHd2022,RÖTTGERMANN_Julia_Literaturgeschichtsschreibung_datenbasiert.xml,"Literaturgeschichtsschreibung datenbasiert und wikifiziert?   Automatische Extraktion thematischer Statements aus französischen Primärtexten mithilfe von Topic Modeling, RDF und eines kontrollierten Vokabulars in LOD","Julia Röttgermann (Universität Trier, Germany); Anne Klee (Universität Trier, Germany); Maria Hinzmann (Universität Trier, Germany); Christof Schöch (Universität Trier, Germany)","TEI/XML, Metadaten, Topic Modeling, Linked Open Data, Literaturgeschichte","Inhaltsanalyse, Modellierung, Bibliographie, Literatur, Metadaten","Welche Formalisierungs- und Modellierungsarbeit ist nötig, um Kulturen des kollektiven Gedächtnisses wie die Literaturgeschichtsschreibung als Daten abfragbar zur Verfügung zu stellen? Wir sehen aktuell einige Umbrüche in den Strategien der Gedächtnisinstitutionen, die sich zunehmend dem ""Linked Open Data""-Paradigma verpflichtet sehen. Anhand von drei verschiedenen Informationsquellen (Primärliteratur, Sekundärliteratur und bibliographische Daten) werden literaturhistoriographische Aussagen extrahiert und in Form eines Wissensnetzwerks modelliert, das die heterogenen Daten integriert und über einen SPARQL-Endpunkt abfragbar macht. Das interdisziplinäre Projekt vereint informationswissenschaftliche, juristische, literaturwissenschaftliche und computerlinguistische Expertise. Den Prinzipien der Open Science verbunden, wurde eine Infrastruktur aufgebaut, die freie Software wie Dies soll zunächst konkret anhand der Extraktion thematischer Aussagen mittels Topic Modeling (1.) sowie der Extraktion von Themenaussagen aus bibliographischen Daten (2.) veranschaulicht werden. Die Relevanz eines kontrollierten Vokabulars im Sinne der Vergleichbarkeit thematischer Aussagewerte unterschiedlicher Informationsquellen (3.) in einem über SPARQL abfragbaren LOD-Wissensnetzwerk (4.) wird im Anschluss dargelegt. Das kontrollierte Vokabular resultiert dabei aus einem Grundstock zeitgeschichtlich relevanter Themen, erweitert um Themenkonzepte, die sich aus der Informationsextraktion der drei Datenquellen ergeben. Als Datengrundlage zur Modellierung von literaturhistorisch relevanten Aussagen dienen uns drei Kategorien an Texten: Primärliteratur (Romane), Sekundärliteratur (Fachliteratur) und bibliographische Quellen. Die erste der drei Informationsquellen besteht aus einem Korpus aus französischen Romanen der zweiten Hälfte des 18. Jahrhunderts (Röttgermann 2021). Dieses umfasst derzeit 115 Texte und wird laufend durch Volltextdigitalisierung mit dem auf historische Drucke spezialisierten OCR-Tool Alle Input-Dateien wurden in TEI-konformes XML nach den Richtlinien der Text Encoding Initiative (vgl. Burnard 2014) nach dem Schema der Mithilfe von Topic Modeling (vgl. Blei 2011) ist es möglich, ""Topics"" aus den Primärtexten zu extrahieren, die Aufschluss über Themen-Cluster innerhalb des Korpus geben können. Die Methode generiert auf der Grundlage der Kookkurrenz von Wörtern Gruppen semantisch verwandter Wörtern, die Topics. Hier wurden mit Mallet (vgl. McCallum 2002) zunächst 30 Topics generiert, welchen im Anschluss Label zugewiesen wurden. Diese wiederum wurden mit Konzepten aus einem Themenvokabular verknüpft. Die gewonnenen Topics verteilen sich folgendermaßen über das Korpus (s. Abb. 1): Neben einigen Topic Modeling Artefakten wie [temps para√Ætre encore] haben sich Topics im Romankorpus herauskristallisiert, die auf literarische Gattungen des 18. Jahrhunderts wie Briefroman (Topic ""correspondance"") oder Reiseroman (Topic ""migration_voyage"") oder auf häufig thematisch verhandelte Konzepte wie Philosophie (Topic ""philosophie"") oder Erziehung (Topic ""éducation_enfance"") hinweisen. Für das Werk Candide (1759) von Voltaire konnte beispielsweise ein Topic extrahiert werden, das mit dem Label ""migration_voyage"" versehen wurde (Abb. 2). Aus diesem Einzelergebnis leiten wir folgendes angedeutetes Statement über das Werk Candide ab: [Candide] ABOUT [label @en: travel | label @de: Reise | label@fr: voyage]. Ein Mapping der Entitäten auf Wikidata ergibt daraufhin dieses hier in menschenlesbarer Form angedeutete Statement: Q215894 [label @fr: Candide] ABOUT Q61509 [label @en: travel | label @de: Reise | label@fr: voyage]. Für den Piloten wurden für das Romankorpus [Stand: 92 Romane] Bei der zweiten Informationsquelle handelt es sich um bibliographische Nachweissysteme zur französischen Literatur 1751-1800 (s. Abb. 3). Im Fokus steht Die Bibliographie bietet in Kombination mit den Ergebnissen des Topic Modelings die Möglichkeit eines Mensch-Maschine-Vergleichs 'wurden die enthaltenen thematischen Schlagworte doch in den 1970er Jahren durch Lektüre und Zusammentragen von Informationen aus anderen Nachschlagewerken erhoben. Die Bibliographie wurde in mehreren Arbeitsschritten aufwendig erschlossen. [Les enfans de la nature] ABOUT [sentiment | Gefühl | sentiment] [Les enfans de la nature] ABOUT [pedagogy | Pädagogik | pédagogie] [Les enfans de la nature] ABOUT [philosophy | Philosophie | philosophie]. Technisch unterscheiden sich die RDF-Triple zu Themen je nach Datenquelle nicht, werden jedoch entsprechend ihrer Herkunft in Wikibase mit der Property In den bibliographischen Daten sind insgesamt knapp 2700 Items (Veröffentlichungen fiktionaler Prosa in französischer Sprache inklusive Übersetzungen) enthalten, von denen 349 das thematische Schlagwort ""voyage"" enthalten. Wie lassen sich die thematischen Muster in der Primärliteratur mit den Daten aus den bibliographischen Nachweissystemen vergleichen? Ein wichtiger Modellierungsschritt ist zunächst das Erstellen eines kontrollierten Vokabulars aus thematischen Konzepten der französischen Aufklärung, auf das die Ergebnisse des Topic Modeling und die Bibliographie-Schlagworte gemappt werden. Das Vokabular der Themenbegriffe besitzt eine hohe Relevanz für mehrere Teilprojekte: Es stellt zum einen die Labelbegriffe für die Topics aus dem Topic Model bereit, liefert daneben aber auch die Konzept-Items für die Objektposition solcher thematischer Statements, die aus der Sekundärliteratur und der Bibliographie extrahiert wurden. An das Vokabular sind somit mehrere Anforderungen geknüpft: Die Begriffe müssen die Themenkonzepte der französischen Aufklärung abdecken, sollen ein gewisses Abstraktionslevel aufweisen, damit sie als kategorische Begriffe fungieren können und die Zusammenstellung der Begriffe sollte transparent und nachvollziehbar sein. Eine erste Grundlage bildet das Themeninventar des Um die multilinguale Vergleichbarkeit zwischen französischsprachigen Primärtexten und deutschsprachiger Sekundärliteratur zu gewährleisten, und im Sinne der Anschlussfähigkeit an und Interoperabilität mit anderen Datenbeständen, werden die Themenkonzepte auf einen Normdatensatz (Wikidata) gemappt, wodurch das kontrollierte Vokabular konsolidiert und multilingual erfasst ist (siehe Abb. 4). Ziel ist es, eine Vielzahl an Statements zu aggregieren und die Items auf vielfältige Weise miteinander in Beziehung zu setzen, sodass durch zunehmende Skalierung der Daten aus einzelnen Subjekt-Prädikat-Objekt Aussagen in RDF (Resource Description Framework) ein dichter ""Knowledge Graph"" (vgl. zum Begriff: Ehrlinger/Wöß 2016) entsteht (s. Abb. 5). Dieser Graph lässt sich sodann auch über einen SPARQL-Endpoint abfragen (s. Abb.6). Ausgehend von der Beobachtung, dass das Themenkonzept ""Reise"" in den Bibliographie-Daten bei immerhin 14,7 % der Einträge vermerkt ist, ließe sich beispielsweise fragen, in welchen Werken auch laut Topic Modeling das mit dem Themenkonzept ""Reise"" verbundene Topic als dominantes Topic vorkommt. Durch Topic Modeling wurden dabei sowohl Werke identifiziert, die auch laut Bibliographie-Daten die Themenkategorie ""Voyages"" enthalten, als auch solche, in denen dieses Schlagwort nicht vorkam. Für das Werk Für das Werk SPARQL-Abfragen zu den Ergebnissen des Topic Modelings und/oder der bibliographischen Schlagworten ermöglichen es, auch weniger bekannte Werke zu spezifischen Themen zu ermitteln. Zudem zeichnen sich Muster an Themenkomplexen im Zeitverlauf ab. Für das Thema ""voyage"" innerhalb der bibliographischen Daten zeigt sich eine (auch statistisch signifikante) ansteigende Entwicklung (vgl. Abb. 8). Eine Erklärung für den Anstieg der Themenkategorie ""voyage"" in den 1780er und 1790er Jahren könnte sein, dass viele Autor:innen die Handlung ihrer Werke aus politischen Gründen in andere Länder ""verlegen"" und zudem, dass der Themenkomplex der Reise im Kontext von Emigration im Zuge der politischen Ereignisse zunehmend in Romanen verhandelt wird. Beispiele hierfür wären Insgesamt ist das Thema ""Reise"" derzeit laut Topic Modeling im Romankorpus in 14,13% der Werke als dominantes Topic vertreten, in den bibliographischen Daten in 14,74% der Werke. Ein makrostruktureller Blick zeigt demnach in diesem Beispiel eine vergleichbare Größenordnung der thematischen Aussage über das gesamte Korpus hinweg, auch wenn in der Bewertung der Einzelwerke nicht immer Kongruenz besteht. Das Projekt MiMoText modelliert die Geschichte des französischen Romans der zweiten Hälfte des 18. Jahrhunderts in Form von RDF-Tripeln in Wikibase als Knowledge Graph. Im Zuge eines Pilotprojekts wurden in einem ersten Schritt aus bibliographischen Daten und aus einem Romankorpus Relationen zwischen Werken und Themen extrahiert, die in Form von Tripeln in eine eigene Wikibase-Instanz eingelesen wurden. Zur Modellierung der Themen des französischen Romans der Aufklärung wurde ein kontrolliertes Vokabular erstellt, welches auf Wikidata gemappt wurde, um anschlussfähig an die Linked Open Data Cloud zu sein. Die Ergebnisse der Informationsextraktion aus den Romanen (mithilfe von Topic Modeling) und der Informationsextraktion aus bibliographischen Daten können nun per SPARQL-Endpoint abgefragt werden. Zu den nächsten Schritten gehört neben der Extraktion weiterer Statements über quantitative Romananalysen der Import von Themen-Statements aus dem dritten Typus von Informationsquellen (Fachliteratur) in unsere Wikibase-Instanz."
2022,DHd2022,DU_Keli_Kontrastive_Textanalyse_mit_pydistinto___Ein_Python_.xml,Kontrastive Textanalyse mit pydistinto Ein Python-Paket zur Nutzung unterschiedlicher Distinktivitätsmaße,"Keli Du (Universität Trier, Germany); Julia Dudar (Universität Trier, Germany); Cora Rok (Universität Trier, Germany); Christof Schöch (Universität Trier, Germany)","Computational Literary Studies, Distinktivitätsmaße, Python-Implementierung, pydistinto","Entdeckung, Programmierung, Stilistische Analyse, Methoden, Software, Text","   Digital Humanities im deutschsprachigen Raum 2022 Kontrastive Textanalyse mit pydistinto Ein Python-Paket zur Nutzung unterschiedlicher Distinktivitätsmaße Du, Keli duk@uni-trier.de Universität Trier, Germany Dudar, Julia dudar@uni-trier.de Universität Trier, Germany Rok, Cora rok@uni-trier.de Universität Trier, Germany Schöch, Christof schoech@uni-trier.de Universität Trier, Germany Viele Wissenschaftsbereiche, die sich mit der quantitativen Textanalyse beschäftigen, wie die Korpuslinguistik oder die Computational Literary Studies (CLS) setzen verschiedene statistische Distinktivitätsmaße ein, um Elemente (z.B. Wortformen oder Wortarten) zu bestimmen, die charakteristisch für eine Textgruppe im Vergleich mit einer anderen Textgruppe sind. Tools wie z.B. WordSmith (Scott 2020) oder AntConc (Anthony 2005), die solche Analysen ermöglichen, sind weit verbreitet, haben jedoch einige Nachteile: Die meisten bieten nur häufigkeitsbasierte Maße (z.B. Log-Likelihood-Ratio Test oder Chi-Squared Test) an, die in vielen Fällen Ergebnisse produzieren, die für die kontrastive (explorative) Textanalyse nicht hilfreich sind (siehe u.a. Baker 2004 und Johnson and Ensslin 2006). Dispersionsmaße wie z.B. DP (Gries 2008) oder dispersionsbasierte Distinktivitätsmaße wie z.B. Zeta (Burrows 2007), die besser interpretierbaren Ergebnisse liefern (siehe Gries 2021; Schöch 2018), werden dagegen nicht implementiert. Eine Ausnahme bildet stylo , das Zeta implementiert (Eder et al. 2016). Ein weiterer Nachteil ist, dass bei den meisten Tools nur ein oder zwei Maße für die Analyse ausgewählt werden können, was einen Vergleich der unterschiedlichen Maße erschwert. Gerade wenn Nutzende ihre Analysen anpassen und eigene Parametereinstellungen vornehmen oder verschiedene Datenformate nutzen wollen, erweisen sich die meisten Tools als ungeeignet. Um den Einsatz relevanter Maße für die kontrastive Textanalyse zu erleichtern und das Bewusstsein für die Vielfalt der Maße zu schärfen, entwickeln wir im Rahmen des Projekts ""Zeta and Company"" ein Python-Paket mit dem Namen pydistinto . 1 Ziel unseres Projekts ist es, zu einem tieferen Verständnis der verschiedenen Distinktivitätsmaße zu gelangen und Verbesserungen für deren Implementierung und Anwendung vorzuschlagen. Mithilfe von pydistinto können zwei Textkorpora mit unterschiedlichen Maßen verglichen werden. Hierfür haben wir zunächst ein konzeptionelles Framework erstellt, auf dessen Basis die Maße in pydistinto implementiert werden (Du et al. 2021a). Das Framework definiert die Bereiche Preprocessing, Berechnung von Häufigkeiten, Korpusaufteilung sowie der eigentlichen Berechnung der Distinktivitätswerte, Visualisierung sowie quantitative und qualitative Evaluation der Ergebnisse. In der Implementierung umfasst das Preprocessing die Tokenisierung, Lemmatisierung und das POS-Tagging der Texte. Danach werden die Texte je nach Parameter entweder segmentiert (dies wird bei der Berechnung von dispersionsbasierten Maßen empfohlen) oder als ganze Dokumente belassen. Die (absoluten, binären, relativen usw.) Worthäufigkeiten in den Segmenten bzw. Dokumenten werden in einer Matrix zusammengefasst. Als Nächstes werden die Segmente bzw. Dokumente in zwei Gruppen, ein Zielund ein Vergleichskorpus, aufgeteilt. Anschließend werden die Distinktivitätswerte auf Basis der Worthäufigkeits-Matrizen berechnet und die distinktiven Wörter für das Zielkorpus visualisiert. Die Implementierung des Moduls zur quantitativen Evaluation steht noch aus. Geplant ist hier, die statistischen Eigenschaften der Wortlisten zu analysieren und die Korrelation verschiedener Maße zu untersuchen (siehe, für Zwischenergebnisse, Du et al. 2021c). Bei der qualitativen Evaluation werden die ausgegebenen Wörter manuell interpretiert und ihre Relevanz für das Zielkorpus wird beurteilt. Das Python-Paket wird auf Github veröffentlicht und steht somit zur freien Nutzung, eigenen Anpassung und weiteren Entwicklung zur Verfügung (Du et al. 2021b). Im pydistinto sind derzeit folgende Distinktivitätsmaße implementiert: Zeta, Ratio of Relative Frequencies, Gris"" Deviation of Proportions based measure (Eta, siehe Du et al. 2021c), Welch's T-test, Wilcoxon Rank-sum Test, Kullback-Leibler Divergence, Chi-Squared Test, Log-Likelihood-Ratio Test, TF-IDF. Ein besonderer Vorteil des Pakets ist, dass es in einem Beginner-Modus und einem ProfiModus genutzt werden kann. Im Beginner-Modus können auch weniger erfahrene Nutzende mit geringen Programmierund Statistikkenntnissen Textkorpora vergleichen. Zielund Vergleichskorpus müssen hierfür lediglich als ""plain text"" vorbereitet und einige Parameter wie z. B. Segmentlänge, Feature-Typen oder Anzahl der Top-Features eingestellt werden. Die Analyse wird dann automatisch durchgeführt und eine Visualisierung angeboten. Wer sich für die statistischen Eigenschaften der unterschiedlichen Maße interessiert und diese vergleichen möchte, kann den Profi-Modus verwenden. Die Nutzenden können dann selbst darüber bestimmen, welche Maße und statistischen Eigenschaften der Features (z.B. absolute Häufigkeit, relative Häufigkeit, Dispersion) für die Berechnung der Distinktivität kombiniert werden sollen. Es gibt in diesem Modus außerdem zusätzliche Möglichkeiten, die Daten zu visualisieren: so kann die Abhängigkeitsstruktur zweier statistischer Merkmale (z.B. Zeta-Wert und absolute Häufigkeiten der Features) auch durch ein Streudiagramm dargestellt werden. Durch die Entwicklung des Pakets möchten wir auf der einen Seite eine reflektierte Nutzung statistischer Distinktivitätsmaße für die kontrastive Textanalyse erleichtern. Auf der anderen Seite soll das Paket ermöglichen, die Eigenschaften und Leistungsfähigkeit der Maße empirisch zu ermitteln und systematisch zu vergleichen. Fußnoten 1. Das Projekt gehört zum DFG-geförderten Schwerpunktprogramm ""Computational Literary Studies"" (SPP 2207) und läuft 1 Digital Humanities im deutschsprachigen Raum 2022 von 2020-2023. Weitere Informationen unter https://zeta-project.eu/de/. Bibliographie Baker, Paul (2004): ""Querying keywords: questions in difference, frequency, and sense in keyword analysis"", in: Journal of English Linguistics 32 (4), pp. 346–59 Du, Keli / Dudar, Julia / Rok, Cora / Schöch, Christof (2021a): Implementation framework of measures of distinctiveness. Zenodo. http://doi.org/10.5281/zenodo.5092328 Du, Keli / Dudar, Julia / Schöch, Christof (2021b): pydistinto. Version 0.1.0. Verfügbar unter: https://github.com/Zeta-and-Company/pydistinto . DOI: https://doi.org/10.5281/zenodo.5094346 . Du, Keli / Dudar, Julia / Rok, Cora / Schöch, Christof (2021c): "" Zeta & Eta: An Exploration and Evaluation of Two Dispersion-based Measures of Distinctiveness"", in: CHR 2021: Computational Humanities Research Conference , November 17'19, 2021, Amsterdam, The Netherlands, https://2021.computational-humanities-research.org/conference/ Eder, Maciej / Rybicki, Jan / Kestemont, Mike (2016): "" Stylometry with R: a package for computational text analysis"", in: R Journal , 8(1): 107-21. https://journal.r-project.org/archive/2016/RJ-2016-007/index.htm l Gries, Stephan (2008): ""Dispersions and adjusted frequencies in corpora"", in: International Journal of Corpus Linguistics, Volume 13(4): 403–437. DOI: https://doi.org/10.1075/ijcl.13.4.02gri Gries, Stephan (2021): ""A New Approach to (Key) Keywords Analysis: Using Frequency, and Now Also Dispersion"", in: Research in Corpus Linguistics, 9, 1–33. DOI: https://doi.org/10.32714/ricl.09.02.02 Johnson, Sally / Ensslin, Astrid (2006): ""Language in the news: some reflections on keyword analysis using WordSmith Tools and the BNC"", in: Leeds Working Papers in Linguistics and Phonetics 11, pp. 96–109. https://www.latl.leeds.ac.uk/wpcontent/uploads/sites/49/2019/05/Johnson-Ensslin_2006.pdf Laurence, Anthony (2005): "" AntConc: A learner and classroom friendly, multi-platform corpus analysis toolkit"", in: Proceedings of IWLeL 2004: An Interactive Workshop on Language eLearning . 7–13. Schöch, Christof (2018): "" Zeta für die kontrastive Analyse literarischer Texte. Theorie, Implementierung, Fallstudie"", in: Bernhart, T., et al. (eds.), Quantitative Ansätze in der Literaturund Geisteswissenschaften. Berlin: de Gruyter, 77'94. https://www.degruyter.com/viewbooktoc/product/479792 . Scott, Mike (2020): WordSmith Tools, Version 8, Stroud: Lexical Analysis Software. 2"
2022,DHd2022,DU_Keli_Evaluating_Hyperparameter_Alpha_of_LDA_Topic_Modelin.xml,Evaluating Hyperparameter Alpha of LDA Topic Modeling,"Keli Du (Universität Trier, Germany)","LDA Topic Modeling, Evaluation, Hyperparameter Alpha","Inhaltsanalyse, Modellierung, Methoden, Text","   Digital Humanities im deutschsprachigen Raum 2022 Evaluating Hyperparameter Alpha of LDA Topic Modeling derstand the influence of hyperparameter Alpha from two perspectives: topic modeling based single-label document classification and topic coherence, representing the quality of the topic model and the quality of the topics, respectively. Method Du, Keli duk@uni-trier.de Universität Trier, Germany Introduction As a quantitative text analysis method, Latent Dirichlet Allocation (LDA), also often referred to as topic modeling (Blei 2012), has been widely used in Digital Humanities in recent years to explore numerous unstructured text data. When topic modeling is used, one has to deal with many parameters that could influence the result, such as the hyperparameter Alpha and Beta, the topic number, document length, or the number of iterations when updating the model. To understand the impact of these parameters, they must be systematically evaluated. In the last few years, there have been several studies evaluating LDA topic modeling in Digital Humanities or Computational Literary Studies (e.g., Jockers 2013; Schöch 2017; Du 2020; Uglanova & Gius 2020) and the presented paper focuses on evaluating the impact of hyperparameter Alpha on LDA topic models. Hyperparameter Alpha can refer to two different types of parameters in the context of LDA topic modeling: LDA model parameter and inference algorithm parameter. As a parameter of the LDA model, Alpha determines the properties of a Dirichlet distribution, which is the prior probability distribution of the topic-document distribution. Together, the hyperparameter Alpha and the prior probability distribution determine which topics we expect to occur more frequently in the corpus and how confident we are about them. In practice, when we employ Gibbs Sampling to train our topic model, Alpha is the parameter, which has the smoothing effect on the topic-document distribution and ensures that the probability of each topic in each document is not 0 throughout the entire inference procedure. More importantly, Alpha represents the assumption about the data on how topics are distributed in documents before inferencing the topic model. In other words, the hyperparameter Alpha affects how often each topic occurs in each document. When the alpha value of a topic is set larger in a document, it means that the topic has a greater chance of appearing in that document. And vice versa. For this reason, the setting of Alpha can affect the quality of the inferenced topic model. Therefore, this paper focuses on evaluating the impact of inference algorithm parameter alpha systematically. According to Griffiths & Steyvers (2004), the topic model has the best quality when the sum of Alphas of all topics is equal to 50. This is probably the reason that in MALLET 2.0.7, the default value of the sum of Alphas was set to 50, while in MALLET 2.0.8, the value is reduced to 5. According to the supervisor of MALLET, David Mimno: ""The general experience was that 50 was too large, and that 5 is a better default.""1 Since there are different opinions on this issue, it is interesting to test how Alpha affects LDA topic modeling, especially on different types of text collections that are not in English. Therefore, this paper presents a study on evaluating Alpha on two German text collections and aims to unTwo collections of German texts were built for the study. The first corpus is a collection of 2000 newspaper articles published between 2001 and 2014. The articles belong to ten different thematic classes, and each class contains 200 articles. The ten classes are ""Digital"", ""Society"", ""Career"", ""Culture"", ""Lifestyle"", ""Politics"", ""Travel"", ""Sports"", ""Study"" and ""Economy"". The corpus contains over 3.4 million words in total, and the average text length is about 1800 words. The second corpus consists of 439 dime novels published between 1961 and 2016, and they belong to five subgenres, namely 100 fantasy novels, 51 horror novels, 88 crime novels, 100 romance novels, and 100 science fiction stories. The corpus contains about 13.4 million words, and the average text length is about 30,000 words. All texts are lemmatized. Since the average document length of the newspaper articles is 1800 words, the novels are also split into 1800 words segments. Thus, the document length is no longer a confounding factor when comparing the test results on the newspaper corpus with the results on the novel corpus. The goal of the following tests is to explore the influence of the hyperparameter Alpha. While training topic models, the setting is varied by the value of Alpha _ {0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 30, 40, 50, 100} and number of topics _ {10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500}. For all other parameter settings, the default values of the topic modeling software were taken. All models were trained without applying hyperparameter optimization, which means that if Alpha is set to 0.1, the Alpha value for each topic is set to 0.1 during the whole training process. Common stop words were removed from both corpora. For technical reasons, namely random initialization in the topic assignment and Gibbs sampling, two topic models from one corpus are not completely identical even if the parameter settings during training are the same. Therefore, ten models were trained for each setting to balance the randomness from the technical side. The topic models were trained using MALLET (McCallum 2002). As a result, a topic-document distribution and the topics are obtained for each topic model. In a topic-document distribution, each document is represented by an N -dimensional vector, while N is the number of topics of the topic model. Based on the topic-document distribution, the document classification was performed, and the classification was done as a 10-fold cross-validation with a linear SVM classifier. For the newspaper corpus, the articles were classified according to their thematic classes. For the novel corpus, the novel segments were classified according to their subgenre. The topic coherence was automatically calculated by the Java program Palmetto (Röder et al. 2015), and the first ten most important words of each topic were taken for the calculation. The reference corpus for the calculation of the topic coherence is the lemmatized German Wikipedia. Several topic coherence mea-sures have been implemented in Palmetto. For this work, the Nor-malized Pointwise Mutual Information (NPMI) based coherence measure proposed in Aletras & Stevenson (2013) was taken. The theoretical range of NPMI based coherence measure is between -1 and 1. The higher the score, the better the topic. By performing Bag-of-Words (BoW) model-based classification, a baseline of document classification has been defined for both corpora. The tests were also done as a 10-fold cross-valida1 Digital Humanities im deutschsprachigen Raum 2022 tion with a linear SVM classifier. The F1(macro) score for the newspaper articles and for the novel segments was 0.758 and 0.993, respectively. A baseline of the NPMI value was also defined for each corpus. With only one iteration, 14 topic models were first trained on each corpus, containing 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500 topics, respectively. In this way, 1,950 ""topics before topic modeling"" have been trained for each corpus. The NPMI scores of these topics were then calculated, and the average NPMI score is the NPMI baseline, which is -0.0619 for the newspaper corpus and -0.1153 for the novel corpus. A black line represents the baselines in Figure 3 and Figure 4. Results Document classification : Figure 1 and Figure 2 show the classification results based on topic models of newspaper articles and novel segments, respectively. It can be seen in both figures that the classification results gradually become worse with the increase of the setting of Alpha, regardless of how many topics have been trained. Especially if Alpha is set to greater than 1, the classification results based on topic models with more topics (the blue lines) show a stronger decreasing trend than the results based on topic models with fewer topics (the red lines). In comparison, most F1 scores change less when Alpha is set to a value smaller than 1. However, we can still see that the blue lines start to decrease when the Alpha is raised from 0.5 to 1. The highest F1-score of classifying newspaper articles and novel segments in this test are 0.752 and 0.998, respectively, which do not differ much from the predefined baseline based on BoW-model. Topic coherence: Compared to the classification results, the evaluation from the perspective of topic coherence shows some differences between the two corpora. Firstly, it can be observed in Figure 3 that the maximum of the NPMI-score distributions decreases with the increase of Alpha from almost 0.3 to about 0.12. In addition, the median of the distributions also shows a decreasing trend. At Alpha = 0.01, the median is lower than the NPMI baseline if the number of topics is set higher than 100. However, at Alpha = 100, the median is already lower than the NPMI baseline if the number of topics is set to 70. Apart from that, we can observe that the topic models with a higher number of topics contain more topics with low NPMI scores, regardless of the setting of Alpha. Compared to the test on the newspaper corpus, the test results on the novel corpus are slightly different. When Alpha is set smaller than 1, the NPMI-score distributions do not show an evident change as Alpha increases, and the range of distribution is often broader when the number of topics is set between 60 and 300. Starting from Alpha being raised to greater than 1, the distributions of the NPMI-scores clearly change, and the results then are similar to the previous test on the newspaper corpus: the maximum of the NPMI-score distributions decreases with the increase of the Alpha, and topic models with a higher number of topics contain more topics with low NPMI scores. Fig. 3: NPMI-score distributions of topics from newspaper articles Fig. 1: Average F1(macro)-scores of topic modeling based classification of newspaper articles Fig. 4: NPMI-score distributions of topics from novel segments Conclusion Fig. 2: Average F1(macro)-scores of topic modeling based classification of novel segments The presented research evaluates the influence of hyperparameter Alpha in topic modeling on a German newspaper corpus and a German literary text corpus from two perspectives, single-label document classification, and topic coherence. Based on the results of the presented investigation, it can be stated that one should 2 Digital Humanities im deutschsprachigen Raum 2022 avoid training topic models with a setting of the Alpha of each topic to greater than 1 in order to ensure better topic modeling based document classification results and to get more coherent topics. In addition to that, LDA topic models with many topics are more vulnerable to changes in Alpha. Therefore, with the result of the presented investigations in this study, one can confirm the explanation of Mimno mentioned earlier that a smaller Alpha is better suitable for LDA Topic Modeling. Footnotes 1. https://stackoverflow.com/questions/45162186/mallet-topic-modeling-topic-keys-output-parameter (15.07.2021) Bibliography Aletras, Nikolaos / Stevenson, Mark (2013): ""Evaluating topic coherence using distributional semantics"". In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) ‚Äì Long Papers (pp. 13-22). Blei, David M. (2012): ""Probabilistic topic models"", in: Communications of the ACM , 55(4), 77-84. Du, Keli (2020): ‚ÄûDer Spielraum zwischen ""zu wenig"" und ""zu viel"""". Presented at the DHd 2020 Spielräume: Digital Humanities zwischen Modellierung und Interpretation. 7. Tagung des Verbands ""Digital Humanities im deutschsprachigen Raum"" (DHd 2020), Paderborn: Zenodo. http://doi.org/10.5281/zenodo.4621770 . Griffiths, Thomas L. / Steyvers, Mark (2004): ""Finding scientific topics"", in: Proceedings of the National Academy of Sciences, 101 (Supplement 1), 5228‚Äì5235. https://doi.org/10.1073/pnas.0307752101 . Jockers, Matthew L. (2013): Macroanalysis: Digital methods and literary history . University of Illinois Press. McCallum, Andrew K. (2002): MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu . Röder, Michael / Both, Andreas / Hinneburg, Alexander (2015): ""Exploring the space of topic coherence measures"", in: Proceedings of the eighth ACM international conference on Web search and data mining , 399‚Äì408. Schöch, Christof (2017): ""Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama."", in: Digital Humanities Quarterly 11, no. 2. ¬ß1-53. http://www.digitalhumanities.org/dhq/vol/11/2/000291/000291.html . Uglanova, Inna / Gius, Evelyn (2020): ""The Order of Things. A Study on Topic Modelling of Literary Texts"", in: Online Workshop on Computational Humanities Research, Proceedings . http://ceur-ws.org/Vol-2723/long7.pdf ."
2023,DHd2023,SCHUMACHER_Mareike_DisKo__Zur_Einbindung_von_Citizen_Humanit.xml,DisKo: Zur Einbindung von Citizen Humanities beim Aufbau eines Diversitäts-Korpus,"Mareike Schumacher (Technische Universität Darmstadt, Deutschland); Flüh Marie (Universität Hamburg); Leinen Peter (Deutsche Nationalbibliothek)","Diversität, Korpusbildung, Citizen Humanities","Sammlung, Kommunikation, Community-Bildung, Identifizierung, Crowdsourcing, Literatur"," Die Korpuskonstituierung ist für Projekte, die Verfahren des maschinellen Lernens einsetzen, ein Dreh- und Angelpunkt. Alle weiteren Verfahrensschritte und Ergebnisse wie z.B. die Performanz eines Classifiers oder Analyseergebnisse, die durch dessen Einsatz erzielt werden, werden vom genutzten Korpus massiv beeinflusst. Unser Konzept der Korpuskonstituierung greift sowohl arbeitspraktische Ansätze der Digital Humanities wie das"
2023,DHd2023,√áAKIR_D√Ælan_Canan_Vom_Finden__Filtern_und_Auswerten_der_rele.xml,"Vom Finden, Filtern und Auswerten der relevanten Daten im digitalen Nachlass von Friedrich Kittler im Deutschen Literaturarchiv Marbach","Alex Holz (Deutsches Literaturarchiv Marbach, Deutschland); Dîlan Canan Çakir (Deutsches Literaturarchiv Marbach, Deutschland)","Literaturarchiv, Born-Digitals, Nachlass, Friedrich Kittler, NSRL","Datenerkennung, Bereinigung, Archivierung, Identifizierung, Literatur, Metadaten","   In einem nächsten Schritt wurde die Menge auf die Dateien begrenzt, die von Seiten der Nachlassverwaltung begutachtet und mit einem Status versehen wurden (freigegeben, vorläufig gesperrt, gesperrt). Unser Arbeitskorpus ist also als Momentaufnahme zu sehen, da er nur auf den bereits bewerteten Dateien basiert. Ohne die bislang unbekannten bzw. noch nicht bewerteten Dateien kommen wir auf 219.989 Dateien. Aus dieser Menge wurden zuletzt nun die für die Forschung vollständig freigegebenen Dateien (Metadaten und Inhalt) herausgefiltert, die zudem von Kittler erstellt wurden. Wir landeten bei etwa 30.000 Dateien, also etwa 0,88% von den 3,3 Millionen Dateien. "
2023,DHd2023,SCHUMACHER_Mareike_GitMA_oder_CATMA_für_Fortgeschrittene.xml,GitMA oder CATMA für Fortgeschrittene,"Mareike Schumacher (Technische Universität Darmstadt, Deutschland); Malte Meister (Technische Universität Darmstadt, Deutschland); Dominik Gerstorfer (Technische Universität Darmstadt, Deutschland)","manuelle Annotation, Textanalyse, kollaboratives Annotieren","Inhaltsanalyse, Annotieren, Netzwerkanalyse, Kollaboration, Einführung, Visualisierung","  Der Workshop bietet: Abbildung 1: Im Workshop vermittelter Workflow zur Annotationsauswertung und -überarbeitung mit dem CATMA-Backend Ablauf: CATMA 6 (45 Minuten) Explorative Annotationsauswertungen (60 Minuten) 15 Minuten Pause Statistische Annotationsauswertungen (45 Minuten) Diskussion und Feedback (30 Minuten) Nutzer*innen, die Annotationen mit CATMA in Forschungsprojekten oder Lehrsituationen managen, sowie alle, die einen schnellen Workflow zwischen Annotation bzw. Annotationsbearbeitung und Annotationsauswertung benötigen. 30   Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Residenzschloss 1, 64283 Darmstadt Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Residenzschloss 1, 64283 Darmstadt  Malte Meister, B.Sc. Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Residenzschloss 1, 64283 Darmstadt  Mareike Schumacher, M.A. Technische Universität Darmstadt, Institut für Sprach- und Literaturwissenschaft, Residenzschloss 1, 64283 Darmstadt Mareike Schumacher koordiniert das DFG-Projekt forTEXT ("
2023,DHd2023,DU_Keli_Evaluating_token_based_DTFs_on_authorship_classifica.xml,Understanding the impact of three derived text formats on authorship classification with Delta,"Keli Du (Universität Trier, Deutschland)","derived text formats, authorship classification, evaluation","Umwandlung, Bearbeitung, Bewertung, Stilistische Analyse, Methoden, Text"," However, as far as I know, there is not much research dedicated to the question, how much the loss of information caused by DTFs affects the TDM results. In Applying the first and the second DTFs to frequency-based authorship attribution does not present any challenge. Take the most well-known method in authorship attribution Burrows""s Delta ( In comparison, if the texts are transformed into the third DTF, although the frequency information of some words in the text will not match the original situation, the sequence information of words could be kept. This opens the possibility of using the data in this form for other TDM tasks such as sentiment analysis or named entity recognition that require the sequence information of words. If a corpus is published with the expectation that it can be applied to multiple TDM tasks, it makes more sense to prepare the corpus in this format. And of course, it is important to understand how much this format will affect the outcome of different TDM tasks. Therefore, this paper evaluates the usefulness of the third token-based DTF on authorship attribution as a start. In the next sections, the method and the results of the evaluation are reported. For the evaluation, three corpora representing different languages and text types have been constructed: deu_DraCor (German plays), fra_ELTeC (French novels) and eng_RSC (English journal articles). The relevant information about the corpora is shown in Table 1. The test is designed as follows: First, for each document in a corpus, a certain percentage of words (0%, 10%, ..., 100%) were randomly selected and replaced by their corresponding POS-tags. Since function words are crucial to authorship attribution, instead of only replacing function words as suggested in Before presenting the classification results, three text passages are prepared to give an impression of readability of the texts in DTF. The original text, the texts with 10% and 50% of words replaced by their corresponding POS-tags, are listed in Table 2. 0% (original) The classification results on the German play collection, the English article collection, and the French novel collection are presented in Figures 1, 2 and 3, respectively. The y-axis is the F1(macro)-score, and the x-axis shows the portion of words that are replaced or removed. The blue boxplots and the yellow boxplots represent the classification results, when the words in texts are replaced with POS-tags or removed, respectively. As the reference value, the classification results for the original data are also shown in the figures. The Welch""s t-test is also performed to determine the difference in classification results. The ""ns"" in the figures means non-significance. In all the three figures, the same trend can be observed: Step by step, the median of F1-score distributions get worse as the percentage increases. Especially when more than half of the words were replaced or removed, the tendency for the classification results to become worse became particularly obvious. In addition, the variance of the F1-scores always becomes larger, if a certain percentage of words in texts are replaced or removed. According to the Welch""s t-test, in all cases, whether the words are replaced or removed does not affect the classification. This observation indicates that the POS-tags do not contribute to the distinction of authorships. Another interesting observation is, when all words in texts are replaced by POS-tags, the classification results improve, relative to a reduction of 90%, in the case of the German and English data, but not for the French data. To understand this situation, the change in the number of word types in each corpus was checked. As presented in Figure 4, when 90% of the words are replaced or removed, there are still around 20,000 word types in each text collection. But when all the words are replaced, only a few dozen types remain. Their number becomes so small that it looks like it is reduced to zero in the Figure 4. Since the classification is based on the most frequent 2000 types, although 90% of the words are replaced or removed, the 2000 features used for classification are still mostly from the remaining 10% of words. In the German and English collections, these words bring apparently noise to the classification task. In contrast, the remaining 10% of words in the French corpus are still able to guarantee a relatively good classification result. From the data in Table 1 we can see that number of authors in the French corpus is smaller, which indicates the classification task on the French corpus is easier. More importantly, as presented in previous studies (e.g., This paper provides an exploration of the usefulness of three token-based DTFs for frequency-based authorship classification with Delta. As presented, selectively reducing information on individual tokens could ensure, to a certain extent, that the authorship classification results are not affected too much. The impact of token-based DTFs on the results of Delta test can be reduced by considering only replacing or removing content words, while all function words remain unchanged. But this limits the application of the texts on other TDM tasks such as topic modeling. For the future work, a series of tests are planned on evaluating the usefulness of token-based DTFs on other TDM tasks. The goal is to find DTFs that could balance various factors (e.g. word frequency, sequence information, content vs. function words, copyright) so that texts could be published and used for as many TDM tasks as possible without violating copyright law."
2023,DHd2023,HÖUSSLER_Julian_Fanfiction_Semantics___Eine_quantitative_Ana.xml,Fanfiction Semantics - Eine quantitative Analyse sensibler Themen in deutscher Fanfiction,"Julian Häußler (Technische Universität Darmstadt, Deutschland)","Digitale Literaturwissenschaft, Quantitative Textanalyse, Fanfiction","Inhaltsanalyse, Visualisierung, Literatur","Als Fanfiction können all jene Texte bezeichnet werden, die zur Veröffentlichung in eigens dafür eingerichteten Internetforen bestimmt sind und in ""appropriativ-derivativer bzw. [‚Ä¶] transformativer"" (Stemberger 2021, 10) Weise Bezug auf Mainstreammedien (meist Romane, Serien oder Filme) nehmen. Für die Autor*innen von Fanfiction, die meist unter Pseudonym schreiben, ist es somit möglich, in einem geschützten Raum (der Fancommunity) neben ersten Schreibversuchen, zu ihren Lieblingsfiguren auch das Verfassen von die Bezugsmedien kontrastierenden Erzählungen zu wagen. Catherine Tosenberger beschreibt letztere Kategorie als ""a freedom especially felt with regard to non-normative and taboo forms and representations of sexuality"" (Tosenberger 2014, 17). Fanfiction, die Literatur als Bezugsquelle gewählt haben, bauen dabei meistens auf großen Mainstream-Jugendbuchreihen aus dem angelsächsischen Raum auf. So sammeln sich in der deutschsprachigen Community auf fanfiktion.de unter der Kategorie ""Bücher"" die meisten Autor*innen in den Fandoms zu Harry Potter (über 55.000 Texte gesamt), Bis(s) (knapp 14.000) und Herr der Ringe (als allg. Gruppe Mittelerde definiert mit über 8.000 Texten). Praktisch befassen sich die Autor*innen oft mit Rekombinationen des Figurenarsenals der Originaltexte. Eine wichtige Rolle spielt dabei das Stereotyp der Slashes (männliche Figuren, die in eine romantisch-sexuelle Beziehung gesetzt werden; vgl. Brottrager et al. 2022). Die Moralvorstellungen der Originale können dabei auch übergangen werden, wenn zum Beispiel der Schüler Harry mit seinem Lehrer Severus Snape eine (romantisch-sexuelle) Beziehung eingeht. Das Masterprojekt ""Fanfiction Semantics 'A Quantitative Analysis of Sensitive Topics in German Fanfiction"" hat zum Ziel, diese Kontrastierungen in verschiedenen Fandoms zu betrachten. Gefragt wird dabei einerseits, wie virulent sensible Themen (sensitive topics) in Fanfiction sind und andererseits, wie diese Themen Wortbedeutungen beeinflussen. Als sensibel werden dabei jene Themen definiert, die in Originaltexten mehrheitlich ausgespart und von Fanfiction-Autor*innen demonstrativ eingeführt werden (v. a. extreme Gewalt und sexuelle Inhalte). 1) Im ersten Schritt, bei der Analyse der Metadaten, kann beispielsweise eine Verteilung davon, welche der Kategorien zur Altersbeschränkung für wie viele Texte verwendet wurde, 2) Die Analyse der Texte erfolgt im folgenden Schritt mithilfe von Word Embedding Modellen (hier word2vec; vgl. Mikolov et al. 2013). In einem Word Embedding Modell können Öhnlichkeiten zwischen verschiedenen Schlüsselwörtern effizient verglichen werden. Die algorithmische Grundlage bietet dabei die Verwendung der Wörter in einem festgesetzten Kontextfenster (vgl. Distributionshypothese nach Firth; vgl. Firth 1957). Durch den Abgleich von Schlüsselwörtern, die auf verschiedene Themen verweisen, können Schnittmengen und Unterschiede untersucht werden. Die Schlüsselwörter wurden hierzu aus den Wortfeldern Gewalt und sexuelle Inhalte gewählt, welche Wörter unter den (hier) 30 ähnlichsten zu einem Zielwort zu finden sind, beschreibt in gewisser Weise wie dieses Zielwort verwendet wird (vgl. Abb. 1). Für das Subkorpus der Das Poster soll die Ergebnisse der oben beschriebenen Analysen grafisch darstellen und einen Eindruck über die thematische Zusammensetzung der Fanfiction-Korpora bieten. Das zugrundeliegende Korpus kann aus urheberrechtlichen Gründen nicht in der verwendeten Form veröffentlicht werden. Dennoch ist es beabsichtigt, nach Abschluss des Projekts alle verwendeten Codes auf GitHub bereitzustellen, um Transparenz zu schaffen und die Methodik für andere Projekte nachnutzbar zu machen (orientiert an der in den FAIR-Prinzipien definierten Wiederverwendbarkeit (vgl. GO FAIR 2022)). Anonymisierte Metadatentabellen sowie die Word Embedding-Modelle sollen zudem auch zugänglich gemacht werden."
2023,DHd2023,JANNIDIS_Fotis_Korpuszusammensetzung_und_Verlässlichkeit_des.xml,Korpuszusammen-setzung und Verlässlichkeit des deutschsprachigen Google Ngram-Viewers,"Fotis Jannidis (Universität Würzburg, Deutschland)","Google Ngram-Viewer, Culturomics, Deutsche Korpora","Inhaltsanalyse, Metadaten, Text","Als Google im Jahre 2009 die erste Version des Ngram-Viewer publizierte, hat die Digital Humanities-Community recht schnell die positiven und negativen Aspekte dieses Werkzeugs analysiert: Die Möglichkeiten einer wort- und begriffsgeschichtlichen Forschung sind sprunghaft erweitert worden, wie auch der begleitende Aufsatz von Michel deutlich belegte (Michel et al. 2011). Dessen teilweise zu naive Umgang mit dem Quellenmaterial machte aber auch deutlich, dass die Autorinnen und Autoren kein Bewusstsein für die möglichen Fallen von korpusbasierten Forschungen hatten. Die wechselnde Zusammensetzung der zugrundeliegenden Textsammlung, die Unmöglichkeit auf die dahinterliegenden Texte zuzugreifen, das Fehlen von Metadaten für die Texte, falsche Jahreszahlen, OCR-Fehler u.a.m. sind dem Ngramm-Korpus wiederholt vorgeworfen worden (z.B. Underwood 2012). Die Arbeit von (Pechenick et al. 2015) hat gezeigt, wie die zunehmende Menge von wissenschaftlicher Literatur die Korpusanteile in der zweiten Hälfte des 20. Jahrhundert merklich verschiebt; unklar bleibt allerdings, ob dies nicht auch eine gesellschaftliche Entwicklung reflektiert, also keineswegs nur als Manko zu betrachten ist. Besonders einschlägig ist die Arbeit (Koplenig 2017), in der Veränderungen der Korpuszusammensetzung während des zweiten Weltkriegs untersucht werden: ""the German GB corpus was strongly biased toward volumes published in Switzerland during WWII"" (Koplenig 2017) Google hat zwei größere Updates vorgelegt, die die zugrundeliegende Textmenge deutlich erweitert haben. Hier die Entwicklung des Umfangs der deutschsprachigen Korpora, auf die ich mich im Folgenden beschränke: Dadurch dass die Anzahl der digitalisierten Bücher noch einmal deutlich gesteigert werden konnte 'und das über den gesamten Zeitraum, den der In diesem Sinne wollte auch eine Kollegin, die zu Fragen der literarischen Kanonisierung arbeitet, den Viewer verwenden, aber bei der Analyse der Ergebnisse fiel uns schnell auf, dass die Namen einer Reihe der hochkanonischen deutschsprachigen Autoren 'Thomas Mann, Goethe, Schiller, Kafka, Brecht 'nach 2005 einen auffälligen Abwärtstrend aufweisen (siehe Fig. 1). Bevor wir nun allgemeinere kulturanalytische Thesen über das Ende der Bildungskultur formulierten, wollte ich die Solidität der Daten prüfen. Doch wie kann man ein Korpus auf möglichen Bias untersuchen, wenn weder die Liste der Texte geschweige die Texte selbst vorliegen, sondern nur eine Reihe von generischen Metadaten und 1-5 Gramme? Da die NGramme, die dem  Wie lässt sich der Widerspruch zwischen den Ergebnissen erklären? Der  Wenn die Rohdaten aus Fig. 3 nun mit den Daten über die Anzahl der Token pro Jahr normalisiert werden, dann ergibt sich der Trend, der im Ngram-Viewer sichtbar wurde, alle Werte stürzen nach 2005 mehr oder weniger steil ab.  Allerdings ist der sehr steile Anstieg der Buchzahlen in Fig. 4 ziemlich überraschend. Wir wissen, dass Google Books auf den Ergebnissen der Digitalisierungskampagne Googles in Kooperation mit einer ganzen Reihe von internationalen Forschungsbibliotheken beruht. Die deutschsprachigen Ergebnisse verdanken sich nicht zuletzt den Kooperationen mit der Österreichischen Nationalbibliothek und der Bayerischen Staatsbibliothek. Deren Bestände speisen sich in den letzten Jahrzehnten aus Pflichtabgabeexemplaren und einer umfassenden Erwerbspolitik. Woher kommt also der plötzliche Anstieg? Sind 'den Klagen der Verlage zum Trotz 'seit 2005 sehr viel mehr Bücher als früher gedruckt worden?. Für die Buchproduktion konnte ich zwei Quellen verwenden, die die Daten gleich in digitaler Form anbieten: Statista, ein kommerzieller Datenanbieter, der Zahlen des Börsenvereins des deutschen Buchhandels zur Anzahl der Neuerscheinungen aufbereitet hat (Börsenverein 2022),  Die Grafik enthält neben den Werten für das aktuelle Korpus von 2019 auch die Werte für die früheren Ngramm-Korpora von Google aus den Jahren 2009 und 2012 , die deutlich kleiner waren. Beginnen wir bei den Werten vor 1995: Es ist auffällig, dass Google mit dem letzten Update den Anteil an der Buchproduktion eines Jahres, der digitalisiert vorliegt, deutlich steigern konnte, so dass bis in die Mitte der 1960er Jahre teils 50% und mehr im Korpus enthalten sind. Für den Zeitraum von den späten 1960ern bis in die späten 1990er stagnieren die Werte der Korpora, während die Buchproduktion in diesen Jahren steil angestiegen ist. Während im Jahr 1967 erstaunliche 67% der nach Rahlfs publizierten Bücher im Korpus liegen, sind es 1997 ""nur"" noch 23%. Erst danach, 1998 bis 2006, wächst der Anteil wieder. In den Korpora von 2009 und 2012 geschieht dies noch relativ langsam, während die Erweiterung von 2019 hier deutlich stärker zulegt. Von 2006 auf 2007 springen die Werte allerdings steil nach oben. Das gilt für alle drei Stufen des Korpus, aber auch hier ist der Anstieg im 2019-Korpus noch einmal deutlich ausgeprägter. Danach, zwischen 2008 und 2010 verhalten sich die Werte im Korpus auf einem hohen Niveau parallel zu den Werten der Buchproduktion und fallen mit dieser sogar leicht ab. Das ändert sich wiederum 2011-2013, wo wir einen weiteren Anstieg beobachten können, der diesmal sogar das Niveau der Buchproduktion übertrifft.  Wenn wir noch einmal auf Grafik 3 und 4 blicken, dann sehen wir dort, dass die Rohwerte für Goethe und zwei andere kanonisierte Autoren in den späten 1990er Jahren deutlich ansteigen, sie zugleich in der normalisierten Darstellung schon abfallen. Das bedeutet, dass bereits in der Phase von 1998-2006 ""andere"" Texte hinzugefügt wurden, deren Zusammensetzung anders war als das bisherige Korpus. Um welche Texte könnte es sich hierbei handeln? Die oben erwähnten verschiedenen Phasen der Veränderung legen es nahe zu vermuten, dass nicht ein einzelner Eingriff in das Korpus, sondern eine Reihe verschiedener Texthinzufügungen die beobachteten Phänomene bedingen. In einem anderen Projekt, in dem es um die Analyse von Heftromanen geht, war bereits aufgefallen, dass sich die einschlägigen Verlage der Komplexität der Feststellung der richtigen Metadaten, insbesondere des Publikationsdatums, dadurch entledigt haben, dass sie einfach alle retrodigitalisierten Texte unter dem Datum veröffentlichen, an dem die digitale Kopie publiziert wird. Um zu testen, ob diese Texte auch im Ngram-Korpus sind, wurden die entsprechenden Serienautoren und -helden gesucht:  Die deutliche Steigerung ab 2010 spricht dafür, dass auch hier die Retrodigitalisate unter dem jeweiligen Jahr aufgeführt sind. Allerdings können selbst einige Tausend Heftromane nicht alleine verantwortlich sein. Wie könnte man nun weitere Faktoren erkennen? Ein offensichtlicher Weg geht über das sprachliche Material. Die neuen Texte würden für bestimmte Worte zu einer relativen Erhöhung führen, selbst wenn viele andere Worte einen relativen Rückgang aufweisen. Da Google die Daten im 2019-Korpus mit Wortklasseninformation ausliefert, war es einfach, alle Substantive aus den 1-Grammen zu extrahieren, rd. 13 mio. Anschließend wurden die Substantive herausgefiltert, die von 1995 bis 2019 jedes Jahr auftauchen und zwar insgesamt mindestens 2 500 mal. Für diese restlichen rd. 350.000 Wörtern wurde für die Daten jedes Wortes eine lineare Regression berechnet und die Steigung der Geraden als Filterungsfaktor verwendet, um die rd. 250 Wörter zu finden, die in dieser Zeit den größten Anstieg verzeichnen. Eine manuelle Sichtung dieser Wortliste zeigte den Einfluss mehrerer Textsorten. Vor allem aber fiel der einzige Name in der Liste auf: GRIN. Es handelt sich um eine deutsche Verlagsgruppe, zu der neben dem GRIN-Verlag selbst u.a. auch die Webseite hausarbeiten.de gehört. Der Verlag, der von Beobachtern als ""vanity publisher"" oder ""predatory publisher"" (Shrestha 2021) eingeschätzt wird, publiziert alle Texte digital oder als Book on Demand. Eine ""Lektorierung findet nicht statt"" (Wikipedia). Laut Verlagswebseite wurden bis ins Jahr 2018 200.000 Texte publiziert. Seitdem sind schätzungsweise mindestens weitere 40.000 Titel publiziert worden.  Die Texte des GRIN Verlags haben zwar alle eine ISBN-Nummer, aber die meisten sind deutlich kürzer als Bücher im herkömmlichen Sinn, viele sind Aufsätze, die nur 20-30 Seiten lang sind. Beide Faktoren, die Unmenge an quasi-wissenschaftlichen kurzen Texten des GRIN-Verlags und die falsch datierten Heftromane führen dazu, dass die durchschnittliche Länge von Büchern im Ngramm-Korpus sich seit 2000 deutlich verringert hat (siehe Fig. 10), allerdings sind die Werte seit 2017 fast wieder auf dem alten Niveau:  Fassen wir zusammen: Nach 1998 ändert sich die Zusammensetzung des deutschsprachigen Ngramm-Korpus einschneidend, so dass es für die meisten Analysen zur Entwicklung von Sprache und Kultur weitgehend unbrauchbar wird. Dazu tragen eine Reihe von Faktoren bei, von denen zwei identifiziert werden konnten: Schwerwiegender ist, schon aus Umfanggründen, der Anteil der Publikationen des GRIN-Verlags. Sie geben zwar einen Einblick in eine bestimmte Form universitärer Wissenschaftskommunikation, haben aber nichts mit der sonstigen Buchproduktion zu tun. Hinzukommen die falsch datierten Retrodigitalisierungen einiger Verlage. Zugleich zeigt die Analyse der Daten, dass dies nicht die einzigen Faktoren sind, die hier ins Gewicht fallen. Wenn man sich auf die Wörter mit den steilsten Karrieren in den letzten 25 Jahren konzentriert (Fig. 11), dann fällt auf, dass dies allgemeine Token sind, die sich eher in Romanen als in Fachtexten finden (besonders die Anführungszeichen, mit denen in den meisten deutschen Drucktexten direkte Rede markiert wird): ""Augen Blick Du Frau Gesicht Hand Kopf Leben Mal Mann Moment Mutter Stimme Tag Tür Vater ¬´ ¬ª"" Da zugleich die Länge ansteigt und die Anzahl der Texte sehr hoch bleibt, außerdem diese Texte aber wohl nicht in den offiziellen Verlagsstatistiken auftauchen, handelt es sich vermutlich um Texte aus literarischen An diese explorative Studie könnte nun eine Untersuchung anschließen, die die Veränderungen der Korpuszusammensetzung als überdurchschnittlich starke Veränderung der Token-Verteilungen formalisiert (Koplenig 2017) und so den hier etwas vernachlässigten Aspekt, wann sich genau die Veränderungen ergeben, herausarbeitet. Die ""typische"" Verwendung des Ngramm-Korpus und -viewers, nämlich die Untersuchung der Verwendungshäufigkeit von Termen in der schriftlichen Öffentlichkeit, ist durch die starken Schwankungen in der Zusammensetzung des Korpus sehr fragwürdig geworden. Da nach 2000 sonst eher randständige Bereiche, quasi-wissenschaftliche Texte und die Produktion der selbstverlegten Autorinnen und Autoren, das Korpus dominieren, ist es auch für rein sprachanalytische Untersuchungen, etwa zur Kollokationsanalyse, kaum verwendbar. Aber insgesamt verdient die Frage, ob und unter welchen Vorzeichen die Daten nicht doch für bestimmte Analysen herangezogen werden können, eine genauere Untersuchung. "
2023,DHd2023,SCHENK_Nicolas_Vom_Heben_verborgener_Schätze___Literarische_.xml,Vom Heben verborgener Schätze 'Literarische Blogs als Ressource,"Nicolas Schenk (Deutsches Literaturarchiv Marbach); André Blessing (Universität Stuttgart, Institut für Maschinelle Sprachverarbeitung); Pascal Hein (Universität Stuttgart, Institut für Literaturwissenschaft); Jan Hess (Deutsches Literaturarchiv Marbach); Kerstin Jung (Universität Stuttgart, Institut für Maschinelle Sprachverarbeitung); Claus-Michael Schlesinger (Universität Stuttgart, Institut für Literaturwissenschaft)","Literarische Blogs, Aufbereitung, Ressource, Korpus, WARC","Sammlung, Strukturanalyse, Bereinigung, Literatur, Metadaten, Text","Bereits seit Ende der 90er Jahre gewannen Weblogs als Medium zur öffentlichen Darstellung unterschiedlicher Themen und Inhalte immer mehr an Popularität. Findige Literatur- und Kulturschaffende zögerten nicht lange, um das neue Medium auch für literarische Zwecke umzunutzen. In dem Beitrag zur Jahrestagung der DHd 2022 (Blessing et al 2022) haben die Autor:innen des vorliegenden Beitrags bereits am Beispiel des u. a. von der Autorin Kathrin Passig ins Leben gerufenen Techniktagebuch Typische Bausteine in Blogs sind Blogposts als (meist datierte) Inhaltseinheiten verschiedenster Länge und unter Verwendung unterschiedlicher Modalitäten wie Text, Bild, Animation, Video, Referenz (z. B. Hyperlinks), etc. Weiterhin finden sich auf der Ebene der Blogposts oft Kommentarformulare und Kommentare sowie eine Etikettierung von Einträgen in Form von Tags, die der Gruppierung und Beschreibung der Einträge dienen und sich auf Themen, Stimmungen, Autoren, etc. des Blogposts beziehen können (vgl. zu den Bausteinen von Blogs: Ernst 2010, 286f.).¬†Zusätzliche Elemente wie Übersichtsseiten, die als Einstiegsseiten der jeweiligen Blogs die einzelnen Blogposts (zusätzlich) in einer bestimmten Reihenfolge auflisten, oder Archivseiten, die den Nutzer:innen einen Zugang zu älteren Blogposts ermöglichen sollen, prägen die Struktur der Blogs und damit ggf. deren Analyse.  Die Spiegelungen der Blogs, die am DLA durchgeführt werden, erfolgen zunächst mit einem Crawling-Vorgang, der der Hyperlinkstruktur im Blog folgt und die clientseitig (wie durch einen Browser) empfangenen Daten gemeinsam mit einigen Metadaten zum Crawlingprozess im Web ARChive- oder kurz: WARC-Format ablegt. Das aus dem ARC-Format des Internet Archive weiterentwickelte Archivformat hat sich inzwischen als internationaler Standard für die Archivierung von Webinhalten etabliert¬†(IIPC, n.d.).¬†Beim Crawling können Inhalte, die ggf. nicht Teil des Blogs sind, wie z. B. zufällig eingebundene Werbeanzeigen oder externe Inhalte, auf die von diesen Werbeanzeigen verwiesen wird, Teil des Archivobjekts werden. Aber auch bezüglich der tatsächlichen Blog-Elemente sind im Archivobjekt Inhalte (z. B. bestimmte Textpassagen) so oft abgelegt, wie der Crawler ihnen auf verschiedenen Seiten begegnet ist. Der Inhalt eines Blogposts kann an verschiedenen Stellen für den Crawler erreichbar sein: auf der Übersichtsseite, auf der eigenen Seite des Posts, auf der Seite jedes Schlagworts, mit dem der Post versehen ist, im Archiv, etc. Aber auch Textpassagen aus Strukturelementen wie Kopf- und Fußzeilen führen zu mehrfachen Vorkommen von Textpassagen oder Begriffen. Werkzeuge, die Strukturelemente ausblenden und (Text-)Duplikate erkennen, sind daher bei der Vorverarbeitung der Daten für Analysen notwendig, arbeiten oft aber statistisch und müssen ggf. auf jedes zu untersuchende Blog neu angepasst werden, was im Spannungsfeld mit dem maschinell unterstützten Distant Reading steht. Im Folgenden wird daher das Vorgehen bei der Aufbereitung der Blogs beschrieben, das notwendig ist, um die Texte auch für quantitative Analysen verfügbar zu machen. Die am DLA archivierten Blogs sind nicht nur inhaltlich und stilistisch sehr heterogen, sondern auch in Bezug auf die technische Umsetzung. In den meisten Fällen werden bekannte Blog-Hoster wie wordpress (40%), twoday (15%), blogger, blogspot usw. verwendet. Diese Blog-Hoster wiederum setzen Content Management Systeme (CMS) ein, die die Blogpost-Erstellung, -Verwaltung sowie die Blogdarstellung sowohl für die Blogverfasser:innen, als auch für die Blogleser:innen vereinfachen. Für die Analyse des Inhalts eines Blogs sind nur die Blogposts relevant, da alle Übersichtsseiten (Home, Tags, Categories) eines Blogs automatisch daraus generiert werden. Darüber hinaus hat sich gezeigt, dass beim Erstellen von WARC-Crawls auch einiges an ""Beifang"", also Webseiten, die nicht zum Blog gehören, aber für das Abspielen teilweise nützlich sein könnten, mit-archiviert werden, was wiederum bei der Aufbereitung der Blogs beachtet werden muss. Die hier beschriebene Umsetzung zielt darauf ab, ein sauberes Textkorpus für jedes Blog zu extrahieren und alle irrelevanten und redundanten Inhalte zu ignorieren. Bei der Aufbereitung werden Verfahren aus dem Information Retrieval eingesetzt (vgl. Manning, Raghavan und Schütze 2008, 443–459). Das Besondere am aktuellen Datensatz liegt darin, dass die Daten bereits im WARC-Format vorliegen. Cormack, Smucker und Clarke (2011) haben bereits gezeigt, wie bei sehr großen Web-Crawls Die Umsetzung läuft in mehreren Schritten: In Schritt 1 werden alle HTML-Seiten des Blogs aus den WARCs extrahiert und erkannte Fremdinhalte entfernt. Schritt 2 erkennt das verwendete CMS-System. Schritt 3 basiert auf einem Regelsystem, das für jedes Blog die Posts ermittelt.¬†Die URL-Pfade der Blogs sind ein Hauptmerkmal, welches in den Regelsystemen verwendet wird. In Schritt 4 werden zu jedem Post Text- und Metadaten mittels trafilatura Da in allen vier dieser Umsetzungsschritte Anpassungen notwendig sind, gehen Weiterentwicklung und Optimierung der Aufbereitung immer einher mit dem Einsatz von Analysewerkzeugen. Bei der Verwendung solcher Werkzeuge können systematische Auffälligkeiten sichtbar werden, wodurch beispielsweise noch vorhandene Redundanzen zum Vorschein kommen. Ein Werkzeug, mit dem große Textkorpora durchsucht und auf grammatikalische sowie syntaktische Strukturen hin untersucht werden können, ist CQPweb  Das Durchsuchen und Abspielen von WARC-Dateien wird durch die Web-Applikation SolrWayback  Der Erfolg bei der Aufbereitung lässt sich anhand der Zahlen in Tabelle 1 ablesen. Hier wird auch die Heterogenität der Blogs sichtbar: Die Aufbereitungsschritte haben je nach Blog unterschiedlich starke Auswirkungen. Während der Unterschied der Anzahl von WARC-Records und HTML-Seiten deutlich macht, wie viele externe Inhalte und Nicht-HTML-Inhalte entfernt werden müssen, zeigt sich an der Differenz zwischen der Anzahl an HTML-Seiten und der Anzahl an Blogposts, wie hoch die Redundanz durch generierte Übersichtsseiten ausfällt. Die Zahlen zur Ressourcengröße aus Tabelle 1 veranschaulichen, dass die Aufbereitung der Blogs nicht vollständig manuell validiert werden kann. Trotzdem ist eine Aussage über die Qualität der aufbereiteten Blog-Daten wichtig. Zu diesem Zweck wurde ein Annotations-Jupyter-Notebook entworfen, mit dessen Hilfe automatisch aus den jeweiligen CMS-Familien wie beispielsweise Wordpress oder Blogger repräsentative Datenmengen entnommen werden, die anschließend von mehreren Annotator:innen bewertet werden. Im ersten Schritt geht es um das Erkennen der Blog-Posts in Abgrenzung zu Übersichtsseiten oder weiteren Seiten wie beispielsweise dem Impressum. Wenn es sich bei der Seite um einen Post handelt, dann wird im zweiten Schritt die Qualität der Metadaten- und Textextraktion bewertet. Dabei wird u. a. darauf geachtet, ob Datum und Überschrift richtig extrahiert wurden und ob der extrahierte Text vollständig ist oder beispielsweise Teile aus der Seitennavigation enthalten sind. Es hat sich gezeigt, dass die beschriebene manuelle Bewertung auch für Menschen nicht immer trivial ist. Daher ist der Annotationsprozess aktuell noch nicht abgeschlossen. Die Evaluationsergebnisse werden jedoch mit dem Release der Daten bereitgestellt. In Blessing et al (2022) wurde am Fallbeispiel des Techniktagebuchs bereits exemplarisch aufgezeigt, welche komplexeren computergestützten Analysen durch die Verwendung der extrahierten Text- und Metadaten vorgenommen werden können. Unter anderem wurde bereits untersucht, welche Zusammenhänge zwischen Inhalt 30 Millionen Zeichen, 140.000 Blogposts, über 200 Blogs: Schon wegen seiner Größe ist das hier vorgestellte, aufbereitete Korpus für viele Bereiche der Digital Humanities, beispielsweise die Computational Literary Studies, die Digital History oder für NLP-Untersuchungen, eine wichtige Quelle für Inhaltsanalysen oder das Trainieren von Sprachmodellen. Wie in diesem Beitrag gezeigt stellt vor allem die Struktur der Weblogs eine Herausforderung dar, enthalten die WARC-Dateien, in denen die Blogs zunächst vorliegen, doch sehr viel Redundantes, das für eine Vielzahl von Inhaltsanalysen nicht nur uninteressant, sondern sogar hinderlich ist. Mit den im Zuge der Veröffentlichung der SDC4Lit-Plattform 2023"
2023,DHd2023,GIOVANNINI_Luca_Onboard_onto_DraCor.xml,Onboard onto DraCor.   Prototyping Workflows to Homogenize Drama Corpora for an Open Infrastructure,Ingo Börner (Universität Potsdam); Frank Fischer (Freie Universität Berlin); Luca Giovannini (Universität Potsdam); Christopher Lu (University of Oxford); Carsten Milling (Universität Potsdam); Daniil Skorinkin (Universität Potsdam); Henny Sluyter-Gäthje (Universität Potsdam); Peer Trilcke (Universität Potsdam),"corpus, drama, onboarding","Annotieren, Bearbeitung, Community-Bildung, Kollaboration, Literatur, Text","Comparative endeavors in Computational Literary Studies typically require corpora which are both diverse, i.e., including texts in different languages and from different sources, and homogenized, i.e., formal and structural consistent. One way to tackle this issue is to establish upstream internal guidelines, such as the ones developed within the ELTeC initiative (Schöch et al. 2021). DraCor, based on the concept of Programmable Corpora (Fischer et al. 2019), is an open platform as well as a growing network for hosting, accessing, and analyzing theater plays. DraCor relies on the general TEI model for dramatic texts, with minimal enhancements, and thus facilitates contributions by external scholars who want to onboard their corpora onto its ecosystem. Once integrated, corpora can benefit from the platform""s APIs and services, ranging from the computation of network metrics via various extraction functions to SPARQL queries. Typically, corpora for DraCor are not built from scratch, but are created either by aggregating formally heterogeneous texts from different sources or by transforming existing corpora. Unlike in ELTeC, the homogenization of texts for DraCor usually does not stand at the beginning of the corpus creation process, but is rather an intervention in existing corpora which are sometimes subject to amendment and growth, hence ""living"". This approach poses a number of challenges, for which we are currently prototyping several workflows. Here, we present the pipelines for mounting to DraCor two new corpora: the English-language From a technical point of view, onboarding corpora onto DraCor is a series of automated and manual transformations of the source data, which depend crucially on the format and markup of the files. Texts from a single, homogeneous collection with pre-existing markup and metadata will require different workflows and pipelines than those coming, for example, from a variety of raw text sources. This heterogeneous point of departure is what shapes our onboarding approach. Consequently, we are developing a modular workflow made up of a set of demand-dependent components. In addition to guideline-based manual revisions (e.g. pre-structuring texts with Markdown), we use XSLT scripts for automated transformations. Edits specific to theater plays, such as the task of speaker identification, are supported by an Oxygen framework; A particular challenge is posed by living corpora. Here, the manual transformations performed during onboarding should be reapplicable in case of edits to the source data. Accordingly, we implemented routines for a ""backward compatibility"" of the markup: the changes made by us during onboarding can later be applied again to a newer version of the source files. To develop our workflows and pipelines, two corpora with very different requirements are currently in the process of onboarding. While UDraCor originates from a growing collection of heterogeneous sources, EPDraCor is based on semantically rich TEI files from the Early Print project. Due to the heterogeneity of the sources, a more case-specific solution must be found for UDraCor in this initial step. Here, the conversion is a semi-automatic procedure with heavy use of string patterns and regex. At the same time, UDraCor takes a community-based corpus-building approach by inviting scholars specializing in Ukrainian studies to work on both technical and content-related tasks. This work on UDraCor once again shows how the technical task of corpus building and community activities are crucially intertwined."
2023,DHd2023,FLÜH_Marie_Analyse__Produktion__Reflexion__Nachnutzungsszena.xml,"Analyse, Produktion, Reflexion: Nachnutzungsszenarien für Forschungsdaten am Beispiel der Daten des Projekts Dehmel digital","Sandra Bläß (Universität Hamburg, Deutschland); Marie Flüh (Universität Hamburg, Deutschland); Julia Nantke (Universität Hamburg, Deutschland); Christian Reul (Universität Würzburg, Deutschland)","Digitale Edition, HTR, Named Entity Recognition, Briefanalyse","Teilen, Datenerkennung, Sammlung, Bilderfassung, Bearbeitung, Daten","Das Ziel wissenschaftlicher Editionen besteht seit jeher in der Nachnutzung durch die (wissenschaftliche) Community. Unter den Vorzeichen von Open Data und Open Science ändern sich allerdings die Möglichkeiten der Bereitstellung und Nachnutzung des erschlossenen Materials. Gleichzeitig haben Wissenschaftler:innen, die mit Methoden der Digital Humanities arbeiten, andere Anforderungen und Bedarfe an bereitgestellte Daten z.B. im Hinblick auf den Umfang der Korpora und die spezifischen Datentypen. Ziel unseres Beitrags ist es, anhand der unterschiedlichen, von uns im digitalen Editions- und Forschungsprojekt Wir beziehen uns auf folgende Datentypen: 1) Metadaten von Briefen unterschiedlicher Schreibender aus dem Korrespondenznetz von Ida und Richard Dehmel, 2) digitale Bilder der Dokumente, 3) maschinenlesbarer Text der Briefe, 4) Annotationen von Entitäten sowie 5) algorithmische Modelle. In Abhängigkeit vom jeweiligen Datentyp ergeben sich unterschiedliche Nachnutzungsszenarien. Diese reichen von dem aus editorischer Sicht klassischen Szenario der Nutzung der bereitgestellten Daten in (literatur)wissenschaftlichen Analysen über die Nutzung zur Produktion eigener Korpora und Modelle, die dann wiederum Gegenstand der Nachnutzung werden können, bis hin zur Algorithmen-gestützten Reflexion der konzeptuellen Grundlagen einer solchen Datensammlung. Szenario 1: Analyse von Briefinhalten auf der Basis von Datentyp 3 Briefe stellen relevante Quellen für die Rekonstruktion historischer Diskurse dar (Baillot 2011). Diese in einem Gesamtüberblick und nicht nur in Einzelbeispielen zu erfassen, ist mittels Close Reading-Verfahren kaum zu bewältigen. Ein zentrales computergestütztes Nachnutzungsszenario für unsere Daten sind daher Analysen mittels Distant Reading-Verfahren. Auf der Basis der erzeugten Transkripte kann u.a. eine automatisierte Exploration der zentralen Briefinhalte über Topic Modeling umgesetzt werden (Andorfer 2017; Henny-Kramer/Neuber 2023). Ergänzend hierzu lassen sich z.B. stimmungsmäßige Gewichtungen in den Briefen durch Sentiment Analysis ermitteln und zu den Topics ins Verhältnis setzen. Szenario 2: Korrespondenznetze sichtbar machen auf der Basis von Datentyp 4 In den im Projekt Szenario 3: Vernetzung mit anderen Briefeditionen auf Basis von Datentyp 1 und 3 Die von uns erzeugten Briefmetadaten können über die Plattform Szenario 4: Texte erschließen auf der Basis von Datentyp 5 Neben den erschlossenen Dokumenten stellen wir auch die im Projekt von uns trainierten HTR- und NER-Modelle zur Nachnutzung zur Verfügung. Auf Basis dieses Datentyps können weitere Dokumente, die nicht Teil des Projekts sind, erschlossen und somit neue Daten für die weitere Nachnutzung produziert werden. Dies gilt zum einen für handschriftliche Dokumente der Schreibenden, für die wir HTR-Modelle trainiert haben (z.B. Stefan Zweig, Detlev v. Liliencron, Julie Wolfthorn). Zum anderen können die Named Entity-Classifier für die Erschließung weiterer deutschsprachiger Briefe aus einem ähnlichen Zeitraum genutzt werden. Es besteht auch die Möglichkeit, auf der Basis unserer Trainingsdaten spezifische Modelle für andere Anwendungsfälle nachzutrainieren (Flüh/Lemke 2022). Szenario 5: gemischte HTR-Modelle trainieren auf Basis von Datentyp 2 und 3 Die in Szenario 6: Reflexion der theoretischen Fundierung von Datensammlungen auf Basis von Datentyp 5 Unsere NER-Classifier wurden auf den Dokumententyp ""Brief um 1900"" trainiert. Eine experimentelle Anwendung z.B. des Orte-Classifiers auf ein Korpus mit Texten eines deutlich abweichenden Dokumententyps (z.B. fiktionale Texte) kann insbesondere in Kombination mit einem auf den Dokumententyp zugeschnittenen Classifier dazu beitragen, die theoretisch-konzeptuelle Fundierung offenzulegen, welche in die Modellierung des Classifiers eingegangen ist, indem die Ergebnisse der Classifier vergleichend betrachtet werden (vgl. dazu die Fallstudie von Flüh/Schumacher/Nantke im Erscheinen)."
2023,DHd2023,KONLE_Leonard_Gattungen_und_Emotionen_in_der_Lyrik_des_Reali.xml,Gattungen und Emotionen in der Lyrik des Realismus und der frühen Moderne,"Merten Kröncke (Universität Göttingen, Deutschland); Leonard Konle (Universität Würzburg, Deutschland); Simone Winko (Universität Göttingen, Deutschland); Fotis Jannidis (Universität Würzburg, Deutschland)","Emotion, Gattung, Lyrik, Realismus, Moderne","Inhaltsanalyse, Modellierung, Annotieren, Literatur, Text","Der literarische Wandel vom Realismus zur Moderne ist nach wie vor Gegenstand vielfältiger literaturwissenschaftlicher Debatten. Eine dieser Debatten betrifft die Frage, wie sich die Gestaltung von Emotionen in lyrischen Texten veränderte. Während einige typologisch argumentierende Forscher:innen (z. B. Zu bedenken ist, dass die damalige Lyrik keine homogene Einheit bildet. Aussagen darüber, inwiefern sich die Emotionsgestaltung ""der"" Lyrik veränderte, lassen sich differenzieren. Ein naheliegendes, wichtiges Differenzmerkmal lyrischer Texte 'und damit ein potentiell relevanter Einflussfaktor auf die Emotionsgestaltung 'ist die Gattung. Ziel dieses Beitrags ist deshalb, zu prüfen, inwiefern sich lyrische Gattungen in unserem Untersuchungszeitraum durch spezifische Emotionsprofile auszeichnen und ob die Entwicklung der Lyrik vom Realismus zur frühen Moderne unter der Perspektive der Emotionsgestaltung, die wir in¬† ( Das zu analysierende Korpus besteht aus Texten in Lyrikanthologien aus dem Untersuchungszeitraum, die sich auf Gedichte von Zeitgenoss:innen konzentrieren. Das Teilkorpus ""Realismus"" umfasst Gedichte aus Anthologien, die zwischen 1850 und den frühen 1880er Jahren publiziert wurden; das Teilkorpus ""Moderne"" enthält Texte aus Anthologien, die um 1900 erschienen sind und deren Herausgeber:innen die Gedichte aufgrund ihrer Modernität ausgewählt haben. Welche Texte als ""realistisch"" und welche als ""modern"" gelten, wird in diesem Beitrag also aus zeitgenössischer Sicht (und nicht aus Sicht heutiger Forscher:innen) modelliert. Tabelle 1: Korpus Statistik Für 1412 Korpustexte wurden die Gattungszugehörigkeit und die Emotionsgestaltung annotiert. Die Gattungsannotation hält fest, ob ein Text bestimmten im Untersuchungszeitraum relevanten thematischen Gattungen (Naturlyrik, Liebeslyrik usw.) sowie nicht-thematischen Gattungen (Ballade, Lied usw.) angehört und ob er situativ bestimmt oder situativ unbestimmt ist. Wenn im Folgenden von ""Gattungen"" die Rede ist, sind in aller Regel die thematischen Gattungen gemeint, auf die sich dieser Beitrag erst einmal konzentriert. Die Gattungszuordnung ist weder exklusiv noch zwingend: Während der Annotation konnten einem Text genau eine, aber auch keine oder mehrere Gattungen zugewiesen werden. Tabelle 2: Annotierte Gattungen Jedes Gedicht wurde zunächst von zwei Annotator:innen annotiert; anschließend haben die beiden Annotator:innen ihre Annotationen miteinander verglichen, die Disagreements diskutiert und eine Konsensannotation erstellt. Auf den Konsensannotationen beruhen alle weiteren Auswertungen. Das Agreement der Einzelannotationen beträgt 0.69 (Krippendorffs Alpha). Um Gattungslabel für das gesamte Korpus herstellen zu können, sollen Classifier trainiert und angewandt werden. Das Modell wird mit den ermittelten Hyperparametern verwendet, um binäre Classifier für alle übrigen Gattungen zu trainieren. Da die Klasse der nicht zur fokussierten Gattung gehörenden Gedichte in jedem Fall größer ist, wird epochenweise Random Undersampling angewandt. Für politische Lyrik wird kein Modell trainiert, da nicht genügend Beispiele vorhanden sind (siehe Tabelle. 2). Tabelle 4 zeigt die Qualität der trainierten Classifier. Da die Performance für die Gattungen Kultur und Poetologie nicht ausreicht, um solide Analysen zu ermöglichen, werden diese im Weiteren nicht behandelt und fallen, wie Politik, der Kategorie ""Sonstige"" zu, die daneben diejenigen Gedichte umfasst, die keiner thematischen Gattung zugeordnet wurden. Für Informationen zur Annotation von Emotionen und Modellen zur Emotionsdetektion siehe Verteilung von thematischen Gattungen in Realismus und Moderne. Absolute Anzahl über Balken. Die Kategorie ""Sonstige"" repräsentiert diejenigen Gedichte, die keiner der übrigen in dieser Abbildung gezeigten thematischen Gattungen zugeordnet wurden. 60% der untersuchten Gedichte konnten (mindestens) einer der fünf Gattungen Liebeslyrik, Naturlyrik, religiöse Lyrik, philosophische Lyrik oder Geschichtslyrik zugeordnet werden. Liebeslyrik und Naturlyrik sind im Korpus deutlich verbreiteter als religiöse Lyrik, philosophische Lyrik und Geschichtslyrik. Im Epochenvergleich werden nur begrenzte Verschiebungen sichtbar; die Rangfolge der Häufigkeiten bleibt konstant (siehe Abb. 1). Wie die Gattungen kommen auch die Emotionen im Korpus unterschiedlich oft vor: Emotionen der Gruppen Liebe, Freude und Trauer werden deutlich häufiger gestaltet als Emotionen der Gruppen Erregung/Überraschung, Angst und Wut. Die Häufigkeit positiver Emotionen 'Liebe und Freude 'nimmt zur Moderne hin ab Die Ergebnisse (Abb. 2) zeigen, dass Gattungen eigenständige Emotionsprofile ausbilden; dies gilt auch für die nicht durch Emotionen definierten Gattungen. Einige Gattungen verhalten sich in Hinsicht auf bestimmte Emotionen ähnlich, z.B. philosophische Gedichte und Geschichtslyrik in Hinsicht auf die Gruppen Freude und Trauer, weichen aber in anderen Emotionen stark voneinander ab. Während z.B. in Liebeslyrik 'wenig überraschend 'Emotionen der Gruppe Liebe dominant sind, werden in Geschichtslyrik Emotionen der Gruppen Liebe und Freude unter- und Emotionen der Gruppen Wut und Angst überproportional dargestellt. Ein kontrastiver Blick auf Gattungen unter der Perspektive von Epochen (Abb. 3) zeigt, dass sich (1) die lyrische Emotionsgestaltung in der frühen Moderne gegenüber dem Realismus verändert, und zwar (2) je nach Gattung in unterschiedlicher Weise. Der summarische Trend über alle Gattungen zu weniger Emotionen, verursacht vor allem durch den Rückgang positiver Emotionen in der Moderne (siehe Konle u.a. 2022), betrifft nicht alle Gattungen in gleicher Weise. Während sich die Ergebnisse zu Liebes-, Natur- und philosophischer Lyrik noch diesem Trend zuordnen lassen, entwickeln sich Geschichts- und religiöse Lyrik durch Zunahme negativer Emotionen eigenständiger. Religiöse Lyrik läuft dem Trend sogar durch ein vermehrtes Auftreten von Emotionen der Gruppe Liebe entgegen. Diese Befunde erlauben einen differenzierten Blick auf Emotionen, Gattungen und deren Veränderung in der frühen Moderne. Anschließend an unsere Ergebnisse stellt sich die Frage nach den Faktoren, welche die gattungsspezifischen Emotionsprofile zu unterschiedlichen Zeitpunkten beeinflussen. Für Geschichtslyrik bietet sich die These an, dass mit der Thematisierung von Konflikten und Feindseligkeiten Die Entwicklung der Emotionen in religiöser Lyrik ist noch komplexer, es nehmen nicht nur negative Emotionen zu, sondern auch die der Gruppe Liebe, bei gleichzeitigem Rückgang der Gruppe Freude. Wald Kirche Kaiser Bischof Segen Grab Priester Glocke Mönch Heil Werk Sieg Haus Friede Dom Andacht Lenz Sonntag Mahl Ehr Gott Seele Nacht Auge Mensch Tod Weib Licht Welt Stimme Traum Kreuz Brust Blut Volk Erde Herz König Kind Leib Freude Ausgeglichenheit Lust (nicht-sexuell) Aufregung Dankbarkeit Liebe Abneigung Sehnsucht Leid Lust (sexuell) Die gleichzeitige Beobachtung von Gattung und Emotion zeigt nicht nur, dass Gattungen wesentlichen Einfluss auf die Verteilung von Emotionen haben, sondern auch, dass der Übergang von Realismus zu früher Moderne innerhalb von Gattungen eigenen Dynamiken folgt. Im Fall religiöser Lyrik und Geschichtslyrik verlaufen diese Dynamiken sogar in Widerspruch zum Gesamttrend. Im Anschluss ergeben sich drei weitere Perspektiven: Wie sehen die Emotionsprofile der nicht-thematischen Gattungen aus? Welche anderen differenzierenden Faktoren neben der Gattung können wir identifizieren, z.B. Autorschaft, Publikationsort, Intertextualität usw.? Ein wichtiger Aspekt könnte die Zeit sein, bezogen auf kleinere Einheiten als Epochen: Wie entwickeln sich die Lyrik und die einzelnen Gattungen unter der Perspektive der Emotionsgestaltung in der Zeit, etwa von Jahrzehnt zu Jahrzehnt?"
2023,DHd2023,KETSCHIK_Nora__textklang____Ein_Mixed_Methods_Workshop_zu_Ly.xml,"""textklang"" 'Ein Mixed-Methods-Workshop zu Lyrik in Text und Ton","Nora Ketschik (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Toni Bernhart (Institut für Literaturwissenschaft, Universität Stuttgart, Deutschland); Markus Gärtner (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Julia Koch (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Nadja Schauffler (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland); Jonas Kuhn (Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Deutschland)","Lyrik der Romantik, Korpusexploration, Sprachsynthese","Sammlung, Kollaboration, Sprache, Methoden, Ton, Werkzeuge","Die Überlieferung von Texten ist vorwiegend an die schriftliche Form gebunden, die bis zur Erfindung von Tonaufnahmetechniken in der zweiten Hälfte des 19. Jahrhunderts die einzige Möglichkeit war, von Sprachbeiträgen nicht nur den Inhalt, sondern weitgehend auch die Darbietung festzuhalten. So haben sich literarische Traditionen und die wissenschaftliche Auseinandersetzung mit Literatur überwiegend entlang der schriftlichen Überlieferung entwickelt. Selbst bei Gattungen wie der Lyrik, in der Klang eine wichtige inhaltliche und ästhetische Rolle spielt (vgl. Richter et al. 2022), steht die Textform im Zentrum der kanonischen Überlieferung. Erst am Ende des 20. Jahrhunderts (u.a. angeregt durch die Sound Studies) haben sich in der Literaturwissenschaft Forschungsfelder zu Stimme, Klang, Akustik, Auditivität und Audioliteralität etabliert (Göttert 1998, Meyer-Kalkus 2001, Schulz 2018, Meyer-Kalkus 2020, Meyer-Sickendiek 2020). Bis auf wenige Ausnahmen (z.B. Rhythmicalizer, vgl. Meyer-Sieckendiek et al. 2017) folgen die Digital Humanities bislang recht stark dieser eingespielten Zugangsweise 'obgleich seit etwa 1900 unzählige Tonaufnahmen von Rezitationen vorliegen. Auch in der linguistischen Prosodieforschung und in der Sprachtechnologie wurde über die letzten Jahrzehnte ein Methodeninventar entwickelt, das eine sehr differenzierte Formulierung von Hypothesen zur Beziehung zwischen Text und lautlicher Realisierung erlaubt. Unser Workshop führt empirische Methoden aus der Phonetik mit aktuellen Technologien der Sprachsynthese und literaturwissenschaftlicher Forschung zur Lyrik der Romantik in einem Mixed-Methods-Workflow zusammen und bietet den Teilnehmenden auf diese Weise die Möglichkeit, das Wechselspiel von Textlichkeit und lautlicher Realisierung im Gedichtekorpus explorativ zu erkunden. Der Workshop knüpft an Arbeiten aus dem BMBF-geförderten Projekt ""textklang"" Das beim Workshop verwendete Forschungskorpus zur Lyrik der Romantik speist sich aus der Mediendokumentation des DLA Marbach, die etwa 2700 Audioaufzeichnungen verschiedener Sprecher*innen seit den 1920ern beherbergt. Diese werden im Zuge des Projekts digitalisiert und um die dazugehörigen Metadaten und Transkripte ergänzt; darüber hinaus werden Texte und Rezitationen mit automatisch erzeugten Annotationen angereichert (siehe Schauffler et al. 2022b für eine Übersicht). Aktuell umfasst das ""textklang""-Korpus 1261 Audioaufnahmen zu 786 Gedichten. Metadaten, Textdateien und lizenzfreie Audiodaten werden kontinuierlich über eine interaktive Webseite veröffentlicht. In unserem Workshop kommen alle Bereiche des Mixed-Methods-Workflows zum Einsatz, indem Ansätze aus traditionell sehr unterschiedlich arbeitenden Disziplinen zusammengeführt werden. Das Analysetool ICARUS (Gärtner et al. 2015) unterstützt den korpus- und textorientierten Zugang, bildet dabei aber neben morphosyntaktischen Annotationen der Texte auch die phonetischen Annotationen der Rezitationen ab. Hierfür kommen Verfahren aus der Phonetik zum Einsatz, die die Eigenschaften des Sprachsignals systematisch erfassen. Sprachtechnologische Verfahren der Signalanalyse und -manipulation ermöglichen es sodann, bestimmte Annahmen über ein Re-Synthese-Tool kontrolliert zu testen. Der Bedarf für ein so weit gefasstes Methodenspektrum folgt aus den Grundeigenschaften des Untersuchungsgegenstands selbst. Der Workshop leistet einen Beitrag, die fachspezifischen Ansätze methodologisch zusammenzuführen und auf diese Weise den insbesondere für Lyrik zentralen Zusammenhang von Text und Klang in den Blick zu rücken. Idee des Workshops ist, dass die Teilnehmenden ihre eigenen Fragestellungen an Rezitationen von Lyrik der Romantik mitbringen können und darauf aufbauend während der Datenexploration Hypothesen entwickeln. Alternativ können die von uns vorgeschlagenen Fragestellungen aufgegriffen werden. Im Workshop thematisieren wir mehrere Use-Cases aus dem Projektkontext, darunter die Realisierung paralleler Strukturen (z.B. Reim, Satzbau), die unter strukturellen, semantischen und melodischen Aspekten von Interesse sind. Eine andere Fallstudie untersucht unterschiedliche Realisierungen von Enjambements (Schauffler et al. 2022a), die im Spannungsfeld von Vers- und Satzstruktur stehen. In Rezitationen können Sprecher*innen die syntaktische Einheit betonen, die Versgrenze markieren oder einen Mittelweg wählen (vgl. Tsur und Gafni 2019). Ein weiterer Anwendungsfall, der exemplarisch etwas näher erläutert werden soll, beschäftigt sich mit Interjektionen. Interjektionen bezeichnen Ausrufe- oder Empfindungsworte (z. B. ""ach"", ""oh"", ""juchhe"") und stehen im Grenzbereich von Schriftlichkeit und Mündlichkeit (Wharton 2003, Liedtke 2019). Sie nehmen eine syntaktische Sonderrolle ein und werden in der Linguistik als eigenständige Klasse behandelt, den Partikeln zugeordnet oder als Satzäquivalente angesehen (Liedtke 2019). Sie tragen einerseits denotativ keine Bedeutung, bringen andererseits Emotionen verschiedenster Art und in unterschiedlichen Intensitätsgraden zum Ausdruck (Schwarz-Friesel 2013, 155-157). Mit dem hier vorgestellten Mixed-Methods-Ansatz soll der Spielraum und der besondere textlich-klangliche (Zwischen-)Status von Interjektionen untersucht werden. Dabei interessiert zum einen die syntaktische Stellung von Interjektionen, zum anderen ihr Bedeutungsspektrum sowie, als dritter Aspekt, ihre lautliche Ausprägung. Die ""Offenheit"" dieser Wortart legt die Hypothese nahe, dass die verschiedenen Ebenen sich gegenseitig beeinflussen können, beispielsweise das syntaktische Umfeld die lautlichen Realisierungen in der Rezitation prägt oder bestimmte klangliche Merkmale die Bedeutung von Interjektionen ausmachen. Die abgedruckten Beispiele deuten die syntaktisch-lautlichen Spielräume der Interjektion ""Ach"" im Gedichtekorpus an: Während sie im ersten Beispiel syntaktisch isoliert steht (markiert durch den Tonhöhenverlauf und die Sprechpause), wird sie im zweiten Beispiel syntaktisch und lautlich in den Satz integriert. Auch die mit dem ""Ach"" ausgedrückten Emotionen (im ersten Beispiel Schwermut, im zweiten Freude) changieren und werden 'neben dem semantischen Kontext des Wortes 'von der jeweiligen sprachlichen Realisierung beeinflusst. Mögliche Leitfragen für weitere Untersuchungen könnten sein: Welche syntaktischen Merkmale von Interjektionen gehen mit welchen lautlichen Merkmalen einher? Werden Interjektionen in gleicher (syntaktischer) Position lautlich parallel realisiert? Welche Varianz ist zwischen unterschiedlichen Sprecher*innen zu beobachten? Inwiefern beeinflusst die lautliche Realisierung die Bedeutung oder Wahrnehmung von Interjektionen? Für die Exploration und Visualisierung des Korpus mit allen Annotationsebenen verwenden wir ICARUS (Gärtner 2015) als Anfrageschnittstelle. ICARUS erlaubt eine gemeinsame Visualisierung von prosodischen Informationen und klassischen morphosyntaktischen Annotationen. Darüber hinaus können gezielt Anfragen unter Einbeziehung aller im Korpus verfügbaren Annotationsebenen gestellt werden, um Instanzen bestimmter Phänomene zu finden. An Annotationen stehen sämtliche für das GRAIN Korpus (Schweitzer et al. 2018) beschriebenen morphosyntaktischen und prosodischen Ebenen zur Verfügung. Darüber hinaus sind die Gedichte auch mit Markierungen zu Vers- und Strophenenden versehen, welche ebenfalls in Abfragen benutzt werden können. Je nach Entwicklungsfortschritt wird ICARUS als Desktop-Applikation Die durch die Datenexploration entwickelten Hypothesen über Zusammenhänge zwischen Text und lautsprachlicher Realisierung sollen in Perzeptionsexperimenten untersucht werden. Mittels Sprachsynthese erstellen wir zu diesem Zweck eine prosodische Replikation der Originalaufnahmen, wobei phonetische Details (z.B. Lautdauer, Tonhöhe) gezielt manipuliert werden können (Koch et al. 2022). Unser Synthesemodell basiert auf der Modellarchitektur von FastSpeech 2 (vgl. Ren 2021), für die Implementierung nutzen wir das open-source Toolkit IMS Toucan Wir beginnen den Workshop mit einer Einführung in den multimodalen Ansatz und adressieren die methodologisch wie wissenschaftstheoretisch relevante Frage, wie die Spezialisierungen der Fachgebiete innerhalb der DH sinnvoll zusammengeführt werden können. Anschlie√üend präsentieren wir mögliche Forschungsbeispiele und führen in die verwendeten Tools ein. In zwei Praxisrunden haben die Teilnehmenden die Möglichkeit, das Lyrikkorpus zu erforschen, eigene Forschungsfragen zu entwickeln sowie diese exemplarisch zu untersuchen. Dies kann individuell oder in Kleingruppen geschehen. Die erste Praxisrunde dient der Exploration des Korpus und der Entwicklung möglicher Hypothesen. Hierfür kommt das Tool ICARUS zum Einsatz, über das die Teilnehmer*innen die verschiedenen Annotationsebenen (u.a. morphosyntaktisch, phonetisch) sichten und komplexe Suchanfragen an die Texte modellieren können. Auf Grundlage der Annotationen zur Text- und Lautgestalt können Forschungsfragen entwickelt oder eine der vorgestellten Fragestellungen aus der theoretischen Einführung exploriert werden. Nach einer Zusammenschau der Hypothesen dient die zweite Praxisrunde dazu, ausgewählte Fragestellungen probeweise zu validieren, indem die Annahmen in das Sprachsynthesemodell überführt werden. Wenn beispielsweise die Annahme besteht, dass die Längung und die Tonhöhe einen Einfluss darauf haben, ob die ""bedeutungsfreie"" Interjektion ""Ach"" negativ oder positiv konnotiert ist, können ebendiese Merkmale in der Sprachsynthese gezielt modifiziert und die Effekte dieser Veränderungen getestet werden. Die Ziele des Workshops bestehen folglich darin, die Möglichkeiten des Mixed-Methods-Ansatzes auszuschöpfen und Lyrik in ihrer Multimodalität erforschbar zu machen. Dabei liegt ein besonderer Schwerpunkt darauf, zu zeigen, wie fruchtbar das Zusammenspiel von textlicher und klanglicher Ebene sein kann. Zwar können die zu behandelnden Fragestellungen im Rahmen des Workshops nur ansatzweise durchgespielt werden, sie können dabei aber die Potenziale des interdisziplinären Ansatzes offenlegen. Unser Workshop ist für ca. 20 Teilnehmer*innen geeignet und richtet sich an Interessierte aus den digitalen Geisteswissenschaften. Bestimmte technische Vorkenntnisse sind nicht erforderlich. Die Teilnehmenden arbeiten an ihren eigenen Laptops. Ausreichend Steckdosen, stabiles Wifi und ein Beamer sollten vorhanden sein. Installationshinweise werden im Vorfeld an die Teilnehmer*innen verschickt. Nora Ketschik (Institut für Maschinelle Sprachverarbeitung (IMS), Universität Stuttgart, Toni Bernhart (Institut für Literaturwissenschaft, Universität Stuttgart, Markus Gärtner (IMS, Universität Stuttgart, Julia Koch (IMS, Universität Stuttgart, Nadja Schauffler (IMS, Universität Stuttgart, Jonas Kuhn (IMS, Universität Stuttgart,"
2023,DHd2023,HAIDER_Thomas_Nikolaus_Barockpoetik_als_Wikibase__Eine_Daten.xml,Barockpoetik als Wikibase: Eine Datenbank zu konfessions- geschichtlichen Aspekten in deutschen Barockpoetiken,"Thomas Nikolaus Haider (Universität Göttingen, Deutschland); Stephanie Schennach (Universität Göttingen, Deutschland); Julius Thelen (Universität Göttingen, Deutschland); Jörg Wesche (Universität Göttingen, Deutschland)","Wikibase, Poetik, Barock, Konfession, Religion, Linked Open Data","Modellierung, Organisation, Literatur, Metadaten, Text","Dieses Poster stellt eine Datenbank vor, die sich¬† auf die thesaurierende Erschließung sämtlicher Konfessionsaspekte in den deutschen Poesielehrbüchern der Barockzeit richtet. Das barocke Poetikparadigma, das sich zeitlich von Opitz"" Die Datenbank entsteht im Rahmen des Teilprojekts ""Uneindeutige Barockdichtung. Poetische und konfessionelle Ambiguität in Schlesien als kulturdynamische Faktoren einer neuen deutschen Dichtkunst (1620 bis 1742)"" der DFG-Forschungsgruppe ""Ambiguität und Unterscheidung. Historisch-kulturelle Dynamiken"", das Prof. Wesche gegenwärtig an der Universität Göttingen leitet. Die Wikibase ist zu finden unter Im Fokus steht, die Modellierung der Daten so vorzunehmen, dass die Inhalte dynamisch abrufbar sind. Dies wurde umgesetzt durch eine dedizierte Wikibase, die unsere spezifischen Inhalte im Stil von Wikidata darstellt und per Volltextsuche und einem SPARQL Query Interface zugänglich macht. Die Daten sind dabei als ""Items"" und ""Properties"" organisiert, wobei Items als Knoten und Properties als Kanten in einem Graph verstanden werden können, was es uns erlaubt beliebige Beziehungen im Graphen darzustellen und zu suchen. Zentrale Elemente der Datenbank sind Autoren und ihre Werke, welche als Items gespeichert wurden. Siehe etwa das Item:Q29 ( Das assoziierte Werk von Opitz, Einzelne Textstellen sind dabei als Items angelegt, um sie mehrfach verschlagworten zu können (wobei eine Textstelle unter verschiedenen Schlagworten geführt werden kann). Zudem besitzt jede Textstelle eine Angabe zu ihrer Fundstelle, um sie im Digitalisat ausfindig zu machen. Die Volltextsuche erlaubt es, nach bestimmten Inhalten zu suchen. Etwa ergibt eine Suche nach dem Wort ""Mensch"" alle Textstellen, in denen dieses Wort vorkommt, oder eine Suche nach ""[aq]"" liefert alle Textstellen mit einer Auszeichnung für die Schriftart ""Antiqua"", welche in frühneuzeitlichen Drucken konventionell für fremdsprachliches Material verwendet wird. Diese Textstellen können wiederum durch eine ""Inverse Suche"" dem jeweiligen Werk zugeordnet werden. Der SPARQL Endpoint erlaubt es uns, beliebige Daten zu extrahieren, wie etwa eine Übersicht der Autoren mit ihren Werken, oder zum Beispiel alle Werke, die ""Exempelpolitik"" enthalten, mit ihren Autoren, und den entsprechenden Textstellen, sortiert (oder gefiltert) nach Publikationsjahr. Die Datenbank leistet im Bereich der Geschichte der Poetik als einem zentralen Forschungsgebiet der germanistischen Literaturwissenschaft einen substantiellen Beitrag zur Exploration aktueller Methoden der Linked Open Data (Chiarcos et al., 2022; Sturgeon, 2022) und der Nutzung von Wikidata in den Digital Humanities (Zhao, 2022) in einem (frühneuzeit-)historischen Forschungsgebiet, indem es nicht nur um die Digitalisierung und (forschungs-)öffentliche Bereitstellung von Textdaten geht, sondern diese zugleich mit einem auf Fragen der Konfessionalität gerichteten Erkenntnisinteresse digital aufbereitet und empirisch ausgewertet werden. Übergeordnetes Forschungsziel ist es dabei, von der Universität Göttingen aus ein erweiterbares Portal ""Barockpoetik digital"" zu etablieren, in dem die Forschungsdaten zum Thema zentral gebündelt und verfügbar gehalten werden. Als konkrete Erweiterungsperspektive erschließt das Team derzeit im Rahmen des DFG-Schwerpunkprogramms ""Übersetzungskulturen der Frühen Neuzeit"" sämtliche Aspekte, die im Poetikkorpus translationsgeschichtlich relevant sind."
2023,DHd2023,PICHLER_Axel_Urheberrechtlich_geschützte_Texte_nachnutzen___.xml,Urheberrechtlich geschützte Texte nachnutzen 'Der XSample-Workflow,"Melanie Andresen (Universität Stuttgart, Deutschland); Markus Gaertner (Universität Stuttgart, Deutschland); Janina Jacke (Universität Stuttgart, Deutschland); Nora Ketschik (Universität Stuttgart, Deutschland); Axel Pichler (Universität Stuttgart, Deutschland)","Forschungsdaten, Textauszüge, Nachnutzung","Archivierung, Veröffentlichung, Daten, Methoden, Forschungsprozess, virtuelle Forschungsumgebungen","Eines der Hindernisse, die der freien Weitergabe von Forschungsdaten im Sinne der Open-Science-Bewegung im Wege stehen, ist das Urheberrecht (UrhG). Dieses erschwert die Einhaltung der guten wissenschaftlichen Praxis und der FAIR-Prinzipien (Wilkinson et al. 2016) insbesondere bei Forschung zu zeitgenössischen Texten. Für urheberrechtlich geschützte Texte besteht bisher nur die Möglichkeit, sog. abgeleitete Textformate (Schöch et al. 2020) für ""non-consumptive access"" (Organisciak und Downie 2021) zu veröffentlichen, etwa in Form von Frequenzlisten. Geisteswissenschaftliche Forschungsprojekte sind allerdings häufig auf die Verfügbarkeit von textuellem Kontext angewiesen, der erst eine angemessene Interpretation der Daten erlaubt. Um die Nachnutzbarkeit von Textdaten in dieser Hinsicht zu unterstützen, wurde im Projekt XSample Im deutschen UrhG unterliegen Texte bis 70 Jahre nach dem Tod der Autor*innen dem urheberrechtlichen Schutz, der die Vervielfältigung und die öffentliche Zugänglichmachung von Texten erheblich einschränkt. Hiervon sind vor allem zeitgenössische Texte betroffen, die aus diesem Grund womöglich gar nicht erst als Gegenstand von Forschungsprojekten in Erwägung gezogen werden. Seit 2018 ist zumindest die Verwendung von urheberrechtlich geschützten Texten zu Zwecken des Text- und Dataminings durch ¬ß60d¬†UrhG legitimiert (vgl. Raue 2021). Die Nachnutzung der Daten nach Abschluss eines Projektes ist aber weiterhin nur unklar geregelt (vgl. Kleinkopf et al. 2021, Andresen et al. 2022). Der XSample-Workflow kombiniert den ¬ß60d¬†UrhG mit ¬ß¬†60c Abs. 1 Nr. 1 UrhG, der es erlaubt, bis zu 15 Prozent von Werken und auch vollständige Werke geringen Umfangs zu Zwecken der nicht-kommerziellen wissenschaftlichen Forschung zu vervielfältigen und an bestimmt abgegrenzte Personenkreise für deren eigene wissenschaftliche Forschung weiterzugeben. Die Weitergabe von nur 15 Prozent eines Textes erscheint auf den ersten Blick nicht hinreichend. Um die Nützlichkeit dieser Textauszüge zu maximieren, wird im XSample-Workflow über eine Benutzeroberfläche eine gezielte Textauswahl unterstützt. So können Forschende die Textauswahl genau auf die eigenen Forschungsanliegen zuschneiden. Dafür werden auch im Korpus enthaltene Annotationen berücksichtigt, sodass bei Interesse an einem bestimmten Phänomen systematisch die mit den relevanten Kategorien annotierten Textstellen extrahiert werden können (vgl. Gärtner 2020). Die Möglichkeiten und Grenzen des Auszugskonzepts sind am Beispiel zweier Anwendungsfälle aus der Literaturwissenschaft und Linguistik erprobt worden (vgl. Andresen et al. 2022). Sie zeigen beispielsweise, dass viele Forschungsfragen davon profitieren, wenn abgeleitete Textformate, die quantitative Analysen auf dem Gesamtkorpus ermöglichen, und Auszüge, die die qualitative Interpretation erlauben, kombiniert werden. Abb. 1: Der XSample-Workflow im Überblick. Abbildung 1 fasst das Auszugskonzept zusammen: Forscher*innen übermitteln ihre (annotierten) Forschungsdaten an Forschungsinfrastruktureinrichtungen (z.¬†B. wissenschaftliche Bibliotheken), die diese verwalten. Nachnutzer*innen können Zugriffsanfragen an die Infrastrukturbetreiber stellen, um Auszüge aus den Forschungsdaten zu erhalten. Der in Abbildung 1 beschriebene Workflow wurde im Rahmen des XSample-Projekts prototypisch in Form eines Webservices implementiert. ""Open Access"" ist und bleibt die Zielvorstellung für offene und reproduzierbare Forschung auch in den digitalen Geisteswissenschaften. Das hier vorgestellte Auszugskonzept stellt demgegenüber eine ""Behelfslösung"" dar, um die Nachnutzung urheberrechtlich geschützter Daten zu ermöglichen und der Tendenz entgegenzuwirken, diese Texte per se aus Forschungsprojekten auszuschließen (vgl. Gärtner et al. 2021). Die Lösung ist an der Forschungspraxis der digitalen Literatur- bzw. Geisteswissenschaften ausgerichtet, für die andere ""verfremdende"" Verfahren wie abgeleitete Textformate (Schöch et al. 2020, Organisciak und Downie 2021) nur eingeschränkt nachnutzbar sind. Das Auszugskonzept ermöglicht hingegen eine größere Nähe zum Text, indem die ursprüngliche Textgestalt beibehalten wird, die für die Interpretation der Daten häufig unabdingbar ist. Darüber hinaus wird der rechtliche Rahmen durch die individuelle Auszugsgenerierung optimal ausgeschöpft und den individuellen Forschungsinteressen angepasst."
2023,DHd2023,DOMINIK_GERSTORFER_Dominik_Konflikte_als_Theorie__Modell_und.xml,"Konflikte als Theorie, Modell und Text 'Ein kategorientheoretischer Zugang zur Operationalisierung von Konflikten","Dominik Gerstorfer (TU-Darmstadt, Deutschland); Evelyn Gius (TU-Darmstadt, Deutschland)","Kategorientheorie, Modellierung, Operationalisierung, Theoriebildung","Inhaltsanalyse, Modellierung, Annotieren, Theoretisierung, Literatur, Text","Für eine theoriegeleitete Analyse von Texten muss man eine Operationalisierung finden, die die theoretischen Konzepte im Text identifizierbar und damit messbar macht. Dies gilt in nicht-digitalen Forschungskontexten gleichermaßen wie in den Digital Humanities, allerdings ist in letzteren das Problem virulenter. Dies hat Moretti (2013) bereits prominent festgestellt. Jenseits von konkreten Fragestellungen (wie etwa von Moretti selbst oder in Fischer & Trilcke 2016) fehlen allerdings generell umsetzbare Vorschläge. In diesem Beitrag wollen wir an einem literaturwissenschaftlichen Anwendungsfall zeigen, wie die angewandte Kategorientheorie Operationalisierung als einen deutlich(er) definierten Workflow ermöglicht, mit dem man von einer theoretischen Grundlage über ein Modell zur Textanalyse –¬†und zurück 'kommen kann. Durch den theorieorientierten Fokus der operationalisierten Konzepte, ihre höhere Granularität sowie ihre Kompositionalität bietet der Workflow zudem eine Reihe von Vorteilen, die computationelle Analysen zugleich besser und einfacher machen können. Um den Anforderungen sowohl der geisteswissenschaftlichen als auch der informatischen Komponenten der Digital Humanities gerecht zu werden, ist es nötig, einen möglichst flexible und abstrakte 'd.h. inhaltsagnostische 'Grundlage zu finden. Wir greifen bei unserem Unterfangen auf die angewandte mathematische Kategorientheorie zurück, da sie die Möglichkeit bietet, sowohl die Explikation und Modellierung geisteswissenschaftlicher Fragestellungen zu unterstützen als auch, die Anschlussfähigkeit an informatische Methoden zu gewährleisten (Ehrig 2001). Die Kernidee der mathematischen Kategorientheorie ist es, beliebige Strukturen als Sammlungen von Objekten und ihren wechselseitigen Beziehungen zueinander zu charakterisieren. Im einfachsten Fall besteht eine Kategorie Durch wechselseitiges Ersetzen 'markiert durch einen Asterisk (*) 'von Morphismen oder Objekten durch Strukturen können Abstraktionsebenen integriert oder Modellierungen mit mehr Details angereichert werden: Der ursprüngliche Zweck der mathematischen Kategorientheorie war der Vergleich mathematischer Theorien (Mac Lane 2010), um so strukturelle Öhnlichkeiten zwischen unterschiedlichen mathematischen Disziplinen zu entdecken und sie zu vereinheitlichen. Um dies zu ermöglichen, werden Objekte rein formal, d.h. unter Absehung des konkreten Inhalts, betrachtet, was es uns erlaubt, auch nicht-mathematische Gegenstände zu modellieren. Hinzu kommt, dass dieser hohe Abstraktionsgrad die Abbildbarkeit verschiedener Theorien 'oder in unserem Fall: Konzepte 'aufeinander und damit auch ihren Vergleich ermöglicht. Wir verwenden Elemente der angewandten Kategorientheorie als Gerüst, um klar und präzise darzustellen, worüber wir sprechen. Diese Darstellung ist nicht reduktionistisch in dem Sinne, dass wir behaupten, Literaturtheorie könne letztlich durch mathematische Strukturen ersetzt werden. Vielmehr zielen wir darauf ab, literarische Konzepte zu explizieren (vgl. Carnap 1945; Dutilh Novaes 2017) und zu operationalisieren. Mit Operationalisierung ist hier lediglich der Prozess oder Workflow gemeint, um Konflikte in literarischen Texten zu identifizieren und zu analysieren (vgl. Pichler & Reiter 2021). Diese Form der Operationalisieren soll nicht mit Operationalismus verwechselt werden, d.¬†h. der streng positivistischen Vorstellung von Operationalisierung, die Bedeutung mit empirischen Operationen gleichsetzt. Ziel dieses Beitrags ist es, ein Framework zur Operationalisierung von Analysekonzepten am Beispiel von literarischen Konflikten zu skizzieren. Dieses Framework besteht im wesentlichen aus drei Formalisierungs- bzw. Abbildungsschritten. Hierzu entwickeln wir einen der mathematischen Kategorientheorie entlehnten Formalismus, der es erlaubt drei Ebenen zu integrieren: 1. Theorie, 2. Modell und 3. Text. Wir zeigen die Schritte im Folgenden am Beispiel einer Analyse von Konflikten in literarischen Texten. Die Aufgabe einer Theorie literarischer Konflikte ist es, festzulegen, unter welchen Bedingungen ein Konflikt vorliegt. Nun gibt es zahlreiche Möglichkeiten, Konfliktkonzepte in der literaturwissenschaftlichen Textanalyse zu nutzen. Für diesen Beitrag fokussieren wir uns auf eine Analyse von Konflikten zwischen Figuren und nutzen ein Konzept aus den Sozialwissenschaften, welches bereits in Gius (2015) im Rahmen einer narratologischen Analyse erprobt wurde. Im Sinne des Um diese informelle Definition zu operationalisieren und einen Konflikt Glasl charakterisiert zwei Beziehungen zwischen den Aktoren, erlebte Unvereinbarkeit  Zwischen den zwei Aktoren gibt es mindestens eine Beeinträchtigung des Verwirklichens  Des Weiteren soll sich die Unvereinbarkeit Ein Konflikt An diesem Diagramm ist eine für die literaturwissenschaftliche Analyse schwerwiegende Einschränkung der Theorie Glasls zu sehen: Die Beobachtung des Konflikts ist in der Beziehung zwischen den Aktoren versteckt, d.h. Erzähl- bzw. Beobachtungsinstanzen kommen nicht explizit vor. Dies ist dem Umstand geschuldet, dass Glasl Konfliktbegriff aus den Sozialwissenschaften stammt und entsprechend, anders als in literarischen Texten, die Konfliktparteien zu ihrer Einschätzung befragt werden können. Da es das Ziel unserer Operationalisierung ist, den Konfliktbegriff so zu explizieren, dass auch komplexe Erzählsituationen erfasst werden können, erweitern wir Im paradigmatischen Fall des von Der erweiterte Konfliktbegriff umfasst nunmehr als Objekte Ein Modell eines Konflikts liegt dann vor, wenn sich in literarischen Texten Kandidaten für Objekte und Morphismen finden, sodass es eine strukturerhaltende Abbildung auf den oben explizierten, erweiterten Konfliktbegriff  Figurenkonstellationen und Beobachtungs- bzw. Erzählinstanzen können aufgrund ihrer Kombinierbarkeit sehr flexibel modelliert werden; ebenso lassen sich weitere Konflikttheorien integrieren, die auf andere Aspekte abstellen, und weitere Objekte, wie gesellschaftlichen Wandel, oder andere Relationstypen zwischen den Objekten einführen. So könnte man Dahrendorfs (1972) Unterscheidung zwischen latenten und manifesten Konflikten durch zusätzliche Bedingungen integrieren, u.a. dadurch, dass beide oder keine der Konfliktparteien den Konflikt wahrnehmen. Oder man kann Definitionen psychischer Konflikte integrieren, indem man u.a. die Identität Auf der Textebene gilt es jene Textphänomene zu bestimmen, welche auf die Figurenkonstellationen und Erzählinstanzen abgebildet werden können. Diese sollen schließlich in der Analyse manuell oder automatisch identifiziert werden. Es geht also um die Bestimmungen von am Konflikt beteiligten Figuren und ihn wahrnehmenden Instanzen und die weiteren, zu Unvereinbarkeiten bzw. wahrgenommener Einschränkung führenden Aspekte. Für jeden Aspekt muss in einer Analyse festgelegt werden, wie er im Text realisiert werden kann. Die Operationalisierung auf der Textebene hängt auch vom anvisierten Untersuchungsmodus ab und kann auf sehr unterschiedliche Weisen erfolgen. So kann es sinnvoll sein, die Figuren als per NER-Analyse erkennbare Personen-Entitäten zu fassen 'oder diese entsprechend zu erweitern –, wenn man eine automatisierte Analyse anstrebt. In einer manuellen Analyse ist es vermutlich eher möglich, Figuren als komplexere Phänomene zu fassen und auch Charakterzüge u.ä. zu annotieren. Bei der wahrgenommenen Unvereinbarkeit könnte der Fokus auf repräsentierte Prozesse des Denkens und Fühlens etc. liegen, die zumindest teilweise automatisch erkannt werden können. Oder es könnte eine Sentimentanalyse in Abhängigkeit von den Erzählinstanzen und den beteiligten Figuren durchgeführt werden, bei welcher automatische und manuelle Verfahren kombiniert werden können. Eventuell bietet es sich auch an, Indikatoren für Konflikthaftigkeit herauszuarbeiten. Unabhängig von den gewählten Phänomenen und ihrer Bestimmung im Text gilt: Durch Gruppierung von Textphänomenen in hierarchischen Tagsets und Kategoriensystemen können Texteigenschaften der Modellebene zugeordnet werden (vgl. Abb. 2, in der Das hier vorgeschlagene Framework erlaubt es, den Workflow für die Operationalisierung auf jeder der drei Ebenen zu beginnen und bei Bedarf auf eine der anderen Ebenen zu wechseln. Es gibt keine starre Beschränkung des Workflows auf top-down, middle-out oder bottom-up. Vielmehr ist es je nach Fragestellung möglich, in die Tiefe zu gehen und mehr Daten und Details einzuarbeiten oder zu abstrahieren, um größere Strukturen sichtbar zu machen. Das bedeutet, dass die genutzte Konfliktdefinition ebenso verändert werden kann wie die Umsetzung der Objekte und Relationen für literarische Texte (wie wäre es etwa, wenn man die Definition auf eine Analyse von poetologischen ‚ÄúKonflikten‚Äù zwischen Autor:innen ausweiten wollte?) oder deren Realisierung auf der Textebene. Dabei kann die Önderung auf einer der Ebenen jeweils Anpassungen auf den anderen Ebenen nötig machen. Mit dieser Formalisierung eines Konfliktbegriffs für die Literaturwissenschaft und seiner skizzierten √úberführung in ein Modell, welches in der Textanalyse genutzt werden kann, haben wir gezeigt, wie man ausgehend von einer Theorie zu ihrer Anwendung auf Texte gelangen kann. Der Prozess ist –¬†wie die angewandte Kategorientheorie selbst auch –¬†ein allgemein anwendbarer Workflow für die Operationalisierung von Analysekonzepten für eine quantifizierende und/oder computationell-algorithmische Analyse. Die angewandte Kategorientheorie ermöglicht dabei die in den Digital Humanities im Bereich von computationellen Analysen zwingend notwendige Formalisierung mit Fokus auf die Theorie. Insgesamt sehen wir vier substanzielle Vorteile in diesem Workflow:  "
2023,DHd2023,JACKE_Janina_Vom_sprachlichen_Indikator_zum_komplexen_Phänom.xml,Vom sprachlichen Indikator zum komplexen Phänomen?,"Janina Jacke (Georg-August-Universität Göttingen, Deutschland)","Operationalisierung, Literaturwissenschaft, Komplexität, Theorie, unzuverlässiges Erzählen, Modellierung","Entdeckung, Beziehungsanalyse, Modellierung, Theoretisierung, Literatur, Text","Im Feld der Digital Humanities hat sich die computationelle Literaturwissenschaft als Teildisziplin etabliert. Von literaturwissenschaftlicher Seite wird allerdings immer wieder angemahnt, die computationelle Auseinandersetzung mit Literatur sei zu reduktionistisch 'unter anderem weil Textanalysen nur statistisch-deskriptiv und weitgehend kontextfrei möglich seien (vgl. Gius/Jacke 2022). Derartigen Bedenken lässt sich auf unterschiedliche Weise begegnen. Eine Möglichkeit besteht in der Akzeptanz, dass die computationelle und die traditionelle Literaturwissenschaft schlichtweg unterschiedliche Fragen an Texte stellen. Soll die Idee eines Brückenschlags zwischen traditioneller und computationeller Literaturwissenschaft dagegen nicht verworfen werden, gibt es zwei weitere Möglichkeiten: Es kann der ambitionierte Versuch unternommen werden, auch für komplexere literaturwissenschaftliche Fragestellungen (vollständig) computationelle Lösungen zu finden. Solche Versuche sind zwar wünschenswert, sollten aber (durch den großen Aufwand und die oft eingeschränkten Erfolgsaussichten) nicht die einzige Möglichkeit darstellen, computationelle Modellierung für traditionelle literaturwissenschaftliche Fragen fruchtbar zu machen. Der vorliegende Beitrag stellt eine andere Möglichkeit vor, wie ein Brückenschlag aussehen kann: Es kann es fruchtbar sein, dezidiert nur eine computationelle Teil-Operationalisierung komplexer literarischer Phänomens anzustreben und möglichst genau zu ergründen und zu explizieren, welcher Status den entwickelten computationellen Analysemethoden im Zusammenhang mit komplexeren literarischen Phänomenen und Fragestellungen zukommt. Computationelle Modelle können deskriptiv-quantitativ Textmerkmale feststellen, die als Indikatoren für komplexere literaturwissenschaftlich interessante Phänomene verstanden werden können. Sinnvoll verwertbar sind solche Analysen, wenn das Indikationsverhältnis (also die genaue Beziehung zwischen Indikator und komplexem Phänomen) spezifiziert wird. Dabei geht es zum einen um die (aus literaturwissenschaftlicher Perspektive nachvollziehbare) Die vorgeschlagenen Ideen zur Im Folgenden sollen zunächst kurz die für den vorgestellten Ansatz zentralen Begriffe ""Operationalisierung"" und ""Komplexität"" diskutiert werden (Abschnitt 2). Im Anschluss wird die vorgeschlagene Analyse des Indikationsverhältnisses genauer expliziert (Abschnitt 3), die auf einer theoretischen Analyse des Unzuverlässigkeitskonzepts sowie ersten Annotations- und Analyseerfahrungen im Rahmen von CAUTION basiert. Die Operationalisierung geisteswissenschaftlicher Konzepte für die computationelle Textanalyse ist bereits zum Thema extensiver Auseinandersetzung geworden (vgl. Moretti 2013, Döring/Bortz 2016, Pichler/Reiter 2021, Krautter et al. im Erscheinen). Unter der Operationalisierung von Begriffen ist die Angabe von Handlungsschritten zu verstehen, die ausgeführt werden müssen, um das Phänomen identifizieren (bzw. messen und quantifizieren) zu können. Im Feld der Digital Humanities wurde der Begriff von Moretti eingeführt (vgl. Moretti 2013). Seine literaturwissenschaftlichen Beispiele zeigen aber, dass die operationalisierten Modelle oft nicht oder nur lose an die zu operationalisierenden literaturwissenschaftlichen Konzepte angeknüpft sind (vgl. Krautter/Pichler/Reiter im Erscheinen). Es liegt der Schluss nahe, dass viele literaturwissenschaftliche Konzepte zu komplex sind, um sich unter Erhaltung ihrer ursprünglichen Bedeutung und Funktion vollständig und eindeutig operationalisieren bzw. computationell modellieren zu lassen. Dennoch scheinen die bisherigen Beiträge nicht grundsätzlich von dem Ziel Abstand zu nehmen, mit computationellen Operationalisierungen vollständige Übersetzungen bzw. direkte Entsprechungen komplexer Konzepte zu entwickeln. Reduzierte Ansprüche lassen sich lediglich insofern feststellen, als im Falle umstrittener Konzepte eine begründete Auswahl aus dem Definitionsangebot (vgl. Döring/Bortz 2016, 226) oder möglicher Kontexte (vgl. Pichler/Reiter 2021, 6) getroffen wird. Grundsätzlich wird aber davon ausgegangen, dass durch die Aufgliederung in Teilschritte (vgl. Pichler/Reiter 2021, 19–23) bzw. die Kombination unterschiedlicher Indikatoren (vgl. Döring/Bortz 2016, 229) die entwickelten Modelle direkt auf die komplexen geisteswissenschaftlichen Phänomene zielen. Es wird nicht in Betracht gezogen, dass es 'um Reduktionismus zu vermeiden 'notwendig oder sinnvoll sein könnte, mit dem computationellen Modell dezidiert nur einen Baustein zu ihrer Analyse beizutragen und dessen genaue Funktion zu explizieren. Das Kriterium der Validität (vgl. Drost 2011) computationeller Modelle wäre aus dieser Perspektive neu zu denken: Es ist nicht notwendig, dass die Modelle dasjenige messen, nach dem Literaturwissenschaftler:innen fragen 'sofern die Relation zwischen Modell und Phänomen so expliziert wird, dass ein (nicht-computationelles) literaturwissenschaftliches Weiterarbeiten mit den erzielten Ergebnissen ermöglicht wird (vgl. hier auch Flick 2012 zu Triangulation).  Unzuverlässiges Erzählen ist insofern zusammengesetzt, als es in distinkte Typen mit unterschiedlichen Eigenschaften zerfällt (vgl. Jacke 2020, 17–57). Zudem muss (zumindest laut einigen Definitionen) eine Kombination aus mehreren (Text-)Eigenschaften gegeben sein, damit unzuverlässiges Erzählen vorliegt. Diese Probleme ließen sich allerdings noch durch begründete Auswahl und ein Zergliedern in Analyseschritte adressieren, wie von Döring/Bortz bzw. Pichler/Reiter vorgeschlagen. Zum anderen können literarische Phänomene dadurch komplex sein, dass ihre Feststellung in einem Text interpretationsabhängig ist. Interpretationsabhängigkeit ist dabei als gradierbare Eigenschaft zu verstehen 'der Grad bemisst sich danach, in welchem Maße nicht-wahrheitserhaltende Schlüsse und strittige (Kontext-)Annahmen notwendig sind, um das Vorliegen des Phänomens festzustellen. Auch bei der Feststellung des Vorliegens interpretationsabhängiger Phänomene spielen aber in der Regel der literarische Text selbst bzw. konkrete identifizierbare Texteigenschaften eine zentrale Rolle 'die alleinige Bezugnahme auf sie ist aber eben nicht ausreichend, um für ihr Vorliegen zu argumentieren. Bei starker Interpretationsabhängigkeit ist es in der Regel extrem kompliziert (bzw. möglicherweise unmöglich), ein literaturwissenschaftliches Konzept vollständig (computationell) zu operationalisieren. Dies ergibt sich zum einen durch die Schwierigkeit, die im Rahmen von Interpretation stattfindenden zahlreichen und schwer zu fassenden Prozesse überhaupt zu rekonstruieren, zum anderen durch die Herausforderung, extratextuelles Wissen in computationellen Analysen abzubilden. Einige zentrale Aspekte, die für das Vorliegen unzuverlässigen Erzählens notwendig sind, sind interpretationsabhängig. Beispielsweise ist es für faktenbezogene Unzuverlässigkeit notwendig, dass eine Erzählfigur falsche Aussagen über die erzählte Welt tätigt (oder relevante Informationen auslässt, vgl. Kindt 2008, 53). Eine derartige Diagnose erfordert eine Entscheidung darüber, was in der erzählten Welt wahr und relevant ist. Für Entscheidungen dieser Art sind zwar Textargumente (vgl. Descher/Petraschka 2019, 88–93) sehr wichtig. Aber zum einen müssen diese Textargumente unter Umständen in komplizierter Weise gegeneinander abgewägt werden und zum anderen sind 'gerade bei potenziell unzuverlässig erzählten Texten 'Kontextannahmen in der Regel unerlässlich für die Rekonstruktion der fiktiven Welt. Während eine vollständige (computationelle) Operationalisierung unzuverlässigen Erzählens also wenig aussichtsreich erscheint, bietet die Relevanz von Textargumenten dennoch einen aussichtsreichen Ausgangspunkt für die Entwicklung computationeller Modelle, die für die Analyse von Unzuverlässigkeit in Texten unterstützend herangezogen werden können. So werden in der (nicht-computationellen) Unzuverlässigkeitsforschung auch tatsächlich Indikatorenlisten zusammengestellt, die unter anderem auch wenig komplexe sprachliche Phänomene enthalten, deren computationelle Modellierung aussichtsreich bzw. bereits umgesetzt ist. Die Indikatoren reichen von konkreten sprachlichen Einzelphänomenen (""Ausrufe, Ellipsen, Wiederholungen"") und nicht spezifizierten linguistischen Sammelphänomenen (""linguistische Signale für Expressivität und Subjektivität"") über Eigenschaften bzw. Zustände von Erzähler:innen (""Hinweise auf kognitive Einschränkungen"") sowie Sprachhandlungen und Absichten (versuchte ""Rezeptionslenkung durch den Erzähler"", Nünning 1998, 27–28) bis hin zu inhaltlich-strukturellen (verschiedene Arten von Widersprüchen) und inhaltlich-kontextuellen Phänomenen (stark unwahrscheinliche oder unmögliche Aussagen). Eine Analyse der Indikationsbeziehungen wird in der Unzuverlässigkeitsforschung allerdings nicht vorgenommen. Um Klarheit über die Relevanz konkreter computationell modellierter sprachlicher Indikatoren im Zusammenhang mit komplexen literarischen Phänomenen zu erlangen, sollte zum einen reflektiert und kommuniziert werden, welche Funktion dem Modell zukommen soll, also ob es beispielsweise als Heuristik zum Auffinden für eine Forschungsfrage potenziell relevanter Texte (bzw. zur Exploration von Korpora), in argumentativen Zusammenhängen genutzt (vgl. Gerstorfer 2020) oder in anderen Funktionen eingesetzt werden soll. Hiervon ist abhängig, wie stark die Indikationsbeziehung überhaupt sein muss, um valide (oder: plausible) Ergebnisse erzielen zu können (vgl. Gius/Jacke 2022). Zwei weitere Aspekte der Indikationsbeziehung, die für eine literaturwissenschaftliche Verwertbarkeit computationeller Modelle im Zusammengang mit komplexen Phänomenen analysiert und kommuniziert werden sollten, werden im Folgenden etwas genauer vorgestellt. Grundsätzlich gilt, dass computationelle Modelle, die für die Textanalyse eingesetzt werden, für viele Literaturwissenschaftler:innen insbesondere dann interessant sind, wenn (zumindest ansatzweise) nachvollziehbar wird, Warum, beispielsweise, soll ein sprachliches Phänomen wie Ausrufe unzuverlässiges Erzählen indizieren? Ausrufe sind ein Merkmal expressiver Sprache. Expressive Sprache weist auf eine emotional aufgewühlte Erzählinstanz hin. Eine emotional aufgewühlte Erzählinstanz neigt dazu, etwas durcheinanderzubringen. Eine Erzählinstanz, die etwas durcheinanderbringt, ist disponiert, inkorrekte Aussagen zu treffen. Eine Erzählinstanz, die inkorrekte Aussagen tätigt, lässt sich als unzuverlässige Erzählinstanz einordnen. Während diese Reihe das Fortschreiten von nicht zu stark interpretationsabhängigen Texteigenschaften illustriert, sollten insbesondere bei dem hier gewählten Beispiel auch kausale Zusammenhänge bzw. Richtungen beachtet werden. Eine entsprechende Analyse zeigt, dass das Vorliegen sprachlicher Merkmale wie Ausrufe im Text und das Vorliegen unzuverlässigen Erzählens nicht im eigentlichen Sinn kausal verbunden sind, sondern dass beide Phänomene (intrafiktionaler Logik folgend) die gleiche Ursache haben können 'nämlich eine emotional aufgewühlte Erzählinstanz (siehe auch Reichenbachs Konkrete sprachliche Texteigenschaften, die als Indikatoren für komplexere literarische Phänomene betrachtet werden, können diese Phänomene in unterschiedlicher Art und mit unterschiedlicher Stärke indizieren: (1) Das Vorkommen bestimmter sprachlicher Indikatoren (ggf. mit einer bestimmten Frequenz, in bestimmten Kombinationen oder an bestimmten Stellen in einem Text) kann notwendig oder (meist in Kombination mit anderen Indikatoren) hinreichend für das Vorliegen eines bestimmten komplexen Phänomens sein. Ein Indikator ist dann notwendig für ein Phänomen, wenn er vorliegen muss, sofern das Phänomen vorliegt; hinreichend ist er dann, wenn sein Vorliegen das Vorliegen des Phänomens garantiert (vgl. Brennan 2022). Während ein einzelnes sprachliches Phänomen in der Regel weder notwendig noch hinreichend für das Vorliegen eines komplexen literarischen Phänomens ist, lassen sich unter Einbeziehung der oben genannten Zwischenphänomene als Verbindungsglieder zwischen sprachlichem Indikator und komplexem Phänomen aussagekräftigere Ergebnisse erzielen. (2) Basierend auf dem Vorkommen bestimmter sprachlicher Indikatoren (ggf. mit einer bestimmten Frequenz, in bestimmten Kombinationen oder an bestimmten Stellen in einem Text) kann dem Vorliegen eines bestimmten komplexen Phänomens eine hohe Wahrscheinlichkeit zugeschrieben werden. Solche bedingte Wahrscheinlichkeit lässt sich zum einen ebenfalls auf einer Mikroebene analysieren: So ist beispielsweise anzunehmen, dass expressive Sprache zwar weder notwendig noch hinreichend für eine emotional aufgewühlte Erzählinstanz ist, diese aber mit hoher Wahrscheinlichkeit indiziert. Auch bei der Analyse von Wahrscheinlichkeitszusammenhängen ist es wichtig, Kausalitätsrichtungen zu beachten, um die Relevanz eines computationellen Modells richtig einzuschätzen: Selbst wenn beispielsweise Emotionalität der Erzählinstanz sich mit sehr hoher Wahrscheinlichkeit in Form (automatisch messbarer) emotionaler Sprache niederschlägt, und wenn dieselbe Emotionalität ebenfalls mit hoher Wahrscheinlichkeit unzuverlässiges Erzählen hervorbringt, muss untersucht werden, ob die sprachlichen Indikatoren und unzuverlässiges Erzählen nicht mit ebenso hoher Wahrscheinlichkeit jeweils andere (verschiedene) Ursachen haben können. Insgesamt kann die Identifikation von mittelkomplexen Phänomenen als Verbindungsgliedern zwischen sprachlichen Indikatoren und komplexen literarischen Phänomenen also nicht nur die Relevanz computationell modellier- und auswertbarer Texteigenschaften für literaturwissenschaftlich interessante Phänomene logisch-inhaltlich besser begreifbar machen. Sie ermöglicht auch eine aussagekräftigere (theoretische und methodologische) Analyse des komplexen literaturwissenschaftlichen Konzepts sowie eine Analyse der Indikationsverhältnisse und schafft die Grundlage für Teilerfolge (bspw. falls zwar letztlich kein signifikantes Indikationsverhältnis zwischen sprachlichem Indikator und komplexem Phänomen festgestellt werden kann, wohl aber zwischen sprachlichem Indikator und literaturwissenschaftlich ebenfalls relevanten Zwischenphänomenen). Der hier vorgeschlagene Weg, komplexe literarische Phänomene im Rahmen computationeller Zugänge bewusst nur partiell zu operationalisieren und zu modellieren, stellt auf diese Weise eine Möglichkeit dar, wie"
2023,DHd2023,HILGER_Agnes_Dunkelgrün__blassgrün__fenchelgrün_oder__Über_d.xml,"Dunkelgrün, blassgrün, fenchelgrün oder: Über die Konkretisierung des Vokabulars im deutschsprachigen Roman (1760–1920)","Agnes Hilger (Julius-Maximilians-Universität Würzburg, Deutschland)","Beschreibung, Telling, Showing, Literaturgeschichte, Roman","Inhaltsanalyse, Modellierung, Stilistische Analyse, Literatur, Forschungsergebnis, Text","Im Jahr 2012 entdecken Ryan Heuser und Long Le-Khac, dass eine Reihe von semantisch verwandten Wörtern in englischsprachigen Romanen über das 19. Jahrhundert hinweg immer häufiger verwendet wird (vgl. Heuser/Le-Khac 2012). Diese Wörter sind tendenziell konkret. Eine Masterarbeit, die dem Poster zugrunde liegt, verfolgte das Ziel, die Beobachtungen von Heuser, Le-Khac und Underwood zunächst versuchsweise für die deutschsprachige Literatur nachzuvollziehen und sodann eine erste Eingrenzung derjenigen Bereiche zu leisten, die von der Entwicklung betroffen sind. Die Ergebnisse sollen hier vorgestellt werden. Das Korpus basiert auf den bei TextGrid und Projekt Gutenberg digital zur Verfügung gestellten Texten (vgl. Neuroth u.a. 2015; Reuters o.J.). Es enthält 1147 zwischen 1760 und 1920 erschiene Romane. Diese sind jedoch nicht gleichmäßig über den Zeitraum verteilt (s. Figure 1). Die Unausgewogenheit soll in anschließenden Arbeiten angegangen werden. Um die Wortfrequenzen zu ermitteln, wurden zunächst Wortlisten erstellt. Als Heuristik diente eine sehr grobe Einteilung in drei Gruppen: 1) Informationen zu Figuren, 2) Informationen zu Räumen und 3) Informationen zu Beschaffenheit und Material. Auf Basis dieser Unterscheidung wurde eine Liste von 31 Wortfeldern erstellt. Um der historischen Sprachstufe und der Domäne Roman gerecht zu werden, wurden die Wortlisten anschließend mit einem Word-Embedding-Modell erweitert. Dafür wurde ein auf CommonCrawl trainiertes Fasttext-Modell auf dem Roman-Korpus weitertrainiert (vgl. Bojanowski 2016). Aufgrund guter Performance in ähnlichen Tasks schien ein solches Fasttext-Modell ausreichend (vgl. Ehrmanntraut u.a. 2021). Um die Wortlisten zu erweitern, wurden zu den extrahierten Wörtern abhängig von der Länge der Liste die zwei bis zwanzig nächsten Nachbarn ermittelt und, sofern nicht schon vorhanden, der Liste angehängt. Neben Wörtern wie Die Romane wurden mit dem Python-Paket spaCy lemmatisiert und für jedes Wortfeld die zugehörigen Wortfrequenzen berechnet (vgl. Honnibal/Montani 2017). Nimmt man die Wortfrequenzen aller 31 konkreten Wortfelder zusammen, ergibt sich ein signifikanter Anstieg (Mann-Kendall-Test, Œ±=0,01, p=2,22e-16). In Figure 2 repräsentiert jeder Punkt einen Roman, die y-Achse gibt jeweils die Wortfrequenz an. Im Korpus gibt es also einen ähnlichen Trend wie in den englischsprachigen Texten. Insbesondere bei den Wortfeldern, die Figuren, Gebäude und Innenräume beschreiben, gibt es signifikante Anstiege. Bei den Wortfeldern, die der Darstellung von Naturräumen (z.B. die Wortfelder Bau/Gebäude Gebäudeteil Zimmer Einrichtungsgegenstand/ Möbel Heimtextilie Haushaltsgegenstand/ Haushaltsprodukt Dekorationsgegenstand/ Ziergegenstand Gras/Grünfläche Gartenanlage/ Grünanlage Weg Gewächs/Pflanze Bewaldung/Wald Kunstobjekt Bild Körperteil Bekleidung/Kleidung Bekleidungsteil/ Kleidungsteil Aufmachung/Outfit Aussehensspezifisch Tasche Geschmeide/Schmuck Gewebe/Stoff/Textil Farbspezifisch Helligkeitsspezifisch Oberflächenspezifisch Muster/Musterung Formspezifisch Geruch Ornament/Verzierung Holz Die Ergebnisse der Arbeit deuten auf eine grundlegende Veränderung im untersuchten Korpus hin, die von der germanistischen Literaturgeschichte bislang nicht wahrgenommen wurde: Die Art und Weise, in der Romane ihre fiktive Welt physisch gestalten, wandelt sich über einen mehrere Epochen umfassenden Zeitraum hinweg erheblich. Zudem ermöglichen die Ergebnisse der Arbeit eine erste Differenzierung. Besonders betroffen scheinen Informationen über das physische Erscheinungsbild von Figuren und Orten, an denen Figuren leben. Eine anschließende Arbeit soll diese Ergebnisse konkretisieren."
2023,DHd2023,GLAWION_Anastasia_Einfluss_des_häufigen_Lesens_auf_Textwahrn.xml,Einfluss des häufigen Lesens auf Textwahrnehmung: Ergebnisse eines Leseexperiments,"Anastasia Glawion (TU Darmstadt, Deutschland); Thomas Weitin (TU Darmstadt, Deutschland)","Literaturrezeption, Leser:innenforschung, Emotion","Literatur, Personen, Text","In dem Vortrag werden Ergebnisse eines Leseexperiments vorgestellt, welches unter anderem darauf abzielte, die Lücke zwischen psychologisch orientierten Lesereaktionsstudien und literaturwissenschaftlich fundierten Rezeptionsstudien (Kavanagh, 2021) zu schließen. Die Stimuli umfassten Passagen aus der beliebten ""Harry Potter""-Buchreihe in deutscher Sprache sowie Auszügen aus ""Harry Potter""-Fanfictions. In dem Experiment sollten folgende Forschungsfragen beantwortet werden: In der Studie wurde eine Reihe von Messmethoden verwendet, darunter die Messung der Augenbewegungen (inklusive der Pupillengröße) und des Hautleitwerts (GSR) der Teilnehmer:innen. Diese beiden Messmethoden werden am häufigsten als Marker von emotionaler Reaktion in Betracht gezogen. Die Originaltexte unter den Stimuli umfassten 40 ""neutrale"" Texte, 40 Texte, die als ""furchteinflößend"", und 40 Texte, die als ""fröhlich"" gekennzeichnet waren (s. Tabelle 1). Diese Textstellen sowie ihre Sentimentmarkierungen wurden aus einer früheren Lesestudie von Hsu (2015) übernommen. Die Liste der Stimuli wurde um Fanfiction-Texte erweitert, die zuvor von 82 Fanfiction-Leser:innen in einer Umfrage ausgewählt wurden, weil sie besonders starke Emotionen bei ihnen ausgelöst hatten. Die vielseitige Auswahl der Stimuli in der Studie von Hsu deckte unterschiedliche Aspekte auf, die mit Wirkung von Literatur verbunden sind. Einer davon war für unsere Analysen besonders anregend: es wurde ein Zusammenhang zwischen Immersion und emotionalen Inhalten, ""especially negative, arousing and suspenseful ones"" (Hsu, Conrad, Jacobs 2014; 1359) festgestellt. Daher interessierten wir uns zunächst dafür, ob die gemessenen Indikatoren für Erregung, Pupillengröße und Hautleitwert (Bradley et al. 2008) vergleichbare Ergebnisse wie andere Studien zur Fiction-Feeling-Hypothese aufzeigen, z. B. dass als ""furchteinflößend"" markierte Passagen stärkere Reaktionen hervorrufen als diejenigen, die das Label ""fröhlich"" oder ""neutral"" tragen (Hsu, Conrad, Jacobs 2014, Eekhof et al. 2021). Diese Reaktion würde in der Klassifikation der Leseemotionen von Miall und Kuiken (2002) in den Bereich der ""narrative feelings"" fallen, also Gefühlen, die gegenüber literarischen Figuren entwickelt werden bzw. auf eine Resonanz mit der Stimmung und dem Schauplatz eines literarischen Textes hindeuten. Von dieser Art der Emotionen erwartet man, dass sie den Emotionsgehalt des Textes ""spiegeln"" (Miall, Kuiken, 2002; 224). Dies ist das erste Experiment in einer Reihe von geplanten Studien am LitLab der TU Darmstadt, die als Ziel die Erforschung des Zusammenhangs zwischen Textsentiment und empirischen Untersuchungen von Leseprozessen haben. Insgesamt haben im Rahmen des aktuellen Experiments 40 deutsche Muttersprachler:innen 150 Textpassagen gelesen (120 Originale, 15 Fanfictions und 15 Badfictions). Anschließend füllten die Teilnehmer:innen einen Fragebogen aus. Entgegen der Vorläuferstudie, wurden keine Fragen zur Immersion gestellt: Die Stimuli waren recht kurz (40-50 Wörter) und wurden in einer zufälligen Reihenfolge präsentiert, was die Immersion behindern würde. Wir erwarteten, dass andere Faktoren das Leseverhalten beeinflussen würden und befragten die 40 Teilnehmer auf drei verschiedene Arten zu ihren Lesegewohnheiten. Da bereits in Experimenten zur Verbindung zwischen Lesen und Theory of Mind die Unterteilung in Gruppen nach Leseerfahrung signifikante Unterschieden aufgedeckt hat (Kidd und Castano 2013, Panero et al., 2016), wollten wir den Einfluss auch bei der emotionalen Reaktion überprüfen. Zunächst wurden Lesegewohnheiten der Teilnehmer:innen mit Hilfe des Reading Habits Questionnaire (Kuijpers et al., 2020) erfasst. Der Fragebogen nimmt die selbst angegebene Lesemenge im Laufe des letzten Jahres auf. Obwohl der Fragebogen vielseitig in seinen Auswertungsmöglichkeiten ist, kann er als Selbstauskunft nur bedingt als zuverlässig eingestuft werden. Intensive Lesephasen und Mehrfachnennungen sind hierbei nur schwer erfassbar. Für die Auswertung wurden Angaben über unterschiedliche Genres summiert und drei fast gleich große Gruppen gebildet: die Teilnehmenden wurden in Vielleser:innen (13 Teilnehmer, Summe der Punktzahlen: 24-80), Durchschnittsleser:innen (13 Teilnehmer, Summe der Punktzahlen: 13-22) und Selten-Leser:innen (14 Teilnehmer, Summe der Punktzahlen: 4-11) eingeteilt. Außerdem haben Proband:innen die deutsche Version des Tests zur Autorenerkennung ausgefüllt (Grolig et al. 2020), ein bewährte Methode um die Kenntnis des Literatursystems oder die langfristige Auseinandersetzung mit Literatur zu erfassen (Panero et al. 2016; Stanovich et al. 1989). Auch hier teilten wir die Proband:innen in drei Gruppen auf: Literaturkenner:innen (14, erkannten 13-38 Autoren richtig), Literatureinsteiger:innen (12, erkannten 1-6 Autoren richtig) und Mittelfeld (14, erkannten 7-12 Autoren richtig). Wie erwartet, gab es einige Überschneidungen zwischen den Gruppierungen, doch die Rangkorrelation zwischen den beiden Angaben war schwach (tau = 0,21). Der dritte Bereich, den wir in Hsus Originalexperiment als nicht ausreichend untersucht betrachteten, war die Einbeziehung des Fandom-Wissens: es wird lediglich erwähnt, dass alle Proband:innen mindestens ein Buch aus der ""Harry Potter""-Reihe gelesen haben. Der Fragebogen der aktuellen Studie enthielt zwei Fragen zum Wissen über das Harry-Potter-Fandom, differenziert nach Filmen und Büchern. Wir erwarteten, dass Fans stärker auf die präsentierten Stimuli reagieren würden. Ein Wert von 0 stand für Teilnehmer, die keines der Bücher gelesen und keinen der Filme gesehen haben, während 5 bedeuten würde, dass alle Filme, alle Bücher und zusätzliches Material gelesen wurden. Insgesamt wurden Teilnehmer mit einer Punktzahl von 4 und 5 der Gruppe ""Fans"" (17) zugeordnet, Teilnehmer mit einer Punktzahl von 0 und 1 galten als ""Nicht-Fans"" (10) und Teilnehmer mit einer Punktzahl von 2 und 3 wurden dem ""Fandom-Mittelfeld"" (13) zugerechnet. Die Rangkorrelation zwischen dem Context Score und den beiden anderen Gruppeneinteilungen ist ebenfalls schwach (tau = 0,26 mit den Autorenerkennungsergebnissen; tau = 0,35 mit der selbstberichteten Lesehäufigkeit). Die Hautleitwertdaten wurden mit Brainvision Recorder aufgenommen und mit Hilfe von Ledalab (Benedek 2010) analysiert und exportiert. Ledalab ist eine Software, die die Segmentierung von Hautleitwertdaten sowie eine automatische Ermittlung von Hautleitwertreaktionen durchführt. Dafür werden zwei unterschiedlichen Methoden verwendet: die TTP-Analyse (trough-to-peak), die auf vorgegebenen zeitlichen Kriterien basiert (Bouscein et al. 2012), und die CDA (continuous decomposition analysis), die das Signal zunächst in seine kontinuierlichen (tonischen) und stimulusbezogenen (phasischen) Komponenten unterteilt und dann mit Hilfe eines Die Pupillometriedaten wurden aus der Eyetracking-Software SR Research Data Viewer exportiert und normalisiert: für jede Testperson wurde eine Baseline der Pupillengröße ermittelt, die auf der durchschnittlichen Pupillengröße basiert, die zwischen den Trials aufgenommen wurde. Um eine mittlere Veränderung der Pupillengröße zu bestimmen, wurde von der mittleren Pupillengröße pro Trial die Baseline subtrahiert. Bei der Datenverarbeitung der Hautleitwertreaktion und der Pupillometrie-Daten wurde besonders darauf geachtet, ob die Daten die Voraussetzungen für einen ANOVA-Test erfüllten: Unabhängigkeit (die durch das Experimentdesign gegeben war), Normalverteilung und Homogenität der Varianz. Es zeigte sich, dass die Pupillengröße und die Daten zur Anzahl der Hautleitwertpeaks normalverteilt waren, die anderen Hautleitwertdaten jedoch nicht. Die normalverteilten Daten wurden mit einem ANOVA-Test untersucht, während für die nicht normal verteilten Werte der Kruskal-Wallis-Test durchgeführt wurde. Nach den Berechnungen mit ANOVA und dem Kruskal-Wallis-Test wurde eine Auswertung der Effekte mit Epsilon-Quadrat durchgeführt, die zeigte, dass die signifikanten Ergebnisse starke Effekte aufweisen und die meisten Ergebnisse, die sich der Signifikanz näherten, mittlere Effekte zeigten. Unsere Ergebnissen zufolge gab es keinen signifikanten Einfluss von Textsentiment auf die Anzahl oder Stärke der Reaktionen, weder beim Hautleitwert noch bei der Pupillengröße. Doch wir konnten einen weiteren signifikanten Faktor ausfindig machen, der bei der Erforschung der Leser:innenreaktionen eine Rolle spielt. Sobald Proband:innen in Gruppen nach den Ergebnissen des Author Recognition Tests eingeteilt wurden, konnte man sehen, dass Literaturkenner:innen signifikant stärkere Reaktionen im Bereich des Hautleitwertes gezeigt haben im Vergleich zu Literatureinsteiger:innen und den Mittelfeld-Proband:innen. Tabelle 2 zeigt die p-Werte des Kruskal-Wallis-Tests. Bei den fettgedruckten Werten wurde der Effekt der Gruppenaufteilung als ""mittel"" eingestuft, während bei unterstrichenen Werten der Effekt als ""stark"" bewertet wurde. Wir sehen, dass die durch den Autorenerkennungstest gebildeten Gruppen bei den meisten Werten signifikante Unterschiede in ihren Mittelwerten aufweisen. Tabelle 3 zeigt, dass eine ähnliche Tendenz in den normalverteilten Variablen auffindbar ist: während die Gruppen, die auf der Basis des Fandomscores und des Reading Habit Questionnaire gebildet wurden, keine signifikanten Unterschiede aufzeigen, zeigen die Gruppen der Autorerkennungstests mittlere bis starke Effekte. Meistens manifestieren sich die signifikanten Effekte in Unterschieden zwischen dem Verhalten der Literaturkenner:innen auf der einen und Literatureinsteiger:innen und dem Mittelfeld auf der anderen Seite, wie in Abbildung 1. Es scheint, als würde die Kenntnis des literarischen Feldes Voraussetzung für häufigere Hautleitwertreaktionen sein. Wird eine Unterteilung nach der Leistung im Autorenerkennungstest vorgenommen, so zeigt sich, dass Literaturkenner:innen bei den meisten Werten signifikant größere Reaktionen zeigten: Es gibt eine höhere Anzahl von Peaks, die Summe der Amplituden ist höher und die durchschnittliche Pupillengröße ist größer. Bei den Literaturkenner:innen sind die Reaktionen auf fröhliche Stimuli am höchsten und auf neutrale Stimuli fast immer am niedrigsten. Die Literatureinsteiger:innen hingegen zeigen meist minimale Werte bei fröhlichen Stimuli und am häufigsten höchste Werte bei neutralen Stimuli. Keiner dieser Werte wich signifikant vom Mittelwert ab. Unsere Ergebnisse zeigen, dass die nach unterschiedlichem Sentiment gelabelten Texte keine signifikanten Unterschiede in der Hautleitwertreaktion und in der Pupillengröße aufzeigen. Die Analysen der Lesegewohnheiten der Proband:innen lassen hingegen darauf schließen, dass diejenigen, die mehr lesen, auch stärkere Reaktionen auf Texte insgesamt aufweisen. Vor allem die Kenntnis des Literatursystems ‚Äì wie der Autorenerkennungstest oft interpretiert wird ‚Äì beeinflusst die körperlichen Reaktionen auf das Lesen in erheblichem Maße. Leser:innen, die mehr Erfahrung mit Literatur haben, reagieren stärker auf literarische Werke und spiegeln dabei den Textsentiment wieder, die ein Text enthält (stärkere Reaktionen auf emotionale Inhalte, schwächere auf neutrale Passagen). Leser mit geringerem Wissen über Literatur scheinen auch auf neutrale Stimuli stark zu reagieren, vielleicht weil sie einen emotionalen Stimulus erwarten und diesen nicht erhalten. Diesen Ergebnissen zufolge ist das Wissen über Literatur für eine andere Art der Reaktion auf Texte verantwortlich. Entgegen unseren Erwartungen zeigte sich kein signifikanter Einfluss von höherer Kenntnis des Werks, obwohl der Gesamtmittelwert des Hautleitwerts bei fröhlichen Stimuli bei Fans höher war. Möglicherweise ist dies auf eine Kombination aus Nostalgie und narrativen Gefühlen zurückzuführen. Schließlich hat die jüngste Leseaktivität, die mit dem RHQ ermittelt wurde, fast keinen Einfluss auf die physiologischen Reaktionen - nur als zusätzlicher Faktor bei der Berücksichtigung der Pupillengröße. Diese korrelativen Zusammenhänge bieten allerdings noch keine Antwort auf die Frage nach der Kausalität ‚Äì die Frage, ob Lektüre die emotionale Reaktion trainiert oder ob empfindsame Menschen sich mehr zu Literatur hingezogen fühlen, bleibt offen. Die Aussagekraft der Ergebnisse ist durch einige Schwachstellen eingeschränkt: beispielsweise sind die Proband:innen überwiegend Studierende und können daher nur schwer als absolute Wenigleser:innen bezeichnet werden. Vielleicht ist das der Grund, warum die Daten so selten Unterschiede zwischen Literatureinsteiger:innen und Mittelfeld-Proband:innen aufzeigen. Darüber hinaus hat die Anzahl der Proband:innen eine eher geringe statistische Aussagekraft (40 Teilnehmer), wovon allerdings nur die Analyse der Proband:innengruppen betroffen ist: Für die Analyse des Sentimenteinflusses auf die Leser:innenreaktion wird die statistische Signifikanz durch die große Anzahl an Trials derselben Sentimentklasse wieder angehoben. Die Ergebnisse dienen zum Anlass, über mehrere Studien hinweg den Einfluss der Lesekompetenz zu berücksichtigen. Zuletzt wären Vergleichsstudien mit anderen literarischen Gegenständen interessant, um die Zusammenhänge der hier vorgestellten Variablen über ""Harry Potter"" hinaus zu beobachten und weitere Aspekte von literarischen Texten wie Stil und Epoche ebenfalls in ihrer Wirkung zu untersuchen."
2023,DHd2023,KRAUTTER_Benjamin_Skalierungspraktiken_in_der_computergestüt.xml,Skalierungspraktiken in der computergestützten Analyse von literarischen Texten,"Benjamin Krautter (Universität zu Köln, Deutschland)","scalable reading, Netzwerkanalyse, Drama","Inhaltsanalyse, Theoretisierung, Netzwerkanalyse, Literatur","In den vergangenen Jahren haben Publikationen aus dem Bereich der digitalen Literaturwissenschaft vermehrt auf das durch den Altphilologen und Anglisten Martin Mueller geprägte Konzept s In einem ersten Schritt meines Beitrags werde ich die verschiedenen Dimensionen, auf die sich Muellers Konzeption von In einem programmatisch ausgerichteten Blogbeitrag hob Mueller 2012 hervor, wie ihn ""[t]he charms of Google Earth"" (Mueller 2012,¬†o.S.) zu Die Skalenpluralität beginnt erstens bei der Textgrundlage: Literarische Texte liegen in ""einer weiten ""Scale"" von Surrogaten"" (Weitin 2015,¬†10) vor, die nebeneinander koexistieren: ""Our typical encounter with a text is through a surrogate"" (Mueller 2013,¬†o.S.). Mueller spricht an dieser Stelle von Surrogaten, da immer schon mit unterschiedlich gearteten Repräsentationen des Originals gearbeitet wurde und wird: Das können beispielsweise Faksimiles, Text- und Werkausgaben, Digitalisate oder auch speziell kodierte Textsammlungen sein (vgl. dazu Mueller 2014,¬†¬ß 4–20). Surrogate können darüber hinaus in stark transformierter oder abstrahierter Form auftreten, beispielsweise in Gestalt von Häufigkeitswortlisten. Auch die Netzwerkanalyse fußt demnach auf Surrogaten. Peer Trilcke und Frank Fischer sprechen von einem ""Zwischenformat"", das in ihrem Fall nur noch diejenigen Strukturinformationen der Dramen vorhalte, die zur Netzwerkerstellung herangezogen werden (Trilcke, Fischer 2018,¬†Kap.¬†3). Der Dramentext selbst ist nicht mehr Teil des Zwischenformats. An diese unterschiedlichen Repräsentationsformen von Literatur ist zweitens die Frage des Umfangs geknüpft: Wie groß ist der Untersuchungsgegenstand? Handelt es sich nur um einen einzelnen Text, vielleicht sogar nur um einen Ausschnitt des Textes, oder aber um eine größere Sammlung von Texten? Wie umfangreich ist diese Sammlung? Nicht nur die Zahl der zu betrachtenden Texte, auch die Textsorte kann hier Teil der Skalierungsfrage sein: Sollen kurze Novellen oder 1000-seitige Langromane untersucht werden, ein kurzer Einakter oder Karl Kraus"" monumentales Lesedrama Drittens stellt sich die Frage nach der Größe der Analyseeinheiten. Morettis Mueller denkt die methodische Bezugsgröße viertens vielmehr selbst auf einer Art Skala. Wie Weitin gemeinsam mit Thomas Gilli und Nico Kunkel (2016,¬†115) herausstellt, umfasse Im folgenden Abschnitt möchte ich die mit einer Praxis des Die Automatisierung führt jedoch zu einigen Einschränkungen. So ist die oben dargelegte Formalisierung von Figureninteraktionen zwar ähnlich, aber nicht deckungsgleich mit dem von Solomon Marcus (1973,¬†358) vorgeschlagenen und zum kodifizierten Handbuchwissen (vgl. etwa Pfister 2011,¬†235–240) gewordenen Begriff der Konfiguration. Die Figurenkonfiguration eines Dramas ändert sich immer dann, wenn eine Figur die Bühne betritt oder verlässt, also das am Bühnengeschehen beteiligte Personal zumindest in Teilen wechselt. Dramen, die Prinzipien des französischen Klassizismus folgen, sind durch die im Nebentext markierten Auf- und Abtritte strukturiert. Konfiguration und Szenengrenze fallen dann 'zumindest in der Theorie 'zusammen. Anders ist das bei Stücken, die sich an Shakespeares Poetik orientieren. Hier sind die Szenengrenzen zumeist an einen Ortswechsel gebunden. Daher können Figuren auf- oder abtreten, ohne dass zwangsläufig eine neue Szene konstituiert wird. Da Auf- und Abtritte von Figuren im Nun stellt sich die Frage, wie sich solche Kopräsenznetzwerke sinnvollerweise in die etablierte Dramenanalyse integrieren lassen. Einen prominenten Versuch, Netzwerkanalysen in den Verstehensprozess literarischer Texte zu integrieren, unternimmt Moretti in seinem Essay Abbildung¬†1: Kopräsenznetzwerk von Friedrich Schillers Die Räuber (GEM force directed layout algorithm). Die Knotengröße repräsentiert den Grad. Wie verhält sich Morettis Studie aber zu Muellers Abbildung 1 verdeutlicht zudem, dass nicht alle Kopräsenznetzwerke vom """"intermediate"" status of visualization"" profitieren, den Moretti in seinem Essay als so wichtig erachtet (Moretti 2011,¬†11). Das Netzwerk von Schillers Der Mehrwert solcher Figurennetzwerke wird jedoch meist auf die Analyse größerer Textsammlungen verschoben, die schon aufgrund ihrer Menge nur schwer durch  Abbildung 2 zeigt ein Beispiel für eine diachrone Analyse anhand 583 deutschsprachiger Dramen, die zwischen 1730 und 1930 veröffentlicht oder uraufgeführt wurden (German Drama Corpus). Die Abbildung reproduziert eine Untersuchung von Trilcke und Fischer (2018, Abbildung¬†6). Wie Trilcke und Fischer habe ich aus dem durchschnittlichen Grad der einzelnen Dramen die Mittelwerte für jedes Jahrzehnt von 1730 bis 1930 ermittelt. Rein deskriptiv ist festzuhalten, dass der durchschnittliche Grad ab dem späten 18. Jahrhundert langsam ansteigt. Zwischen 1830 und 1880 sind dann nur relativ geringe Schwankungen zu erkennen, ehe auf einen Anstieg bis etwa 1890 ein abrupter Fall und ein erneuter starker Anstieg folgen. Trilcke und Fischer haben diese Werte als Indikator dafür gedeutet, dass Dramatiker:innen mit ihren Stücken ""auf die gesellschaftliche Modernisierung und Ausdifferenzierung seit der zweiten Hälfte des 18. Jahrhunderts"" reagieren. Sie weisen im Anschluss gleichwohl darauf hin, dass diese Erkenntnis nichts Neues sei (Trilcke, Fischer 2018, Kap.¬†4.1). Mit Fotis Jannidis (2019,¬†65) gesprochen lässt sich diese Art der Wissenskonsolidierung als Form der ""Kreuzpeilung"" begreifen. Wie überzeugend ist diese Deutung der Werte aber? Vergleicht man den Werteverlauf des durchschnittlichen Grads in Abbildung¬†2 mit der Zahl der auftretenden Figuren aus Abbildung¬†3, scheint ein Zusammenhang zu bestehen. Die Berechnung der Korrelation zwischen Grad und Figurenzahl bestätigt diese Relation: Abbildung 3: Zahl der Figuren (schwarz) und durchschnittlicher Grad (grau) von 583 deutschsprachigen Dramen. Die Abbildung zeigt die Mittelwerte pro Dekade. Die vier von mir beschriebenen Dimensionen des"
2023,DHd2023,ANDRESEN_Melanie_Klassifikation_von_Figurenauf__und__abtritt.xml,Klassifikation von Figurenauf- und -abtritten in XML-kodierten Dramen,"Lena Ehlers (Universität Stuttgart, Deutschland); Melanie Andresen (Universität Stuttgart, Deutschland)","Drama, Annotation, Computational Literary Studies","Datenerkennung, Strukturanalyse, Annotieren, Literatur, Text","In diesem Beitrag wird eine regelbasierte Methode vorgestellt, um Figurenauf- und -abtritte in den Regieanweisungen dramatischer Texte zu klassifizieren. In der Forschung wurde Regieanweisungen meist nur wenig Beachtung geschenkt, etwa weil sie während einer Theateraufführung nicht textuell in Erscheinung treten (Schößler, 2017, S.3). Als eine der wenigen quantitativen Untersuchungen stellen Trilcke et al. (2020) fest, dass sich die vermutete Episierung des Dramas im Laufe der Jahrhunderte am Korpus GerDraCor bestätigt. Eine Differenzierung nach Funktionen der Regieanweisungen erfolgt nicht. Dabei ist die (Ko-)Präsenz von Figuren auf der Bühne eine häufig genutzte Grundlage für quantitative Dramenanalysen, insbesondere in der Netzwerkanalyse (z.B. Marcus 1973, Krautter et al. 2018, Fischer et al. 2018). Trilcke et al. (2017) konnten bereits am Beispiel von Lessings Emilia Galotti zeigen, dass in statischen Netzwerken, die das ganze Drama auf einmal darstellen, wichtige Informationen zur Dynamik der Beziehungen zwischen den Figuren verloren gehen können. Unserer Ansicht nach ist außerdem zu berücksichtigen, dass Figuren auch innerhalb von Szenen auf- und abtreten und die Anwesenheit von zwei Figuren in einer Szene nicht zwangsläufig bedeutet, dass diese Figuren auch gleichzeitig auf der Bühne stehen. Das Deutsche Dramenkorpus GerDraCor enthält über 550 deutschsprachige, TEI-kodierte Dramentexte aus dem Zeitraum von 1650 bis 1947 (Fischer et al. 2019). Die Regieanweisungen sind als Insgesamt wurden 16 Dramentexte manuell annotiert, wovon vier zur Implementierung Das entwickelte Verfahren klassifiziert den unstrukturierten Text innerhalb der Wurde im ersten Schritt ein Auf- oder Abtritt erkannt, folgt als zweiter Schritt die Zuordnung der betroffenen Figuren. Hierfür wird die im XML enthaltene Liste der Namen aller sprechenden Figuren genutzt, die auch die Abbildung auf die Figuren-IDs ermöglicht. Der Auf- oder Abtritt wird entweder einer in der Regieanweisung genannten Figur oder derjenigen Figur, deren Rede die Regieanweisung zugeordnet ist (vgl. Fall in Abb. 1), zugeschrieben. Die Evaluation erfolgt anhand von zwölf manuell annotierten Texten, die nicht zur Aufstellung der Regeln herangezogen wurden. Evaluiert werden 1) die Klassifikation der Regieanweisungen in Figurenauf- und -abtritte und 2) die Zuordnung der betroffenen Figuren. Im zweiten Schritt werden nur die Elemente in die Evaluation einbezogen, die im ersten Schritt korrekt klassifiziert wurden. Tabelle 1 zeigt, dass die durchschnittlichen Werte für Precision, Recall und F1-Score für die Auf-/Abtritterkennung bei 0,85 liegen. Auch die Figurenerkennung liefert gute Ergebnisse (F1 = 0,87). Zwischen den Texten zeigt sich allerdings eine erhebliche Variation in der Qualität. Aufgrund des regelbasierten Verfahrens schneiden Texte, die stark von den zur Erstellung der Regeln verwendeten Texten abweichen, in der Evaluation schlechter ab. Besonders Texte mit langen Regieanweisungen sorgen dafür, dass viele Schlüsselwörter auch in anderen Kontexten vorkommen. Das zeigt sich insbesondere beim Abbildung 2 zeigt, dass die meisten Auf- und Abtritte tatsächlich innerhalb von Szenen stattfinden (ca. 46%). Etwas weniger erfolgen am Beginn einer Szene (41%) und etwa 13% am Ende. Erwartungsgemäß handelt es sich am Anfang der Szene fast ausschließlich um Auftritte, am Ende um Abtritte. Innerhalb der Szenen komme Auf- und Abtritte zu jeweils gleichen Anteilen vor. In diesem Paper haben wir ein mit Figurenauf- und -abtritten annotiertes Teilkorpus zu GerDraCor präsentiert und einen regelbasierten Algorithmus vorgestellt, der diese Annotationen mit einem mittleren F1-Wert von über 0,85 reproduzieren kann. Ein Großteil der annotierten Auf- und Abtritte erfolgt innerhalb von Szenen. Diese Veränderungen in den Figurenkonstellationen werden bei einer szenenweisen Betrachtung der Figurenpräsenz nicht berücksichtigt, haben aber potenziell Auswirkungen auf beispielsweise netzwerkanalytische Arbeiten. Alle Daten und Skripte zu diesem Beitrag sind unter"
2023,DHd2023,VARACHKINA_Hanna_Pipelines_für_Natural_Language_Processing_u.xml,Pipelines für Natural Language Processing und digitale Literaturanalyse in spaCy,"Hanna Varachkina (Seminar für Deutsche Philologie, Georg-August-Universität Göttingen); Florian Barth (Göttingen Centre for Digital Humanities, Georg-August-Universität Göttingen); Tillmann Dönicke (Göttingen Centre for Digital Humanities, Georg-August-Universität Göttingen); Johannes Biermann (Niedersächsische Staats- und Universitätsbibliothek Göttingen); Friederike Altmann (Seminar für Deutsche Philologie, Georg-August-Universität Göttingen); Thorben Neitzke (Göttingen Centre for Digital Humanities, Georg-August-Universität Göttingen); Caroline Sporleder (Göttingen Centre for Digital Humanities, Georg-August-Universität Göttingen)","Textanalyse, Pipelines, Python","Programmierung, Inhaltsanalyse, Strukturanalyse, Literatur, Text","In diesem halbtägigen Workshop stellen wir ein auf spaCy basierendes Pipeline-System für das Natural Language Processing (NLP) narrativer Texte vor und erproben mit den Teilnehmer*innen dessen praktische Anwendung, besonders im Hinblick auf Untersuchungsgegenstände der digitalen Literaturanalyse. Die Analyse von literarischen Texten ist eine besondere Herausforderung für die automatische Sprachverarbeitung, da sie oft komplexe Interaktionen linguistischer Strukturen auf der syntaktischen, semantischen und pragmatischen Ebene betrifft. Für die Interpretation solcher Texte ist es zum Beispiel wichtig, neben traditionellen NLP-Verarbeitungsschritten wie Eigennamenerkennung, Sentiment-Analyse etc., auch komplexere Analysen durchzuführen, um z.¬†B. die Sprechinstanzen im Text zu identifizieren, Bezüge zur realen Welt zu erkennen oder zeitliche Strukturen im Text zu analysieren. Auf der praktischen Ebene bedeutet dies, dass automatische Analysen in der digitalen Literaturwissenschaft in der Regel die (oft komplexe) Kombination mehrerer basaler Sprachverarbeitungswerkzeuge auf Token-, Teilsatz-, Satz- und Passagen-/Diskursebene erfordert. Dies ist in der Praxis nicht immer trivial, z.¬†B. weil Ein- und Ausgabeformate verschiedener Werkzeuge nicht kompatibel sind.  Der Workshop ist als Tutorial geplant und richtet sich an Literaturwissenschaftler*innen, Linguist*innen, DH-Forschende, und andere Personen, die an Textanalyse interessiert sind. Die Teilnehmer*innen bekommen die Möglichkeit, die Funktionalitäten von MONAPipe auszuprobieren und in vorbereiteten Texten eine Reihe von Phänomenen automatisch zu identifizieren. Die Teilnehmerzahl ist auf 30 beschränkt. Lernziele und Methodik Der Workshop verfolgt mehrere Ziele: (1) Er soll die Teilnehmer*innen mit spaCy und dessen Kernkomponenten vertraut machen und Ihnen praktische Erfahrung in der Nutzung von MONAPipe für typische Textanalysekomponenten auf Token-, Satz-/Teilsatz- und Passagenebene vermitteln. (2) Darüber hinaus erproben die Teilnehmer*innen die Einbindung neuer Komponenten, um damit wie sie MONAPipe für eigene Zwecke anpassen können. Aufbauend auf diesen Grundlagen lernen die Teilnehmer*innen an einem konkreten Beispiel, (3) wie sie MONAPipe konkret für Forschungsprojekte insbesondere in der digitales Literaturanalyse nutzen können. Dies umfasst die Auswahl geeigneter Komponenten für die Forschungsfrage sowie die Reflektion der Ergebnisse. Am Ende des Workshops haben die Teilnehmer*innen zum einen (i) ein besseres theoretisches Verständnis für die verschiedenen Sprachanalyseschritte, können komplexe Analysen durch Kombination mehrerer basaler Werkzeuge durchführen und die Qualität der automatischen Analyse beurteilen; Zum anderen (ii) haben die Teilnehmer*innen praktische Erfahrung im Umgang mit spaCy und verschieden Sprachverarbeitungswerkzeugen erworben¬† und Problemlösungsstrategien für den Umgang mit NLP-Werkzeugen gelernt. Methodisch kombiniert der Workshop Theorie und Praxis, wobei der Praxisanteil überwiegt. Um das Gelernte zu festigen und zu vertiefen, bekommen die Teilnehmer*innen zunächst kurze Arbeitsaufträge (zu den Sprachverarbeitungskomponenten) und später komplexere Aufgaben (zur Analyse narrativer Texte), deren Lösungen im Anschluss diskutiert werden. Der Praxisteil im zweiten Teil des Workshops bietet außerdem die Möglichkeit, MONAPipe für ein eigenes Forschungsproblem anzuwenden und dazu Feedback von den Organisator*innen des Workshops zu bekommen. Auf technischer Ebene arbeiten wir mit der interaktiven Programmierumgebung Jupyter-Notebook und stellen vorbereitete und ausführlich dokumentierte Notebooks zur Verfügung, um einen möglichst reibungslosen Ablauf zu ermöglichen und den Teilnehmer*innen zu helfen, sich auf die Workshopinhalte zu konzentrieren. Organisation und Ablauf Wir planen einen vierstündigen Workshop bestehend aus zwei Blöcken. Der erste Block (1:45 h) beinhaltet aus einem einführenden Vortrag sowie einem Zeitslot zur Einrichtung der Jupyter-Notebooks , wobei die Organisator*innen nach Bedarf Hilfestellung bei der Einrichtung leisten. Anschließend erfolgt eine 45-minütige Session mit vorbereiteten Notebooks, bei der zunächst kürzere textuelle Phänomene auf Token-Ebene (wie Named Entities), Phänomene auf Teilsatz-Ebene (z.¬†B. Zeitformen) sowie Phänomene, die längere Textpassagen umfassen (z.¬†B. Redeformen), behandelt werden. Im zweiten Block des Workshops (1:45 h) erstellen die Teilnehmer*innen eine eigene Komponente in spaCy. Anschließend erhalten die Teilnehmer*innen die Möglichkeit durch Lektüre narrative Strukturen in exemplarischen Textpassagen qualitativ zu bestimmen. Anhand der zur Verfügung stehenden spaCy-Komponenten soll evaluiert werden, welche Features sich zur Identifikation komplexer narrativer Strukturen eignen. Alternativ können die Teilnehmer*innen an eigenen Texten und Fragenstellungen arbeiten und hierfür Unterstützung durch die Workshoporganisator*innen erhalten.   "
2023,DHd2023,NEUBER_Frederike_Open_Jean_Paul.xml,Open Jean Paul.   Funktionen und Potentiale offener Editionsdaten,"Frederike Neuber (Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland); Axelle Lecroq (Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland)","digitale Edition, Briefedition, Open Data","Teilen, Transkription, Kontextsetzung, Veröffentlichung, Daten, Forschungsergebnis","Jean Paul (1763–1825) zählt zu den bedeutendsten Schriftstellern der deutschen Literatur um 1800 und war ein überaus produktiver und geistreicher Briefeschreiber, der mit bekannten Persönlichkeiten wie Heinrich Jacobi, Caroline und Johann Gottfried Herder, Charlotte von Kalb und Rahel Levin Varnhagen korrespondierte. Die Briefe Jean Pauls erschienen bereits Mitte des 20. Jahrhunderts in der Historisch-kritischen Ausgabe (Berend 1952–1964); Anfang des 21. Jahrhunderts folgten die Briefe an Jean Paul (Begemann et al. 2003–2017), ebenfalls im Druck. Seit 2018 ist Jean Pauls Briefkosmos auf dem Weg in die digitale Welt: Die 5562 Von-Briefe, die zunächst buchzentriert retrodigitalisiert Aus methodisch-technischer Perspektive, setzt die Edition mit der Verwendung von XML/TEI und dem Basisformat des Deutschen Textarchivs (DTA 2011–2020) sowie der Anreicherung mit Normdaten (GND, GeoNames) auf Standards. Im Zeichen von ""Open Data"" erscheinen die XML/TEI-Dokumente der Briefe unter Creative Commons-Lizenz (CC-BY-SA 4.0), und zwar in drei Publikationsmodi, die verschiedene Funktionen hinsichtlich ihrer Nutzung erfüllen: (1) Zur (2) Zur (3) Zur Der Beitrag, der die Publikationsmodi der Jean Paul Briefedition und ihre jeweilige Funktion illustriert, ist für die DHd-Konferenz höchst relevant, da ""Open Data"" im Editionskontext immer noch eher die Ausnahme als die Regel ist. Aus Greta Franzinis Editionenkatalog (2016–2022) geht hervor, dass von 320 digitalen Editionen lediglich ~27% CC-Lizenzen verwenden, ~23% TEI-Daten zum Download bereitstellen und ~5% APIs anbieten. Die Zahlen sind bedauerlich, da Daten das primäre Forschungsergebnis digitaler Editionen sind: Im Kontext der Jean Paul-Edition gelten die Daten den Herausgeberinnen als"
2023,DHd2023,BRUNNER_Annelen_KoMuX___Der_Kompositamuster_Explorer.xml,KoMuX - Der Kompositamuster-Explorer,"Annelen Brunner (Leibniz-Institut für Deutsche Sprache, Deutschland); Hein Katrin (Leibniz-Institut für Deutsche Sprache, Deutschland)","Komposita, Muster, Webanwendung","Strukturanalyse, Annotieren, Webentwicklung, Visualisierung, Daten, Sprache","KoMuX, der Kompositamuster-Explorer, (www.owid.de/plus/komux) ist eine Webanwendung, die es ermöglicht, mehr als 50.000 nominale Komposita des Deutschen gezielt nach abstrakten oder lexikalisch-teilspezifizierten Mustern zu durchsuchen. Unterschiedliche Visualisierungen helfen dabei, Strukturen und Zusammenhänge innerhalb der Ergebnismenge zu erfassen. Mit KoMuX machen wir einen Teil der Datengrundlage frei verfügbar, auf der unsere empirischen Forschungen zur Wortbildung basieren und integrieren Analysen und Visualisierungen aus unseren Arbeiten. Der Explorer ist damit auch ein Beitrag zu OpenScience, indem er es ermöglicht, unsere Forschungsergebnisse in Teilen nachzuvollziehen und zu reproduzieren.  D Die musterbasierte Suche in KoMuX beruht darauf, dass grammatische Merkmale (Wortbildungstyp oder Wortart) oder lexikalische Eigenschaften (konkretes Lemma) für das Erst- und Zweitglied spezifiziert werden. Dies erlaubt es beispielsweise, gezielt alle Adjektiv+Nomen-Komposita (z.B. Visualisierungen helfen dabei, die Ergebnismenge näher zu analysieren. Auch hier steht die musterhafte Betrachtung von Erst- und Zweitgliedposition im Mittelpunkt. Quantitative Verteilungen in Hinblick auf Wortart, Wortbildungstyp und Lemma werden mit Hilfe von mehrstufigen Tortendiagrammen sichtbar gemacht. Die Konstituenten-Ansicht zeigt alle Erst- und Zweitglied-Lemmata der Ergebnismenge, sowie deren Vorkommenshäufigkeiten in den jeweiligen Positionen. So lässt sich untersuchen, in welcher Position die lexikalische Vielfalt größer ist und welche Lemmata starke Tendenzen zu einer der beiden Positionen aufweisen. Die Verknüpfungsansicht zeigt Komposita, deren Erst- oder Zweitglied-Lemma in mindestens einem weiteren Kompositum der Ergebnismenge auftritt und weist so auf produktive Bildungsmuster hin. "
2023,DHd2023,FISCHER_Frank_Internationale_Autor_innen_zu_Gast_in_der_DDR_.xml,Internationale Autor*innen zu Gast in der DDR: Die Einreisekartei des Schriftstellerverbandes und ihre digitale Aufbereitung,Frank Fischer (Freie Universität Berlin); Viktor Jonathan Illmer (Freie Universität Berlin); Lukas Nils Regeler (Freie Universität Berlin); Jutta Müller-Tamm (Freie Universität Berlin); Luise von Berenberg-Gossler (Freie Universität Berlin); Franziska Diehr (Robert Koch-Institut),"Deutscher Schriftstellerverband, Literaturwissenschaft, DDR","Archivierung, Literatur","Marcel Reich-Ranicki kam 1955 und 1956. Der sowjetische Autor Michail Scholochow besuchte die DDR 1964, zwei Jahre bevor er den Nobelpreis erhielt; ähnlich der guatemaltekische Schriftsteller und spätere Nobelpreisträger Miguel Asturias, der 1965 nach Ostberlin reiste. Friederike Mayröcker folgte einer Einladung im Mai 1987. Andere kamen wiederholt, wie der ungarische Dichter G√°bor Hajnal, der sich zwischen 1957 und 1986 dreizehn Mal in Ostberlin aufhielt. Eingeladen hatte jeweils der Deutsche Schriftstellerverband (DSV), über den der Großteil der internationalen literarischen Kontakte in der DDR organisiert wurde. Tausende Daten zur Einladungspolitik des Verbandes sind in einer Kartei in der Akademie der Künste in Berlin hinterlegt, deren Bestand im Rahmen der Archivarbeiten für das Forschungsprojekt ""Writing Berlin"" digitalisiert wurde. ""Writing Berlin"" ist Teil des Exzellenzclusters ""Temporal Communities. Doing Literature in a Global Perspective"" (EXC 2020) und befasst sich mit den facettenreichen Aktivitäten zur Förderung des internationalen literarischen Austauschs in der geteilten Stadt nach dem Bau der Berliner Mauer. Ein besonderes Augenmerk liegt dabei auf den Auswahlprozessen und den kulturpolitischen Implikationen dieser Aktivitäten, ihrem Niederschlag in literarischen Texten sowie auf der Frage, inwiefern die sich verändernde politische Gemengelage Biografien und die soziale Stellung der betreffenden Autor*innen beeinflusste. Die Internationalisierung der Berliner Literaturszene ist bislang nur in einigen wenigen Fallstudien untersucht worden, vor allem im Hinblick auf die Netzwerktätigkeit einzelner Schriftsteller*innen (vgl. Böttiger 2005, Berbig 2005). Der institutionalisierte Austausch, der einen Großteil der internationalen Kontakte im Osten der Stadt ausmachte, war bislang noch nicht Gegenstand weitergehender Studien 'zwar liegen allgemeine Untersuchungen zum Schriftstellerverband der DDR vor, diese erwähnen die politisch so relevante Auslandsarbeit der Organisation jedoch bestenfalls beiläufig (vgl. zum DSV allgemein Pamperrien 2004, Walther 2006, Michael et al. 1997) und betrachten lediglich einen sehr eingeschränkten Zeitraum (vgl. insbesondere zu den 1950er-Jahren Degen 2011, Gansel 1997). Die Einreisekartei des DSV, der wichtigsten nichtstaatlichen Literaturinstitution im Ostteil der Stadt, erlaubt es nun, die internationalen Kontakte und ihre Konjunkturen insbesondere in der spannungsgeladenen Zeit während des Bestehens der Berliner Mauer zu erforschen: den Verlauf dieser Aktivitäten insgesamt, die länderbezogene Einladungspolitik, die Umstände individueller Aufenthalte und ihre politische Rolle für das Herkunftsland. Sie ermöglicht auch, Literaturkontakte weniger um besonders hervorstechende Einzelpersonen zentriert zu denken und dabei gerade auch Autor*innen zu berücksichtigen, die durch Kanonisierungsprozesse der Vor- und Nachwendezeit ggf. in Vergessenheit geraten sind. Zunächst wurden die Einreisekarteien im Archiv des DSV transkribiert. Als Grundlage dafür wurden die nach Ländern und Autor*innennamen geordneten Karteien verwendet. Für jede*n einreisende*n Autor*in existiert so mindestens ein separates Blatt, auf dem die verschiedenen Aufenthalte vermerkt sind. Die Mitarbeiter*innen der Auslandsabteilung des DSV ergänzten ggf. noch biografische Informationen oder auch ein Presse- oder Passfoto. Über die Jahrzehnte änderte sich vielfach die Art der Aufzeichnung, ein Großteil der etwa 3.000 Karteien orientiert sich jedoch an dem in Tabelle 1 wiedergegebenen Schema, das am Beispiel des kubanischen Dichters Nicol√°s Guillén in Abbildung 1 illustriert werden soll.  Um die Informationen aus der nach Autor*innennamen sortierten Einreisekartei zu komplettieren, wurden auch die chronologischen Karteien herangezogen sowie etwa punktuell weitere Akten aus dem Archiv des DSV, etwa die zu etlichen Aufenthalten vorhandenen Freundschaftsverträge, Korrespondenzen und Zeitpläne. Als Ergebnis dieser Transkriptionsarbeit entstand eine Excel-Tabelle mit insgesamt 3.709 Einträgen. Die Tabelle enthält Informationen zum Zeitraum des jeweiligen Aufenthalts, den Autor*innen (Name und Staatsangehörigkeit), zu beteiligten Institutionen sowie Angaben zum Anlass bzw. Einladungsgrund. Mit OpenRefine (Version 3.5.0) wurden die in der Excel-Tabelle enthaltenen Daten vereinheitlicht. So konnten Einträge, die zwar denselben Anlass betrafen, aber unterschiedlich verschriftlicht waren, zusammengeführt werden. In einem weiteren Schritt wurden die teils in problematischer Weise notierten Autor*innennamen über OpenRefine aufbereitet und mit Normdatensätzen verknüpft. Dadurch wurde zum einen die Verifizierung bzw. Identifizierung der in der Kartei verzeichneten Einträge vereinfacht; zum anderen konnten aus den verknüpften Datenbanken weitere Informationen zu den Autor*innen importiert werden. Ein erstes umfangreiches Reconciling erfolgte mit dem Virtual International Authority File (VIAF). Als Grundlage hierfür diente der von Jeff Chiu über Codefork bereitgestellte Reconciliation Service (Version 3.0.5, Ein weiteres Reconciling wurde 'über das in OpenRefine integrierte Tool 'mit Wikidata vorgenommen. Auch hier konnte durch einige Nachjustierungen eine hohe Trefferquote von 75 % erzielt werden. Der erfolgreiche Abgleich ermöglichte nun den Import weiterer Informationen aus Wikidata, etwa Angaben zu Sprachen, Parteizugehörigkeit oder Geschlecht der Autor*innen. Zudem konnten weitere Identifier über Wikidata importiert und somit Schnittstellen zur Gemeinsamen Normdatei der Deutschen Nationalbibliothek (GND) und zum WorldCat geschaffen werden, wodurch nun auch bibliografische Informationen zu den eingeladenen Autor*innen recherchierbar sind. Jeder einzelne der hier dargestellten Schritte stellt eine Interpretationsleistung der Daten dar, die ihrerseits wieder nur heuristisch erfolgen, unvollständig und fehlerbehaftet sein kann. Bei dem über OpenRefine bereinigten und abgeglichenen Datensatz handelt es sich somit nur um eine mögliche Lesart der ursprünglichen Einreisekarteien, die der fortwährenden Überprüfung und Modifizierung bedarf.  Wegen der teils unvollständigen Datumsangaben haben wir auf das Extended Date/Time Format (EDTF) gesetzt. Dieses 2019 von der International Organization for Standardization als Erweiterung zu ISO-8601 gedachte Datumsformat erlaubt es unter anderem, verschiedene Arten von Ungewissheit formalisiert auszudrücken. Für die Zwecke dieses Projekts besonders fruchtbar ist die Einbeziehung von ""unspecified digits"" (Library of Congress 2019), die unbekannte Teile eines Datumsformats explizieren: Ein nicht spezifizierter Tag im Februar 1972 kann etwa als ""1972-02-XX"" dargestellt werden, der gleiche Fall bezogen auf einen Tag im Jahr 1986 als ""1986-XX-XX"". Darüber hinaus muss die Ungewissheit nicht zwingend von den niedrigstwertigen Stellen herrühren 'auch ""XXXX-09-24"" oder sogar ""19XX-05-XX"" sind gültige EDTF-Werte. Zwar existiert eine JavaScript-Bibliothek zum Parsen von EDTF-Datumsangaben (vgl. Keil 2022), nicht jedoch zur menschenlesbaren Darstellung. Die Logik zur sprachenübergreifenden Darstellung unvollständiger Angaben wurde deshalb eigens in TypeScript implementiert. Mit den vorliegenden Daten kann ein spezifischer Aspekt des literarischen Lebens in der DDR nun zum ersten Mal auch statistisch ausgewertet werden. Durch chronologische Verlaufsdiagramme zeichnen sich Einladungstendenzen ab, die sich unter anderem politisch deuten lassen. Ein Blick auf süd- und westeuropäische Länder zeigt nur sporadische Besuche, mit der Ausnahme Frankreichs, dessen breit aufgestellte Linke teils verstärkt mit dem Schriftstellerverband der DDR kooperierte (vgl. Fabre-Renault 2015). Auch mit Autor*innen aus dem englischsprachigen Ausland, vor allem den USA und Australien, gab es noch in den 1960er-Jahren einen vergleichsweise regen Austausch, der in den 1970er-Jahren allerdings vollends zum Erliegen kam. Dokumentieren lässt sich auch ein hohes Interesse des DSV an Autor*innen aus den sich als neutral verstehenden Staaten Finnland und Schweden, die in den 1960er-Jahren von der SED zu Schwerpunktländern auslandspropagandistischer Aktivitäten erkoren wurden: Autor*innen aus Ostblockstaaten waren jedoch weitaus regelmäßiger bei literarischen Terminen in Ostberlin zu Gast. Hier zeigen die Daten, dass sowjetische Besucher*innen stets in der Überzahl waren, ein Beleg für die Quotenregelung, die der Einladungspolitik zugrunde lag. Die von uns angebotene Schnittstelle ermöglicht viele weitere statistische Anfragen. Ihre Funktion ist aber nicht auf die projektbezogene Auswertung beschränkt. Vielmehr kann der von uns erstellte, semantisch angereicherte Datensatz auch langfristig eine Funktion im wachsenden √ñkosystem der digitalen Literaturwissenschaft übernehmen und bietet sich für den Austausch mit komplementären Projekten wie der ""Forschungsplattform Literarisches Feld DDR"" an (vgl. Gefördert durch die Deutsche Forschungsgemeinschaft (DFG) im Rahmen der Exzellenzstrategie des Bundes und der Länder innerhalb des Exzellenzclusters Temporal Communities: Doing Literature in a Global Perspective 'EXC 2020 'Projekt-ID 390608380."
2023,DHd2023,JUNG_Kerstin_Die_Wahl_der_Mittel___Jupyter_Notebooks_als_For.xml,Die Wahl der Mittel 'Jupyter-Notebooks als Forschungsinfrastruktur,"Kerstin Jung (Universität Stuttgart, Institut für Maschinelle Sprachverarbeitung); Pascal Hein (Universität Stuttgart, Institut für Literaturwissenschaft); André Blessing (Universität Stuttgart, Institut für Maschinelle Sprachverarbeitung); Jan Hess (Deutsches Literaturarchiv Marbach); Volodymyr Kushnarenko (Höchstleistungsrechenzentrum, Universität Stuttgart)","Jupyter-Notebooks, ausführbarer Quellcode, Python","Teilen, Programmierung, Veröffentlichung, Visualisierung, Infrastruktur, Software","Mit Python als vielgenutzter Programmiersprache in den Digital Humanities Im Forschungskontext werden solche Notebooks daher verwendet, um auf einer (Web-)Seite Datensätze einzulesen, zu analysieren, visualisieren und die verwendete Methodik zu erläutern, ohne dies auf verschiedene Orte oder Zugänge verteilen zu müssen. In der (Nach-)Nutzung können z. B. Parameter in der Analyse oder Visualisierung direkt im Browser verändert werden und eine Anpassung ohne Programmierkenntnisse oder -erfahrung ermöglichen. Die Notebook-Dateien können wiederum über entsprechende Softwareentwicklungs-Repositorien zur Verfügung gestellt werden, was Anpassungen für weitere Datensätze oder Forschungsfragen erlaubt. Jupyter-Notebooks sind dabei als JSON-Dokumente strukturiert verarbeitbar. Im Rahmen unseres Projekts geht es uns um die Möglichkeit, Jupyter-Notebooks so zur Verfügung zu stellen, dass sie für eine sehr heterogene Nutzendengruppe (u. a. Autor*innen, Forschende, Schüler*innen) einen Mehrwert bedeuten. Zu den Vorteilen der Bereitstellung von Zugängen zu Daten und Analysen durch Notebooks gehören (i) die Möglichkeit, ein Angebot an eine breite Nutzendengruppe zu machen: Je nach Aufbereitung der Notebooks (interaktive Elemente wie Dropdown-Menüs oder Range-Sliders sind möglich) können sie fast ohne Vorkenntnisse mit Python betrieben werden und an das individuelle Forschungsinteresse angepasste Ergebnisse produziert werden, (ii) dass die technischen Voraussetzungen, z. B. benötigte Pakete, im Notebook selbst spezifiziert sind. Diese Vorteile kommen allerdings nur in einer konfigurierten Ausführungsumgebung zum Tragen. Werden nur die Jupyter-Notebook-Dateien bereitgestellt, setzt das bei den Nutzenden Kenntnisse in Python, Bash o. Ö. sowie im Umgang mit Jupyter voraus. Oft sind Pakete in aufeinander abgestimmten Versionen erforderlich oder in Abhängigkeit vom Betriebssystem verfügbar, so dass nur eine vorkonfigurierte Umgebung den Nutzenden tatsächlich die technischen Hürden abnimmt. So stellt sich die Frage, in welchem Rahmen ausführbare Jupyter-Notebooks zur Verfügung gestellt werden können. Der Betrieb einer zugänglichen Ausführungsumgebung (""Hub"") setzt Hardware, Administrations- und Wartungskapazitäten voraus. Eine Nutzungsverwaltung (Vergabe und Pflege von Accounts, Monitoring von Speicher- und Rechenkapazitäten) ist dabei ebenso unerlässlich wie Aktualisierungen mittels Updates auf Ebene von Maschine, Hub und Paketen und damit verbundene Wartungsarbeiten durch Abhängigkeiten in den Notebooks. Der Betrieb einer nachhaltigen Ausführungsumgebung setzt dies für einen längeren Zeitraum voraus, so dass die Idee der eigenen Ausführungsumgebung den Rahmen eines Forschungsprojekts oft übersteigt. Des Weiteren muss der Sicherheitsaspekt berücksichtigt werden, da es sich bei ausführbaren Jupyter-Notebooks um ausführbaren Quellcode handelt, der gewollt oder ungewollt Schaden am eigenen oder an externen Systemen verursachen kann. Mit dem Service Colaboratory Eine Alternative hierzu kann der Betrieb einer stark restringierten Ausführungsumgebung sein, die zwar die vorhandenen Notebooks abspielen kann und Nutzende ggf. aus vorgegebenen Parametern wählen lässt, Forschenden aber kaum Flexibilität bezüglich einer eigenen Exploration oder Einbindung weiterer Pakete ermöglicht. Sofern spezifische technische Expertise angenommen werden kann, ist eine weitere Möglichkeit, Docker-Container zum Download zur Verfügung zu stellen oder eine detaillierte Dokumentation zur Nutzung eines Notebooks innerhalb einer integrierten Entwicklungsumgebung zu liefern. Die Zielgruppe wird damit allerdings auf Nutzende der entsprechenden Infrastruktur einschränkt. Notebooks, die über bestimmte Repositorien öffentlich zur Verfügung gestellt werden, können über Binder Ein entsprechender Ansatz für die breite Forschungscommunity wäre ein großer Gewinn bezüglich der Verfügbarmachung, Nachnutzung und Dokumentation von Forschungsmethoden und -ergebnissen."
2023,DHd2023,HINZMANN_Maria_SPARQL_für__digitale__Geisteswissenschaftler_.xml,SPARQL für (digitale) Geisteswissen-schaftler:innen 'Querying Wikidata und die MiMoTextBase,"Maria Hinzmann (Trier Center for Digital Humanities, Universität Trier, Deutschland); Anne Klee (Trier Center for Digital Humanities, Universität Trier, Deutschland); Johanna Konstanciak (Trier Center for Digital Humanities, Universität Trier, Deutschland); Julia Röttgermann (Trier Center for Digital Humanities, Universität Trier, Deutschland); Christof Schöch (Trier Center for Digital Humanities, Universität Trier, Deutschland); Moritz Steffes (Trier Center for Digital Humanities, Universität Trier, Deutschland)","Linked Open Data, SPARQL, Literaturgeschichte, Wikidata","Modellierung, Visualisierung, Literatur, Metadaten, benannte Entitäten (named entities), Software","Nicht nur in Kultur- und Gedächtnisinstitutionen, auch in DH-Projekten ist derzeit eine Zunahme des Linked Open Data-Paradigmas sichtbar. Wie können Daten im Sinne von ""Open Data, Open Cultures"" offen, gut zugänglich, interoperabel vernetzt, maschinenlesbar und langfristig verfügbar dargeboten werden? Im Projekt "" Der Workshop setzt es sich zum Ziel, theoretisches und praktisches Wissen zur Modellierung geisteswissenschaftlichen und speziell literaturgeschichtlichen Wissens in Form von Linked Open Data (LOD) zu vermitteln, Einblick in die Syntax der Abfragesprache SPARQL zu geben und den Mehrwert der Aufbereitung von Daten als Wissensgraphen in Anwendungsszenarien aufzuzeigen. Dabei liegt der Schwerpunkt auf der Vermittlung von SPARQL in theoretischen und praktischen Sessions. Teilnehmende sollen die Kompetenz erlangen, die Struktur von SPARQL zu verstehen und eigenständig Queries zu schreiben. Es ist zu beobachten, dass es ein zunehmendes Interesse in der DH-Community gibt, die eigenen Daten in Form von LOD zu veröffentlichen und mit dem Semantic Web zu vernetzen oder die aktuellen Entwicklungen zu reflektieren (Hogan et al. 2021; Ikoniƒá Ne≈°iƒá et al. 2021; Thornton et al. 2021; Alves 2022; Dörpinghaus 2022; Ohmukai / Yamada 2022; Zhao 2022). Auch das Projekt "" SPARQL (SPARQL Protocol and RDF Query Language) ist eine 2008 vom W3C veröffentlichte, graphenbasierte Abfragesprache für RDF (Resource Description Framework). RDF ist ein Datenmodell, mit dem sich Ressourcen im World Wide Web darstellen lassen. Es ist der zentrale Standard des W3C, der semantische Daten in der charakteristischen Tripel-Struktur bestehend aus ""Subjekt 'Prädikat 'Objekt"" repräsentiert. Ausgehend von einem einzelnen solchen Tripel wird die Struktur eines Knowledge Graphen im Workshop entfaltet und die ""Übersetzung"" von Forschungsfragen in natürlicher Sprache in die SPARQL-Syntax erläutert. Die Abfragesprache SPARQL setzt sich aus mehreren Bausteinen zusammen: SPARQL-Abfragen werden häufig innerhalb eines einzelnen Knowledge Graphen gestellt. Es besteht jedoch auch die Möglichkeit, über mehrere Knowledge Graphen hinweg Abfragen zu stellen, sogenannte  Der Workshop vermittelt Grundlagenwissen und Möglichkeiten, die das LOD-Paradigma bietet. Der im Projekt erstellte multilinguale Wissensgraph MiMoTextBase zur Domäne der französischen Literatur des 18. Jahrhunderts soll dabei als Anschauungsbeispiel dienen. Der Workshop möchte praktisches Wissen vermitteln: Wie schreibt man SPARQL-Queries? Welchen Mehrwert kann ein Knowledge Graph für literaturgeschichtliche Fragen im Besonderen und die Geisteswissenschaften im Allgemeinen bieten? In dem halbtägigen Workshop wird der Wissensgraph Konkrete Lernziele sind: Erwerb von Grundlagenwissen zu Semantic Web und RDF, LOD, Wikidata Graph; vertiefte Kenntnisse zu SPARQL und die praktische Fähigkeit, eigene SPARQL-Queries zu formulieren; Kennenlernen der Software Wikibase und Exploration der Visualisierungsmöglichkeiten des SPARQL-Endpoints. Der Workshop wendet sich an digitale Geisteswissenschaftler:innen mit Interesse an LOD und SPARQL. Spezielle Vorkenntnisse sind nicht notwendig. Teilnehmende benötigen einen Laptop. Der Workshop setzt sich aus aufeinander aufbauenden Sessions zusammen, die jeweils Input-Phasen und Übungsphasen verbinden. Es wird vorab eine ausführliche Tutorial-Seite (inklusive Verlinkung auf weitere hilfreiche Ressourcen zum SPARQL-Lernen) zur Verfügung gestellt, die den Teilnehmenden (und allen weiteren Interessierten) in der Vorbereitung sowie zur Vertiefung nützlich sein kann (Hinzmann et al. 2022b). Im Zentrum des Workshops stehen drei Blöcke mit jeweils unterschiedlichem Schwerpunkt, in denen das Formulieren von SPARQL-Queries geübt wird (vgl. für Details den Ablauf im Appendix). Auch Teilnehmende ohne Vorkenntnisse werden schrittweise an zunehmend komplexere Queries herangeführt. Der Schwierigkeitsgrad wächst innerhalb der einzelnen Blöcke, wobei der Fokus auf dem eigenständigen Formulieren sowie Anpassen von Beispiel-Queries und dem Klären aller dabei auftretenden Fragen liegen wird. 1. Im ersten Teil liegt der Fokus auf Abfragen zu literarischen Werken. Im Hinblick auf SPARQL geht es hier zunächst um die zentralen Grundlagen wie das Schreiben einfacher 2. Im zweiten Teil widmen wir uns Wikidata als größtem öffentlichen Wissensgraphen, der sich zugleich als ""Hub"" begreifen lässt (Neubert 2017), und fokussieren Autor:innen als Entitäten. Autor:innen sind in allen geisteswissenschaftlichen Disziplinen relevant und ein wichtiges Scharnier zwischen verschiedenen Wissensgraphen. Bezogen auf die SPARQL-Syntax gehen wir einen Schritt weiter und integrieren Funktionen wie OPTIONAL und FILTER, um das Spektrum der Abfragemöglichkeiten zu erweitern. Ein Einstieg wird hier mit Queries zu Literat:innen der MiMoText-Domäne gemacht. Im nächsten Schritt können die Teilnehmenden die Daten von Autor:innen in ihrer jeweiligen Domäne in Wikidata explorieren. 3. Der dritte Teil verknüpft die beiden vorigen Teile auf mehreren Ebenen. Der Schwerpunkt liegt auf Es soll in der abschließenden Diskussion auch Raum sein, einen kritischen Blick auf Entwicklungen im Bereich des Semantic Web zu werfen, beispielsweise die Frage, welche Monopolisierungskräfte und Marktkräfte Einfluss nehmen (van Hooland / Verborgh 2014, 247–48; Singhal 2012). Zum Abschluss werden die wichtigsten Anwendungsmöglichkeiten und Fragen zusammengetragen und weiterführende Ressourcen (DuCharme 2013; van Hooland / Verborgh 2014; Lincoln 2015; Blaney 2017) sowie bei Interesse Möglichkeiten der Kooperation thematisiert. Maximale Zahl der Teilnehmenden: 25. Wir benötigen einen Raum mit WLAN und Beamer und bieten gern ein Hybrid-Szenario an. ""Mining and Modeling Text"" (Universität Trier, Trier Center for Digital Humanities) wird von der Forschungsinitiative des Landes Rheinland-Pfalz 2019-2023 gefördert. Der Workshop wird von Mitarbeiter:innen des LOD-Projekts ""Mining and Modeling Text"" durchgeführt. Das interdisziplinäre Projekt verfügt über einen eigenen SPARQL-Endpoint und wurde in Wikibase implementiert. Maria Hinzmann; hinzmannm@uni-trier.de; Trier Center for Digital Humanities, Universität Trier | Historisches Seminar: Digital Humanities, Bergische Universität Wuppertal; Forschungsinteressen: Datenmodellierung, LOD, Textanalyseverfahren. Anne Klee; klee@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: Digitale Textverarbeitung; Digitale Lexikographie. Johanna Konstanciak; konstanciak@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: Digitale Textverarbeitung; XML/Web-Technologien. Julia Röttgermann; roettger@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: LOD, Textmining-Verfahren wie Topic Modeling, NER und Sentiment Analysis. Christof Schöch; schoech@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: Computational Literary Studies. Moritz Steffes; steffesm@uni-trier.de; Trier Center for Digital Humanities; Forschungsinteressen: Softwaresysteme, Semantic Web Technologien, Forschungsinfrastrukturen."
2023,DHd2023,CUGLIANA_Elisa_Coding_editions__Computational_approaches_to_.xml,Coding editions. Computational approaches to the editing of pre-modern texts.,"Elisa Cugliana (CCeH - Universität zu Köln, Deutschland)","computational edition, normalisation, processing","Programmierung, Modellierung, Annotieren, Bearbeitung, Manuskript, Text","The endlessness of the digital space allows scholarly editors to conceive edition projects on a grand scale. Indeed, the eternal mission of philology can be identified in the quest for the best way to represent and/or reconstruct primary sources, in order to save them from oblivion and grant the public the most informed access to them. Consequently, the lack of space boundaries characterising the digital environment is one of the first aspects making such an environment perfectly suitable for hosting ambitious edition projects. Considering then the possibilities disclosed by hypertextuality, multimodality and multimediality, one can hardly argue against the digital way of editing, despite the ongoing challenges it must face (cf. for instance Rosselli Del Turco 2016), of which sustainability is clearly in the forefront. However, there are reasons to believe that state-of-the-art digital editions have not yet overcome the limits of a still rather bookish paradigm, obeying a mostly representational logic (Van Zundert 2018, Cugliana and Van Zundert 2022). In this contribution, I will argue that the key to the next level of digital scholarly editing is to be found in computation, that is, in the actual coding of the whole editorial workflow. I will present the theory, the advantages and the challenges of such a computational approach to the editing of pre-modern texts, backing up my claims with examples from the praxis and from my own scholarly work in the field of digital philology. Of course, the field of computational editing is still in its infancy, which means that fully computational editions have not been published yet. However, there are projects While the digital paradigm has been widely described and commented upon (among others by Stella 2007 and Sahle 2016), there is a crucial aspect that has not been sufficiently underlined, which however could represent a turning point in the history of (digital) philology. It is the case of the use of programming code throughout the different phases of the editorial process, aiming at the realisation of what Barabucci and Fischer (2017) defined as the formalisation of textual criticism. The authors, in their conclusions, state that a ""shared formalization would lead to the semi-automatization of the editorial process"", where ""the responsibility of the editors would be to describe their choices and decisions"", while that of the computers would be to ""deal with applying these rules and decisions in the best way"". The act of formalising the competence of the editor is to be seen as an achievement If methods are there to realise theoretical principles in the best possible way, at the same time they can in turn influence the principles themselves, opening up new perspectives uncovering, for instance, implicit biases and illogicalities. Indeed, the use of computation for the modelling and operationalisation of editorial knowledge can contribute to perfecting the editorial workflow in the way envisioned by McCarty (2005), who referred to the ""meaningful surprise"" often arising from the process of modelling and computing. Some examples from the actual application of this approach will hopefully prove its potential for the field of Digital Scholarly Editing. During my PhD, which I completed in February 2022, I edited an Early High German version of Marco Polo""s travel account, also known as ""Version DI"" of the Together with my colleague Gioele Barabucci (Norwegian University of Science and Technology), we developed a method based on a rule-and-exception principle, featuring three XProc pipelines, one for each level of normalisation (Cugliana and Barabucci 2022). Each pipeline consists of a series of XSLT stylesheets which deal with the different steps of the normalisation process, such as the levelling of allographs, the expansion of abbreviations, the regularisation of capitalisation etc. This proved to be a very suitable strategy for dealing with the complexity of the normalisation process. Concerning the choice of XProc pipelines, it has already been shown that small-step pipelines making use of a stateless language such as XSLT prove to be advantageous in that they reduce the complexity of computer programs, they can be easily shared with the peers and they improve the sustainability of the code (Barabucci and Schaeben 2021). Not only were the pipelines successful in generating three levels of normalisation of the texts, but they could also be applied to the transcriptions of all the witnesses edited, despite the fact that some were written in the East-Swabian dialect, and some in Bavarian. This was probably due to the geographical proximity and the contemporaneity of their production, which leads to hypothesise the possibility of creating ""pools of rules"" for the editing of witnesses written in specific areas and historical periods. In my talk I will present the system at the basis of the normalisation of the texts featured in the edition of DI, giving some insights into the very development of the pipelines, both from a strictly philological and from a computational perspective. In particular, I will focus on some tricky aspects such as the cases of ambiguities and exceptions, which might represent a hurdle for the full systematisation of the editorial workflow. A case of reuse of the normalisation pipelines written for Version DI of the  As can be evinced from this abstract, my contribution has a twofold purpose: on the one hand giving a short, but hopefully convincing introduction to the theoretical aspects justifying the scholarly value of a computational approach to editing and, on the other, presenting some meaningful results obtained in the praxis. In particular, the focus will mainly be on the success and challenges of the normalisation pipelines developed in collaboration with Gioele Barabucci, showing what it actually meant to translate into code the usually very analogue task of normalising medieval texts, what problems we encountered and how we solved them. Finally, as an outlook for the possible applications of the computational principle, I will briefly illustrate the work I am doing for my next project, the edition of the"
2023,DHd2023,CALVO_TELLO_José_GND_und_Normdaten_für_europäische_Literatur.xml,GND und Normdaten für europäische Literatur? Personen und Werke in den multilingualen Korpora von ELTeC,"José Calvo Tello (Niedersächsische Staats- und Universitätsbibliothek Göttingen, Georg-August-Universität Göttingen); Nanette Rißler-Pipka (Gesellschaft für wissenschaftliche Datenverarbeitung mbH (GWDG)); Florian Barth (Niedersächsische Staats- und Universitätsbibliothek Göttingen, Georg-August-Universität Göttingen)","Normdaten, Mehrsprachigkeit, Korpora, GND, ELTeC, Wikidata, VIAF, Literatur","Bereinigung, Identifizierung, Literatur, Metadaten, Personen, Text","Viele Projekte in den Digital Humanities verwenden Identifier für Entitäten wie Personen, Werke, Körperschaften oder Orte, die auf eindeutige Einträge in Normdaten-Verzeichnissen oder in Knowledge Bases verweisen Nichtsdestotrotz ist die Sprache des Forschungsobjekts (z.¬†B. von Textkorpora) und die Wahl der Normdaten-Ressource stark voneinander abhängig. Einige Projekte, die mit deutschsprachigen Texten arbeiten, haben sich für die GND entschieden, um Personen oder Werke zu identifizieren, u.¬†a. die Digitale Bibliothek im TextGrid Repository, Die Bibliotheken und Fachinformationsdienste (FIDs) vieler Philologien in Deutschland verwenden die GND für die Sacherschließung ihrer Titel. Sie reichern umgekehrt die GND mit immer mehr Daten zu fremdsprachigen Autor*innen und deren Werke an. Es liegt nahe, dass die GND weiterhin hauptsächlich Autor*innen und Werke aus dem deutschsprachigen Raum verzeichnet. Das heißt jedoch nicht, dass die GND eine Quelle ist, die sich nur für die Germanistik eignet. Generell ist die GND in Form von Agenturen organisiert, die sich auf viele Bibliotheken und andere Institutionen in Deutschland verteilen und von der DNB koordiniert werden. Daher fragen wir uns: Wie stark ist das Ungleichgewicht innerhalb der GND zwischen Einträgen zu deutsch- und fremdsprachiger Literatur? Unsere Frage beantworten wir anhand der multilingualen Korpora von ELTeC. Dabei handelt es sich um literarische Korpora in verschiedenen europäischen Sprachen, die in der COST-Action Distant Reading erstellt wurden Die Personen und Werke in ELTeC sind teilweise bereits mit Wikidata, GND oder VIAF eindeutig identifiziert. Die Abbildungen 1 und 2 zeigen im Vergleich die Annotation mit Normdaten für Autor*innen und Werke pro Sprache und die Wahl der Normdatenressource für die Identifikation von Autor*innen. Für die Sprachen Griechisch (GRE), Kroatisch (HR), Litauisch (LIT) und Rumänisch (ROM) konnten offenbar keine Normdaten verwendet werden. Um die Vergleichbarkeit der Ergebnisse zu gewährleisten, werden diese Sprachen bei den folgenden Analysen ausgeschlossen. Außer für das französische (FR) und in kleinen Teilen für das spanische (SPA), serbische (SRP) und ukrainische (UKR) Korpus wurden keine Werknormdaten eingetragen. Für die Autor*innen wurde überwiegend VIAF genutzt. Nur das französische und norwegische Korpus wurde auch mit Wikidata und das deutsche Korpus als einziges mit GND-IDs versehen. Wir gehen in zwei Schritten vor, um ein vollständiges Bild über die mögliche Abdeckung mit Normdaten zu erhalten. Zunächst extrahieren wir die IDs der Autor*innen aus den TEI-Dokumenten der ELTeC-Korpora. Anhand der IDs werden die fehlenden Identifier aus Wikidata, GND und VIAF extrahiert. Das gelingt für die GND über die API von Lobid, Im nächsten Schritt werden die Werke mit Rückgriff auf die Autor*innen-ID identifiziert. Auch wenn für vier ELTeC-Korpora Werk-IDs (überwiegend mit VIAF) bereits vom Projekt erfasst wurden, ignorieren wir diese, um die gleiche Methode für alle Korpora anzuwenden. Wir führen drei parallele Ausgehend von den bereits in ELTeC identifizierten Autor*innen (vgl. Abb. 1) wird überprüft, ob diese auch in den jeweils anderen Normdatenressourcen (GND, Wikidata, VIAF) vorhanden sind. Daher sind hier Sprachen, für die keine Normdaten in ELTeC existieren (Griechisch, Kroatisch, Litauisch, Rumänisch), nicht berücksichtigt.  Abbildung 3 zeigt die Summe der Normdaten zu Autor*innen pro Ressource, die in den hier betrachteten Korpora gefunden oder ergänzt werden konnten. Für alle drei Ressourcen ist die Abdeckung hier sehr gut. Die GND liegt im Vergleich nur leicht zurück. Die Verteilung dieser Daten pro Sprache wird in Abbildung 4 gezeigt. Das Bild entspricht der Zusammenfassung aus Abbildung 3. Erwartungsgemäß hat das deutsche Korpus die höchste Quote in der GND. Das norwegische Korpus kann mehr Treffer mit Wikidata als mit VIAF erzielen. Für Tschechisch und Polnisch erreichen sowohl VIAF als auch Wikidata sehr gute Ergebnisse. Neben dem Deutschen bietet die GND eine gute Abdeckung für Sprachen wie Englisch, Französisch, Italienisch, Norwegisch, Schwedisch und Ukrainisch.  Das Bild ändert sich erwartungsgemäß, wenn nicht Autor*innen, sondern Werke in den drei Ressourcen gesucht werden (vgl. Abb.5). Während VIAF und Wikidata mehr als 700 Werke aus ELTeC verzeichnen, erreicht die GND nur knapp über 200.  Um diese Zahlen besser zu verstehen, zeigt Abbildung 6, dass nur das deutsche Korpus akzeptable Ergebnisse aus der GND erreicht (80 % der Werke). Alle anderen Sprachen bewegen sich zwischen null und knapp über 40 %. Die Abdeckung von Wikidata oder VIAF ist für viele Sprachen deutlich höher: von 60 % bis zu 100 %.  Entscheidend für eine Bewertung der Ressource ist nicht nur, ob eine Entität vorhanden ist, sondern wie gut sie mit Metadaten beschrieben ist. Für die GND ist zu erwarten, dass Entitäten aus dem deutschsprachigen Raum ausführlicher beschrieben werden als Entitäten aus anderen Regionen. Um dies zu messen, werden die Daten von allen ELTeC-Autor*innen und deren Werke als XML-RDF-Dokumente aus der GND heruntergeladen und die Anzahl der XML-Elemente quantifiziert. Ohne den semantischen Gehalt der Elemente zu bewerten, gehen wir davon aus, dass mehr Elemente auch mehr Informationen pro Entität bedeuten. Abbildung 7 zeigt daher für die GND die Anzahl der Elemente pro Autor*in. Während für Autor*innen aus dem deutschsprachigen Raum 200-330 Elemente vorhanden sind, werden nur 100-240 für andere Sprachen verzeichnet. Auch wenn Sprachen wie Französisch oder Ukrainisch mittlere Werte (bis 240) zeigen, ist der Abstand zwischen diesen Sprachen und dem Deutschen immer noch sehr groß.  Für die Werke (vgl. Abb. 8) erreichen die deutschsprachigen Entitäten wieder deutlich höhere Werte als alle anderen Sprachen in der GND. Hier ist der Unterschied im Vergleich weniger groß, weil insgesamt für Werke weniger Elemente angelegt werden.  Für einen Vergleich wurden diese Daten auch aus Wikidata extrahiert (Abbildungen 9 und 10). Wir prüfen, ob in Wikidata ähnliche Verzerrungen gegenüber dem Englischen oder anderen Sprachen zu beobachten sind. Jedoch hat in Wikidata keine Sprache einen so klaren Vorsprung im Vergleich zu allen anderen Sprachen wie das Deutsche in der GND. Insgesamt ist ein möglicher Grund für die bessere Abdeckung für Autor*innen und Werke in Wikidata und VIAF ist die Tatsache, dass die Teilnehmenden von ELTeC die Entitäten in Wikidata selbst eingetragen haben Eine weitere Hypothese ist, dass der Kanonisierungsgrad die Abdeckung in den drei Ressourcen beeinflusst. In den ELTeC Korpora wurde dies anhand des Metadatenfelds ""reprints"" belegt. Für Autor*innen und Werke, die keine oder wenige ""reprints"" (""low"") haben, zeigt Abbildung 13, dass alle drei Ressourcen eine niedrigere Abdeckung haben. Dabei ist der Unterschied für die GND deutlich größer als bei VIAF und Wikidata. Die Daten deuten darauf hin, dass die GND stärker vom Kanonisierungsgrad beeinflusst ist als die anderen zwei Ressourcen. Besonders niedrig ist die Abdeckung von nicht kanonisierten Werken in der GND (Abb. 13, ""low reprints""). Zu beachten ist, dass die Verteilung von solchen Metadaten in den ELTeC Korpora nicht gleichmäßig ist. Die Ergebnisse können dementsprechend allein durch die Zusammenstellung der Korpora und die Metadatenanreicherung beeinflusst sein.  Wie gut können nicht-germanistische Projekte aus dem deutschsprachigen Raum mit der GND Autor*innen und Werke identifizieren? Sollten sie lieber auf Wikidata oder VIAF zurückgreifen? Um das zu beantworten, wurden die multilingualen Korpora von ELTeC analysiert. Auch wenn diese Korpora nicht vollständig ausgewogen hinsichtlich Repräsentation der Inhalte und der Persistenz ihrer Identifier sein können, sind sie eine wertvolle Ressource von und für die Community. Weitere ähnliche Evaluationen könnten in Zukunft durchgeführt werden, wenn umfassenderes Vergleichsmaterial identifiziert oder zusammengestellt wird. Das ELTeC Korpus ist zeitlich (19. Jh.), quantitativ und sachlich (100 Romane pro Sprache) notwendig beschränkt und vor diesem Hintergrund sind auch die vorliegenden Ergebnisse zu betrachten. Generell zeigt die GND eine gute Abdeckung von Personendaten und ist damit sehr nah an Wikidata oder VIAF. Jedoch füllt die GND deutlich mehr Felder (d.h. mehr Informationen) zu deutschen Autor*innen als zu anderen europäischen Autor*innen (für Personendaten außerhalb der Literatur mag das anders aussehen). Hinsichtlich der Werknormdaten kann die GND nur für das Deutsche akzeptable Ergebnisse liefern. Auch für andere große Sprachen wie Französisch, Englisch oder Spanisch enthält die GND nur 40 % der enthaltenen Werke in ELTeC. Nicht nur die Abdeckung, sondern auch der Informationsgehalt ist für Werke deutschsprachiger Autor*innen höher als für alle anderen Sprachen. Darüber hinaus scheint die GND stärker vom Kanonisierungsgrad abhängig zu sein als VIAF oder Wikidata. Wenn die GND damit als national-ausgerichtete Normdateninstitution erwartbar schlechter abschneidet, dann wäre zu prüfen, ob die Normdaten anderer Einrichtungen (vor allem von Nationalbibliotheken) eine ähnliche oder sogar stärkere Favorisierung der eigenen Sprache verzeichnen. Durch die Einbindung der GND (der DNB und GND-Agenturen in anderen Bibliotheken) in die NFDI öffnet sich die GND nicht nur der Community, sondern es werden auch wichtige Diskussionen zu Multilingualität (GNDmul)"
2023,DHd2023,HATZEL_Hans_Ole_Narrativität_und_Handlung.xml,Narrativität und Handlung: Zum Verhältnis von Handlungs- zusammenfassungen und relevanten Ereignissen,Hans Ole Hatzel (Universität Hamburg); Evelyn Gius (Technische Universität Darmstadt); Haimo Stiemer (Technische Universität Darmstadt); Chris Biemann (Universität Hamburg),"Narrativität, Handlung, Erzählwürdigkeit, Zusammenfassung, Ereignisse, Events, Ereignis, Event, pyramide, pyramid, n-gram, bleu, ruge, Ereignishaftigkeit, Pyramiden-Methode, Pyramiden Methode","Inhaltsanalyse, Annotieren, Theoretisierung, Bearbeitung, Daten, Literatur","Welche Ereignisse in Erzähltexten sind besonders relevant? Diese Frage wird in der Literaturwissenschaft im Kontext von verschiedenen Konzepten verhandelt. So können relevante Ereignisse identifiziert werden, indem man die für die Textinterpretation als besonders wichtig erachteten Stellen (so genannte ""Schlüsselstellen"") betrachtet (Arnold & Fiechter, 2022). Auf die Rezeption orientiert sind ebenfalls die empirische Leser:innenforschung (Groeben 1977; Miall & Kuiken 2001) oder die Rezeptionsästhetik (Iser 1976). Steht hingegen der Text im Fokus, kann die Frage nach der Wichtigkeit von Ereignissen in Bezug auf Ereignishaftigkeit oder die so genannte Erzählwürdigkeit untersucht werden (z. B. Hühn 2014; Baroni 2012). Allen Ansätzen gemeinsam ist, dass sie bestimmte Qualitäten von Texten bzw. Textbestandteilen betrachten.  Im Fortgang des Projektes wollen wir überprüfen, inwiefern zwischen diesen grundlegenden Ereignistypen, die auch unter dem Konzept ""Event I"" subsumiert werden können, und besonders erzählwürdigen Ereignissen bzw. so genannten Events II, eine Verbindung besteht. Was bereits mit unseren bestehenden Daten und ohne die weitere Operationalisierung des Event II-Konzepts möglich ist, ist der Abgleich unserer Annotationen mit als besonders handlungsrelevant markierten Textstellen. Diesen Vergleich stellen wir im vorliegenden Beitrag an, indem wir unsere Annotationen von Ereignissen in literarischen Texten mit Zusammenfassungen der entsprechenden Texte abgleichen. Wir gehen davon aus, dass Textstellen durch ihre Erwähnung in Zusammenfassungen als für die Handlung wichtig markiert werden. Für den Abgleich dieser Textstellen mit unseren Narrativitätsverläufen nutzen wir drei Typen von Zusammenfassungen: (1) semiprofessionelle Zusammenfassungen, die Studierende der Literaturwissenschaft verfasst haben, (2) professionelle Zusammenfassungen aus Kindlers Literatur Lexikon und (3) nutzer:innengenerierte Zusammenfassungen der Online-Enzyklopädie Wikipedia. Die Verwendung dieser verschiedenen Zusammenfassungstypen zielt darauf ab, zu analysieren, welche Aspekte bzw. Textstellen für die jeweiligen Zusammenfassungstypen relevant sind und dadurch Rückschlüsse auf ihre Qualität zu ziehen. Die (1) Zusammenfassungen der Studierenden waren Teil der Studienleistungen in einem Seminar. Sie wurden als explizit auf die Handlung bezogene Zusammenfassungen verfasst, die eine maximale Länge von 20 Sätzen haben durften. Außerdem wurden die Studierenden aufgefordert, keine Hilfsmittel (wie Zusammenfassungen auf Wikipedia oder aus Literaturlexika) zu nutzen. Für die vier genutzten Primärtexte Aus den Beiträgen des (2) Kindler-Literaturlexikons und von (3) Wikipedia wurden nur jene Passagen verwendet, die sich auf die Handlung in den Primärtexten beziehen. Passagen, die sich der Autor:in, Rezeption oder Interpretation widmen, wurden nicht berücksichtigt. Durch eine kollaborative Annotation der einzelnen Sätze wurde deren Bezug auf die Handlung des Textes annotiert und ein entsprechender Goldstandard erstellt, der den weiteren Analysen zugrunde liegt. So wurde sichergestellt, dass alle drei Zusammenfassungstypen handlungsorientiert sind. Die Zusammenfassungen wurden dahingehend annotiert, dass jeder Satz der Zusammenfassung mit einer Referenz auf alle Spannen des Primärtextes versehen wurde, auf die er Bezug nimmt. Um die Qualität der Zusammenfassungen und mögliche Unterschiede zwischen den drei Zusammenfassungstypen zu analysieren, evaluieren wir deren Öhnlichkeit. Dazu nutzen wir drei Metriken, mit denen implizit drei unterschiedliche Auffassungen von Öhnlichkeit verbunden sind: Eine weitgehend lexikalische (N-Gramme), eine Metrik auf Basis distributioneller Semantik (Word Embeddings) und eine, die sich weitgehend von der sprachlichen Struktur löst und inhaltsbezogene Vergleiche vornimmt (adaptierte Pyramiden-Methode). Wir nehmen zunächst an, dass die semiprofessionellen Zusammenfassungen durchweg handlungsbezogen sind. Deshalb vergleichen wir jeweils eine semiprofessionelle Zusammenfassung mit allen anderen semiprofessionellen und jede Zusammenfassung aller anderen Typen mit allen semiprofessionellen. Als erstes berechnen wir BLEU- (Papineni et al., 2002) und ROUGE-Scores (Lin et al., 2004), die Öhnlichkeiten unter Zusammenfassungen als N-Gramm-Öhnlichkeit abbilden. Wir gewichten BLEU-{1,2,3} und ROUGE-{1,2,3} jeweils gleich und quantifizieren so die Überlappung von 1-, 2- und 3-Grammen zwischen den unterschiedlichen Texten und geben für BLEU die Precision und ROUGE den F1-Score an. Anhand der Scores in Tab. 1 und Tab. 2 wird ersichtlich, dass die semiprofessionellen Zusammenfassungen nahezu durchgehend die höchsten Öhnlichkeitswerte aufweisen. N-Gramm basierte Metriken haben den Nachteil, dass kleine Unterschiede in der Wortwahl zu einer deutlich geringeren Öhnlichkeit führen können. Um Vergleiche stärker auf die Semantik zu fokussieren, wurde mit BERTScore (Zhang et al., 2020) eine embedding-basierte Methode etabliert. Wenden wir diese auf unsere Texte an, zeigt sich ein deutlich geringerer Unterschied der Zusammenfassungstypen (siehe Tab. 3). Dies weist darauf hin, dass Unterschiede in der N-Gramm-basierten Bewertung zu einem großen Teil auf Unterschiede in der Wortwahl zurückzuführen sind. Für den letzten Vergleich der Zusammenfassungen adaptieren wir die Pyramiden-Methode, die für die automatische Evaluation maschinell generierter Zusammenfassungen entwickelt wurde (Nenkova et al., 2004). Die Zusammenfassungen werden auf Basis von sogenannten Summary Content Units (SCU) mit Referenzzusammenfassungen verglichen. Eine SCU repräsentiert dabei eine semantische, inhaltliche Aussage aus dem Zusammengefassten. Die namensgebende Pyramide repräsentiert dabei das Vorkommen der unterschiedlichen SCUs in der Menge der Referenzzusammenfassungen, wobei die Höhe der Pyramide n der Anzahl der Referenzzusammenfassungen entspricht. Dabei ist in der Regel eine Verteilung zu beobachten, die tatsächlich eine Pyramide aufbaut: eine SCU taucht in allen Wir passen die Pyramiden-Methode in zwei Punkten an unsere Fragestellung an. Zum einen haben wir keine Referenztexte, sondern benutzen das Verfahren zum Vergleich verschiedener Zusammenfassungen. Zum anderen enthalten unsere Daten keine SCUs, diese werden deshalb über Textspannen approximiert. Dafür nehmen wir zunächst an, dass jede Spanne des Textes eine Menge von SCUs enthält, die insofern eindeutig ist, als sie keine Schnittmenge mit den SCUs anderer, disjunkter Textabschnitte hat. Insofern kann jede Textspanne auf eine oder mehrere SCUs abgebildet werden. Textspannen werden derart in Unterspannen zerlegt, dass Spannen sich nur überlagern, wenn sie identisch sind. Somit erhalten wir Spannen, die gemäß unserer Annahme semantisch eindeutig sind (siehe Abb. 2). Eine Textspanne kann nach unseren Annahmen mehrere SCUs enthalten die wir als eine behandeln, dies entspricht einer Ereignis Modellierung in gröberer Granularität.  Die Auswertung der Zusammenfassungen mit der Pyramiden-Methode ist in Tab. 4 zu sehen. Nahezu alle semiprofessionellen Zusammenfassungen liegen dabei über dem Wert 0,70 (siehe Abb. 3), während die anderen Zusammenfassungen im Vergleich zum Mittelwert schlechter abschneiden (siehe Tab. 4).  Insgesamt wird so deutlich, dass eine Betrachtung auf Pyramiden-Ebene Unterschiede offenbart, die zwar denen der N-Gramm-Methoden ähnlich, jedoch nicht grundsätzlich anhand oberflächlichen maschinellen Textauswertungen (z.B. BERTScore) festzumachen sind. Wir wollen nun evaluieren, wie Erzählwürdigkeit, repräsentiert durch die Handlungszusammenfassungen, und Narrativität, repräsentiert durch unsere Narrativitätsgraphen, zusammenhängen. Wir überprüfen dafür, ob der Teil des Originaltextes, auf den sich die Zusammenfassungen beziehen, einen großen Narrativitätswert aufweist. Als erste Analyse berechnen wir dazu den Narrativitätswert der in der Zusammenfassung referenzierten Passagen. Wir setzen diesen ins Verhältnis zum erwarteten Gesamtscore, gegeben der Länge der in der Zusammenfassung enthaltenen Textstellen (in Ereignissen). Auch ein Vergleich von Ereignissen, die in Zusammenfassungen genannt werden, mit jenen die es nicht werden, bestätigt dies anhand der Narrativitätswerte: im Mittel 3,13 für genannte und 2,86 für nicht genannte Ereignisse. Für den Vergleich der Ausschläge der Narrativtiätskurven verwenden wir den Gipfelprominenzfaktor. Dabei handelt es sich um ein Maß, welches die Wichtigkeit eines Ausschlags und damit seinen Wert im Vergleich zum umliegenden Kurvenverlauf quantifiziert. Für diesen Vergleich werden alle lokalen Maxima in der cosinusgeglätteten Narrativitätskurve (window size=50) berücksichtigt und für jedes lokale Maximum wird die Gipfelprominenz berechnet.  Die vorgestellten Ergebnisse deuten darauf hin, dass die Nutzung von Zusammenfassungen für die weitere Arbeit mit Ereignissen und Ereignishaftigkeit produktiv ist. Hervorzuheben ist, dass wir einen Relevanzzusammenhang zwischen Ereignissen und Handlung nachweisen konnten, der auf einer Operationalisierung ersterer anhand der sprachlichen Oberfläche aufbaut. Damit kann unser Ereigniskonzept mit reduziertem Handlungsbezug anhand von handlungsbezogenen Informationen aus Zusammenfassungen weiterentwickelt werden. Die bereits umgesetzte, vergleichsweise erfolgreiche Automatisierung der Ereigniserkennung und damit der Narrativitätsverläufe wird nun in Bezug auf die handlungsbezogene Relevanz von Ereignissen erweiterbar, ohne dass Handlungsinformationen mühevoll manuell für die einzelnen Ereignisse bestimmt werden müssen. Dafür erscheint es vielversprechend, den Handlungsbezug von Zusammenfassungen weiter zu evaluieren und dabei ein Verfahren zu entwickeln, das besonders relevante Stellen identifizieren kann. Dieser Beitrag entstand im von der DFG im Schwerpunktprogramm Computational Literary Studies (SPP 2207) geförderten Projekt ‚ÄûEvelautaing Events in Narrative Theory"" (EvENT)."
2023,DHd2023,HORSTMANN_Jan_Textliche_Relationen_maschinenlesbar_formalisi.xml,Textliche Relationen maschinenlesbar formalisieren: Systeme der Intertextualität,"Jan Horstmann (Westfälische Wilhelms-Universität Münster, Deutschland); Christian Lück (Westfälische Wilhelms-Universität Münster, Deutschland); Immanuel Normann (Westfälische Wilhelms-Universität Münster, Deutschland)","Intertextualität, Kategoriensystem, Formale Methoden, Beschreibungslogik, Semantic Web","Beziehungsanalyse, Modellierung, Annotieren, Theoretisierung, Literatur, Standards","Wie können intertextuelle Beziehungen formalisiert und annotiert werden? Was wäre ein kohärentes Kategoriensystem der Intertextualität und welche Formalisierung ist geeignet, um es computergestützt berechenbar zu machen, ohne seine Aussagekraft zu verlieren. Intertextualität ist eine komplexe und zugleich sehr zentrale Kategorie in der Literaturanalyse. Per Definition betrifft sie nicht nur In der Forschung finden sich daher zahlreiche Ansätze, das von Julia Kristeva (1967) benannte theoretische Konzept Intertextualität als einen Beschreibungsbegriff für die Beziehung zwischen Texten zu systematisieren. Zu nennen sind in diesem Zusammenhang insbesondere Barthes (1984), Genette (1982), Pfister (1985), oder in digitaler Hinsicht Scheirer et al. (2016), Schlupkothen und Nantke (2019) und Burghardt und Liebl (2020). Die einzelnen Ansätze und Systematisierungen haben in der Regel verschiedene theoretische oder praxeologische Hintergründe (z.B. Strukturalismus, Poststrukturalismus oder Digital Humanities) und damit verbunden verschiedene Fokusse. Für Kristeva (1967) etwa bildet das Konzept Intertextualität einen theoretischen Zugang zur Dialogizität literarischer Texte. Mit Bezug auf Michail Bachtin entwickelt sie ein Verständnis von Text als ""Mosaik von Zitaten"" (Kristeva 1967, 348). Genette (1982) beschreibt 'ebenfalls mit dem Ziel einer Gattungstypologie 'textuelle Bezugnahmen als Transformation oder Nachahmung hypertextueller Textgattungen. Er differenziert fünf Formen: 1. Intertextualität durch Zitate, Plagiate oder Anspielungen, 2. Paratextualität, womit er Rahmungen wie Titel, Genreklassifikationen, Autorname etc. meint, 3. Metatextualität (d.h. kritische Kommentare), 4. Architextualität (d.h. externe, z.B. durch Kritiker zugewiesene Rahmungen) sowie 5. Hypertextualität, bei der ein späterer Text (Hypertext) ohne einen vorherigen Bezugstext (Hypotext) nicht denkbar ist. Hypertextualität unterteilt er schließlich in Transformation (James Joyce transformiert in In Operationalisierungsansätzen der Digital Humanities wird Intertextualität als Ziel des Beitrags ist 'statt von bestimmten digitalen Verfahren auszugehen 'die theoriegeleitete Modellierung eines maschinenlesbaren Schemas, eines Kategoriensystems, das strukturell und grundlegend Analysen von Intertextualität, wie sie in literaturwissenschaftlichen und -theoretischen Abhandlungen zu finden sind, repräsentieren kann. Schlupkothen und Nantke (2019) verfolgen ein ähnliches Ziel. Bei ihnen ist aber nicht weiter ausgearbeitet, wie sich das Vorhaben, analytisch-interpretatorische Lektürepraktiken zu repräsentieren, zur eingesetzten Technologie X-Link verhält und welche Beziehung diese zu der von den Autoren ins Spiel gebrachten Situationslogik hat. Ob ein logisches System (Prädikatenlogik, Beschreibungslogik, Situationslogik etc.) für die Formalisierung eines Forschungsgegenstandes geeignet ist, hängt von den Zielen ab, die mit der formalen Repräsentation des Gegenstandes erreicht werden sollen. Ist der Gegenstand formal repräsentiert, lassen sich durch ein formalen Kalkül Entscheidungfragen hinsichtlich ihres Wahrheitswertes auswerten und (neue) Aussagen aus dem Formalisierten ableiten, worunter auch das Abfragen der 'Fakten'-Basis zählt. Ganz allgemein sind bei der Wahl eines logischen Systems folgende Aspekte ausschlaggebend: Es sollte so ausdrucksmächtig sein, dass es für die formale Repräsentation des Gegenstandes geeignet ist (Ausdrucksmächtigkeit). Und für die DH ist es wünschenswert, dass der formale Kalkül von einem Computer ausgeführt werden kann (Implementierung), und zwar zudem effizient (Komplexität). Ziel unserer Formalisierung der Domäne Intertextualität ist die Repräsentation von Intertextualitätsanalysen. Eine Einschränkung auf eine bestimmte Intertextualitätstheorie oder auf eine bestimmtes Teilphänomen, etwa werkästhetisch manifeste Intertextualität, soll zunächst nicht erfolgen. Stattdessen versuchen wir, einen gemeinsamen Kern von Intertextualitätskonzepten freizulegen, und zwar so, dass er für spezielle Theorien erweiterbar ist. Ziel ist also, intertextuelle Relationen zu annotieren, abzufragen und ggf. Aussagen abzuleiten. Die Formalisierung wird zunächst mit halbformalen Mitteln durchgeführt: mit einer Liste dessen (der Aspekte oder Merkmale), was repräsentiert werden soll. Einen ersten Zugang zum gemeinsamen Kern der verschiedenen kursierenden Intertextualitätstheorien bietet eine Analyse des Wortes Intertextualität. Es besteht aus den lexikalischen Morphemen Der auf diese halb-formale Art entworfene Kern von Intertextualität lässt sich nicht mit allen formalen Methoden repräsentieren. In der Ontologie ist ausgedrückt, dass intertextuelle Relationenen Mediatoren sein können: Der Kern ist erweiterbar, indem die Klassen Vor allem aber gewinnt Genette aus der Analyse der Hypertextualität eine Gattungstypologie. Dazu unterteilt er diese Beziehung nach zwei Gesichtspunkten: Es kann sich entweder im Hinblick auf den Relationstyp um eine Imitation oder um eine Transformation handeln, und sie kann im Hinblick auf die Art und Weise entweder spielerisch, satirisch oder ernst sein. Daraus gewinnt er durch Kombination sechs Gattungen. In RDFS/OWL formalisiert heißt das: Die Formalisierung wäre dadurch zu ergänzen, dass eine solche hypertextuelle Relation auch über eine Vermittlungsinstanz verfügen muss. Die hier vorgeschlagene Formalisierung auf Grundlage der Beschreibungslogik kommt an bestimmten Stellen an ihre Grenzen. Eine davon steckt im Begriff Situation: Eine der fundamentalen Unterscheidungen intertextueller Relationen ist die zwischen solchen, die werkästhetisch manifest sind (etwa durch paratextuelle Signale oder andere Marker), und solchen, die rezeptionsästhetisch gefunden worden und damit unklarer sind (vgl. Pfister 1985, 23f.). Mit einem rezeptionstheoretischen Hintergrund schlagen Schlupkothen und Nantke (2019) die Situtationslogik nach Barry und Parwise als angemessene formale Methode vor. Die hier vorgeschlagene auf der Beschreibungslogik bzw. OWL basierende Formalisierung bietet die Möglichkeit, die Situation durch Metadaten (Name, Datum) zu kodieren und einem Datensatz anzuhängen. Allerdings ist das keine Implementierung der Situationslogik, bei dem es insbesondere um eine Fromalisierung des Zusammenhangs von Situation und Konsistenz geht. Denselben Einwand wird man auch gegenüber dem Beitrag von Schlupkothen und Nantke einwenden können, denn die von ihnen eingesetzt Technologie X-Link ist ebenfalls keine Implementierung der Situtationslogik. Unser Beitrag hat das Ziel, in der Pluralität unterschiedlicher Konzeptionen von Intertextualität einen Kern von Intertextualität herauszuschälen und so zu formalisieren, dass er durch Theorien erweiterbar ist. Er bahnt damit einen Weg zur Repräsentation intertextueller Beziehungen, welche einerseits der Komplexität der literaturtheoretischen Konzepte gerecht wird und die andererseits Berechenbarkeit gewährleistet. Der Beitrag richtet sich damit einerseits an Intertextualitätstheoretiker*innen und -praktiker*innen. Erstere können durch unsere Formalisierung ihren Intertextualitätsbegriff schärfen: durch eine weitere Verfeinerung des von uns vorgeschlagenen Modells oder durch eine klare Abgrenzung des eigenen Intertextualitätsbegriffs. Intertextualitätspraktiker*innen wird ein (erweiterbares) Modell an die Hand gegeben, um Intertextualität zu identifizieren/zu annotieren und zu analysieren (z.B. mittels Netzwerkvisualisierung, Netzwerkanalyse oder durch eine synoptische Gegenüberstellung von Textpassagen mit intertextuellem Bezug). Andererseits kann unsere theoretische Konzeption die Grundlage für die Architektur einer möglichen Forschungsumgebung bilden, die den Intertextualitätsforschenden sowohl eine Weiterentwicklung oder Anpassung des vorgeschlagenen Intertextualitätsmodells als auch die Erforschung der Intertextualität eines annotierten Textkorpus auf Basis dieses Modells erlaubt."
2024,DHd2024,20240108_SCHAUFFLER_Nadja_Annotieren__Visualisieren__Explorieren___ei.xml,"Annotieren, Visualisieren, Explorieren 'ein integrativer Ansatz zur Erschließung von Lyrik in Text und Rezitation","Nora Ketschik (Uni Stuttgart, Deutschland); Nadja Schauffler (Uni Stuttgart, Deutschland); André Blessing (Uni Stuttgart, Deutschland); Markus Gärtner (Uni Stuttgart, Deutschland); Kerstin Jung (Uni Stuttgart, Deutschland); Florin Rheinwald (Uni Stuttgart, Deutschland); Toni Bernhart (Uni Stuttgart, Deutschland); Anna Kinder (Deutsches Literatur Archiv Marbach, Deutschland); Julia Koch (Uni Stuttgart, Deutschland); Sandra Richter (Uni Stuttgart, Deutschland, Deutsches Literatur Archiv Marbach, Deutschland); Rebecca Sturm (Deutsches Literatur Archiv Marbach, Deutschland); Gabriel Viehhauser (Uni Stuttgart, Deutschland); Thang Vu (Uni Stuttgart, Deutschland); Jonas Kuhn (Uni Stuttgart, Deutschland)","Text und Ton, Analysetools, Lyrik, Exploration, Visualisierung, Metadaten","Annotieren, Visualisierung, Metadaten, Ton, Text, Werkzeuge"," Darüber hinaus besteht die Möglichkeit, die Primärdaten 'd.h. die Transkripte der Gedichte sowie die Rezitationen 'einzusehen und (sogar wortweise) abzuspielen, sodass makroanalytische Queries mit mikroanalytischen Untersuchungen kombiniert werden  "
2024,DHd2024,SLUYTER_G_THJE_Henny_Zur_Perspektive_in_Erz_hltexten__Ein_An_final.xml,Zur Perspektive in Erzähltexten. Ein Ansatz der Computational Literary Studies.,"Henny Sluyter-Gäthje (Universität Potsdam, Deutschland)","Erzählperspektive, Computational Literary Studies, Computational Narratology, Annotation","Erzählperspektive, Computational Literary Studies, Computational Narratology, Annotation","  Daraus ergeben sich zwei Fokusse: Das Vorhaben ist in drei Arbeitspakete unterteilt: Die Erarbeitung der narratologischen Modellierung, die Operationalisierung und Annotation sowie die Implementierung eines Systems zur automatischen Identifizierung. Am Anfang steht die Erarbeitung narratologischer Modellierungen von Perspektive unter dem Gesichtspunkt der Operationalisierbarkeit. Dabei werden etablierte Definitionen auf den Grad der Abstraktheit bzw. in Bezug auf ihre Textnähe geprüft. In dem Projekt wird mit der Modellierung von Schmid (2014) gearbeitet, der fünf Parameter (räumlich, zeitlich, sprachlich, perzeptiv, ideologisch), die die Perspektive bedingen, unterscheidet. Der sprachliche und ideologische Parameter sind weiter unterteilt, für den zeitlichen und räumlichen Parameter werden textuelle Indikatoren zur Erkennung genannt. Aufgrund dieser Ausdifferenziertheit und der Textnähe bildet die Modellierung nach Schmid eine gute Grundlage für die Operationalisierung. Gius (2015) erarbeitete darauf aufbauend bereits ein erstes, rudimentäres Tagset zur Bestimmung von Perspektive.  "
2024,DHd2024,FISCHER_Frank_Das__ureigenste_theatralische_Element____Autom.xml,"Das ""ureigenste theatralische Element"" 'Automatische Extraktion von Requisiten aus deutschsprachigen Dramentexten","Jonah Lubin (Freie Universität Berlin, Deutschland); Anke Detken (Georg-August-Universität Göttingen, Deutschland); Frank Fischer (Freie Universität Berlin, Deutschland)","Drama, Theater, Requisiten, Regieanweisungen, DraCor","Sammlung, Strukturanalyse, Literatur"," Der vorliegende Beitrag fokussiert auf einen wichtigen Teilaspekt, nämlich die Requisiten, die in Nebenbemerkungen Erwähnung finden (vgl. die grundlegende Studie von Sofer 2003). Das Requisit ist ""das ureigenste dramatische Element"", das ""den Übergang von der sprachlichen zur theatralischen Ebene"" markiert. Es ist ""zur Kompetenz des Dichters"" zu rechnen und darf ""bei der Untersuchung eines dramatischen Werkes nicht übergangen werden"" (Schwarz 1974, S. 12). Die bisherige Forschung konzentrierte sich meist auf einzelne herausgehobene Requisiten, etwa das Attributsrequisit, das zur Verdeutlichung einer dramatischen Figur dient, und das Emblemrequisit, das unabhängig von den Ziel dieses Beitrags ist es, den Blick über diese speziellen Requisiten hinaus tendenziell auf die Gesamtheit der Requisiten zu richten. Dabei greifen wir auf folgende Definition zurück, ohne diese jedoch beim derzeitigen Stand vollständig operationalisieren zu können, besonders was den Interaktionsaspekt betrifft: ""Im Unterschied zu den Dingen, die der dekorationsmäßigen Ausgestaltung des Bühnenraums dienen, sind Requisiten im engeren Sinne ""Gegenstände, mit denen der Schauspieler bei der Aufführung von Bühnenstücken agiert""."" (Schwarz 1974, S. 18) Anhand des German Drama Corpus (GerDraCor; Fischer et al. 2019), das als ""living corpus"" beständig wächst und derzeit über 600 deutschsprachige Dramen vom 16. bis zum 20. Jahrhundert im Volltext enthält, soll die Verteilung dramatischer Requisiten quantifiziert werden, sowohl chronologisch als auch genrebezogen. Durch die im Vergleich zu bisherigen Arbeiten größere Anzahl an Texten geraten auch nicht-kanonische Texte mit ins Bild und ermöglichen einen repräsentativeren Blick auf die Dramenproduktion des betrachteten Zeitraums. Bezogen auf den Dramentext unterscheidet Roman Ingarden in seiner formalen Betrachtungsweise die Requisiten von nur im Haupttext, also in der Figurenrede erwähnten Gegenständen. Allein die Nennung im Haupttext macht ein Ding also noch nicht zwingend zu einem Requisit. Dies ist erst der Fall, wenn der Gegenstand zusätzlich in den Regiebemerkungen (oder auch nur dort) erwähnt wird (vgl. Ingarden 1965, S. 405). Wir folgen Ingarden hierin und betrachten nur Requisiten, die in Regiebemerkungen genannt werden. Im Folgenden wird zunächst der Workflow für die Extraktion der Requisiten vorgestellt, gefolgt von einigen exemplarischen Analysen.  Aufgrund der verwendeten historischen Orthografie in einer Vielzahl der in GerDraCor enthaltenen digitalisierten Editionen wurde ein Normalisierungsschritt nötig, um die Lemmata zu vereinheitlichen (""Schwerd"" bzw. ""Schwerdt"" wird zu ""Schwert""). Dies geschah mithilfe des DTA::CAB Web Service (vgl. Jurish 2012). Die resultierende Liste modifizierter Wörter haben wir manuell moderiert, um die Ergebnisse zu optimieren. Um im nächsten Schritt die Requisiten zu extrahieren, wurden die Regiebemerkungen zunächst mit ""spaCy"" POS-getaggt, um die Suche auf Substantive bzw. Mithilfe von GermaLemma ( Als Ansatz für die Disambiguierung haben wir dann einen Simplified Lesk Algorithmus mit Glossen von Wiktionary und lexikalischen Feldern von GermaNet verwendet (wie beschrieben in Henrich/Hinrichs 2012). Konnte diese Methode keine definitive Word Sense Disambiguation liefern, haben wir den ranghöchsten Sinn von Wiktionary übernommen. Obwohl wir dank GerDraCor zwar sehr viel Text, also auch Regieanweisungen haben, sind die Ergebnislisten doch überschaubar und taugen für einen explorativen Abbildung 2 zeigt die Zahlen für das gesamte Korpus in chronologischer Darstellung. In dieser Darstellung finden wir bestätigt, dass sich die relative Häufigkeit von Requisiten in unseren Daten ab Ende des 19. Jahrhunderts ändert, parallel zur viel beschriebenen Krise des Dramas, die dann unter anderem Epifizierungstendenzen zeitigte (vgl. Weber 2017, S. 216). Allerdings sind die Outlier nach oben Stücke ohne Szeneneinteilung, die Segmente entsprechen hier umfangreichen Akten: Hermann Bahrs ""Das Konzert"" (1909) und ""Das Phantom"" (1913) sowie wiederum ""Ignorabimus"" von Arno Holz.  Die berechneten Prozentzahlen geben an, in wie vielen Komödien und Tragödien bzw. Prosa- und Versdramen ein bestimmtes Requisit bzw. ein bestimmter Typ von Requisit mindestens einmal vorkommt. Die Unterteilung in Vers- bzw. Prosadramen haben wir auf einfache Weise getroffen: Enthält ein Werk mehr Verszeilen (in TEI kodiert mit dem Element ""l"") als Prosaabsätze (TEI-Element ""p""), dann gilt es als Versdrama, ist das Verhältnis umgekehrt, dann klassifizieren wir es als Prosadrama. Kaffee lässt sich also als Element der Prosakomödie bezeichnen, der Dolch ist ein Requisit der Verstragödie. Diese zwar nicht überraschenden, aber nun bezifferbaren Tendenzen mögen als Fingerzeig dafür dienen, wie sich die Quantifizierung von Requisiten sinnvoll einsetzen lässt. Neben der Fokussierung auf einzelne Requisiten in größeren Korpora gerät auch die Breite des gesamten Arsenals in den Blick, was am Beispiel von erwähnten Waffen in Regieanweisungen erfolgen soll. So lassen sich mindestens 38 individuelle Waffentypen (bzw. Munition) ausmachen, inklusive spezifizierenden Komposita: Armbrust, Bajonett, Bogen, Bombe, Büchse, Degen, Dienstgewehr, Dolch, Doppelflinte, Dreizack, Fangmesser, Flinte, Florett, Geißel, Gewehr, Hellebarde, Hetzpeitsche, Kanone, Keule, Klinge, Knüppel, Knüttel, Lanze, Messer, Muskete, Peitsche, Pfeil, Pistole, Rasiermesser, Revolver, Schild, Schläger, Schnitzmesser, Schwert, Seitengewehr, Speer, Spieß, und Säbel. Waffen im Drama sind immer auch chronologisch kodiert und verorten ein Stück in der Zeit der Handlung. Unter Rückgriff auf die Terminologie des Militärhistorikers Trevor N. Dupuy stammen die meisten Waffentypen im Korpus aus dem ""Age of Muscle""; dem ""Age of Gunpowder"" sind nur ein knappes Dutzend zuzuordnen (Dupuy 1980, S. 288f.).  Der hier präsentierte quantitative Ansatz soll einen Eindruck von der Häufigkeit und Distribution von Requisiten innerhalb deutschsprachiger Dramentexte vermitteln und es ermöglichen, auch die Rolle bisher wenig beachteter Typen von Requisiten zu erforschen. Durch Anpassung der verwendeten computerlinguistischen Tools ließe er sich auch auf Dramenkorpora in anderen Sprachen übertragen. Insgesamt funktioniert die Extraktion von Requisiten recht zuverlässig. Die beobachteten Es wäre wünschenswert, wenn die Befunde dieser kleinen Studie mittelfristig dazu führen würden, dass ein größeres deutschsprachiges Dramenkorpus hinsichtlich vorhandener Requisiten mit entsprechendem Markup versehen wäre, das als Evaluierungsbasis wie auch als Trainingsdatenset dienen könnte."
2024,DHd2024,HILGER_Agnes_Figurenbeschreibungen_in_deutschsprachigen_Roma_final.xml,Figurenbeschreibungen in deutschsprachigen Romanen (1789–1914),"Agnes Hilger (Universität Würzburg, Deutschland)","Figurenbeschreibung, Computational Literary Studies, Mixed Methods, Distant Reading, Close Reading, Annotation, Literaturgeschichte","Figurenbeschreibung, Computational Literary Studies, Mixed Methods, Distant Reading, Close Reading, Annotation, Literaturgeschichte","Das Dissertationsprojekt verortet sich im Bereich Computational Literary Studies (CLS) und erforscht die Entwicklung der Figurenbeschreibung in deutschsprachigen Romanen im ""langen 19. Jahrhundert"". Den Ausgangspunkt bildet die für englischsprachige Romane des 19. Jahrhunderts gezeigte Zunahme konkreter Wörter (Heuser und Le-Khac, 2012; Underwood 2019; Piper 2022; Reeve 2023). In Vorarbeiten konnte gezeigt werden, dass in deutschsprachigen Romanen ein ähnlicher Trend existiert: Die relativen Häufigkeiten von Wörtern, die physisch Wahrnehmbares bezeichnen, etwa Kleidung, Möbel oder Köperteile, steigen im untersuchten Korpus über das 19. Jahrhundert hinweg an (Hilger, 2023). Diese Entwicklung lässt sich mit herkömmlichen literaturwissenschaftlichen Darstellungen nicht erklären. Das Dissertationsprojekt setzt hier an und untersucht in einer Kombination aus Distant und Close Reading wie sich der Teilbereich der Beschreibung von Figuren verändert. Dafür werden textuelle Phänomene annotiert: Figurenbeschreibungen, deskriptive Elemente, direkte Charakte-risierung und charakterisierende Elemente. Figurenbeschreibungen und deskriptive Elemente werden als Texteinheiten verstanden, in denen der physisch wahrnehmbaren Außenseite der Figur vergleichsweise stabile Eigenschaften zugeschrieben werden. Die Annotation erfolgt zunächst manuell in CATMA (Gius, 2022), wird jedoch später automati-siert und auf alle Texte im Korpus ausgeweitet. Das Korpus besteht im Moment aus 925 zwischen 1789 und 1914 erschienenen Romanen. Es basiert größtenteils auf den bei TextGrid und im Projekt Gutenberg offen verfügbaren Texten, soll aber noch erweitert und ausbalanciert werden. Neben anderen Metadaten wird die jeweilige ""Kanonizität"" eines Texts erfasst, einerseits um die Zusammensetzung des Korpus transparent zu machen, andererseits, um später bei den Ergebnissen differenzieren zu können. Grundlegend ist dabei Winkos Beschreibung von Kanonisierung als Phänomen der unsichtbaren Hand (2002): Zahlreiche Handlungen auf einer Mikroebene führen gemeinsam auf einer Makroebene zur Kanonisierung eines Autors/einer Autorin, ohne dass dies im Einzelnen beabsichtigt sein muss. Die Rekonstruktion folgt der Logik, dass diese Handlungen zugleich Indikator für die Kanonizität zu einer bestimmten Zeit und in Bezug auf eine bestimmte Gruppe sein können. ""Kanonizität"" wird dementsprechend gemessen über: Nennungen in Literaturgeschichten (Jannidis, 2013; Brottrager u.a., 2021), in der BDSL, in universitären Kurskatalogen und auf Leselisten. Im Analyse-Teil wird untersucht, wie sich die Figurenbeschreibung über das lange 19. Jahrhundert hin im Korpus entwickelt: Nimmt ihr Auftreten zu, und, wenn ja, in welcher Form? Welche Unterscheide gibt es hinsichtlich der Kanonizität? In welchem Verhältnis stehen direkte Charakterisierung und Figurenbeschreibung? Wie werden männliche, wie weibliche Figuren beschrieben? Wie entwickeln stereotype Zuschreibungen, etwa hinsichtlich Ethnizität über die Zeit hin und welche Eigenschaften werden Gruppen als ""typisch"" markiert? Im Anschluss an die Forschung zu Literatur und Physiognomik interessieren Fragen nach der Verknüpfung von äußeren und inneren Eigenschaften. Erkenntnisgewinn verspricht sich die Arbeit vor allem von der Kombination quantitativer und qualitativer Verfahren in einem Mixed Methods-Design. Ein solches wird in den CLS seit mehreren Jahren unter verschiedenen Begriffen eingefordert."
2024,DHd2024,20240108_BRUNNER_Annelen_Das_kleine_W_rterbuch_der_Redeeinleiter.xml,Das kleine Wörterbuch der Redeeinleiter,"Annelen Brunner (Leibniz-Institut für Deutsche Sprache, Deutschland); Ngoc Duyen Tanja Tu (Leibniz-Institut für Deutsche Sprache, Deutschland); Lukas Weimer (Niedersächsische Staats- und Universitätsbibliothek Göttingen, Deutschland)","Redewiedergabe, Linguistik, Redeeinleiter","Sammlung, Archivierung, Sprache, Literatur","Das Poster Redeeinleiter sind sprachliche Ausdrücke, die relativ zu einer direkten oder indirekten Rede- oder Gedankenwiedergabe in Voran-, Mittel- oder Nachstellung stehen und diese einleiten (Breslauer, 1996; Michel, 1966; Jäger, 1968). Auch wenn Verben wie (1) (2) (3) [‚Ä¶] Durch ihre Vielfältigkeit sind Redeeinleiter ein interessanter Untersuchungsgegenstand, sowohl aus linguistischer als auch literaturwissenschaftlicher Perspektive. Sie eignen sich, um dynamische Prozesse im lexikalischen Inventar der Sprache zu untersuchen (Tu, erscheint 2024) und spielen eine wichtige Rolle bei der Einbettung von Figurenrede in den narrativen Kontext (McHale 2014, Abschnitt 2).  Grundlage für die Ressource ist das Kernkorpus ""Redewiedergabe"" (RW-Korpus; Brunner et al., 2020a), welches im Rahmen eines DFG-Projekts entstand. Das Korpus umfasst ca. 49.000 Tokens und enthält Ausschnitte aus Erzählungen sowie Zeitungs- und Zeitschriftenartikeln aus dem Zeitraum 1850-1919. Das Textmaterial ist balanciert nach Dekaden sowie dem Merkmal fiktional vs. nicht-fiktional und wurde aufwendig manuell nach Formen von Rede-, Gedanken- und Schriftwiedergabe annotiert (Brunner et al., 2020b): Eine Konsensannotation wurde auf Basis zweier unabhängiger Annotationen erstellt. Zwar ist das RW-Korpus sowohl in TEI-konformem XML-Format als auch in einem spaltenbasierten Textformat vollständig frei verfügbar(https://zenodo.org/records/3739239), es bedarf jedoch technischer Kenntnisse, spezialisierte Informationen wie die über Redeeinleiter zu extrahieren. Die vorgestellte Ressource bietet hierfür einen bequemen und niedrigschwelligen Zugang. Alle 3059 Einleiter-Vorkommen von direkter und indirekter Rede- oder Gedankenwiedergabe wurden mit ihren Attributen extrahiert und zu einer Häufigkeitsliste zusammengefasst. Für jeden der 523 Redeeinleiter-Typen bietet die Ressource einen Überblick über die Vorkommensverteilung nach den Dimensionen ""Medium"" (Rede- oder Gedankenwiedergabe), ""Wiedergabetyp"" (direkt oder indirekt), ""Position"" (initial, medial oder final) und ""Textsorte"" (fiktional oder nicht-fiktional). Abbildung 1 illustriert dies für den Einleiter  Für jede mögliche Attributkombination, mit der ein Redeeinleiter vorkommt, wurde ein zufällig gewählter Korpusbeleg extrahiert. Dieser besteht aus dem Satz, in dem der Redeeinleiter vorkommt, sowie dem vorangehenden und dem nachfolgenden Satz. Eine Verlinkung verknüpft ihn mit dem entsprechenden Dokument im RW-Korpus. Die Liste der Redeeinleiter kann zudem nach Attributwerten gefiltert werden und Redeeinleiter sowie die zugehörigen Belege können als Excel- oder CSV-Tabellen exportiert werden. Mit Hilfe der Ressource bekommt man nicht nur einen grundsätzlichen Überblick darüber, welche Einleiter in welchen quantitativen Verhältnissen verwendet werden, sondern sie erlaubt auch Kontrastierungen in unterschiedlichen Dimensionen. Die Filterfunktion ermöglicht zudem, gezielt solche Einleiter anzuzeigen, die in mehreren Kontexten vorkommen können (z.B. als Rede- und als Gedankeneinleitung). Neben den interessanten Beobachtungen, die die Ressource zu dem Datenbestand des RW-Korpus selbst erlaubt, bietet es sich an, diese mit Daten aus anderen Textgrundlagen zu vergleichen (z.B. moderne Literatur). Das ""Kleine Wörterbuch der Redeeinleiter"" ist unter der Adresse"
2024,DHd2024,H_USSLER_Julian_Lautst_rke_und_Konflikt_in_Realismus_und_Nat.xml,Lautstärke und Konflikt in Realismus und Naturalismus,"Julian Häußler (Technische Universität Darmstadt, fortext lab, Deutschland); Svenja Guhr (Technische Universität Darmstadt, fortext lab, Deutschland); Evelyn Gius (Technische Universität Darmstadt, fortext lab, Deutschland)","Konflikt, Sound, 19. Jahrhundert, Prosa, Annotation, Naturalismus, Realismus","Annotieren, Kontextsetzung, Visualisierung, Literatur, Methoden, Text","Der Naturalismus gilt als ""radikale Form des Realismus"" (Fricke et al., 2000, S. 684). Die Autor:innen des Naturalismus widmeten sich den aus ihrer Sicht drängenden sozialen Fragen der Zeit. Mit den dadurch veränderten thematischen Schwerpunkten ging auch der Anspruch einher, wirklichkeitsnahe Literatur zu schreiben. Dies steht im Kontrast zu den Themen und poetologischen Prinzipien der parallel weiter existierenden Strömung des Realismus. Auf der Ebene der Textgestaltung lässt sich der Gegensatz der beiden Strömungen u. a. daran festmachen, dass Expositionen, also Anfänge von literarischen Texten, im Realismus typischerweise ereignisarm und gleichzeitig ausführlich gestaltet sind, während im Naturalismus das Prinzip des zeitdeckenden Erzählens (i. e., die Übereinstimmung von erzählter Zeit und Erzählzeit) wichtig wurde. Dieser sogenannte ""Sekundenstil"" sollte als Erzähltechnik eine gewisse Unmittelbarkeit erzeugen. Neben der thematischen Neuausrichtung bzw. Zuspitzung des Naturalismus gibt es also zwischen Realismus und Naturalismus auch damit zusammenhängende Unterschiede in der sprachlichen Darstellung. In unserem Beitrag untersuchen wir deutschsprachige Texte des Realismus und des Naturalismus auf Lautstärke und Konflikthaftigkeit. Wir nehmen an, dass die eben erläuterten poetologischen Prinzipien auch Auswirkungen auf die textliche Gestaltung der so genannten Die Erkennung von Geräuschen des fiktionalen Mit diesen Methoden gehen wir im folgenden Beitrag auf die folgenden Hypothesen ein: (1) Aufgrund der grundsätzlich ablehnenden Haltung des Naturalismus gegenüber dem bürgerlichen Realismus erwarten wir zum einen, dass naturalistische Texte sowohl lauter als auch konflikthafter sind als realistische. (2) Zum anderen vermuten wir, dass die poetologischen Unterschiede der beiden Epochen sich auch auf eventuelle Zusammenhänge der beiden untersuchten Phänomene auswirken. Lautstärke und Konflikthaftigkeit im Realismus sollten eher negativ korrelieren, da dort Konflikte vermutlich eher abgeschwächt dargestellt werden, während im Naturalismus Konflikte auch lautstark ausgetragen werden und eine positive Korrelation zwischen Lautstärke und Konflikthaftigkeit bestehen dürfte. Zur Untersuchung dieser Hypothesen verwenden wir 192 Prosatexte aus dem Realismus und 69 Prosatexte aus dem Naturalismus. Es sind fiktionale Texte unterschiedlicher Länge, die in einschlägiger Sekundärliteratur dem Realismus und Naturalismus des deutschen Sprachraums zugeordnet werden (Böttcher und Geerdts, 1983; Brenner, 2011, Killy, 2016). Die Texte wurden dem deutschsprachigen Prosakorpus Durchschnittliche Textlänge in Wörtern längster Text kürzester Text Tokensumme manuell annotiertes Testset in Wörtern 39.116 169.510 2.773 7.510.306 8.805 33.470 159.161 2.713 2.309.440 10.358 Die systematische Untersuchung von Geräuschen und ihrer Lautstärke ist ein Ansatz aus dem literaturwissenschaftlichen Anwendungsbereich der Für die Operationalisierung von Umgebungsgeräuschen und ihrer Lautstärke verwenden wir die in Guhr und Algee-Hewitt (2023b) fürs Englische entwickelte Geräuschdefinition mit ihrer Unterscheidung zwischen expliziten und interpretationsbedürftigen impliziten Geräuschbeschreibungen sowie die Methoden zur manuellen und lexikonbasierten Annotation von Geräuschen und übertragen diese auf deutschsprachige Prosa. Zum Beispiel wird im Satz ""Der Zug fährt ratternd in den Bahnhof ein"" das ratternde Geräusch des einfahrenden Zuges explizit auf der Wortebene des literarischen Textes angegeben, sodass das Geräusch einer lexikalischen Einheit 'hier dem Adverb ""ratternd"" 'zugeordnet werden kann. In der Geräuschannotation wird dieses Wort als Annotationseinheit betrachtet und lexikonbasiert um die Angabe eines Lautstärkelevels (1-5) von leisen bis sehr lauten Geräuschen erweitert. Als ersten Schritt in Richtung einer Automatisierung der Geräuschannotation verwendeten wir einen lexikonbasierten Ansatz, in dem tokenisierte und lemmatisierte Korpustexte mit einem deutschsprachigen Geräuschwortlexikon abgeglichen wurden. Die Lexikoneinträge sind Schlüssel-Wertpaare von Geräuschwörtern, die einem nach Sachgruppen sortierten Wörterbuch (Dornseiff und Wiegand, 2004) entnommen (u. a. die Sachgruppen: ""Geräusch"", ""lautlos"", ""Stimme"") und hinsichtlich der Dezibel-Heuristik mit Lautstärkelevels ausgezeichnet wurden (Geräuschlexikon = {Lemma : Geräuschlautstärkelevel}). Mithilfe eines String-Matching-Algorithmus und Lexikonabgleichs werden die Korpustexte automatisiert mit dem Tag ""sound"" sowie einem Lautstärkelevel annotiert. Anschließend wird pro Korpustext und auch pro Korpus (Realismus- und Naturalismuskorpus) ein durchschnittlicher Lautstärkewert berechnet, der die annotierten Lautstärkelevels in Relation zur Anzahl der Geräuschwörter und der absoluten Anzahl an Wörtern pro Text abbildet. Für die Evaluation wurden die Annotationen aus dem Lexikonansatz mit manuell erstellten Annotationen verglichen. Dafür wurden die bewährten Evaluationsmetriken   0.987 0.2 0.28 0.23 Die Analyse von Konflikthaftigkeit basiert auf der Anwendung und Adaption der im Projekt Diese Methodik wurde bisher genutzt, um Sentiment- und Konfliktwerte in Romantik-, Realismus-, und Naturalismus-Korpora zu erheben, mit dem Ziel konflikthafte Textstellen zu ermitteln und die Korpora im Hinblick auf ihre Konflikthaftigkeit zu betrachten. Darüber hinaus wurden Sentimentwerte erhoben, in der Erwartung, dass Sentiment ein Signal für Konflikt sein kann (vgl. Häußler und Gius, 2023a und b). Für unseren Anwendungsfall wurden für das Realismus- und Naturalismuskorpus je ein Um unsere Hypothesen zu überprüfen, dass (1) der Naturalismus durchschnittlich lauter und konflikthafter als der Realismus ist, und, dass (2) Konflikte im Naturalismus eher in laute bzw. im Realismus in leise Mit Blick auf die dadurch erhaltenen Ergebnisse zeigt sich im Vergleich der Lautstärkewerte in den beiden Subkorpora zunächst keine auffälliger Unterschied zwischen Realismus und Naturalismus (vgl. Abb. 1). Betrachtet man jedoch die durchschnittlichen Konfliktwerte der einzelnen Texte, so fallen die Texte aus dem Naturalismus als konflikthafter auf (vgl. Abb. 2). In der Betrachtung der einzelnen Texte hinsichtlich ihrer Durchschnittswerte zeigt sich bei den lautesten bzw. leisesten Texte eine Gruppierung nach Autor:innen. So sind die drei durchschnittlich lautesten Texte des gesamten Korpus Texte der Naturalistin Clara Viebig (vgl. Tab. 4) und die drei konflikthaftesten Texte vom Naturalisten Ludwig Thoma. Der Realist Theodor Fontane fällt zudem als wenig ""konflikthafter"" Autor auf (vg. Tab. 4). Um die Korrelation von Lautstärke und Konflikt zu prüfen, betrachten wir als nächstes, welchen Texten sowohl extreme Lautstärke-, als auch extreme Konfliktwerte zugewiesen wurden. Wir definieren hier als extrem die obersten bzw. untersten 10% in der Rangordnung der lautesten bzw. konflikthaftesten Texte und heben jene Texte hervor, die in überschneidenden extremen Gruppen auftreten (vgl. Tab. 5). Zwar bestätigen die Ergebnisse die Vermutung, dass die eher konflikthaften auch die eher lauten Texte sind (Thoma) und, dass die eher weniger konflikthaften auch die eher leisen Texte sind (Fontane), doch zeigt sich für den Naturalisten Thoma, dass auch eine starke negative Korrelation (hohe Konflikthaftigkeit, geringe Lautstärke) im Naturalismus vertreten ist. laute und konfliktarme Texte laute und konflikthafte Texte leise und konfliktarme Texte leise und konflikthafte Um Konflikt und Lautstärke sowie deren Korrelation auch qualitativ zu betrachten, analysieren wir im Folgenden den Text Hinsichtlich der Lautstärke- und Konfliktwerte fallen höhere Konfliktwerte und mehr Lautstärkewörter in der zweiten Hälfte des Textes auf. Zwar entspricht die Eskalation von verbaler zu physischer Gewalt dem Höhepunkt der Konfliktwerte, doch treten z.B. Beleidigungen auch mit unterschiedlicher Konflikthaftigkeit auf. Eine Eskalation des Konfliktes korreliert hier mit einer Häufung der Lautstärkewörter, doch sind die Lautstärkewörter mit dem höchsten Wert nicht gleichbedeutend mit dem Höhepunkt der Eskalation. Die Knaben sprechen vor der eigentlichen Explosion darüber, ob Modellschiffe auch Munition verschießen können (‚Äöschießt‚Äò, ‚Äöschießen‚Äò bzw. ‚Äöknallen‚Äò). Arthur fragt z.B. den Protagonisten, ob der Verschuss der Munition ""recht knallen wird"". Die Eskalation kündigt sich dann mit der Erwartung der Explosion an (‚Äöknallt‚Äò). Die Explosion selbst wird mit Worten beschrieben, die nicht den Lautstärkewert 5 besitzen bzw. nicht im Geräuschwortlexikon enthalten sind. Erst in den Momenten physischer Gewalt, den Ohrfeigen des Weiherbesitzers, knallt es wieder. Dieser unterstellt ihnen zudem, sein Haus sprengen zu wollen. Durch unsere Untersuchungen konnten wir zeigen, dass mit diesem ersten Ansatz zur Analyse von Konflikthaftigkeit und Geräuschlautstärke in fiktionalen Texten bereits einige vergleichende Rückschlüsse auf Texte des Realismus und Naturalismus getroffen werden können. Wir konnten herausstellen, dass naturalistische Texte auffällig konflikthafter sind (vgl. Tab. 4), während sie sich hinsichtlich ihrer Lautstärke kaum von Texten des Realismus unterscheiden (vgl. Tab. 3). Interessant ist, dass sich mit Blick auf die Texte mit den höchsten/niedrigsten durchschnittlichen Lautstärkewerten und den höchsten/niedrigsten Konfliktwerten Autor:innencluster herausstellen lassen. Dabei entspricht die Beobachtung der Hypothese 1), da die lautesten Texte (Viebig) und konflikthaftesten Texte (Thoma) dem Naturalismuskorpus zugehören, während die am wenigsten konflikthaften Texte realistisch sind (Fontane). Die Verschränkung von Lautstärke und Konflikthaftigkeit (vgl. Hypothese 2) scheint zwar einerseits zu bestätigen, dass die konflikthaften Texte von Thoma (Naturalismus) auch laut sind (positive Korrelation), doch trifft diese Beobachtung nicht auf alle Texte des Autoren (hier: Bei der Analyse des Beispieltextes ("
2024,DHd2024,G_GGELMANN_Michael_Automatische_Erkennung_von_Bez_gen_zwisch_final.xml,Automatische Erkennung von Bezügen zwischen Epistolographie und Literatur,"Michael Göggelmann (Universität zu Köln, Universität Tübingen, Deutschland)","Machine Learning, Briefe, Literatur, Briefkorpora, Computational Literary Studies, Referenzen, Intertextualität, Korpus, Epistolographie, Charles Dickens, Quotation Detection, Named Entity Recognition, Text Reuse","Machine Learning, Briefe, Literatur, Briefkorpora, Computational Literary Studies, Referenzen, Intertextualität, Korpus, Epistolographie, Charles Dickens, Quotation Detection, Named Entity Recognition, Text Reuse","Die Epistolographie von Schriftstellerinnen und Schriftstellern tritt vor dem eigentlichen literarischen Werk naturgemäß eher in den Hintergrund. Dabei können einige Briefsammlungen sowohl quantitativ als auch hinsichtlich ihrer ästhetischen Tragweite als ""Werk neben dem Werk"" Das Projekt verspricht in zweifacher Hinsicht Innovationspotenzial: neben einem Beitrag zur Entwicklung quantitativer Methoden der Textanalyse soll die Beantwortung solcher literaturwissenschaftlicher Forschungsfragen vereinfacht oder ermöglicht werden, die eine stärkere Verknüpfung von Briefen und fiktionalen Werken voraussetzen. Die Digitalisierung und Edition von Briefkorpora rückte schon früh in das Blickfeld computergestützter Geisteswissenschaft (Cheney, 1983). Auch aktuelle Projekte zur digitalen Briefedition knüpfen häufig an ältere Editionsprojekte an, die nach Jahrzehnten analogen Arbeitens um ein Digitalisierungsvorhaben ergänzt wurden. Computergestützte Projekte zu Briefsammlungen verbindet somit, dass sie als dezidierte Editionsprojekte zumeist spezifisch-epistolare Fragestellungen zur Aufbereitung eines Datensatzes behandeln und sich computergestützter Analysen vorrangig zur Visualisierung vorhandener, oder der Generierung neuer Metadaten bedienen. Methodische Schnittpunkte hingegen ergeben sich insbesondere mit Projekten, die sich computergestützt mit (Teil-)Aspekten der Intertextualität befassen. Hierzu gab es auch in den vergangenen Jahren immer wieder Beiträge bei der DHd (u.a. Liebl und Burkhardt, 2020). Das Arbeitsvorhaben stützt sich zunächst auf das digitale Briefkorpus von Charles Dickens, das etwa 14.000 Briefen umfasst und auf der zwölfbändigen Pilgrim-Edition seiner Briefsammlung basiert (House et al., 2001). Das im Projekt bislang noch nicht weiter untersuchte Teilkorpus der literarischen Werke von Dickens wurde aus dem Gutenberg-Projekt zusammengestellt. Als erster Orientierungspunkt für die automatische Erkennung von Bezügen dient die methodische Zweiteilung in eine vorangestellte Hinsichtlich der Daraus ergeben sich für die Während nicht auszuschließen ist, dass sich der Schritt der"
2024,DHd2024,FISCHER_Frank_Literatur_im_Wikiversum___Eine_praktische_Ann_.xml,Literatur im Wikiversum 'Eine praktische Annäherung über API-Abfragen und Wikipedia-Metriken,"Viktor J. Illmer (Freie Universität Berlin, Deutschland); Bart Soethaert (Freie Universität Berlin, Deutschland); Lilly Welz (Freie Universität Berlin, Deutschland); Frank Fischer (Freie Universität Berlin, Deutschland); Robert Jäschke (Humboldt-Universität zu Berlin, Deutschland)","Wikipedia, Literatur, API, Python","Crowdsourcing, Einführung, Lehre, Literatur, Werkzeuge","Die kollaborativ erstellte Online-Enzyklopädie Wikipedia bietet mit derzeit über 60 Millionen Artikeln in über 300 Sprachversionen Neben der individuellen Lektüre der Fließtexte (und ihrer jeweiligen Versionshistorie) über die Website bietet die Online-Plattform über eine API weitere Möglichkeiten zur Analyse der enzyklopädischen Inhalte und des Community-Engagements. Die Vielzahl an Metadaten, sowohl zu den einzelnen Themen selbst als auch zur Bearbeitung und Nutzung durch die aktiv partizipierende Community bzw. die Leser*innen sowie die semantischen Verknüpfungen lassen sich auch mit digitalen Methoden sammeln, quantifizieren und auswerten. Auch die rezeptionsorientierte Literaturwissenschaft hat das Projekt inzwischen als Forschungsgegenstand und Datenressource entdeckt (vgl. Hube et al., 2017; Chiu, 2022; Fischer et al. 2023b), da es viele enzyklopädische Beiträge und Metadaten zur Literatur und zum literarischen Leben versammelt, zu Autor*innen, literarischen Werken, Genres, Epochen und weiteren literaturgeschichtlich relevanten Kategorien. Jüngste Untersuchungen in diesem Bereich werten die inhaltliche Reichweite von Wikipedia etwa im Hinblick auf die Aufnahme und Darstellung von einzelnen Autor*innen (Blakesley, 2018; Bronner, 2018; Fischer et al., 2019; Blakesley, 2022b), Gruppen (Blakesley, 2020; Carrillo-Jara, 2023), literarischen Werken (Blakesley, 2022a), literarischen Figuren (Picard et al., 2023; Wojcik et al., 2023), Gattungen (Figlerowicz, 2023) und Kanones (Miller et al., 2016; van der Deijl et al., 2018; Wojcik et. al, 2019; Lippolis, 2023) aus. Unterschiede in der Verteilung enzyklopädischer Artikel zu bestimmten Themen in verschiedenen Sprachen geben Aufschluss über das unterschiedliche Interesse und die attestierte Relevanz dieser Themen für bestimmte Sprachgemeinschaften. Darüber hinaus können Veränderungen der Seitenaufrufe, der Überarbeitungen und der Beitragenden auch im Zeitverlauf analysiert werden, um das sich entwickelnde Interesse an und die Auseinandersetzung mit bestimmten literarischen Autor*innen und Werken zu verfolgen. Die datenanalytische Auswertung anhand solcher Wikipedia-Metriken ermöglicht es somit, die Auseinandersetzung mit Literatur in Wikipedia evaluierbar zu machen und Aussagen über literarische Kanonizität, Wertungspraktiken und Popularität im Kontext offener Enzyklopädieprojekte weiter zu diversifizieren. In kritischer Auseinandersetzung mit der Kanon- und Popularitätsforschung in globaler Perspektive wird unter anderem besonders deutlich, dass sich in der Wikipedia kein monolithischer Kanon zeigt, sondern viele, sich zudem dynamisch verändernde Kanones manifestieren. Im Zentrum des Hands-On-Workshops steht die Wikipedia-API, mit deren Funktionsweise die Teilnehmer*innen vertraut gemacht werden. Sukzessive werden Abfrageskripte in Form eines Jupyter Notebooks erarbeitet. Um eine benutzerfreundliche Programmierumgebung anzubieten und langwierige Installationsprozesse zu umgehen, wird für das Ausführen des Notebooks auf Google Colaboratory zurückgegriffen. Im Folgenden werden drei Typen von Abfragen kurz vorgestellt, die im Workshop jeweils im Hinblick auf eigene, von den Teilnehmer*innen mitgebrachte Fragestellungen und Forschungsinteressen modifiziert werden können. Der Programmiercode wurde um Annotationen ergänzt, die es auch Python-Anfänger*innen ermöglichen, über die bereitgestellten Formulare eigene Anfragen auszuführen. Eine mögliche Abfragestrategie ist die autor*innenzentrierte Abfrage, wie am Beispiel von Theodor Fontane demonstriert werden soll. Über die Wikipedia-API lässt sich herausfinden, wie viele der über 300 Sprachversionen der Wikipedia einen eigenen Artikel über den Autor bereithalten 'die Anzahl dieser Sitelinks gilt in der Forschung als ""a simple measure of canonicity"" (Kukkonen 2020). Diese können dann etwa diagrammatisch auf ihre Artikelgröße hin verglichen werden (Abb. 1). Auf diese Weise können ebenfalls die Anzahl der Überarbeitungen des Artikels, die Anzahl der Bearbeiter*innen, die Backlinks oder das Datum der Artikelerstellung untersucht werden. Diese Datenpunkte können sprachübergreifend Aufschluss über etwaige Konjunkturen der Fontane-Rezeption geben. Es lassen sich Rückschlüsse auf Anlässe ziehen, die eine Erweiterung der Informationsbasis in der Wikipedia ausgelöst haben könnten (Preise, Jubiläen, Übersetzungen, Schulstoff). Im Folgenden kann als digitale Entsprechung der literaturhistorischen Praxis ein Vergleichspool anderer Autor*innen zusammengestellt werden, um Fontane und sein literarisches Werk mit denen zeitgenössischer Kolleg*innen zu vergleichen. Fontanes Geburtsjahr ist 1819, eine mögliche Operationalisierung von Zeitgenossenschaft wäre etwa die Zusammenstellung anderer deutschsprachiger Autor*innen, die bis zu 20 Jahre vor und nach Fontane geboren wurden. Diese Festlegung ist natürlich kontingent und kann individuellen Informationsbedürfnissen angepasst werden. Die Visualisierung der Artikellänge und ein entsprechendes Ranking ergeben dann beispielsweise, dass Wilhelm Busch unter den zeitgenössischen literarisch schreibenden Autor*innen den aktuell umfangreichsten Artikel vorweisen kann, Fontane aber immerhin in den Top-5 rangiert (Abb. 2). Auch wenn sich diese Artikellängen, die oft über mehr als 20 Jahre gewachsen sind, größenordnungsmäßig nicht so schnell ändern, sind diese Werte in einer communitybetriebenen digitalen Enzyklopädie natürlich durch die Zeit variabel. Dass Wilhelm Wundt, als Psychologe und Philosoph ebenfalls einflussreicher Autor, bei diesem Ranking mit dem umfangreichsten Artikel ganz vorn steht, zeigt auch, dass es eines weiteren Schritts bedürfte, wollte man die Ergebnisliste auf vorderhand literarisch schreibende Zeitgenoss*innen eingrenzen. Außerdem zeigt sich ein systematischer Bias in der Artikellänge: Zu umfangreiche ""Werk""-Abschnitte werden oft in eigene Artikel für Werke ausgegliedert, während die Werke ""kleinerer"" Autor*innen oft Teil der Personenartikel bleiben. Die genaue Kenntnis der Gepflogenheiten innerhalb von Wikipedia erweist sich daher als Voraussetzung für eine sinnvolle Einschätzung der Quantifizierungen.  Neben den inhaltsbezogenen Informationen, die über Wikipedia direkt bezogen werden können, lassen sich über die internen Verweise zwischen Artikeln (Wikilinks) auch Netzwerkmetriken wie der PageRank berechnen (vgl. Thalhammer, 2016; Hube et al., 2017). Öhnlich wie dies Suchmaschinenalgorithmen zur Bestimmung der Rangfolge von Ergebnissen tun, können diese Zahlen dazu verwendet werden, den Wert und die Bedeutung eines Wikipedia-Artikels und seines Themas innerhalb des Hyperlink-Netzwerks der Plattform einzuschätzen. Der Wert, der einem Wikipedia-Artikel auf der Grundlage dieser Metriken zugewiesen wird, gibt Aufschluss über die relative Bedeutung und Konnektivität eines Themas innerhalb der vernetzten Informationen von Wikipedia. Die Abbildung der Literatur in Wikipedia und in darauf aufbauenden oder damit verwandten Projekten (DBpedia, Wikidata) wird in den Literaturwissenschaften zunehmend als möglicher Forschungsgegenstand wahrgenommen. Einen repräsentativen Überblick über solche Zugänge und Fragestellungen bietet der Sonderband des Der Workshop zielt auf Literaturwissenschaftler*innen, aber auch auf Kolleg*innen angrenzender Gebiete. Vorkenntnisse der Programmiersprache Python sowie zu Programmierschnittstellen (APIs) sind hilfreich, aber keine Voraussetzung zur Teilnahme am Workshop. Den Teilnehmer*innen soll das nötige Praxiswissen vermittelt werden, um eigenständig weiterzuarbeiten. max. 20 Personen Die Teilnehmer*innen benötigen einen eigenen Laptop Gefördert durch die Deutsche Forschungsgemeinschaft (DFG) im Rahmen der Exzellenzstrategie des Bundes und der Länder innerhalb des Exzellenzclusters Temporal Communities: Doing Literature in a Global Perspective 'EXC 2020 'Projekt-ID 390608380."
2024,DHd2024,JANNIDIS_Fotis_Bedeutung_in_Zeiten_gro_er_Sprachmodelle.xml,Bedeutung in Zeiten großer Sprachmodelle,"Tessa Gengnagel (Cologne Center for eHumanities, Universität zu Köln, Deutschland); Fotis Jannidis (Julius-Maximilians-Universität Würzburg); Rabea Kleymann (Technische Universität Chemnitz, Deutschland); Julian Schröter (Ludwig-Maximilians-Universität München, Deutschland); Heike Zinsmeister (Universität Hamburg, Deutschland)","Sprachmodelle, Bedeutung, Theorie der Digital Humanities","Sprachmodelle, Bedeutung, Theorie der Digital Humanities","Die Performanz künstlicher Intelligenz ist, nicht zuletzt durch die großen Sprachmodelle (LLMs) in den letzten Jahren rasant angestiegen. Das hat zu einer intensiven Diskussion um die Definition anthropologisch relevanter Konzepte geführt; so wurde etwa die Diskussion des Begriffs Insbesondere die Entwicklung großer Sprachmodelle hat zuletzt eine Auseinandersetzung mit menschlicher und maschineller Sinnbildung provoziert (Kirschenbaum 2023), weswegen wir uns auf sprachliche Bedeutung, also die Bedeutung von Worten, Sätzen und Texten, konzentrieren werden. Das Panel wird daher vier Perspektiven zusammenführen:  Nach einer gemeinsamen einführenden Einleitung werden alle Panelist:innen ihre oben skizzierten Perspektiven in 5–7-minütigen Statements erläutern. Auf diese Impulse wird eine 10-minütige Phase folgen, in der die Panelist:innen auf die Statements der anderen Diskussionsteilnehmer:innen reagieren können. Anschließend wird die Diskussion für das Publikum geöffnet, um eine engagierte Debatte zu ermöglichen. Je nach Publikumspartizipation soll so außerdem der Raum geschaffen werden, weitere relevante Aspekte einzubringen, so etwa aus dem Bereich der Leseforschung und Kognitionspsychologie. Das Panel verspricht nicht nur menschliche und maschinelle Bedeutungsverfahren in den DH zu explorieren, sondern stellt auch einen ersten Versuch dar, ein geisteswissenschaftliches Vokabular für die Beschreibung und Evaluierung von intelligenten Systemen zu entwickeln. Insbesondere die Konjunktur des Bedeutungsbegriffes in den Datenwissenschaften (vgl. Donoho 2017, 746) macht eine systematische Auseinandersetzung mit der geisteswissenschaftlichen Begriffstradition erforderlich, um die Rolle der Geisteswissenschaften zukünftig zu vermessen. Vor dem Hintergrund der projektbasierten Arbeit in den DH stellt sich außerdem die Frage, wie sich diese in ihren Aufgaben und Zielen durch die Fortschritte in der generativen KI sowohl unmittelbar als auch langfristig verändern wird. Hierzu wird das Panel unter Einbeziehung des Konferenzthemas ""Quo vadis?"" wichtige Impulse in einer Zeit des Umbruchs liefern."
2024,DHd2024,20240108_HATZEL_Hans_Ole_Narrativit_t_visualisieren___Eine_Rezeptions.xml,Narrativität visualisieren - Eine Rezeptionsstudie zur Evaluation der heuristischen Qualität von Narrativitätsgraphen,"Hans Ole Hatzel (Universität Hamburg, Deutschland); Haimo Stiemer (Technische Unversität Darmstadt, Deutschland); Chris Biemann (Universität Hamburg, Deutschland); Evelyn Gius (Technische Unversität Darmstadt, Deutschland)","graphen, studie, narrativität, interpretierbarkeit","Programmierung, Modellierung, Annotieren, Interaktion, Literatur, Metadaten","Die visuelle Repräsentation von literarischen Phänomenen ist ein etablierter Ansatz in den Computational Literary Studies, um die aus Texten extrahierten Daten bzw. abgeleiteten Strukturen zu explorieren und zu interpretieren (cf. Baillot u. Lassner 2022; Krämer 2014).¬† Vor diesem Hintergrund soll das vorgeschlagene Poster die Ergebnisse einer Rezeptionsstudie präsentieren, mit der die heuristische Qualität von den im Projekt EvENT (""Evaluating Events in Narrative Theory"") bislang generierten Narrativitätsgraphen überprüft wurde. Ziel der Studie war es, die Erkennbarkeit der den Graphen zugrunde liegenden Texte zu untersuchen, um hieraus Rückschlüsse für die Weiterentwicklung des EvENT-Ansatzes wie auch die Anwendbarkeit der Graphen für die literaturwissenschaftlich-hermeneutische Praxis zu ziehen. Dabei stellen die Graphen das Ausmaß der Narrativität (auf der y-Achse) über den Textverlauf (auf der x-Achse) dar. Die Narrativitätsgraphen basieren auf den vier, im EvENT-Projekt auf der Grundlage des narratologischen Forschungsstands konzipierten Ereigniskategorien (Zustandsveränderungen, Prozesseereignisse, statische Ereignisse und Nicht-Ereignisse) und der ihnen zugewiesenen Narrativitätsgrade (cf. Vauth u. Gius 2021). Die über die automatisierte Annotation von Verbalphrasen auf der Textoberfläche detektierten Ereignisse¬† (cf. Hatzel 2022) werden im EvENT-Projekt verwendet, um die Narrativität von Texten über den Textverlauf als Narrativitätsgraphen abzubilden und damit auch die Handlung ihrer Geschichten zu modellieren (cf. Vauth et al. 2021; Gius u. Vauth 2022). Eine Annahme bei dieser Modellierung¬† war, dass die Graphen ebenso die ""Erzählwürdigkeit"" der Ereignisse in den Texten indizieren und sich damit dem in der Narratologie als Event II diskutierten Phänomen annähern (cf. Hühn 2009, S. 80). Event II stellt einen Ereignistyp mit zusätzlichen Merkmalen im interpretativen Kontext dar, wie z. B. Relevanz oder Unerwartetheit, geht also über die vier oben genannten, grundlegenden Ereigniskategorien hinaus.  Die Studie wurde als Webanwendung konzipiert. React und Plotly wurden im Front-End verwendet und die Antworten an ein fastAPI-basiertes Back-End übermittelt, welches diese in einer PostgreSQL-Datenbank protokollierte. Auf diese Weise wurde eine schnelle Iteration des Studiendesigns sowie die Teilnahme auf allen wichtigen Plattformen ermöglicht. Die 19 Teilnehmer:innen der Studie (aktuelle und ehemalige Studierende der Germanistik), wurden gebeten, einem literarischen Text den richtigen Narrativitätsgraphen zuzuordnen, wobei für jeden Text vier Graphen als Antwortmöglichkeiten ausgegeben wurden. Die Textdarbietung erfolgte auf drei unterschiedlichen Abstraktionsniveaus bzw. in drei Phasen. In der ersten Phase wurden die Teilnehmenden gebeten, zwei kurze deutschsprachige literarische Texte zu lesen und diese jeweils dem richtigen Graphen zuzuordnen. In der zweiten Phase erfolgte die Auswahl der Graphen auf der Grundlage von Zusammenfassungen, in denen die wesentlichen Ereignisse des jeweiligen Textes in chronologischer Reihenfolge präsentiert werden. In der dritten Phase wurden den Teilnehmenden nur die Titel der Texte angeboten, denen ein entsprechender Graph zugewiesen werden sollte. Bei diesen Texten handelt es sich um kanonische Texte (z. B. Hänsel und Gretel von den Brüdern Grimm), deren allgemeine Bekanntheit vorausgesetzt werden konnte. Zu den auf dem Poster zu präsentierenden Resultaten der Studie gehört, dass der Anteil der korrekten Zuordnungen der Graphen zu den Texten durch die Teilnehmenden bei 25,56 % und damit nicht signifikant über dem Zufallsprinzip liegt. Allerdings gibt es eine positive Korrelation bei Teilnehmenden mit EvENT-Erfahrung (durch Mitarbeit im Projekt, Kenntnis der Annotationsguidelines), welche die korrekten Texte in 47.5% der Fälle ausgewählt haben, ein Wert der mittels Binomialtest als statistisch signifikant identifiziert wurde (p < 0.01). Wir konnten zudem mit statistischer Signifikanz zeigen, dass die Zeit, die sich Teilnehmende zur Beantwortung einer Frage nahmen, mit der Quote der richtigen Antworten korreliert. Auch waren lange Texte für unsere Annotator:innen statistisch signifikant schwieriger zu identifizieren als kurze (hier ist anzumerken, dass alle vier Optionen für eine Identifikation stets so gewählt waren, dass sie ähnliche Längen aufwiesen). Offenkundig ist die Identifikation der textzugehörigen Graphen voraussetzungsreich. Im Anschluss an die Zuordnungsaufgaben beantworteten die Teilnehmenden zwei offene Fragen nach ihren Entscheidungsgrundlagen im Verlauf der Studie. Die Antworten legen nahe, dass die Narrativitätsverläufe der Graphen als Repräsentationen von Event II-Vorkommen und die Amplitudenausschläge damit als Verweise auf besonders handlungsrelevant erscheinende Textpassagen interpretiert wurden. Als entscheidungsrelevant wurden von den Teilnehmenden außerdem nicht nur die Peaks der Graphen, sondern auch deren Anzahl sowie Anfang und Ende eines Narrativitätsverlaufs ausgewiesen."
2024,DHd2024,SCHMIDT_Thomas_Fanfictions___Literatur_von_Frauen__ber_M_nne.xml,Fanfictions 'Literatur von Frauen über Männer? Korpusbasierte Analyse der Geschlechterrollen bei Texten und Autor*innen deutschsprachiger Fanfictions,"Thomas Schmidt (Lehrstuhl für Medieninformatik, Universität Regensburg, Deutschland); Jonathan Sasse (Lehrstuhl für Medieninformatik, Universität Regensburg, Deutschland); Christian Wolff (Lehrstuhl für Medieninformatik, Universität Regensburg, Deutschland)","Fanfiction, Gender, Fan Studies, Internet Studies, Social Media, NER","Sammlung, Inhaltsanalyse, Literatur, Metadaten, benannte Entitäten (named entities), Text","Fanfictions sind literarische Texte, erstellt von Fans und Hobby-Autor*innen, die Figuren und Geschichten aus bereits bestehenden Medien wie Filmen oder Büchern nutzen, um neue Geschichten über diese zu schreiben und auf Online-Plattformen zu veröffentlichen (Dym et al., 2018). Dieses spezielle literarische Genre wurde in den letzten Dekaden mit dem Aufstieg des Internets immer populärer und deswegen auch vielseitig in den Geistes- und Kulturwissenschaften in Hinblick auf Geschichte und kulturellen Einfluss untersucht (siehe Hellekson und Busse, 2006; Jamison, 2013). Die Verfügbarkeit von großen narrativen Textmengen mit detaillierten Metadaten macht Fanfictions auch zu einer beliebten Quelle für verschiedene Aufgaben im Geschlechtsspezifische Fragestellungen spielen eine wichtige Rolle im Kontext von Fanfictions. Bisherige Analysen für mehrheitlich englischsprachige Texte deuten auf eine erhöhte weibliche Autorschaft in diesem Genre hin (Barnes, 2015; Duggan, 2020). Motivation und Bedeutung für die Popularität von Slash-Fanfictions (Fanfictions mit Fokus auf homo-romantischen Beziehungen zwischen Männern) und damit die Dominanz von männlichen und Vernachlässigung weiblicher Figuren wurden vielfach anhand begrenzter Mengen von Texten diskutiert (Jung, 2002; Tosenberger, 2008; Rossdal, 2015; Busse und Lothian, 2017). Dem entgegen argumentiert andere Forschung mit ähnlichem methodischem Zugang (""close reading"", vgl. Busse, 2009; Leow, 2011; Handley, 2012; Duggan, 2017; 2020; 2022), dass die Autorschaft wesentlich diverser ist und weibliche Charaktere eine wichtige und nicht-stereotype Rolle spielen. Derartige Analysen werden computergestützt in größeren Rahmen auch von den Untersuchungen von Milli und Bamman (2016) getragen, während Fast et al. (2016) eine stereotype und negative Repräsentation von weiblichen Figuren identifizieren. Wir präsentieren im Folgenden die Ergebnisse eines Projekts, dass die bisher vorliegenden computergestützten Analysen mit Fokus auf den deutschsprachigen Bereich weiterführt. Unsere Forschungsbeiträge sind (1) die Akquise und Bereitstellung eines strukturierten Korpus speziell für die Analyse deutschsprachiger Texte und Communites, (2) allgemeine Korpus- und Metadatenanalysen und (3) erste Analysen zur Verteilung von Geschlechtern bezüglich Figuren und Autor*innen in diesem Korpus. Als Plattformen für die Korpusakquise wurden Fanfiktion.de (FF.de) Die Inhalte beider Plattformen (Texte, Metadaten, Kommentare/Reviews, Nutzer*innen-Profile) wurden mittels Tabelle 1 illustriert die allgemeinen Statistiken des Korpus in Summe und aufgeteilt nach Plattform. Mit 394.848 einzelnen Fanfictions liefert FF.de im Vergleich zu AO3 (18.075) deutlich mehr deutschsprachigen Inhalt. In beiden Plattformen muss man sich mittels eindeutigem Nutzernamen anmelden, um Texte zu posten oder damit in Kommentaren und Bewertungen zu interagieren 'hierauf beziehen sich die Nutzerstatistiken. Als Reviews bezeichnen wir im Folgenden Kommentare. Token-Statistiken wurden mittels Als Fandom wird die mediale Referenz, also das fiktionale Universum bzw. Thematik bezeichnet, in dem eine Fanfiction spielt. Tabelle 2 und 3 illustrieren die Top 10 Fandoms für FF.de und AO3 respektive. Die grundsätzlichen Fandom-Verteilungen verhalten sich konform zu Analysen auf größeren englischsprachigen Plattformen mit Fandoms wie Harry Potter, Marvel und Supernatural als besonders häufigen Fandoms. Im Fall von FF.de wird die besondere historische Bedeutung von Anime (Naruto, One Piece) für die deutsche Fanfiction-Community deutlich (siehe auch Cuntz-Leng und Meintzinger, 2015). Für AO3 kristallisieren sich spezielle nationale Besonderheiten heraus wie die Häufigkeit von Tatort-, Die drei ??? Ein weiteres wichtiges Metadatum im Kontext dieser Forschung sind Beziehungstypen, die für beide Plattformen äquivalent vorliegen. Dadurch wird markiert, ob eine romantische/erotische Beziehung zwischen Figuren eine wichtige Rolle spielt und welcher Geschlechternatur diese ist. Tabelle 4 zeigt die kumulierte Verteilung für beide Plattformen. Dabei ist zu beachten, dass eine Fanfiction im Fall von AO3 auch mehrere Angaben bezüglich Beziehungstypen haben kann. Obwohl der Großteil der Geschichten als Generisch (Generic) (66,5%) gekennzeichnet ist und damit definitionsgemäß keinen spezifischen Beziehungstyp fokussiert, spielen Slash-Fanfictions (M/M) eine bedeutende Rolle, da sie den größten Teil der verbleibenden Fanfictions ausmachen. Im Vergleich dazu sind Beziehungen, welche weibliche Figuren beinhalten, eher selten. Zur vertieften Analyse wurde eine Geschlechtsklassifikation genannter Personennamen in den Fanfictions durchgeführt. Dazu wurde zunächst eine Nach ersten Experimenten mit vortrainierten Modellen zur geschlechtsbasierten Namenserkennung Insbesondere die Liste von babynames.com hat einen besonderen Mehrwert da hier fiktionale Namen aus Kunst und Kultur enthalten sind. Für Analysen, die das Autor*innen-Geschlecht verwenden, wird das Korpus auf den FF.de-Anteil beschränkt, da nur in diesem Nutzer*innen über ihr Profil freiwillige Geschlechtsangaben machen können. Eine Geschlechtserkennung auf Nutzernamen ist aufgrund ihrer Beliebigkeit and Abstraktheit in diesem Kontext nicht sinnvoll. Tabelle 5 zeigt das Verhältnis von männlichen zu weiblichen Namen bezüglich der fiktionalen Charaktere in den Fanfictions auf. Diejenigen Namen, die keine Erkennungssicherheit von mindestens 80% erreicht haben, wurden als unsicher markiert. Insgesamt zeigt sich, dass die Nennungen von männlichen Eigennamen überwiegen, im Schnitt in einem Verhältnis von 61% zu 33% mit ca. 6% Namen, die nicht eindeutig klassifiziert werden konnten. In Tabelle 6 wird die Verteilung der freiwilligen Selbstangaben von Nutzer*innen auf FF.de aufgezeigt, wobei eine Gesamtübersicht sowie eine Unterteilung nach Autor*innen und Reviewer*innen gegeben ist. Unter letzteren werden die Geschlechtsangaben der Poster*innen von Reviews/Kommentaren verstanden. Jede Autor*in und jede Reviewer*in werden dabei einmal gezählt, unabhängig von der Zahl der veröffentlichten Geschichten oder Reviews. Es ist bei der Interpretation der Zahlen zu beachten, dass viele Autor*innen gleichzeitig auch als Reviewer*innen aktiv sind und diese beiden Kategorien Duplikate enthalten, die Gesamt-Information bezieht sich aber auf die tatsächliche Gesamtzahl aller eindeutig differenzierbaren Nutzer*innen. Ein hoher Teil der Nutzer*innen gibt kein Geschlecht an (29%). Bezogen auf Nutzer*innen, die ein Geschlecht angeben, zeigt sich jedoch eine deutliche Dominanz von weiblichen Personen. Abstrahiert man von den Nicht-Angaben (N/A), ist das Verhältnis sogar ca. 92% zu 8%. Es gibt keinen wesentlichen Unterschied beim Vergleich von Autor*innen und Reviewer*innen. Die Plattform wird basierend auf Selbstauskunft also primär von weiblichen Personen genutzt. Die Altersinformationen dienen lediglich der demographischen Vertiefung und sind nicht Fokus dieses Beitrags. Sie zeigen aber eine durchschnittlich eher junge Nutzer*innen-Gruppe auf (etwa 27 Jahre). In Tabelle 7 werden die beiden Analysen in ein Verhältnis zueinander gesetzt und der Anteil weiblicher und männlicher Figurennamen in den einzelnen Autor*innen-Geschlechtsgruppen untersucht. Es zeigt sich kein wesentlicher Unterschied im Vergleich zu männlichen und weiblichen Autor*innen in der Nutzung von weiblichen oder männlichen Figurennamen. Weibliche Autor*innen nutzen männliche Figuren in einem Verhältnis von 64% zu 36% weiblichen Figuren. Der Anteil von weiblichen Figuren verringert sich lediglich um 1% für männliche Autoren. In diesem Beitrag wurden die Ergebnisse der Korpusakquise einer Sammlung von deutschsprachigen Fanfictions aufgezeigt. Es ist zu beachten, dass wir dabei einige wesentliche Bestandteile noch nicht vertieft präsentieren konnten, wie z.B. weitere Metadaten und Review-Analysen. Das Korpus ist eine relevante Ressource für den Bereich der Dennoch konnten bereits durch allgemeine Metadatenanalysen nationale Besonderheiten eines deutschsprachigen Fanfiction-Korpus wie z.B. die Bedeutung von Fandoms wie Tatort oder Die drei ??? in AO3 sowie die Bedeutung von Anime in FF.de (Cuntz-Leng und Meintzinger, 2015) herausgearbeitet werden. Dies verdeutlicht die Notwendigkeit der Analyse nicht-englischer Texte nicht nur für die lokalen Wissenschafts-Communities, sondern auch für ein angemessenes Verständnis des Phänomens an sich. Im Kontext der geschlechtsspezifischen Fragestellungen konnten Analysen und Behauptungen, wonach Frauen Fanfictions nutzen, um Geschichten über unterrepräsentierte weibliche Figuren zu schreiben (Busse, 2009; Leow, 2011; Handley, 2012; Duggan, 2017; 2020; 2022; Milli und Bamman, 2016) nicht bestätigt werden. Im Gegensatz bestätigen sich bisherige Annahmen (Jung, 2002; Busse und Lothian, 2017; Tosenberger, 2008; Rossdal, 2015; Fast et al., 2016), die Fanfictions als Literatur von Frauen primär über männliche Figuren mit Fokus auf homo-romantischen Beziehungen verstehen auch für die deutschsprachige Fanfiction-Community. Auch eine Reduktion auf das Harry Potter-Fandom analog zu Duggan (2017; 2020; 2022) zeigt dieselben Verhältnisse auf. Es sei hier jedoch auch zu beachten, dass dieses Phänomen auch als Spiegelung der allgemeinen √úberrepräsentation von Männern in kulturellen Medien betrachtet werden kann was jedoch bisher in mehrheitlich qualitativen Studien untersucht wurde (Collins, 2011; Bretthauer et al., 2007; Garcia et al., 2015; Jia et al., 2015; Neville und Anastasio, 2019; Schmidt et al., 2020b). Wir halten es auch für eine sehr spannende Idee, die hier vorliegende binäre Geschlechtsauffassung durch weitere Geschlechtsgruppen wie non-binär oder androgyn zu erweitern, wie dies teilweise schon in Computer Vision-Projekten in den DH gemacht wurde (Schmidt et al. 2021a; Schmidt und Kurek, 2022). Annotation und Akquise von non-binären Namen wäre hier für weitere Studien notwendig. Insgesamt ist mehr Forschung die"
2024,DHd2024,PICHLER_Axel__LLMs_for_everything___Potentiale_und_Probleme_.xml,"""LLMs for everything?"" Potentiale und Probleme der Anwendung von In-Context-Learning für die Computational Literary Studies","Axel Pichler (Universität Stuttgart, Deutschland); Nils Reiter (Universität zu Köln, Deutschland)","Large Language Models, In-Context-Learning, Computational Literary Studies","Modellierung, Annotieren, Theoretisierung","Große Sprachmodelle, sogenannte Large Language Models (LLMs), haben das Natural Language Processing (NLP) seit dem Aufkommen der Transformer-Architektur in den letzten Jahren revolutioniert. Spätestens seit der Veröffentlichung von ChatGPT ist das Potential dieser Modelle auch der nicht akademischen Öffentlichkeit bekannt. Ein noch nicht vollständig erklärtes Merkmal dieser Modelle ist, dass sie mit zunehmender Größe 'als Schwellenwert werden hier um die 10 Milliarden Parameter genannt, 'auch Problemlösungskompetenzen entwickeln, für die sie nicht trainiert wurden (Wei et. al. 2022). Zu diesen sogenannten ""Emergent Abilities"" zählt auch eine Trainingsmethode, bei der es sich im strengen Sinne gar nicht um eine ""klassische"" Form des Fine-Tunings handelt, da dabei keine Anpassungen der Gewichte durchgeführt werden: das In-Context-Learning (ICL, Dong et al. 2023). Darunter versteht man die Praxis, einem LLM durch die Eingabe von natürlichsprachlich verfassten Beispielen, das in diesen Beispielen inkorporierte und implizierte ""Wissen"" zu vermitteln. Wie bereits Brown et. al. (2020) für GPT-3 zeigten, können LLMs eine Vielzahl komplexer Aufgaben mithilfe von ICL lösen. Im Detail noch nicht geklärt sind die Gründe, warum sie das tun. Jüngere Untersuchungen lassen vermuten, dass dabei die Tatsache, dass die verwendeten Beispiele plausibel bzw. wahr für die Aufgabe sind, weniger wichtig ist, als andere Faktoren wie zum Beispiel die zugrundeliegende Verteilung der Beispiele bzw. deren Format (Min et al. 2022) oder die über die Trainingsdaten implizit vermittelten semantischen Relationen von Begriffen (Xie et al. 2021). Clav√≠e et. al. 2023 zeigen zum Beispiel, dass bei der binären Klassifikation der Qualifikationsvoraussetzungen für eine Stellenausschreibung große LLMs wie OpenAIs text-davinci-003-Model klassische ML-Ansätze wie SVM aber auch kleinere ""foundational models"" wie DeBERTaV3 klar übertreffen. Für die Digital Humanities im Allgemeinen und die Computational Literary Studies (CLS) im Besonderen ist das ICL auf den ersten Blick sehr attraktiv, da es Wir wollen im Folgenden das Potential von ICL an einem konkreten Beispiel aus den CLS überprüfen. Dabei handelt es sich um den Versuch, die Resultate der Operationalisierung und Modellierung von generischen Aussagen aus Andrew Pipers Cambridge Element Die von Piper und seinem Team auf den annähernd ausgeglichenen Daten trainierten Modelle erzielten F1-Scores zwischen 0.591 und 0.769 sowie Accuracy-Werte zwischen 0.638 und 0.745, wobei es sich bei dem am besten performenden Modell um ein CNN mit ELMo-Embeddings handelt, bei dem der Recall die Precision deutlich übersteigt (Piper 2020, 34). Für unsere Experimente haben wir mit OpenAIs kostenpflichtigem Das beste ICL-Verfahren erzielt somit eine um 5-7 Prozentpunkte niedrigere Performance als das beste von Piper beschriebene Modell. Im Gegensatz zu Beispielen aus anderen Feldern zeigt sich also hier keine wesentlich bessere Performance als bei der Arbeit mit kleineren ""foundational models"" wie z.B. BERT. In diesem konkreten Fall erachten wir unter anderem folgende Möglichkeiten als plausible Ursachen dafür: Erstens ist die Explikation der Unterscheidung zwischen ""generalization"" und ""neutral"" im allgemeinen Sprachgebrauch nicht üblich 'man spricht zwar von generalisierenden Aussagen, bezeichnet aber gemeinhin nicht sämtliche Aussagen, die nicht unter diese Klasse fallen als ""neutral"". Pipers theoretisch durchweg gerechtfertigtes Klassifikationsschema wird somit vom Sprachgebrauch nicht gestützt. Ergänzend zu diesen konkreten Fragen zur verhältnismäßig schwachen Performance von ICL in Hinblick auf Pipers Daten wollen wir auch noch auf weitere potentielle Problemfelder und offene Fragen in Hinblick auf den Einsatz von In-Context-Learning in den CLS hinweisen. Dazu zählt, erstens, die prinzipielle Gefahr, dass das ICL durch seinen Fokus auf Beispiele dazu einlädt, Begriffe undefiniert und unreflektiert zu verwenden. Wenn, wie in unserem Fall, die besten Resultate mit jenem Prompt erzielt werden, der keine Definition der verwendeten Begriffe beinhaltet, lädt dies dazu ein, auf die Bestimmung dieser Begriffe von Anfang an zu verzichten. Die problematischen Konsequenzen eines solchen Vorgehens liegen auf der Hand: Ohne die Begriffe definiert zu haben, läuft ein re-import der Resultate in den fachspezifischen Diskurs Gefahr, deren Umfang zu verunklaren, da die bloße Nennung von Beispielen unterschiedliche Interpretationen von der Extension dieser Begriffe zulassen. Eine ähnliche Gefahr besteht jedoch, zweitens, auch wenn der Begriff vor und für das ICL definiert wird, da die Mechanismen hinter selbigen noch nicht geklärt sind. Bei einem Prompt, der sich aus Definition, Instruktion und Beispiel zusammensetzt, wissen die Nutzenden nicht, welche der drei Komponenten für die Klassifikation letztendlich ausschlaggebend ist. Ob es tatsächlich die dabei verwendete Definition ist, bleibt unklar. Dies führt, drittens, zu einem weiteren prinzipiellen Problem beim Einsatz von kommerziellen LLMs, das hinlänglich bekannt ist: Kommerzielle Anbieter wie OpenAI stellen ihre Modelle nicht öffentlich zur Verfügung. Die per se bereits breit diskutierte vermeintliche Opazität von LLMs wird so noch zusätzlich verstärkt. Viertens sind LLMs wie das hier verwendete text-davinci-003-Model von OpenAI nicht deterministisch. Die Resultate sind dementsprechend nicht stabil. In den CLS wird die Pflicht, dass man sich im Zuge des Operationalisierungs- bzw. Annotationsprozesses festlegt (welche Kategorien man wann vergibt, was diese bedeuten, wo Annotationen anfangen und aufhören, etc.) oft als Vorteil von computergestützten Verfahren gegenüber der ""traditionellen"" Literaturwissenschaft genannt (z.B. Meister 1995), da deren Begriffe ""in der Regel zu vage oder zu abstrakt [seien], als dass man sie eindeutig formalisieren könnte"" (Meister 2012, 294). Die insbesondere von Harald Fricke seit mehreren Jahrzehnten propagierte Auffassung, dass literaturwissenschaftliche Begriffe ausgehend vom standardsprachlichen Gebrauch zu präzisieren seien, um durch die solcherart hergestellte Exaktheit Vagheiten und Mehrdeutigkeiten aus dem literaturwissenschaftlichen Sprachgebrauch zu tilgen (Fricke 1989), bildet zwar mittlerweile das sprachtheoretische Fundament des Für den Einsatz von ICL in den CLS bedeutet das unseres Erachtens Folgendes: Erstens sollte man, unabhängig davon auf welches Sprachmodell man bei der Textanalyse zurückgreift, die für die Analyse zentralen Begriffe definieren und 'idealerweise 'manuell einen Referenzdatensatz erstellen. Dies erlaubt es, auch opake Modelle auf eine Art und Weise empirisch zu verankern, die den Nachvollzug sowie die Überprüfung der Validität der Analysen erleichtert bzw. in manchen Fällen überhaupt erst ermöglicht. Zweitens sollte man, falls man sich für den Einsatz von ICL entscheidet, zuerst mit kleineren Samples arbeiten, um zu überprüfen, ob das ICL überhaupt traditionelle Verfahren übertrifft: Bei Begriffen, deren Definitionen sich vom Alltagsgebrauch unterscheiden, ist das Fine-Tuning eines"
2024,DHd2024,GIOVANNINI_Luca_Doctoral_Consortium__Luca_Giovannini_final.xml,Quantitative Ansätze zur Untersuchung der frühneuzeitlichen Dramengeschichte,"Luca Giovannini (Universität Potsdam, Deutschland / Universität Padua, Italien)","Theathergeschichte, Drama, Vergleichende Literaturwissenschaft, Formalismus, Operationalisierung","Theathergeschichte, Drama, Vergleichende Literaturwissenschaft, Formalismus, Operationalisierung","In den letzten Jahren hat sich die quantitative Forschung zum Drama als ein wichtiger Teil der computergestützten Literaturwissenschaft etabliert. Zum frühneuzeitlichen Drama gibt es allerdings noch wenig umfassende Studien zu verzeichnen, die über die Grenzen der nationalen Philologien hinausgehen und quantitative Beiträge zur Komparatistik liefern. Davon ausgehend ist Ziel des Promotionsvorhabens, eine quantitative Geschichte des frühneuzeitlichen europäischen Theaters zu skizzieren, die die Evolution der verschiedenen Nationalliteraturen vergleichend rekonstruiert. Als Ausgangspunkt der Dissertation dient die u. a. von Moretti (1994) verbreitete These, dass die Entwicklung des europäischen Theaters in der frühen Neuzeit als ein Prozess der biologischen Artbildung interpretiert werden kann. Im Laufe des 17. Jahrhunderts, so Moretti, wurde ein europaweites Modell der Tragödie, das aus der Antike und dem Mittelalter übernommen wurde, durch nationale Varianten wie das deutsche Trauerspiel oder die französische Dank der steigenden Textverfügbarkeit und den Fortschritten in den computationellen Methoden lässt sich diese bisher unhinterfragte literaturgeschichtliche These nun empirisch überprüfen. Daher lauten die konkreten Fragestellungen, wie eine solche Entwicklung der dramatischen Formen mit quantitativen Methoden nachzuvollziehen ist und ob der von Moretti beschriebene ""Verzweigungsprozess"" nicht nur für die Tragödie, sondern auch für die Komödie und andere Gattungen stattgefunden hat. Das Promotionsvorhaben erfolgt im Umfeld des Die Methodologisch inspiriert sich das Promotionsprojekt an den Forschungsansätzen des quantitativen Formalismus (Allison et al. 2011): Im Fokus steht die Struktur dramatischer Texte, d.h. eine der Komponenten, anhand derer die Entwicklung der Gattung Drama gezeigt werden kann. Da sich diese Dimension auf nicht sprachbedingte Elemente bezieht, etwa Figurenkonstellationen oder Redeverteilung, kann man sie produktiv für eine komparative Studie verschiedener Nationaltraditionen einsetzen. Als zentrale analytische Praxis für die Untersuchung der Variation des europäischen Dramas wird dann die Vektorisierung von Theaterstücken nach ihren strukturellen Merkmalen eingesetzt. Öhnlich wie bei Im Rahmen des Promotionsvorhabens sind zwei Anwendungsmöglichkeiten für die vektorisierten Stücke vorgesehen. Zum einen können mithilfe verschiedener Abstandsmessungen (z. B. euklidischer Abstand oder Kosinus-Öhnlichkeit) die Distanzen zwischen den Vektoren berechnet werden, wobei ein größerer Abstand auf eine größere strukturelle Unterschiedlichkeit hinweisen soll. Zum anderen ist es möglich, die Vektoren durch Techniken wie die Hauptkomponentenanalyse (PCA) auf einer niedrigdimensionalen Ebene zu visualisieren, um Cluster zu identifizieren. Erste Ergebnisse zeigen, dass ein Narrativ von kontinuierlicher Verzweigung zwischen literarischen Traditionen nicht ohne Einschränkungen vertretbar ist. Obwohl eine Tendenz zur Diversifizierung bemerkbar ist, ist die Gattungsevolution scheinbar durch komplexe und mehrschichtige Dynamiken geprägt. Auch wenn die Arbeit mit einem kleinen, aber sorgfältig kuratierten Korpus wie"
2024,DHd2024,KONLE_Leonard_Modellierung_von_Gattungsunterschieden__Emotio.xml,"Modellierung von Gattungsunterschieden. Emotionen in Lyrik, Prosa und Drama","Merten Kröncke (Universität Würzburg, Deutschland); Leonard Konle (Universität Göttingen, Deutschland); Simone Winko (Universität Würzburg, Deutschland); Fotis Jannidis (Universität Göttingen, Deutschland)","Emotion, Gattung, Domäne, Fehleranalyse, Machine Learning","Modellierung, Annotieren, Literatur, Methoden, Forschungsprozess, Text","Literaturwissenschaftliche Untersuchungen zielen häufig darauf ab, verschiedene Textgruppen (zum Beispiel Gedichte, Romane und Dramen) hinsichtlich verschiedener Texteigenschaften (zum Beispiel Themen oder Emotionen) miteinander zu vergleichen. In der Computerlinguistik (oder allgemeiner im gesamten Feld Machine Learning) wird die erläuterte Problemstellung unter dem Schlagwort Domain Adaptation intensiv beforscht (z.B. Ramponi und Plank 2020). Machine Learning-Probleme lassen sich als Versuch beschreiben, eine automatische Zuweisung von Datenpunkten x zu Labeln y zu lernen. Dabei wird unterstellt, dass alle Punkte x aus der gleichen Verteilung stammen, die zu lernende Zuweisung x‚Üíy also für jeden Datenpunkt ähnlich funktioniert. Diese Annahme ist in angewandter Forschung (darunter Computational Literary Studies) jedoch selten zu halten. Die Gründe für einen Domain Shift, also die Veränderung von x, während y stabil bleibt, können vielfältig sein (z.B. historischer Sprachwandel, Übersetzungen). Die Bereitstellung von Datensätzen, die dezidiert mehrere Domänen enthalten, ist also sowohl für Machine Learning Forschung als auch für die CLS hoch relevant. Als Untersuchungsbeispiel dient die Gestaltung von Emotionen in deutschsprachigen Lyrik-, Prosa- und Dramentexten der zweiten Hälfte des 19. und des beginnenden 20. Jahrhunderts. In früheren Studien hat sich unsere Forschungsgruppe auf Lyrik konzentriert und für diese Gattung umfangreiche manuelle Annotationen erstellt. Die Studie verwendet drei Korpora: ein vergleichsweise großes Lyrikkorpus, für das umfangreiche manuelle Annotationen vorliegen, und zwei deutlich kleinere, zu Testzwecken zusammengestellte Korpora mit einerseits Prosa- und andererseits Dramentexten. Das Lyrikkorpus besteht aus Texten in Anthologien aus dem Untersuchungszeitraum, die sich auf Gedichte von Zeitgenoss:innen konzentrieren. Die Anthologien stammen aus der Zeit von 1859 bis 1919 und enthalten mehr als 6000 Gedichte, von denen 1412 (270k Token) annotiert wurden. Die Emotionsannotation zielt darauf ab, die im Text gestalteten Emotionen (und nicht die Emotionen der Leser:innen) zu erfassen. Genutzt wurde ein Set von 40 diskreten Emotionen, darunter zum Beispiel Liebe, Trauer, Hoffnung, Sehnsucht oder Hass. Einerseits handelt es sich um Emotionen, die in gängigen Emotionstheorien Tabelle 2 zeigt die Qualität der Emotionsklassifikation in den drei Gattungen. Verwendet wird ein Modell, welches lediglich mit den Annotationen für Lyrik trainiert ist. Basis ist das deutsche Bert-Modell gbert-large (Chan et al. 2020). Dieses wird zusätzlich auf Lyrik angepasst Die folgenden Abschnitte beschäftigen sich mit der Suche nach möglichen Erklärungen für die großen Qualitätsunterschiede (siehe Tab. 2). Es werden sowohl Eigenschaften des Modells als auch die Verteilung der annotierten Emotionen, die Zusammensetzung einzelner Emotionen und das zugrundeliegende Textmaterial untersucht. Zunächst lässt sich danach fragen, wie sicher sich das Modell bei den Klassifikationen ist (Abb. 1). Blickt man auf die Vorhersage von (vorhandenen) Emotionen, ist die Sicherheit erwartungsgemäß bei Lyrik am größten, gefolgt von Prosa und danach Drama. Bei der Vorhersage ""keine Emotion"" ist sich das Modell hingegen im Fall von Dramentexten besonders sicher (sogar noch sicherer als im Fall von Gedichten) und im Fall von Prosa besonders unsicher. Zum einen scheint die Klassifikationsperformance also mit der Sicherheit des Modells zusammenzuhängen; zum anderen weisen die Differenzen zwischen Prosa- und Dramentexten in puncto ""Emotion""/""Keine Emotion"" auf klassifikationsrelevante Gattungsunterschiede hin. Um einen Eindruck von der Emotionsverteilung innerhalb der Gattungen zu erhalten, werden jeweils 50 Segmente (Verse bzw. Sätze) zu einer Einheit zusammengefasst, die als Vektor über die Anzahl der enthaltenen Emotionen repräsentiert wird. Um diese Vektoren zu visualisieren, werden sie in den 2-dimensionalen Raum projiziert (siehe McInnes 2018). Das Resultat (Abb. 2) zeigt, dass die annotierten Gedichte stärker streuen als die annotierten Texte der übrigen Gattungen, also vielfältigere Mischungen an Emotionen enthalten. Auffällig ist zusätzlich die Häufung von Dramen und Prosa im oberen rechten Bereich der Grafik. Die Emotionsverteilung innerhalb der beiden Gattungen ähnelt sich nach diesem Befund und weicht zugleich von der Verteilung in den meisten lyrischen Texten ab. Abbildung 3 macht deutlich, dass die Gattungen Emotionen in stark unterschiedlicher Häufigkeit gestalten. Lyrik enthält mit Abstand die meisten Emotionen, beinahe das Dreifache im Vergleich zu Dramen. Diese enthalten wiederum das Doppelte an Emotionen, gemessen an Prosa. Abbildung 4 ermöglicht einen Einblick in die einzelnen Emotionsgruppen nach Shaver ( Abbildung 5 zeigt beispielhaft für die Emotionsgruppe Erregung/Überraschung, wie sich die Gruppe je nach Gattung anteilig zusammensetzt. Es zeigen sich erhebliche Unterschiede: Während in lyrischen Texten die Kategorie ""Emotionalität"" dominiert, die vor allem für unspezifische Emotionen eingesetzt wird (""Er war ein grundsätzlich emotionaler Mensch"" usw.), kommt in den annotierten Dramen ""Aufregung"" am häufigsten vor; in den annotierten Prosatexten ist wiederum die Einzelemotion ""Spannung"", verglichen mit den anderen Gattungen, besonders verbreitet. Diese Unterschiede erzeugen ein großes Fehlerpotential, da sich mit der Zusammensetzung auch die Repräsentation der Gruppe im Modell ändert. Während in Lyrik bereits gute Ergebnisse erzielt werden können, wenn lediglich die Einzelemotion ""Emotionalität"" erkannt wird, ist diese für Prosatexte nutzlos. Umgekehrtes gilt für Spannung. Nachdem die Verteilungsunterschiede in den Emotionen dargestellt sind, werden Differenzen in der sprachlichen Gestaltung der annotierten Texte untersucht. Neben der bislang betrachteten Emotionsannotation wurde separat festgehalten, welche Wörter im Text über ihre lexikalische Bedeutung markieren, dass eine Emotion gestaltet wird, zum Beispiel Ausdrücke wie ""Angst"", ""lachen"" oder ""jauchzen"". Eine Emotionsannotation wird meist, aber nicht immer, von der Annotation entsprechender Emotionswörter begleitet; umgekehrt kommen Emotionswörter nie ohne zugehörige Emotionsannotation vor. Tabelle 3 zeigt, wie viele Emotionswörter pro Emotionsannotation je nach Gattung vorkommen. In Dramen werden etwas mehr Emotionswörter verwendet als in Prosa und Lyrik, eine explizitere Nennung von Emotionen in Lyrik als mögliche Fehlerquelle in der anschließenden Klassifikation der anderen Gattungen kann damit ausgeschlossen werden. Abschließend lässt sich danach fragen, wie groß der Abstand zwischen den Gattungen hinsichtlich des Textmaterials ist und ob die Unterschiede in der Klassifikationsperformance dazu ""passen"". Abb. 6 zeigt, dass der Abstand zwischen Lyrik und Prosa am größten, der Abstand zwischen Lyrik und Drama bereits deutlich geringer und der Abstand zwischen Drama und Prosa am geringsten ausfällt. Dass Lyrik also, was das Textmaterial angeht, eher den einbezogenen Dramen- als den Prosatexten ähnelt, schlägt sich jedoch nicht unmittelbar in der Klassifikationsperformance nieder, die nämlich im Fall von Dramen nicht besser als im Fall von Prosa ist. Die Studie ist von der Frage ausgegangen, wie sich etwaige Unterschiede zwischen den literarischen Großgattungen Prosa, Drama und Lyrik in puncto Emotionsgestaltung mit Fragen der Domain Adaptation verknüpfen und aus dieser Perspektive modellieren lassen. Der (zu erwartende) Befund, dass ausschließlich auf Lyrikannotationen trainierte Modelle deutlich schlechter performen, wenn sie auf Prosa- und Dramentexte angewendet werden, kann mit einer ganzen Reihe von Faktoren zusammenhängen, von denen einige näher untersucht wurden. Neben pragmatischen Gesichtspunkten, zum Beispiel den etwas niedrigeren Inter-Annotator-Agreement-Werten, scheinen vor allem Spezifika der Domänen eine Rolle zu spielen. Erhebliche Unterschiede zwischen den Gattungen zeigen sich unter anderem, wenn man die Häufigkeit und Verteilung der gestalteten Emotionen betrachtet und wenn man danach fragt, aus welchen Einzelemotionen sich die Emotionsgruppen zusammensetzen. Demgegenüber deuten weitere Ergebnisse darauf hin, dass sich manche Gattungsunterschiede Die Ergebnisse deuten indizienhaft an, dass Gedichte, verglichen mit Dramen und Prosatexten, besonders häufig Emotionen gestalten. Dieser Befund passt zu den in der Einleitung erwähnten gattungstypologischen Vermutungen, wenngleich berücksichtigt werden muss, dass an dieser Stelle nur sehr wenige Prosa- und Dramentexte einbezogen werden konnten. Um noch besser abgesicherte Schlüsse über die Gattungen zu ermöglichen, werden wir weitere Texte annotieren und verschiedene Verfahren der Domain Adaptation testen, um letztlich auch für Prosa und Dramen zuverlässige Klassifikatoren trainieren zu können."
2024,DHd2024,20240108_LEMKE_Marc_CANSpiN__Zur_computergest_tzten_Analyse_narrative.xml,CANSpiN: Zur computergestützten Analyse narrativen Raums im Roman des 19. und 20. Jahrhunderts,"Marc Lemke (Universität Rostock, Deutschland); Ulrike Henny-Krahmer (Universität Rostock, Deutschland); Nils Kellner (Universität Rostock, Deutschland)","Raum, Literatur, Roman, Deep Learning, BERT","Entdeckung, Programmierung, Räumliche Analyse, Modellierung, Annotieren, Methoden","Mit unserer Einreichung stellen wir den aktuellen Arbeitsstand des Projekts ""Computational Approaches to Narrative Space in 19th and 20th Century Novels"" (CANSpiN) vor, das im Rahmen des DFG-Schwerpunktprogramms ""Computational Literary Studies"" (SPP 2207) von April 2023 bis März 2026 gefördert wird. Ziel des Vorhabens ist es, computergestützte Methoden zur Erkennung und Analyse narrativen Raums in literarischen Texten zu entwickeln und diese Methoden für die Untersuchung literaturhistorischer Fragen zum Verhältnis von Raum und nationaler Identität in deutsch- und spanischsprachigen Romanen des 19. und 20. Jahrhunderts zur Anwendung zu bringen. Dieser Zielstellung entspricht die Zusammensetzung der Projektgruppe, die aus Wissenschaftler:innen der Germanistik, Romanistik, den Digital Humanities und der Mathematik besteht. Ausgangspunkt der Überlegungen ist die Definition des narrativen Raums: Darunter verstehen wir im weiteren Sinne die Räumlichkeit eines narrativen Textes nach Schumacher (2022b), die durch raumreferentielle sprachliche Ausdrücke bestimmt ist. In einem engeren narratologischen Sinne begreifen wir narrativen Raum als den Raum der erzählten Welt, wie er durch die Erzählung konstituiert ist (Genette, 2010). Wie dieser Raum strukturell und funktional zu beschreiben ist, dazu existieren bereits zahlreiche literaturwissenschaftliche Vorschläge (Dennerlein, 2009; Piatti, 2008; Ryan, 2014; Lotman, 1977; Renner, 2004). Wie raumanalytische Zugänge formalisiert und auf konkrete Textmerkmale abgebildet werden können, dazu bieten Arbeiten der Computational Literary Studies schon erste Antworten (Viehhauser und Barth, 2017; Barth, 2021, 2022; Viehhauser, 2020; Schumacher, 2022a, 2022b). Auf diesem Forschungsstand setzen wir auf: In einem offenen explorativen Verfahren erproben wir verschiedene raumanalytische Kategorien-Sets hinsichtlich ihrer literaturwissenschaftlichen Aussagekraft und ihrer computergestützten Operationalisierbarkeit (Arbeitspaket 1). Für jedes dieser Sets entwickeln wir Annotationsrichtlinien, um die verwendeten Korpora computergestützt zu annotieren und mit quantitativen Methoden vergleichend zu untersuchen. So haben wir ein erstes Kategoriensystem CANSpiN.CS1 für die Annotation von Raumreferenzen in Erzähltexten definiert und mit Hilfe des Tools CATMA (Gius et al., 2023) erprobt. Es ist geplant, auf diesem Wege eine Ground Truth aufzubauen, mit der im Tool NTEE (Lemke et al., 2023) Modelle für die Erkennung von Raumentitäten trainiert werden können, welche die semi-automatisierte, vollständige Annotation der Textkorpora im Projekt erlauben. Das Interesse des Projekts richtet sich dabei insbesondere auch auf die Möglichkeiten und Grenzen des Deep Learnings mit Sprachmodellen der BERT-Architektur (Devlin et al., 2019) für die computergestützte Annotation literaturwissenschaftlich definierter Textphänomene: Um diese auszuloten, werden die im Arbeitspaket 1 erzeugten Modelle mit Methoden eines Explainable AI-Ansatzes (XAI) untersucht (Arbeitspaket 2), was konkrete Maßnahmen und Vorschläge zur Optimierung der Annotationsprozesse erwarten lässt, aber auch Erkenntnisse über die technischen Grenzen dieses Vorgehens (Rogers et al., 2021). Die Korpora werden derzeit ausgehend von verschiedenen Vorarbeiten zusammengestellt, aus deutschsprachigen Romanen des 19. Jahrhunderts (Zeit: 1790-1910, Ziel: etwa 200 Romane, Quellen: ELTeC-deu (Konle et al., 2021), DTA (Berlin-Brandenburgischen Akademie der Wissenschaften, 2023), TextGrid Repository (TextGrid Konsortium, 2014)), spanischsprachigen Romanen des 19. Jahrhunderts (Zeit: 1790-1870, Ziel: etwa 100 Romane, Quellen: ELTeC-spa (Navarro-Colorado et al., 2021), Biblioteca Virtual Miguel de Cervantes (Centro de Humanidades Digitales en la Universidad de Alicante, 2021)), spanischsprachigen Romanen aus Lateinamerika des 19. Jahrhunderts (Zeit: 1830-1910, Ziel: etwa 200 Romane, Quellen: Corpus de novelas hispanoamericanas del siglo XIX (Conha19) (Henny-Krahmer, 2021)) und deutschsprachigen Romanen des 20. Jahrhunderts (Zeit: 1950-2000, Ziel: etwa 100 Romane, Quellen: TEI-Dateien der Uwe Johnson-Werkausgabe, E-Books (u.a. zu den Autor:innen Heinrich Böll, Christine Brückner, Uwe Johnson, Walter Kempowski, Christa Wolf)). Für ein einheitliches Format der Texte haben wir uns für das Schema ELTeC Level 0 (Burnard, 2012) entschieden, das wir für unsere Zwecke im CANSpiN-Projekt anpassen. Die Auswahl der Korpora gründet auf unserer Arbeitshypothese, dass narrativer Raum in Romanen sprach- und zeitübergreifend ein für die Analyse zugängliches Phänomen ist, das unterschiedlich ausgeformt, also dargestellt und funktionalisiert sein kann. Durch die quantitativen Analysen aus Arbeitspaket 1 erwarten wir in dieser Hinsicht neue Erkenntnisse zu literaturhistorischen Fragestellungen (Arbeitspaket 3), etwa zur Darstellung und Semantik von Raum in Romanen im Kontext der Nationenbildung im 19. Jahrhundert in Deutschland und Lateinamerika (Sommer, 1993; Pe√±aranda Medina, 1994; Hanway, 2003; Viel, 2009; Ferrer, 2018) und der Herausbildung zweier deutscher Identitäten in der Nachkriegsliteratur des 20. Jahrhunderts (Bond, 1996; Erll et al., 2003; Helbig et al., 2007; Westphal, 2007; Nies, 2018)."
2024,DHd2024,G_GGELMANN_Michael_Co_Kreativit_t_digital_erschlie_en___ber_.xml,Co-Kreativität digital erschließen: Über die Annotation komplexer ästhetischer Phänomene,"Matthias Bauer (Universität Tübingen, Deutschland); Michael Göggelmann (Universität Tübingen, Universität zu Köln, Deutschland); Sara Rogalski (Universität Tübingen, Deutschland); Sandra Wetzel (Universität Tübingen, Deutschland); Angelika Zirker (Universität Tübingen, Deutschland)","Annotieren, Co-Kreativität, Akte, Artefakte, Shakespeare, Spenser, Donne, Herbert, Vaughan, Lyrik, Frühe Neuzeit, Gemeinschaftliche Autorschaft, Gedichte","Annotieren, Kollaboration, Artefakte, Literatur, Forschungsprozess, Text","Obwohl gemeinschaftliche Autorschaft in der frühen Neuzeit häufig der Normalfall war, geht die Forschungsliteratur weiterhin und gerade dann von einem Konzept der Einzelautorschaft aus, wenn sie sich lediglich auf die Identifizierung der Anteile individueller Autoren (insbesondere Shakespeares) an Gemeinschaftswerken fokussiert (z.B. Vickers, 2002). Das Aufkommen und die Verbesserung digitaler Methoden hat leider nur zu einer Bagatellisierung des Konzepts gemeinschaftlicher Autorschaft geführt; stilometrische Untersuchungen von dramatischen Texten befördern die Vorstellung von gemeinschaftlicher Autorschaft als Summe von Einzelautorschaften und reduzieren die Autor- und Urheberschaft auf den Stil. Gemeinschaftliche Autorschaft ist aber mehr als die Summe ihrer Teile; um sie zu erforschen, braucht man eine Idee davon, was an ihr anders ist. Um dies herauszufinden, sollte man wissen, wie in der Frühen Neuzeit selbst darüber gedacht wurde. Im Projekt zur Östhetik gemeinschaftlicher Autorschaft Wir entschieden uns zu Beginn der Arbeit für kurze, aber reflexionsdichte Texte, d.h. Gedichte, um möglichst schnell einen Einblick in verschiedene Autoren und Werke zu erlangen. Bei der Aufbereitung der unserer Arbeit zugrunde liegenden Gedichtkorpora griffen wir überwiegend auf editierte, in digitaler Form vorliegende Werke bekannter frühneuzeitlicher Autoren zurück und bereiteten diese auf Basis intern entwickelter Richtlinien für die Weiterarbeit im Aufgrund der Komplexität des Phänomens, das wir untersuchen, nutzten wir den Der erste Versuch, co-kreative Reflexionen in Gedichten zu annotieren, basierte auf dem Expertenvorwissen aller Annotator:innen. Wir suchten zunächst nach Bausteinen, die kombiniert, so lautete unsere Hypothese, ein bestimmtes Konzept der Co-Kreativität bildeten. Wenn also z.B. Herbert von einem Austausch der Gedichte zwischen Gott und ihm spricht, könnte man dieses Konzept als aus den Bausteinen des Gebens und Nehmens gebildet analysieren. Die Herausforderung dieser Herangehensweise lag darin, die Bausteine ohne Kenntnis des im Gedicht anzutreffenden Konzepts zu identifizieren. Ein erster Versuch bestand darin, Synonyme bereits bekannter Bausteine in unseren Fallbeispielkorpora ausfindig zu machen und manuell 50 Dieses Modell geht davon aus, dass die Reflexion über co-kreative Prozesse einen Sonderfall der Reflexion über Produktionsvorgänge bildet, an denen mehrere Akteure beteiligt sind. Dementsprechend erfolgt die Annotation co-kreativer Reflexionen in mehreren Schritten: zunächst muss der im Text erwähnte Akt der Produktion oder das produzierte Artefakt (A) in den Blick genommen werden, dann die Akteure, auf denen die Co-Kreativität (CO) beruht, sowie im letzten Schritt die damit verbundene Prädikation (P), d.h. das, was über die Produktion gesagt wird. Das Zusammenspiel aller drei Komponenten bilden eine co-kreative Konstellation. Verdeutlicht werden kann dies am Beispiel von George Herberts Gedicht ""A True Hymne"": Hier heißt es in Z.17-18: ""Although the verse be somewhat scant, / God doth supplie the want"" (Herbert, 2008, 574). ""Verse"" ist das geschaffene Artefakt (A), das auf dem (hier implizierten) Akt des Schreibens basiert; die Co-Kreativität (CO) besteht in der Zusammenarbeit des Sprechers mit Gott, und die Prädikation (P) ist ""supplie the want,"" d.h. die Beschreibung der Leistung des Co-Autors. Co-Kreativität wird hier demnach als Aktivität beschrieben, in der ein Beteiligte:r die Mängel des oder der anderen ausgleicht. Die Annotation sieht wie folgt aus: ""Although the [verse] Ein erstes zentrales Ergebnis des Entwicklungsprozesses der Annotationsrichtlinien für die erste Komponente, die Akte und Artefakte (A), war die Feststellung, dass wir einen ganz offenen und neuen Minimalkonsens dafür schaffen mussten, worin ""Gemachtheit"" besteht. Als Grundlage für die Annotationsrichtlinien gilt daher: Die Annotation der Akte und Artefakte erforderte zudem ausführliche Angaben zu ihrer Verankerung im Text, wie z.B. die Regelung, dass wir maximal annotieren, d.h. dass wir alle syntaktischen Elemente, die einen Akt oder ein Artefakt spezifizieren, mitannotieren. Darüber hinaus führen die Annotationsrichtlinien eine Reihe von Spezialfällen auf, wie z.B. die Annotation von hypothetischen oder destruktiven Akten und Artefakten. Wenn alle Akte und Artefakte in einem Korpus erfasst und nummeriert wurden, erfolgt im zweiten Schritt die Annotation der Komponente ""CO"". Die Annotationsrichtlinien sehen hier vor: Da in einem Gedicht mehrere Akte und Artefakte mit unterschiedlichen COs auftreten können, gilt es an dieser Stelle im Annotationsprozess, die einzelnen Komponenten und ihre Zugehörigkeit zu einer Konstellation über verbindende Marker kenntlich zu machen. Die dritte Komponente, die Annotation der Prädikation (P), befindet sich momentan noch in der Erprobung. Grundsätzlich soll P eine Verhältnisbestimmung der einzelnen Komponenten beitragen. Über die Erfragung des Verhältnisses zwischen ""A"" und ""CO"" kommt man zur Prädikation. In Donnes ""A Valediction of Weeping"" (2008, 112) stehen der Sprecher und die Geliebte z.B. im wechselseitigen Abhängigkeitsverhältnis (""Since thou and I sigh one anothers breath,"" Z. 26). Diese wechselseitige Abhängigkeit zwischen den Beteiligten bildet also eine Aussage über einen co-kreativen Prozess, der nach erfolgter Annotation mit anderen Aussagen über ähnliche Prozesse bzw. mit ähnlichen Aussagen über andere co-kreative Prozesse verglichen werden kann. Das Ziel der P-Annotationen ist es, möglichst viele Aussagen über die Verhältnisse der annotierten Bislang wurden in unserem ersten Korpus mit fünfzig Gedichten alle Akte und Artefakte sowie aller CO-Akteure annotiert. Während die Erprobung der P-Annotationsrichtlinien läuft und das zweite 50er Korpus auf A und CO hin annotiert wird, können uns erste Auswertungen der vorhandenen A- und CO-Annotationen bereits Erkenntnisse liefern, die ohne diesen digitalen Zugang nicht ersichtlich wären und zudem das Potential dieser Annotationsmethode und des Die im Die Anzahl an Akten und Artefakten, die im Rahmen einer einzigen co-kreativen Aktivität erschaffen werden, bezeugt die Komplexität des untersuchten Phänomens. In manchen Fällen werden sogar bis zu neun verschiedene Akte und Artefakte in einer co-kreativen Konstellation kreiert. Die Vielschichtigkeit der Reflexionen, die wir zu erfassen suchen, wird auch in der folgenden Abbildung deutlich, die unterschiedliche CO-Konstellationen gemäß der Häufigkeit ihres Auftretens sortiert: Die Abbildung zeigt auf, welche CO-Akteure besonders häufig miteinander produktiv tätig sind: der Sprecher des Gedichts etwa tritt 41x als CO im Korpus auf, Gott 33x; gemeinsam sind sie in dieser Konstellation 12x co-kreativ tätig. Das Diagramm verschafft also einen ersten Eindruck in die unterschiedlichen CO-Konstellationen zwischen zwei oder mehr Akteuren, die wir in Reflexionen über Co-Kreativität antreffen. Die Datenlage zeigt insgesamt, dass diese Reflexionen in doppelter Hinsicht hochkomplex sind: sie involvieren mehrere COs in unterschiedlichen Konstellationen, und es werden meist die Schöpfungsprozesse von mehr als zwei Akten und Artefakten reflektiert. Unsere Annotationsmethode ermöglicht uns aber nicht nur die Erfassung der beteiligten COs und geschaffenen As, obgleich allein diese Daten bereits erkenntnisreich sind, wenn beispielsweise Untersuchungen zu den am häufigsten auftretenden Kollaborationspartnern Gottes erwünscht sind. Unser komplexes Annotationssystem bietet darüber hinaus die Möglichkeit, über die Annotation zusätzlicher Eigenschaften den Konstellationen bestimmte Kategorisierungen zuzuteilen. In diesem Korpus führten wir z.B. den sog. Marker ""enabling"" ein, da der heuristische Prozess des Annotierens und Revidierens der Annotationsrichtlinien bereits zur Aufstellung einer Hypothese führte: es schien, als würde Co-Kreativität zwischen Gott und Mensch auf einer Abhängigkeit des Menschen von Gott beruhen. Diese Abhängigkeit konnten wir weiter als eine Befähigung des Menschen zum Kreieren durch Gott spezifizieren. Das ganze Korpus wurde dahingehend untersucht und alle co-kreativen Reflexionen in denen ein CO das andere CO zum schöpferischen Prozess befähigt, erhielten den Marker ""enabling."" Da es sich nicht um eine spezifische Aussage über den Produktionsprozess handelt, wurde diese Abhängigkeit zwischen den Personen nicht als P annotiert. Die Kreisdiagramme zeigen, dass Gott in über 52% aller ""enabling""-Konstellationen, die fast ein Drittel aller co-kreativen Konstellationen ausmachen, partizipiert. Damit können wir eine gängige Annahme, dass Gott und Mensch in der frühen Neuzeit nicht als kreative Partner gedacht wurden, auf Grundlage unserer Annotationsmethode und der Auswertung widerlegen. Erkenntnisse dieser Art legen den Baustein für ein breiteres Verständnis eines vormodernen Konzepts der Co-Kreativität. Die Grundlage unserer digitalen Arbeit bilden die Konstellationen aus As und COs, die über Marker um Eigenschaften ergänzt werden, bspw. durch den ""enabling""-Marker oder Marker, die Selbstreferenzialität oder Metaphorik kennzeichnen. Die Einführung unserer dritten Komponente, der Prädikation P, wird zusätzliche Eigenschaften der co-kreativen Konstellationen aufzeigen. Der größte Gewinn dieser Annotationsarbeit ist also der große Datensatz an bereits erfassten co-kreativen Reflexionen, die mit Eigenschaftsmarkern versehen und deren Verhältnisse zueinander bestimmt wurden. Diese aufbereiteten Daten lassen uns erkennen, wie über co-kreative Prozesse gesprochen und gedacht wurde; das so entstandene Bild wird helfen, auch die Praxis der kreativen Zusammenarbeit neu zu betrachen. Die vorgestellte komplexe Annotationsmethode erlaubt es nicht nur, komplizierte Reflexionen in Einzelkomponenten herunterzubrechen und zu systematisieren. Sie bietet darüber hinaus methodische Ansätze für die Annotation weiterer komplexer literarischer Phänomene."
2024,DHd2024,20240108_HORSTMANN_Jan_InterAnnotator__Interfaces_f_r_die_Annotation_.xml,InterAnnotator: Interfaces für die Annotation intertextueller Relationen,"Jan Horstmann (Universität Münster, Deutschland); Christian Lück (Universität Münster, Deutschland); Immanuel Normann (Universität Münster, Deutschland); Jan-Erik Stange (Universität Münster, Deutschland)","Intertextualität, Annotation, Interface","Annotieren, Theoretisierung, Kollaboration, Text, Visualisierung, virtuelle Forschungsumgebungen","Theoriegetriebene Überlegungen zur Annotation von Intertextualität fortführend, wird ein Entwurf zum Ausgangspunkt war eine Erhebung über verschiedene Theorien der Intertextualität seit Einführung des Begriffs 1967 durch Julia Kristeva (Kristeva 1972) und das Herausarbeiten ihres gemeinsamen konzeptionellen Kerns (Horstmann, Lück, Normann 2023 und dies. angenommen). Dieser konzeptionelle Kern und einflussreiche Ausprägungen der Theorie (etwa Hypertextualität, Genette 1993), wurden in RDF/OWL formalisiert, was zu einer modularen Ontologie geführt hat. Die Kern-Ontologie beschreibt ein Datenmodell, dessen zentrale Relation die intertextuelle Relation ist: Sie ist eine gerichtete Relation von einem späteren Text bzw. einer Textstelle auf einen Avant-Text bzw. eine Stelle darin; sie verfügt über weitere Properties, insbesondere ist sie entsprechend einer theoretischen Ausprägung klassifizierbar (z.B. als Travestie im Sinne Genettes) und die Technisch gesehen ist der Während in unseren Vorarbeiten formale Methoden und das Datenmodell im Vordergrund standen, folgt die weitere Entwicklung des Tools einem offenen und iterativen Co-Kreationsprozess zur Visualisierung intertextueller Relationen nebst Konzentration auf Prinzipien der User-Experience. In einem Intertextualitätsnetzwerk sind Knoten ganze Texte und Textstellen. Es ist wünschenswert, in das Netzwerk Ausgehend von dieser Grundidee soll der  In Fig. 3 ist der Screen in zwei Bereiche unterteilt, links das Netzwerk, rechts die Textansicht. Aus dem Netzwerk lassen sich Texte in die Textansicht herüberziehen, um mit ihnen zu arbeiten. Durch das Markieren von Textstellen in expandierten Texten können Annotationen erzeugt werden, zwischen denen wiederum Verbindungen gezogen werden können. Rechts neben den lesbaren Texten werden diese als Balken sowie ihre Verbindungen als Bogendiagramm repräsentiert, was die Navigation intertextueller Relationen zweier Texte ermöglicht. Diese Konfiguration ist besonders nützlich bei Annotation und Analyse intertextueller Relationen einzelner Textstellen. Für hermeneutische Zugänge und Close-Reading-Ansätze ist sie besonders wertvoll. Netzwerk und der als Synopse organisierte Textbereich sind in Fig. 4 visuell stärker voneinander getrennt. In der Skizze sieht man fünf ausgewählte Texte, die jeweils eine eigene scrollbare Spalte zum Lesen haben und eine schmale, detailarme Balken-Repräsentation zur Navigation rechts daneben. Öhnlich wie in den anderen Varianten lassen sich parallel in mehreren Texten Stellen markieren, die dann miteinander verbunden werden können, um intertextuelle Relationen zu erfassen. Auch diese synoptische Konfiguration ist insbesondere für Close-Reading-Methoden geeignet. Öhnlich wie in der ersten Konfiguration startet Fig. 5 mit einer bildschirmfüllenden Ansicht des globalen Netzwerks. In diesem lassen sich Nodes auswählen (oder neue Nodes hinzufügen), um ein Korpus zusammenzustellen. Die ausgewählten Texte werden in einem weiteren Bereich als Balken radial angeordnet und bereits existierende Verbindungen zwischen ihnen angezeigt. Die Selektion eines Balkens öffnet eine lesbare Textansicht rechts daneben, mit der wiederum Querverbindungen zwischen Textstellen erzeugt werden können, welche im Radialdiagramm links daneben sichtbar werden."
2024,DHd2024,BOENIG_Matthias_Edierst_Du_noch_oder_trainierst_Du_schon__Fo.xml,Edierst Du noch oder trainierst Du schon? Forschungsdaten als Grundlage von Trainingsdaten für die automatische Texterkennung,"Matthias Boenig (Berlin-Brandenburgische Akademie der Wissenschaften, Deutschland); Konstantin Baierer (Staatsbibliothek zu Berlin 'Preußischer Kulturbesitz, Deutschland); Lena Hinrichsen (Herzog August Bibliothek Wolfenbüttel, Deutschland); Kay-Michael Würzner (Sächsische Landesbibliothek ‚Äî Staats- und Universitätsbibliothek Dresden (SLUB), Deutschland); Christian Reul (Zentrum für Philologie und Digitalität der Universität Würzburg, Deutschland)","Standard, Ground-Truth, OCR","Datenerkennung, Transkription, Inhaltsanalyse, Strukturanalyse, Annotieren, Archivierung","Wichtigste Grundlage der textorientierten Forschung in den Digital Humanities ist eine ausreichende Verfügbarkeit von hochwertigem maschinenlesbarem Text. Diese Anforderung kann bei grundständig digitalen Texten häufig einfacher erfüllt werden als bei historischen Texten, wo zunächst die Transformation vom gedruckten oder geschriebenen Wort auf Papier in eine geeignete digitale Repräsentation zu realisieren ist. Mit der Anwendung von Verfahren des maschinellen Lernens in der automatischen Texterkennung ist in den letzten zehn Jahren ein enormer Fortschritt vollzogen worden. Dies betrifft vor allem die Zeichenerkennung und deren Genauigkeit. Hierbei kommen Methoden zum Einsatz, die dem Paradigma Aber GT dient nicht nur dem Training der Zeichenerkennung (sowohl dem Training eines neuen Modells ""from scratch"", als auch dem ""Finetuning"" eines bestehenden Modells auf einen spezifischen Anwendungsfall hin), sondern wird auch zur Datenvalidierung, -evaluation und -referenzierung eingesetzt. Neben der Zeichenerkennung können aber weitere Teilprozesse der automatischen Texterkennung vom Einsatz maschinellen Lernens profitieren. Dies gilt insbesondere für die Erkennung und Auszeichnung der Seitenstruktur bzw. des Seitenlayouts. Diese unterschiedlichen Anwendungen setzen differenzierte GT-Typen voraus. Allgemein kann zwischen Struktur-GT und Text-GT unterschieden werden. Die Erstellung von GT erfolgt zu einem Großteil manuell, was einen hohen zeitlichen und finanziellen Aufwand erfordert. Um brauchbaren GT zu erstellen, sind abgestimmte Konventionen und Richtlinien notwendig. Aus diesem Grund entwickelt, pflegt, vermittelt und diskutiert das Projekt OCR-D Im Rahmen des vorgeschlagenen Workshops soll eine solche offene Datenkultur am Beispiel von Forschungsdaten des Deutschen Textarchivs (DTA) Das DTA wurde im Rahmen eines sprachwissenschaftlich orientierten DFG-Projektes erstellt. Der Kernbestand besteht aus 1500 Druckpublikationen mit einem Gesamtumfang von 540.000 Seiten. Die Text- und Textsortenauswahl, die zeitliche Spanne des Publikationszeitraumes vom frühen 17. bis frühen 20. Jahrhundert, die Verwendung von Erstausgaben und die vorlagengetreue Transkription kennzeichnen diesen Bestand als Grundlage eines Referenzkorpus der frühneuhochdeutschen Sprache. Die Bereitstellung der digitalen Texte erfolgt sowohl in einem XML-basierten Format als auch als unannotierter Rohtext. Für die Einschätzung der Nutzbarkeit des DTA als Quelle für GT sind nicht nur die Ergebnisdaten relevant. Ein genauerer Blick auf die einzelnen Etappen des ursprünglichen Datenerfassungsworkflows im DTA zeigt bisher ungenutzte Potenziale der einzelnen Datenstände als Trainingsmaterialien für Text- und Strukturerkennung. Die folgende Abbildung illustriert die beiden grundsätzlichen Wege der Volltexterstellung, die im DTA zur Anwendung kamen: Automatische Texterkennung mit anschließender Nachkorrektur (""OCR way"") und manuelle Transkription im Vier-Augen-Prinzip (""Double Keying way"").  Letzterer kam für den Großteil des Bestands zur Anwendung. Das Double-Keying-Verfahren wurde von Nicht-Muttersprachlern vorgenommen und ist sehr genau. Die Zeichengenauigkeit kann mit 99,99 % angesetzt werden (Haaf, 2013; Geyken, 2012). Mit OCR wurden hauptsächlich Titel des 19. und Mitte des 18. Jahrhunderts erfasst. Beiden Wegen gemein ist ein manueller Segmentierungsschritt. In diesem wurden Textzonen und Abbildungen lokalisiert und klassifiziert. Diese Segmentierung diente zwar ""nur"" der nachträglichen Auszeichnung der Volltexte im XML (und nicht etwa der Unterstützung der automatischen Texterfassung). Sie bilden aber dennoch eine der größten bekannten Sammlungen an Strukturdaten für historische deutschsprachige Drucke. Aus der Untersuchung des Datenerfassungsworkflows können somit Segmentierungsdaten und Textdaten identifiziert werden, die für die Verwendung als GT in Frage kämen. Größtes Manko der Datensammlung ist jedoch die fehlende Verknüpfung zwischen Text und Bild, die die Einsatzszenarien als Trainingsdaten massiv einschränkt. An dieser Stelle setzt der vorgeschlagene Workshop an. Die Teilnehmenden des Workshops werden mit Verfahren und Methoden der Erstellung, Erschließung und Speicherung von GT für die automatische Texterkennung vertraut gemacht. Der Workshop ist in zwei Teile geteilt: einen theoretischen und einen praktischen. Ziel des theoretischen Teils ist, dass die Teilnehmenden in die Lage versetzt werden, anhand einer Liste von Kriterien sowie einer Validierung der Daten, Forschungsdaten für die Erstellung von GT einzuschätzen. Mit den OCR-D-GT-Richtlinien bekommen die Teilnehmenden eine in der Praxis erprobte Anleitung für die Erstellung von GT zur Verfügung gestellt. Inhalt und Aufbau, aber auch die Möglichkeiten der praktischen Anwendung dieser Richtlinien im jeweiligen Projekt bilden in diesem ersten Workshopteil den Schwerpunkt. Im praktischen Teil sollen nun die Teilnehmenden in verschiedenen Szenarien GT-Daten erstellen. Dabei werden Forschungsdaten des DTA und vorhandener GT geprüft und eingeschätzt. Dazu werden die im theoretischen Teil vorgestellten Metriken und Validierungsmethoden angewendet. Mit Transformations- und Konvertierungsprogrammen kann in der Folge nun der GT automatisiert erstellt werden. Ebenfalls können spezielle Softwareprogramme für die manuelle Erstellung von GT verwendet werden. Um sich sowohl mit dem Funktionsumfang als auch mit der Leistungsfähigkeit der Tools vertraut zu machen, ist es notwendig, diese im theoretischen Teil kennenzulernen. Der unmittelbare Umgang und die Handhabung des Tools für die GT-Erstellung stehen nicht im Mittelpunkt, sondern die Entscheidung, welches Tool für das jeweilige Vorhaben am geeignetsten scheint. Zum Abschluss steht die Speicherung des GT in einem Repositorium. So können die Daten entsprechend der FAIR-Prinzipien zugänglich gemacht werden. Erklärungen zum Aufbau des Repositoriums sowie die Erschließung mit Metadaten, die Nutzung des OCR-D-GT-Repo-Template Den Teilnehmenden des Workshops sollen verschiedene Methoden und Verfahren der GT-Erstellung vorgestellt werden. die jeweiligen Teilnehmenden verfügen über: Der Raum verfügt über:   "
2024,DHd2024,JACKE_Janina_Agreement_und_Kookkurrenz_bei_unzuverl_ssigem_E.xml,"Agreement und Kookkurrenz bei unzuverlässigem Erzählen. Ziele, Herausforderungen und erste Ergebnisse aus dem Projekt CAUTION","André Blessing (Universität Stuttgart, Deutschland); Janina Jacke (Georg-August-Universität Göttingen, Deutschland); Jonas Kuhn (Universität Stuttgart, Deutschland)","Annotation, Inter-Annotator Agreement, Literaturwissenschaft","Beziehungsanalyse, Modellierung, Annotieren, Theoretisierung, Literatur","Ziel des Projekts CAUTION (""Computer-aided Analysis of Unreliability and Truth in Fiction 'Interconnecting and Operationalizing Narratology"") ist die computergestützte Auseinandersetzung mit dem erzähltheoretischen Konzept des unzuverlässigen Erzählens. Unzuverlässiges Erzählen liegt dann vor, wenn die fiktive Erzählinstanz eines fiktionalen Textes unwahre Öußerungen über die fiktive Welt des Textes tätigt (vgl. Shen 2011). Ziel des Projekts ist es, in einem experimentellen Mixed-Methods-Setting Erkenntnisziele auf unterschiedlichen Ebenen zu erreichen. Zum einen soll geprüft werden, ob eine computationelle Modellierung des Konzepts (oder zumindest eine literaturwissenschaftlich nützliche Annäherung, vgl. Jacke 2023) möglich ist. Zum anderen geht es um die Erforschung bestimmter theoretischer und methodologischer Fragen, die mit dem Konzept verbunden sind und deren Klärung eine potenzielle reflektierte computationelle Modellierung informieren kann: Wie interpretationsabhängig ist die Feststellung von unzuverlässigem Erzählen in literarischen Texten? Und welche Rolle spielen formale bzw. sprachliche Textmerkmale (textuelle Indikatoren) bei der Feststellung? Nach einer genaueren Vorstellung der Erkenntnisziele (Abschnitt 2) und der sich daraus ergebenden Projektkonzeption (Abschnitt 3) sollen erste Ergebnisse der Auswertung im Projekt erstellter Annotationen 'mit besonderem Fokus auf Inter-Annotator Agreement und Annotationskookkurrenz 'präsentiert und diskutiert werden (Abschnitt 4). Abschließend wird ein vorläufiges Fazit zu den im Projekt zutage tretenden Herausforderungen bezüglich adäquater Standards und Methoden bei der Evaluation statistischer Ergebnisse in der computationellen Literaturwissenschaft gezogen (Abschnitt 5). Das Konzept des unzuverlässigen Erzählens stellt eine besondere Herausforderung für die computationelle Modellierung dar, denn es wird in der Literaturwissenschaft gemeinhin als stark interpretationsabhängiges Konzept verstanden (vgl. Yacobi 1981; Kindt 2008: 53-67). Gleichzeitig werden aber auch Listen von (teilweise sprachlichen) Indikatoren zusammengestellt, die auf unzuverlässiges Erzählen hindeuten können (vgl. Nünning 1998). Dabei lassen sich in der Regel aber keine genauere Angaben darüber finden, wie relevant oder verlässlich die einzelnen Indikatoren sind. Eine weitere Herausforderung besteht darin, dass das Konzept meist als Kategorie zur Einordnung von Texten oder Erzählfiguren genutzt wird, zugleich aber sowohl eine Verwendung als Analysekategorie zur Einordnung von Textsegmenten als auch eine graduelle Anwendung des Konzepts naheliegen (vgl. Jacke 2020: 173-183). Vor diesem Hintergrund stehen im hier vorgestellten Projektteil von CAUTION folgende Fragen und Hypothesen im Zentrum: Um im Hinblick auf die im vorangegangenen Abschnitt genannten Fragen zumindest erste Einblicke zu erzielen, wird eine Variante der Methode der kollaborativen Annotation (vgl. Gius und Jacke 2017) eingesetzt. Das Kernkorpus für diesen Projektteil setzt sich aus neun Im Rahmen des annotationsbasierten Projektteils gibt es zwei Annotationsaufgaben: Die erste zielt auf die Feststellung ausgewählter Indikatoren, die andere auf die Identifikation von unzuverlässigem Erzählen. Jeder Korpustext wird von mindestens zwei Annotator:innen auf der Basis gemeinsamer Annotationsrichtlinien bearbeitet. Allgemeine Annotationsprobleme werden regelmäßig gemeinsam diskutiert und die Richtlinien werden entsprechend überarbeitet. Sobald jeder Text in einer ersten Runde bearbeitet worden ist, erfolgt eine Überarbeitungsrunde, die der finalen Version der Richtlinien folgt. Individuelle Annotationsentscheidungen werden nicht diskutiert oder von den Annotator:innen verglichen, aber es besteht die Möglichkeit, Gründe oder Unsicherheiten bei der Annotation als Kommentar im Text zu vermerken. Zum Verfassungszeitpunkt dieses Beitrags ist die erste Annotationsaufgabe abgeschlossen. Für die zweite Aufgabe liegen Ergebnisse für einige der Korpustexte vor.  Während sich das so errechnete Agreement auf diese Weise noch nicht uneingeschränkt mit üblicherweise in den Feldern der Computerlinguistik und der computationellen Literaturwissenschaft erzielten Werten vergleichen lässt, lassen sich zumindest projektintern interessante Beobachtungen treffen: So kann beispielsweise festgestellt werden, dass 'entgegen unserer ursprünglichen Hypothese 'unzuverlässiges Erzählen anscheinend mit einem höheren Agreement festgestellt wird als die sprachnahen Indikatoren (Abb 2). Mögliche Erklärungen könnten sein, dass Erzähler:inneneigeschaften wie Emotionalität sich schwieriger an genauen Textstellen festmachen lassen als eine inkorrekte Öußerung (also Unzuverlässigkeit). Außerdem ist anzunehmen, dass die Eigenschaften aus der ersten Annotationsaufgabe öfter in Abstufungen vorliegen und die Annotator:innen möglicherweise unterschiedliche intuitive Schwellenwerte haben, ab denen sie eine Annotation vornehmen. Auffällig ist weiterhin, dass 'innerhalb der ersten Annotationsaufgabe (Abb. 3) 'für die Eigenschaft Das hohe Agreement ließe sich dadurch erklären, dass das Bewusstsein einer Erzählinstanz, sich in einer kommunikativen Situation zu befinden, sich in den meisten Fällen tatsächlich an linguistischen Texteigenschaften festmachen lässt 'beispielsweise an direkter Adressat:innenansprache unter Verwendung der zweiten Person bei Personalpronomen und Verbformen. Diese Hypothese ließe sich zukünftig durch Anwendung geeigneter Computermodelle auf das Korpus überprüfen, die auf die automatische Feststellung potenziell relevanter sprachlicher Eigenschaften zielen.  Um die Indikationskraft der Erzähler:inneneigenschaften weiter zu prüfen, könnte zukünftig genauer untersucht werden, ob eine bestimmte Kombination von Indikatorkategorien besonders häufig mit dem Auftreten von Unzuverlässigkeit korreliert. Zudem sollen neben der hier in einem ersten Zugang skizzierten textstellenbasierten Analyse der Kookkurrenzen auch die Gesamttexte untersucht werden, um festzustellen, ob die Indikatorphänomene gehäuft in Texten mit vielen Vorkommnissen von Unzuverlässigkeit auftreten 'auch wenn die genauen Textstellen nicht notwendigerweise übereinstimmen. Die hier präsentierten ersten Auswertungen der im Projekt CAUTION durchgeführten Annotationen zeigen, dass sich besondere Herausforderungen aus dem doppelten Erkenntnisinteresse ergeben: Es sollen sowohl computerlinguistische Erkenntnisse hinsichtlich der computationellen Modellierbarkeit von unzuverlässigem Erzählen als auch literaturtheoretische Einsichten in die Interpretationsabhängigkeit des Konzepts und seine Relation zu bestimmten sprachnahen Indikatoren gewonnen werden. Obwohl die Erkenntnisinteressen in weiten Teilen Schnittflächen aufweisen, ergeben sich bei der genaueren Konzeption der Annotationsaufgaben sowie bei der Auswertung tendenziell Diskrepanzen u.a. im Zusammenhang mit Korpusgröße, Annotationstools, annotierten (oder nicht-annotierten bzw. nicht-segmentierten) Einheiten, der Wahl eines Agreement-Maßes und der Beurteilung der Agreementwerte. Dennoch zeichnen sich erste interessante Tendenzen ab. So ist aus literaturtheoretischer Perspektive bemerkenswert, dass bei der Feststellung des vermeintlich stark interpretationsabhängigen unzuverlässigen Erzählens tendenziell ein höheres Agreement erzielt werden konnte als bei der Identifikation vermeintlich sprachnaher Indikatoren. Darüber hinaus scheinen die Indikatoren insgesamt eine schwache positive Indikationskraft für Unzuverlässigkeit zu haben, wobei insbesondere Emotionalität und ein betont sicheres Auftreten der Erzählinstanz wichtige Rollen spielen könnten. Nächste Schritte könnten sich der Analyse von textbasierten (im Gegensatz zu textstellenbasierten) Kookkurrenzen zuwenden. Außerdem ist die genauere Analyse von besonders auffälligen Einzeltexten und Textstellen interessant 'beispielsweise dort, wo unzuverlässiges Erzählen ohne die untersuchten Indikatoren auftritt. Hierfür eignen sich besonders interaktive Visualisierungen, die Annotationen im Textverlauf abbilden und ein Close Reading von Text und Annotator:innenkommentaren zulassen (vgl. Münz-Manor und Marienberg-Milikowsky 2023). Weitere Auswertungen könnten sich der Frage zuwenden, ob die literaturwissenschaftlichen Einschätzungen zur Unzuverlässigkeit der Korpustexte mit der Häufigkeit von Unzuverlässigkeitsannotationen in den jeweiligen Texten im Einklang stehen und welche weiteren Faktoren zusätzlich zur Frequenz ggf. einzubeziehen wären. Im Zusammenhang mit der computationellen Modellierbarkeit ist interessant, dass sich die Unzuverlässigkeitsannotationen durch ihr höheres Agreement tendenziell eher für ein Trainieren von Modellen eignen als 'wie ursprünglich angenommen 'die annotierten Indikatoren. Potenziell schwierig ist bei diesem Szenario aber die Tatsache, dass die Unzuverlässigkeitsannotationen auf dem Verstehen der ausgedrückten Proposition und einer Vorstellung von der erzählten Welt basieren. Vor diesem Hintergrund erscheint ein paralleles Experimentieren mit Modellen zur automatischen Feststellung relevanter Indikatoren sinnvoll."
2025,DHd2025,TUMFART_Barbara_Die_Insel___eine_digitale_Zeitschriftenediti.xml,Die Insel 'eine digitale Zeitschriftenedition,"Barbara Tumfart (Österreichische Akademie der Wissenschaften, Österreich); Silvia Waltl (Österreichische Akademie der Wissenschaften, Österreich)","Zeitschrift, Edition, Jahrhundertwende, Literatur, Buchkunst, Belletristik, Normdaten","Transkription, Annotieren, Veröffentlichung, Visualisierung, Literatur, Text"," Das in der Abteilung ""Die Insel"" wurde als ""ästhetisch-belletristische Monatsschrift"" zwischen Oktober 1899 und September 1902 im Berliner Verlag Schuster & Löffler als Teil des Insel-Verlags, ab Jahrgang 3 (1901/02) im Leipziger Insel-Verlag herausgegeben und gilt als eine der wichtigsten Zeitschriften der Jahrhundertwende und der Literatur der Moderne. Unter wechselnder Redaktion (Otto Julius Bierbaum, Alfred Walter Heymel, Rudolf Alexander Schröder) publizierten in der ""Insel"" unter anderem namhafte Autoren wie Franz Blei, Richard Dehmel, Heinrich Mann, Rainer Maria Rilke, Felix Salten, Robert Walser und Frank Wedekind. In der ""Insel"" finden sich diverse belletristische Textsorten in Prosa, Lyrik und in dramatischer Form als Einzelveröffentlichungen oder in Fortsetzungen, darunter Novellen, Erzählungen, Skizzen, dramatische Werke, Gedichte, Aufsätze und Aphorismen, außerdem kunst- und kulturtheoretische und -historische Abhandlungen, Rezensionen und Kritiken. Die Zeitschrift beinhaltet darüber hinaus zahlreiche Übersetzungen. Besondere Bedeutung kommt der Buchkunst in Form von Illustrationen, Drucken, Holzschnitten und dergleichen zu. Aus der Zeitschrift mit dem Signet des Segelschiffs ging später der ""Insel-Verlag"" hervor. Die Herausgeber der Insel sahen sich vor dem Hintergrund der Jahrhundertwende einem neuen Kunstverständnis verpflichtet. Die formenreiche Ausgestaltung des Buchschmucks sowie typographische Besonderheiten zeichnen die ""Insel"" zusätzlich aus und stellen zugleich eine besondere Herausforderung bei der Erstellung der Edition dar; mit Jahrgang 2 (1900/01) wechselt die Drucktype zudem von Fraktur zu Antiqua. Als Grundlage für die Datengenerierung zur Editionserstellung dienten die digitalen Bestände des Schwerpunkts zu Zeitungen und Zeitschriften im Austrian Academy Corpus (AAC). Diese Digitalisate basieren auf der Produktion hochauflösender Scans mit Zeutschel¬Æ-Buchscannern zur Herstellung von Faksimiles im unkomprimierten TIFF-Format, optimiert zur OCR-Erkennung. Neben den Schwarz-Weiß-Scans wurden auch Aufnahmen der Umschläge in Vollfarbe angefertigt. Zur Volltext-Gewinnung aus den Bilddateien wurde Texterkennung mit ABBYY¬Æ OCR (Optical Character Recognition) Software in den Versionen FineReader 7.0 Corporate Edition für Antiqua-Druck und FineReader 7.0 Scripting Edition für Fraktur durchgeführt mit anschließender manueller Nachkorrektur. Die OCR-Textdateien wurden schließlich mit Makros in Unicode-basierte XML-1.0-Dateien transformiert und anschließend gemäß des damals gültigen ""AAC Markup"" -Standards und einem page-per-page-Prinzip als Einzeldateien annotiert. Der weitere Workflow beinhaltete zunächst die Transformation der einzelnen XML-Dateien in TEI (Text Encoding Initiative) P5-konforme Formate und die Zusammenführung der Einzeldateien in eine Gesamtdatei pro ""Insel""-Heft. Insgesamt lagen am Ende dieses Konvertierungsprozesses 46 Dateien inklusive separater Inhaltsverzeichnisse pro Quartalsband vor. Für die Entwicklung und Bereitstellung des Transformations-Tools XMLJoiner war Andreas Basch (ACDH-CH) verantwortlich. Das Tool war zuvor schon an der Erstellung der digitalen ""Schaubühne""-Edition erprobt worden ( Mit der Umwandlung der ""alten"" Annotationen in valide TEI P5-Elemente, -Attribute und Das Projektziel besteht in der Erstellung einer digitalen als Volltext durchsuchbaren Zeitschriftenedition mit Faksimile- und Transkriptionsansicht als Grundlage für weiterführende Forschungsfragen, textwissenschaftliche Analysen und diverse Nachnutzungs-szenarien. Neben der Integration von Bild und Text liegt ein weiterer Fokus auf der Erstellung eines Personenverzeichnisses, das auf Autor:innen, Übersetzer:innen und Illustrator:innen der ""Insel"" verweist. In diesem Zusammenhang ist eine Verlinkung von Personennamen mit der Normdatei ""PMB 'Personen der Moderne Basis"" ( Die Gleichzeitig soll eine Verknüpfung dieser Entitäten mit Normdatensätzen wie der GND oder WikiData erfolgen. Durch die automatische Zuweisung unikaler IDs in der PMB und die Rückverlinkung der Datensätze in die annotierten XML-Dateien soll ein umfassendes externes Register erstellt werden, das die digitale Edition der ""Insel"" begleitet. Das Poster soll einen Überblick über den geplanten Aufbau der Edition bieten, sowie die bislang erfolgten Schritte der Datengenerierung und den Workflow illustrieren, der beispielhaft für die Erstellung weiterer Zeitschriften-Editionsprojekte innerhalb des ACDH-CH sein könnte. Zusätzlich sollen diverse Herausforderungen im editorischen Prozess vor dem Hintergrund der besonderen druckgraphischen Ausgestaltung der Zeitschrift thematisiert und Fragen des Umgangs mit Normdaten und ihrer Integration in Editionsprojekte erörtert werden."
2025,DHd2025,SPIELBERG_Marina_Wie_Shakespeare__bersetzungen_digital_edier.xml,Wie Shakespeare-Übersetzungen digital edieren?,"Marina Spielberg (Trier Center for Digital Humanities, Universität Trier, Deutschland); Claudia Bamberg (Trier Center for Digital Humanities, Universität Trier, Deutschland)","digitale Edition, Übersetzung, Shakespeare, Datenmodell","Transkription, Modellierung, Annotieren, Literatur, Manuskript","Auf dem Gebiet der digitalen Edition von literarischen Übersetzungen sind bislang noch sehr wenige Ansätze einer spezifischen Konzeptionalisierung vorhanden. Gab es schon für Druckeditionen kaum eine Theoriebildung zu dieser Editionsform (Plachta und Woesler 2002) oder Beispiele für eine adäquate Umsetzung Das Poster möchte das geplante Projekt einer digitalen Edition der Shakespeare-Übersetzungen August Wilhelm Schlegels und des Tieck-Kreises 'unter dem Namen ""Schlegel-Tieck"" bekannt geworden 'vorstellen. Dieses wird derzeit u.a. am Trier Center for Digital Humanities und an der Shakespeare Forschungsbibliothek der LMU München vorbereitet und konzipiert. Ziel ist es, am Beispiel dieser romantischen Übertragungen, die von 1798 bis 1833 erstmals erschienen sind, auch ein nachhaltiges Konzept für die digitale Edition von Übersetzungen zu entwickeln. Die Text- und Publikationsgeschichte des ""Schlegel-Tieck"", der neben dem Werk Goethes und Schillers lange als ""dritter deutscher Klassiker"" galt und die deutsche Shakespeare-Rezeption bis weit ins 20. Jahrhundert maßgeblich geprägt hat (Jansohn 2023), ist überaus komplex. Darin mag ein Grund liegen, dass es bis heute weder eine historisch-kritische Edition noch eine Studienausgabe gibt. Bei den heutigen Drucken, die auf die Übersetzungen August Wilhelm Schlegels und des Tieck-Kreises zurückgehen, wie sie etwa noch bei Reclam zu finden sind (Jansohn 2014), handelt es sich in der Regel um nicht weiter ausgewiesene Kompilationen verschiedener Übersetzer:innen aus verschiedenen Epochen. Da in den letzten gut zehn Jahren zahlreiche Quellen zu Schlegel und Tieck erstmals erschlossen wurden (Strobel und Bamberg 2014–2021; Latifi 2018; Hölter 2014–2023, Knödler 2018), existiert inzwischen eine wichtige, ja unabdingbare Voraussetzung für eine erfolgreiche Durchführung des Projekts. Die geplante Übersetzungsedition möchte die Probleme der Autorschaftszuweisung des ""Schlegel-Tieck"" endgültig aufklären. Während August Wilhelm Schlegel die erste Ausgabe der Übersetzung von siebzehn Shakespeare-Dramen von 1797 bis 1809 allein unternahm, wurde der erste ""Schlegel-Tieck"" erst von 1825 bis 1833 (Shakespeare 1825–1833) gedruckt. Der Berliner Verleger Georg Andreas Reimer hatte die beiden ""berühmten"" romantischen Namen auf den Titel gesetzt, um sich gegen die inzwischen zahlreiche Konkurrenz der deutschen Shakespeare-Übersetzer auf dem Buchmarkt durchzusetzen. An dieser Ausgabe hatte Schlegel jedoch so gut wie keinen aktiven Anteil mehr, und auch Tieck fungierte nicht als Übersetzer. Vielmehr ließ er seine Tochter Dorothea Tieck und den Schriftsteller Wolf von Baudissin die von Schlegel noch nicht übersetzten Dramen übertragen 'Ludwig Tieck selbst brachte sich nur als Berater ein und ""korrigierte"" zudem Schlegels Übersetzungen, die in der Ausgabe wiederabgedruckt wurden. Als Schlegel davon erfuhr, war er darüber sehr empört und forderte von Reimer, die Önderungen in den folgenden Auflagen wieder zurückzunehmen (August Wilhelm Schlegel an Georg Andreas Reimer 1825). In späteren Nachdrucken zu Lebzeiten der Übersetzer:innen wurde sodann immer wieder in die Texte eingegriffen. Bei der Konzeption einer digitalen Edition der romantischen Shakespeare-Übersetzungen geht es zunächst darum zu erarbeiten, wie mit der Fülle an Texten 'Ausgangstexten, Übersetzungen, Revisionen, mit Manuskripten und mit Drucken –, aber auch mit den Co-Autorschaften umgegangen werden kann. Sodann stellt sich die Frage, wie eine digitale Kommentierung aussehen könnte: Wie etwa lassen sich Kommentare zum interlingualen Transfer und zu den jeweiligen Übersetzungsprinzipien modellieren und sodann in einem entsprechenden UX-Design darstellen und für die Nutzer:innen abrufen? Und wie lässt sich eine konsistente Datenmodellierung für die verschiedenen Übersetzungsfassungen entwickeln 'wie muss diese im Hinblick auf die Textsorte Übersetzung gegenüber konventionellen Werkausgaben modifiziert werden (vgl. ausführlich zum Datenmodell und einem darauf aufbauenden möglichen UX-Design für die romantischen Shakespeare-Übersetzungen Schlegels und des Tieck-Kreises: Bamberg und Burch 2023: 313-323)? Die Transkription und Annotation der digitalen Faksimiles erfolgt mit dem Werkzeug Das Poster möchte ein erstes Datenmodell für die digitale Edition der Shakespeare-Übersetzungen August Wilhelm Schlegels und des Tieck-Kreises präsentieren und vorstellen, welche Tools dabei zum Einsatz kommen. Da sich das Projekt in einer frühen Phase befindet, liegt der Fokus zunächst auf Vorüberlegungen und offenen Fragen. So soll das Poster die grundlegenden Ideen, theoretischen Ansätze und Herausforderungen des Vorhabens skizzieren, die die Datenmodellierung, Kommentierung und das UX-Design betreffen. Es möchte folglich auch schon beispielhaft demonstrieren, wie die Edition im Frontend aussehen und genutzt werde könnte. Das entworfene Konzept soll dabei auch modellbildend für künftige digitale Übersetzungseditionen sein."
2025,DHd2025,DUDAR_Julia_Exploring_Measures_of_Distinctiveness__An_Evalua.xml,Exploring Measures of Distinctiveness: An Evaluation Using Synthetic Texts,"Julia Dudar (Universität Trier, Deutschland); Christof Schöch (Universität Trier, Deutschland)","measures of disrinctiveness, evaluation, synthetic texts, French novels, literature","Programmierung, Inhaltsanalyse, Visualisierung, Literatur, Methoden, Forschungsergebnis","Comparing groups of texts with each other in order to investigate what is characteristic about each group is a fundamental approach used in many research contexts, and measures of distinctiveness (also known as keyness measures) support such research in a quantitative perspective. The research we report on here is a further step in our fundamental work on measures of distinctiveness used for comparison of groups of texts. In the research reported on here, we focus on evaluating measures of distinctiveness through an analysis based on synthetic texts. Recent work has shown the importance of both frequency and dispersion of words for keyness analysis (Gries, 2021). By conducting keyness analysis using synthetically created datasets and through inserting an artificial word with precisely-manipulated frequency and dispersion into the synthetic dataset, we aim to systematically uncover the characteristics of different measures. Our goal is to determine the sensitivity of each measure to variations in frequency and dispersion. Evaluating measures of distinctiveness is challenging due to the fact that generating a gold-standard annotation is not possible. Distinctiveness is not an inherent characteristic of a word but can only be detected in the context of the entire target corpus and in comparison to another corpus. To tackle this challenge, several studies have attempted to evaluate distinctiveness measures using various methods. Kilgarriff (2001) examined corpus similarity by reviewing the mathematical characteristics of various distinctiveness measures. Paquot & Bestgen (2009) compared three different measures in their ability to identify words distinctive of academic prose as opposed to fictional prose. Lijffijt et al. (2014) explored a broad array of measures, focusing on the statistical characteristics of these measures. Within the framework of our project ""Zeta and Company,"" we conducted an in-depth analysis of the qualitative characteristics of these measures (Schröter et al., 2021). To enhance accessibility and usability, we implemented nine measures of distinctiveness in the Python package Our research proposes a new method for evaluating measures of distinctiveness, utilizing synthetically created text collections that reflect word frequencies as they occur in a real corpus, but within an artificially homogeneous corpus design. Studies based on naturally-occurring language must work around the fact that frequency and dispersion of any word will both vary and correlate to some extent. Our approach allows for precise, independent manipulation of word frequency and dispersion by inserting an artificial word. Our method enables us to uncover new advantages and limitations of distinctiveness measures and to compare their sensitivity to frequency and dispersion variations under consistent conditions. Our research is conducted on a synthetic text collection generated through random sampling from a corpus of French contemporary novels. The foundation for this corpus is a balanced subset from our larger collection of French contemporary popular novels and consists of 320 novels first published during the time period 1980 to 1999. This custom-built corpus maintains equal representation, per decade, across four subgroups: highbrow novels and lowbrow novels with three subgenres (sentimental novels, crime fiction, and science fiction). The original text corpus comprises approximately 19 million words. We load the entire corpus as a single dataset and randomly sample synthetic ""novels,"" each with a consistent length of 40,000 words. Our newly-generated corpus contains 320 synthetic ""novels,"" matching the number of novels in the original corpus. This approach addresses two main objectives. Firstly, it ensures that the generated corpus reflects real-life word occurrences and frequencies. Secondly, it results in a homogeneous corpus, eliminating subgenre differences, since each text is sampled from the entire corpus. To conduct our evaluation we used nine measures of distinctiveness implemented in our Python package The original French corpus was annotated with spaCy to create the input required by Our experiment had two primary settings to assess the impact of frequency and dispersion on distinctiveness scores. In the first setting, an artificial word was added to one segment of both the target and comparison corpus. This setting enables us to analyze the influence of only one parameter, namely the frequency. To maintain a constant total word count while adding an artificial word, other words in the corpus that occupied the same position as the artificial word were replaced. The frequency of the artificial word was constant (10 words) in the comparison corpus, but varied in the target corpus (10 to 2000 words). In the second setting, the artificial word's frequency was fixed at 1000 occurrences, but its dispersion varied. The idea was again to isolate one parameter, in this case dispersion, and analyze its influence on the performance of measures. The dispersion experiment involved different numbers of segments receiving the artificial word with specified frequencies, leading to 20 parameter settings. In the comparison corpus, scenarios included 1 segment with 1000 words and 1000 segments with 1 word. For each of these parameters, we conducted distinctiveness analyses with variations in the target corpus (the first number refers to the number of segments that receive the artificial word, and the second to how many times the artificial word is included in each of the selected segments): 1/1000, 2/500, 5/200, 10/100, 20/50, 50/20, 100/10, 200/5, 500/2, 1000/1. The results were compiled into a single dataframe, with all words in the corpus sorted and ranked by their distinctiveness scores. Each measure's performance was evaluated based on the rank of the artificial word (where a rank of 1 indicates the highest distinctiveness score). Since our corpus is based on naturally occurring word frequencies, we conducted an additional analysis to evaluate the potential artifacts caused by random sampling effects in the synthetic texts without the artificial word. This analysis aimed to identify the frequency differences of words in the corpus across multiple runs. Figure 1 illustrates the relationship between rank and the Ratio of Relative Frequencies (RRF) scores, based on 100 runs of randomly sampled synthetic corpora. As shown, the first rank is typically achieved with scores ranging from 10 to 18. This suggests that, due to the natural variations in the frequencies of existing words, an RRF score below 10 for the artificial word is unlikely to secure the first rank. In describing the results, our main focus lies on unexpected interesting observations, rather than on a description of all findings. Generally speaking, our expectations formulated in Hypotheses 1-4 are confirmed: Frequency-based measures are sensitive to differences in frequency but not to dispersion, and dispersion-based measures are sensitive to differences in dispersion, but not in frequency. However, this result comes with many nuances. Analyzing the frequency-based measures such as the chi-square test, LLR, and RRF, we observe a tendency for the score to increase with increasing frequency, but there are notable differences among the various measures (Fig. 2). When the artificial word reaches a frequency of 200 or higher in the target corpus, its RRF rank is consistently 1, indicating that it achieves the highest score among all words in the corpus. This observation aligns with our earlier analysis conducted without the artificial word (Fig. 1). Notably, RRF scores of 10 or below (corresponding to a frequency of 100 words in the target corpus) fail to achieve the first rank. This also explains the wide distribution of ranks observed for RRF scores based on a frequency of 100 words in the target corpus. Regarding the LLR and chi-squared tests, both measures are even more sensitive to frequency variation compared to RRF. At a frequency of 40 and higher, we observe the artificial word achieving the first rank. TF-IDF shows moderate sensitivity to frequency variation, partially supporting Hypothesis 5. Regarding the performance of dispersion-based measures, such as both variants of Zeta and the rank-sum test, when the artificial word is inserted into only one segment of the comparison corpus and the number of segments containing the artificial word in the target corpus increases, the word moves up in the ranking. Specifically, starting with 10 words in 100 segments, the artificial word almost consistently receives the first rank according to these three measures (Fig. 3). Eta shows interesting results in these settings. As a dispersion-based measure, we expected Eta to effectively identify an artificial word as distinctive, especially when the word is evenly spread across a high number of segments. However, as the number of segments containing the artificial word increases, its scores remain consistently low compared to randomly assigned words. Only in a scenario with one occurrence in 1000 segments does the artificial word receive the first rank (Fig. 3). This indicates that Hypothesis 4 is supported solely by both variants of Zeta and the rank-sum test. Unexpected results are also observed with TF-IDF. Similarly to the results of dispersion-based measures, as the number of segments increases, the rank of the artificial word moves up. However, in contrast to the moderate movement with respect to rank seen with dispersion-based measures, in the scenario with 1000 words in one segment of the comparison corpus, the artificial word achieves the first rank starting with a dispersion of just 100 words in 10 segments (Fig. 3). This result partially contradicts our expectation in Hypothesis 5 regarding the moderate sensitivity of TF-IDF to variations in dispersion. Conducting analyses based on synthetic texts, we created ideal conditions to uncover the hidden properties of a range of distinctiveness measures. Through our experiment, we tested the sensitivity of these measures to variations in the frequency and dispersion of a specific word. We found that LLR and chi-square tests are even more sensitive to frequency variation than RRF, which is simple and relies only on word frequency. Both Zeta variations and the rank-sum test demonstrated similar scores and abilities to detect distinctive words. Moreover, we discovered that TF-IDF is more sensitive to slight dispersion differences of the target word compared to other dispersion-based measures. Finally, we found that Eta does not detect a word with a clear contrast in dispersion when its frequency is the same in both the target and comparison corpora. Despite the interesting observations derived from these analyses, there is significant potential for future work. One key step is to extend our framework by implementing new measures of distinctiveness, particularly those that rely purely on dispersion rather than doing so only primarily, in a combination of frequency and dispersion. Another crucial step is to explore practical applications of this newfound knowledge about distinctiveness measures. Understanding the specific contexts and scenarios where these measures can be effectively utilized will open up new possibilities and enhance our ability to analyze and compare textual corpora more predictably and more accurately. Additionally, this approach can easily be applied to corpora in languages other than French. While we assume that the method will work similarly with other languages, we encourage other researchers to test our method on further corpora to validate the robustness of our results. Data and Code Repository:"
2025,DHd2025,FL_H_Marie_Das_Projekt_CompAnno__Comparative_Annotation_to_E.xml,Das Projekt CompAnno: Comparative Annotation to Explore and Explain Text Similarities,"Marie Flüh (Universität Hamburg, Deutschland); Julia Nantke (Universität Hamburg, Deutschland); Janis Pagel (Universität zu Köln, Deutschland); Nils Reiter (Universität zu Köln, Deutschland)","Computational Literary Studies, Intertextualität, Figurenanalyse","Beziehungsanalyse, Annotieren, Literatur","Das DFG-Projekt CompAnno entwickelt einen vergleichenden Annotationsworkflow zur computergestützten Detektion und Klassifizierung von literarischen Textähnlichkeiten am Beispiel von Figureneigenschaften als einer Kategorie, die für die Gestaltung literarischer Erzähltexte und für die Interpretation intertextueller Beziehungen zentral ist (Müller 1991:101). Der Workflow für eine computergestützte Untersuchung von Textähnlichkeit soll so gestaltet sein, dass er über die Erkennung von text-reuse hinausgeht und nicht auf ein festes Korpus bezogen ist. Gleichzeitig greifen wir mit der vergleichenden Annotation eine literaturwissenschaftliche Basismethode auf (Unsworth 2000, Epple et al. 2020:7) und entwickeln einen neuen Weg für die Arbeit mit interpretativen Kategorien. Wir arbeiten mit vier Annotatorinnen, wobei die Annotationsaufgaben zur Annotation von Figureneigenschaften ineinandergreifen. Die Annotatorinnen sind alle Germanistik- und Linguistikstudentinnen. Da Figuren in der Regel zu Beginn eines Textes eingeführt werden, ist hier mit besonders zahlreichen Eigenschaften zu rechnen. Deshalb werden in unterschiedlichen Annotationsphasen jeweils die Anfangspassagen (circa 20.000 Tokens) aus verschiedenen Prosatexten aus Um einen differenzierten und spezifischen Blick auf das Phänomen zu erlangen Figureneigenschaften werden erst manuell ermittelt und kategorisiert. Auf Grundlage der Guidelines ist jede Annotatorin für die Analyse einer ausgewählten Kategorie zuständig. In Diskussionsrunden werden die Annotationsdaten besprochen und angepasst. Darauf aufbauend wird ein Ranking der Öhnlichkeiten erstellt, das zum Trainieren oder Prompten eines maschinellen Lernsystems bzw. großen Sprachmodells verwendet wird. Ziel dieses Modells ist die automatische Erkennung von Stellen, an denen eine Figureneigenschaft vorkommt, sowie die Kategorie der Figureneigenschaft, so dass eine vergleichende Annotation Sinn ergibt. Im Gegensatz zu etablierten Annotationsansätzen beruht die vergleichende Annotation auf der gleichzeitigen Betrachtung mehrerer Textausschnitte: Auf Grundlage von Richtlinien für die vergleichende Annotation von Figureneigenschaften werden den Annotatorinnen jeweils zwei Textabschnitte vorgelegt, zu denen dann die ihnen enthaltenen Figureneigenschaften vergleichend zu annotieren sind. Benutzt wird hierzu die Webanwendung Im weiteren Projektverlauf sollen die vergleichenden Annotationen dazu benutzt werden, die zuvor genannten intertextuellen Beziehungen zu beleuchten. Bisher entstandene Teilergebnisse sind Annotationsrichtlinien, Einblicke in die Annotationspraxis, qualitative und quantitative Einblicke in die Darstellung und Verteilung von Figureneigenschaften sowie erste Automatisierungsversuche. Aus der induktiven Auswertung der explorativen Annotationsphase, in der unter Einbezug etablierter Figurenkonzepte (Forster 1949, Hansen 2000, Jannidis 2004) vor allem konzeptuelle Fragestellungen im Fokus standen, ergeben sich fünf bzw. sechs Oberkategorien, die häufig für die Beschreibung literarischer Figuren verwendet werden und deshalb als Analysekategorien für die automatisierungsorientierte und vergleichende Annotation in Frage kommen (s. Abb. 2). Bisher zeigt sich, dass Figuren vor allem über Rollen und Charaktereigenschaften näher beschrieben werden (s. Tabelle 1). Für jede Annotation legen die Annotatorinnen den Interpretationsaufwand auf einer Skala von sehr niedrig bis sehr hoch fest. Ausgehend von der Annahme, dass ein geringer Interpretationsgrad auf explizit im Text thematisierte Figureneigenschaften und ein hoher Interpretationsgrad auf implizite Figureneigenschaften hindeutet, fungiert er als Marker für den Grad der Explizitheit. Bei der Auswertung eines Teils der Annotationsdaten zeigt sich, dass der Interpretationsgrad meistens als gering eingeschätzt wird. Dieser Befund ist als individuelle Annotationsentscheidung aufzufassen. Außerdem lässt sich schlussfolgern, dass Eigenschaften vor allem explizit erwähnt werden (s. Abb. 3) und implizite Eigenschaften eher ein Randphänomen darstellen. Ein niedriger Interpretationsgrad findet sich vor allem in den Kategorien ""Kleidung"", ""Alter"" und ""Physiognomie"" und ein höherer in den Kategorien ""Rolle"" und ""Charakter"". Eigenschaften, für die ein niedriger Interpretationsgrad angegeben wurde (explizite Eigenschaften), sind in allen Texten häufig. Sie sind ""hochgradig intertextuell"" und relativ generisch (bspw. ""jung"", ""schön"" oder ""groß""). Eigenschaften, die mit einem hohen Interpretationsgrad ausgezeichnet wurden, kommen in einzelnen Texten und in geringerer Anzahl vor. Implizite Eigenschaften scheinen individuell zu sein, können aber gerade deshalb eine spezifischere Verbindung zwischen zwei Texten markieren als die explizit-generischen Eigenschaften. Erste Pilotexperimente zur automatischen Klassifizierung von Figureneigenschaften zeigen, dass zwar schon moderat gute Ergebnisse mit relativ simplen Methoden zu erreichen sind, aber die Performanz noch ausbaufähig ist. Tabelle 2 zeigt die Durchschnittswerte für Precision, Recall und F1-Score für die Klassifikation von drei Modellen Die Ergebnisse zeigen, dass die Klassifikation von Charaktereigenschaft, Physiognomie und Rolle mit ca. 60 % F1-Score im ersten Anlauf akzeptabel funktioniert, die Modelle jedoch Probleme haben, ""Alter"" und ""Kleidung"" richtig zu klassifizieren (16 bzw. 36 % F1-Score), außerdem ist die Precision für Rolle mit 6 % deutlich niedriger als für die anderen Kategorien. Tabelle 2: Klassifikationsergebnisse für das Erkennen von Figureneigenschaften Geplante Arbeitspakete betreffen vor allem die Automatisierung der vergleichenden Annotation und die Verbesserung der automatischen Erkennung der Kategorien. Eine weitere offene Frage ist der ideale Kotext, den es braucht, um automatisiert Entscheidungen bezüglich der Figureneigenschaften zu treffen."
2025,DHd2025,LEMKE_Marc_Raumreferentielle_Ausdr_cke_in_deutschsprachigen_.xml,Raumreferentielle Ausdrücke in deutschsprachigen Romanen des 19. und 20. Jahrhundert. Ein Werkstattbericht des Projekts CANSpiN,"Marc Lemke (Universität Rostock, Deutschland); Nils Kellner (Universität Rostock, Deutschland); Ulrike Henny-Krahmer (Universität Rostock, Deutschland)","Computational Literary Studies, Raum, Annotation, Roman","Entdeckung, Räumliche Analyse, Annotieren, Text","Das Poster präsentiert erste Daten aus dem Projekt ""Computational Approaches to Narrative Space in 19th and 20th Century Novels"" (CANSpiN), die aus der Annotation raumreferentieller Ausdrücke mithilfe der Annotationsrichtlinie CANSpiN.CS1 (Category Set 1) in deutschsprachigen Romanen des 19. und 20. Jahrhunderts hervorgehen. Ziel des Posters ist es, Konzeption und Potentiale offenzulegen, die die mit der Richtlinie erzeugten Daten für quantitative und qualitative Auswertungen im literaturwissenschaftlichen Kontext bieten. Grundlage der Annotation ist ein epistemologisches Raumverständnis: Raum, wie er sich anhand punktueller raumreferentieller Ausdrücke in unserer Sprache und in den von uns untersuchten Romanen als kulturell verankertes Konzept darstellt. In dieser Perspektive ist Raum hierarchisch in Bezugnehmend auf diese Phänomene und in Anlehnung an die Systematiken von Raumindikatoren bei Mareike Schumacher (2023) und von raumreferentiellen Bezeichnungen bei Katrin Dennerlein (2009) haben wir fünf Kategorien raumreferentieller Ausdrücke gebildet, die in 21 Klassen ausdifferenziert sind (siehe Abbildung 1). Die Annotationsrichtlinie CANSpiN.CS1 zielt grundsätzlich darauf ab, über die raumreferentiellen Ausdrücke, deren Menge, Verteilung, Auswahl und Korrelation die Die damit erzeugten Annotationen können einerseits für rein quantitative Analysen im Sinne eines Distant Reading auf ganzen Textkorpora verwendet werden. Auf die Relevanz dieses Ansatzes für literaturhistorische Fragestellungen wurde im Allgemeinen bereits hingewiesen. (Moretti 2013, 48f, 53f; Underwood 2019, 3) Speziell anhand räumlichen Vokabulars sind entsprechende Untersuchungen bislang von Schumacher (2023) unternommen worden, deren Ansatz jedoch keine semantische Subklassifikation beinhaltet. Andererseits bieten die Annotationen auch das Potential, Ausgangspunkt für Mixed-Methods-Ansätze und textimmanente Interpretationen zu sein. Um dies nachzuvollziehen, werden im Folgenden exemplarisch die Annotationen des 1. Kapitels von Gustav Freytags ""Die verlorene Handschrift"" diskutiert. (Freytag 2021) Die Definitionen der hier besprochenen Annotationsklassen werden dafür in aller Kürze dargelegt. Für eine umfassende Einsicht in das Kategoriesystem verweisen wir auf die publizierte Annotationsrichtlinie. (Henny-Krahmer et al. 2024) Abbildung 2 zeigt die Annotationsmengen in allen Kategorien. Das Gros der Annotationen bilden Bewegungen und Orte. Unter ersterem verstehen wir Verben, die eine räumliche Distanz dadurch produzieren, dass sie eine gerichtete Bewegung von Subjekten und Objekten ausdrücken, aber auch Wahrnehmungen und die Produktion von Licht, Schall und Gerüchen. Die hohe Anzahl von BEWEGUNG-SUBJEKT- und BEWEGUNG-SCHALL-Annotationen ist in der Exposition von Freytags Text Ausdruck dessen, dass hier häufig Figurenbewegungen und Gespräche dargestellt sind. Die hohe Anzahl von BEWEGUNG-ALT-Fällen ist zu einem wesentlichen Teil Ergebnis der häufig bildlichen Sprache, mit der die Welt beschrieben wird: Nicht-räumliche Sachverhalte, die mittels eines räumlichen Vokabulars ausgedrückt werden, erfassen wir generell mit den ALT-Klassen (siehe Abbildung 1). Abbildung 3 lenkt den Fokus auf die Ortsannotationen, unter anderem auf Container: Bereiche und Räume, in denen sich Figuren prototypischerweise aufhalten könnten. Sind diese Container im Satzzusammenhang ein Ziel- oder Ausgangspunkt von Bewegungen, stehen sie in einem Bewegungskontext (ORT-CONTAINER-BK). Ein Aspekt, der bei der Betrachtung der Daten auffällt ist, dass Container hier viel häufiger erwähnt werden, als dass sie in einem Bewegungskontext stehen: Figuren-Bewegungen und Wahrnehmungen finden eher innerhalb von Containern und nicht zwischen ihnen statt. Mit Blick auf die Analyse ganzer Texte legen die dargestellten Beziehungen zwischen der Räumlichkeit eines Erzähltextes und seiner Diegese Möglichkeiten nahe, die Annotationen als Einstiegspunkt für textimmanente Interpretationen und Textvergleiche nutzen zu können. Vor dem Hintergrund eines raumtheoretisch fundierten Gattungsbegriffs des Reiseromans beispielsweise ist zu erproben, ob die Menge an ORT-CONTAINER-BK- im Verhältnis zu ORT-CONTAINER-Annotationen als ein gattungsspezifisches Muster funktioniert. (Vgl. Sicks 2009, 342f) Und in einer umfassenderen Perspektive bietet das Verhältnis von Räumlichkeit zum erzähltem Raum grundsätzlich das Potential, ein Kennzeichnen spezifischer Schreibweisen zu sein: Wird im Text viel oder wenig räumliches Vokabular für die Konstruktion der erzählten Welt verwendet? Ist der Text in seiner Sprache sozusagen ""räumlicher"" als die erzählte Welt oder nutzt er all sein räumliches Vokabular zur Darstellung der Diegese?"
2025,DHd2025,SLUYTER_G_THJE_Henny_QUADRIGA_Fallstudien_zur_Datenkompetenz.xml,"QUADRIGA-Fallstudien zur Datenkompetenz in den Humanities: Jupyter Books als ""scalable Open Educational Resources""","Henny Sluyter-Gäthje (Universität Potsdam, Deutschland); Daniil Skorinkin (Universität Potsdam, Deutschland); Peer Trilcke (Universität Potsdam, Deutschland); Maria Chlastak (Gesellschaft für Informatik, Deutschland); Evgenia Samoilova (Universität Potsdam, Deutschland); Melanie Seltmann (Humboldt-Universität zu Berlin, Deutschland)","Open Educational Resource, Jupyter Book, OER, Lehrmaterial, Fallstudie, Datenkompetenz","Veröffentlichung, Einführung, Lehre, Text","Datenkompetenz, die sowohl den kritischen Umgang mit Daten (von Sammeln über Verwalten bis hin zu Bewerten) als auch die Analyse und Interpretation von Daten (vgl.¬† Ridsdale et al., 2015) umfasst, wird auch für traditionell ausgebildete Geisteswissenschaftler:innen zunehmend wichtiger. Im Berlin-Brandenburgischen Datenkompetenzzentrum QUADRIGA Um den verschiedenen Dimensionen der Kompetenzvermittlung gerecht zu werden, die sich von basalen technischen Fähigkeiten über methodenspezifisches Wissen bis hin zur Daten- und Methodenkritik erstrecken, orientieren sich die von QUADRIGA entwickelten OER an Fallstudien (Foran, 2001) und setzen so problembasiertes Lernen (Kay et al., 2000) ein. Das heißt, QUADRIGA geht von einer konkreten Fragestellung aus Jede dieser Aufgaben ist in strukturell identische Komponenten unterteilt: Jupyter-Book Die Struktur des Books wird über ein Inhaltsverzeichnis hierarchisch festgelegt. Mit Hilfe dieser Struktur lassen sich die Aufgaben und ihre Komponenten gut abbilden. Wie auf der Landing-Page (Abb. 02) zu sehen, besteht die Website aus drei vertikalen Abschnitten. Dadurch kann die Fallstudie zum einen chronologisch durchlaufen werden, zum anderen können einzelne Aufgaben und Komponenten direkt angesteuert werden. So kann selbst ein Lernpfad im Sinne des aktiven Lernens (Markant, 2016) gewählt werden. Aufgaben, Komponenten und ihre Bestandteile können über URLs referenziert werden, was die Modularität des Books weiter erhöht. Die interaktiven Komponenten werden in Jupyter Notebooks erstellt, in denen sowohl Markdown als auch Python-Code geschrieben werden kann. So können kleine UI-Elemente erzeugt werden wie die Worteingabe in Abb. 03. Zusätzlich kann das gesamte Jupyter Notebook über Google Colab Markdown ermöglicht eine multimodale Gestaltung (durch Tabellen, Abbildungen, GIFs, Querverweise etc.), bibliographische Angaben werden über BibTeX erzeugt. Mittels der MyST (Markedly Structured Text) Markdown-Erweiterung lassen sich weitere UI-Elemente hinzufügen, z.B. Informationsboxen (Abb. 04). Diese Strukturierungsmöglichkeit macht Informationen leicht auffindbar (scannable) und Inhalte schnell überfliegbar (skimmable). Die Realisation als Jupyter Book ermöglicht es, eine Modularisierung der Fallstudie vorzunehmen und so atomisierte Einheiten zu spezifizierten Lernzielen aus dem Feld des Datenkomptenzerwerbs zu erstellen und referenzierbar zu machen. Darüber hinaus birgt das technische Konzept von Jupyter Books die Möglichkeit, etwas zu entwickeln, was wir ""scalable OER"" nennen. Die dargestellte Fallstudie kann nämlich auf verschiedenen ""levels of expertise"" durchgeführt werden: Die Modularisierbarkeit sowie die flexible Skalierbarkeit von Jupyter Books bieten das Potential, eine Vielzahl von Lernenden mit unterschiedlichem Vorwissen zu erreichen. Fördervermerk QUADRIGA wird im Rahmen der Richtlinie Förderung von Projekten zum Aufbau von Datenkompetenzzentren in der Wissenschaft des Bundesministeriums für Bildung und Forschung unter Kennzeichen 16DKZ2034A, 16DKZ2034G und 16DKZ2034H gefördert."
2025,DHd2025,NANTKE_Julia_Projekt_MuMokA.xml,Projekt MuMokA - Multimodale Modellierung kultureller Artefakte im digitalen Raum,"Julia Nantke (Universität Hamburg, Deutschland); Frank Steinicke (Universität Hamburg, Deutschland); Vanessa Klomfaß (Universität Hamburg, Deutschland); Qianqi Huang (Universität Hamburg, Deutschland)","Multimodalität, born digital, Datenrestaurierung, Modellierung","Inhaltsanalyse, Modellierung, Annotieren, Visualisierung, Literatur, Multimodale Kommunikation","Den Ausgangspunkt für das Projekt MuMokA ( Die Multimodalität, die konzeptionelle Unvollständigkeit und der fragmentarische Charakter der  Das Ziel des Projekts ist die Entwicklung von prototypischen Szenarien zur teilautomatisierten Exploration und Strukturierung sowie zur digitalen Repräsentation multimodaler born-digital-Korpora. Die Exploration und Modellierung der Strukturen und Inhalte des Korpus erfolgt anhand zweier komplementärer Zugriffe, die tendenziell einem Close und einem Distant Reading-Ansatz zuzuordnen sind. 1. Manuelle Modellierung der Strukturen und Inhalte des Korpus in einer Tabelle mit vier hierarchisch angeordneten und untereinander dynamisch verknüpften Ebenen, welche die überlieferte Ordnerstruktur mit der von Kempowski angelegten Ordnung verbinden und jedes Dokument in seiner Position im Korpus verortbar machen (siehe Abbildung 1). 2.1 Nutzung von gKI zur Datenextraktion und -kategorisierung, um die Zuordnung von Dokumenten zu den verschiedenen Ebenen der Tabelle soweit wie möglich zu automatisieren (vgl. Abb. 2). Wir folgen dem Prompting-Ansatz von Marvin et al. 2024 und nutzen aktuell die von der Universität Hamburg lizensierte Version UHHgpt 4omni (vgl. 2.2 Einsatz von maschinellen Lernverfahren zur Korpusanalyse mittels u.a. Named Entity Recognition, Topic Modeling und Multidimensional Scaling (vgl. Abb. 3).  Während wir bereits Konzepte zur Wiederherstellung und Strukturierung der Daten entwickelt haben, liegt unser aktueller Fokus auf der Exploration und Repräsentation der Inhalte des Korpus mittels BERTopics (Grootendorst 2022, vgl. Abb. 2). Hierbei planen wir ebenfalls, künftig verstärkt die anderen Modalitäten wie Bilder und Tondateien einzubeziehen. Darauf aufbauend werden wir an Konzepten arbeiten, um das Korpus für andere Wissenschaftler:innen zugänglich zu machen und das fragmentierte Kunstwerk"
2025,DHd2025,ACHMANN_Michael_Aspektbasierte_Sentimentanalyse_von_Bookstag.xml,Aspektbasierte Sentimentanalyse von Bookstagram-Posts,"Emma Sophie Reichert (Universität Regensburg, Deutschland); Anna-Lena Babl (Universität Regensburg, Deutschland); Kyuhee Kim (Universität Regensburg, Deutschland); Michael Achmann-Denkler (Universität Regensburg, Deutschland); Christian Wolff (Universität Regensburg, Deutschland)","Aspektbasierte Sentimentanalyse, Social Media Analyse, Bookstagram, GPT, LLM","Inhaltsanalyse, Multimodale Kommunikation, Text","Der Buchmarkt befindet sich in einem Wandel, der unter anderem durch die zunehmende Präsenz von Büchern auf Online-Plattformen geprägt ist. Sogenannte ""Buchblogger"" verbreiten auf Social Media Inhalte über Bücher (Giacomuzzi 2021) und werden dabei nicht selten von Verlagen durch besondere Aktionen oder kostenlose Exemplare, sogenannte Rezensionsexemplare, unterstützt. Aufkleber wie ""#TikTokMadeMeBuyIt"" auf Büchern in Buchgeschäften (Sahner 2023) und Bestseller-Listen auf TikTok, verdeutlichen die wachsende Rolle sozialer Medien bei Kaufentscheidungen: Laut Angaben des Plattformbetreibers wurden 2023 in Deutschland über 12 Millionen '#BookTok'-Bücher verkauft (TikTok Technology Limited 2024). Auch auf Instagram hat sich eine aktive Buch-Community etabliert, die durch ihre Reichweite und Interaktionen zum Erfolg von Büchern beiträgt. Diese Arbeit befasst sich mit der aspektbasierten Stimmungsanalyse von Buchrezensionen auf Instagram. Dabei wurden folgende Forschungsfragen untersucht: Die Ergebnisse dieser Analyse ergaben neue Einblicke in die Möglichkeiten der aspektbasierten Stimmungsanalyse und lieferten Aufschlüsse über die Buch-Community auf Instagram, das sogenannte Bookstagram. Untersuchungsgegenstand dieser Arbeit waren 3745 deutschsprachige Buchrezensionen der Bookstagram-Community (siehe Abbildung 1 für ein Beispiel). Diese wurden von 144 Accounts mit unterschiedlichen Reichweiten über die Suche nach Hashtags mithilfe des Analysetools CrowdTangle gesammelt. Alle Posts enthielten dabei das Wort ""Rezension"" oder die Kurzform ""Rezi(e)"" sowie mindestens einen der folgenden Hashtags: #bookstagramgermany, #bookstagramdeutschland, #buchrezension, #buch, #bookstagram, #bücherliebe, #leseliebe, #buchblogger, #buchcommunity. Die aspektbasierte Stimmungsanalyse liefert ein detaillierteres Verständnis der Stimmung, indem sie den Text in verschiedene Aspekte zerlegt und deren Stimmungen einzeln analysiert (Kim and Song 2022). Die betrachteten Aspekte dieser Arbeit wurden anhand vorheriger Forschungsarbeiten (Zhang et al. 2019, Stollfuß 2023) unter Einbeziehung weiterführender Überlegungen festgelegt. Zhang et al. zeigten, dass auf sozialen Medien inhaltsbezogene Aspekte öfter vorkommen als externe Faktoren (2019). Die externen Faktoren (z.B. Cover, Schriftart, Illustrationen) wurden daher zu einem Aspekt zusammengefasst, während inhaltliche Aspekte weiter aufgeteilt wurden. Für unsere Analyse ergaben sich folgende Aspekte: Autor, Schreibstil, externe Faktoren, Charaktere, Logik und Handlung/Spannung. Darüber hinaus wurde das allgemeine Sentiment jeder Rezension bestimmt. Das Sentiment wurde als ""Positiv"", ""Neutral"", ""Negativ"" oder ""Nicht vorhanden"" bewertet. Nach der iterativen Entwicklung eines Prompts wurde die aspektbasierte Stimmungsanalyse mit den GPT-Modellen ""gpt-3.5-turbo-0613"" und ""gpt-4-1106-preview"" durchgeführt und verglichen. Die Reliabilität der Ergebnisse wurde durch die Bewertung von 250 zufällig ausgewählten Beiträge des Datensatzes von sechs menschlichen Annotatoren sichergestellt. Die Ergebnisse lassen auf die Verlässlichkeit des methodischen Vorgehens für künftige Arbeiten schließen. Für sechs von sieben Aspekten ergaben sich mit GPT-3.5 gewichtete F1-Scores zwischen 77% und 94% und mit GPT-4 zwischen 79% und 100%. Die davon abweichenden gewichteten F1-Scores bei dem Aspekt Der Vergleich der GPT-Modelle ergab, dass die Erkennung von Rezensionsexemplaren mit GPT-3.5 (F1-Score = 0,965) zuverlässiger war, als mit GPT-4 (F1-Score = 0,905). Diese Erkennung war zentral für die Beantwortung unserer Forschungsfragen. Außerdem waren die von GPT-4 zurückgegebenen Daten teilweise unvollständig oder nicht richtig formatiert. Für die Untersuchung der weiteren Fragestellungen wurde deshalb das GPT-3.5 Modell gewählt. Bei der Betrachtung der Verteilung der Sentiments fällt auf, dass für jeden Aspekt, außer für Für die Kategorie Es wurde untersucht, ob es Unterschiede zwischen Foto-Posts (nur ein Bild) und Album-Posts (mehrere Bilder) bezüglich der Verteilung des Sentiments gibt. Dabei konnten in der Kategorie Anhand des von CrowdTangle bestimmten Overperforming Scores konnte mittels Spearman""s Korrelationskoeffizienten gezeigt werden, dass Beiträge umso besser performten, je negativer das allgemeine Sentiment war ( Die analysierten Bookstagram-Beiträge zeigen überwiegend positives Sentiment. Eine ähnliche Verteilung konnte bereits bei Posts über das Lesen auf Instagram (Zhan et al., 2018) und bei den Plattformen Goodreads und Amazon nachgewiesen werden (Dimitrov et al., 2015). Weiter konnten wir einen negativen Zusammenhang zwischen den betrachteten Sentiments und der Performance des Beitrags feststellen. Diese Ergebnisse stimmen mit den Erkenntnissen von Hsu et al. (2019) überein. Unsere Studienergebnisse legen nahe, dass die Veröffentlichung von Rezensionen auf Instagram, unterstützt durch das überwiegend positive Sentiment, potenziell förderlich für den Buchmarkt sein könnte. Sie geben einen Einblick in den Literaturdiskurs auf sozialen Plattformen und vertiefen das Verständnis für den Einfluss auf die Bookstagram-Community. Unsere Arbeit ist durch die begrenzten Erhebungsmöglichkeiten limitiert. Zukünftige Arbeiten könnten andere Plattformen, Buchgenres oder nicht-textuelle Bestandteile von Bookstagram-Beiträgen berücksichtigen."
2025,DHd2025,DENNERLEIN_Katrin_Zum_Aufbau_digitaler_Dramenkorpora__OCR4al.xml,Zum Aufbau digitaler Dramenkorpora. OCR4alltoDraCorTEI als Baustein für die Edition von maschinenlesbaren Versionen historischer Dramendrucke,"Katrin Dennerlein (Julius-Maximilians-Universität Würzburg, Deutschland); Martin Rupnig (Julius-Maximilians-Universität Würzburg, Deutschland); Christian Reul (Julius-Maximilians-Universität Würzburg, Deutschland)","digitale Edition, TEI, quantitative Dramenanalyse","Transkription, Programmierung, Annotieren, Bearbeitung, Software","Die Computational Literary Studies (CLS) können nur so gut sein, wie die Korpora, die ihnen zur Verfügung stehen. Insbesondere für die Geschichte des deutschsprachigen Dramas vom 17. bis 19. Jahrhundert repräsentieren diese bislang jedoch fast nur die hochkanonischen Texte. Die Präferenz liegt, wie bereits in den kodifizierten Literaturgeschichten der Germanistik auf original deutschsprachigen Sprechtheaterwerken und dabei vorwiegend auf Tragödien (Alt, 1994, Meid, 2009: 327-501, Schulz, 2007). Hingegen bleiben Libretti, populäre Komödien und generell Übersetzungen und Dramen von Frauen zumeist gänzlich unberücksichtigt, obwohl sie die Mehrheit der gedruckten und gespielten Werke ausmachen (Jahn, 1996, Krämer, 1998, Dennerlein, 2021, Kord, 1992). Dadurch ist die Geschichte des deutschsprachigen Dramas nicht nur äußerst lückenhaft, sondern entbehrt auch zahlreicher populärer und wegweisender Werke. Die Textauswahl für einzelne Autor:innen, Genres, Textgruppen wie Repertoires oder Sammlungen ist jeweils so klein, dass quantitative, genre- und periodenvergleichende Studien nur sehr eingeschränkt durchgeführt werden können. Für eine Erforschung der Gesetzmäßigkeiten der literarischen Evolution ist die gezielte Korpuserweiterung deshalb unabdingbar. Auf edierte und normalisierte Neuausgaben, wie sie der Digitalen Bibliothek und den auf sie aufsetzenden Projekten Textgrid und GerDracor zu Grunde lagen, kann für diese Erweiterungen allerdings nicht zurückgegriffen werden, weil die fehlenden Dramentexte nicht neu ediert wurden. Dabei stellen sich sowohl editorische Fragen der Transkription und Normalisierung als auch die Fragen der Automatisierung des TEI-Taggings. Im Folgenden sollen einige Vorgehensweisen zur Edition, Volltextdigitalisierung und Textauszeichnung historischer Dramentexte mit und im Anschluss an die Open Access-Software OCR4all Das Layout von Dramendrucken um 1800 ist nicht normiert und variiert von Drucker zu Drucker, die prototypische Struktur ist jedoch wie folgt aufgebaut: Titelseiten enthalten Angaben zu Titel, Untertitel, Verfasser:in, Druckerei und zumeist auch zum Erscheinungsjahr. Es folgt das Personenverzeichnis inklusive der Figurenauflistung, gefolgt von dem Beginn des Drameninhaltes mit folgender Struktur: Akt/Aufzug > Szene/Auftritt > Ortsangabe > Figurenaufzählung > Figurenname > Dialogtext > Regieanweisung. Je früher ein Dramentext erschienen ist, desto wahrscheinlicher ist es auch, dass er eine Vorrede vor dem Personenverzeichnis enthält. Üblicherweise entspricht die Reihung der Dramenelemente im Druck der tatsächlichen Lesereihenfolge. Jedoch gibt es Fälle, die dieser Logik nicht folgen. Ein Beispiel sind am Ende der Seite abgedruckte Fußnoten im Drama ""Dido"" (1794) von Charlotte von Stein, die bestimmte Textstellen kommentieren (vgl. Abb. 1). Eine weitere Besonderheit stellt die uneinheitliche Gestaltung von Figurenaufzählungen dar. Üblicherweise beginnen Szenen mit einer Aufzählung aller in der Szene auftretenden Figuren gefolgt von den Dialogen. Einige Dramen verzichten jedoch in einzelnen Szenen auf die Aufzählung ganz oder integrieren die Nennung der Figuren in die anfängliche Regieanweisung. Um alle diese Elemente berücksichtigen zu können, sollte eine Digitalisierungsumgebung gewählt werden, die eine differenzierte semantische Auszeichnung von Segmenten erlaubt. Da es bei den knappen Ressourcen im wissenschaftlichen Bereich unabdingbar ist, eine kostenfrei nutzbare Digitalisierungsumgebung zu verwenden, die dennoch bestmögliche Ergebnisse liefert und stetig gewartet und aktualisiert wird, bietet sich OCR4all an. Einige Elemente werden nicht als Layoutregionen ausgezeichnet und werden deshalb bei der späteren Texterkennung nicht berücksichtigt. Dazu zählen insbesondere Seitenzahlen, Seitentitel und Kustoden. Auch die Seitenumbrüche gehen verloren, nicht jedoch die Zeilenumbrüche, die automatisch bei der Zeilensegmentierung in LAREX erkannt werden. Zentrale Eigenschaften des Drucks gehen auf diese Weise verloren, dafür wird der Segmentierungsprozess bzw. die händische Nachkorrektur der Segmente etwas beschleunigt und das Hauptziel 'maschinenlesbare, für die Zwecke der CLS verwendbare Dramentexte zur Verfügung zu stellen 'erreicht. Repliken werden nicht durch Seitenumbrüche, Seitenzahlen oder Kustoden unterbrochen und dadurch unbrauchbar für Stilometrie, Topic Modelling oder Sentiment bzw. Emotion Analysis (vgl. Dennerlein et al., 2023). Ziel ist es nicht, eine diplomatische Transkription zu erstellen, sondern die Datenpublikation zu gewährleisten (vgl. Sahle 2013, Teil 2: 256-266). Daher ist auch ein bestimmter Umgang in OCR4all mit druckspezifischen Zeichen wie Superskripte oder die Verwendung von Schaft-S zu gewährleisten. In OCR4all können unterschiedliche Ergebnisse erzielt werden, je nachdem welches Modell für die Texterkennung angewendet wird. Mit dieser Methodik ist es unerheblich, ob das Modell auf die exakte Erkennung der konkreten Zeichen trainiert ist oder bereits eine Normalisierung ausgewählter Zeichen berücksichtigt. Die Normalisierung ausgewählter Zeichen für die DraCorTEI-Datei kann in einem weiteren Schritt über das Konvertierungsskript vorgenommen werden. LAREX bietet zudem die Möglichkeit, die Lesereihenfolge der Textregionen individuell anzupassen, sodass Sonderfälle wie Fußnoten in der Weiterverarbeitung an der entsprechenden Stelle platziert werden können. Abb. 1 zeigt die angepasste ""Reading Order"" in LAREX: Nach Akt, Szene, Regieanweisung und Figurenangaben folgt der erste Satz der Replik, dann ist die Fußnote eingegliedert. Sie ist damit hinterher als zugehörig zu diesem Satz identifizierbar und kann 'etwa als Endnote 'die spätere digitale Edition des Dramas ergänzen. Um später alle Elemente automatisch mit XML-Elementen auszeichnen zu können, lohnt sich die akribische Vorarbeit der Regionsauszeichnung, die bei entsprechender Übung nur etwa 5 Minuten pro Seite dauert. Mittelfristig sollen diese Auszeichnungen als Trainingsdaten verwendet werden, um den Automatisierungsgrad kontinuierlich zu steigern. Anschließend werden die ausgezeichneten Regionen vollautomatisch in Zeilen zerlegt. Es folgt die eigentliche Texterkennung, bei der aus den Bildzeilen der maschinenlesbare Text mittels Modellen extrahiert wird. Hierbei können entweder existierende, ""gemischte Modelle"" direkt angewendet werden oder ""werksspezifische Modelle"", durch gezieltes Training auf die Erkennung einer bestimmten Drucktype hin optimiert werden. Derzeit kann bei Erkennung mit einem gemischten Modell zuverlässig eine Zeichengenauigkeit von über 98 % erreicht werden, meist sogar über 99 %. Durch das Training gemischter Modelle kann die Genauigkeit noch deutlich weiter gesteigert werden. Die dazu benötigten Trainingsdaten können ebenfalls in LAREX erstellt werden und bestehen aus Bildzeilen sowie der korrekten Transkription des darauf zu sehenden Texts. Das Ergebnis des Digitalisierungsprozesses in OCR4all, wie auch von fast allen anderen OCR-Programmen, sind PAGE-XMLs, die neben dem gesamten Textinhalt der einzelnen Seiten weitere Informationen wie Erstellungsdatum, Metadaten, Layoutregionen und Koordinaten enthalten. Die Angabe der Reading-Order ist für jene Fälle wichtig, in denen die Koordinaten von Textregionen als Information nicht ausreichen, um die gewünschte Struktur in DraCorTEI abzubilden. Mit EzDrama existiert bereits ein Konvertierungsskript, das Dramen, die im Plaintext in lateinischer Schrift vorliegen, teilautomatisch mit den Tags von DraCorTEI auszeichnet (Skorinkin et al., 2022). Da auch diese Dramen in der Regel nicht vollständig automatisch in TEI konvertiert werden können, wurde eine Markup-Sprache mit sehr wenigen Markups und Regeln entworfen, bei der Markierungen für bspw. Szenen, Sprecher:in oder Regieanweisungen direkt in den Text hineingeschrieben werden. In einem Colab oder mithilfe eines Jupyter-Notebooks können die so ausgezeichneten Dramen dann mit TEI-Tags ausgezeichnet werden, wie sie in DraCor verwendet werden. Dazu müssen jedoch zwei Voraussetzungen erfüllt sein: Zum einen müssen die Texte bereits maschinenlesbar in Form von lateinischer Druckschrift vorliegen, zum anderen muss Zeit für eine händische Auszeichnung der Dramen in einem proprietären Format aufgewendet werden. Das im Folgenden vorgestellte Skript versteht sich ausdrücklich als Ergänzung dieser verdienstvollen Arbeit und ist für diejenigen Fälle vorgesehen, in denen beide Voraussetzungen nicht gegeben sind. Das im Folgenden kurz charakterisierte Skript wandelt die PAGE-XMLs in eine gültige DraCorTEI-Datei um. Das Skript spielt eine zentrale Rolle in der automatisierten Erfassung und Verarbeitung der Dramendrucke, indem es spezifische Merkmale und Besonderheiten der Dramendrucke adressiert. Es beinhaltet vier Klassen, die folgenden Zwecken dienen: Das Skript verarbeitet der Reihe nach alle in einem Ordner liegenden PAGE-XML-Dateien und arbeitet sich von der äußersten zur innersten Ebene vor: Seite > TextRegion > TextLine > Wort > Glyphe. Bei Initialisierungsfehlern bei der Verarbeitung werden Fehlermeldungen mit Angaben zur entsprechenden Stelle und dem Dateinamen ausgegeben, um Fehler in der Vorbearbeitung oder Beschädigungen in den Dateien zu finden. Für den Fall, dass das Skript Textregionen verarbeitet, die falsch platziert sein sollten oder deren Inhalt einer falschen Textregion entsprechen, wird in der DraCorTEI-Datei an entsprechender Stelle ein Tag mit dem Inhalt ""WARNING"" ausgegeben, die eine notfalls mögliche händische Nachbearbeitung ermöglicht. Auch in diesem Fall soll gewährleistet werden, dass Fehler in der Vorbearbeitung in OCR4all ausfindig gemacht und korrigiert werden können. Eine der Besonderheiten, die in der Bearbeitung durch das Skript berücksichtigt werden, sind spezielle Zeichen, die für die endgültige DraCorTEI-Datei normalisiert werden müssen. Folgende regelmäßig vorkommende Zeichen werden dementsprechend normalisiert: ≈ø ‚Üí s,  í ‚Üí z, aÕ§ oÕ§ uÕ§ ‚Üí ä ö ü, etc. Sollten in etwaigen Projekten, die dieser Methodik folgen, weitere spezielle Sonderheiten in den Drucken auftreten, können diese durch kleine Anpassungen im Skript mit aufgenommen werden. Mit dem hier beschriebenen Verfahren der Auszeichnung, der Transkription mit OCR4all und der XML-Kodierung, benötigt die Digitalisierung eines Dramas derzeit noch immer acht bis zehn Stunden. Verzichtet man auf die Korrektur des automatisch erfassten OCR-Textes, weil man bspw. sehr große Textmengen erfassen möchte, bei denen Fehler nicht mehr ins Gewicht fallen, verkürzt sich die Bearbeitungszeit um die Hälfte. Für andere Zeitabschnitte müsste das Verfahren zudem angepasst werden, wenn das Layout signifikant abweicht. Ein besonderer Fall sind bspw. Drucke wie die Libretti der Insgesamt ist zu bedenken, dass der Prozess der Volltextdigitalisierung historischer Dramentexte verhältnismäßig komplex ist und dass man für ideale Ergebnisse deutlich mehr Zeit investieren muss als etwa für Prosatexte. Die händisch segmentierten und korrigierten Daten können jedoch als Trainingsdaten genutzt werden, so dass der Automatisierungsgrad in Zukunft sukzessive gesteigert werden kann."
2025,DHd2025,PICHLER_Axel_Empirische_Evaluation_des_Verhaltens_von_LLMs_a.xml,Empirische Evaluation des Verhaltens von LLMs auf Basis sprachphilosophischer Theorien: Methode und Pilotannotationen,"Axel Pichler (Universität Stuttgart, Deutschland); Dominik Gerstorfer (Technische Universität, Darmstadt, Deutschland); Jonas Kuhn (Universität Stuttgart, Deutschland); Janis Pagel (Universität zu Köln, Deutschland)","Large Language Model, behavioral analysis, Bedeutung, Verstehen","Modellierung, Theoretisierung, Methoden, Text","Die Fähigkeit großer Sprachmodelle (LLMs), Bedeutung und Verstehen zu simulieren, hat in den letzten Jahren zu einer regen Debatte darüber geführt, inwiefern LLMs tatsächlich bedeutungsvolle Sprache generieren und diese verstehen. Bedeutungs- und Verstehensbegriff sind zugleich zentrale Konzepte der Kultur- und Geisteswissenschaften. Dementsprechend beteiligen sich auch viele traditionelle und digitale Geisteswissenschaftler:innen an der laufenden Debatte. So wurde 2023 auf einem Panel der DHd-Konferenz ein Zugang zu dieser Debatte präsentiert, der das Ziel hatte, durch theoretische Impulse eine präzisere Beschreibungssprache für die Digital Humanities (DH) zu schaffen, die dabei helfen sollte, die Unterschiede zwischen den Bedeutungsprozessen von Maschinen und Menschen herauszuarbeiten, um so die Anwendung von Künstlicher Intelligenz kritisch zu hinterfragen (Gengnagel et al. 2024). Während das Panel derartig eine geisteswissenschaftliche Kernkompetenz 'die Begriffsanalyse 'reaktivierte, um ordnend in die Diskussion einzugreifen, inwiefern LLMs bedeutungsvolle Sprache generieren und diese verstehen, folgte es zugleich der auch in der Natural-Language-Processing-Community weitverbreiteten Tendenz, besagte Fragen primär theoretisch zu verhandeln. Wir wollen dieser theoretischen Debatte keine weitere theoretische Position hinzufügen, sondern im Folgenden einen Vorschlag machen, wie ergänzend überprüft werden kann, inwiefern das Verhalten eines LLMs existierenden Sprachtheorien entspricht. Dabei knüpfen wir an die NLP-Tradition des ""behavioral testings"" an, die sich mit der Prüfung verschiedener Fähigkeiten eines Systems durch Validierung des Eingabe-Ausgabe-Verhaltens ohne Kenntnis der internen Struktur befasst (Beizer 1995). Hierbei verfolgen wir einen theoriegeleiteten Top-Down-Ansatz, der von einer gegebenen Sprachtheorie ausgeht und diese so modelliert, dass ihre zentralen Begriffe in einer Form operationalisiert werden können (Krautter/Pichler/Reiter 2023; Gerstorfer/Gius 2023), welche die Erstellung eines Testdatensatzes erlaubt, der die zentralen sprachtheoretischen Annahmen der Referenztheorie in einem angemessenen Grad repräsentiert. Im Falle von Bedeutungstheorien sollte ein derartiger Testdatensatz das Output eines kompetenten Sprechers auf eine Art und Weise abbilden, wie es von der untersuchten Sprachtheorie impliziert wird. Mit Hilfe eines solchen Testdatensatzes könnte dann der Grad bestimmt werden, in dem das Sprachverhalten eines LLMs dem einer Sprachtheorie entspricht. Ein derartiges Wissen über das Verhalten von LLMs ist insbesondere für jene Zweige der DH relevant, deren Theorien und Analysen auf bestimmten sprachphilosophischen Vorannahmen aufbauen. Sie könnten dann für ihre Analysen jene LLMs verwenden, die diesen entsprechen. Wir werden daher im Folgenden eine Methode präsentieren, die eine derartige Evaluation erlaubt, sie anhand eines Beispiels 'der wahrheitskonditionalen Semantik von Donald Davidson 'vorführen, und die Resultate der ersten Pilotannotationen sowie erster Experimente mit LLMs präsentieren. An diesem Punkt sei darauf hingewiesen, dass die Auswahl von Davidsons Sprachtheorie nicht daher rührt, dass wir glauben, dass sie in höherem Grad als alternative Sprachtheorien dem Textgenerierungsverhalten von LLMs entspricht, sondern daher, dass Ausgangsszene und Kernkonzept seiner Sprachphilosophie sehr treffend die Situation beschreiben, mit der Benutzer von großen Sprachmodellen konfrontiert sind: die radikale Interpretation. Bei dieser steht der radikale Interpret einer Sprecher:in einer ihm unbekannten Sprache gegenüber und versucht auf Basis einer spezifischen Form des Abgesehen von diesen Parallelen ist die im Folgenden vorgestellte Methode zur Entwicklung eines Testdatensatzes zur Überprüfung, inwiefern das Verhalten eines LLMs den Erwartungen einer Sprachtheorie bezüglich des Verhaltens eines kompetenten Sprechers entspricht, sprachtheorie-agnostisch. Mit ihrer Hilfe können auch alternative Sprachtheorien getestet werden, was in Anbetracht der Vielfalt des Theorieangebots in Sprachphilosophie und Semantik sowie von deren zentraler Rolle zum Beispiel in den Interpretationstheorien der Literaturwissenschaft ein Forschungsdesiderat darstellt. Längerfristig streben wir an, weitere sprachphilosophische Theorien derartig zu überprüfen. Die Generierung eines Testdatensatzes zur Überprüfung, inwiefern das Verhalten eines LLMs den Erwartungen einer Sprachtheorie entspricht, erfolgt in drei Schritten: In einem ersten Schritt sind die zentralen Annahmen der zu testenden Sprachtheorie rational zu rekonstruieren. Ziel dieser Rekonstruktion ist, zweitens, die Rückführung besagter Sprachtheorie auf eine oder mehrere sprachtheoretische Hypothesen, die im Folgenden getestet werden. Dafür sind, drittens, die zentralen Begriffe dieser Hypothesen so zu operationalisieren, dass mit ihrer Hilfe ein Testdatensatz erzeugt werden kann. Im Zentrum von Davidsons Bedeutungstheorie steht die These, ""that a theory of truth, modified to apply to a natural language, can be used as a theory of interpretation"" (Davidson 2006, S. 189). Davidson kommt einem Ansatz wie dem hier präsentierten, der auf eine operationalisierbare Rekonstruktion einer Theorie abzielt, nun insofern entgegen, als dass Davidson selbst mit dem Konzept der radikalen Interpretation bereits eine Operationalisierung der Kernelemente seiner Sprachtheorie vorgelegt hat, die nur in Hinblick auf jene Voraussetzungen zu adaptieren ist, die LLMs im Unterschied zu kompetenten menschlichen Sprechern nicht erfüllen. Davidson schreibt: ""A theory of meaning (in my mildly perverse sense) In Hinblick auf die zu konstituierende Leithypothese heißt das, dass einem Sprachmodell eine Vielzahl von Gelegenheitssätzen in Bezug auf eine bestimmte Situation vorzulegen ist, um dann zu überprüfen, inwiefern das Model diese Sätze für wahr hält. Die absurd anmutende Formulierung des soeben artikulierten verweist bereits auf jene Elemente der Davidson""schen Theorie, die zu adaptieren sind, wenn man sie auf LLMs anwenden möchte. Dazu zählen insbesondere, dass 1.) große Sprachmodelle keine Agenten sind, 2.) keine Überzeugungen besitzen und dementsprechend 3.) auch keine Propositionen für-wahr-halten können. LLMs können jedoch das entsprechende Verhalten eines kompetenten Sprechers simulieren. Zudem entspricht die ""Kommunikationssituation"" zwischen einem LLM und einem Menschen nicht derjenigen der radikalen Interpretation: Weder besitzt eine solche einen realweltlichen Situations- und Bezugsrahmen, auf Basis dessen in Übereinstimmung mit dem Gesamtverhalten eines Sprechers einer fremden Sprache bestimmt werden kann, ob dieser einen Satz zu einem bestimmten Zeitpunkt an einem bestimmten Ort für wahr hält oder nicht, noch handelt es sich dabei um eine kausale Relation zwischen Bezugsrahmen und Verhalten besagten Sprechers (vgl. Davidson 2001). Das im Folgenden entwickelte Testset überprüft dementsprechend nur, inwiefern sich diese Simulation zum Kommunikationsverhalten eines kompetenten Sprechers im Sinne Davidsons verhält bzw. zu welchem Grade es diesem entspricht. Ausgangspunkt bei der Entwicklung des Testsets ist dabei folgende Leit-Hypothese: (H) Ein Sprachmodell verwendet eine Sprache wie ein kompetenter Sprecher im Sinne Davidsons, gdw. es die selben zum Zeitpunkt Z geäußerten Sätze im Verhältnis zum sprachlichen Kontext K als wahr bestimmt. Die Testdatenerzeugung erfolgt dementsprechend in Bezug auf einen sprachlichen Kontext K, der von jedem beliebigen Text gefüllt werden kann, der eine Situation beschreibt, die im Hinblick auf situative Aussagen auf ihren Wahrheitswert hin überprüft werden kann. Dieser Fokus auf wahrheitskonditionale situative Aussagesätze ist der Orientierung an den theoretischen Grundannahmen der Davidson""schen Theorie geschuldet. Im Falle alternativer sprachphilosophischer Theorien können andere Frage-Antwort-Typen relevant sein. Für unsere Pilotannotation haben wir auf die Texteröffnung von Franz Kafkas Erzählung Wir haben uns für unsere Experimente auf die ersten beiden der drei genannten Schritte konzentriert und dementsprechend in einem ersten Schritt aus den beiden Textstellen das Basisvokabular extrahiert, indem wir mithilfe von spaCy Wir führen eine Pilotstudie zur Annotation der Testdaten durch, um die Durchführbarkeit des Vorhabens zu demonstrieren. Die Testdaten zu Kafkas Urteil wurden von drei Annotatoren (drei der Autoren; die 25 manuellen Sätze wurden vom ersten Autor erstellt, der selber nicht annotiert hat) annotiert, und zwar bezüglich des Wahrheitswertes des Satzes im Hinblick auf den gegebenen Kontext als auch bezüglich der Angabe, ob der Satz extrinsisch oder intrinsisch wahr oder falsch ist. Die Auswertungen der Annotationen in Form einer Inter-Annotator-Agreement-Studie befinden sich in Tabelle 1 (Agreement bezüglich der Wahrheitswerte) und Tabelle 2 (Agreement bezüglich intrinsisch/extrinsisch). Für die Auswertungen wurden Sätze, die von den Annotatoren als nicht-entscheidbar eingeschätzt wurden, auf die Werte falsch, bzw. intrinsisch gesetzt. Die Tabellen zeigen das IAA für alle Sätze und für Teilmengen (subsection) der Sätze: (i) ist der Satz ein manuell oder automatisch erstellter Satz, (ii) enthält der Satz einen Junktor oder nicht und (iii) falls der Satz einen Junktor enthält, welchen? Gezeigt wird das resultierende Fleiss"" Kappa (Fleiss 1971) als Maß dafür, wie stark die Annotationen übereinstimmen Für die Wahrheitswerte gibt es mit einem Fleiss"" Kappa von 0.56 ein moderat gutes Agreement. Am höchsten ist das Agreement für Sätze ohne logische Junktoren und für die Kon- und Disjunktion (Fleiss"" Kappa von ca. 0.75). Am wenigsten Übereinstimmungen gibt es für die Implikations-Sätze, wobei das Ergebnis nicht statistisch signifikant ist (p>0.05). Für die automatisch erstellten Sätze lässt sich ein etwas höheres Agreement ablesen als für die manuell erstellten. Im Bezug auf extrinsische/intrinsische Wahrheitswertzuschreibungen ist Fleiss"" Kappa durchweg negativ, was darauf hindeutet, dass die Annotatoren die Annotationsguidelines unterschiedlich interpretiert haben. Erste Experimente mit zwei LLMs 'OpenAI""s GPT-4o und Anthropics Claude 3.5 Sonnet Model In summa haben wir gezeigt, dass es möglich ist eine Sprachtheorie so zu modellieren und anschließend in Hinblick auf einen bestimmten Testkontext zu operationalisieren, dass die Resultate als Testdaten für diese Sprachtheorie hinreichen. Im Zuge unserer ersten Experimente haben wir festgestellt, dass eine solche Modellierung und Operationalisierung jedoch zahlreiche Fallstricke besitzt: So führt zum Beispiel eine vollständig automatisierte Testdatengenerierung auf Basis eines gegebenen Vokabulars mehrheitlich zu sinnlosen Sätzen, ebenso schränkt einen die Limitierung auf das in einem bestimmten Kontext gegebene Vokabular unnötig ein. Zudem hat im konkreten Fall Davidsons formallogische Orientierung den Annotierenden Probleme gemacht. Des Weiteren haben wir darauf verzichtet, die Input-Sequenzen so zu manipulieren, dass das LLM explizit dazu aufgefordert wird, einer bestimmten Sprachtheorie entsprechend zu handeln. Bei den hier durchgeführten Experimenten ging es uns nur darum, wie die LLMs ohne zusätzliche Informationen oder Prompt Engineering Strategien auf die ""Gelegenheitssätze"" ""reagieren""."
2025,DHd2025,STIEMER_Haimo_Pause_im_Text__Zur_Exploration_semantisch_kond.xml,Pause im Text. Zur Exploration semantisch konditionierter Sprechpausen in Hörbüchern,"Haimo Stiemer (TU Darmstadt, Deutschland); Hans Ole Hatzel (Universität Hamburg, Deutschland); Chris Biemann (Universität Hamburg, Deutschland); Evelyn Gius (TU Darmstadt, Deutschland)","Segmentierung, Narratologie, Hörbücher, Sprechpausen","Transkription, Strukturanalyse, Daten, Ton, Text","Die für die Textanalyse grundlegende Segmentierung von Prosatexten, also deren Zerlegung in diskrete Einheiten, ist in den Computational Literary Studies (CLS) weiterhin ein prominentes Problem. Obgleich viele computationelle Verfahren die vorhergehende Unterteilung von Texten voraussetzen, fehlt es bislang an standardisierten Segmenten (cf. Bartsch et al. 2023). In Abhängigkeit von der jeweiligen Forschungsfrage und der zum Einsatz bestimmten computationellen Methoden finden sich sowohl Segmentierungen von Layoutelementen bzw. Einheiten der materiellen Textgestaltung (cf. Herzog 2018), die Tokenisierung (auf Wort- oder Satzebene) oder aber das Ein in den CLS bislang nicht geprüfter Ansatz der Segmentierung, die Zergliederung von Erzähltexten mittels der in der Rezitation emergenten Sprechpausen, ist Gegenstand dieses Beitrags. Wir präsentieren erste Beobachtungen hinsichtlich der Möglichkeiten wie Bedingungen für die Identifikation und Analyse semantisch konditionierter Sprechpausen in Hörbüchern. Ein solcher Zugang würde die genannten, allein vom Text ausgehenden Zugänge zur Segmentierung um eine in der Textrezeption erzeugte Segmentierung komplementieren, auch um die bestehenden Segmentierungsoptionen überprüfen zu können. Ausgangspunkt unserer Analyse ist das in der Sprechwissenschaft und Sprecherziehung entwickelte Konzept des interpretierenden Textsprechens (cf. Geißner 1981:175; Brand 2021), mit welchem jedwede Rezitation von Texten vor Publikum als originärer Interpretationsvorgang und mithin die Sprechfassung eines Textes als dessen Interpretation verstanden werden. In der Konsequenz verstehen wir die Pausensetzung und -länge, welche der prosodischen Dimension eines literarischen Werks angehören, als die von Vortragenden ad hoc oder planmäßig vorgenommene sinnhafte Segmentierung eines Textes. Unbestimmt bleibt dabei zunächst, inwiefern es sich bei den potentiell multifunktionalen Sprechpausen um syntaktische und typographisch bedingte, auf z.B. Interpunktion oder Absätze rekurrierende, oder aber um semantisch konditionierte Unterbrechungen handelt, denen eine noch näher zu bestimmende handlungsrelevante Funktion zugewiesen werden kann. In diesem Beitrag untersuchen wir demzufolge die semantisch-interpretative Qualität von Sprechpausen in Tonaufzeichnungen von eingelesenen Erzähltexten. Der Beschreibung unseres Audio-Korpus (2.) sowie der Transkriptions-Methode (3.) folgt die Exploration der aus diesem Korpus gewonnenen, transkribierten Daten, um die mögliche Motivierung der Sprechpausen zu erfassen. Analysiert wird hierfür die Verteilung der Anzahl wie Länge der in den Daten detektierten Pausen (4.). Anschließend diskutieren wir den Zusammenhang von Sprechkompetenz und Pausensetzung (5.) und ziehen aus der Datenexploration Rückschlüsse für weitergehende Forschungsaktivitäten (6.). Für unsere Untersuchung haben wir die aufgezeichneten Lesungen von drei, aus dem EvENT-Projektkorpus (Vauth et al. 2021) entnommenen Erzähltexten analysiert. Bei der Auswahl der Texte wurde darauf geachtet, dass sich diese durch verschiedene narrative Profile bzw. Charakteristika auszeichnen. Den analysierten Vortragsaufzeichnungen zugrunde lagen somit Franz Kafkas hypotaktisch geprägte Erzählung Für die automatische Pausenerkennung verwendeten wir Whisper (Radford et al. 2023), ein neuronales Modell zur Transkription von gesprochener Sprache, welches annähernd die Fehlerraten von professionellen Transkriptor:innen erreicht (Radford et al. 2023, Abbildung 7). Prinzipiell wäre es wünschenswert, einen Alignierungsansatz zu nutzen, die Audiodaten also mit einem 'in unserem Fall verfügbaren 'Originaltext abzugleichen. Schiel et al. (2017) beschreiben ein derartiges System. In der Praxis war jedoch keine Implementation einfach auf unseren Daten anwendbar, sodass die automatische Transkription für diese explorative Arbeit passend war. Konkret setzen wir die Implementation WhisperX (Bain et al. 2023) ein, welche zahlreiche zusätzliche Funktionen im Vergleich zur ursprünglichen Whisper-Implementation bietet. Die Anwendung von WhisperX auf eine beliebige Audiodatei erzeugt ein Transkript eben dieser, in dem einzelne Segmente mit Zeitcodes versehen sind. Die Segmente entsprechen dabei linguistischen Sätzen (da wir WhisperX mit den Standardoptionen aufrufen und somit Die Verteilung der insgesamt 4.542 im Audio-Korpus mit WhisperX detektierten Pausen (an Satzenden) nach den Sprechfassungen der Texte auf die Pausenlängenwerte (x-Achse) sowie die Pausenanzahl (y-Achse) findet sich in Abbildung 1. Mit der Auswertung der Daten und in Anlehnung an die vom Grammatischen Informationssystem grammis (cf. Institut für Deutsche Sprache 2013) vorgeschlagene Differenzierung von Sprechpausen ergeben sich zunächst folgende Beobachtungen: In allen Sprechfassungen dominieren erwartungsgemäß die sehr kurze Pausen, die lediglich auf Atemeinschnitte bei der Satzbeendigung verweisen (x<0,3 Sek.), während Verzögerungspausen (0,3<x<1) deutlich seltener auftreten. Wird ab dem Längenwert 1 Sekunde den Pausen eine dann noch zu bestimmende semantische Relevanz zugemessen (Relevanzpause; cf. ebd.), weisen beide Sprechfassungen der Die dialogreiche Gestaltung der Die höchsten Anteile an Relevanzpausen weisen die beiden Sprechfassungen des Kleist-Textes auf (professionelle Lesung 44 % und Laienlesung 41 % aller Pausen der jeweiligen Sprechfassung). Es handelt sich im Korpusvergleich um den Text mit den meisten Figuren, mit einem sehr handlungsintensiven, also auch diverse Ortswechsel enthaltenden, Plot. Nachdem das Verhältnis der Segmentlänge zur durchschnittlichen Satzlänge der Texte unterschiedlich ist, nehmen wir an, dass eher bestimmte narrative Eigenschaften für Pausen relevant sind. Zumindest scheint die höhere Heterogenität, die durch vermehrte narrative Elemente wie Dialoge, Figuren oder auch weitere, handlungsbezogene Elemente entsteht, auch die Anzahl an Relevanzpausen zu steigern. Abgesehen von der Laienfassung des Zu bemerken ist, dass sich in Absätzen auch weitere, möglicherweise durch typographische Trigger erzeugte Pausen befinden. Neben den Anführungszeichen der direkten Rede zählt dazu z.B. ungewöhnliche Interpunktion am Satzende. Wir gehen zugleich davon aus, dass die Textkenntnis bei den professionellen Sprecher:innen in der Regel höher ist, nicht zuletzt aufgrund der redaktionellen Betreuung, ihrer Ausbildung und der im Vorfeld der Aufnahme mutmaßlich erfolgten, eingehenden Beschäftigung mit dem Text. 6. Fazit und Ausblick Unsere tentative Annäherung an die Segmentierungsfunktion von Sprechpausen hat ergeben, dass es lohnend erscheint, diesen Ansatz weiter zu verfolgen und durch ein größeres Audio-Korpus zu validieren. Die Exploration unserer Daten bislang deutet an, dass mittels der sprechpausenbezogenen Segmentierung Textprofile erstellt werden können und eine größere Sprecher:innenkompetenz vermutlich zu mehr nicht primär typographisch getriggerten Sprechpausen führt. Die Erstellung eines größeren Audio-Korpus für weitere Untersuchungen erscheint dabei nicht zuletzt aufgrund der spezifischen Rhetorizität von Vortragenden geboten, ihrer idiosynkratischen Realisierung der Sprechfassung eines Textes. Um die damit verbundenen Parameter der Pausensetzung und -längen weitestmöglich zu neutralisieren, wäre die Untersuchung von deutlich mehr Sprechfassungen nur eines Textes notwendig. In einem nächsten Schritt werden wir daher acht zusätzliche Audioaufnahmen von Laien-Lesungen des"
2025,DHd2025,SCHR_TER_Julian_Zur_Modellierung_von_Unsicherheit__Machine_L.xml,Zur Modellierung von Unsicherheit: Machine Learning und begriffliche Vagheit am Beispiel der Novellen im 19. Jahrhundert,"Julian Schröter (LMU München, Deutschland)","Machine Learning, Unbestimmtheit, Vagheit","Programmierung, Strukturanalyse, Modellierung, Visualisierung, Literatur, Text","Die Modellierung von Vagheit und Unsicherheit in der sprachlichen Kategorisierung ist ein offenes Problem im Bereich des Die Sprachphilosophie und die literarische Gattungstheorie stellen mit der Prototypentheorie (Rosch, 1978; Taylor, 2007; Hempfer, 2010) und dem Familienähnlichkeitsbegriff nach Wittgensteins Die folgende Modellierung ist durch drei wesentliche Schritte gekennzeichnet: (1a) der Aufbau eines geeigneten Verfahrens maschinellen Lernens, das (1b) eine nutzbare Kennzahl für eine Bemessung der relativen Nähe und Distanz zum begrifflichen Zentrum bereitstellt, (2) die Erfassung eines Bereichs relativer Vagheit, und (3) ein Maß zur Bemessung des relativen Vagheitscharakters eines Gattungsbegriffs wie der Novelle. Zu (1): Gewählt wird (1a) Logistische Regression als klassisches Modell überwachten maschinellen Lernens. Auch wenn logistische Regression ein bereits in die Jahre gekommenes Lernverfahren darstellt, hat es zwei entscheidende Vorteile. Erstens die bereits von Underwood (2019a und 2019b) betonte gute Interpretierbarkeit der Features, und zweites die relative Vorhersagewahrscheinlichkeit für die Zuordnung einer Instanz zu einer der in Frage kommenden Klassen. Die grundlegende Idee zu Schritt (1b) ist es, relative Nähe einer Instanz zum begrifflichen Zentrum im Sinn von Prototypikalität durch die Vorhersagewahrscheinlichkeit für eine bestimmte Klasse auszudrücken. Der entscheidende und neue Schritt ist nun (2) die Einführung eines Bereichs relativer Vagheit. Das vorgestellte Modell verwendet den sogenannten  Im nächsten Schritt bedarf es einer Entscheidungsfunktion für Enthaltung. Hierfür wird mit logistischer Regression ein Algorithmus verwendet, der in der sigmoiden Funktion (Logistische Funktion, Gl. 2) mit der Link-Funktion (Gl. 3) über die Berechnung von Vorhersagewahrscheinlichkeiten läuft. Abbildung 1 zeigt, wie der Bereich einer Enthaltung symmetrisch um eine Vorhersagegenauigkeit von 0,5 gewählt werden kann.  Anstelle einer Entscheidungslinie bei einem Wert von y = 0,5 für die Vorhersagewahrscheinlichkeit wird eine Art Grauzone eingeführt, so dass bei Funktionswerten innerhalb dieses Bereichs anstelle einer binären Vorhersage für ei¬≠ne der beiden möglichen Klassen eine Enthaltung erfolgt. Die Enthaltung gibt lediglich eine episte¬≠mische Enthaltung des Modells wieder, ohne Impli¬≠kation für den Status der Objekte. Was der Die optimale Breite des Enthaltungsbereichs kann in Form einer Gridsuche ermittelt werden. Abbildung 2 zeigt den Graphen, der diese Gridsuche am Beispiel eines Modells zur Klassifikation von Romanen vs. Märchen als Funktion abbildet, bei der für das trainierte Modell und die verfügbaren Validierungsdaten jeweils zu einem sukzes¬≠sive ausgeweiteten Enthaltungsbereich für Vorhersagewahrscheinlichkeiten zwischen 0 und 1 der zugehörige  Zu (3): Zur Berechnung des Grads an begrifflicher Vagheit, die auf diese Weise algorithmisch modelliert wird, stehen mehrere Maße als potenzielle Kandidaten zur Verfügung: (a) die Breite des optimalen Enthaltungsbereichs und (b) die In einem Prozess interner Evalutation wurden Daten und simuliert, um das Verhalten der Berechnung optimaler Enthaltungsbreiten besser zu verstehen. Die Ergebnisse werden in den Visualisierungen aus Abbildung 3 dargestellt. Hierfür wurde Vagheit im Sinn einer Kombination von Familienähnlichkeit und potenzieller Prototypikalität dadurch simuliert, dass in einigen Fällen jede Klasse nur ein Feature-Cluster mit mehreren informativen Features und mehr oder weniger großer Varianz sowie Störrauschen (im Sinn einer Annäherung an Prototypikalität) erzeugt wurde (linke Spalte) und in einigen Fällen jede Klasse mit mehreren Feature-Clustern (2 Cluster je Klasse in der mittleren Spalte sowie 5 Cluster je Klasse in der rechten Spalte). Jedes Simulation wurde 1000-fach iteriert. Die Boxplots in der Mitte zeigen die Verteilung der optimalen Breite des optimalen Enthaltungsbereichs über alle Iterationen. Die untere Zeile zeigt die Verteilung der Verbesserungsrate. Es zeichnet sich ab, dass die Verbesserungsrate die robustere und aussagekräftigere Kennzahl ist, um komplexe Merkmalsbündel mit komplizierten Mehrfachclustern innerhalb der Klasse abzubilden. Wenn man annimmt, dass Kategorien, die durch mehrere interne Cluster gegliedert sind und dadurch vage werden, dem Familienähnlichkeitsbegriff entsprechen, dann lässt sich auf einer strukturellen Ebene sehen, dass die Verbesserungsrate anhand des C@1-scores bei einem optimalen Enthaltungsbereich eine Modellierung von Prototypikalität darstellt, die Vagheit im Sinn der Familienähnlichkeit einzufangen erlaubt. Der Vergleich von Simulation und realen Daten erlaubt Schluss¬≠folgerungen für künftige Studien. Obwohlperspektivi¬≠sche Modellierung auf Klassifikati¬≠onsaufgaben beruht, die über die Logik von ""ja oder nein"" und nicht über die von ""mehr oder weniger"" laufen, ist das ""Mehr oder Weniger"" in einige Klassifikationsal¬≠gorithmen sozusagen eingebaut. Der  Mit dem entwickelten Verfahren lassen sich aufschlussreiche Anomalien historischer Gattungen sichtbar machen, die wichtige, aber im kodifizierten ""Literaturwissen"" nach wie vor marginalisierte Positionen in der historischen Gattungsforschung stützen (Meyer, 1987; Lukas, 1998). Hierfür wurde eine Projektionsmethode entwickelt, die jeweils eine Gattung im Verhältnis zu den beiden Nachbargattungen visualisiert. Abbildung 4 zeigt die Prototypikalität nach paarweise konstruierten Gattungsunterscheidungen. Die Achsen zeigen jeweils die invertierten Vorhersagewahrscheinlichkeiten (siehe y-Achse aus Abbildung 1). Dargestellt werden die Texte einer Gattung aus dem Set der  Die im Detail in Spezialfragen historischer Gattungsforschung führenden Ergebnisse lassen sich methodologisch auf gewinnbringende Weise für Fragen der algorithmischen Modellierung von Vagheit innerhalb historischer Semantiken generalisieren, wenn es gelingt, diese Modellierung von Unbestimmtheit so zu plausibilisieren, dass die sichtbar gemachte Vagheit tatsächlich einem Ausmaß erwartbarer Vagheit in der historischen Semantik entspricht. Bis hierher lässt sich feststellen, dass das entwickelte Modell so etwas wie die Struktur der Prototypikalität im Raum algorithmischer Konzeptualisierung abbildet. Ob die Resultate indes dem entsprechen, was man in der Linguistik sowie in der Literaturwissenschaft unter Prototypen einer Gattung oder Kategorie sowie unter dem Grad an prototypikalischer begrifflicher Struktur eines bestimmten Gattungsbegriffs versteht, bedarf der Kombination (oder Triangulation) mit weiterer philologischer, evtl. auch psychologischer Evidenz. Daher wird eine Möglichkeit der Triangulation in Form eines Vergleichs der entwickelten und auf dem Konzept der Unentscheidbarkeit beruhenden Verbesserungsrate mit einer alternativen Konzeptualisierung von Vagheit diskutiert: Zumindest für die simulierten Fälle sollten die durch mehrere Feature-Cluster (s. Abb. 3 Mitte und Rechts) erzeugten Klassen mit potenzieller kategorialer Vagheit in Form von "" Zuletzt lassen sich zwei weitere wichtige Schritte für künftige Anschlussforschung präsentieren und so zur Diskussion stellen: Zum einen die Möglichkeit und vor allem die Herausforderungen einer Evaluation anhand psycholinguistischer Evidenz, indem menschliche Urteile zu mehr oder weniger prototypischen Beispielen von Kategoriezugehörigkeit herangezogen werden. Zum anderen ist der Umgang mit uneindeutigen Kategorisierungen ein offenes Problem im Arbeitsfeld des"
2025,DHd2025,MENDE_Jana_Katharina_Gender__under__construction__Daten_und_.xml,Gender (under) construction: Daten und Diversität im Kontext digitaler Literaturwissenschaft,"Jana-Katharina Mende (Martin-Luther-University Halle-Wittenberg, Deutschland); Claudia Resch (Österreichische Akademie der Wissenschaften Wien, Austrian Centre for Digital Humanities and Cultural Heritage, Abteilung Literatur- und Textwissenschaft, Österreich); Mareike Schumacher (Universität Stuttgart / Universität Regensburg, Deutschland); Laura Untner (FU Berlin / Österreichische Akademie der Wissenschaften, Wien, Deutschland und Österreich); Imelda Rohrbacher (Österreichische Akademie der Wissenschaften Wien, Austrian Centre for Digital Humanities and Cultural Heritage, Abteilung Literatur- und Textwissenschaft, Österreich); Elena Suarez Cronauer (Akademie der Wissenschaften und der Literatur, Mainz, Deutschland); Andrea Gruber (Österreichische Nationalbibliothek, Wien, Österreich); Frederike Neuber (Berlin-Brandenburgische Akademie der Wissenschaften, Berlin, Deutschland)","Gender, Infrastruktur, Literaturwissenschaft, Gender Data Gap","Gender, Infrastruktur, Literaturwissenschaft, Gender Data Gap","Die sogenannte Gender Data Gap wirkt sich in mehrfacher Hinsicht auf digitale literaturwissenschaftliche Forschung aus: Historische Ungleichheiten in Bezug auf Gender zeigen sich auf der Ebene von Quellen, Textkorpora, Metadaten wie auch in literarischen Texten selbst. Eine systematische Betrachtung und Erfassung von Geschlechterverhältnissen spielt in DH- und digital-literaturwissenschaftlichen Projekten jedoch kaum eine Rolle. Das Panel widmet sich daher der kritischen Auseinandersetzung von Repräsentation und Erfassbarkeit von Gender in der digitalen Literaturwissenschaft und diskutiert anhand von vielfältigen Lösungsansätzen aus der Fachgemeinschaft, wie mit Gender im Aufbau von Datensätzen, Textkorpora und Forschungsprojekten umgegangen werden kann. Die Gender Data Gap, die sich in Bezug auf Forschungsdaten zeigt, betrifft alle datenbasiert arbeitenden Disziplinen und steht besonders seit dem Erscheinen des Buches ""Data Feminism"" (D""Ignazio/Klein 2020) immer wieder in Diskussion. Von Data Science zu den Digital Humanities offenbart sich ein Bias, der Gender, Race sowie andere marginalisierte Perspektiven weniger oder gar nicht berücksichtigt (vgl. Leyrer 2021, 50). Das resultiert in einer Unterrepräsentation von FLINTA* in Zugängen, Daten und Infrastrukturen (vgl. D""Ignazio/Klein 2020, Kap. 4; Saeger 2016) (Das Akronym FLINTA* steht für Frauen, Lesben, intergeschlechtliche, nichtbinäre, transgeschlechtliche und agender Personen, das Sternchen bezieht weitere Geschlechter mit ein (vgl. Ash 2023). Wir verwenden den Ausdruck in diesem Kontext, um darauf hinzuweisen, dass die Gender Data Gap sowohl Frauen als auch andere Geschlechter betrifft.). Die Gender Data Gap ist dabei intersektional zu verstehen, d.h. sie bezieht sich auf miteinander verknüpfte Kategorien, die jeweils mit eigenen Marginalisierungserfahrungen verbunden sind (zu DH und Intersektionalität siehe Bordalejo/Risam 2019, Losh/Wernimont 2019). Die feministische Literaturwissenschaft beschäftigt sich schon seit ihrer Entstehung in der zweiten Hälfte des 20. Jahrhunderts mit den Lücken in Bezug auf Geschlecht und Literatur. Dabei stand lange Zeit, zuletzt bei Seifert (2021), die Wiederentdeckung von Autorinnen sowie die Kritik an männlich dominierten universitären Leselisten und Kanones im Vordergrund. Die feministische Literaturtheorie bietet wichtige Grundlagen zu intersektionalen Genderbegriffen (vgl. Babka 2004), die mögliche nicht-binäre Genderkonzepte enthalten. Insgesamt zeigt sich jedoch in der traditionellen wie auch der digitalen Literaturwissenschaft, dass FLINTA* in Bezug auf Textkorpora, Kanonisierung, Datensätze und generell als (indirekte) Stakeholder (Leyrer 2021, 5.1) weniger berücksichtigt werden als Männer. Angesichts der tradierten Kanonbildung und etablierter Literaturlisten, die in den traditionellen Literaturwissenschaften oft eine dominierende Rolle spielen, stellt die digitale Transformation literarischer Texte eine Chance, aber auch eine Herausforderung dar. Wenn Texte zu Daten werden, offenbaren sich auf mehreren Ebenen Gender- und Diversitätslücken, die es zu analysieren und zu adressieren gilt. Der aktuelle Stand der Forschung zeigt, dass die Gender Data Gap in den Digital Humanities immer wieder Thema ist (vgl. Lang/Borek/Probst 2023), etwa wenn Juen (2021) konkrete Ungleichheiten in Infrastrukturen und Bibliothekskatalogen vorstellt. In der literaturwissenschaftlichen Forschung mittels digitaler Methoden bilden diese Ungleichheiten oft nicht den Ausgangspunkt, sondern werden erst im Workflow oder durch die Ergebnisse sichtbar (vgl. z.B. Weitin 2021, 47f). Kanonisierungsfragen werden durch Digitalität neu belebt (vgl. Baum 2020). Das Wechselverhältnis von feministischer Literaturtheorie in Bezug auf Genderkonzeptionen und ihre technologische Verarbeitung beschreiben Caughie et al. (2018). Flüh und Schumacher (2020) zielen in ihrem vorbildhaften Projekt dezidiert auf die Modellierungen von (nicht-binären) Genderdarstellungen in der Literatur ab. In dieser Bandbreite zeigt sich eine Vielfalt von technologischen, methodologischen und theoretischen Verschränkungen, die auf die doppelte Verschränkung von literarischer Ebene und Metadaten abzielt. Ein zentrales Anliegen des Panels ist die Diskussion über die Modellierung von Gender sowohl im Text selbst als auch auf der Ebene der Metadaten. Dabei stellt sich die Frage, wie Gender in historischen Texten erfasst und dargestellt wird und wie Normdaten zur Geschlechtszuordnung beitragen. Hier werden historische Ungleichheiten bezüglich Gender durch aktuelle Analysen in DH-Projekten entweder weitergetragen oder durch Maßnahmen gezielt korrigiert. In Bezug auf das Thema der Tagung ""Under Construction. Geisteswissenschaften und Data Humanities"" will das Panel gemeinsam mit Expert*innen erörtern, durch welche Maßnahmen der beschriebene Status quo verbessert werden könnte, wobei die Erfassbarkeit, Konstruktion und Repräsentation von Gender im Mittelpunkt stehen. Es soll untersucht werden, wie feministische Kritik dazu beitragen kann, Grenzen der Computational Literary Studies zu überwinden und Zukunftsszenarien zu entwerfen, die eine angemessene Berücksichtigung von Gender ermöglichen. Durch die kritische Reflexion und Diskussion dieser Themen will das Panel einen Beitrag zur Weiterentwicklung der Digital Humanities leisten und aufzeigen, wie digitale Methoden und feministische Theorien synergetisch zusammenwirken können, um mit Diversität in literarischen Textdaten und Datensätzen umzugehen. Um über die beschriebenen Problemkonstellationen gemeinsam reflektieren zu können, lädt das Panel ausgewiesene Expert*innen aus Forschungs- und Infrastrukturprojekten zur Diskussion ein, wobei die verschiedenen Perspektiven von Doktorandinnen und Postdocs vertreten werden, um die unterschiedlichen Erfahrungen und Ressourcen im Umgang mit diesen Lücken strukturell zu demonstrieren. Das Panel beginnt mit einer kurzen Einleitung der Organisatorinnen, dann folgen 3-5 minütige Impulse der Panelist:innen, die danach in einer Paneldiskussion besprochen werden. Die letzten 30 Minuten des Panels stehen für eine Debatte im Plenum mit Publikum zur Verfügung. In diesem Impulsvortrag wird ein Überblick über DH-Ansätze gegeben, die sich mit Gender befassen. Es wird gezeigt, dass und warum derzeitige Studien meist mit einem binären Genderverständnis operieren und inwiefern es bereits erste Schritte in Richtung einer Überwindung eines solchen gibt. Neben analytischen Ansätzen werden auch aktivistische Beiträge aus Data Feminism und Queer Studies berücksichtigt. Obwohl es derzeit noch keine ""Digital Gender Studies"" gibt, wird gezeigt, dass der Boden bereitet ist, auf denen ein solches Feld aufgebaut werden könnte. Das Projekt ""Sappho Digital"" zielt darauf ab, die deutschsprachige literarische Rezeptionsgeschichte der antiken griechischen Dichterin Sappho als Linked Data zu modellieren. Durch Biases bedingte Lücken, Unsicherheiten und Falschinformationen in Metadatensätzen stellen dabei eine bedeutende Herausforderung dar, besonders da Sappho eine zentrale Figur für weibliche und queere Autor_innen war und ist. Die im digitalen Raum erkennbaren Biases verdeutlichen nicht nur die Notwendigkeit, sondern auch die Möglichkeit, diesen Verzerrungen entgegenzuwirken wie in diesem Impulsbeitrag gezeigt wird.  Für die Untersuchung von Frauen und ihrer(/n) Lebenswelt(/en) bieten Briefe als historische Quelle vielversprechende Perspektiven: Sie sind hierbei ""Ausdruck weiblichen Lebens und Erlebens"" (Barbara Becker-Cantarino). Gleichwohl sind die Vorüberlegungen und Modellierungsebenen, gerade auch in der Arbeit mit digitalen Methoden, entscheidend, um diese Zugänge über Briefe zu finden. Anhand des Netzwerks der frühromantischen Korrespondenzen soll kurz diskutiert werden, wie mit diesen Herausforderungen in Bezug auf Modellierung von Gender umgegangen werden kann und welche Chancen für die Forschung zu Frauen um 1800 dadurch entstehen. Die Überprüfung und Aktualisierung von Klassifikationssystemen und Normdateien ist entscheidend, um die Gender Data Gap zu schließen und adäquate, inklusive Repräsentationen von Gender und Diversität zu gewährleisten. Dafür müssen historische Bias und zeitgenössische Definitionen verstanden, Regelwerke analysiert und neue Modelle entwickelt werden. Der Impulsbeitrag zeigt, wie diese Modelle mit bestehenden Infrastrukturen verbunden werden sollten, um die fortlaufende Nutzung alter Klassifizierungen zu ermöglichen, Konsistenz zu gewährleisten und zukünftige Anforderungen zu antizipieren. An der Berlin-Brandenburgischen Akademie der Wissenschaften entstehen zahlreiche digitale Editionen, die fast alle um namenhafte Männer wie Aristoteles, Alexander von Humboldt und Jean Paul konzipiert wurden. Da sich hingegen kein einziges Projekt explizit einer weiblichen Protagonistin widmet, schlagen TELOTA und die Frauenvertretung der BBAW sieben Schritte zur Überwindung der Gender Data Gap (Jahnke et al. 2023) vor, die der Beitrag schlagwortartig thematisieren wird. Der Schritt der Modellierung von Geschlecht in den TEI-Daten erfolgt seit kurzem mit der Software ediarum (Dumont/Fechner 2014/15) und steigert nicht nur die Findbarkeit und Sichtbarkeit von Frauen und anderen marginalisierten Personengruppen, sondern macht die Kategorie ""Geschlecht"" auch in Zusammenhang mit anderen Faktoren aus intersektionaler Perspektive erforschbar (Neuber et al. 2024, 3-4). Jana-Katharina Mende, Martin-Luther-Universität Halle-Wittenberg, Literaturwissenschaft; Claudia Resch, √ñsterreichische Akademie der Wissenschaften Wien, Austrian Centre for Digital Humanities and Cultural Heritage, Abteilung Literatur- und Textwissenschaft."
2025,DHd2025,KETSCHIK_Nora_Netzwerkanalysen_narrativer_Texte___ein_Vorgeh.xml,Netzwerkanalysen narrativer Texte - ein Vorgehensmodell,"Nora Ketschik (Universität Stuttgart, Deutschland)","Netzwerkanalyse, Figuren, Methode","Annotieren, Netzwerkanalyse, Literatur, Methoden, benannte Entitäten (named entities)","Die soziale Netzwerkanalyse ist in den Computational Literary Studies (CLS) seit mehreren Jahrzehnten als eine Methode etabliert, mit der Figurenbeziehungen in verschiedenen Textgattungen exploriert und analysiert werden. Der Fokus liegt dabei auf dramatischen Untersuchungsgegenständen (vgl. z.B. Szemes und Vida, 2024; Trilcke, 2013; Trilcke et al., 2024; Krautter und Vauth, 2023; Viehhauser, 2023); die Verwendung der Netzwerkanalyse für narrative Texte ist hingegen vergleichsweise selten. Dies liegt m.E. insbesondere daran, dass es um ein Vielfaches voraussetzungsreicher ist, die für Netzwerkanalysen benötigten Daten aus narrativen Texten zu extrahieren, als dies für Dramen der Fall ist. Während Netzwerkanalysen dramatischer Texte i.d.R. auf den Nebentextangaben basieren und daraus szenenbasierte Kookkurrenzen ableiten, bedarf es mehrerer komplexer Schritte, um die gleiche Art der Information aus Erzähltexten zu extrahieren. Die Mehrarbeit resultiert nicht nur aus komplexen Tasks wie Entitätenreferenzerkennung und Koreferenzauflösung, sondern auch aus dem Umstand, dass die relevanten Informationen mit anderen Aspekten auf Die ursprünglich aus den Sozialwissenschaften stammende Methode der (sozialen) Netzwerkanalyse wird bereits seit vielen Jahren in den CLS für die Analyse von Figurenrelationen, darunter Informations- und Machtstrukturen (Vauth, 2019; Krautter und Vauth, 2023), zur Klassifikation von Figurentypen (Krautter et al., 2020; Vauth, 2023) oder zur Unterscheidung dramatischer Formen (Trilcke, 2013; Szemes und Vida, 2024; Viehhauser, 2023) eingesetzt. Dem Gros der Beiträge ist gemein, dass sie sich dabei erstens auf dramatische Texte und zweitens auf die Szenen- oder Gesprächsstrukturen konzentrieren. Dies liegt u.a. daran, dass die notwendigen Informationen über die Textstruktur leicht greifbar sind: Die szenische Einteilung im Drama gibt Einheiten für Figurenkonfigurationen vor. Eine solch direkte und intuitive Datenerhebung ist für narrative Texte nicht möglich 'weswegen es nach wie vor keinen gleichermaßen etablierten Zugang zur Erfassung von Kookkurrenzen zwischen Figuren gibt. Rochat und Kaplan (2014) verwenden beispielsweise einen Index bestehend aus Eigennamen und Seitenzahlen, um kookkurrenzbasierte Figurennetzwerke zu Rousseaus Autobiographie zu erstellen; sie nehmen immer dann eine Figurenrelation an, wenn zwei Figuren in einem Kontext von drei Seiten gemeinsam genannt werden. Argawal et al. (2012) stützen sich hingegen auf ein an Zeitungstexten entwickeltes Konzept von Die Beispiele zeigen, dass eine große Varianz dahingehend besteht, (erstens) welche Arten von Referenzausdrücken, (zweitens) welche Arten von Relationen, und (drittens) auf welche Weise die Informationen erfasst werden. Darüber hinaus werden Kontexte der Figurenvorkommen (z.B. innerhalb vs. außerhalb von Figurenrede) und Einflüsse von Es ist anzunehmen, dass die fehlende Integration verschiedener, für die Erfassung von Figurenvorkommen und -relationen relevanter Aspekte in der Komplexität dieser Teilaufgaben begründet liegt. So wäre 'analog zum Drama 'die Einteilung eines Textes in ""Szenen"", wie sie Gius et al. (2019a) vorschlagen, ein naheliegender Schritt für die Extraktion von Kookkurrenzen, der aber seinerseits extrem voraussetzungsreich ist, da er auf komplexen Kategorien wie Raum und Zeit/Chronologie aufbaut. Das hier vorgestellte Vorgehensmodell zur Extraktion von Kookkurrenznetzwerken aus narrativen Texten baut auf Analysen zu mittelhochdeutschen Romanen (Ketschik, 2024) auf und wurde im Rahmen des vorliegenden Beitrags an anderen narrativen Texten in neuhochdeutscher Sprache weiterentwickelt und erprobt. Für die Analysen und Statistiken wurden Texte aus dem Deutschen Romankorpus (DROC, Krug et al., 2018) verwendet, in denen Annotationen zu Figuren (inkl. Koreferenzresolution) und Figurenrede enthalten sind. Die Textstatistiken vermitteln einen Eindruck davon, inwiefern die Wahl des Figurenreferenzausdrucks, die Segmentierungsgröße und die Kontexte von Figurenreferenzen die resultierenden Netzwerke beeinflussen. Tabelle 1 zeigt, dass Eigennamen mit durchschnittlich 14% und Werten zwischen knapp 10% und gut 20% den geringsten Anteil der Figurenreferenzen im Textkorpus ausmachen, gefolgt von Appellativen mit durchschnittlich 18%. Die häufigsten Referenzausdrücke sind Pronomina, wobei ihr Anteil zwischen 57% und 77% liegt. Homodiegetische Erzählungen (hier Die Verteilung ist aber innerhalb eines Textes figurenspezifisch. In Die Wahl des Referenzausdrucks hat konsequenterweise gravierende Auswirkungen auf das resultierende Netzwerk. Exemplarisch seien zwei Netzwerke zu einem Auszug aus Fontanes Neben der Wahl der Referenzausdrücke hat die Segmentgröße als Grundlage für Kookkurrenzen einen entscheidenden Einfluss auf die Netzwerke. Die Größe der Segmentierung hängt (auch) mit der Wahl der Referenzausdrücke zusammen. Werden z.B. keine Pronomina berücksichtigt, ist es sinnvoll, die Segmente zu vergrößern. Welche Segmentgröße angemessen ist, lässt sich nicht pauschal beantworten. Hinweise können die Abstände zwischen Pronomen und Antezedent geben, da etwa ein Netzwerk ohne pronominale Referenzen den ""Weg"" bis zur nächsten nicht-pronominalen Referenz (Appellativ oder Eigenname) überbrücken muss. Die Abstände sind wiederum stark textabhängig (und figurenspezifisch), beispielsweise stehen in Fontanes Das Beispielnetzwerk zu Die fehlende Differenzierung zwischen Kontexten der Figurennennungen führt nicht nur dazu, dass im Netzwerk ggf. Relationen zwischen Figuren visualisiert werden, die in der Handlung nie kookkurrieren, sondern auch, dass (etwa innerhalb von Figurenrede) erwähnte Figuren die Netzwerkmetriken (z.B. die Netzwerkgröße, Zentralitätswerte, Dichtemaße) in hohem Maß beeinflussen. So umfasst das Netzwerk zum Die verschiedenen Aspekte, die für eine reflektierte Netzwerkanalyse von narrativen Texten relevant sind, werden nun in einem Vorgehensmodell zusammengefasst (Abb. 3). Das Modell kann unabhängig davon eingesetzt werden, ob die Schritte durch manuelle Annotation oder (teil-)automatisch umgesetzt werden. Bestimmte Arbeitsschritte (v.a. III–V) können je nach Texteigenschaften oder Untersuchungsfrage ggf. wegfallen. Die einzelnen Schritte werden im Folgenden kurz erläutert.   Das hier vorgestellte Vorgehensmodell umfasst relevante Schritte für die Extraktion kopräsenter Figuren aus narrativen Texten. Ein Hauptanliegen des Beitrags ist es, aufzuzeigen, welche Aspekte bei der Netzwerkanalyse narrativer Texte (potenziell) eine Rolle spielen, und diese in die Datenerhebung zu integrieren 'oder, sollte dies nicht möglich sein, zumindest das Bewusstsein für deren Einfluss auf die netzwerkanalytischen Daten zu schärfen. Zweifelsohne können nicht alle Sonderfälle in einem möglichst generischen Modell berücksichtigt werden; vielmehr dient das Modell als methodische Grundlage, die dazu befähigt, Kookkurrenznetzwerke aus Erzähltexten zu erstellen, die für Einzelfälle aber angepasst oder ergänzt werden muss. Durch seine Modularität und die ""Filter""-Schritte soll das Modell für viele Fragestellungen und Untersuchungsgegenstände einsetzbar sein."
2025,DHd2025,BROOKSHIRE_Patrick_Daniel_Warum_wird_was_wie_klassifiziert_.xml,Warum wird was wie klassifiziert?   Scalable Reading + Explainable AI am Beispiel historischer Lebensverläufe,"Patrick Daniel Brookshire (Akademie der Wissenschaften und der Literatur | Mainz / Universität zu Köln, Deutschland)","Scalable Reading, eXplainable AI, Historische Biographien, Sentimentanalyse","Entdeckung, Annotieren, Bereinigung, Visualisierung, Methoden","Eine Klassifikation von Textabschnitten ist im DH-Kontext häufig ""under construction"", da vorliegende Verfahren viele Varianten aufweisen, aber nicht domänen-spezifisch genug sind. Deshalb ist stets eine Evaluation mit dem konkreten Datensatz nötig. Zudem sind bei besonders kontextabhängigen Tasks, für die Sentimentanalysen ein verbreitetes Beispiel sind, Deep-Learning-Methoden performanter, dafür aber weniger interpretierbar als bspw. Lexikon-basierte Verfahren (Singh und Singh, 2021; Schmidt et al., 2022; Rebora et al., 2023). Eine mögliche Lösung sind Explainability-Modelle wie In einer Pilotstudie werden Daten des retrodigitalisierten biographischen Nachschlagewerks Aus technischer Sicht umfasst das vorgestellte Verfahren die fünf Module  Durch die Markierung des neutralen Bereichs mit gängigen Schwellenwerten (¬± 0,05; vgl. Hutto und Gilbert, 2014, 224) wird deutlich, dass das Korpus einen leicht positiven Trend mit nur wenigen Schwankungen aufweist. Allerdings zeichnet ein Reinzoomen im Sinne des Scalable Readings auf 36 zufällig ausgewählte Einzelbiographien ein deutlich anderes Bild:  Die Variabilität ist bei einem individuellen Genre wie Biographien nicht überraschend, aber bei der Analyse größerer Datenmengen wegen der Tendenz von Durchnittswerten zum neutralen Bereich (vgl. Jockers, 2015) nur durch Skalenwechsel zu beobachten. Aufgrund der nicht perfekten automatischen Annotationen kann auch diese Einzelansicht mitunter täuschen, weshalb im vorgelegten Verfahren eine am Close-Reading orientierte  So gehen die hier durch Farbsättigung visualisierten Ergebnisse des (aus Performance-Gründen gewählten) Explainability-Modells von Chefer et al. (2021) über die reinen Klassifikationswahrscheinlichkeiten hinaus. Denn sie illustrieren, dass die negativ-Labels auf Subword-Tokens wie ""nur"", ""muß"" und ""Aufhebung"" zurückgehen, aber den gegebenen neutral formulierten philosophischen Abschnitt nur bedingt abbilden. In diesem Fall wäre also eine manuelle Bereinigung der entsprechenden Labels sinnvoll und dank der mitvisualisierten Position im Gesamtdatensatz durch Quellen-ID und Satznummer auch möglich. Zudem ist auch ein erneutes Finetuning mit diesen Datensätzen denkbar. Das vorgestellte Verfahren zeigt, wie sich Fehlklassifikationen, die für nachgelagerte Analyseschritte besonders relevant sind, durch eine Kombination aus Scalable-Reading- und Explainability-Ansätzen gezielt identifizieren und ggf. manuell korrigieren lassen. Denn zur Steigerung der Validität ist es durch Aggregierungsschritte nicht immer nötig, alle Datensätze zu bereinigen 'und je nach personeller und technischer Ausstattung auch nicht immer umsetzbar. Auch wenn das Verfahren hier am Beispiel von Sentimentwerten illustriert wurde, ist es grundsätzlich auf beliebige Klassifikationsaufgaben anwendbar. Daher ist derzeit ein entsprechender Ausbau in Richtung nominal-skalierter Kategorien ""under construction"". Zudem ist angedacht, die Modularisierung so konsequent umzusetzen, dass künftig neben beliebigen Klassifikationsmodellen auch bei den übrigen Modulen beliebige Komponenten angedockt werden können, um dem Variantenreichtum gerecht zu werden."
2025,DHd2025,KUPIETZ_Marc_National_Library_as_Corpus__Introducing_DeLiKo_.xml,National Library as Corpus: Introducing DeLiKo@DNB 'a Large Synchronous German Fiction Corpus,"Marc Kupietz (Leibniz-Institut für Deutsche Sprache, Germany); Peter Leinen (Deutsche Nationalbibliothek, Germany); Nils Diewald (Leibniz-Institut für Deutsche Sprache, Germany); Philippe Gen√™t (Deutsche Nationalbibliothek, Germany); Rebecca Wilm (Leibniz-Institut für Deutsche Sprache, Germany); Andreas Witt (Leibniz-Institut für Deutsche Sprache, Germany); Rameela Yaddehige (Leibniz-Institut für Deutsche Sprache, Germany)","corpus, literature, fiction, contemporary, linguistic annotation, metadata, corpus analysis, IPR, library as corpus","Umwandlung, Sammlung, Annotieren, Literatur, Text, virtuelle Forschungsumgebungen","Fiction books are weakly represented in the German Reference Corpus DeReKo (Kupietz et al. 2010, 2018). This is primarily due to the tremendously higher costs associated with licensing and converting raw fiction data into TEI-encoded XML format, compared to newspaper articles (Kupietz et al. 2014, p. 2). In the following, we discuss how we addressed these challenges and successfully created a large extensible corpus of recent fiction books. Linguistics and literary studies both face the challenge that their research data is affected by third-party rights. Obtaining transferable, uniform licenses for German fiction books is particularly costly as no licensing models for non-expressive use (previously also called ""non-consumptive use"", see Kamocki 2018) of entire texts as primary research data are generally established. Moreover, individual author permissions are often required, since the use of texts as research data is not covered by standard licensing agreements between authors and publishers. Our solution to this challenge consists of two main pillars: The first leverages ¬ß 14 of the German National Library Act (DNBG) requiring submission of all digitally published media works to the DNB. The second pillar is part of the general strategy adopted for DeReKo to address legal issues through infrastructural means, following Jim Gray's (2003) famous principle: To address the second challenge, and to make conversion into high-quality TEI-XML encoded corpora feasible, we ignored PDF ebooks and limited our focus to the 273,976 books available in the XML-based EPUB format and drew a 10% random sample from these as a first step, stratified by year of publication, resulting in a sample of 26,091 ebooks. As a second step, on the occasion of the 20th anniversary of the German Book Prize in October 2024, we added all 362 digitally available longlisted titles from the past two decades to the corpus, in cooperation with the German Publishers and Booksellers Association. To convert the data to the TEI I5 format (Lüngen and Sperberg-McQueen 2012) used by DeReKo, we applied XSLT 3.0 stylesheets in three passes, via the Saxon XSLT processor and GNU Make, using the DNB SRU API to retrieve consistent metadata, a heuristic genre classifier based on this, and a MALLET (McCallum 2002) based implementation of the standard DeReKo topic domain classifier (Weiß 2005; Klosa, Lüngen, and Kupietz 2012). In subsequent steps, the TEI-XML data was converted to KorAP-XML format and annotated for POS and lemma using the TreeTagger (Schmid 1994), for POS and morphosyntactic properties using MarMoT (Mueller et al. 2013), and dependencies using MaltParser (Nivre et al. 2007). These tools were selected due to their good balance between accuracy and performance. The entire conversion and annotation process was completed in 48 hours on a Linux server with 96 cores and 1.5 TB of RAM. The composition of the resulting corpus, categorized by genres and publication years, is presented in Figure 1. Genre classifications were derived from the DNB metadata using string matching heuristics.  DeLiKo@DNB, currently comprising 2.02 billion words, is freely accessible through the website We aim to regularly expand the corpus by incorporating newly published books. Additionally, we plan to enhance the search and analysis capabilities by integrating advancements from the long-term KorAP project, including updates to the user interface and client libraries. Further additions, improvements, and extensions, concerning e.g. the addition of books, text classifications, or annotation layers, will be driven by the demands of the user communities engaging with DeLiKo@DNB. "
2025,DHd2025,KELLNER_Nils_Literaturgeschichte__under_construction____was_.xml,"Literaturgeschichte ""under construction"" 'was können die Computational Literary Studies beitragen? Ein Panel zur digitalen Untersuchung von Raum in der Literatur","Berenike Herrmann (Universität Bielefeld, Deutschland); Daniel Kababgi (Universität Bielefeld, Deutschland); Marc Lemke (Universität Rostock, Deutschland); Nils Kellner (Universität Rostock, Deutschland); Ulrike Henny-Krahmer (Universität Rostock, Deutschland); Fotis Jannidis (Julius-Maximilians-Universität Würzburg, Deutschland); Katrin Dennerlein (Julius-Maximilians-Universität Würzburg, Deutschland); Matthias Buschmeier (Universität Bielefeld, Deutschland)","Computational Literary Studies, Raum, Literaturgeschichte","Computational Literary Studies, Raum, Literaturgeschichte","Im aktuellen Verhältnis derjenigen Literaturwissenschaften, deren methodischer Fokus nicht auf computergestützten Ansätzen liegt, und den Computational Literary Studies (CLS) zeigt sich ein Problem, das im Forschungsdesign vieler DH-Projekte erkennbar ist. Während erstere die Fragestellungen meist anhand literaturgeschichtlicher Kontexte entwickeln, stellen letztere deutlicher die Operationalisierung eines literarischen Phänomens, die Datenmodellierung, die Implementierung und das Finetuning computationeller Verfahren in den Mittelpunkt. Obwohl die zentralen Ziele der CLS darin liegen, ""to explain, or to provide, general laws of literature, and even of history and culture"" (Bode, 2023, 14), bleibt die Frage, ob einer literaturhistorisch fundierten Kontextualisierung oft zu wenig Raum beigemessen wird. Doch auch der Status von Historisierung in den Literaturwissenschaften generell wird debattiert; nicht nur Daniel Fulda sieht Literaturgeschichtsschreibung im starken Sinne ""wissenschaftslogisch [...] allenfalls am Rande des Fachs"" (Fulda, 2014, 104). Dass eine bewusste Historisierung jedoch vorteilhaft wäre, kann auch das Modell zur Beschreibung der Komplexität computationeller Textanalysen (Gius, 2019) zeigen. Insofern stellt sich die Frage, welchen Stellenwert literaturhistorische Grundlagen in Projekten der CLS 'oder anderen literaturwissenschaftlichen Bereichen 'derzeit einnehmen und zukünftig sollten. Ziel des Panels ist es, dieses Verhältnis sowohl allgemein mit Blick auf die Planung eines Forschungsdesigns als auch ausgehend von Perspektiven der Literaturwissenschaft sowie von Praxis-Bezügen aus zwei DH-Projekten zu erörtern. Nicht zuletzt scheint eine diachrone Perspektive in CLS-Projekten oftmals naheliegend, etwa, wenn Publikationsdaten als Metadaten vorliegen und auf der Suche nach Mustern historischer Wandel explorativ modelliert wird. Thematisch soll ""Raum in literarischen Texten"" als exemplarisch für derartige Überlegungen fruchtbar gemacht werden, zumal hierfür bereits eine breite theoretische Fundierung (allein die Bezeichnung eines spatial turns zeigt dies anschaulich (Bachmann-Medick, 2006)) und erste Ergebnisse computationeller Textanalysen vorliegen. Das Panel wird durch fünf verschiedene Panel-Teilnehmer:innen und deren Perspektiven zu dem Thema diskutiert, die im Folgenden schlaglichtartig vorgestellt werden. Diese fünf Perspektiven werden unterstützt durch die Moderation von Nils Kellner und die Mitarbeit bei der Konzeption durch Ulrike Henny-Krahmer und Daniel Kababgi. Die Literaturgeschichte befindet sich in einer schwierigen Lage: aus verschiedenen Lagern werden ihr tiefgreifende theoretische Probleme unterstellt. Nach Wellek (1973) sind diese in drei große Stoßrichtungen zu teilen: Der Literaturbegriff sei im Kern autonom und würde damit als der geschichtlichen Kontextualisierung enthobenes Objekt gelesen werden. Zweitens, aus gegensätzlicher Perspektive: die Literaturgeschichte wird von Kulturwissenschaften vereinnahmt und literarische Aspekte ausgeklammert. Drittens wird einer primär an den literarischen Gegenständen orientierten Literaturgeschichte vorgeworfen, sich in einer Geschichte literarischer Formen zu erschöpfen, die andere diskursive Kontexte ausblendet. Darüber hinaus hat Mario Valdés (2002) konstatiert, dass es keine Geschichte geben kann, die annähernd alle Kontexte abbilden kann und damit auf einen zentralen Vorwurf reagiert, dem sich Literaturgeschichte ausgesetzt sieht: Sie sei in ihrem ausgewerteten Material limitiert und in der Auswahl ästhetisch-normativ oder gar ideologisch eingeengt. Die CLS bieten sich der Literaturgeschichte als neues methodisches Verfahren an, das in der Lage ist, größere Textkorpora schneller zu analysieren, und damit sowohl dem Vorwurf der Eingeschränktheit als auch der Kanonzentrierung zu entgehen. Damit antworten die CLS aber vor allem auf die quantitativen Herausforderung der Literaturgeschichte. Verfahren der Literaturgeschichte, wie jede andere Form der Historisierung, bestehen aus der kontrollierten Erhebung von Daten einerseits und der anschließenden Überführungen der Ergebnisse in eine narrativierte Darstellung andererseits. Mindestens genauso entscheidend und herausfordernd wie die kontrollierte Erhebung von Daten sind die synthetisierenden Verfahren der Literaturgeschichte. Wie wird was mit wem verknüpft? Was lässt sich daraus schließen? Erst aus diesen Verknüpfungsleistungen entsteht die geschichtliche Darstellung. Viel Energie wird in den CLS dafür aufgewendet auf Seiten der Datenerhebung und -visualierung zu möglichst kontrollierten und damit 'unter den gegebenen Prämissen 'objektiven Befunden zu kommen. Diese Befunde aber generieren noch nicht ihre Geschichte. Der Übergang zwischen objektiver Datenerhebung und geschichtlicher Darstellung dieser Daten scheint mir in den CLS strukturell analog zu jeder anderen Form der Literaturgeschichte zu sein: es stellen sich immer unmittelbar Fragen, in welche Kontexte denn nun ein spezifischer Befund, etwa zur Semantisierung literarischer Räume in einer bestimmten Zeit, eingebunden werden sollen. Da diese größeren diskursiven Kontexte i.d.R. nicht selbst Gegenstand der Datenerhebung waren, bedarf es narrativer Verknüpfungsverfahren. Damit kehren jene Probelme von Literaturgeschichte wieder, die zur ihrer prinzipiellen Infragestellung in der Disziplin geführt haben (Buschmeier, 2014). Werden die CLS von vielen noch immer als Provokation der klassischen Literaturgeschichte gesehen, die dieser die Limitiertheit der Reichweite ihrer Aussagen in der Begrenzheit ihres ausgewerteten Materials vor Augen führt, so gilt festzuhalten: Die Provokation der CLS durch die Literaturgeschichte besteht, so die These, in der Markierung ihrer methodischer Grenze bzw. ihres blinden Flecks: der Überführung von historischen Daten in Erzählungen von Geschichte. Ein guter Grund also, zusammen zu denken und zu arbeiten und zu schreiben. Das Interesse der Geistes- und Kulturwissenschaften am Raum ist seit Jahrzehnten ungebrochen (Alidou, 2002; Ryan, 2019; Ryan et al., 2016; Caracciolo et al., 2022; Leetsch et al. 2023). Der Raum ist zentral für die Orientierung des Lesers in der erzählten Geschichte und die Bedeutung eines Textes. Jede ausführliche Charakterisierung und Funktionalisierung von Raum und Bewegung erfolgt intentional und ist wichtig für die Raumanalyse. Die Gestaltung des konkreten Raums und die Bewegung der Figuren sind oftmals wesentliche Bedeutungsträger, die sich kultur- und mentalitätsgeschichtlich interpretieren lassen. Für die Computational Literary Studies (CLS) ergeben sich zwei Herausforderungen: Erstens, die Frage der Operationalisierung und Modellierung von literarischen Phänomenen und zweitens, die Einbindung literaturhistorischer und theoretischer Grundlagen. Die Analyse von Raum und Mobilität in fiktionalen Welten dient dabei als konkretes Vehikel, um über Methodik und Praxis der Literaturgeschichtsschreibung ins Gespräch zu kommen. Exemplarisch sollen diese Zusammenhänge an der literaturgeschichtlichen und computationellen Modellierung von zwei Aspekten des erzählten Raumes diskutiert werden: 1) Die Erkennung von Schauplätzen, das heißt Räumen der erzählten Welt, in denen Ereignisse und Wahrnehmungen von Figuren verortet werden (Dennerlein, 2009). 2) Die Charakterisierung von räumlichen Gegebenheiten und Bewegungen mit den Parametern Art, Umfang und Spezifikation. Vorgestellt werden synchrone und diachrone Fragestellungen, die sich untersuchen ließen, wenn man diese Aspekte aus großen Korpora literarischer Texte extrahieren könnte. Die Literaturwissenschaft hat ein zunehmend skeptisches Verhältnis zur Literaturgeschichte entwickelt: Is literary history possible, fragte David Perkins 1992 im Titel seines Buchs und bejahte die Frage. Buschmeier, Erhart und Kauffmann brechen gar über allen Literaturgeschichten der Gegenwart den Stab: ""Die gegenwärtige Praxis der Literaturgeschichtsschreibung ist demnach nicht nur widersprüchlich, sondern auch theoretisch prekär"" (Buschmeier, Erhart und Kaufmann 2014, 3). Diese Position ist, mehr oder weniger deutlich, eng mit einem höchst normativen Begriff von Literatur verbunden, der oftmals einen großen Teil der gelesenen fiktionalen Texte der Vergangenheit ausschließt - zugunsten des Interesses am (großen) Einzelwerk. Dem steht nun gerade die Arbeit der meisten im Bereich der Computational Literary Studies entgegen: Sie interessieren sich für langfristige und großflächige Entwicklungen. In dieser Perspektive verschwindet der Unterschied zwischen Goethe und Kotzebue, zwischen Kafka und Hans Dominik vorübergehend, um später dann durch eine Kontextrelationierung wieder eingeführt werden zu können. Der Literaturgeschichte, so verstanden, kann man Fragen, die lange Zeit als unbeantwortbar zur Seite geschoben wurden, etwa nach dem Verhältnis von Binnendynamik der literarischen Entwicklung (inwieweit wird wird Literaturgeschichte von literaturinternen Dynamiken geprägt) und Kontextwirkungen (inwieweit reagieren literarische Entwicklungen auf ideengeschichtliche oder sozialhistorische Dynamiken?), wieder stellen und an ihrer Beantwortung arbeiten. Durch diese neuen Langzeit-Perspektiven werden nicht zuletzt Anschlüsse an frühere Projekte der Literaturgeschichte möglich. Im DFG-Projekt ""Computational Approaches to Narrative Space in 19th and 20th Century Novels"" (CANSpiN) werden verschiedene Ansätze der computergestützten Analyse von Raum erprobt. Mit der Annotationsrichtlinie CANSpiN.CS1 ist beabsichtigt, die Räumlichkeit des Textes an der Textoberfläche sichtbar zu machen: Raumreferenzielle Ausdrücke, deren Menge, Verteilung, Auswahl und Korrelation bilden das räumliche Vokabular eines Textes (Orte, Bewegungen, Dimensionierungen, Richtungen, Positionierungen). Für deren Erkennung ist weder die Analyse der semantischen Tiefenstrukturen des Textes nötig (wie beispielsweise in der Erzähltextanalyse), noch handelt es sich um das bloße Erfassen von Formativen. Stattdessen werden kotextuelle Zusammenhänge innerhalb von Sätzen berücksichtigt. Dieser Ansatz kommt dem Anspruch nach Operationalisierung entgegen und nutzt dafür vorhandene technische Lösungen (Large Language Models). Das räumliche Vokabular verstehen wir wie auch die narrative Struktur grundsätzlich als erfassbare, historisch bedingte Eigenschaften von Texten und damit als der literaturhistorischen Analyse gleichermaßen für quantitative wie qualitative Methoden zugänglich. Der Einsatz von CANSpiN.CS1 ist einerseits explorativ: Wir erwarten Muster, die zuvor noch nicht beschrieben worden sind. Sie bedürfen der Evaluation und Interpretation unter Bezugnahme auf literaturwissenschaftliche Wissensbestände. Andererseits finden Korpusaufbau und Richtliniendefinition hinsichtlich literaturhistorischer Fragestellungen statt und setzen damit an vorhandenen Forschungsprozessen an: Nehmen Raumdarstellungen in Romanen vom 19. zum 20. Jahrhundert zu (Schumacher, 2023, 207‚Äì217)? Gibt es ähnliche Muster der Räumlichkeit in spanisch- und deutschsprachigen Romanen im Zuge des nation buildings des 19. Jahrhunderts? Beim Spatial Distant Reading werden fiktive und fiktionale Darstellungen von Raum als zentrale Kategorie der Sinngebung untersucht (Lefebvre, 1974): dies betrifft die phänomenologische Konstruktion der fiktionalen Welten - etwa als ‚Äòurban‚Äô (Bologna, 2020) - und auch die sozial-kulturelle Dimension erkennbarer nationaler und transnationaler Räume (Wilkens, 2021). Unser Projekt untersucht die affektiven Topologien deutschschweizerischer Literatur in einem Zeitraum zwischen 1854 und 1930, indem verschiedene Arten der räumlichen Darstellung auf Differenzen wie Kultur/Natur, Stadt/Land (Rehm, 2015) und die Rolle von Interieurs (Herrmann et al., 2022) sowie die Rolle der (alpinen) Berge in der Gestaltung einer spezifisch ‚ÄòSchweizer‚Äô Literatur (Zimmer, 1998) untersucht werden. Eine wichtige Ressource ist eine Liste räumlicher Begriffe (derzeit N=187.421 Entitäten), die räumliche named entities, aber auch non-named entities urbaner, ländlicher und naturbezogener Art enthält (Grisot und Herrmann, 2023). Zudem arbeiten wir an der automatisierten Erkennung räumlicher Entitäten mittels distributioneller Semantik (Herrmann et al., 2022) und maschinellen Lernens (Kababgi et al., eingereicht). Literaturhistorisch fragt das Projekt nach den sozialhistorischen Bedingungen, aber eben auch nach den textuellen Merkmalen einer ""Schweizer"" Literatur auf deutsch. Formiert sich diese über unterschiedliche Gattungen und Subgattungen hinweg als ""Nationalliteratur""? Welche Rolle spielen prestigeträchtig wahrgenommene Autor*innen, wie etwa Jeremias Gotthelf oder Gottfried Keller, aber auch Johanna Spyri? Die Gattung der Dorfgeschichte spielt seit Mitte des 19. Jahrhunderts eine wichtige Rolle, nicht nur im deutschsprachigen Raum (Twellmann, 2019). Hier kommen systematische, sozial- und literaturhistorische Aspekte zusammen, die unsere diachrone Analyse und auch Vorhersagen über die affektive Enkodierung von Landschaft und Raum in den literarischen Texten informieren. Die Panel-Teilnehmer:innen werden nach der Hinführung zur Problemstellung des Panels in 5-minütigen Beiträgen ihre fach- bzw. projektspezifische Sicht auf die aufgeworfenen Fragen darlegen. Ansätze, Erkenntnisse, Probleme und offene Fragen zur Analyse von Raum in der Literatur dienen als konkretes Vehikel dazu, über Methodik und Praxis der Literaturgeschichtsschreibung zwischen close und distant reading, nomothetischer und idiographischer Forschung, quantitativen und qualitativen Ansätzen, den Computational Literary Studies und Ansätzen einer historisch perspektivierenden Literaturwissenschaft ins Gespräch zu kommen. Die √ñffnung der Diskussion vom Raum-Thema ausgehend zu anderen thematischen Feldern der Literaturwissenschaft ist entsprechend beabsichtigt. Im Anschluss an die aufgeworfenen Fragen wird die 30-minütige Diskussion für das Publikum geöffnet. Es wird erwartet, dass das Panel die Sensibilität von CLS-Forscher:innen für einen stärkeren Einbezug bestehender literaturgeschichtlicher Forschung und umgekehrt diejenige der historisierenden Literaturwissenschaft gegenüber computationellen Zugriffen erhöht. So soll das Ziel eines Dialogs zwischen computergestützter und anderer historisierender Forschung verfolgt werden, der die literaturwissenschaftliche Theoriebildung stärkt."
2025,DHd2025,PICHLER_Axel_Die_deutschsprachige_Kurzgeschichte_nach_1945__.xml,Die deutschsprachige Kurzgeschichte nach 1945. Skizze einer hypothesen-geleiteten Operationalisierung.,"Axel Pichler (Universität Stuttgart, Deutschland)","Operationalisierung, theoriegeleitet, digitale Literaturwissenschaft","Inhaltsanalyse, Modellierung, Annotieren, Text","In der Datenanalyse lassen sich 'stark vereinfacht 'zwei Forschungsansätze unterscheiden: Ein theorie- bzw. hypothesengeleiteter Ansatz, der ausgehend von domänenspezifischen Theorien und deren Grundbegriffen diese auf das Datenmaterial ""projiziert"" und anschließend quantifiziert, und ein datengetriebener Ansatz, der Theorie durch ""massive amounts of data and applied mathematics"" (Anderson 2008) ersetzt und solcherart unbekannte Muster in den Daten freizulegen versucht. Diese beiden Ansätze kennzeichnen auch das Forschungs-Design der digitalen Literaturwissenschaft bzw. Computational Literary Studies, wobei in diesem Zweig der DH 'so mein Eindruck 'aktuell der datengetriebene Ansatz dominiert (vgl. Jannidis 2022). Diesen beiden Ansätzen entsprechen unterschiedliche Leitvorstellungen und Praktiken, wie traditionelle literaturwissenschaftliche Begriffe mit digitalen Mess- und Analysemethoden zusammengeführt werden. Diesen Prozess 'also die Überführung eines fachspezifischen theoretischen Begriffs in eine Messvorschrift 'bezeichnet man gemeinhin als Operationalisierung. Im Folgenden möchte ich anhand eines konkreten Beispiels aus der literaturwissenschaftlichen Gattungsgeschichte bzw. -theorie 'der deutschsprachigen Kurzgeschichte nach 1945 'zeigen, wie auf Basis eines bestimmten Operationalisierungsverständnisses ein derartiger Gattungsbegriff ""hypothesengeleitet"" für seine quantitative Analyse aufbereitet werden kann. ""Hypothesengeleitet"" bezeichnet dabei 'wie der Name nahelegt –, dass von gattungs- bzw. literaturgeschichtlichen (Hypo-)Thesen ausgegangen wird. Dabei verfolge ich zwei Ziele: Erstens möchte ich vorführen, wie ein komplexes literaturwissenschaftliches Konzept auf eine Art und Weise operationalisiert werden kann, welche die Rückführung der auf seiner Basis erzielten Messresultate in die nicht-digitale Literaturwissenschaft erlaubt. Voraussetzung dafür ist, so meine Vermutung, dass die einzelnen Teilschritte eines solchen Prozesses auf eine Art und Weise verbalisiert werden, sodass diese Verbalisierungen nicht nur in den domänenspezifischen Diskurs re-importiert werden können, sondern den Teilnehmenden an diesem Diskurs auch als plausibel erscheinen. Zweitens soll im Zuge dessen der Arbeitsablauf für jenes Verständnis von Operationalisierung, an dessen Explikation Nils Reiter, Benjamin Krautter und ich in den letzten Jahren in unterschiedlichen Konstellationen gearbeitet haben (Pichler/Reiter 2022; Den beiden Zielen entsprechend werde ich im Folgenden immer wieder die konkrete Operationalisierungspraxis unterbrechen, um die einzelnen Arbeitsschritte explizit zu machen und methodologisch zu reflektieren. Beginnen möchte ich diesbezüglich mit einer Rechtfertigung der Fallstudien-Auswahl: der deutschsprachigen Kurzgeschichte nach 1945. Diese eignet sich für die Realisierung der oben genannten Ziele unter anderem aus folgenden Gründen: Erstens ist die Kurzgeschichte gut erforscht (Marx 2005, Meyer 2014, Wenzel 2007). Die einschlägigen Monographien, Lehrbücher und Lexikaeinträge bieten einen guten Ausgangspunkt für die Operationalisierung ihrer zentralen Merkmale. Zweitens handelt es sich bei Kurzgeschichte um, wie der Name schon nahelegt, kurze Texte. Dies erleichtert deren Vollannotation. Drittens stellen 'dank des Aufkommens großer Sprachmodelle (LLMs) 'weder die Kürze der Texte noch etwaige Korpusgrößen mittlerweile ein Problem für die computergestützte Textanalyse dar, da es im Rahmen des sogenannten Während eine dem Stand der Forschung angemessene Analyse eines literaturwissenschaftlichen Begriffs sämtliche bzw. möglichst viele seiner Verwendungsweisen begriffsanalytisch erfassen sollte (Schröter et. al 2021), beschränke ich mich hier für Demonstrationszwecke auf die jüngere synoptische Forschung zur deutschsprachigen Kurzgeschichte 'd.s. die Arbeiten von Leonie Marx und Anne-Rose Meyer sowie den Eintrag im Reallexikon von Peter Wenzel –, um aus dieser die zentralen Kennzeichen der historischen Ausprägung der Gattung zu sammeln, ohne dabei deren Verhältnis eingangs definitorisch zu bestimmen. Fasst man diese zusammen, so lassen sich folgende gattungstypologische Merkmale festhalten: Die deutschsprachige Kurzgeschichte nach 1945 sei gekennzeichnet durch narrative Strategien der Reduktion, Selektion, Verdichtung und Begrenzung im Hinblick auf Handlung, das Figurenarsenal, Zeit und Ort, einen andeutenden oder verrätselnden Titel, einen unmittelbaren Einstieg in die Handlung sowie einen (mehrheitlich) offenen Schluss. Eine derartige Gattungsbeschreibung ist nicht als Definition im strengen Sinne zu verstehen. Sie vereint bloß jene Prädikate, welche die genannten Forschungsbeiträge als charakteristisch für die Kurzgeschichte nach 1945 erachten. Sie kann, zumindest im Rahmen eines Top-Down-Ansatzes, nicht am Stück operationalisiert werden, sondern nur über die Operationalisierung ihrer einzelnen Prädikate bzw. Merkmale. Wie sich diese Merkmale zum Gesamt-Begriff verhalten, ist in der jüngeren gattungstheoretischen Forschung umstritten. In ihr finden sich nicht nur unterschiedliche Begriffe des Begriffs (Zymner 2003), sondern auch Kritiken am Einsatz von vordefinierten, klassifikatorischen Begriffen in der Gattungsgeschichte (siehe z.B. Schröter 2024). Insofern liegt es nahe, das Verhältnis der einzelnen Textmerkmale zum Gesamt-Begriff fürs Erste offen zu halten. An die Stelle einer feststellenden bzw. festlegenden Definition tritt dann in einem ersten Schritt die am Stand der Forschung oder der historisch gegebenen poetologischen Reflexion orientierte Sammlung bzw. Auswahl der begriffsrelevanten Merkmale. Inwiefern diese eine klassifikatorische oder bloß graduelle Funktion im jeweiligen Gattungskonzept innehaben, ist dann in einem nächsten Schritt empirisch zu überprüfen. Ich werde mich im Folgenden ausschließlich auf den ersten Schritt dieses Prozesses, die Teiloperationalisierung, konzentrieren. Sie wird exemplarisch anhand eines der Merkmale vorgeführt, der Charakterisierung von Kurzgeschichten durch das Verfahren der Selektion, was 'so Anne-Rose Meyer (Meyer 2014, S. 20f.) 'bei der Gestaltung der Textanfänge von Kurzgeschichten dazu führe, dass diese durch Auslassungen gekennzeichnet seien. Idealiter handelt es sich bei dem Merkmal, das operationalisiert werden soll, bereits um ein Phänomen, für das ein etablierter Begriff vorliegt. In einem solche Fall ist in einem ersten Schritt die Bedeutung und Verwendung des Begriffes in der Forschung zu rekonstruieren und zu analysieren, um so zu einer feststellenden Definition des Begriffs zu gelangen. In sehr wenigen Fällen wird eine solche feststellende Definition ausreichend beobachtungssprachliche Hinweise enthalten, sodass sie ohne großen Aufwand in eine operationale Definition überführt werden kann. In vielen Fällen ist dies jedoch nicht der Fall 'zum Bespiel bei der Kennzeichnung des Erzähleinstieges der Kurzgeschichte qua Auslassung. Diese Kennzeichnung verweist weder auf einen etablierten literaturwissenschaftlichen oder linguistischen Begriff noch auf ein direkt auszeichenbares Textoberflächenphänomen. Um zu einem solchen zu gelangen, bedarf es der präzisierenden Begriffsarbeit. In der aktuellen Methodendiskussion der digitalen Literaturwissenschaft wird dabei oft für jenes Verfahren plädiert (Pichler/Reiter 2021, Gerstorfer/Gius 2023, Jacke 2023), das bereits Harald Fricke zum Leit-Verfahren bei der Arbeit am Zur Explikation der ""Auslassung"" bietet es sich an, einen Umweg über die Literaturtheorie zu gehen. Diese spricht bezüglich des notwendigen Auslassens von Informationen über die Ontologie erzählter Welten in Anknüpfung an Roman Ingarden auch von Unbestimmtheitsstellen. Fotis Jannidis hat Unbestimmtheit 'abseits des Kontextes der Ingard""schen Literaturtheorie 'als ""Mangel an Information in einer Öußerung"" (Jannidis 2003, S. 308) definiert. Nun ist aber 'und das ist nicht unbedeutend 'nicht jeder Informationsmangel in einer fiktionalen Erzählung handlungs- oder interpretationsrelevant. Bei der hier zu operationalisierenden ""Auslassung"" handelt es sich also um keinen rein deskriptiven Begriff. Greift man auf weitere der zuvor genannten Charakteristika der Kurzgeschichte zurück, kann die Unbestimmtheit ihrer Texteröffnungen folgendermaßen bestimmt werden: Die Texteröffnung einer Kurzgeschichte ist unbestimmt, wenn sie durch einen Mangel an handlungsrelevanter Information in Bezug auf eine zentrale Figur, den Gegenstand, den Ort oder die Zeit der Handlung gekennzeichnet ist. Wie lässt sich eine derartige Bestimmung in eine operationale Definition überführen? Eine solche hat jene Indikatoren an der Textoberfläche zu nennen, die eine Beleg-Funktion für das Vorliegen des besagten Sachverhaltes besitzen. Ein solcher Indikator für Bestimmtheit bzw. Unbestimmtheit ist im Deutschen die Definitheit von Nominalphrasen. Hadumond Bußmann definiert sie im Für die Operationalisierung von ""Auslassungen"" am Anfang von Kurzgeschichten folgt aus dieser Bestimmung, dass von einer ""Auslassung"" gesprochen werden kann, wenn eine zentrale Figur, der Gegenstand, der Ort oder die Zeit der Handlung im Text durch eine definite Nominalphrase eingeführt werden, deren Referenz und Eigenschaften zuvor im Text noch nicht bestimmt wurden. Um diese Bestimmung von ""Auslassung"" in eine finale operationale Definition und im Anschluss daran in eine Messvorschrift zu überführen, ist es notwendig die Bestimmtheitsgrade von zentraler Figur, Gegenstand, Ort und Zeit der Handlung in Eingangssätzen weiter auszudifferenzieren und dabei so zu gewichten, dass denjenigen Eingangssätzen von Kurzgeschichten, die aufgrund der hohen Frequenz von definiten Nominalphrasen einen hohen Grad an Unbestimmtheit besitzen, ein niedriger Gesamtwert zugeschrieben wird. Neben definiten Nominalphrasen können auch noch indefinite Nominalphrasen sowie Pronomen und Eigennamen auf Figur, Gegenstand, Ort und Zeit der Handlung in einer Kurzgeschichte verweisen bzw. diese einführen, wobei indefinite NPs einen geringeren, Eigennamen und Pronomen im ersten Satz eines Textes einen höheren Grad an Unbestimmtheit als definite NPs aufweisen. Insofern kann man den drei genannten Kategorien unterschiedliche Werte zuschreiben, mit dem Ziel höhere Werte zu erhalten, wenn ein Verweis grammatikalisch einen geringeren Grad an Unbestimmtheit besitzt. Dementsprechend kann indefiniten Nominalphrasen der höchste, Eigenamen und Personalpronomen der niedrigste Wert in Betreff ihres Grades an Bestimmtheit zugschrieben werden, was zu folgender Messvorschrift führt: Wenn im ersten Satz einer Kurzgeschichte ein Verweis auf Person, Gegenstand, Zeit oder Ort der Handlung erfolgt, gibt es einen Punkt, wenn dies mittels Personalpronomen oder Eigennamen geschieht, 2 Punkte, wenn dies mittels definiter Nominalphrase geschieht, und 3 Punkte, wenn es über eine indefinite Nominalphrase erfolgt. Dies führt dazu, dass je geringer der Grad an Unbestimmtheit der Referenzen in einem Satz ist, desto höher sein Gesamtscore wird und vice versa. In Anbetracht der vier Kategorien Person, Gegenstand, Ort und Zeit ergibt sich so ein möglicher Maximalscore von 12 Punkten. Damit ist der eigentliche Operationalisierungsprozess abgeschlossen. Er sei hier noch einmal in seinen zentralen Schritten zusammengefasst: An seinem Anfang stand die Rekonstruktion fachspezifischer historischer Gattungsbeschreibungen. Im gegebenen Fall erfolgte diese über die Zusammenführung von drei einschlägigen Beschreibungen aus der Forschung. Im Anschluss daran habe ich mich zu Demonstrationszwecken auf die Operationalisierung eines der Merkmale dieser Deskriptionen konzentriert: die Auslassungen am Textanfang von Kurzgeschichten. Dabei habe ich in einem ersten Schritt den Begriff der ""Auslassung"" analysiert, um festzustellen, dass für ihn keine beobachtungssprachlich relevante fachspezifische Bestimmung vorliegt. Dementsprechend wurde in einem nächsten Schritt eine Explikation des Begriffes erarbeitet, im Zuge derer auf literaturtheoretische Reflexionen zurückgegriffen wurde. Diese Explikation wurde dann in einem dritten Schritt mithilfe linguistischer Indikatoren in eine operationale Definition überführt, auf deren Basis abschließend eine Messvorschrift erstellt wurde. Hier ist es wichtig darauf hinzuweisen, dass die Realisierungen dieser Messvorschrift genau dann valide sind, wenn sie den Vorgaben der operationalen Definition 'nicht jedoch denjenigen des Ursprungbegriffes 'angemessen sind. Um einen ersten Eindruck zu bekommen, wie gut die auf ihr basierende Messvorschrift von großen Sprachmodellen realisiert werden kann, habe ich die Eingangssätze von 50 Kurzgeschichten aus der Zeit nach 1945 mit einem Fokus auf Texte von Ilse Aichinger, Heinrich Böll und Wolfgang Borchert manuell annotiert und daraufhin die besagten Sätze auf Basis der entwickelten Metrik nach ihrem Grad an Unbestimmtheit mit zwei proprietären Modellen (GPT-4o gpt-4-turbo (ohne Socring)  (ohne Socring) gpt-4-turbo (mit Scoring)  (mit Scoring) gpt-4-turbo (mit Beispiel)  (mit Beispiel) Die dabei erzielten Resultate bewegen sich im gängigen Rahmen von Klassifikationstasks in den Computational Literary Studies (vgl. Bamman/Kent/Li/Zhou 2024). Die höchste Vorhersage-Genauigkeit mit einem F1-Score von 75% erzielt Anthropics claude-sonnet-3.5-Modell mit einem Prompt mit den Annotationsrichtlinien und einem handverlesenen Beispiel. Es liegt damit ganze 25% über einer Majority Baseline also einem Verfahren, das schlichtweg immer jene Kategorie vorhersagt, die im Datensatz am häufigsten vertreten ist, und 9% über GPT-4o. In zukünftigen Experimenten sollte geklärt werden, ob diese Ergebnisse auch bei einer größeren Stichprobe und anderen Beispieltexten generalisieren und ob ein Fine-Tuning der Modelle oder Prompts, z.B. durch verschiedene Prompt-Tuning-Techniken, zu einem statistisch signifikanten Effekt in der Vorhersage führt. Jedoch kann bereits jetzt konstatiert werden: Das hier gegebene Beispiel für die Operationalisierung einer literaturgeschichtlichen Gattungsbestimmung zeigt, dass eine vom Stand der Forschung ausgehende Rückführung einzelner Teilmerkmale einer literaturhistorischen Gattungsbeschreibung auf sprachliche Indikatoren insbesondere dann möglich ist, wenn diese Rückführung sich nicht allzu eng an der ursprünglichen Bestimmung dieser Merkmale hält, sondern sie mithilfe des Verfahrens der Explikation sowohl den Anforderungen der computationellen Erkennung oder Weiterverarbeitung als auch dem literaturwissenschaftlichen Forschungskontext entsprechend anpasst. Ein solcherart adaptierter Begriff eines gattungsspezifischen Textmerkmales deckt selbstverständlich nicht mehr sämtliche semantischen Dimensionen seiner fachspezifischen Verwendung ab. Gegenüber dieser Verwendung hat er jedoch den Vorteil, dass er a.) seine Einschränkung explizit macht, b.) derartig weder mehrdeutig und vage ist und sich so c.) eindeutig auf Textoberflächenphänomene zurückführen lässt. Zudem besitzt ein derartiger Ansatz, der sich primär auf die Operationalisierung von einzelnen Merkmalen einer Beschreibung oder eines Begriffes fokussiert, den Vorteil, dass er sich nicht vorweg festlegen muss, welchem Begriff des Begriffes er folgt: Eine Operationalisierung der Prädikate, welche auf jene Gattungsmerkmale referieren, die laut Forschung diese Gattung zu einem bestimmten historischen Zeitpunkt charakterisieren, lässt fürs Erste offen, in welchem Verhältnis diese Merkmale zueinander stehen. Dieses Verhältnis kann 'und soll 'erst auf Basis der Operationalisierung sämtlicher Teilmerkmale empirisch bestimmt werden. Mithilfe eines solchen Verfahrens sollte es möglich werden, sowohl eine bestimmte historische Ausprägung einer Gattung näher zu bestimmen als auch auf Basis der selben theorie-geleitete Hypothesentests in den CLS durchzuführen, wie sie in den empirischen Sozialwissenschaften bereits üblich sind. Ein solches Verfahren hat zudem das Potential, die Kommunikation zwischen digitalen und nicht-digitalen Literaturwissenschaften zu vereinfachen, da es von einem geteilten Begriffsapparat ausgeht und diesen unter Rückgriff auf traditionelle geisteswissenschaftliche Praktiken 'die Begriffsarbeit in Form von Begriffsanalyse und Begriffsexplikation 'adaptiert. Längerfristig sollte mithilfe des hier vorgestellten Verfahrens 'das ich als ergänzendes Korrektiv der datengetriebenen Exploration verstehe 'möglich werden, existierende literaturhistorische Hypothesen zu überprüfen und teilweise zu revidieren."
2025,DHd2025,BOGDANOVIC_Arsenije_Layout_und__Para__Text__Erprobung_hybrid.xml,Layout und (Para-)Text: Erprobung hybrider Ansätze und Heuristiken zur Erforschung von Werkausgaben des 18. Jahrhunderts,"Arsenije Bogdanoviƒá (Universität Stuttgart, Deutschland); Liesen-Sophie Lange (Johannes Gutenberg-Universität Mainz, Deutschland); Philip Ajouri (Johannes Gutenberg-Universität Mainz, Deutschland); Gabriel Viehhauser (Universität Wien, Österreich)","Taxonomien, Document Layout Analysis, OCR, Scalable Reading, Buchwissenschaft","Strukturanalyse, Annotieren, Bibliographie, Metadaten, Methoden, Forschungsprozess","In der digitalen Literaturwissenschaft werden meist weitumspannende Konzepte wie Genres oder Epochen in den Blick genommen. Die dort entwickelten Methoden können jedoch auch durchaus gewinnbringend zur Untersuchung von buchgeschichtlichen Fragestellungen zum Einsatz gebracht werden. Das in Kooperation von Buchwissenschaft und Digital Humanities durchgeführte DFG-Projekt ""Scalable Reading von ""Gesammelten Werken"" des 18. Jahrhunderts, exemplarisch durchgeführt an Friedrich-von-Hagedorn-Werkausgaben"" widmet sich dementsprechend der Erforschung des Buchtypus der Gesamtausgabe, der sich im 18. Jahrhundert etabliert hat und zu einem zentralen Medium des Buchmarkts geworden ist, das für das Selbstverständnis von Autor*innen sowie für Leser*innen, Verlage und Bibliotheken große Bedeutung erlangt hat. Die Untersuchung der Ausbildung dieses Typus lässt sich mit quantitativen Methoden unterstützen, im Projekt werden etwa die 'mengenmäßig durchaus umfangreichen 'Werkausgaben Friedrichs von Hagedorn exemplarisch untersucht. Dabei gilt es, die Unterschiede in der Zusammenstellung dieser Werkausgaben mit einem Scalable-Reading-Ansatz (Mueller 2014) in den Blick zu bekommen, um so Aussagen über die Ausprägung des Buchtyps treffen zu können. Dafür wird im Projekt ein zweistufiger Workflow anvisiert: Zunächst werden unterschiedliche Methoden von Layout-Erkennungen eingesetzt, um möglichst automatisch die einzelnen Textteile der Werkausgaben identifizieren zu können. Diese werden in einem zweiten Schritt mit Methoden der Text-Reuse-Detection miteinander aligniert und auf Abweichungen bzw. Parallelen hin untersucht. Der vorliegende Beitrag widmet sich dem ersten Schritt dieses Workflows, der Layouterkennung der Werkausgaben des 18. Jahrhunderts. Wie der Blick auf ein exemplarisches Faksimile einer Hagedorn-Ausgabe in Abbildung 1 verrät, ist es zunächst die ausgeprägte Materialität und Paratextualität von historischen Werkausgaben, die unweigerlich ins Auge fällt. Dies spiegelt sich u.a. in der Auswahl von Format/Papier, Druckschrift, Layout; der Ausgestaltung von Titelseiten, Kupferstichen, Vignetten; dem Umfang, der Autorschaft und Tendenz von Titeln, Vor- und Nachworten, ""Beigaben"" und ""Nachlesen"". Bei Hagedorn interessieren darüber hinaus dessen ausgiebigen Anmerkungen (Münster 1999, 10), welche mit der Zeit durch editorische Eingriffe immer weiter aus dem Blickfeld des Publikums verbannt wurden. Paratexte gliedern und framen das ""Werk"" somit immer aufs Neue, steuern die Rezeption und ermöglichen 'einmal systematisch in ihrem Wandel erfasst 'anderweitige kulturgeschichtliche Rückschlüsse und Hypothesenbildungen (Ajouri 2017). Um eine so gegebene Komplexität weitestgehend aufzufangen, wurde im Projekt zunächst eine Taxonomie gängiger Kern- und Paratextformen erstellt, wobei Umfang und Granularität anpassbar bleiben. Ankerpunkt hierfür waren die Arbeiten von McConnaughey et al. (2017) und Underwood (2014), nicht zuletzt die Erläuterungen zur formalen/inhaltlichen Erschließung von Volltexten des DTA-Basisformats ( Bei Einheiten unterhalb der Seitenebene wächst erwartungsgemäß der Ambiguitätsgrad, weshalb sie iterativ neu verhandelt werden. So etwa die genaue Einordnung von (a) rekursiven Anmerkungen; (b) Zitaten, die mal indiziert in den Apparat eingebunden, mal selbstständig stehen; oder (c) ""schwebenden Fußnoten"", die an Gedichttexte anschließen können und einem weiteren Kerntext voranstehen, aber keine Endnoten sind (s. Abb. 2). Um auch hier die Komplexität einzudämmen, wurde eine basale Typologie beobachteter Layout-Konstellationen erstellt. Daraus lassen sich tendenziell auch typographische Vorlieben der jeweiligen Verleger ablesen, was über das Hagedorn-Korpus hinaus von Interesse sein dürfte, da es sich hierbei um im gesamten deutschsprachigen Raum tätige Akteure handelte (insgesamt 18 Verleger bzw. Ko-Verleger, s.u. Abb. 5). Das Hagedorn-Korpus hat damit ein hohes exemplarisches Potential, dessen Übertragung auf andere Gesamtausgaben lohnend erscheint. Die Layouttypen lassen sich weiter auf einzelne Seitentypen zerlegen und mit anderen Eigenschaften kombinieren (Format, Druckschrift), um neue Cluster sichtbar zu machen. Seitentypen lassen sich wiederum weiter auf Komponenten runterbrechen und zur Generierung theoretisch möglicher Layouts verwenden, was in Zeiten kostspieliger Ground Truth seine Vorteile haben kann (vgl. Fleischhaker 2024). In einem weiteren Schritt gilt es nun, TEI- oder vergleichbaren XML/JSON-Dateien zu erzeugen, in denen die Texte nach den Maßgaben dieser Taxonomie ausgezeichnet sind. Dadurch wären sie leicht auf ihre Position, Inhalt, Umfang und Abfolge bzw. die daraus folgenden (para-)textuellen Konstellationen hin beforschbar. Eine manuelle Erstellung derart detailreicher Ausgaben erscheint jedoch freilich zu aufwändig; in Ausnahmefällen, wo solche vorliegen (dreibändige Bohn-Ausgabe im DTA-Archiv, Aus den genannten Gründen müssen OCR-Verfahren 'genauer die der Die DLA ist eine unentbehrliche Vorstufe der Zeichenerkennung (OCR im engeren Sinne), steht aber im Gegensatz zu dieser weiterhin vor nicht wenigen Schwierigkeiten. Defizite und Desiderata im Bereich historischer Drucke sind in der Community allgemein bekannt und betreffen Segmentierung wie  Obwohl nicht für historische Dokumente vorgesehen, beabsichtigen wir zusätzlich Probeläufe mit sog. Instance-Based-Engines wie YOLO und Detectron2 durchzuführen, die für grobe Zoneneinteilungen, ergänzende Bilderkennung (z.B. Vignetten), oder synergisch mit den o.g. Pixel-Classifiers einsetzbar sind (vgl. Najem-Mayer/Matteo 2022). Sich auf Computer Vision allein zu stützen resp. vom ""flachen"" Text ausgehend nur auf NLP-Methoden zu setzen 'dies gilt bei perfekter Transkription (vgl. Pagel et al. 2021), geschweige denn qualitativ stark schwankender OCR –, scheint angesichts der skizzierten Herausforderungen wenig erfolgversprechend. Daher empfehlen sich hybride bzw. holistische Ansätze, die entweder 1) multimodal vorgehen, also zeitgleich Positionierung und Inhalt von Textteilen auf der Seite berücksichtigen, oder 2) eine Nachbearbeitung von anderweitigen DLA-Outputs durchführen. In die erste Kategorie fällt die LayoutLM-Modellfamilie ( Da sich das Projekt aktuell in der Annotationsphase befindet, wird eine Ground Truth iterativ in PAGE XML hergestellt (Pletschacher/Antonacopoulos 2010), einem Format, das aufgrund hoher Flexibilität und Interoperabilität alle angerissenen Lösungsszenarien bedienen kann. Je nach gewähltem Ansatz muss allerdings abgewogen werden, ob und wie man mit fehlerhaften Vorergebnissen umgeht, bzw. ob man diese unkorrigiert belässt; gleiches gilt für die nachgeschaltete OCR-Erkennung, wobei hier eine ""schmutzige"" OCR bewusst in Kauf genommen wird. Das Annotationskorpus zählt aktuell ca. 200 S. und wurde in Anlehnung an die OCR-D-Richtlinien ( Obwohl die Layouterkennung für historische Drucke weiterhin vor vielen Hürden steht und v.a. jene Forschende, die aus materialitätsbezogenen Fragestellungen neue Erkenntnisse schöpfen wollen, oft enttäuscht zurücklässt, gilt es die angerissenen und sich stets weiterentwickelnden Ansätze zu erproben und auszubauen, um eine möglichst genaue Isolierung fokussierter Buchteile für nachgeschaltete Analyseschritte zu ermöglichen. Bereits eine Übersicht über bevorzugte Layouts gibt Anlass genug, um solche vermeintlich unscheinbaren Konstellationen weiter qualitativ wie quantitativ 'also skalierbar 'zu befragen. Das Wissen um Verteilungen von Seitentypen und deren Komponenten nach Verlegern u.a. Metadaten kann weiter praktisch bei der Zusammenstellung des Annotationskorpus für OCR-relevante Tasks oder auch für die automatisierte Erstellung von Trainingsdaten herangezogen werden. Darüber hinaus ist der Buchtypus ""Werkausgabe"" 'und insofern die beachtliche Menge an Hagedorn-Exemplaren 'ein interessanter Anwendungsfall für die Erprobung von layoutgerechten Methoden: Hier wird ein relativ überschaubarer (Para-)Textkorpus in relativ vielen ""Fassungen"" visuell realisiert, was zu einer weitergehenden Ergänzung von Computer-Vision- und textbasierten Ansätzen einlädt. Dies betrifft zuvorderst die zahlreichen Anmerkungen 'ein Phänomen, dessen genauere Erfassung bislang eine eher untergeordnete Rolle spielte oder auf weniger variable Erscheinungsformen fokussiert war. Alle relevanten im Projekt erarbeiteten Forschungsdaten, Tools und Modelle werden abschließend im Einklang mit den FAIR-Prinzipien u.a. auf"
2025,DHd2025,YAKUPOVA_Vera_Interdisziplin_re_transkulturelle_Analyse_von_.xml,Interdisziplinäre transkulturelle Analyse von Mensch-KI Interaktionen in Science-Fiction Literatur und im politischen Diskurs   Kulturelle Wahrnehmungen von Privatsphäre und KI-Überwachungstechnologien,"Vera Yakupova (Trinity College Dublin, Republik Irland)","KI, Science Fiction, Literaturwissenschaften, Politik, Digital Humanities","KI, Science Fiction, Literaturwissenschaften, Politik, Digital Humanities","In einer Zeit, in der Künstliche Intelligenz (KI) zunehmend unseren Alltag und politische Debatten prägt, widmet sich diese Doktorarbeit der Frage, wie Science-Fiction-Literatur als kulturelle Wissensquelle zur Reflexion von Privatsphäre, Datenschutz und Überwachungssystemen dienen kann. Der Schwerpunkt liegt darauf, wie fiktionale Figuren in Science-Fiction KI-Überwachung wahrnehmen und mit diesen Systemen interagieren, um zu zeigen, wie spekulative Fiktion gesellschaftliche und kulturelle Diskurse widerspiegelt. Durch einen transkulturellen Vergleich der USA, der EU und Russlands sollen Unterschiede in der kulturellen Akzeptanz und Kritik solcher Technologien beleuchtet werden. Science-Fiction inspiriert nicht nur die technologische Entwicklung von KI (Dillon & Schaffer-Goddard, 2023), sondern regt auch ethisches Bewusstsein und kulturelle Reflexionen an (Hudson et al., 2021; Dolan, 2020; Cave et al., 2019; Cave et al., 2020; Cave & Dihal, 2023). Werke wie George Orwells Erstens wird analysiert, wie KI-Überwachungssysteme in der Science-Fiction dargestellt werden und welche sozialen und kulturellen Kontexte sie widerspiegeln. Hierfür werden Foucaults Theorie der Überwachung (1977) und Jasanoff und Kims Konzept der soziotechnischen Imaginationen (2015) als theoretische Grundlage herangezogen, um die narrativen und symbolischen Mechanismen der Texte zu untersuchen. Zweitens erfolgt ein transkultureller Vergleich, der Gemeinsamkeiten und Unterschiede in der Darstellung von KI-Überwachung zwischen den USA, Russland und der EU mithilfe von Bakhtins ""Dialogismus"" (1981) und Kristevas ""Intertextualität"" (1969), um die kulturellen und historischen Kontexte der Narrative zu analysieren. Drittens sollen diese literarischen Darstellungen mit aktuellen Datenschutzregelungen wie der DSGVO (EU), der Privatsphärendebatte in den USA (Fazlioglu, 2020) und den Überwachungspraktiken in Russland (Pallin, 2017) verglichen werden, um die Wechselwirkungen zwischen spekulativer Fiktion und politischem Kontext zu untersuchen. Die Methodik kombiniert qualitative Analysen mit computergestützten Verfahren. In der ersten Phase wird ein Korpus von maximal 30 Science-Fiction-Werken zusammengestellt, die sich auf KI-Überwachung konzentrieren und kulturell sowie politisch repräsentativ für die untersuchten Regionen sind. Für die Analyse wird eine Mischung aus ""close reading"" und ""distant reading"" angewandt. Das Distant Reading wird mithilfe des Privacy-Wörterbuchs von Vasalou et al. (2011) spezifische Textpassagen identifiziert werden, die sich mit Privatsphäre und Überwachung befassen. Diese Passagen werden anschließend im Close Reading in den Kontext des Gesamtwerks eingebettet. Ziel ist es, sprachliche Muster zu identifizieren und zu analysieren, wie Privatsphäre thematisiert wird und wie die literarische Sprache im Vergleich zu politischen Diskursen genutzt wird. Das Close Reading mithilfe von Jonathan Cullers Literaturtheorie (2007) und Faircloughs Kritischen Diskursanalyse (1992) durchgeführt. Mithilfe von Foucaults Überwachungstheorie werden diese Passagen kategorisiert, etwa danach, ob die KI direkte Kontrolle ausübt und Widerstand provoziert oder ob Kontrolle internalisiert ist, wie im metaphorischen Panoptikum. Zudem wird untersucht, ob die KI als staatlich kontrolliertes System, unternehmensgesteuerte Technologie oder metaphorische Präsenz dargestellt wird und welche gesellschaftlichen Öngste dies reflektiert. Ziel ist es, die Darstellung von Kontrolle und Privatsphärenverletzungen in der Science-Fiction zu analysieren. Dabei werden Diskurse untersucht, die etwa Charaktere als machtlos oder widerstandsfähig gegenüber Überwachungssystemen beschreiben. Ebenso werden die Sprache, Metaphern und Symbole analysiert, die zur Beschreibung von KI-Überwachung verwendet werden. Diese Diskurse werden in ihren sozio-kulturellen und historischen Kontext eingeordnet, um zu verstehen, wie verschiedene Länder, wie Russland und die USA, KI-Überwachung unterschiedlich kritisch reflektieren. Die abschließende Phase umfasst eine vergleichende Analyse, eine Kontextualisierung, der literarischen Ergebnisse mit aktuellen politischen Dokumenten und Datenschutzrichtlinien, wie sie von der OECD, der OSTP (USA), Roskomnadzor (Russland) und dem EU-Parlament zwischen 2025 und 2028 herausgegeben werden. Inspiriert von Dillon und Craigs ""Storylistening"" (2022) wird untersucht, wie literarische Darstellungen von KI-Überwachung mit zeitgenössischen Datenschutzrichtlinien in Beziehung stehen. Ziel ist es, die kognitiven und kollektiven Funktionen dieser Darstellungen zu analysieren und ihren Einfluss auf gesellschaftliche Identitäten und politische Debatten zu diskutieren. Die Ergebnisse des Projekts sollen zeigen, wie kulturelle Unterschiede die Wahrnehmung von KI-Überwachung prägen und wie spekulative Fiktion Debatten über KI-Governance, Ethik und Datenschutz bereichern kann. Das Projekt diskutiert, wie literarische Narrative als Werkzeuge für gesellschaftliche und politische Reflexion genutzt werden können und welchen Einfluss sie auf das Verständnis von Überwachung und Privatsphäre haben. Die Kritikpunkte aus den Gutachten wurden in der überarbeiteten Version berücksichtigt, insbesondere durch eine stärkere Fokussierung des Projektrahmens. Anstelle einer allgemeinen Untersuchung von Mensch-KI-Interaktionen konzentriert sich das Projekt nun explizit auf die menschliche Wahrnehmung von KI-Überwachung und die Verletzung der Privatsphäre durch KI-Technologien. Diese thematische Eingrenzung erlaubt eine präzisere Definition der Analysekategorien und der Suchbegriffe für das Distant Reading, das mithilfe von Vasalou et al.s Privacy-Wörterbuch (2011) durchgeführt wird. Dabei wurde jedoch auch die Einschränkung akzeptiert, dass die analysierten Texte für das Distan Reading in englischer Übersetzung vorliegen müssen. Die Methodologie der Analysen wurde stärker in etablierten literaturwissenschaftlichen Traditionen verankert, um eine tiefere theoretische Fundierung zu gewährleisten. Die Auswahl der Länder 'USA, EU und Russland 'ist mit Blick auf das Thema Privatsphäre besser begründet, da diese Regionen nicht nur Vorreiter in der KI-Entwicklung sind, sondern auch unterschiedliche kulturelle Wahrnehmungen von Privatsphäre (Bellman et al., 2004) und divergierende rechtliche Rahmenbedingungen aufweisen. Fiero und Beier (2022) haben dieselben Länder bereits für eine vergleichende Analyse der Regulierung von KI und Privatsphäre im rechtlichen Diskurs herangezogen, was die Relevanz dieser Auswahl unterstreicht. Die Einbeziehung Chinas wurde in Betracht gezogen, da es ebenfalls ein Vorreiter in der KI ist und eine besondere kulturelle Wahrnehmung der Privatsphäre aufzeigt (Hua und Wang, 2023), erscheint jedoch aufgrund sprachlicher und zugangstechnischer Hürden unpraktikabel. Die Methodologie und der analytische Rahmen für den politischen Diskurs müssen weiterhin präzisiert werden. Allerdings wurde durch die Eingrenzung des Projektthemas 'von KI im politischen Diskurs allgemein auf KI-Überwachung und Privatsphäre 'bereits ein zentraler Fokus geschaffen. Beim Doctoral Consortium werde ich den Analyseprozess durch konkrete Fallbeispiele visualisieren, um die Verbindung zwischen theoretischem Ansatz und praktischer Umsetzung klar darzustellen."
2025,DHd2025,R_TTGERMANN_Julia_Qualitative_Genre_Profile_und_distinktive_.xml,Qualitative Genre-Profile und distinktive Wörter: Eine Studie zu Keyness in Subgenres des französischen Romans,"Julia Röttgermann (Universität Trier, Deutschland); Keli Du (Universität Trier, Deutschland); Christof Schöch (Universität Trier, Deutschland)","Computational literary studies, Keyness, Distinktivität, Evaluation, Französische Literatur","Inhaltsanalyse, Literatur, Text","In In diesem Beitrag konzentrieren wir uns auf die Analyse der Untergattungen französischer Romane und versuchen, die Lücke zwischen qualitativen und quantitativen Aspekten in der Computational Literary Studies (CLS) zu schließen, indem wir ein Mapping zwischen qualitativen, expertenbasierten ""Subgenre-Profilen"" und distinktiven Wörtern erstellen, die mit verschiedenen Distinktivitätsmaßen extrahiert wurden. In Computerlinguistik und CLS existiert eine zunehmend unübersichtliche Vielzahl an statistischen Maßen, um große Textmengen hinsichtlich distinktiver Wörter zu untersuchen. Dies ist teilweise begründet in fachlichen oder nationalen Traditionen, liegt aber auch teilweise an der Implementierung in Tools.  Der französische populäre Roman hat eine lange Geschichte, die mindestens bis ins mittlere bis späte 19. Jahrhundert zurückreicht. Die Periode von 1860–1920 wird oft als ""goldenes Zeitalter"" des populären französischen Romans angesehen, als Phänomene wie der ""roman-feuilleton"" und der Fortsetzungsroman mit wiederkehrenden Protagonisten wie Rocambole und Rouletabille aufkamen. Frühere Studien zum Populärroman konzentrierten sich häufig auf das 19. und frühe 20. Jahrhundert (Angenot 1975; Olivier-Martin 2013). Auch in der zweiten Hälfte des 20. Jahrhunderts existiert in Frankreich eine vielfältige Landschaft populärer Romane (Migozzi, 2005). Diese Romane werden von etablierten Verlagen (z.B. Harlequin, Fleuve noir, éditions du Masque) in spezialisierten Sammlungen mit hohen Auflagen und einer klaren Zielgruppenorientierung herausgegeben. Einige Subgenres des Populärromanes erhielten besondere Aufmerksamkeit, wie der ""roman policier"" (Todorov 1971; Vanoncini 2002; Dubois 2005), der Science-Fiction-Roman (Slusser 1989; Thomas 1989; Millet und Labbé 2001; Baudou 2003; Mather und Rheault 2016) oder der Liebesroman / ""roman sentimental"" (Helgorsky 1985; 1987; Constans 1999).  In jüngster Zeit wurde dem zeitgenössischen französischen populären Roman aus linguistischer Perspektive im Rahmen des Projekts ""Phraséorom"" Serien von Kriminal- oder Science-Fiction-Romanen sind zudem Beispiele für serielles Erzählen, das auf eine lange Geschichte in der französischen Literatur zurückblicken kann. Um gemeinsame Merkmale von Kriminalromanen, Science-Fiction-Romanen, Sentimentalromanen und Hochliteratur zu identifizieren, haben wir zunächst eine Liste typischer Elemente wie Themen, Figuren, sprachliche Muster, Tonalität und Erzählform durch Sichtung relevanter Sekundärliteratur qualitativ zusammengestellt. Diese Profile dienten als Grundlage für die Kategorisierung distinktiver Wörter pro Subgenre, die von den Distinktivitätsmaßen identifiziert wurden. Es wurde ein Korpus französischer Romane der 1970er, 1980er und 1990er Jahre erstellt, welches vier Gattungen enthält: Science-Fiction, Sentimentalroman, Kriminalroman und Hochliteratur, die in Frankreich sogenannte ""littérature blanche"".  Das Korpus zeitgenössischer französischer Literatur enthält 33 Millionen Tokens und umfasst 600 Romane, gleichmäßig verteilt auf vier Subgenres (Sentimentalromane, Kriminalromane, Science-Fiction-Romane, Hochliteratur) und drei Zeitperioden (1970er, 1980er, 1990er Jahre). Somit besteht das Korpus aus 50 Romanen pro Jahrzehnt und Subgenre. Die Jahrzehnte sind gleichmäßig vertreten, allerdings ist ein Höhepunkt der Veröffentlichungen in den Jahren 1989–1990 zu verzeichnen (Abb. 2). Da alle Romane urheberrechtlich geschützt sind, wurde das Korpus in Form von ""abgeleiteten Textformaten"" (Schöch et al. 2020) veröffentlicht, einem Format, das für bestimmte computergestützte Analysen geeignet ist, aber für Menschen unlesbar bleibt. Diese Strategie soll Transparenz und Reproduzierbarkeit der Studie ermöglichen. Mit dem Ziel, distinktive Wörter pro Subgenre mit drei verschiedenen Distinktivitätsmaßen zu extrahieren, vergleichen wir jedes Subgenre mit allen anderen, z. B. Science-Fiction vs. den Rest. Dazu werden 150 Romane eines Subgenres mit 450 Romanen der übrigen Subgenres verglichen. Da Französisch eine flektierte Sprache ist, werden alle Texte mit SpaCy lemmatisiert (Montani et al. 2023). Anschließend werden sie mit dem Python-Paket pydistinto (Du et al. 2022) verarbeitet. Die drei angewendeten Distinktivitätsmaße sind Zeta, Welch und LLR. Für die Berechnung von Zeta werden die Romane in 5000-Wörter-Segmente unterteilt. Aus jedem Test erhalten wir drei Listen mit distinktiven Wörtern und wählen die Top 50 Wörter für den Abgleich mit den qualitativen Subgenre-Profilen aus. Unsere Erwartung ist dabei, dass die extrahierten distinktiven Wortlisten pro Subgenre in mehr oder weniger ausgeprägtem Umfang Wörter enthalten, die sich auf die thematischen Konzepte, Sprachmuster, Figuren, Schauplätze, Tonalität, Erzählform oder Erzählstruktur aus den qualitativen Subgenre-Profilen beziehen lassen und so eine qualitative Evaluation der Maße erlauben.  Das Ziel ist es, den Anteil der interpretierbaren distinktiven Wörter pro Maß zu quantifizieren. Wörter, die nicht zu den Subgenre-Profilen passen, werden als ""unerwartet"" klassifiziert. Was verstehen wir im hier skizzierten Kontext unter Interpretierbarkeit? In dieser Studie gehen wir über die Klassifizierung von Keyness-Maßnahmen allein nach ihrer Leistung hinaus. Unter dem breiteren Paradigma von explainability (Erklärbarkeit) und interpretability (Interpretierbarkeit) von algorithmischen Methoden (u.a. Benois-Pineau et al. 2023), untersuchen wir die Ergebnislisten der Extraktion mit Keyness-Maßen. Insbesondere gleichen wir die Top 50 der distinktiven Wörter aus diesen Listen mit den von Fachwissenschaftler:innen erstellten Genreprofilen ab. Wir definieren dabei Interpretierbarkeit auf der Ebene der einzelnen Wörter: Ein distinktives Wort wird als interpretierbar definiert, wenn menschliche Annotator:innen es einem deskriptiven Genreprofil zuordnen kann. In einem ähnlichen Sinne haben Chang et al. (2009) einen Ansatz entwickelt, bei dem menschliche Annotator:innen die Aufgabe haben, aus einer Reihe von Topic Modeling Ergebnissen ""the odd one out"" (das nicht passende Wort innerhalb der Wortliste) zu identifizieren, als Operationalisierung der Interpretierbarkeit von Topic Modeling Ergebnissen. Bei der Beurteilung der Interpretierbarkeit lautet die übergreifende Frage außerdem: ""Can we trust the model?"" (Mas√≠s 2023, 6)? In unserem Fall lautet diese Frage demnach analog: Können wir dem Keyness-Maß (für diese Aufgabe im genannten Setting) vertrauen? Je höher der Anteil interpretierbarer Wörter ist, als desto zuverlässiger kann das Maß für ähnliche Aufgaben angesehen werden.   Logarithmisches Zeta erwies sich beim Matching-Prozess der Wortlisten mit den Genreprofilen als am Besten geeignet zur Extraktion distinktiver Wörter, gefolgt von Welch""s t-Test. Der Log-Likelihood-Ratio-Test zeigte im untersuchten Setting die schwächste Leistung, was überrascht, da das Maß in zahlreichen Korpusanalysetools wie Antconc oder Wordsmith Tools implementiert ist. Dieses Ergebnis wirft Fragen nach den standardmäßig implementierten Keyness-Maßen in DH-Tools auf und deutet darauf hin, dass eine kritische Überprüfung in Betracht gezogen werden sollte, zumindest in Szenarien, die mit narrativer Prosa arbeiten."
2025,DHd2025,GERSTORFER_Dominik_Operationalizing_operationalizing.xml,Operationalizing operationalizing,"Dominik Gerstorfer (TU-Darmstadt, Deutschland); Evelyn Gius (TU-Darmstadt, Deutschland)","Operationalisierung, Explikation, Validieren, Kalibrierung, Workflow, Forschungsdesign","Modellierung, Methoden, Forschung, Forschungsprozess","In this paper we suggest a comprehensive account of operationalization. The goal is to define a workflow which specifies the normative components that ensure that the results match the research question. The focus here lies on epistemic norms 'as opposed to social norms 'which can be understood as rules that govern processes, thoughts, and actions that ought to be followed in order to produce the intended result (cf. Wedgwood 2018). This workflow is based on a measurement workflow, which is based on input-process-output components (cf. Mari, Wilson, and Maul 2023). In order to make the workflow implementable in Digital Humanities research, we need to identify the components of a generalized method for getting from a research question to its answer. While some of the components have already been discussed in Digital Humanities"" research on operationalization, we propose to put more focus on explication, measurement, and validation. In our view, measurement and validation are underdeveloped in the Digital Humanities. This becomes apparent when looking at discussions on operationalization. Before doing that we briefly line out our understanding of central terms in the discussion In order to develop our account, we will make the following assumptions: The leading question for scientific workflows involving the analysis of data is this: In the case of text based digital humanities, we can specify the scope of theory and data further. Theory and text depend on the disciplinary background of the research question. For example, in Computational Literary Studies approaches, theory means literary theory and empirical data respectively text. A first approach to address this question attempted by Franco Moretti, who introduced operationalization to the discourse of the digital humanities with his seminal paper ""Operationalizing or, the Function of Measurement in Modern Literary Theory"" (Moretti 2013). In this paper, Moretti goes back to Bridgman""s ([1927] 1958) original conception of operationalization and applies it to literary concepts. His answer to question Q"" famously is: We must imagine a bridge that is built by operations, which take us from the (qualitative) concepts of literary theory to (quantitative) measurement results. This implies existence of a simple and easy way to connect a concept C to an indicator I, resulting in a straightforward measure, which allows inferences, like the more I, the more C. Finally numbers are assigned to produce a measurement metric, be it by counting words or by using more elaborate techniques, like network analysis. In his paper, Moretti gives examples to show how this can be done, but he does not provide a detailed and generalized method that can be followed in different contexts. We therefore specify our question even further: In order to tackle this question, we have to address at least three ambivalent aspects of operations that are at least implicitly underlying understandings meaning of operationalization: In digital humanities research operationalization often only means (a), i.e. that in the research design some kind of workflow or pipeline is developed, that is used to automatically extract some features from the empirical material (text). Often literary concepts are adopted uncritically, when, in fact, critical reflection on the usage of the concepts in the sense of (b) is needed to guarantee that the workflow returns results that (c) are actually valid. The last point holds even then, when statistical validity metrics are incorporated in the workflow, since those tests can only answer how good the machine learning model or the algorithm worked, but not, if the quantified results translate back to literary concepts. This objection follows the main line of argumentation of Pichler and Reiter (2022), who argue for the importance of validity while maintaining its untenability for many real-world scenarios in the digital humanities. Overall, we understand operationalization as a workflow or process in the sense of (c), specifically in terms of the scientific input-process-output workflow (Mari, Wilson, and Maul 2023, see Fig. 1). In the following, we will introduce explication, measurement, and validity as components that make this workflow consistent and appropriate for the respective scientific context. The starting point of operationalization in our account are concepts which are transformed to empirical mini-theories that fit the research task at hand. To reach this goal, we resort to Carnapian explication (cf. Carnap 1950; Brun 2016) which allows us to accommodate various epistemic goals and permits different concept variations for different purposes. Explications involve two areas, the one of the explicandum and the one of the explicatum. These domains are sometimes viewed as pre-scientific and scientific uses of the term, but we can assume that explications can occur in any area. This construes a more exact explication in a more restricted or specialized area. For instance, moving from chemistry to molecular quantum chemistry or literary theory to computational literary theory (CLS). Measurement can be thought of as a process that takes some inputs and returns some outputs, where the input is some kind of empirical data and the output a value, see Fig 1a. But not any arbitrary process is suitable for measurement, it is necessary to specify the properties of the process, which will result in a procedure which is designed for that specific task (Mari, Wilson, and Maul 2023, 27), see Fig. 1b. Validation A common conception of validation is the following: A e.g. thermometer is intended to measure the actual temperature of an object, where temperature is a property of said object. This notion seems to be sufficiently clear, but on closer inspection it turns out that the terms in (M) are underspecified. How do we determine ""measurement"", ""measure"", and ""intended""? What exactly are we referring to, when we use those terms? ""Measurement"" is ambiguous at least respective its target (Mari, Wilson, and Maul 2023; Maul, Mari, and Wilson 2019). Measurement may either denote a measuring process or the result of such process. To avoid misunderstandings, we will adopt a strict terminology which discriminates measurement processes and measurement results. Where process means a set of operations which are implemented as an instrument, that can range from physical artefacts, like thermometers, to computational devices, like machine learning algorithms. The result is an assigned value on an appropriate scale. Calibration Calibration denotes the inferential activity of assigning values to the measurement instrument indications and thus producing the measurement results, where: Calibration links the outcomes of the measurement process to the epistemic claims about the object that is measured, this step is necessary since the indications of the measurement instrument are not of the same kind as the properties of the object, see Fig. 1c. The scale of a thermometer e.g. presents its indications visually a further step is needed to ensure that those values can be used to represent temperature. The same applies to literary concepts, like, in Moretti""s case character space. Network analysis or word counts provide indicators that need to be mapped to the possible values of the intended property being measured. We can now revise Moretti""s picture of operationalization as a simple measurement process, that connects theory to world directly (see Fig. 2a) and replace it with the refined scheme developed above (Fig. 2b). For our account of operationalization we consider operations as activities that are performed on some type of input, which return some type of outputs. The kind of activity performed in operations can vary wildly, ranging from simple reasoning about concepts over doing physical activity to actual computation. In constructing an operationalization workflow we denote operations as connected boxes: We are finally able to put all the pieces together and construct a workflow that starts with concepts and ends with measurement results. The operationalization workflow has two sources: (i) The concept(s) originating in the source domain (humanities, literary theory, CLS) and (ii) the empirical data / text. Starting with the concepts, the workflow follows these steps:"
2025,DHd2025,SPIELBERG_Marina_Literary_Metaphor_Detection_with_LLM_Fine_T.xml,Literary Metaphor Detection with LLM Fine-Tuning and Few-Shot Learning,"Marina Spielberg (Trier Center for Digital Humanities (TCDH), Universität Trier, Deutschland)","large language models, metaphor detection, fine-tuning","Programmierung, Daten, Literatur, Forschungsergebnis","The study of literary metaphors plays an integral part in literature-focused disciplines within the humanities. In the field of Natural Language Processing (NLP), computational metaphor detection (MD) has produced a wealth of approaches focusing on everyday metaphors (Ptiƒçek and Dob≈°a 2023). Computational literary MD, however, has received considerably less attention. In their Graph Project, Kesarwani et al. (2017) have applied rule-based and statistical machine learning approaches to an English poetry corpus. The aim of this paper is to take the field of literary MD one step further by using the NLP state-of-the-art approach of fine-tuning Large Language Models (LLMs) on the Graph project""s datasets. This paper tests two assumptions: 1. Fine-tuning the LLM DistilBERT with the Transformers approach and the LLM all-MiniLM-L6-v2 with the SetFit approach on the Graph project""s datasets will yield better results than using the combined statistical and rule-based approach from Kesarwani et al. (2017). 2. The SetFit approach will outperform the Transformers approach. The paper starts by setting out the theoretical background of MD and then explains the data, method and experimental setup used. It closes with a description of the evaluation results and a discussion. Since the current theories and methods regarding MD come from NLP, it is beneficial to understand prevalent research in this area before focusing on the state of the field in the Digital Humanities. In most of the MD research in NLP, metaphors are understood as a mapping of a source domain like WAR to a target domain like ARGUMENT that can result in the metaphorical linguistic expression ""Your claims are Since 1975, research on MD is ongoing because ""the task is not considered solved"" (Dankin, Bar, and Dershowitz 2022, 125). The methodologies applied to MD have evolved with the development of computational capabilities starting with rule-based, statistical and machine learning methods (Ptiƒçek and Dob≈°a 2023). The current state-of-the-art method for MD involves utilizing LLMs, often derivations of the BERT model (Devlin et al. 2019), which are based on the Transformer architecture by Vaswani et al. (2017) that allows to fine-tune an existing language model on a specific downstream task like MD (Babieno et al. 2022; Li et al. 2023; Z. Song et al. 2024). Recently, prompt engineering started to be utilized for MD. Instead of labelled data this method uses task-specific formulated prompts (Jia and Li 2024). Chen et al. (2024) expanded the task to include metaphor reason in addition to detection since they found that methods focusing solely on a metaphorical versus literal distinction did not generalise well. Another recent development is to detect metaphor in multi-modal settings, such as memes, where the classification task includes both texts and images (Xu et al. 2024). In the Digital Humanities, MD focusing on literary texts is understudied. To my knowledge, there are only a handful of papers that concern themselves specifically with this task. Reinig and Rehbein (2019) proposed a supervised machine learning method for MD in German expressionist poetry, while Schneider et al. (2022) developed an unsupervised approach for Middle High German. Toker et al. (2024) used LLMs on their own Early Medieval Hebrew poetry dataset. In their Graph Project, Kesarwani et al. (2017) developed models to detect poetic metaphors in English, which has neither been done in NLP nor in the Digital Humanities. Diverging from the Conceptual Metaphor Theory, the authors based their metaphor definition on observations by Neuman et al. (2013), who found three metaphor types that signify their metaphoricity by different part-of-speech (POS) sequences. They focused on detecting Type I metaphors, which are comprised of a ""Noun-Verb-Noun"" sequence and added the sequence ""Noun-Verb-Det-Noun"" to this concept. An example for Type I metaphor is the sentence ""As if the The authors trained and tested on four datasets: Their own PoFo (Poetry Foundation) dataset, comprising 680 sentences that include Type I metaphor scraped from the Poetry Foundation website, the benchmark datasets TroFi by Birke and Sarkar (2006) (6,435 sentences from the Wall Street Journal Corpus) and MOH by Mohammad, Shutova, and Turney (2016) (647 sentences from WordNet). Finally, they created a fourth concatenated dataset which combines PoFo, TroFi and MOH. See table 1 for an overview of sample sentences labelled ""metaphorical"" from each dataset. The methods of the Graph Project mirror the progression of methods in the NLP tradition as the authors experimented with rule-based and statistical approaches (Kesarwani et al. 2017). The F1 scores for their rule-based and machine learning models were 0.669 for PoFo, 0.827 for TroFi, 0.779 for MOH and 0.781 for the concatenated dataset. Tanasescu, Kesarwani, and Inkpen (2018) used deep learning with convolutional neural networks on the concatenated dataset and reached an F1 score of 0.833. Since fine-tuning LLMs proved to be a very successful approach in recent NLP research (e.g., a F1 score of 0.944 on TroFi by Ma et al. 2021), this paper tests whether this method will improve the MD performance on the Graph project""s datasets. Based on the assumption that the metaphoricity of a word stems from the context of the whole sentence rather than a single aspect word or POS sequence, I define MD as a sentence-level classification problem, where each sentence within the four datasets is labelled as either ""metaphorical"" or ""literal"" (Ma et al. 2021). This approach differs from Kesarwani et al. (2017), who annotate metaphor on a token level. For preprocessing, I reused the cleaned versions of the TroFi and MOH datasets by Su et al. (2020) due to a lack of clear preprocessing information from the Graph project and used the original version of PoFo by Kesarwani et al. In the concatenated dataset there are more literal than metaphorical sentences since it consists of 75% of sentences belonging to TroFi, which suffers from label imbalance, having about 12% more literal than metaphorical examples (figure 1). The first method to improve the MD performance is fine-tuning the four datasets on the Transformer-based pre-trained LLM DistilBERT (Sanh et al. 2019), which is an efficient variant of BERT. This model is smaller and faster while maintaining 97% of BERTs performance. The training for this Transformers approach is threefold: The model-specific tokenizer maps the dataset""s text tokens to indexes, the transformer converts these indexes to contextual embeddings and the pre-trained head is fine-tuned on the MD task (Wolf et al. 2020). The fine-tuning procedure consisted of training with 70% of the data and evaluating with the remaining 30% for each dataset. For hyperparameters, the batch size of 32, the learning rate of 2e-5 and 5 training-epochs yielded best results. Since Transformer LLMs require fine-tuning on relatively large datasets which is a challenge for literary metaphor datasets, as can be seen from the sizes of the Graph Project""s datasets, this paper also employs the SetFit framework (Tunstall et al. 2022). It is designed for few-shot learning, that is, learning with a small number of labelled examples. SetFit operates in two steps: It generates sentence pairs, thereby enlarging the dataset significantly, and then fine-tunes embeddings of these sentences to create a sentence transformer embedding model. Then the dataset is used to train a logistic regression classifier using the fine-tuned embeddings to predict the right labels. The impact of the Sentence Transformer on the dataset size is immense: From 441 samples of the PoFo train dataset, SetFit generated 98366 unique pairs. The all-MiniLM-L6-v2 Sentence Transformer model is used within this framework because it is 5 times faster than larger models while maintaining comparable performance (Reimers et al. 2019). For implementation, the same test-train split ratio, seed and hyperparameters are used as for the Transformers implementation to maintain comparability. Figure 2 presents the evaluation results of fine-tuning with the Transformers and SetFit methods on the MD task at sentence-level. All reported results are from evaluating the fine-tuned models on the test splits. At least one of the LLM fine-tuning approaches proposed in this paper outperformed the baseline on all datasets except for TroFi. For PoFo there was a significant performance increase of 12.41% with SetFit (F1 0.752) and 2.84% with Transformers (F1 0.688). The MOH dataset displayed an F1 score of 0.785 with the Transformers approach (0.77% increase). The SetFit approach demonstrated a significant improvement with an F1 score of 0.862 (10.37% increase) on the concatenated dataset. However, the TroFi dataset performed much better with the rule-based and statistical method, surpassing SetFit by 22.25%. As for the comparison of Transformers with SetFit, SetFit achieved better performance on PoFo, TroFi and the concatenated dataset, while Transformers gave better results on the MOH dataset. Overall, no method excelled across all datasets, although the LLM approaches generally performed better. The promising results indicate the potential of fine-tuning LLMs for literary MD in general and using the SetFit method for small datasets in particular. The concatenated dataset's heterogeneity and diversity in sentence length and number, complexity, and metaphor domains provided broader contextual information, explaining its high F1 score. Despite these positive outcomes fine-tuning did not achieve better performance on TroFi. TroFi""s suboptimal performance could stem from training difficulty caused by the sentence-level approach and label imbalance. However, these arguments can be refuted by TroFi""s substantial representation in the concatenated dataset (75.1%, table 1), which showed exceptionally good results. Thus, TroFi""s characteristics must have significantly contributed to these positive results. The chosen LLM models might also contribute to the low result. Being condensed models, DistilBERT and all-Mini-LM-L6-v6 are useful for initial experiments but might not generalise well on a domain-specific task like MD. Using SetFit on a state-of-the-art Sentence Transformer model like all-mpnet-base-v2 (Song et al. 2020) may help improve results. However, the core difficulty of finding concrete explanations for the poor TroFi results and the excellent concatenated dataset and PoFo results is the interpretability limitation of LLMs due to their ""black-box"" nature. Contrary to statistical machine learning approaches, no information is provided about which features contribute most to the classification task during LLM fine-tuning, making the reasoning behind the model""s classification result not entirely comprehensible (Dobson 2023, 431). This is especially concerning for the Digital Humanities, where understanding the domain is just as important as raw performance. Ablation techniques and visualisations of attention weights could help understanding how model output was created. This paper contributed to the understudied task of literary MD by applying state-of-the-art NLP methodology, like fine-tuning the Transformer model DistilBERT and few-shot learning with the Sentence Transformer approach, on four literary metaphor datasets. Metaphor was defined quite narrowly as consisting of one of two specific POS sequences. The evaluation baseline was the combined rule-based and statistical approach of Kesarwani et al. (2017). The results demonstrate performance increases in F1 score for the fine-tuning approach over the baseline and even more so for the SetFit methods, especially for PoFo (12.41% increase) and the concatenated dataset (10.37% increase). However, improvement was not observed for the TroFi dataset, which could stem from sentence complexity and label imbalance or from small model sizes. These findings emphasise that while the current practise of fine-tuning LLMs for linguistic MD can also yield good results for literary MD and that SetFit is a valuable tool for small datasets, these methods do not guarantee improved performance. Due to the black-box nature of LLMs they might not be the right tool for literary scholars, who prioritise interpretability. Further work needs to be performed to establish whether larger models could optimise the work done in this paper. Future studies on literary MD could focus on creating larger datasets with different kinds of metaphors or employ prompt engineering. The fine-tuned models could be deployed to build interactive tools for teaching and studying metaphors in educational settings."
2025,DHd2025,FISCHER_Frank_Wikipedia_als_Hallraum_der_Kanonizit_t___1001_.xml,"Wikipedia als Hallraum der Kanonizität: ""1001 Books You Must Read Before You Die""","Jonas Rohe (Freie Universität Berlin, Deutschland); Viktor J. Illmer (Freie Universität Berlin, Deutschland); Lisa Poggel (Freie Universität Berlin, Deutschland); Frank Fischer (Freie Universität Berlin, Deutschland)","Wikipedia, Wikidata, Weltliteratur, Kanon, QRank","Bewertung, Daten, Literatur, Werkzeuge","Wikipedia-Sitelinks (auch: Interwiki-Links) sind Verbindungen zwischen einem Wikipedia-Artikel und den entsprechenden Artikeln in anderen Sprachversionen der Wikipedia. Diese Links ermöglichen es, schnell zwischen verschiedenen Sprachversionen eines Lemmas zu wechseln. Für unsere Untersuchung ist die Anzahl verschiedener Sprachversionen zu einem Lemma von besonderem Interesse, wie anhand eines Beispiels demonstriert werden soll. Derzeit gibt es aktive Wikipedia-Versionen für 331 verschiedene Sprachen (Wikipedia statistics 2024). Zum englischen Schriftsteller Charles Dickens gibt es etwa 162 Sitelinks, das heißt, es gibt über den Autor derzeit Artikel in 162 verschiedenen Sprachen in der Wikipedia. Zu seinem Roman ""A Tale of Two Cities"" gibt es momentan 51 Sitelinks (Wikidata 2024a). Im Weltliteratur-Diskurs verbreitet sich seit einiger Zeit die Idee, die Anzahl von Wikipedia-Sitelinks als Teil der ""Metrics of World Literature"" zu verwenden (Robinson 2017), als ""a simple measure of canonicity"" (Kukkonen 2020, S. 244; vgl. auch Fischer et al. 2023). Das bloße Vorhandensein von mehreren Sprachversionen kann dabei als Kanonizitätsmarker verstanden werden. Diese Idee greifen wir auf und möchten sie anhand eines konkreten Kanonprojekts entwickeln.  Diese Tabelle ist unser Ausgangspunkt für die Anreicherung mit Normdaten mithilfe von OpenRefine, das wir in der Version 3.8.2 genutzt haben. Über den Reconciling Service konnten wir alle Autor*innen und eine Großzahl der Werke mit ihren Wikidata-Einträgen verknüpfen. Insgesamt haben von den 1318 Werken bis dato 1257 einen Wikidata-Eintrag. Die mit Wikidata-IDs angereicherte Tabelle haben wir neben anderen Materialien und dem Code in unserem GitHub-Repositorium veröffentlicht (Rohe et al. 2024 bzw. Trotz der von Boxall im Vorwort zur zweiten Auflage angekündigten Diversifizierung des Kanons durch Erweiterung um nicht-englischsprachige Werke (vgl. Boxall 2008) wird nach Betrachtung der Geocodes offenkundig, dass sich die Auswahl weiterhin stark auf englischsprachige Texte konzentriert und sich insgesamt ein eurozentristischer Blick auf Weltliteratur manifestiert, der sich auch in den Neuausgaben nicht grundlegend ändert (Abb. 2). Es dominieren Texte aus Großbritannien (347), gefolgt von den USA (266), Frankreich (106), Deutschland (57) sowie Italien und Russland (je 30). Boxall schreibt: ""Does a body of writing, a canon of essential texts, emerge from a national context, or does it in some way transcend nationality, rising above the contexts that generate it? What does it mean to try to respond to all of these different national contexts at the same time? Is it possible to produce a list that can speak at once to the readers in Turkey and in Greece, in Serbia and Croatia?"" (Boxall 2008)  Nachdem die Zusammensetzung des Korpus beschrieben wurde, wenden wir uns nun der eigentlichen Analyse zu. Dafür betrachten wir die Wikipedia-Sitelinks und den QRank der aufgeführten Werke und Autor*innen als ""simple measure of canonicity"".  Werk- und Autor*innendaten sind somit mit zwei verwandten langfristigen ""Relevanzmaßen"" angereichert. Die vollständige Tabelle mit den Relevanzmaßen kann in unserem GitHub-Repositorium eingesehen bzw. live berechnet werden.  Im Gegensatz zu den Werken tun sich bei den Autor*innen keine Wikipedia-Lücken auf, alle haben mindestens einen zugehörigen Wikipedia-Artikel (Abb. 4). Die Mittelwerte liegen hier deutlich höher: Autor*innen haben im Durchschnitt 58,6 Sitelinks mit einem Medianwert von 48. Tabelle 1 zeigt für die Autor*innen die Korrelation zwischen QRanks und Sitelinks, konkret die zehn Autor*innen mit den höchsten QRanks. Rankt man die Top-10 nach Anzahl der Sitelinks, ergeben sich interessante Önderungen (Tab. 2). Hier befindet sich nur ein englischsprachiger Autor unter den ersten zehn. Für die Werke haben wir die QRank-Sitelink-Korrelation visualisiert (Abb. 5). Erwartungsgemäß zeigt sich ein positiver Zusammenhang zwischen der Anzahl an Sitelinks und dem QRank der Werke. Werke mit einer höheren Anzahl an Sitelinks weisen im Durchschnitt einen höheren QRank auf. Es wurde eine quadratische Regression durchgeführt, um den Zusammenhang zwischen der Anzahl an Werk-Sitelinks (x) und dem Werk-QRank (y) zu untersuchen. Diese konnte einen Wert von 0,72 für das Bestimmtheitsmaß R¬≤ erreichen, was darauf hinweist, dass 72¬†% der Varianz der QRank-Variable durch die Anzahl der Sitelinks und die quadratische Komponente erklärt werden.   Insgesamt hat sich gezeigt, dass sich Wikipedia und besonders die Wikipedia-Sitelinks gut eignen als ""Hallraum für Kanonizität"". Zu ausnahmslos allen Autor*innen sowie über 90% der Werke des Beispielkorpus gibt es mindestens einen Wikipedia-Artikel. Die Korrelation mit QRank hat gezeigt, dass es andere hilfreiche Ranking-Varianten gibt, die sich ebenso leicht heranziehen lassen. "
2025,DHd2025,JANNIDIS_Fotis_M_glichkeiten_und_Grenzen_der_KI_gest_tzten_A.xml,Möglichkeiten und Grenzen der KI-gestützten Annotation am Beispiel von Emotionen in Lyrik,"Merten Kröncke (Universität Würzburg, Deutschland); Fotis Jannidis (Universität Göttingen, Deutschland); Leonard Konle (Universität Göttingen, Deutschland); Simone Winko (Universität Würzburg, Deutschland)","Textannotation, Sprachmodelle, ChatGPT","Annotieren, Bewertung, Literatur, Forschungsprozess, Software"," Einleitung Die rasante Entwicklung der Verarbeitung natürlicher Sprache durch neuronale Netze hat in den letzten 11 Jahren auch die Arbeitsweisen der digitalen Geisteswissenschaften deutlich verändert. Die Entwicklung neuronaler Architekturen hat zwei Ansätze ermöglicht, die auch heute das NLP bestimmen: 1. ""Finetuning"": Ein Sprachmodell wird auf vielen Daten vortrainiert und dann auf deutlich weniger Daten für eine bestimmte Aufgabe feinjustiert. 2. ""Chat"": Sehr große Sprachmodelle werden auf sehr vielen Daten vortrainiert und dann in einem zweiten Schritt auf die Kommunikation mit Anwendern hin eingerichtet. Der Finetuning-Ansatz hat sich schnell in den Digital Humanities durchgesetzt. Allerdings ist er mit vergleichsweise hohen Kosten verbunden, da die Leistungsfähigkeit für das Finetuning stark mit der Anzahl der Trainingsbeispiele korreliert. Deswegen ist die Verwendung von sehr großen Sprachmodellen ohne eine größere Anzahl von Trainingsbeispielen (zero-shot oder few-shot prompting) besonders interessant, schließlich muss in einem solchen Kontext nur eine kleine Menge von Testdaten annotiert werden. Eine Antwort auf die Frage, ob in einem Forschungsprojekt der klassische Finetuning-Ansatz durch zero-shot oder few-shot prompting in sehr großen Sprachmodellen ersetzt werden kann, ist nicht einfach, da die Antwort von der Komplexität der Aufgabenstellung ebenso abhängt wie vom Zeitpunkt, zu dem man die Frage stellt: Die Finetuning-Ansätze entwickeln sich ebenso weiter wie die sehr großen Sprachmodelle. Dazu kommen pragmatische Fragen: Hat die Arbeitsgruppe Zugriff auf die technische Infrastruktur, die man für das Finetuning von großen Sprachmodellen benötigt? Hat sie die finanziellen Ressourcen, um die kommerziellen Modelle für umfangreiche Annotationsaufgaben zu verwenden? Unser Beitrag will eine (wenn auch nur temporär gültige) Antwort für einen bestimmten Bereich liefern, die Annotation von Emotionen in literarischen Texten, und dadurch zugleich an der Diskussion darüber mitwirken, wie in den DH eine Antwort auf jene Frage gefunden werden kann. Grundlage für unsere Arbeit sind die Annotationen von lyrischen Texten im Rahmen des DFG-Projekts The Beginnings of Modern Poetry (https://uni-goettingen.de/moderne-lyrik/). Die annotierten Texte haben wir drei großen Sprachmodellen mit der Aufgabe vorgelegt, jeweils eine Strophe mit Blick auf das Vorkommen von sechs Emotionsgruppen zu annotieren. Dabei haben wir nach einigen Vorstudien systematisch zwei Aspekte variiert: kurzer vs. langer Prompt und einfach randomisiertes vs. stratifiziert randomisiertes Sampling. Forschungsstand Die Möglichkeit, ChatGPT und verwandte Dienste zur Annotation von Daten zu verwenden, wurde sehr schnell erkannt. Ding et al. 2023 beobachten bei wenigen und klar definierten Labeln gute bis sehr gute Resultate, sehen allerdings auch eine deutliche Varianz abhängig vom Prompt. Öhnlich optimistische Ergebnisse haben Gilardi et al. 2023 erzielt. Törnberg 2023 vergleicht ChatGPT4s Annotationen von Tweets 'wird eine republikanische oder eine demokratische Position vertreten? 'mit denen von Expert:innen und von Arbeitern von Mechanical Turk und kommt zu dem Ergebnis, dass die Ergebnisse von ChatGPT deutlich besser und konsistenter sind als die der beiden menschlichen Gruppen. Reiss 2023 warnt allerdings davor, ChatGPT als Annotationswerkzeug ohne manuelle Datenvalidierung zu verwenden, da das System sehr empfindlich auf die Manipulation einzelner Wörter und Einstellungen reagiert. Rebora et al. 2023 vergleichen für die Aufgabe der Sentiment Analysis ChatGPT mit einem Finetuning-Modell und kommen zu dem Ergebnis, dass letzteres immer noch bessere Ergebnisse liefert (ähnlich Wang 2023). Die Diskrepanzen zwischen den Ergebnissen lassen sich durch drei Aspekte gut erklären: Wie schwierig ist die Aufgabe? Named Entity Recognition ist einfacher als Sentiment Analyse usw. Welches Modell wurde verwendet? Alle Aufsätze, die wir gesichtet haben, verwenden (auch) ChatGPT, aber OpenAI bietet zu einem Zeitpunkt unterschiedliche Modelle mit unterschiedlichen Leistungsniveaus an 'und die Modelle werden laufend aktualisiert. Was ist der Referenzpunkt des Vergleichs? Zum einen geht es um die Frage, ob man menschliche Annotator:innen durch große Sprachmodelle ersetzen kann, zum anderen darum, ob ChatGPT & Co. die Leistungsfähigkeit von Finetuning-Modellen erreichen.  Für die Promptgestaltung haben wir uns an den Empfehlungen von https://www.promptingguide.ai/ orientiert. Nach dem Überblick von Vatsal und Dubey 2024 gibt es keine klare Empfehlung zum Prompting bei Emotionsannotationen. Ressourcen Das Untersuchungskorpus besteht aus Texten in Lyrikanthologien, die sich auf Gedichte von Zeitgenoss:innen konzentrieren. Die Anthologien stammen aus der Zeit von 1859 bis 1919 und enthalten mehr als 6000 Gedichte, von denen 1412 manuell annotiert wurden (vgl. Winko et al. 2022a, Winko et al. 2022b).  Die Emotionsannotation zielt darauf ab, die im Text gestalteten Emotionen (und nicht die Emotionen der Leser:innen) zu erfassen. Möglich war, einer Textstelle genau eine, aber auch keine oder mehrere Emotionen zuzuweisen. Genutzt wurde ein Set von 40 diskreten Emotionen, darunter zum Beispiel Hoffnung, Sehnsucht oder Hass. Die Annotationseinheiten sind Wörter bzw. Wortfolgen (vgl. Kröncke et al. 2022). Da für viele einzelne Emotionen nur eine sehr geringe Zahl von Annotationen vorliegt, werden die Emotionen nachträglich zu sechs Gruppen zusammengefasst, orientiert an Shaver et al. 1987: Liebe (Love), Freude (Joy), Trauer (Sadness), Erregung/Überraschung (Agitation), Angst (Fear) und Wut (Anger).  Für das maschinelle Lernen wurde der Task leicht angepasst. Um die Komplexität der Aufgabe und den Rechenaufwand zu reduzieren, haben wir eine bestimmte Segmentierung vorgegeben, nämlich die Einheit ""Strophe"". Die Multi-Label-Klassifikation basiert auf dem (mithilfe einer großen Zahl manueller Annotationen trainierten) Modell SauerkrautLM-7B-HerO und wurde für die sechs Emotionsgruppen nach Shaver et al. 1987 durchgeführt. Sowohl das Korpus als auch das Annotationsverfahren und das maschinelle Lernen haben wir an anderen Stellen bereits ausführlicher erläutert (Konle et al. 2022, Konle et al. 2024).  Die folgenden Experimente basieren auf zwei Samples aus dem Gesamtkorpus: Zum einen verwenden wir ein einfach randomisiertes Sample (350 Strophen), das die im Korpus de facto vorhandenen Häufigkeitsverhältnisse der Emotionsklassen widerspiegeln soll, zum anderen ein stratifiziert randomisiertes Sample (ebenfalls 350 Strophen: 50 x jede der 6 Emotionsgruppen + 50 Strophen ohne Emotion), das eine gewisse Mindesthäufigkeit pro Emotionsgruppe garantiert, aber durch die Kookkurrenz von Emotionsgruppen ebenfalls nicht zu einer Gleichverteilung führt.1  Methoden In allen Experimenten lassen wir Chat-Modelle Fragen zu den Emotionen in lyrischen Texten beantworten. Der Task ist der gleiche, den bereits das Finetuning-Modell SauerkrautLM-7B-HerO übernommen hat, also die Zuweisung von keiner, einer oder mehreren der sechs Emotionsgruppen nach Shaver et al. 1987 zu einzelnen Strophen. Wir verwenden drei (kommerzielle) Modelle: GPT4o (OpenAI), Claude (Anthropic) und Gemini (Google).  In unseren Experimenten testen wir einen kurzen und einen langen Prompt. Der kurze Prompt enthält keine Erläuterungen der Emotionskonzepte und keine Annotationsbeispiele, der lange Prompt schon. Die Gestaltung des langen Prompts ist durch verschiedene Vorexperimente informiert. Ziel ist unter anderem, die (ansonsten zu große) Häufigkeit, mit der Emotionen zugewiesen werden, zu reduzieren. Wir setzen explizites CoT-Prompting ein und weisen darauf hin, dass hinreichend starke Emotionsindikatoren vorliegen müssen. Insgesamt ergeben sich vier Experimente: 	1.	Simple random Sampling. Short Prompt 	2.	Simple random Sampling. Long Prompt 	3.	Stratified random Sampling. Short Prompt 	4.	Stratified random Sampling. Long Prompt  Ergebnisse Tabelle 2 und 3 geben Auskunft über die Performance der Emotionserkennung. Für das Modell Claude 3.5 Sonnet können wir in der Variante Stratified random Sample 'Long Prompt noch keine Ergebnisse mitteilen, da unsere langen Prompts bei Anthropic wiederholt zur Überschreitung des rate limits geführt haben. Wir planen, das Experiment nachzuholen.  Diskussion Die Performance aller Short-Prompt-Modelle bleibt hinter den Finetuning-Ergebnissen zurück, wenn auch unterschiedlich deutlich. Andere Studien sind auf Basis anderer Daten und anderer Tasks zu ähnlichen Ergebnissen gekommen (etwa Rebora et al. 2023).  Die Long-Prompt-Modelle performen besser als die Short-Prompt-Modelle. Ein besonders deutliches Beispiel ist die Emotionsgruppe Agitation, die von den Short-Prompt-Modellen gar nicht erkannt wird, möglicherweise weil der Begriff in unserem Annotationsdesign ein spezifisches Konzept bezeichnet, das mit der alltagssprachlichen Bedeutung des Worts ""Agitation"" wenig gemein hat.2 In einigen Fällen reicht die Performance der besten Long-Prompt-Modelle fast an die Finetuning-Ergebnisse heran, z. B. im Fall von ""Love"", oder zieht gleich, etwa im Fall von ""Sadness"". Im Stratified random Sample performen die Modelle entweder ungefähr gleich gut oder deutlich besser (Anger, Fear) als im Simple random Sample. Relevant dafür ist allerdings, dass Anger und Fear im Simple random Sample nur selten vorkommen, weshalb die entsprechenden Ergebnisse nicht allzu belastbar sind. Zwischen den drei Modellen GPT4o, Gemini 1.5 und Claude 3.5 Sonnet zeigen sich je nach Sample und je nach Prompt einige Unterschiede. Im Simple random Sample ist die Performance von GPT4o oder Claude 3.5 Sonnet am besten, im Stratified random Sample von GPT4o (wobei hier für Claude 3.5 Sonnet in der Long-Prompt-Version keine Daten verfügbar sind). Die bisherigen Beobachtungen haben sich am F1-Score orientiert. Unterscheidet man Precision und Recall, werden weitere Befunde sichtbar. Das gilt nicht zuletzt für die Erkennung von solchen Strophen, die keine Emotion enthalten (vgl. Tabelle 4, exemplarisch für das Simple random Sample). Für alle Modelle und für alle Prompts gilt,3 dass bei der Erkennung von Emotionslosigkeit die Precision höher als der Recall ist. Der Befund hängt damit zusammen, dass die Modelle den Strophen häufiger Emotionen (bzw. seltener keine Emotionen) als menschliche Annotator:innen zuschreiben. Erklärungsrelevant dürfte sein, dass manuell ""sparsam"" annotiert werden sollte, auch mit Rücksicht auf das Inter-Annotator-Agreement. Die Modelle sind nicht an diese Annotationspraxis gebunden, wenngleich der lange Prompt sie (wie beabsichtigt) in diese Richtung zu lenken scheint, immerhin steigt hier der Recall, besonders bei GPT4o und Gemini 1.5. Das wichtigste Ergebnis unserer Studie ist, dass die sehr großen Sprachmodelle auch bei einer komplexen Aufgabe wie der Annotation von Emotionen teilweise das Niveau von Finetuning-Modellen erreichen, aber die Ergebnisse abhängig von der Kategorie stark und in schwer zu prognostizierender Weise schwanken. Die große und nur teilweise transparente Varianz in Abhängigkeit von der Promptgestaltung gilt es ebenfalls zu berücksichtigen. Zahlreiche weitere Studien sind denkbar. Aufschlussreich wäre, die Experimente auch für die 40 Einzelemotionen (statt nur für die 6 Emotionsgruppen) durchzuführen. Zudem lassen sich die Prompts anpassen, etwa insofern als den Modellen eine bestimmte Rolle zugewiesen wird (‚ÄúDu bist Expertin für ‚Ä¶‚Äù, ‚ÄúDu bist eine Person des 19. Jahrhunderts‚Äù usw.).4 Des Weiteren wäre zu testen, ob sich die Performance ändert, wenn der Task modifiziert oder anders modelliert wird, zum Beispiel inklusive Segmentierung (die Teil der manuellen Annotation ist) und/oder als Reihe binärer Klassifikationen. Die binäre Klassifikation haben wir exemplarisch getestet. Es zeigte sich, dass das Modell nun seltener (statt wie im bisherigen Setup häufiger) als menschliche Annotator:innen Emotionsgruppen zuweist.5 Der Befund deutet abermals auf die große Varianz des Modellverhaltens hin. Schließlich wäre informativ, das Inter-Annotator-Agreement zwischen menschlichen Annotator:innen mit dem Agreement zwischen Sprachmodellen zu vergleichen. Dass die Performanz je nach Kategorien und Prompts stark variiert, veranlasst uns zu folgendem Schluss: Auch wenn die Sprachmodelle ständig verbessert werden, wird man wohl auf absehbare Zeit nicht ohne die Entwicklung von Annotationsguidelines und die Annotation von ausreichend Testdaten auskommen.  Bibliographie Fügen Sie hier die von Ihnen benutzten Quellen ein:  Ding, Bosheng, Chengwei Qin, Linlin Liu, Yew Ken Chia, Shafiq Joty, Boyang Li und Lidong Bing. 2023. ‚ÄúIs GPT-3 a Good Data Annotator?‚Äù arXiv. http://arxiv.org/abs/2212.10450. Gilardi, Fabrizio, Meysam Alizadeh und Ma√´l Kubli. 2023. ‚ÄúChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks.‚Äù In Proceedings of the National Academy of Sciences 120. 30: e2305016120. https://doi.org/10.1073/pnas.2305016120. Konle, Leonard, Merten Kröncke, Fotis Jannidis und Simone Winko. 2022. ‚ÄúEmotions and Literary Periods.‚Äù In DH2022: Responding to Asian Diversity. Conference Abstracts, July 25‚Äì29, 2022, Tokyo, Japan, 278‚Äì281. Konle, Leonard, Merten Kröncke, Fotis Jannidis und Simone Winko. 2024. ‚ÄúOn the Unity of Literary Change. The Development of Emotions in German Poetry, Prose, and Drama between 1850 and 1920 as a Test Case.‚Äù In CHR 2024: Computational Humanities Research Conference, December 4‚Äì6, 2024, Aarhus, Denmark [eingereicht]. Kröncke, Merten, Fotis Jannidis, Leonard Konle und Winko, Simone. 2022. ‚ÄúAnnotationsrichtlinien Emotionsmarker und Emotionen.‚Äù Zenodo. https://doi.org/10.5281/zenodo.6021152. Rebora, Simone, Marina Lehmann, Anne Heumann, Wei Ding und Gerhard Lauer. 2023. ‚ÄúComparing ChatGPT to Human Raters and Sentiment Analysis Tools for German Children""s Literature.‚Äù In CHR 2023: Computational Humanities Research Conference, December 6‚Äì8, 2023, Paris, France, 333‚Äì343. Reiss, Michael V. 2023. ‚ÄúTesting the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark‚Äù. arXiv. https://doi.org/10.48550/arXiv.2304.11085. Shaver, P., J. Schwartz, D. Kirson und C O""Connor. 1987. ‚ÄúEmotion Knowledge: Further Exploration of a Prototype Approach.‚Äù In Journal of Personality and Social Psychology 52.6: 1061‚Äì1086.¬† Törnberg, Petter. 2023. ‚ÄúChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning‚Äù. arXiv. https://doi.org/10.48550/arXiv.2304.06588. Vatsal, Shubham und Harsh Dubey. 2024. ‚ÄúA Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks.‚Äù arXiv. https://doi.org/10.48550/arXiv.2407.12994. Winko, Simone, Konle, Leonard, Kröncke, Merten und Fotis Jannidis. 2022a. ‚ÄúLyrik-Anthologien 1850-1910.‚Äù Zenodo. https://doi.org/10.5281/zenodo.6053952. Winko, Simone, Konle, Leonard, Kröncke, Merten und Fotis Jannidis. 2022b. ‚ÄúKorpusbeschreibung der Lyrik-Anthologien 1850-1910.‚Äù Zenodo. https://doi.org/10.5281/zenodo.6204787. "